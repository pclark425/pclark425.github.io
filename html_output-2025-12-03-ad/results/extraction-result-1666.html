<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1666 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1666</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1666</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-248266477</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2204.09667v2.pdf" target="_blank">Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments</a></p>
                <p><strong>Paper Abstract:</strong> Recent work in Vision-and-Language Navigation (VLN) has presented two environmental paradigms with differing realism -- the standard VLN setting built on topological environments where navigation is abstracted away, and the VLN-CE setting where agents must navigate continuous 3D environments using low-level actions. Despite sharing the high-level task and even the underlying instruction-path data, performance on VLN-CE lags behind VLN significantly. In this work, we explore this gap by transferring an agent from the abstract environment of VLN to the continuous environment of VLN-CE. We find that this sim-2-sim transfer is highly effective, improving over the prior state of the art in VLN-CE by +12% success rate. While this demonstrates the potential for this direction, the transfer does not fully retain the original performance of the agent in the abstract setting. We present a sequence of experiments to identify what differences result in performance degradation, providing clear directions for further improvement.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1666.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1666.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN-2-VLNCE (sim-2-sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-2-Sim Transfer of Vision-and-Language Navigation agents from VLN to VLN-CE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper demonstrates sim-2-sim transfer of topological VLN agents (VLN BERT) into continuous 3D environments (VLN-CE) using a modular harness (subgoal generation + navigation policy), improving VLN-CE success rate and diagnosing the sources of transfer loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>VLN-CE BERT (reconstruction-trained VLN BERT + SGM + Local Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A vision-and-language navigation agent based on VLN BERT (recurrent transformer) adapted to continuous environments by (1) a learned Subgoal Generation Module to produce radial subgoal candidates from panoramas and a 360° laser scan, and (2) a Local Policy (mapping & planning) to execute low-level VLN-CE actions (FORWARD 0.25m, TURN ±15°, STOP). It is designed to follow natural-language navigation instructions in previously unseen indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied navigation / vision-and-language navigation (robotic navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat Simulator (Habitat-Sim) with Matterport3D scene reconstructions</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>3D reconstructed indoor scenes rendered in Habitat-Sim: panoramic RGB (reconstructed renders), depth, a navigation mesh for pathfinding/geodesic distances, and an emulated 360° 2D laser scanner (radial occupancy map). The simulator provides collision-free navigation via a navmesh and allows low-level VLN-CE actions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Medium-fidelity reconstructed 3D rendering with navigation-mesh-based kinematics and sensor emulation (panoramic RGB, depth, 2D laser); not full physical/actuator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Scene geometry via 3D mesh reconstructions; panoramic RGB rendering; depth; navigation mesh (geodesic distance and collision-free traversability); emulated 2D laser scanner (radial occupancy) and occupancy mapping for local planning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Photorealism limited by reconstruction artifacts and lighting mismatch (reconstruction renders differ from original MP3D panoramas); no explicit actuator dynamics (motor delays, torque), no tactile/contact dynamics, limited/no sensor noise modeling beyond rendering differences, simplified assumption that unexplored map areas are free space for Local Policy, and SGM training uses 2D occupancy which poorly captures elevation/subfloor geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Topological instruction-following/navigation priors learned in the abstract/topological VLN setting transferred to continuous low-level action execution in VLN-CE (instruction-guided navigation / subgoal selection).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Pretrained VLN BERT (supervised / pretraining regime used in VLN literature) and then fine-tuned in VLN-CE via imitation learning (teacher forcing) on SGM-generated candidate distribution; SGM trained to minimize Sinkhorn divergence to nav-graph subgoals using Habitat renders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR) (primary), with supplemental metrics: SPL, navigation error (NE), oracle success (OS), trajectory length (TL).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>After transfer and simulator fine-tuning, the submitted agent achieved 44% SR on VLN-CE Test, improving prior published VLN-CE SOTA of 32% SR by +12 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Identified sim-2-sim gap factors that also map to sim-to-real: (1) visual domain gap between high-quality Matterport3D panoramas and reconstructed Habitat renders (lighting, furniture resolution, missing objects); (2) dataset subset differences (conversion / episode differences); (3) navigation errors introduced by realistic local policies (pose jitter, failures to reach subgoals); (4) mismatch between nav-graph subgoal distribution and learned Subgoal Generation Module (SGM) outputs, especially poor recall on elevation-changing subgoals (stairs); (5) simplified sensor/modeling assumptions (e.g., assuming unexplored space free).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Key enabling choices: training the VLN agent and SGM on reconstructed Habitat renders (reduces visual domain gap), modular harness (drop-in SGM + Local Policy) that emulates nav-graph candidates, using an Oracle or high-quality navigation policy to show navigation is not the primary bottleneck, and fine-tuning VLN BERT on the SGM candidate distribution (imitation learning) to adapt priors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Qualitative requirements: alignment of visual domain (train models on reconstructed renders to avoid visual overfitting), subgoal candidate generation that captures elevation changes (multi-floor awareness), and reliable local navigation (low navigation error); no quantitative fidelity thresholds were specified.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning was performed in simulation: VLN BERT was fine-tuned with teacher forcing on SGM candidate distributions (batch size 12, learning rate 1e-7, early stopping) for ≈12 GPU-hours to partially recover performance under SGM-generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Visual fidelity: switching VLN evaluation from MP3D panoramas to Habitat reconstructed renders caused drops of 10–13 SR in seen splits and ~3 SR in unseen; retraining on reconstructed renders recovered most loss. Navigation fidelity: Teleportation vs Oracle produced similar performance (Oracle approximates teleportation); Local Policy induced a drop of ~3 SR (Val-Seen) and ~5 SR (Val-Unseen) vs Oracle, with higher tail failure rates (>0.5m errors). Subgoal fidelity: using an SGM (online predicted candidates) caused substantial drops (~12 SR Val-Seen, ~8 SR Val-Unseen) relative to nav-graph subgoals; even an optimal discretized SGM mapping caused 3–6 SR drops.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sim-2-sim transfer from topological VLN to continuous VLN-CE is effective and yields state-of-the-art VLN-CE performance (+12 SR vs prior work) when models and modules are adapted to the reconstruction-rendered domain; however, significant transfer loss remains, primarily from subgoal candidate generation (especially for elevation changes), visual domain gap between original panoramas and reconstructed renders, and navigation errors from realistic local policies. Training on reconstructed renders and fine-tuning on SGM outputs mitigates but does not eliminate the gaps, indicating where higher simulation fidelity or improved modules are required for robust transfer (and by extension, for future sim-to-real work).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1666.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1666.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anderson2020 sim-to-real (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer for vision-and-language navigation (Anderson et al., CoRL 2020) - referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that directly transferred VLN agents to real-world robots by emulating VLN action space with subgoal prediction and executing navigation with ROS; reported large performance drops when moving to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>VLN agents transferred to real robots (as reported by Anderson et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Topological VLN agents adapted for real-world execution by predicting subgoal candidates and using a robot navigation stack (Robot Operating System) to execute actions in the physical environment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied navigation / sim-to-real robotic navigation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Not detailed in this paper; referenced as prior sim-to-real effort that emulated VLN action space with predicted subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real-world environments used for evaluating transferred VLN agents (details are in the referenced paper); navigation executed via ROS stack.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Instruction-following navigation policies (VLN) transferred from simulator to real robot execution.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Reported in referenced work as drop in performance (the paper states 'more than 2x drop in performance' when evaluating agents in reality).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Referenced result: more than 2x drop in performance on real-world evaluation relative to simulation (exact numbers not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mentioned alignment and candidate-generation issues analogous to the sim-2-sim gaps here: misalignment between nav-graph candidates and subgoal predictions; navigation-stack execution differences; visual and pose mismatch between simulator and reality.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>They emulated VLN action space with subgoal candidate prediction and executed navigation via ROS, but the work found large performance degradation in reality, indicating these were insufficient for robust sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The referenced sim-to-real effort experienced a substantial (>2x) drop in performance when moving from simulator to real-world execution, demonstrating that effective sim-to-real transfer for VLN agents remains an open challenge and motivating the sim-2-sim diagnostic approach used in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>On evaluation of embodied navigation agents <em>(Rating: 2)</em></li>
                <li>Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation <em>(Rating: 2)</em></li>
                <li>Robothor: an open simulation-to-real embodied ai platform <em>(Rating: 1)</em></li>
                <li>Splitnet: sim2sim and task2task transfer for embodied visual navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1666",
    "paper_id": "paper-248266477",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "VLN-2-VLNCE (sim-2-sim)",
            "name_full": "Sim-2-Sim Transfer of Vision-and-Language Navigation agents from VLN to VLN-CE",
            "brief_description": "This paper demonstrates sim-2-sim transfer of topological VLN agents (VLN BERT) into continuous 3D environments (VLN-CE) using a modular harness (subgoal generation + navigation policy), improving VLN-CE success rate and diagnosing the sources of transfer loss.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "VLN-CE BERT (reconstruction-trained VLN BERT + SGM + Local Policy)",
            "agent_system_description": "A vision-and-language navigation agent based on VLN BERT (recurrent transformer) adapted to continuous environments by (1) a learned Subgoal Generation Module to produce radial subgoal candidates from panoramas and a 360° laser scan, and (2) a Local Policy (mapping & planning) to execute low-level VLN-CE actions (FORWARD 0.25m, TURN ±15°, STOP). It is designed to follow natural-language navigation instructions in previously unseen indoor environments.",
            "domain": "embodied navigation / vision-and-language navigation (robotic navigation)",
            "virtual_environment_name": "Habitat Simulator (Habitat-Sim) with Matterport3D scene reconstructions",
            "virtual_environment_description": "3D reconstructed indoor scenes rendered in Habitat-Sim: panoramic RGB (reconstructed renders), depth, a navigation mesh for pathfinding/geodesic distances, and an emulated 360° 2D laser scanner (radial occupancy map). The simulator provides collision-free navigation via a navmesh and allows low-level VLN-CE actions.",
            "simulation_fidelity_level": "Medium-fidelity reconstructed 3D rendering with navigation-mesh-based kinematics and sensor emulation (panoramic RGB, depth, 2D laser); not full physical/actuator fidelity.",
            "fidelity_aspects_modeled": "Scene geometry via 3D mesh reconstructions; panoramic RGB rendering; depth; navigation mesh (geodesic distance and collision-free traversability); emulated 2D laser scanner (radial occupancy) and occupancy mapping for local planning.",
            "fidelity_aspects_simplified": "Photorealism limited by reconstruction artifacts and lighting mismatch (reconstruction renders differ from original MP3D panoramas); no explicit actuator dynamics (motor delays, torque), no tactile/contact dynamics, limited/no sensor noise modeling beyond rendering differences, simplified assumption that unexplored map areas are free space for Local Policy, and SGM training uses 2D occupancy which poorly captures elevation/subfloor geometry.",
            "real_environment_description": null,
            "task_or_skill_transferred": "Topological instruction-following/navigation priors learned in the abstract/topological VLN setting transferred to continuous low-level action execution in VLN-CE (instruction-guided navigation / subgoal selection).",
            "training_method": "Pretrained VLN BERT (supervised / pretraining regime used in VLN literature) and then fine-tuned in VLN-CE via imitation learning (teacher forcing) on SGM-generated candidate distribution; SGM trained to minimize Sinkhorn divergence to nav-graph subgoals using Habitat renders.",
            "transfer_success_metric": "Success Rate (SR) (primary), with supplemental metrics: SPL, navigation error (NE), oracle success (OS), trajectory length (TL).",
            "transfer_performance_sim": "After transfer and simulator fine-tuning, the submitted agent achieved 44% SR on VLN-CE Test, improving prior published VLN-CE SOTA of 32% SR by +12 percentage points.",
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Identified sim-2-sim gap factors that also map to sim-to-real: (1) visual domain gap between high-quality Matterport3D panoramas and reconstructed Habitat renders (lighting, furniture resolution, missing objects); (2) dataset subset differences (conversion / episode differences); (3) navigation errors introduced by realistic local policies (pose jitter, failures to reach subgoals); (4) mismatch between nav-graph subgoal distribution and learned Subgoal Generation Module (SGM) outputs, especially poor recall on elevation-changing subgoals (stairs); (5) simplified sensor/modeling assumptions (e.g., assuming unexplored space free).",
            "transfer_enabling_conditions": "Key enabling choices: training the VLN agent and SGM on reconstructed Habitat renders (reduces visual domain gap), modular harness (drop-in SGM + Local Policy) that emulates nav-graph candidates, using an Oracle or high-quality navigation policy to show navigation is not the primary bottleneck, and fine-tuning VLN BERT on the SGM candidate distribution (imitation learning) to adapt priors.",
            "fidelity_requirements_identified": "Qualitative requirements: alignment of visual domain (train models on reconstructed renders to avoid visual overfitting), subgoal candidate generation that captures elevation changes (multi-floor awareness), and reliable local navigation (low navigation error); no quantitative fidelity thresholds were specified.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Fine-tuning was performed in simulation: VLN BERT was fine-tuned with teacher forcing on SGM candidate distributions (batch size 12, learning rate 1e-7, early stopping) for ≈12 GPU-hours to partially recover performance under SGM-generated candidates.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Visual fidelity: switching VLN evaluation from MP3D panoramas to Habitat reconstructed renders caused drops of 10–13 SR in seen splits and ~3 SR in unseen; retraining on reconstructed renders recovered most loss. Navigation fidelity: Teleportation vs Oracle produced similar performance (Oracle approximates teleportation); Local Policy induced a drop of ~3 SR (Val-Seen) and ~5 SR (Val-Unseen) vs Oracle, with higher tail failure rates (&gt;0.5m errors). Subgoal fidelity: using an SGM (online predicted candidates) caused substantial drops (~12 SR Val-Seen, ~8 SR Val-Unseen) relative to nav-graph subgoals; even an optimal discretized SGM mapping caused 3–6 SR drops.",
            "key_findings": "Sim-2-sim transfer from topological VLN to continuous VLN-CE is effective and yields state-of-the-art VLN-CE performance (+12 SR vs prior work) when models and modules are adapted to the reconstruction-rendered domain; however, significant transfer loss remains, primarily from subgoal candidate generation (especially for elevation changes), visual domain gap between original panoramas and reconstructed renders, and navigation errors from realistic local policies. Training on reconstructed renders and fine-tuning on SGM outputs mitigates but does not eliminate the gaps, indicating where higher simulation fidelity or improved modules are required for robust transfer (and by extension, for future sim-to-real work).",
            "uuid": "e1666.0"
        },
        {
            "name_short": "Anderson2020 sim-to-real (mentioned)",
            "name_full": "Sim-to-real transfer for vision-and-language navigation (Anderson et al., CoRL 2020) - referenced work",
            "brief_description": "Referenced prior work that directly transferred VLN agents to real-world robots by emulating VLN action space with subgoal prediction and executing navigation with ROS; reported large performance drops when moving to reality.",
            "citation_title": "Sim-to-real transfer for vision-and-language navigation",
            "mention_or_use": "mention",
            "agent_system_name": "VLN agents transferred to real robots (as reported by Anderson et al.)",
            "agent_system_description": "Topological VLN agents adapted for real-world execution by predicting subgoal candidates and using a robot navigation stack (Robot Operating System) to execute actions in the physical environment.",
            "domain": "embodied navigation / sim-to-real robotic navigation",
            "virtual_environment_name": null,
            "virtual_environment_description": "Not detailed in this paper; referenced as prior sim-to-real effort that emulated VLN action space with predicted subgoals.",
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Real-world environments used for evaluating transferred VLN agents (details are in the referenced paper); navigation executed via ROS stack.",
            "task_or_skill_transferred": "Instruction-following navigation policies (VLN) transferred from simulator to real robot execution.",
            "training_method": null,
            "transfer_success_metric": "Reported in referenced work as drop in performance (the paper states 'more than 2x drop in performance' when evaluating agents in reality).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Referenced result: more than 2x drop in performance on real-world evaluation relative to simulation (exact numbers not provided in this paper).",
            "transfer_success": false,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mentioned alignment and candidate-generation issues analogous to the sim-2-sim gaps here: misalignment between nav-graph candidates and subgoal predictions; navigation-stack execution differences; visual and pose mismatch between simulator and reality.",
            "transfer_enabling_conditions": "They emulated VLN action space with subgoal candidate prediction and executed navigation via ROS, but the work found large performance degradation in reality, indicating these were insufficient for robust sim-to-real transfer.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "The referenced sim-to-real effort experienced a substantial (&gt;2x) drop in performance when moving from simulator to real-world execution, demonstrating that effective sim-to-real transfer for VLN agents remains an open challenge and motivating the sim-2-sim diagnostic approach used in this paper.",
            "uuid": "e1666.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer for vision-and-language navigation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "On evaluation of embodied navigation agents",
            "rating": 2,
            "sanitized_title": "on_evaluation_of_embodied_navigation_agents"
        },
        {
            "paper_title": "Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation",
            "rating": 2,
            "sanitized_title": "are_we_making_real_progress_in_simulated_environments_measuring_the_sim2real_gap_in_embodied_visual_navigation"
        },
        {
            "paper_title": "Robothor: an open simulation-to-real embodied ai platform",
            "rating": 1,
            "sanitized_title": "robothor_an_open_simulationtoreal_embodied_ai_platform"
        },
        {
            "paper_title": "Splitnet: sim2sim and task2task transfer for embodied visual navigation",
            "rating": 1,
            "sanitized_title": "splitnet_sim2sim_and_task2task_transfer_for_embodied_visual_navigation"
        }
    ],
    "cost": 0.01383975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments
24 Apr 2022</p>
<p>Jacob Krantz krantzja@oregonstate.edu 
Oregon State University</p>
<p>Stefan Lee 
Oregon State University</p>
<p>Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments
24 Apr 2022vision-and-language navigation (VLN), embodied AI
Recent work in Vision-and-Language Navigation (VLN) has presented two environmental paradigms with differing realism -the standard VLN setting built on topological environments where navigation is abstracted away[3], and the VLN-CE setting where agents must navigate continuous 3D environments using low-level actions[20]. Despite sharing the high-level task and even the underlying instruction-path data, performance on VLN-CE lags behind VLN significantly. In this work, we explore this gap by transferring an agent from the abstract environment of VLN to the continuous environment of VLN-CE. We find that this sim-2-sim transfer is highly effective, improving over the prior state of the art in VLN-CE by +12% success rate. While this demonstrates the potential for this direction, the transfer does not fully retain the original performance of the agent in the abstract setting. We present a sequence of experiments to identify what differences result in performance degradation, providing clear directions for further improvement.</p>
<p>Introduction</p>
<p>Vision-and-Language Navigation (VLN) is a popular instruction-guided navigation task where a vision-equipped agent must navigate to a goal location in an never-before-seen environment by following a path described by a natural language instruction. In the standard VLN setting, the environment is abstracted as a topology of interconnected panoramic images (called a nav-graph) such that navigation amounts to traversing the graph by iteratively selecting among a small set of neighboring node locations at each time step. In effect, this induces a prior of possible locations for the agent and assumes perfect navigation between nodes. Recent work has identified that these assumptions do not reflect the challenges a deployed system would experience in a real environment. To reduce this gap, Krantz et al. [20] introduced the VLN in Continuous Environments (VLN-CE) setting which instantiates VLN in a 3D simulator and drops the nav-graph assumptions -requiring agents to navigate with low-level actions.</p>
<p>So far, VLN-CE has proven to be significantly more challenging than its more abstract counterpart, with published work reporting episode success rates less than half of those reported in standard VLN. Typically, models for VLN-CE have been end-to-end systems trained to directly predict low-level actions (or nearby waypoints) from language and observations. As such, the lower performance compared to VLN is commonly ascribed to the challenge of learning navigation and language grounding jointly in a long-horizon task. This suggests two avenues towards improvement -developing more capable models directly in VLN-CE or exploring transfer of knowledge from VLN to VLN-CE. Given the significant differences between the abstract and continuous settings, the potential of sim-2sim transfer has so far been under-explored and uncertain.</p>
<p>In this work, we explore the sim-2-sim transfer of a VLN agent to VLN-CE. To support this transfer, we build a modular harness such that any VLN agent can be run in VLN-CE. Analogous to sim-2-real experiments from [2], this harness includes a local navigation policy and a subgoal generation module to mimic navgraph provided candidates. We present a sequence of systematic experiments to quantify what differences between the two task settings are most impactful. Our final model demonstrates that transfer can lead to significant improvements over end-to-end VLN-CE-trained methods -achieving an increase of +12% success rate over prior published state-of-the-art (32% vs 44%) on the VLN-CE test set.</p>
<p>Along the way, our analysis quantifies the effect of (1) differences between VLN and VLN-CE data subsets, (2) the visual domain gap between real panoramas and simulated renders in VLN-CE, (3) error induced by navigation policies, and (4) subgoal candidate generation -identifying fruitful directions for future work. Through an analysis of our model's errors, we identify that episodes containing significant elevation change (i.e. stairs) are a challenge for our model.</p>
<p>Overall, our results demonstrate an alternative research direction for improving instruction-guided navigation agents in continuous environments. We show transfer with compelling results and present the community with clear remaining challenges. We will release our model-agnostic VLN-2-VLNCE harness code to facilitate research in this area -helping to unify progress between both settings.</p>
<p>Contributions. To summarize the contributions of this work:</p>
<p>-We present a first-of-its-kind demonstration that instruction-following agents trained in the abstract VLN setting can effectively transfer to VLN-CEsetting a new state of the art in VLN-CE by +12% success rate.</p>
<p>-Through systematic experiments, we quantify performance loss for key steps of the transfer process -providing insight to the community about existing gaps in our transfer paradigm that could be improved.</p>
<p>-We develop a modular VLN-2-VLNCE transfer harness that will be released to the community to spur innovation on transfer techniques and help share progress from VLN to VLN-CE.</p>
<p>Related Work</p>
<p>VLN Agents. The VLN task [3] has received significant attention in recent years with the continual improvement of benchmark models. A higher-level panoramic action space was introduced by Fried et al. [11] and widely adopted in follow-up works. Early efforts proposed cross-modal grounding of vision and language [26]. Pre-training and data augmentation methods can mitigate issues regarding limited data [11,25] and challenges of jointly processing vision, language, and action [14]. Recently, transformer-based architectures and multi-step training routines have demonstrated superior performance [21,16,8]. VLN BERT [16] is one such model that introduced a recurrent history context to a transformerbased agent. We adopt VLN BERT as the core VLN agent in this work. With continual interest in improving topological instruction followers, we establish expectations and infrastructure for their transfer to continuous environments. VLN-CE Agents. Recognizing the unrealistic affordances of navigation graphs, Krantz et al. [20] proposed replacing the topological definition of VLN with lower-level actions in continuous environments. Initial end-to-end baselines that directly predict lower-level actions were modeled after sequence-to-sequence [3] and cross-modal [26] VLN methods and demonstrated significantly lower performance than in VLN. Raychaudhuri et al. [24] proposed language-aligned path supervision to better guide off-the-path decision-making. Krantz et al. [19] explored a higher-level waypoint action space trained with deep reinforcement learning, finding increased performance but still far below VLN. Other works exploited continuous environments to study topological map generation [7] and semantic mapping for navigation [17]. Rather than train agents from scratch in VLN-CE, we analyze the transfer of pre-trained VLN agents to continuous environments. Sim-2-Real Transfer of Instruction Following. Transferring skills learned in simulation to reality is a critical step for embodied agents [18,12,10,4]. The most similar to our work is that of Anderson et al. [2], which directly transferred VLN agents to reality. We analyze the intermediate step: sim-2-sim transfer from highlevel simulation to more realistic, lower-level simulation. The study of Anderson et al. [2] emulated the VLN action space with subgoal candidate prediction and performed navigation with the Robot Operating System (ROS) [23]. They found a more than 2x drop in performance when evaluating agents in reality, showing that effective VLN sim-2-real transfer remains an open research question. We identify similar challenges in sim-2-sim transfer as Anderson et al. [2] did in sim-2-real. We diagnose and begin mitigating these issues in VLN-CE using methods infeasible in VLN and impractical in reality.</p>
<p>Task Descriptions</p>
<p>In this section, we describe both the vision-and-language navigation task (VLN) and its continuous-environment counterpart, VLN-CE. Vision-and-Language Navigation (VLN). VLN was proposed by Anderson, et al. [3], in which agents navigate previously-unseen indoor environments. The task is to follow a path described in natural language and stop at the goal.</p>
<p>Instructions are unstructured and crowd-sourced in English to form the Roomto-Room (R2R) dataset. VLN takes place in a topological simulator, where a pre-defined nav-graph dictates allowable navigation through edge connections.</p>
<p>At each time step t, a VLN agent receives the instruction I, a panoramic RGB observation O t , and a set of n subgoal candidates C (1...n) t . The instruction I is a sequence of L words (w 0 , w 1 , . . . , w L ). The panoramic observation O t is represented as 36 views such that 12 equally-spaced horizontal headings (0°, 30°, . . . , 360°) are represented at each of 3 elevation angles (-30°, 0°, +30°). Each image frame in O t has a resolution of 480x640 with a horizontal field of view of 75°. Each subgoal candidate C i t is represented by a relative heading angle θ i t , an elevation angle φ i t , and the observation view frame O j t that most closely centers the candidate:
C i t = (θ i t , φ i t , O j t ).
The VLN agent then navigates by selecting a candidate. The environment then transitions and produces observations for time t + 1. This repeats until the agent issues the STOP action. VLN in Continuous Environments (VLN-CE). VLN-CE as proposed by Krantz et al. [20] converts the topologically-defined VLN task into a continuousenvironment task more representative of real-world navigation. Instead of selecting and navigating by environment-provided subgoal candidates, agents in VLN-CE must navigate a continuous-valued 3D mesh by selecting actions from a lower-level action space (FORWARD 0.25m, TURN-LEFT 15°, TURN-RIGHT 15°, STOP). In this work, we seek to port VLN agents into continuous environments. Thus, we adopt what we can of the VLN observation space: the instruction I and the panoramic observation O t . We introduce a 2D laser scan which we describe in Sec. 4.3. This provides our subgoal generation module with 2D occupancy. Such a sensor is commonly used for both localization and mapping in real-world navigation stacks [23]. In our analysis below, we perform intermediate experiments that may allow oracle navigation or topological subgoal candidates but note that these experiments do not result in admissible VLN-CE agents. Room-to-Room Dataset. The Room-to-Room dataset (R2R) consists of 7, 189 shortest-path trajectories across all train, validation, and test splits [3]. The VLN-CE dataset is a subset of R2R consisting of 5, 611 trajectories traversible in continuous environments (78% of R2R) [20]. For both tasks, we report performance on several validation splits. Val-Seen contains episodes with novel paths and instructions but from scenes observed in training. Val-Unseen contains novel paths, instructions, and scenes. Train * is a random subset of the training split derived by Hong et al. [16] to support further analysis. Matterport3D Scene Dataset. Both VLN and VLN-CE use the Matter-port3D (MP3D) Dataset [5] which consists of 90 scenes, 10,800 panoramic RGBD images, and 90 3D mesh reconstructions. VLN agents interact with MP3D through a connectivity graph that queries panoramic images. VLN-CE agents interact with the 3D mesh reconstructions through the Habitat Simulator [22]. Evaluation Metrics. We report evaluation metrics standard to the public leaderboards of both VLN and VLN-CE. These include trajectory length (TL), navigation error (NE), oracle success rate (OS), success rate (SR), and success weighted by inverse path length (SPL) as described in [1,3]. We consider SR the primary metric for comparison and SPL for evaluating path efficiency. Both SR and SPL determine success using a 3.0m distance threshold between the agent's terminal location and the goal. We report the same metrics across both VLN and VLN-CE to enable empirical diagnosis of performance differences. We note that distance is computed differently between VLN and VLN-CE resulting in subtle differences in evaluation for the same effective path. In VLN, the distance between nodes is Euclidean 1 , whereas in VLN-CE, distances are geodesic as computed on the 3D navigation mesh. This tends to result in longer path lengths in VLN-CE and a drop in SPL.</p>
<p>Porting a VLN agent to Continuous Environments</p>
<p>To perform a sim-2-sim transfer, we emulate the VLN action space in VLN-CE by developing a harness consisting of a lower-level navigation policy and a subgoal generation module. An overview can be seen in Fig. 1. Our harness follows the structure used in sim-2-real experiments by Anderson et al. [2]. We develop our harness such that subgoal generation modules, VLN agents, and navigation policies are all modular components for drop-in replacement and evaluation. This not only enables the analysis in this work, but the released codebase will ease sim-2-sim transfer of future VLN agents to continuous environments.</p>
<p>Core VLN Agent</p>
<p>We adopt VLN BERT [16], a VLN agent that performs near the state-of-theart 2 on R2R Test. This agent uses observation and action spaces typical of recent work in VLN. VLN BERT has a recurrence-modified transformer architecture and encodes RGB vision with a ResNet-152 trained on Places365 [15,27]. We use VLN BERT as our core VLN agent for all experiments but note that our harness and related design choices are agnostic to the choice of VLN agent.</p>
<p>Navigation Policies</p>
<p>With the abstracted action space in VLN, navigation between subgoal locations is effectively performed by teleportation. In contrast, continuous settings require a navigation policy to convey the agent between subgoals. This navigation subtask can be considered a short-range version of PointGoal navigation [1] given that the average distance between nodes in the VLN nav-graph is 2.25m. To evaluate the impact of navigation between subgoals in VLN-CE, we first establish upper bounds with two oracle methods: teleportation and an oracle policy. We then evaluate with a local policy admissible in VLN-CE. We describe these navigation policies as follows. Teleportation. A teleportation action involves translation to the target location and rotation to face away from the previous location. In our teleportation policy, the heading is snapped to the nearest global 30°increment to match the heading discretization in VLN. This policy matches the navigation assumption made in VLN and serves as both an upper bound on navigation performance and a confirmation that the VLN agent is operating as expected in VLN-CE. Oracle Policy. An oracle navigation policy has access to the navigation mesh used by the simulator to take optimal actions from the VLN-CE action space. The input to our oracle policy is 3D coordinates that exist on the navigation mesh. Analytical search using the A* algorithm then determines an action plan. We set a stopping distance threshold of 0.15m to the goal which we empirically determine results in minimal navigation error and minimal action jitter near the goal. The Oracle Policy serves as an upper bound on navigation performance to policies operating under the same action space. Local Policy. We employ a mapping and planning navigation policy based on [6] to convey the agent to selected subgoal locations as shown in Fig. 1 "Navigation". The Local Policy receives a target location specified by a distance and relative heading (r, θ). At each time step, a local occupancy map is aggregated from the geometric projection of an egocentric depth observation. The unexplored map area is assumed to be free space. We then plan a path to the target location using the Fast Marching Method (FMM) which produces position coordinates along the shortest path lying 0.25m away from the agent. Following [13], we greedily decode a discrete action to approach this point. We adopt the VLN-CE action space and stop once the agent is within 0.15m of the target or 40 actions have been taken. The occupancy map is re-initialized for each new target.</p>
<p>Subgoal Candidate Generation</p>
<p>The VLN action space requires a set of subgoal candidates to select from. Where VLN uses neighboring nodes in the nav-graph, such oracle information does not exist in VLN-CE. As shown in Fig. 1 "Subgoal Generation", we emulate the presence of a nav-graph by having a learned module predict subgoal candidates from observations. Anderson et al. [2] developed a subgoal generation module (SGM) for VLN sim-2-real transfer. We adopt SGM and modify it for VLN-CE.</p>
<p>The observation space of the SGM is panoramic RGB vision (same as the VLN agent), supplemented with a 2D laser scanner for range-finding. We emulate a 360°laser scanner in the Habitat Simulator mounted at 0.24m above ground level. We note that such scanners are commercially available. As shown in Fig. 1 "Laser Scan", the laser scan is represented as a radial obstacle map limited to a 4.8m range. The obstacle map size is 24x48 with 24 discrete range bins of size 0.2m and 48 heading bins of size 7.5°. The RGB panorama frames are encoded with the same ResNet-152 used by the VLN agent.</p>
<p>A U-Net architecture fuses these RGB features with the obstacle map to predict a radial map of subgoal candidates. This map follows the same range and heading discretization as the input. Gaussian non-maximum suppression filters localized predictions. The k = 5 candidates with the highest probability mass are converted into (r, θ) candidates consumed by the VLN agent.</p>
<p>Following Anderson et al. [2], we train the SGM to minimize Sinkhorn divergence [9] to ground-truth subgoal candidates on the VLN nav-graph. Nodes from Val-Unseen are held out for validation. To better match the visual domain of 3Dreconstructed environments, we train the SGM with panoramas rendered using the Habitat Simulator. We provide an ablation analysis motivating our changes to 360°laser scanning and reconstructed RGB vision in the supplementary.</p>
<p>Results and Analysis</p>
<p>We evaluate the transferability of VLN agents to continuous environments using the harness described above. We identify potential sources of performance degradation and evaluate their isolated impacts. Specifically, our analysis starts in VLN and moves toward VLN-CE, covering differences in dataset episodes (5.1), replacing MP3D panoramas with reconstructed vision (5.2), using a navigation policy to reach selected subgoals (5.3), and removing nav-graph subgoals (5.4). After demonstrating favorable performance over previous agents trained purely in VLN-CE (5.5), we diagnose a remaining failure mode to be addressed in both future VLN-2-VLNCE transfer and sim-2-real transfer efforts (5.6).</p>
<p>How different is the VLN-CE subset of R2R?</p>
<p>We find the VLN-CE subset is slightly more difficult but comparable to the full R2R dataset. This suggests that performance in VLN is accurate to, but slightly over-estimates, the upper bound on performance transferred to VLN-CE. We make this comparison in Tab. 1 by evaluating VLN BERT on the topological VLN task for the VLN and VLN-CE subsets of R2R. Differences in these datasets arise from conversion errors which cause the VLN-CE subset of R2R to contain 22% fewer trajectories than in VLN. VLN BERT performs worse on the VLN-CE subset by 3 SR in Val-Seen and 1 SR in Val-Unseen. Table 1. Difficulty of Room-to-Room (R2R) episodes in the VLN-CE subset vs. all episodes of R2R. We find performance on the VLN-CE subset is slightly lower than full R2R; Val-Seen success drops by 3 points and Val-Unseen success drops by 1 point.<br />
Task: VLN Train * Val-Seen Val-Unseen # Dataset TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ 1 Room-to-</p>
<p>What is the visual domain gap from VLN to VLN-CE?</p>
<p>We find a significant visual domain gap from VLN to VLN-CE in previouslyseen environments of 10-13 SR and a moderate gap in novel environments of 3 SR (Tab. 2). This suggests that VLN agents may be overfitting to visual representation in training. This visual domain gap exists despite VLN and VLN-CE being derived from the same real-world scenes and sharing paths. We demonstrate characteristic examples in Fig. 2. VLN observations rendered from Mat-terport3D panoramas (MP3D panos) are high quality and captured directly from Matterport Pro2 3D cameras [5]. On the other hand, VLN-CE observations are rendered from 3D scene reconstructions in Habitat-Sim (Habitat renders) which introduce reconstruction errors and a domain gap in lighting. We demonstrate that training the agent with Habitat renders eliminates this gap almost entirely. Experiment Setup. We evaluate VLN BERT in VLN with MP3D panos and Habitat renders. To compute vision features from Habitat, we map each node in VLN to a matching location in VLN-CE using known camera poses. We capture a panorama in the scene reconstruction matching the camera height, angle, and resolution expected in VLN. We then encode the panorama with the same feature </p>
<p>What is the navigation gap between VLN and VLN-CE?</p>
<p>Conveying the VLN agent to selected subgoals using a realistic navigator causes a decrease in performance across all splits, highlighted by a 5 point drop in Val-Unseen success. The VLN task, which effectively teleports between subgoals, makes navigation assumptions of perfect obstacle avoidance and stopping precision. These challenges must be contended with in VLN-CE. In the worst case, navigation failure causes episode failure regardless of the decisions made by the VLN agent. In the best case, an unexpected pose and observation are produced at the next time step, a situation VLN agents do not encounter in training. Experiment Setup. We evaluate our reconstruction-trained VLN BERT in VLN-CE with each navigation policy from Sec. 4.2 and provide known subgoal candidates from the VLN nav-graph. We note that navigation errors introduce ambiguity in graph localization which must be resolved to provide the next subgoal candidates. In our experiments, we keep the agent at its navigationterminated location but provide subgoal candidates by assuming it is located Table 3. Impact of navigation policies on VLN agents operating in VLN-CE. We assume a known nav-graph but require lower-level actions between nodes. The Oracle Policy results in better performance than the Local Policy, with a 5 SR Val-Unseen gap.</p>
<p>Task: VLN-CE Train * Val-Seen Val-Unseen  [20]). We additionally record the navigation error of the Oracle Policy for each short-range navigation performed, finding an average navigation error of 0.11m in Val-Seen and 0.08m in Val-Unseen. This demonstrates that the VLN agent is robust to position jitter about official graph nodes. Optimal Navigation vs. Realistic Navigation. In row 3, we present performance under our Local Policy. Compared to the Oracle Policy, we find a drop in success in both Val-Seen (3 points) and Val-Unseen (5 points). The SPL drop is identical, suggesting that path efficiency remains high in successful episodes. The average short-range navigation error is 0.26m in Val-Seen and 0.40m in Val-Unseen, which are higher than that of the Oracle Policy. We observe that most calls to the Local Policy perform similarly to the Oracle Policy. However, the Local Policy has a longer tail of significant navigation failures (error &gt;0.5m). Specifically, the Local Policy has a &gt;0.5m failure rate that is 15x higher than the Oracle Policy. We expand on this error comparison in the supplementary.</p>
<h1>Navigator TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ 1</h1>
<p>What is the subgoal candidate gap?</p>
<p>In the previous experiments, VLN agents were selecting from subgoal candidates provided by the nav-graph, but these are not available in VLN-CE. Here, we predict subgoal candidates online (at each time step) and observe significant performance degradation (14-23 SR). Fine-tuning the VLN agent on the online distribution of subgoal candidates slightly mitigates this drop, recovering 1-2 SR.  Optimal SGM Predictions. We first consider the case where SGM predictions optimally match the nav-graph (Tab. 4, row 2). We convert the nav-graph subgoals into a radial prediction map that matches the SGM output space. This discretizes the subgoal candidate locations into polar bins with a 0.20m range and 7.5°resolution. This prediction space accounts for a non-trivial drop in performance, resulting in a 3 point drop in Val-Seen success rate and a 6 point drop in Val-Unseen success rate (row 2 vs. 1).</p>
<h1>Subgoal Candidates F-tune TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ 1</h1>
<p>Performance with SGM. Row 3 demonstrates subgoal predictions from the SGM with no nav-graph reliance. This experiment is compliant with the VLN-CE task definition; subgoal candidates are predicted from egocentric observations, a VLN agent selects a candidate, and a local policy conveys the agent using VLN-CE actions. We find a decrease of 12 SR in Val-Seen and 8 SR in Val-Unseen.</p>
<p>These drops indicate a large domain gap between the nav-graph and the SGM. This result parallels findings in sim-2-real transfer by Anderson et al. [2] that point to an alignment problem between the VLN nav-graph and the SGM.</p>
<p>Fine-Tuning VLN BERT with SGM. We seek to determine if fine-tuning a VLN agent on the SGM candidates can reclaim nav-graph performance. To motivate, consider how learned priors in VLN could result in detrimental behavior under a different distribution of subgoals. For example, a slight change in the distribution of subgoal distances could break a learned time horizon prior -the VLN agent may choose to stop either too early or too late. Such a prior can be very strong in VLN since all paths require 5-7 actions. We fine-tune our reconstruction-trained VLN BERT via imitation learning in VLN-CE. Specifically, we train with teacher forcing to maximize the probability of predicting the subgoal candidate that greedily minimizes the geodesic distance to the goal. We set a batch size of 12, a learning rate of 1e-7, and train with cross-entropy loss and early stopping. In Tab. 4 row 4, we present the result of fine-tuning on SGM predictions. Performance increases slightly across all validation splits, Table 5. Results on the VLN-CE Challenge Leaderboard. Our submission outperforms previously-published results, demonstrating a 12 point improvement over the next best model in success rate on Test (a 38% relative improvement).</p>
<p>Task: VLN-CE Val-Seen Val-Unseen Test highlighted by a 2-point increase in Val-Unseen success rate. This still leaves a 6 point gap in success rate to predicting optimal subgoal candidates and a 12 point gap in success rate to candidates directly from the nav-graph. Fine-tuning experiments such as this one cannot be performed on the rigid topology of VLN. Neither can they be performed practically in reality, making VLN-CE a promising setting for generalizing VLN agents for sim-2-sim or sim-2-real transfer.</p>
<h1>Model TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ 1 VLN-CE BERT (</h1>
<p>Comparison With Previous VLN-CE Models</p>
<p>We compare our best agent against previously published methods on the VLN-CE Challenge Leaderboard 3 . This agent includes subgoal candidates generated by the SGM, our reconstruction-trained VLN BERT fine-tuned on SGM candidates, and Local Policy navigation. We label our submission VLN-CE BERT. As shown in Tab. 5, VLN-CE BERT surpasses the success rate of all previous models on all splits including Test. Notably, all previous methods are trained exclusively in VLN-CE, with the best model amongst them requiring extensive compute (7000+ GPU-hours) to achieve a 32 SR. With just 12 GPU-hours of fine-tuning, our model surpasses this performance level by 12 points SR to achieve a 44 SR in Test (a relative improvement of 38%). This result shows that VLN to VLN-CE transfer is a highly promising avenue for making progress on the VLN-CE task and closing the gap to VLN.</p>
<p>Failures of Subgoal Candidate Generation</p>
<p>Starting from VLN BERT operating in VLN, we diagnosed a 1 SR drop from dataset differences, a 1 SR drop from the visual domain gap, a 5 SR drop from navigation policies, and a 12 SR drop from subgoal candidate generation. We now look into what failure modes constitute the subgoal candidate drop. We find that half of these failures (6/12 SR) are caused by episodes that require the agent to gain or lose at least 1.0m of elevation (i.e. traversing stairs). In such episodes, an oracle VLN agent (a greedy-optimal subgoal selector) paired with the Oracle Policy for navigation is successful less than 50% of the time. This indicates that the SGM has a recall problem in generating elevation-changing subgoals necessary for success. Details and experimental support are below.</p>
<p>Oracle Selection and Navigation. We first consider whether candidates produced by the SGM can lead to episode success independent of failures stemming from selection or navigation issues. We define a greedy oracle VLN agent that selects the subgoal candidate with minimal geodesic distance to the goal. STOP is called when no candidate is closer to the goal than the current position. We run this oracle over SGM candidates and navigate via the Oracle Policy. We find success rates of 89% in Train * , 93% in Val-Seen, and 90% in Val-Unseen ( Fig. 3(a)). However, failures disproportionately stem from episodes requiring elevation change. This is exemplified in Train * where episodes requiring at least a 1.0m elevation change result in 38% success -a highly restrictive upper bound.</p>
<p>Elevation-Based Performance of VLN-CE BERT. We compare success rates for VLN-CE BERT in episodes requiring a high (&gt;1.0m) vs. low (&lt;1.0m) elevation delta ( Fig. 3(b)). In Val-Unseen, there is a 29 SR gap between such episodes (19 SR vs. 48 SR). This is unsurprising given the poor performance of the oracle VLN agent with high elevation change. The failure of the SGM to predict elevation subgoals (a recall problem) may be related to how it is trained; subgoal candidate locations are highly correlated with free-space in the 2D laser scan. However 2D free-space is a poor predictor of subgoals with elevation change, which are less represented in the training data ( Fig. 3(d)).</p>
<p>Elevation-Based Performance in VLN. Elevation-based VLN evaluation provides an upper bound on VLN-CE performance in elevation episodes. Surprisingly, we find in Fig. 3 </p>
<p>Qualitative Example.</p>
<p>We present a qualitative example of VLN-CE BERT successfully navigating in a novel environment within VLN-CE (Fig. 4). This example demonstrates all components of our agent: subgoal candidate generation, subgoal selection, and navigation. In each step, the subgoal candidates (large, yellow triangles) enable navigation in primary directions. However, not all candidates can be reached, like in Step 1 where a candidate is centered on the kitchen countertop. The SGM candidates diverge from nav-graph node locations. This leads the VLN agent to observe the environment from poses off the nav-graph, yet proper candidate selections are still made over the course of 5 actions. Notice that between steps 0 and 1, the selected subgoal cannot be navigated to in a straight line. Our Local Policy demonstrates obstacle avoidance while planning and executing a path that traverses around the countertop. This example demonstrates that through modular construction, VLN agents can operate successfully and efficiently in continuous environments. We provide additional examples in the supplementary.</p>
<p>Conclusion</p>
<p>In summary, we explore the sim-to-sim transfer of instruction-following agents from topological VLN to unconstrained VLN-CE. Our transfer results in an absolute improvement of 12% over the prior state-of-the-art in VLN-CE. We diagnose the remaining VLN-to-VLNCE gap, identifying subgoal candidate generation as a primary hindrance to transfer. We outline the problem of generating candidates in multi-floor environments to guide future work. Operating VLN agents in continuous environments enables a new interplay between language and navigation topologies. This can lead not only to higher performance in realistic environments, but also to the development of more robust topological navigators. </p>
<p>Oracle Policy Detail</p>
<p>We use the Oracle Policy with known subgoal candidates that are specified in 3D coordinates. We note that using this oracle with 2D subgoal predictions requires projecting the target location from 2D to 3D. We define a projection procedure P : (r, θ) → (x, y, z) that maps distance and relative heading to global 3D coordinates. In this procedure, the agent's current pose is used to project (r, θ) to global 2D coordinates (x,ẑ). We assume the target exists at the elevation of the agent's pose,ŷ. Finally, we snap the resulting 3D coordinates (x,ŷ,ẑ) to the nearest position (x, y, z) that exists on the navigation mesh and that has a finite geodesic distance from the agent's current position.</p>
<p>Subgoal Module Ablations</p>
<p>In Tab. 6 we evaluate ablations of the subgoal generation module (SGM) in VLN-CE. In row 1 vs. 2, we find that training the SGM with Habitat-rendered vision (Recon.) leads to better downstream performance in VLN-CE than MP3D panoramas across all splits (3 SR Val-Unseen, 5 SR Val-Seen, 6 SR Train * ). We further ablate the 360°laser scan to 270°and observe an additional performance drop of 5 SR in Val-Unseen, 4 SR in Val-Seen, and 8 SR in Train * . Altogether, our modifications of reconstructed vision and 360°scanning improve performance under the SGM by 8 SR in Val-Unseen over the SGM proposed in [2].</p>
<p>Distribution of Navigation Errors</p>
<p>In Tab. 7 we present the navigation errors for both the Oracle Policy and Local Policy when used to perform the VLN-CE task. The presented short-range navigation errors were collected during the experiments reported in Tab. 3. We characterize the distribution of navigation errors by the percent of navigations that result in various thresholds of error. The Oracle Policy rarely produces a high navigation error -just 0.53% of navigations result in at least a 0.20m error  in Val-Unseen. In the same setting, the Local Policy has a 0.20m failure rate of 8.04%. This extends to even larger failure thresholds where the Local Policy fails to navigate to within 1.0m of the subgoal 5.73% of the time in Val-Unseen. These failure rates provide context to Tab. 3 which demonstrated a performance drop when navigating with the Local Policy.</p>
<p>Fig. 1 .
1We diagnose and quantify the VLN to VLN-CE gap by operating a topological VLN agent in continuous environments. We emulate the VLN observation space by generating subgoal candidates from egocentric observation (Sec. 4.3) and emulate the action space by calling a map-based navigation policy (Sec. 4.2).</p>
<p>Fig. 2 .
2Visual observation differences between VLN and VLN-CE. VLN renders observations from high-quality Matterport3D panoramas (top) and VLN-CE renders observations from 3D scene reconstructions (bottom). Examples of domain differences include lighting (A, B), furniture resolution (C), and object presence in the scene (D).</p>
<p>( a )Fig. 3 .
a3Greedy Oracle (b) VLN-CE BERT (c) VLN: rt-VLN BERT Success rates in episodes with high (&gt;1.0m) vs. low (&lt;1.0m) elevation delta. Black bars indicate success rate irrespective of elevation. (a,b) use SGM subgoal candidates in VLN-CE and perform worse with elevation. However in VLN (c), our reconstruction-trained agent performs better with elevation. (d) shows the percentage of episodes that have a &gt;1.0m delta.</p>
<p>Fig. 4 .
4Qualitative example of our VLN-CE BERT agent operating in VLN-CE.</p>
<p>Table 2 .
2Effect NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ Switching to Habitat renders results in success rate drops of 13 points in Train * , 10 points in Val-Seen, and 3 points in Val-Unseen. The significant performance degradation in seen environments (Train * and Val-Seen) demonstrates the impact of the visual domain gap between VLN and VLN-CE. The relatively smaller drop in Val-Unseen suggests that this gap is not as catastrophic when generalizing to novel environments and that VLN agents may be overfitting to visual representation in training environments. Domain Transfer. We show in row 4 that re-training VLN BERT directly on reconstructed vision can entirely recover task performance in seen environ-of the visual domain gap between Matterport3D panoramas (MP3D) 
and reconstruction renders (Recon.) on VLN performance. Reconstructed vision de-
creases success rate in Val-Unseen by 3 points (rows 1 vs. 2). This drop is mitigated 
by training VLN agents directly on reconstructed images (row 4). </p>
<p>Task: VLN 
Camera 
Train  *<br />
Val-Seen 
Val-Unseen </p>
<h1></h1>
<p>Train 
Eval 
TL ↓ 1 </p>
<p>MP3D 
MP3D 
9.98 0.93 95 93 90 
10.81 3.00 76 72 68 
11.85 4.24 67 61 56 </p>
<p>2 </p>
<p>Recon. 11.17 2.20 84 80 76 
11.66 4.02 68 62 58 
11.69 4.56 64 58 53 </p>
<p>3 </p>
<p>Recon. 
MP3D 10.63 1.18 92 90 86 
11.35 3.41 72 66 61 
11.72 4.12 66 60 54 </p>
<p>4 </p>
<p>Recon. 9.76 0.48 98 98 96 
10.56 2.88 77 72 69 
11.16 4.14 66 60 56 </p>
<p>encoder used by VLN BERT, a ResNet-152 trained on Places365 [15,27]. These 
features are then used as a drop-in replacement for MP3D pano features. 
Zero-Shot Generalization. We present zero-shot generalization performance 
in Tab. 2, row 1 vs. 2. ments and recover all by 1 point in success in Val-Unseen (row 4 vs. 1). This 
reconstruction-trained model achieves a success rate of 60 in Val-Unseen regard-
less of using MP3D panos or Habitat renders (rows 3 and 4). This suggests that 
training VLN agents on Habitat renders may result in less visual overfitting. </p>
<p>Teleportation vs. VLN-CE Actions. We use the Oracle Policy to represent ideal actions within the VLN-CE action space. We find that performance under the Oracle Policy matches performance under Teleportation in Val-Unseen (row 2 vs. 1). This suggests that the navigation gap between VLN and VLN-CE can be minimized with performant navigation policies. This is an expected result due to the navigation verification that originally culled episodes for the VLN-CE dataset (seeTeleportation 10.04 0.58 97 97 88 
11.28 3.24 75 70 63 
11.98 4.06 66 60 52 
2 Oracle Policy 10.09 0.72 97 96 87 
11.19 3.10 75 69 61 
12.04 4.07 68 60 52 
3 Local Policy 10.58 1.57 90 88 78 
11.37 3.49 72 66 58 
12.28 4.51 63 55 47 </p>
<p>at the nearest node on the nav-graph by geodesic distance. This reflects the 
consequences of poor navigation where a large navigation error may cause the 
VLN agent to observe candidates from a different graph node than intended. We 
first evaluate the impact of using the VLN-CE action space instead of teleporta-
tion. We then evaluate the effect of a realistic navigation policy. Results of both 
comparisons are in Tab. 3. </p>
<p>Table 4 .
4Comparing VLN-CE performance when subgoal candidates are provided bythe nav-graph (rows 1,2) vs. predicted by a subgoal generation module (SGM)(rows 
3,4). Rows 3 and 4 demonstrate a complete harness for a VLN agent admissible in 
VLN-CE, consisting of both subgoal candidate generation and lower-level navigation. </p>
<p>Task: VLN-CE 
Train  *<br />
Val-Seen 
Val-Unseen </p>
<p>(c) that our reconstruction-trained VLN BERT has a higher success rate in episodes requiring a high elevation delta (68 SR vs. 57 SR in Val-Unseen). We suspect this result stems from episodes that feature stairs have a smaller search space; nav-graph nodes on stairs have a smaller degree. From this result, we estimate that solving the elevation problem in VLN-CE can mitigate 6 points of the 12 SR drop attributable to subgoal candidate generation, leaving a cumulative 12 SR gap between VLN and VLN-CE.</p>
<p>We use the VLN BERT model in VLN experiments to evaluate the impact of dataset differences between VLN and VLN-CE (Sec. 5.1) and MP3D vs. reconstructed vision (Sec. 5.2). For consistency across our experiments, we retrain VLN BERT using the official codebase 4 . We train with the (init. PREVA-LENT) backbone. Our re-trained version performs at 2 SR and 1 SPL lower in Val-Unseen than the published result in[16] but matches performance in Val-Seen SR and SPL. We repeat training and evaluation with 3 different random seeds and find performance consistent with what we present in Tab. 1 and Tab. 2.7 Supplementary </p>
<p>7.1 VLN BERT: Validation Performance </p>
<p>Table 6 .
6Ablations against the subgoal generation module (SGM). Results are in VLN-CE with reconstruction-trained VLN BERT and Local Policy navigator. Row 1 matches Tab. 4 row 3 in the main paper. Row 2 ablates training with reconstructed vision (Recon.) and row 3 ablates both Recon. and 360°laser scan to match the SGM used by Anderson et al.[2].# Subgoal Candidates Vision HFOV TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑ TL ↓ NE ↓ OS ↑ SR ↑ SPL ↑Task: VLN-CE 
Train  *<br />
Val-Seen 
Val-Unseen </p>
<p>1 </p>
<p>SGM </p>
<p>Recon. 360 
13.12 3.54 71 65 55 
12.69 4.51 60 51 44 
13.74 5.83 51 41 35 </p>
<p>2 </p>
<p>MP3D 360 
10.08 3.52 64 59 54 
10.41 4.80 52 46 41 
10.07 5.61 45 38 34 </p>
<p>3 </p>
<p>MP3D 270 
11.24 4.14 59 51 46 
10.73 5.02 47 42 37 
10.62 5.97 41 33 29 </p>
<p>Table 7 .
7Short-range navigation errors (subgoal-to-subgoal) of navigation policies. Evaluated while VLN BERT is performing the VLN-CE task with nav-graph subgoals (Tab. 3). Navigation errors are in meters and thresholded error values (&gt;Xm) are reported as a percent of all short-range navigations. &gt;0.2m &gt;0.5m &gt;1.0m NE ↓ &gt;0.2m &gt;0.5m &gt;1.0m NE ↓ &gt;0.2m &gt;0.5m &gt;1.0mTask: VLN-CE 
Train  *<br />
Val-Seen 
Val-Unseen </p>
<h1>Navigator</h1>
<p>NE ↓ 1 Oracle Policy 0.09 0.24 
0.24 
0.24 
0.11 0.74 
0.69 
0.69 
0.08 0.53 
0.42 
0.29 
2 Local Policy 0.40 9.44 
7.60 
6.31 
0.26 7.09 
5.63 
4.84 
0.40 8.04 
6.36 
5.73 </p>
<p>Sim-2-Sim Transfer for VLN-CE
As defined by the Matterport3D Simulator used in VLN. 2 eval.ai/web/challenges/challenge-page/97
eval.ai/web/challenges/challenge-page/719
github.com/YicongHong/Recurrent-VLN-BERT
Acknowledgements. This work was supported in part by the DARPA Machine Common Sense program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.Sim-2-Sim Transfer for VLN-CE
P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757On evaluation of embodied navigation agents. arXiv preprintAnderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et al.: On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757 (2018)</p>
<p>Sim-to-real transfer for vision-and-language navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, CoRLAnderson, P., Shrivastava, A., Truong, J., Majumdar, A., Parikh, D., Batra, D., Lee, S.: Sim-to-real transfer for vision-and-language navigation. In: CoRL (2020)</p>
<p>Vision-and-language navigation: interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, A Van Den Hengel, CVPRAnderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., van den Hengel, A.: Vision-and-language navigation: interpreting visually-grounded navigation instructions in real environments. In: CVPR (2018)</p>
<p>Learning to map natural language instructions to physical quadcopter control using simulated flight. V Blukis, Y Terme, E Niklasson, R A Knepper, Y Artzi, CoRLBlukis, V., Terme, Y., Niklasson, E., Knepper, R.A., Artzi, Y.: Learning to map natural language instructions to physical quadcopter control using simulated flight. In: CoRL (2020)</p>
<p>Matterport3d: learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, MatterPort3D dataset license available at. 3Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: learning from rgb-d data in in- door environments. In: 3DV (2017), MatterPort3D dataset license available at: http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, ICLRChaplot, D.S., Gandhi, D., Gupta, S., Gupta, A., Salakhutdinov, R.: Learning to explore using active neural slam. In: ICLR (2020)</p>
<p>Topological planning with transformers for vision-and-language navigation. K Chen, J K Chen, J Chuang, M Vázquez, S Savarese, CVPRChen, K., Chen, J.K., Chuang, J., Vázquez, M., Savarese, S.: Topological planning with transformers for vision-and-language navigation. In: CVPR (2021)</p>
<p>History aware multimodal transformer for vision-and-language navigation. S Chen, P L Guhur, C Schmid, I Laptev, NeurIPSChen, S., Guhur, P.L., Schmid, C., Laptev, I.: History aware multimodal trans- former for vision-and-language navigation. In: NeurIPS (2021)</p>
<p>Sinkhorn distances: Lightspeed computation of optimal transport. M Cuturi, NeurIPS. Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS (2013)</p>
<p>Robothor: an open simulation-to-real embodied ai platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, CVPRDeitke, M., Han, W., Herrasti, A., Kembhavi, A., Kolve, E., Mottaghi, R., Sal- vador, J., Schwenk, D., VanderBilt, E., Wallingford, M., et al.: Robothor: an open simulation-to-real embodied ai platform. In: CVPR (2020)</p>
<p>Speaker-follower models for vision-and-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, T Darrell, NeurIPSFried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.P., Berg- Kirkpatrick, T., Saenko, K., Klein, D., Darrell, T.: Speaker-follower models for vision-and-language navigation. In: NeurIPS (2018)</p>
<p>Splitnet: sim2sim and task2task transfer for embodied visual navigation. D Gordon, A Kadian, D Parikh, J Hoffman, D Batra, CVPRGordon, D., Kadian, A., Parikh, D., Hoffman, J., Batra, D.: Splitnet: sim2sim and task2task transfer for embodied visual navigation. In: CVPR (2019)</p>
<p>No rl, no simulation: learning to navigate without navigating. M Hahn, D S Chaplot, S Tulsiani, M Mukadam, J M Rehg, A Gupta, NeurIPSHahn, M., Chaplot, D.S., Tulsiani, S., Mukadam, M., Rehg, J.M., Gupta, A.: No rl, no simulation: learning to navigate without navigating. In: NeurIPS (2021)</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. W Hao, C Li, X Li, L Carin, J Gao, CVPRHao, W., Li, C., Li, X., Carin, L., Gao, J.: Towards learning a generic agent for vision-and-language navigation via pre-training. In: CVPR (2020)</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPRHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)</p>
<p>Vln bert: a recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, CVPRHong, Y., Wu, Q., Qi, Y., Rodriguez-Opazo, C., Gould, S.: Vln bert: a recurrent vision-and-language bert for navigation. In: CVPR (2021)</p>
<p>Sasra: semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments. M Z Irshad, N C Mithun, Z Seymour, H P Chiu, S Samarasekera, R Kumar, arXiv:2108.11945arXiv preprintIrshad, M.Z., Mithun, N.C., Seymour, Z., Chiu, H.P., Samarasekera, S., Kumar, R.: Sasra: semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments. arXiv preprint arXiv:2108.11945 (2021)</p>
<p>Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, IROSKadian, A., Truong, J., Gokaslan, A., Clegg, A., Wijmans, E., Lee, S., Savva, M., Chernova, S., Batra, D.: Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation. In: IROS (2020)</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. J Krantz, A Gokaslan, D Batra, S Lee, O Maksymets, ICCVKrantz, J., Gokaslan, A., Batra, D., Lee, S., Maksymets, O.: Waypoint models for instruction-guided navigation in continuous environments. In: ICCV (2021)</p>
<p>Beyond the nav-graph: vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, ECCVKrantz, J., Wijmans, E., Majumdar, A., Batra, D., Lee, S.: Beyond the nav-graph: vision-and-language navigation in continuous environments. In: ECCV (2020)</p>
<p>Improving vision-and-language navigation with image-text pairs from the web. A Majumdar, A Shrivastava, S Lee, P Anderson, D Parikh, D Batra, ECCVMajumdar, A., Shrivastava, A., Lee, S., Anderson, P., Parikh, D., Batra, D.: Im- proving vision-and-language navigation with image-text pairs from the web. In: ECCV (2020)</p>
<p>Habitat: a platform for embodied ai research. Manolis Savva, * , Abhishek Kadian, * , Oleksandr Maksymets, * Zhao, Y Wijmans, E Jain, B Straub, J Liu, J Koltun, V Malik, J Parikh, D Batra, D , ICCVManolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: a platform for embodied ai research. In: ICCV (2019)</p>
<p>Ros: an open-source robot operating system. M Quigley, K Conley, B P Gerkey, J Faust, T Foote, J Leibs, R Wheeler, A Y Ng, ICRA Workshop on Open Source Software. Quigley, M., Conley, K., Gerkey, B.P., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A.Y.: Ros: an open-source robot operating system. In: ICRA Workshop on Open Source Software (2009)</p>
<p>Language-aligned waypoint (law) supervision for vision-and-language navigation in continuous environments. S Raychaudhuri, S Wani, S Patel, U Jain, A X Chang, EMNLPRaychaudhuri, S., Wani, S., Patel, S., Jain, U., Chang, A.X.: Language-aligned waypoint (law) supervision for vision-and-language navigation in continuous envi- ronments. In: EMNLP (2021)</p>
<p>Learning to navigate unseen environments: back translation with environmental dropout. H Tan, L Yu, M Bansal, NAACL HLT. Tan, H., Yu, L., Bansal, M.: Learning to navigate unseen environments: back trans- lation with environmental dropout. In: NAACL HLT (2019)</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. X Wang, Q Huang, A Celikyilmaz, J Gao, D Shen, Y F Wang, W Y Wang, L Zhang, CVPRWang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y.F., Wang, W.Y., Zhang, L.: Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In: CVPR (2019)</p>
<p>Places: A 10 million image database for scene recognition. B Zhou, A Lapedriza, A Khosla, A Oliva, A Torralba, TPAMIZhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million image database for scene recognition. TPAMI (2017)</p>            </div>
        </div>

    </div>
</body>
</html>
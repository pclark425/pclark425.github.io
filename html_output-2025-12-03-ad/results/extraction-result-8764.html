<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8764 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8764</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8764</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-259341991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.01548v1.pdf" target="_blank">Knowledge Graph for NLG in the context of conversational agents</a></p>
                <p><strong>Paper Abstract:</strong> The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8764.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8764.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization+Seq2Seq (PLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization with sequence-to-sequence pretrained language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert a knowledge graph into a linear textual sequence (e.g., serialized triples or node lists) and fine-tune a pretrained seq2seq Transformer (BART/T5/GPT-style) to generate natural language from that sequence; chosen in this paper's project for production due to runtime and validity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is transformed into a flat sequence (typically a sequence of node labels and/or RDF/triple strings) that is provided as input tokens to a seq2seq Transformer; aims to reuse PLM textual tokenization and generation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF-style triples (heterogeneous graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize graph as a sequence of triples or node lists (linearization/serialization) and feed that sequence to a pretrained seq2seq model (BART/T5/GPT) for fine-tuning; paper describes general linearization but does not prescribe a single traversal algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge graph-to-text generation for conversational agents (text generation from KG triplets), multilingual/emotional NLG planned</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical metrics reported in this review for specific linearization implementations; paper states that fine-tuned PLMs empirically outperform more complex specialized architectures on KG-to-text benchmarks (qualitative claim only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper reports qualitatively that PLM-based linearization approaches often outperform specialized graph encoders in generation quality when fine-tuned, but suffer from loss of structural information compared to graph-native encoders; chosen for better inference time and engineering fit versus GNN/GT.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reuses powerful pretrained text generation models (PLMs); strong linguistic fluency; good generalization with relatively simple engineering and easier deployment; lower inference time and easier integration in DAVI's pipeline according to authors.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Tends to lose explicit graph topology/structural information during encoding; can be computationally expensive during pretraining/fine-tuning for large linearized inputs; potential difficulty learning explicit graph-to-text alignments from limited parallel data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Degraded semantic fidelity (hallucination, omission, ordering errors) due to structural information loss and weak explicit graph-text alignment; poorer performance on very large graphs or when structural constraints are critical.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8764.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Networks (GCN / GAT / GGNN variants) + decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use graph neural network encoders (GCNs, GATs, GGNNs and variants) to compute node/subgraph representations that are then decoded (often with an RNN or Transformer decoder) into text; captures graph structure explicitly via neighborhood aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-encoder (GNN) + sequence decoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode graph topology and node/edge features with message-passing or attention-based graph layers to produce contextual node embeddings; a sequence decoder conditions on these embeddings to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (heterogeneous graphs), other graph-structured inputs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linearization: run GNN layers (GCN, GAT, GGNN, or specialized variants) to aggregate neighbor information into node/subgraph vectors; feed encoded representation to a decoder (e.g., LSTM/Transformer) to perform text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation, question generation, conversational reasoning over KGs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The review does not present numeric benchmarks for specific GNN models; it cites multiple works that report promising results but notes shortcomings in scalability and inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GNNs better preserve graph topology than linearization; however, the paper states that fine-tuned PLMs (linearization + seq2seq) often achieve higher overall generation quality in practice, and GNNs incur greater computational cost and inference latency.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models graph structure and relations, potentially improving semantic fidelity with respect to the input graph; flexible to heterogeneous graphs and relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Computationally expensive, harder to scale to large graphs; often requires a separate decoder; inference time and memory requirements may exceed conversational-agent constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance degradation on very large or complex graphs; high inference latency unacceptable for real-time conversational systems; may struggle when deployed under strict MLOps memory/time budgets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8764.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer (GT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-adapted Transformer (Graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapt Transformer architectures to graph inputs by applying self-attention over nodes with relation-aware encodings (e.g., pairwise relation encodings or visibility masks) so that global node dependencies are captured while attempting to preserve graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Transformer / relation-aware attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes are treated as Transformer tokens; self-attention is augmented with explicit relation encodings or visibility matrices so pairwise attention scores reflect graph relations instead of assuming a fully connected structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, general graphs / trees</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Provide node representations to a Transformer encoder and modify attention computation with relation encodings (dynamic parameters) or visibility matrices so attention is structure-aware; output fed to a decoder to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (KG-to-text), graph-to-sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No concrete numeric metrics provided in this review; paper cites GTs as promising but computationally demanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GTs can model long-range dependencies more directly than local-aggregation GNNs, but they risk treating graphs as fully connected if not augmented; PLM linearization still competitive in practice per review.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures global context and long-range dependencies between nodes; can incorporate relation encodings to be structure-aware; potentially better than local GNN aggregation for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>High computational and memory cost; if naively applied, tends to treat graphs as dense fully connected structures and can dilute explicit graph topology unless relation encodings/visibility masks are used.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Excessive training/inference costs for real-time conversational agents; reduced interpretability of structure unless explicitly encoded; may violate runtime constraints in production.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8764.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-BERT (Enabling language representation with knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augments a Transformer by expanding input sentences with related KG triplets (knowledge layer), preserving sentence structure via a visibility matrix, and modifying attention (Mask-Transformer) to incorporate the visibility constraints so knowledge can be injected without overwhelming the sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-bert: Enabling language representation with knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sentence expansion + visibility-masked attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input sentence entities are expanded with their KG triplets to form an expanded sentence-tree; a visibility (seeing) matrix preserves original sentence structure by controlling which expanded nodes can attend to which others; attention is masked accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity triplets), sentence-centric subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Take a sentence and a set of triplets, expand sentence entities with their triplets to form a tree-structured input, compute a visibility matrix encoding permitted attention links, and run a modified self-attention (Mask-Transformer) respecting visibility.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Language representation for downstream NLG/NLU tasks; can be adapted to KG-to-text by leveraging the augmented input</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric results provided in this review; K-BERT is cited as a method that augments Transformer attention with KG structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>K-BERT explicitly injects KG facts into Transformer inputs and attempts to preserve sentence structure, contrasting with naive linearization which may lose structure; review does not provide direct empirical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly injects KG facts while preserving sentence integrity through visibility masks; integrates KG knowledge into pretrained Transformers without retraining from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Alters original input structure and requires designing visibility matrices; scalability and impact on generation fidelity not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential for over-expansion of inputs and attention interference if visibility is not properly constrained; not evaluated numerically in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8764.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoinGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JoinGT (Graph-text joint representation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining approach that explicitly learns graph–text alignments rather than only fine-tuning seq2seq PLMs on parallel KG-to-text data, aiming to improve graph-text alignment for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-text joint pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Pretrain models with objectives designed to align graph structures and textual representations jointly, so downstream KG-to-text generation benefits from learned correspondences between graph elements and tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Pretraining that jointly consumes graph and text inputs to learn alignment (specific pretraining tasks are cited in the referenced work but not detailed in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this review; JoinGT is described as explicitly learning graph-text alignments and contrasted with direct fine-tuning of PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as an alternative to naively fine-tuning PLMs on KG-to-text data; the review notes JoinGT focuses on pretraining to learn alignments rather than direct tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Aims to build explicit graph-text alignment into the model via pretraining, which could reduce hallucination and improve fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires additional pretraining data and compute; specifics and empirical tradeoffs are not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed numerically here; potential dependence on availability of graph-text pretraining data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8764.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GTR-LSTM: triple encoder for sentence generation from RDF</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A triple-encoder model that encodes RDF triple structures to generate sentences, using a tailored encoder to represent triples and an LSTM decoder for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GTR-LSTM: A triple encoder for sentence generation from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-encoder (GTR) + LSTM decoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode each RDF triple (subject-predicate-object) with a dedicated encoder to obtain structured representations that feed into a sequence decoder (LSTM) producing text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF graphs / knowledge graphs represented as triples</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode triples directly (triple-wise encoding) rather than linearizing whole graph; decoded sequentially by an LSTM to produce sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation from RDF/knowledge graphs (KG-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric metrics included in this review; referenced as a prior approach in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Triple-encoder is a graph-native approach contrasting with linearization + PLM and GNNs; review cites it among historical approaches but gives no comparative numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models RDF triple structure which can help preserve semantic fidelity of facts.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Older architecture (LSTM decoder) may lack fluency advantages of modern PLMs; scalability/latency not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified in this review; general concerns about older encoder-decoder architectures' fluency vs. PLMs apply.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8764.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycLeGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycLeGT (Unsupervised cycle graph↔text training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised cycle-training framework that jointly learns graph-to-text and text-to-graph conversions using non-parallel data, aiming to improve fidelity without needing large parallel corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>cycle training (unsupervised graph↔text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train graph→text and text→graph models jointly in a cycle-consistency setup so a generated text can be converted back to the original graph and compared to input, enabling training from non-parallel data.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / graph-text paired domains (can work with non-parallel data)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Alternate/paired training: graph-to-text model generates text, text-to-graph model reconstructs a graph; cycle loss enforces that reconstructed graph matches original graph (and vice versa).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Unsupervised KG-to-text and text-to-graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric metrics in this review; method is cited as an approach to address scarcity of parallel data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared conceptually to supervised fine-tuning approaches; offers a route when parallel graph-text data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables learning with non-parallel data via cycle-consistency; can improve alignment without heavy reliance on labeled datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Cycle training complexity and stability issues; empirical gains and costs not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential training instability and reliance on quality of both directions; specifics not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8764.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8764.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromoteGraphAwareness (linearized-awareness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promoting graph awareness in linearized graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques to make linearized graph inputs more 'graph-aware' when fed to text-oriented PLMs, e.g., augmenting linearization with structural signals to reduce topology loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Promoting graph awareness in linearized graph-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>augmented linearization (graph-awareness)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Keep linearized input but augment it with additional structural cues or special tokens/ordering strategies so sequence encoders better capture graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (linearized for PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize graph but add structural annotations (ordering, delimiters, special tokens or position encodings) to help the PLM infer graph relations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation with pre-trained text PLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported numerically in this review; cited as a line of research to mitigate structural loss from naive linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Sits between naive linearization and graph-native encoders: easier to deploy than GNNs yet attempts to recover topology information lost by serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Lower engineering/deployment cost than graph-native models, potential to recover some structural signals while keeping PLM benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still may not fully capture complex graph structure; effectiveness depends on design of augmentation; no numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May fail on graphs where structural relations are complex and cannot be encoded via simple linear cues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating pretrained language models for graph-to-text generation. <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation. <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers. <em>(Rating: 2)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from RDF data. <em>(Rating: 2)</em></li>
                <li>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. <em>(Rating: 2)</em></li>
                <li>Kgpt: Knowledge-grounded pretraining for data-to-text generation. <em>(Rating: 2)</em></li>
                <li>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. <em>(Rating: 2)</em></li>
                <li>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8764",
    "paper_id": "paper-259341991",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Linearization+Seq2Seq (PLM)",
            "name_full": "Graph linearization with sequence-to-sequence pretrained language models",
            "brief_description": "Convert a knowledge graph into a linear textual sequence (e.g., serialized triples or node lists) and fine-tune a pretrained seq2seq Transformer (BART/T5/GPT-style) to generate natural language from that sequence; chosen in this paper's project for production due to runtime and validity constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearization / serialization",
            "representation_description": "The graph is transformed into a flat sequence (typically a sequence of node labels and/or RDF/triple strings) that is provided as input tokens to a seq2seq Transformer; aims to reuse PLM textual tokenization and generation capabilities.",
            "graph_type": "Knowledge graphs / RDF-style triples (heterogeneous graphs)",
            "conversion_method": "Serialize graph as a sequence of triples or node lists (linearization/serialization) and feed that sequence to a pretrained seq2seq model (BART/T5/GPT) for fine-tuning; paper describes general linearization but does not prescribe a single traversal algorithm.",
            "downstream_task": "Knowledge graph-to-text generation for conversational agents (text generation from KG triplets), multilingual/emotional NLG planned",
            "performance_metrics": "No numerical metrics reported in this review for specific linearization implementations; paper states that fine-tuned PLMs empirically outperform more complex specialized architectures on KG-to-text benchmarks (qualitative claim only).",
            "comparison_to_others": "The paper reports qualitatively that PLM-based linearization approaches often outperform specialized graph encoders in generation quality when fine-tuned, but suffer from loss of structural information compared to graph-native encoders; chosen for better inference time and engineering fit versus GNN/GT.",
            "advantages": "Reuses powerful pretrained text generation models (PLMs); strong linguistic fluency; good generalization with relatively simple engineering and easier deployment; lower inference time and easier integration in DAVI's pipeline according to authors.",
            "disadvantages": "Tends to lose explicit graph topology/structural information during encoding; can be computationally expensive during pretraining/fine-tuning for large linearized inputs; potential difficulty learning explicit graph-to-text alignments from limited parallel data.",
            "failure_cases": "Degraded semantic fidelity (hallucination, omission, ordering errors) due to structural information loss and weak explicit graph-text alignment; poorer performance on very large graphs or when structural constraints are critical.",
            "uuid": "e8764.0"
        },
        {
            "name_short": "GNNs",
            "name_full": "Graph Neural Networks (GCN / GAT / GGNN variants) + decoder",
            "brief_description": "Use graph neural network encoders (GCNs, GATs, GGNNs and variants) to compute node/subgraph representations that are then decoded (often with an RNN or Transformer decoder) into text; captures graph structure explicitly via neighborhood aggregation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "graph-encoder (GNN) + sequence decoder",
            "representation_description": "Encode graph topology and node/edge features with message-passing or attention-based graph layers to produce contextual node embeddings; a sequence decoder conditions on these embeddings to generate text.",
            "graph_type": "Knowledge graphs (heterogeneous graphs), other graph-structured inputs",
            "conversion_method": "No linearization: run GNN layers (GCN, GAT, GGNN, or specialized variants) to aggregate neighbor information into node/subgraph vectors; feed encoded representation to a decoder (e.g., LSTM/Transformer) to perform text generation.",
            "downstream_task": "KG-to-text generation, question generation, conversational reasoning over KGs",
            "performance_metrics": "The review does not present numeric benchmarks for specific GNN models; it cites multiple works that report promising results but notes shortcomings in scalability and inference time.",
            "comparison_to_others": "GNNs better preserve graph topology than linearization; however, the paper states that fine-tuned PLMs (linearization + seq2seq) often achieve higher overall generation quality in practice, and GNNs incur greater computational cost and inference latency.",
            "advantages": "Explicitly models graph structure and relations, potentially improving semantic fidelity with respect to the input graph; flexible to heterogeneous graphs and relation types.",
            "disadvantages": "Computationally expensive, harder to scale to large graphs; often requires a separate decoder; inference time and memory requirements may exceed conversational-agent constraints.",
            "failure_cases": "Performance degradation on very large or complex graphs; high inference latency unacceptable for real-time conversational systems; may struggle when deployed under strict MLOps memory/time budgets.",
            "uuid": "e8764.1"
        },
        {
            "name_short": "Graph Transformer (GT)",
            "name_full": "Graph-adapted Transformer (Graph Transformer)",
            "brief_description": "Adapt Transformer architectures to graph inputs by applying self-attention over nodes with relation-aware encodings (e.g., pairwise relation encodings or visibility masks) so that global node dependencies are captured while attempting to preserve graph structure.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph Transformer / relation-aware attention",
            "representation_description": "Nodes are treated as Transformer tokens; self-attention is augmented with explicit relation encodings or visibility matrices so pairwise attention scores reflect graph relations instead of assuming a fully connected structure.",
            "graph_type": "Knowledge graphs, general graphs / trees",
            "conversion_method": "Provide node representations to a Transformer encoder and modify attention computation with relation encodings (dynamic parameters) or visibility matrices so attention is structure-aware; output fed to a decoder to generate text.",
            "downstream_task": "Graph-to-text generation (KG-to-text), graph-to-sequence learning",
            "performance_metrics": "No concrete numeric metrics provided in this review; paper cites GTs as promising but computationally demanding.",
            "comparison_to_others": "GTs can model long-range dependencies more directly than local-aggregation GNNs, but they risk treating graphs as fully connected if not augmented; PLM linearization still competitive in practice per review.",
            "advantages": "Captures global context and long-range dependencies between nodes; can incorporate relation encodings to be structure-aware; potentially better than local GNN aggregation for some tasks.",
            "disadvantages": "High computational and memory cost; if naively applied, tends to treat graphs as dense fully connected structures and can dilute explicit graph topology unless relation encodings/visibility masks are used.",
            "failure_cases": "Excessive training/inference costs for real-time conversational agents; reduced interpretability of structure unless explicitly encoded; may violate runtime constraints in production.",
            "uuid": "e8764.2"
        },
        {
            "name_short": "K-BERT",
            "name_full": "K-BERT (Enabling language representation with knowledge graph)",
            "brief_description": "Augments a Transformer by expanding input sentences with related KG triplets (knowledge layer), preserving sentence structure via a visibility matrix, and modifying attention (Mask-Transformer) to incorporate the visibility constraints so knowledge can be injected without overwhelming the sentence.",
            "citation_title": "K-bert: Enabling language representation with knowledge graph.",
            "mention_or_use": "mention",
            "representation_name": "sentence expansion + visibility-masked attention",
            "representation_description": "Input sentence entities are expanded with their KG triplets to form an expanded sentence-tree; a visibility (seeing) matrix preserves original sentence structure by controlling which expanded nodes can attend to which others; attention is masked accordingly.",
            "graph_type": "Knowledge graphs (entity triplets), sentence-centric subgraphs",
            "conversion_method": "Take a sentence and a set of triplets, expand sentence entities with their triplets to form a tree-structured input, compute a visibility matrix encoding permitted attention links, and run a modified self-attention (Mask-Transformer) respecting visibility.",
            "downstream_task": "Language representation for downstream NLG/NLU tasks; can be adapted to KG-to-text by leveraging the augmented input",
            "performance_metrics": "No numeric results provided in this review; K-BERT is cited as a method that augments Transformer attention with KG structure.",
            "comparison_to_others": "K-BERT explicitly injects KG facts into Transformer inputs and attempts to preserve sentence structure, contrasting with naive linearization which may lose structure; review does not provide direct empirical comparison.",
            "advantages": "Explicitly injects KG facts while preserving sentence integrity through visibility masks; integrates KG knowledge into pretrained Transformers without retraining from scratch.",
            "disadvantages": "Alters original input structure and requires designing visibility matrices; scalability and impact on generation fidelity not quantified in this review.",
            "failure_cases": "Potential for over-expansion of inputs and attention interference if visibility is not properly constrained; not evaluated numerically in this paper.",
            "uuid": "e8764.3"
        },
        {
            "name_short": "JoinGT",
            "name_full": "JoinGT (Graph-text joint representation learning)",
            "brief_description": "A pretraining approach that explicitly learns graph–text alignments rather than only fine-tuning seq2seq PLMs on parallel KG-to-text data, aiming to improve graph-text alignment for generation.",
            "citation_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs.",
            "mention_or_use": "mention",
            "representation_name": "graph-text joint pretraining",
            "representation_description": "Pretrain models with objectives designed to align graph structures and textual representations jointly, so downstream KG-to-text generation benefits from learned correspondences between graph elements and tokens.",
            "graph_type": "Knowledge graphs",
            "conversion_method": "Pretraining that jointly consumes graph and text inputs to learn alignment (specific pretraining tasks are cited in the referenced work but not detailed in this review).",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Not reported in this review; JoinGT is described as explicitly learning graph-text alignments and contrasted with direct fine-tuning of PLMs.",
            "comparison_to_others": "Presented as an alternative to naively fine-tuning PLMs on KG-to-text data; the review notes JoinGT focuses on pretraining to learn alignments rather than direct tuning.",
            "advantages": "Aims to build explicit graph-text alignment into the model via pretraining, which could reduce hallucination and improve fidelity.",
            "disadvantages": "Requires additional pretraining data and compute; specifics and empirical tradeoffs are not quantified in this review.",
            "failure_cases": "Not discussed numerically here; potential dependence on availability of graph-text pretraining data.",
            "uuid": "e8764.4"
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "GTR-LSTM: triple encoder for sentence generation from RDF",
            "brief_description": "A triple-encoder model that encodes RDF triple structures to generate sentences, using a tailored encoder to represent triples and an LSTM decoder for generation.",
            "citation_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data.",
            "mention_or_use": "mention",
            "representation_name": "triple-encoder (GTR) + LSTM decoder",
            "representation_description": "Encode each RDF triple (subject-predicate-object) with a dedicated encoder to obtain structured representations that feed into a sequence decoder (LSTM) producing text.",
            "graph_type": "RDF graphs / knowledge graphs represented as triples",
            "conversion_method": "Encode triples directly (triple-wise encoding) rather than linearizing whole graph; decoded sequentially by an LSTM to produce sentences.",
            "downstream_task": "Text generation from RDF/knowledge graphs (KG-to-text)",
            "performance_metrics": "No numeric metrics included in this review; referenced as a prior approach in the literature.",
            "comparison_to_others": "Triple-encoder is a graph-native approach contrasting with linearization + PLM and GNNs; review cites it among historical approaches but gives no comparative numbers.",
            "advantages": "Directly models RDF triple structure which can help preserve semantic fidelity of facts.",
            "disadvantages": "Older architecture (LSTM decoder) may lack fluency advantages of modern PLMs; scalability/latency not discussed here.",
            "failure_cases": "Not specified in this review; general concerns about older encoder-decoder architectures' fluency vs. PLMs apply.",
            "uuid": "e8764.5"
        },
        {
            "name_short": "CycLeGT",
            "name_full": "CycLeGT (Unsupervised cycle graph↔text training)",
            "brief_description": "An unsupervised cycle-training framework that jointly learns graph-to-text and text-to-graph conversions using non-parallel data, aiming to improve fidelity without needing large parallel corpora.",
            "citation_title": "Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training.",
            "mention_or_use": "mention",
            "representation_name": "cycle training (unsupervised graph↔text)",
            "representation_description": "Train graph→text and text→graph models jointly in a cycle-consistency setup so a generated text can be converted back to the original graph and compared to input, enabling training from non-parallel data.",
            "graph_type": "Knowledge graphs / graph-text paired domains (can work with non-parallel data)",
            "conversion_method": "Alternate/paired training: graph-to-text model generates text, text-to-graph model reconstructs a graph; cycle loss enforces that reconstructed graph matches original graph (and vice versa).",
            "downstream_task": "Unsupervised KG-to-text and text-to-graph generation",
            "performance_metrics": "No numeric metrics in this review; method is cited as an approach to address scarcity of parallel data.",
            "comparison_to_others": "Compared conceptually to supervised fine-tuning approaches; offers a route when parallel graph-text data is scarce.",
            "advantages": "Enables learning with non-parallel data via cycle-consistency; can improve alignment without heavy reliance on labeled datasets.",
            "disadvantages": "Cycle training complexity and stability issues; empirical gains and costs not quantified in this review.",
            "failure_cases": "Potential training instability and reliance on quality of both directions; specifics not reported here.",
            "uuid": "e8764.6"
        },
        {
            "name_short": "PromoteGraphAwareness (linearized-awareness)",
            "name_full": "Promoting graph awareness in linearized graph-to-text generation",
            "brief_description": "Techniques to make linearized graph inputs more 'graph-aware' when fed to text-oriented PLMs, e.g., augmenting linearization with structural signals to reduce topology loss.",
            "citation_title": "Promoting graph awareness in linearized graph-to-text generation.",
            "mention_or_use": "mention",
            "representation_name": "augmented linearization (graph-awareness)",
            "representation_description": "Keep linearized input but augment it with additional structural cues or special tokens/ordering strategies so sequence encoders better capture graph topology.",
            "graph_type": "Knowledge graphs (linearized for PLMs)",
            "conversion_method": "Serialize graph but add structural annotations (ordering, delimiters, special tokens or position encodings) to help the PLM infer graph relations.",
            "downstream_task": "KG-to-text generation with pre-trained text PLMs",
            "performance_metrics": "Not reported numerically in this review; cited as a line of research to mitigate structural loss from naive linearization.",
            "comparison_to_others": "Sits between naive linearization and graph-native encoders: easier to deploy than GNNs yet attempts to recover topology information lost by serialization.",
            "advantages": "Lower engineering/deployment cost than graph-native models, potential to recover some structural signals while keeping PLM benefits.",
            "disadvantages": "Still may not fully capture complex graph structure; effectiveness depends on design of augmentation; no numeric comparisons provided here.",
            "failure_cases": "May fail on graphs where structural relations are complex and cannot be encoded via simple linear cues.",
            "uuid": "e8764.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation.",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation.",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers.",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data.",
            "rating": 2,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        },
        {
            "paper_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs.",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Kgpt: Knowledge-grounded pretraining for data-to-text generation.",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training.",
            "rating": 2,
            "sanitized_title": "cyclegt_unsupervised_graphtotext_and_texttograph_generation_via_cycle_training"
        },
        {
            "paper_title": "Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation.",
            "rating": 2,
            "sanitized_title": "genwiki_a_dataset_of_13_million_contentsharing_text_and_graphs_for_unsupervised_graphtotext_generation"
        }
    ],
    "cost": 0.01327925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph for NLG in the context of conversational agents</p>
<p>Hussam Ghanem 
Massinissa Atmani 
Christophe Cruz 
Knowledge Graph for NLG in the context of conversational agents
Received: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)Conversational agents · Knowledge graphs · Natural Language Generation
The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.</p>
<p>integrated with messaging platforms, mobile applications, and websites to provide instant support to customers and handle simple tasks, such as answering questions or helping with bookings. Conversational agents using knowledge graphs (KG) [9] are a type of chatbot that leverages structured data stored in a knowledge graph to generate human-like responses. The knowledge graph is a graph-based representation of entities and their relationships, providing a structured source of information for the chatbot to access and use during conversation. This enables the chatbot to provide more accurate and comprehensive answers to user's questions. The use of knowledge graphs can greatly enhance the capabilities of conversational agents and make interactions more informative and useful [57].</p>
<p>Generating answers during conversations consists in generating text from data. Data-to-text processes require algorithms that generate linguistically correct sentences for humans and express the semantics and structure of nonlinguistic data (sequence, tree, graph, etc.). In addition, the generation of textual answers requires supporting several languages. And for a better interaction with a conversational agent, the emotional context of the conversation (Common Ground) is fundamental. The aim of this work in collaboration with the company DAVI is to integrate a socio-emotional dimension into humanmachine interactions which complement the technical and "business" skills linked to professional expertise. The company DAVI is a software publisher in SaaS mode which has expertise in the fields of AI, Affective Computing, and Human Machine Interactions (HMIs). The following picture presents the composite AI of DAVI's solution including Natural Language Understanding, Emotion detection, skills modeling, Natural Language Generation, and Body Language Generation. For now, the natural language generation (NLG) of a conversational engine does not benefit from the latest technological advances in natural language processing (NLP). The Natural Language processing step is based on a manual process to define the template of the answer. This process requires a costly amount of time. Thus, the purpose of this project is to automate and reduce the burden of the generation of template-based responses (as the responses are manually written through a set of rules) in the implementation of conversa-tional agents. The template-based responses are modellized and stored in the skills' database. Regarding the emotional dimension, emotional responses are injected automatically in the answer depending the emotional analysis of the user. To automate NLG answers from Skills, knowledge graphs were identified. In NLG, Two criteria [33] are used to assess the quality of the produced answers. The first criterion is semantic consistency (Semantic Fidelity) which quantifies the fidelity of the data produced against the input data. The most common indicators are 1/ Hallucination: It is manifested by the presence of information (facts) in the generated text that is not present in the input data; 2/ Omission: It is manifested by the omission of one of the pieces of information (facts) in the generated text; 3/ Redundancy: This is manifested by the repetition of information in the generated text; 4/ Accuracy: The lack of accuracy is manifested by the modification of information such as the inversion of the subject and the direct object complement in the generated text; 5/ Ordering: It occurs when the sequence of information is different from the input data. The second criterion is linguistic coherence (Output Fluency) to evaluate the fluidity of the text and the linguistic constructions of the generated text, the segmentation of the text into different sentences, the use of anaphoric pronouns to reference entities and to have linguistically correct sentences.</p>
<p>Today, neural approaches offer performances exceeding all classical methods for linguistic coherence. However, limits are still present to maintain semantic consistency, and their performance deteriorates even more on long texts [30]. Another limitation due to the complexity of neural approaches is that text generation is non-parameterized with no control over the structure of the generated text. Thus, most of the current neural approaches arrive behind template-based approaches on the criterion of semantic consistency [31], but they are far superior to them on the criterion of linguistic consistency. This can be explained by the fact that large language models manage to capture certain syntactic and semantic properties of the language.</p>
<p>The present review is organized as follows, Section 2 presents a comprehensive overview of the current state-of-the-art approaches for knowledge graphto-text generation. In Section 3, we present the latest architectures and techniques that have been proposed in this field. Finally, Section 4 critically examines the strengths and limitations of these techniques in the context of conversational agents 2 Knowledge Graph-to-Text Generation KG-to-text generation aims at producing easy-to-understand sentences in natural language from knowledge graphs (KGs) while maintaining semantic consistency between the generated sentences and the KG triplets. Compared to the traditional text generation task (Seq2Seq), generating text from a knowledge graph is an additional challenge to guarantee the authenticity of the words in the generated sentences. The existing methods can be classified according to three categories ( Figure 3) and will be detailed later:</p>
<p>-Linearisation with Sequence-to-sequence (Seq2Seq): convert the graph to a sequence which is the fed to a sequence-to-sequence model; -Graph Neural Networks (GNNs): encode topological structures of a graph and learn the representation of an entity by the aggregation of the features of the entities and neighbors. They are not used as a standalone model and require a decoder to complete the encoder-decoder architecture; -Graph Transformer (GT): the enhanced version of the original transformer adapted to handle graphs.</p>
<p>The term "knowledge graph" has been around since 1972, but its current definition can be traced back to Google's 2012 announcement of their Knowledge Graph. This was followed by similar announcements from companies such as Airbnb, Amazon, eBay, Facebook, IBM, LinkedIn, Microsoft, and Uber, among others, leading to an increase in the adoption of knowledge graphs by various industries [4]. As a result, academic research in this field has seen a surge in recent years, with an increasing number of scientific publications on knowledge graphs [4]. These graphs utilize a graph-based data model to effectively manage, integrate, and extract valuable insights from large and diverse datasets [5].</p>
<p>Knowledge graphs, which are composed of nodes that represent different types of entities and edges that denote various types of relationships between those entities, are known as heterogeneous graphs. The integration of information from multiple sources and domains in knowledge graphs leads to an even greater degree of heterogeneity. To address this, recent research has applied heterogeneous graph embedding methods to represent knowledge graphs effectively. For example, ERNIE [40] and KnowBERT [41] employ knowledge graph embedding techniques such as TransE [42] and TuckER [58] to encode knowledge graphs.</p>
<p>Generating text and learning alignments between source entities/relationships and target tokens from scratch is a challenging task for standard language models because of the limited amount of parallel graph-text data [17,34]. To overcome this limitation, recent research has focused on developing generalist pre-trained language models for KG-to-text generation. A common approach is to linearize input graphs into text sequences and fine-tune pre-trained seq2seq Transformer models such as GPT [35], BART [10], or T5 [36] based on KG-to-text datasets [1,37]. These pre-trained language models can generate high-quality texts with a simple fine-tuning to the target task, thanks to their self-supervised pre-training on large-scale corpora of unlabeled texts. In fact, pre-trained language models outperform other models with sophisticated structures in KG-to-text generation tasks. This type of approach will be detailed in section 3.</p>
<p>According to [14], text generation tasks using KG-to-text models mainly fall under three aspects:</p>
<p>-Encoder modification: To reduce the loss of structural information in sequence encoders with graph inputs that have been linearized [24,6,32], proposals concentrate on constructing more intricate encoder structures to improve the representation of graphs, including GNNs and GTs; -Unsupervised training: These proposals consist in designing unsupervised training methods to jointly learn graph-to-text and text-to-graph con-version tasks with non-parallel graph-to-text data [39,17,18]. This makes it possible to compare the final result of the process with the input data; -Build pre-trained models: With the development of pre-trained Natural Language Generation (NLG) models such as GPT, BART, and T5, some recent work directly refines these models on graph-to-text datasets and reports significant performance [1,37,15,2].</p>
<p>Compared to existing work on pre-trained models for KG-text generation, the JoinGT model [14] uses pre-training methods to explicitly learn graphtext alignments instead of directly tuning the pre-trained models seq2seq on KG-to-text datasets.</p>
<p>Architectures</p>
<p>In this section, the different architectures used in data-to-text tasks will be presented. As the nature of the data greatly influences the choice of the architecture of the neural approaches, most works either try to adapt the inputs to the architectures of the models or propose new architectures better adapted to the types of input data. Due to the nature of sentences and the tree structure of its representations, several works have proposed to model data structures of this type to enhance performance.</p>
<p>Graph linearisation and sequence to sequence models (Seq2seq)</p>
<p>Recent years have been marked by significant achievements in the field of PLMs (Pretrained Language Models) [38,44]. Pre-trained on massive corpora, PLMs exhibit good generalization ability to solve related NLG (Natural Language Generation) downstream tasks [43]. However, most existing PLMs were trained on textual data [44,10] without ingesting any structured data input. The seq-to-seq category consists of linearizing the KG [1, 25, 24, 3] and then formulating a Seq2seq generation task using PLMs like GPT [35], BART [10] or T5 [36] with linearized KG nodes as input to generate sentences. The use of pre-trained language models (PLMs) in KG-to-text generation has shown superior performance, but still faces two major challenges: 1) loss of structural information during encoding, as existing models like BERT do not explicitly take into account the relationship between input entities; and 2) lack of explicit graph-text alignments, as complex knowledge graph structures make it difficult to learn graph-text alignments through text reconstruction-based pretraining tasks. Despite attempts to retain as much of the graph topology as possible with seq2seq methods, the Transformer-based seq2seq models' cost is not cheap (especially in the pretraining phase). Also, the computational cost of linearization can be high for large knowledge graphs. Hence, and so to better keep the graph topology, Graph Neural Networks (GNNs) have been proposed, which will be discussed in the next section.</p>
<p>Graph Neural Networks (GNNs)</p>
<p>Different approaches use different variants of GNNs architectures such as GCNs (Graph Convolutional Networks) [27] or extensions of GCNs such as Syn-GCNs [45] or DCGCNs [11]. Other approaches use the variant GATs (Graph Attention Networks) [8]. Or approaches that use the GGNNs (Gated Graph Neural Networks) variant [16,21,47]. Graph Neural Networks (GNNs) are a type of neural network that are well-suited for processing graph-structured data. In the context of knowledge graph-to-text generation, GNNs can be used to model the relationships between entities in a knowledge graph and generate text based on those relationships. Recent research on using GNNs for knowledge graph to text generation has shown promising results. Some studies have used graph convolutional networks (GCNs) [27] to encode the relationships between entities in a knowledge graph into a low-dimensional representation, or extensions of GCNs such as Syn-GCNs [45] and DCGCNs [11]. Other studies have used graph attention networks (GATs) [8] to dynamically weight the importance of different entities and relationships in the knowledge graph when generating text. Other studies have used a gating mechanism (Gated Graph Neural Networks or GGNNs) that allows for effectively controlling the flow of information between nodes in the graph, which is useful for incorporating contextual information [16,21,47]. Additionally, some researchers have combined GNNs with reinforcement learning to generate text that maximizes a reward function defined over the generated text and the knowledge graph [21] .</p>
<p>Overall, the use of GNNs for knowledge graph to text generation is an active area of research, with many recent studies exploring different architectures and training methods. The results so far suggest that GNNs can effectively capture the relationships between entities in a knowledge graph and generate high-quality text based on that information. A limitation relied to KG-to-Text generation with GNNs, is that GNNs can be computationally expensive and may struggle to handle large knowledge graphs. Additionally, their performance may degrade for graphs with complex relationships or structures. Despite these limitations, GNNs remain a promising direction for knowledge graph-to-text generation.</p>
<p>Graphs Transformers (GTs)</p>
<p>In order to benefit from the power of models based on Transformer and to be able to model tree or graph-type data structures as with GNNs, and to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases, recent works have proposed to adapt the Transformer architecture. As Graph Transformers are equipped with self-attention mechanisms, they can capture global context information by applying these mechanisms to the nodes of the graph.</p>
<p>According to [12], GT differs from GNNs in that it allows direct modeling of dependencies between any pair of nodes regardless of their distance in the input graph. An undesirable consequence is that it essentially treats any graph as a fully connected graph, greatly reducing the explicit structure of the graph. To maintain a structure-aware view of the graph, their proposed model introduces an explicit relationship encoding and integrates it into the pairwise attention score computation as a dynamic parameter.</p>
<p>From the GNNs pipeline, if we make several parallel heads of neighborhood aggregation and replace the sum on the neighbors by the attention mechanism, e.g. a weighted sum, we would get the Graph Attention Network (GAT). Adding normalization and MLP feed-forward, we have a Graph Transformer [46]. For the same reasons as Graph Transformer, [13] presents the K-BERT model, they introduce four components to augment the Transformer architecture and to be able to handle a graph as input. The first knowledge layer component takes a sentence and a set of triplets as input and outputs the sentence tree by expanding the sentence entities with their corresponding triplets. They also add the Seeing Layer component to preserve the original sentence structure and model the relationships of the triples by building a Visibility Matrix. Another component is the Mask-Transformer where they modify the self-attention layer to consider the visibility matrix when calculating attention.</p>
<p>The use of Graph Transformers for Knowledge graph to text generation has gained popularity in recent years due to their ability to effectively handle graph structures and capture the relationships between nodes in the graph. Additionally, Graph Transformers can handle large graphs and have the ability to model long-range dependencies between nodes in the graph. Despite the advantages, the training of Graph Transformers can be computationally expensive and the interpretability of the model remains a challenge. Overall, the use of Graph Transformers for Knowledge graph to text generation is a promising area of research and can lead to significant improvements in the generation of text from knowledge graphs.</p>
<p>Discusion</p>
<p>The Graph Neural Network (GNN), the Graph Transformer, and linearization with seq2seq models are three different architectures for knowledge graph to text generation, a task that involves generating natural language text from structured knowledge representations like knowledge graphs. GNNs are a type of deep learning model that are well-suited for processing graph-structured data. They provide a flexible and scalable way to model the graph structure and relationships, but they may not be able to efficiently handle large and complex graphs. The Graph Transformer is a specialized version of the Transformer, designed for graph-to-sequence learning tasks. It provides a more direct and efficient way to process the graph structure compared to linearization with seq2seq models, but it may require more training data and computation resources. Linearization with seq2seq models is a simpler and easier to implement approach that involves converting the knowledge graph into a linear sequence, such as a sentence, and then using a seq2seq model to generate text from the linearized representation. However, this approach can lose some of the structural information and relationships in the knowledge graph during the linearization process.</p>
<p>In summary, each of the three architectures has its own advantages and disadvantages, and the choice of architecture will depend on the specific requirements of the actual task.</p>
<p>As our project is part of a context of conversational agents, we must take into account all the consequent constraints such as the execution time (to respect the instant conversation constraint) and the validity of the model where the answer must not be incorrect or ambiguous. If most of the current models have satisfactory validity performances, the inference time of models based on GNN and GraphTransformer exceeds the limit threshold found in a fluid and natural conversation and requires a huge memory load that violates the standards of current industrialization (MLOps) of neural models.</p>
<p>In light of these elements, and with the constraint of the data labellisation for the domains of DAVI, we choose to go further with seq2seq Transformer based models (PLMs) in our Knowledge Graph-to-Text Generation. We also want to shed light on the fact that DAVI already handles such models in their pipeline and they have the knowledge and infrastructure to optimize the integration and deployment of the seq2seq models. Hence, DAVI should still remain in control of the time to market of the NLG solution.</p>
<p>Conclusion</p>
<p>In conclusion, the document discusses different data-to-text architectures and highlights their advantages and limitations in the context of graph-to-text project with DAVI. The Graph Neural Network (GNN), Graph Transformer, and seq2seq models are three approaches that have been applied to the task of generating natural language text from structured knowledge representations like knowledge graphs. Each approach has its own advantages and disadvantages, and the choice of architecture will depend on the specific requirements of the task. Considering the constraints of our project, which includes developing a conversational agent that must generate valid responses in real-time, we have decided to move forward with seq2seq Transformer-based models (PLMs) as they have satisfactory performance on validity and execution time. Additionally, DAVI already handles such models in their pipeline and can optimize their integration and deployment. Our next step will be to explore state-of-the-art approaches that take into account the emotional and multilingual dimensions to achieve the objectives of the graph-to-text project.</p>
<p>Fig. 1
1Composite AI pipeline of virtual agents at DAVI</p>
<p>Fig. 2
2Automatic text generation from the knowledge graph</p>
<p>Fig. 3
3The architecture of KG-to-text generation with the three categories of representation, a) Linearization + Seq2Seq, b) GNNs with decoder (e.g. LSTM), and c) Graph Transformer (GT)</p>
<p>Investigating pretrained language models for graph-to-text generation. L F Ribeiro, M Schmitt, H Schütze, I Gurevych, arXiv:2007.08426arXiv preprintRibeiro, L. F., Schmitt, M., Schütze, H., &amp; Gurevych, I. (2020). Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426.</p>
<p>Gpt-too: A language-model-first approach for amr-to-text generation. M Mager, R F Astudillo, T Naseem, M A Sultan, Y S Lee, R Florian, S Roukos, arXiv:2005.09123arXiv preprintMager, M., Astudillo, R. F., Naseem, T., Sultan, M. A., Lee, Y. S., Florian, R.,&amp; Roukos, S. (2020). Gpt-too: A language-model-first approach for amr-to-text generation. arXiv preprint arXiv:2005.09123.</p>
<p>Promoting graph awareness in linearized graph-to-text generation. A Hoyle, A Marasović, N Smith, arXiv:2012.15793arXiv preprintHoyle, A., Marasović, A.,&amp; Smith, N. (2020). Promoting graph awareness in linearized graph-to-text generation. arXiv preprint arXiv:2012.15793.</p>
<p>Knowledge graphs. A Hogan, E Blomqvist, M Cochez, C Amato, G D Melo, C Gutierrez, S Kirrane, J E Gayo, R Navigli, S Neumaier, A C Ngomo, ACM Computing Surveys (CSUR). 544Hogan A, Blomqvist E, Cochez M, d'Amato C, Melo GD, Gutierrez C, Kirrane S, Gayo JE, Navigli R, Neumaier S, Ngomo AC. Knowledge graphs. ACM Computing Surveys (CSUR). 2021 Jul 2;54(4):1-37.</p>
<p>Industryscale knowledge graphs: Lessons and challenges. N F Noy, Y Gao, A Jain, A Narayanan, A Patterson, J Taylor, ACM Queue. 17N. F. Noy, Y. Gao, A. Jain, A. Narayanan, A. Patterson, and J. Taylor. 2019. Industry- scale knowledge graphs: Lessons and challenges. ACM Queue 17, 2 (2019).</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. B Distiawan, J Qi, R Zhang, W Wang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Distiawan, B., Qi, J., Zhang, R.,&amp; Wang, W. (2018, July). GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1627-1637).</p>
<p>Structural adapters in pretrained language models for amr-to-text generation. L F Ribeiro, Y Zhang, I Gurevych, arXiv:2103.09120arXiv preprintRibeiro, L. F., Zhang, Y.,&amp; Gurevych, I. (2021). Structural adapters in pretrained lan- guage models for amr-to-text generation. arXiv preprint arXiv:2103.09120.</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. L F Ribeiro, Y Zhang, C Gardent, I Gurevych, Transactions of the Association for Computational Linguistics. 8Ribeiro, L. F., Zhang, Y., Gardent, C.,&amp; Gurevych, I. (2020). Modeling global and local node contexts for text generation from knowledge graphs. Transactions of the Association for Computational Linguistics, 8, 589-604.</p>
<p>Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. S Moon, P Shah, A Kumar, R Subba, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsMoon, S., Shah, P., Kumar, A.,&amp; Subba, R. (2019, July). Opendialkg: Explainable con- versational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 845-854).</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, . . Zettlemoyer, L , arXiv:1910.13461arXiv preprintnot graphLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...&amp; Zettle- moyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. (not graph)</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Z Guo, Y Zhang, Z Teng, W Lu, Transactions of the Association for Computational Linguistics. 7Guo, Z., Zhang, Y., Teng, Z.,&amp; Lu, W. (2019). Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computa- tional Linguistics, 7, 297-312.</p>
<p>Graph transformer for graph-to-sequence learning. D Cai, W Lam, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Cai, D.,&amp; Lam, W. (2020, April). Graph transformer for graph-to-sequence learning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 05, pp. 7464- 7471).</p>
<p>. W Liu, P Zhou, Z Zhao, Z Wang, Q Ju, H Deng, P Wang, Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H.,&amp; Wang, P. (2020, April).</p>
<p>Enabling language representation with knowledge graph. K-Bert , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34K-bert: Enabling language representation with knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 03, pp. 2901-2908).</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, . . Huang, M , arXiv:2106.10502arXiv preprintKe, P., Ji, H., Ran, Y., Cui, X., Wang, L., Song, L., ...&amp; Huang, M. (2021). Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. arXiv preprint arXiv:2106.10502.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. W Chen, Y Su, X Yan, W Y Wang, arXiv:2010.02307arXiv preprintChen, W., Su, Y., Yan, X.,&amp; Wang, W. Y. (2020). Kgpt: Knowledge-grounded pre- training for data-to-text generation. arXiv preprint arXiv:2010.02307.</p>
<p>Toward subgraph guided knowledge graph question generation with graph neural networks. Y Chen, L Wu, M J Zaki, arXiv:2004.06015arXiv preprintChen, Y., Wu, L.,&amp; Zaki, M. J. (2020). Toward subgraph guided knowledge graph question generation with graph neural networks. arXiv preprint arXiv:2004.06015.</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Q Guo, Z Jin, X Qiu, W Zhang, D Wipf, Z Zhang, arXiv:2006.04702arXiv preprintGuo, Q., Jin, Z., Qiu, X., Zhang, W., Wipf, D.,&amp; Zhang, Z. (2020). Cyclegt: Unsu- pervised graph-to-text and text-to-graph generation via cycle training. arXiv preprint arXiv:2006.04702</p>
<p>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. Z Jin, Q Guo, X Qiu, Z Zhang, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsJin, Z., Guo, Q., Qiu, X., &amp; Zhang, Z. (2020, December). Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Conference on Computational Linguistics (pp. 2398- 2409).</p>
<p>Stage-wise Fine-tuning for Graph-to-Text Generation. Q Wang, S Yavuz, V Lin, H Ji, N Rajani, arXiv:2105.08021arXiv preprintWang, Q., Yavuz, S., Lin, V., Ji, H.,&amp; Rajani, N. (2021). Stage-wise Fine-tuning for Graph-to-Text Generation. arXiv preprint arXiv:2105.08021.</p>
<p>Structural information preserving for graph-to-text generation. L Song, A Wang, J Su, Y Zhang, K Xu, Y Ge, D Yu, arXiv:2102.06749arXiv preprintSong, L., Wang, A., Su, J., Zhang, Y., Xu, K., Ge, Y.,&amp; Yu, D. (2021). Structural information preserving for graph-to-text generation. arXiv preprint arXiv:2102.06749.</p>
<p>Reinforcement learning based graph-to-sequence model for natural question generation. Y Chen, L Wu, M J Zaki, arXiv:1908.04942arXiv preprintChen, Y., Wu, L.,&amp; Zaki, M. J. (2019). Reinforcement learning based graph-to-sequence model for natural question generation. arXiv preprint arXiv:1908.04942.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. H Harkous, I Groves, A Saffari, arXiv:2004.06577arXiv preprintHarkous, H., Groves, I.,&amp; Saffari, A. (2020). Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. arXiv preprint arXiv:2004.06577.</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, arXiv:1904.02342arXiv preprintKoncel-Kedziorski, R., Bekal, D., Luan, Y., Lapata, M.,&amp; Hajishirzi, H. (2019). Text generation from knowledge graphs with graph transformers. arXiv preprint arXiv:1904.02342.</p>
<p>The WebNLG challenge: Generating text from RDF data. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationGardent, C., Shimorina, A., Narayan, S.,&amp; Perez-Beltrachini, L. (2017, September). The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation (pp. 124-133).</p>
<p>Improving text-to-text pre-trained models for the graph-to-text task. Z Yang, A Einolghozati, H Inan, K Diedrick, A Fan, P Donmez, S Gupta, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Yang, Z., Einolghozati, A., Inan, H., Diedrick, K., Fan, A., Donmez, P.,&amp; Gupta, S. (2020). Improving text-to-text pre-trained models for the graph-to-text task. In Proceed- ings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (pp. 107-116).</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, 30&amp; Polosukhin, IVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ...&amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907arXiv preprintKipf, T. N.,&amp; Welling, M. (2016). Semi-supervised classification with graph convolu- tional networks. arXiv preprint arXiv:1609.02907.</p>
<p>A new model for learning in graph domains. M Gori, G Monfardini, F Scarselli, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksIEEE2M. Gori, G. Monfardini, and F. Scarselli, "A new model for learning in graph domains," in Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., vol. 2, pp. 729-734, IEEE, 2005.</p>
<p>The graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE Transactions on Neural Networks. 201F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, "The graph neural network model," IEEE Transactions on Neural Networks, vol. 20, no. 1, pp. 61-80, 2008.</p>
<p>Data-to-text generation with content selection and planning. Ratish Puduppully, Li Dong, Mirella Lapata, Ratish Puduppully, Li Dong, and Mirella Lapata. 2018. Data-to-text generation with content selection and planning.</p>
<p>E2e nlg challenge: Neural models vs. templates. Yevgeniy Puzikov, Iryna Gurevych, Yevgeniy Puzikov and Iryna Gurevych. 2018. E2e nlg challenge: Neural models vs. templates.</p>
<p>A Moryossef, Y Goldberg, I Dagan, arXiv:1904.03396Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv preprintMoryossef, A., Goldberg, Y.,&amp; Dagan, I. (2019). Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv preprint arXiv:1904.03396.</p>
<p>Neural datato-text generation: A comparison between pipeline and end-to-end architectures. T C Ferreira, C Van Der Lee, E Van Miltenburg, E Krahmer, arXiv:1908.09022arXiv preprintFerreira, T. C., van der Lee, C., Van Miltenburg, E.,&amp; Krahmer, E. (2019). Neural data- to-text generation: A comparison between pipeline and end-to-end architectures. arXiv preprint arXiv:1908.09022.</p>
<p>Partially-aligned data-to-text generation with distant supervision. Z Fu, B Shi, W Lam, L Bing, Z Liu, arXiv:2010.01268arXiv preprintFu, Z., Shi, B., Lam, W., Bing, L.,&amp; Liu, Z. (2020). Partially-aligned data-to-text gen- eration with distant supervision. arXiv preprint arXiv:2010.01268.</p>
<p>. A Radford, K Narasimhan, T Salimans, I Sutskever, Improving language understanding by generative pre-trainingRadford, A., Narasimhan, K., Salimans, T.,&amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</p>
<p>. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ...</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. P J Liu, J. Mach. Learn. Res. 21140&amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.</p>
<p>Text-to-text pre-training for data-to-text tasks. M Kale, A Rastogi, arXiv:2005.10433arXiv preprintKale, M.,&amp; Rastogi, A. (2020). Text-to-text pre-training for data-to-text tasks. arXiv preprint arXiv:2005.10433.</p>
<p>J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M. W., Lee, K.,&amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>An unsupervised joint system for text generation from knowledge graphs and semantic parsing. M Schmitt, S Sharifzadeh, V Tresp, H Schütze, arXiv:1904.09447arXiv preprintSchmitt, M., Sharifzadeh, S., Tresp, V., &amp; Schütze, H. (2019). An unsupervised joint system for text generation from knowledge graphs and semantic parsing. arXiv preprint arXiv:1904.09447.</p>
<p>Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.07129ERNIE: Enhanced language representation with informative entities. arXiv preprintZhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., &amp; Liu, Q. (2019). ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129.</p>
<p>M E Peters, M Neumann, I V Logan, R L Schwartz, R Joshi, V Singh, S Smith, N A , arXiv:1909.04164Knowledge enhanced contextual word representations. arXiv preprintPeters, M. E., Neumann, M., Logan IV, R. L., Schwartz, R., Joshi, V., Singh, S., &amp; Smith, N. A. (2019). Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164.</p>
<p>Translating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Advances in neural information processing systems. 26Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Trans- lating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26.</p>
<p>Pretrained language models for text generation: A survey. J Li, T Tang, W X Zhao, J R Wen, arXiv:2105.10311arXiv preprintLi, J., Tang, T., Zhao, W. X.,&amp; Wen, J. R. (2021). Pretrained language models for text generation: A survey. arXiv preprint arXiv:2105.10311.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,&amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.</p>
<p>A simple and accurate syntaxagnostic neural model for dependency-based semantic role labeling. D Marcheggiani, A Frolov, I Titov, arXiv:1701.02593arXiv preprintMarcheggiani, D., Frolov, A.,&amp; Titov, I. (2017). A simple and accurate syntax- agnostic neural model for dependency-based semantic role labeling. arXiv preprint arXiv:1701.02593.</p>
<p>Transformers are Graph Neural Networks. The Gradient. K Chaitanya, Joshi, Chaitanya K.Joshi. Transformers are Graph Neural Networks. The Gradient.</p>
<p>Y Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493Gated graph sequence neural networks. arXiv preprintLi, Y., Tarlow, D., Brockschmidt, M.,&amp; Zemel, R. (2015). Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493.</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.09699arXiv preprintDwivedi, V. P.,&amp; Bresson, X. (2020). A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699.</p>
<p>Evaluating semantic accuracy of data-to-text generation with natural language inference. O Dušek, Z Kasner, arXiv:2011.10819arXiv preprintDušek, O.,&amp; Kasner, Z. (2020). Evaluating semantic accuracy of data-to-text generation with natural language inference. arXiv preprint arXiv:2011.10819.</p>
<p>Sticking to the facts: Confident decoding for faithful data-to-text generation. R Tian, S Narayan, T Sellam, A P Parikh, arXiv:1910.08684arXiv preprintTian, R., Narayan, S., Sellam, T.,&amp; Parikh, A. P. (2019). Sticking to the facts: Confident decoding for faithful data-to-text generation. arXiv preprint arXiv:1910.08684.</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Y Xiao, W Y Wang, arXiv:2103.15025arXiv preprintXiao, Y.,&amp; Wang, W. Y. (2021). On hallucination and predictive uncertainty in condi- tional language generation. arXiv preprint arXiv:2103.15025.</p>
<p>Challenges in data-todocument generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman , Stuart Shieber , and Alexander Rush . 2017 . Challenges in data-to- document generation . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253 -2263 ,Copenhagen, Denmark. Association for Computational Linguistics .https://doi.org/10.18653/v1/D17-1239</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. E Durmus, H He, M Diab, arXiv:2005.03754arXiv preprintDurmus, E., He, H.,&amp; Diab, M. (2020). FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. arXiv preprint arXiv:2005.03754.</p>
<p>Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. Katja Filippova, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870. the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870.</p>
<p>Conversational agents: Theory and applications. M Wahde, M Virgolin, Deep Learning, Intelligent Control and Evolutionary Computation. 2Wahde, M.,&amp; Virgolin, M. (2022). Conversational agents: Theory and applications. In HANDBOOK ON COMPUTER LEARNING AND INTELLIGENCE: Volume 2: Deep Learning, Intelligent Control and Evolutionary Computation (pp. 497-544).</p>
<p>A survey on conversational agents/chatbots classification and design techniques. S Hussain, O Ameri Sianaki, N Ababneh, WAINA-2019) 33Web, Artificial Intelligence and Network Applications: Proceedings of the Workshops of the 33rd International Conference on Advanced Information Networking and Applications. Hussain, S., Ameri Sianaki, O.,&amp; Ababneh, N. (2019). A survey on conversational agents/chatbots classification and design techniques. In Web, Artificial Intelligence and Network Applications: Proceedings of the Workshops of the 33rd International Conference on Advanced Information Networking and Applications (WAINA-2019) 33 (pp. 946-956).</p>
<p>KBot: A Knowledge graph based chatBot for natural language understanding over linked data. A Ait-Mlouk, L Jiang, IEEE Access. 8Ait-Mlouk, A.,&amp; Jiang, L. (2020). KBot: A Knowledge graph based chatBot for natural language understanding over linked data. IEEE Access, 8, 149220-149230.</p>
<p>Tucker: Tensor factorization for knowledge graph completion. I Balažević, C Allen, T M Hospedales, arXiv:1901.09590arXiv preprintBalažević, I., Allen, C., &amp; Hospedales, T. M. (2019). Tucker: Tensor factorization for knowledge graph completion. arXiv preprint arXiv:1901.09590.</p>            </div>
        </div>

    </div>
</body>
</html>
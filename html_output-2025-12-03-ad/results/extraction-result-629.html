<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-eb2b3d8b76355700a0cd2bdbb347a5b1ecd362ab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/eb2b3d8b76355700a0cd2bdbb347a5b1ecd362ab" target="_blank">Controlling Recurrent Neural Networks by Conceptors</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A mechanism of neurodynamical organization, called conceptors, is proposed, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic, and helps explain how conceptual-level information processing emerges naturally and robustly in neural systems.</p>
                <p><strong>Paper Abstract:</strong> The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e629.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e629.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptor framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptor-based neurodynamical system (Conceptor framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neuro-symbolic mechanism that derives matrix 'conceptors' from recurrent neural network state correlations to mediate symbolic-like operations (Boolean, abstraction) over neural dynamics, enabling selection, morphing, and logical combination of dynamical patterns within a single RNN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conceptor-based hybrid neuro-symbolic system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An architecture combining a reservoir (randomly connected recurrent neural network) with conceptor matrices C that are computed from the reservoir state correlation R as C = R(R + α^{-2} I)^{-1}. Conceptor matrices act as dynamical filters inside the feedback loop (x(n+1) = C tanh(W x(n))) to constrain reservoir trajectories to pattern-specific state-space ellipsoids. Conceptor matrices admit linear combinations and Boolean-like operations (OR, AND, NOT), an abstraction ordering, and aperture adaptation; they are used to (i) select/re-generate specific learned dynamical patterns from a loaded reservoir, (ii) morph/interpolate/extrapolate between patterns, (iii) implement incremental memory allocation and prevent catastrophic forgetting, and (iv) support concept-level symbolic operations on dynamical representations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Algebraic and logical structures over conceptor matrices (matrix-valued 'concepts') implementing symbolic/declarative operations: Boolean-like operators (C1 ∨ C2, C1 ∧ C2, ¬C), abstraction ordering (C1 ≤ C2 defined by existence of B s.t. C2 = C1 ∨ B), and a formal conceptor logic (sections on 'Conceptor Logic' and 'Conceptor Logic as Institutions') that gives semantical interpretation to these operators.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Imperative neural substrate is a reservoir computing RNN (random recurrent weight matrix W*, tanh activations, optionally trained internalized W) together with standard readout training; also variants: random-feature reservoir decomposition (F, G projections, feature neurons z and scalar conception weights c_i) and online adaptation rules (autoconceptor adaptation) implemented procedurally.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, closed-loop integration: declarative conceptors are computed from neural (imperative) state statistics and then re-injected multiplicatively into the neural update loop as filters (C × neural activation). Symbolic operations (Boolean, linear mixtures) are defined algebraically on conceptor matrices and then applied as neural filters; top-down/bottom-up interaction in hierarchical designs passes conceptor-derived contextual constraints downwards while neural layers perform procedural denoising and feature extraction. Integration also includes online adaptation (autoconceptors) where conceptors self-adapt from neural activity.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Emergent capabilities include (1) content-addressable dynamical memory (cue-based re-creation of stored patterns via autoconceptor adaptation), (2) morphing/interpolation and extrapolation of dynamical patterns from few prototypes, (3) incremental life-long learning with explicit memory quota tracking and avoidance of catastrophic forgetting, (4) robust noise suppression via singular-value nulling during autoconceptor adaptation, (5) compositional symbolic-like reasoning over dynamical patterns (Boolean combinations, abstraction), and (6) class-learning: with many examples the system generalizes to the parametric class producing similar recall accuracy for novel unseen members.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demonstrations across tasks: (a) pattern storage and re-generation (periodic, chaotic attractors), (b) morphing / generalization of dynamical motor-like patterns, (c) Japanese vowels speaker recognition benchmark (9-way speaker ID), (d) incremental pattern loading and memory management experiments, (e) content-addressable memory recall experiments, (f) hierarchical denoising + classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Japanese vowels: average test misclassifications = 3.4 (averaged over 50 random reservoirs); re-generation example MSEs for four simple loaded patterns: 3.3e-05, 1.4e-05, 0.0040, 0.0019 (mean square errors); content-addressable auto-adaptation: log10 NRMSE improved from ≈ -0.4 after cueing to ≈ -1.1 after autoadaptation. (Units: misclassification counts; errors: normalized root mean square error, reported in log10 for some experiments.)</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Shows strong within-family generalization (class-learning): when many patterns from a parametric family are loaded, recall accuracy for novel unseen family members approaches that for loaded members; supports interpolation and extrapolation by linear combination of conceptors (mixtures with coefficients summing to 1 for interpolation; negative coefficients allow extrapolation). Also demonstrates robustness to strong reservoir state noise after autoconceptor adaptation (SNR = 1 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability from the declarative side: conceptor matrices have geometric semantics (ellipsoids in state space) and admit Boolean- and abstraction-operators with formal semantics; single neurons (in random-feature variant) can encode individual conceptors, enabling explicit pattern-addressing; hypothesis vectors (γ) in hierarchical model provide an explicit, interpretable belief distribution over prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Stated limitations include: conceptor matrix computations are non-local and biologically implausible in raw form; aperture parameter α must be chosen/adapted (though attenuation-based automatic choice is proposed); random-feature approximations increase aperture sensitivity; memory capacity is finite and a reservoir can become 'full' (quota ≈ 0.99) and then further loading fails; no direct comparison to purely symbolic reasoning systems is provided; some theoretical propositions were revised in later revisions (author notes).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>A mathematical theory: conceptors are regularized identity maps derived from reservoir state correlation matrices (C = R(R + α^{-2}I)^{-1}), with proofs and formal properties (singular-value behavior, Boolean algebra-like laws, aperture adaptation). The paper develops connecting theory across linear algebra, dynamical systems, and formal logic (including a category-theoretical framing as 'Conceptor Logic as Institutions').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controlling Recurrent Neural Networks by Conceptors', 'publication_date_yy_mm': '2014-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e629.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e629.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-feature conceptors (RFC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Feature Conceptors (biologically plausible approximation of matrix conceptors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically motivated variant that approximates matrix conceptors by projecting reservoir states into a high-dimensional random feature space and applying scalar conception weights per feature neuron, enabling local learning and storage of conceptors as single-neuron connections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Random-feature conceptor system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The reservoir state x is projected by random matrix F into a feature space z = F' x (or similar), each feature neuron i is multiplied by a scalar conception weight c_i (local gating), and the feature activity is projected back into the reservoir by random matrix G. Conceptors are thus represented implicitly by the vector c and implemented with strictly local computations; learning/adaptation rules analogous to matrix-conceptor updates are applied to c_i scalars.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Conceptual operations (mixing, Boolean-like operations and aperture concepts) are carried over to the random-feature representation via elementwise operations and algebraic definitions on the conception weight vectors; these retain declarative semantics in an approximated form (prototype conceptors stored as weight vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Same reservoir-computational substrate (random recurrent network) and procedural online adaptation; additionally explicit random projection networks (F, G) and per-feature scalar adaptation rules (local synaptic updates).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Approximation-based integration: the declarative matrix-level conceptor algebra is projected into a randomized feature basis and implemented by local scalar gates c_i; the vector c is adapted from neural activity and then applied multiplicatively to feature activations before projection back, forming a closed-loop hybrid.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Preserves key emergent behaviors of full matrix conceptors (pattern selection, morphing, content-addressable recall, incremental memory) while enabling local/neural-plausible storage (single-neuron representation for a concept). Trade-offs emerge: similar pattern regeneration accuracy but increased sensitivity to aperture α and need for more features (2–5× reservoir size) for comparable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Re-runs of several conceptor demonstrations: pattern re-generation, content-addressable recall, hierarchical denoising/classification experiments (same tasks as matrix-conceptor versions).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported: pattern re-generation accuracy 'essentially the same' as matrix conceptors when using 2–5× more random features; aperture setting more sensitive (qualitative). No single numeric aggregate given beyond qualitative parity and increased sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Retains morphing/interpolation/extrapolation and the class-learning generalization effect when sufficient random features are used; can represent conceptors compactly enabling scalable incremental learning behaviors similar to matrix conceptors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Less direct geometric interpretability than full matrix conceptors (ellipsoids) but conceptor vectors c correspond to neurally stored prototypes; single neuron representation supports explicit addressing of patterns which aids interpretability at the network/neuron level.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires larger feature dimensionality (2–5× reservoir size) to match matrix-conceptor accuracy; more sensitive to aperture tuning; approximation may lose some exact algebraic properties; paper notes aperture sensitivity as a practical limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Constructed as an approximate, neurally-plausible implementation of the matrix conceptor algebra: random-projection theory + local scalar adaptation; theoretical correspondences and transfer of Boolean and aperture laws are claimed to carry over (with approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controlling Recurrent Neural Networks by Conceptors', 'publication_date_yy_mm': '2014-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e629.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e629.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoconceptor CAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoconceptor-based Content-Addressable Memory (conceptor CAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A content-addressable memory mechanism where conceptors are created and adapted online (autoconceptors) from short cues and used to drive a reservoir to accurately re-generate stored dynamical patterns without storing full matrix conceptors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autoconceptor content-addressable memory</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage recall: (1) brief cueing of target pattern drives online formation of a preliminary conceptor C_cue, enabling imperfect regeneration; (2) autonomous running with continuous online adaptation of the conceptor (autoconceptor update rule) converges to C_auto and accurate reconstruction. Autoconceptor adaptation tends to nullify small singular values, suppressing noise components and increasing robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>The declarative aspect is implicit in the conceptor representation (ellipsoid/regularized identity) and the logical operations used to combine/compare conceptors for memory management and recognition; conceptor singular-value structure provides interpretable decomposition of stored content.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Reservoir computing dynamics with online adaptation rules applied to the conceptor in the loop (procedural autoconceptor update), and readout-based pattern observation for reconstruction evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Online adaptive loop: conceptor is both derived from and constrains neural dynamics in real time. The declarative structure (conceptor) emerges from imperative dynamics and is then used imperatively to shape future dynamics—tight bidirectional coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables (a) robust cue-triggered recall of dynamical patterns even under strong reservoir noise (SNR=1 used in experiments), (b) progressive noise suppression via singular-value nulling during adaptation, and (c) class-learning effect where with many stored members the system forms a class-level representation enabling recall of novel family members.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Content-addressable recall experiments with a 200-neuron reservoir loaded with 5 two-sine irrational-period patterns (2-parameter family); memory load scaling experiments up to 100 patterns showing class-learning transition.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Autoconceptor recall: average log10 NRMSE improved from ≈ -0.4 immediately after cueing to ≈ -1.1 after autoadaptation (units: log10 normalized RMSE). Robust recall demonstrated under strong injected state noise (SNR = 1).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Class-learning emergent: when many patterns from a parametric family are stored, the system generalizes to novel members from the family almost as well as to the loaded instances; with few stored patterns the system performs rote memorization and generalization to novel samples is worse.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Conceptor singular-values and ellipsoid geometry provide interpretable diagnostics of which state-space directions are used (quota metric), and autoconceptor singular-value collapse explains noise-suppression behavior in interpretable linear-algebraic terms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance degrades with increasing memory load; there is a finite capacity and a measurable 'quota' metric indicating reservoir fullness; storing many diverse patterns eventually reduces accurate recall; autoconceptor adaptation requires sufficiently long autonomous adaptation periods (e.g., 10k timesteps used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Autoconceptor update dynamics analyzed mathematically (singular-value dynamics, convergence results) and linked to robustification via nulling of small singular directions; formal analyses provided in the paper (section on autoconceptor adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controlling Recurrent Neural Networks by Conceptors', 'publication_date_yy_mm': '2014-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e629.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e629.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hierarchical conceptor architecture</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical filtering and classification architecture using random-feature conceptors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilayer hybrid architecture that combines bottom-up procedural denoising with top-down symbolic-like guidance: higher-layer conceptors represent weighted OR combinations of stored prototypes and pass context downward while lower layers provide evidence upward and trust variables arbitrate top-down influence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hierarchical conceptor filtering & classification system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-layer system (demonstrated with three layers) where each layer is a random-feature conceptor system. Prototype conceptor vectors for known patterns are stored per layer. Bottom-up pathway progressively denoises an externally noisy input; top-down pathway passes conceptor constraints (weighted OR combinations of prototypes) to lower layers. Each layer maintains hypothesis weights γ^j(n) summing to 1 representing belief over prototypes; trust variables τ_[l,l+1](n) (range 0–1) modulate how much top-down conceptor guidance influences the lower layer. The top layer autoadapts a conceptor constrained to be a weighted OR of prototypes, closing the inference-and-generation loop.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Prototype conceptor vectors (stored declarative prototypes), algebraic OR-combination for hypothesis-based conceptor construction, explicit hypothesis vectors γ representing symbolic-like beliefs, and trust variables functioning as meta-declarative gating.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Layered random-feature reservoirs with procedural denoising dynamics, local conception-weight adaptation, and online inference/update procedures for γ and τ variables.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tightly coupled hierarchical modular integration: declarative prototype combinations (top-down) constrain procedural neural denoising (bottom-up); online adaptation of top-layer conceptor (autoadaptation) updates symbolic hypothesis weights which in turn modulate lower-layer procedural processing via multiplicative gating; trust variables mediate the relative influence of bottom-up evidence and top-down priors.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Simultaneous denoising and online classification: system dynamically converges to correct hypothesis and produces denoised reconstructions; robustness to strong input noise; interpretable belief dynamics (γ) and trust adaptation allow context-sensitive top-down correction; supports combination of positive and negative evidence through conceptor logic in classification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Simultaneous denoising and classification demonstration: noisy inputs drawn from one of four prototype patterns (previously used patterns), evaluate denoised outputs y_[l] and hypothesis convergence γ over time.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported qualitatively and via plots: multi-layer denoising reduces NRMSE progressively across layers (plots show log10 NRMSE decreasing from layer 1 to layer 3); precise numeric aggregate is not reported in the excerpt. Performance includes successful hypothesis convergence and improved denoising compared to a linear baseline (plotted).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>The hierarchical scheme leverages top-down prototype guidance to improve denoising and classification under noise; generalization to unseen prototypes not quantified in excerpt but the architecture can use OR-weighted combinations to represent interpolated/extrapolated pattern hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: hypothesis vectors γ provide explicit, time-varying interpretable beliefs about which prototype generates the input; trust variables τ explicitly encode confidence in higher-layer representations; conceptor prototypes are interpretable geometric objects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Quantitative sensitivity to aperture and prototype selection in RFC layers noted elsewhere applies here; exact numeric performance depends on aperture tuning and reservoir randomness; no head-to-head comparison with alternative hierarchical denoising-classification systems provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Operational combination of RFC approximations, conceptor Boolean algebra and online adaptation rules; rationale and formal properties of conceptor operations underpin the architecture, but no single formal end-to-end learning-theoretic proof for hierarchical performance is claimed in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Controlling Recurrent Neural Networks by Conceptors', 'publication_date_yy_mm': '2014-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The 'echo state' approach to analysing and training recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Neural networks and physical systems with emergent collective computational abilities <em>(Rating: 2)</em></li>
                <li>Institutions: Abstract model theory for specification and programming <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-629",
    "paper_id": "paper-eb2b3d8b76355700a0cd2bdbb347a5b1ecd362ab",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Conceptor framework",
            "name_full": "Conceptor-based neurodynamical system (Conceptor framework)",
            "brief_description": "A hybrid neuro-symbolic mechanism that derives matrix 'conceptors' from recurrent neural network state correlations to mediate symbolic-like operations (Boolean, abstraction) over neural dynamics, enabling selection, morphing, and logical combination of dynamical patterns within a single RNN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Conceptor-based hybrid neuro-symbolic system",
            "system_description": "An architecture combining a reservoir (randomly connected recurrent neural network) with conceptor matrices C that are computed from the reservoir state correlation R as C = R(R + α^{-2} I)^{-1}. Conceptor matrices act as dynamical filters inside the feedback loop (x(n+1) = C tanh(W x(n))) to constrain reservoir trajectories to pattern-specific state-space ellipsoids. Conceptor matrices admit linear combinations and Boolean-like operations (OR, AND, NOT), an abstraction ordering, and aperture adaptation; they are used to (i) select/re-generate specific learned dynamical patterns from a loaded reservoir, (ii) morph/interpolate/extrapolate between patterns, (iii) implement incremental memory allocation and prevent catastrophic forgetting, and (iv) support concept-level symbolic operations on dynamical representations.",
            "declarative_component": "Algebraic and logical structures over conceptor matrices (matrix-valued 'concepts') implementing symbolic/declarative operations: Boolean-like operators (C1 ∨ C2, C1 ∧ C2, ¬C), abstraction ordering (C1 ≤ C2 defined by existence of B s.t. C2 = C1 ∨ B), and a formal conceptor logic (sections on 'Conceptor Logic' and 'Conceptor Logic as Institutions') that gives semantical interpretation to these operators.",
            "imperative_component": "Imperative neural substrate is a reservoir computing RNN (random recurrent weight matrix W*, tanh activations, optionally trained internalized W) together with standard readout training; also variants: random-feature reservoir decomposition (F, G projections, feature neurons z and scalar conception weights c_i) and online adaptation rules (autoconceptor adaptation) implemented procedurally.",
            "integration_method": "Modular, closed-loop integration: declarative conceptors are computed from neural (imperative) state statistics and then re-injected multiplicatively into the neural update loop as filters (C × neural activation). Symbolic operations (Boolean, linear mixtures) are defined algebraically on conceptor matrices and then applied as neural filters; top-down/bottom-up interaction in hierarchical designs passes conceptor-derived contextual constraints downwards while neural layers perform procedural denoising and feature extraction. Integration also includes online adaptation (autoconceptors) where conceptors self-adapt from neural activity.",
            "emergent_properties": "Emergent capabilities include (1) content-addressable dynamical memory (cue-based re-creation of stored patterns via autoconceptor adaptation), (2) morphing/interpolation and extrapolation of dynamical patterns from few prototypes, (3) incremental life-long learning with explicit memory quota tracking and avoidance of catastrophic forgetting, (4) robust noise suppression via singular-value nulling during autoconceptor adaptation, (5) compositional symbolic-like reasoning over dynamical patterns (Boolean combinations, abstraction), and (6) class-learning: with many examples the system generalizes to the parametric class producing similar recall accuracy for novel unseen members.",
            "task_or_benchmark": "Demonstrations across tasks: (a) pattern storage and re-generation (periodic, chaotic attractors), (b) morphing / generalization of dynamical motor-like patterns, (c) Japanese vowels speaker recognition benchmark (9-way speaker ID), (d) incremental pattern loading and memory management experiments, (e) content-addressable memory recall experiments, (f) hierarchical denoising + classification tasks.",
            "hybrid_performance": "Japanese vowels: average test misclassifications = 3.4 (averaged over 50 random reservoirs); re-generation example MSEs for four simple loaded patterns: 3.3e-05, 1.4e-05, 0.0040, 0.0019 (mean square errors); content-addressable auto-adaptation: log10 NRMSE improved from ≈ -0.4 after cueing to ≈ -1.1 after autoadaptation. (Units: misclassification counts; errors: normalized root mean square error, reported in log10 for some experiments.)",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Shows strong within-family generalization (class-learning): when many patterns from a parametric family are loaded, recall accuracy for novel unseen family members approaches that for loaded members; supports interpolation and extrapolation by linear combination of conceptors (mixtures with coefficients summing to 1 for interpolation; negative coefficients allow extrapolation). Also demonstrates robustness to strong reservoir state noise after autoconceptor adaptation (SNR = 1 experiments).",
            "interpretability_properties": "High interpretability from the declarative side: conceptor matrices have geometric semantics (ellipsoids in state space) and admit Boolean- and abstraction-operators with formal semantics; single neurons (in random-feature variant) can encode individual conceptors, enabling explicit pattern-addressing; hypothesis vectors (γ) in hierarchical model provide an explicit, interpretable belief distribution over prototypes.",
            "limitations_or_failures": "Stated limitations include: conceptor matrix computations are non-local and biologically implausible in raw form; aperture parameter α must be chosen/adapted (though attenuation-based automatic choice is proposed); random-feature approximations increase aperture sensitivity; memory capacity is finite and a reservoir can become 'full' (quota ≈ 0.99) and then further loading fails; no direct comparison to purely symbolic reasoning systems is provided; some theoretical propositions were revised in later revisions (author notes).",
            "theoretical_framework": "A mathematical theory: conceptors are regularized identity maps derived from reservoir state correlation matrices (C = R(R + α^{-2}I)^{-1}), with proofs and formal properties (singular-value behavior, Boolean algebra-like laws, aperture adaptation). The paper develops connecting theory across linear algebra, dynamical systems, and formal logic (including a category-theoretical framing as 'Conceptor Logic as Institutions').",
            "uuid": "e629.0",
            "source_info": {
                "paper_title": "Controlling Recurrent Neural Networks by Conceptors",
                "publication_date_yy_mm": "2014-03"
            }
        },
        {
            "name_short": "Random-feature conceptors (RFC)",
            "name_full": "Random Feature Conceptors (biologically plausible approximation of matrix conceptors)",
            "brief_description": "A biologically motivated variant that approximates matrix conceptors by projecting reservoir states into a high-dimensional random feature space and applying scalar conception weights per feature neuron, enabling local learning and storage of conceptors as single-neuron connections.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Random-feature conceptor system",
            "system_description": "The reservoir state x is projected by random matrix F into a feature space z = F' x (or similar), each feature neuron i is multiplied by a scalar conception weight c_i (local gating), and the feature activity is projected back into the reservoir by random matrix G. Conceptors are thus represented implicitly by the vector c and implemented with strictly local computations; learning/adaptation rules analogous to matrix-conceptor updates are applied to c_i scalars.",
            "declarative_component": "Conceptual operations (mixing, Boolean-like operations and aperture concepts) are carried over to the random-feature representation via elementwise operations and algebraic definitions on the conception weight vectors; these retain declarative semantics in an approximated form (prototype conceptors stored as weight vectors).",
            "imperative_component": "Same reservoir-computational substrate (random recurrent network) and procedural online adaptation; additionally explicit random projection networks (F, G) and per-feature scalar adaptation rules (local synaptic updates).",
            "integration_method": "Approximation-based integration: the declarative matrix-level conceptor algebra is projected into a randomized feature basis and implemented by local scalar gates c_i; the vector c is adapted from neural activity and then applied multiplicatively to feature activations before projection back, forming a closed-loop hybrid.",
            "emergent_properties": "Preserves key emergent behaviors of full matrix conceptors (pattern selection, morphing, content-addressable recall, incremental memory) while enabling local/neural-plausible storage (single-neuron representation for a concept). Trade-offs emerge: similar pattern regeneration accuracy but increased sensitivity to aperture α and need for more features (2–5× reservoir size) for comparable performance.",
            "task_or_benchmark": "Re-runs of several conceptor demonstrations: pattern re-generation, content-addressable recall, hierarchical denoising/classification experiments (same tasks as matrix-conceptor versions).",
            "hybrid_performance": "Reported: pattern re-generation accuracy 'essentially the same' as matrix conceptors when using 2–5× more random features; aperture setting more sensitive (qualitative). No single numeric aggregate given beyond qualitative parity and increased sensitivity.",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Retains morphing/interpolation/extrapolation and the class-learning generalization effect when sufficient random features are used; can represent conceptors compactly enabling scalable incremental learning behaviors similar to matrix conceptors.",
            "interpretability_properties": "Less direct geometric interpretability than full matrix conceptors (ellipsoids) but conceptor vectors c correspond to neurally stored prototypes; single neuron representation supports explicit addressing of patterns which aids interpretability at the network/neuron level.",
            "limitations_or_failures": "Requires larger feature dimensionality (2–5× reservoir size) to match matrix-conceptor accuracy; more sensitive to aperture tuning; approximation may lose some exact algebraic properties; paper notes aperture sensitivity as a practical limitation.",
            "theoretical_framework": "Constructed as an approximate, neurally-plausible implementation of the matrix conceptor algebra: random-projection theory + local scalar adaptation; theoretical correspondences and transfer of Boolean and aperture laws are claimed to carry over (with approximations).",
            "uuid": "e629.1",
            "source_info": {
                "paper_title": "Controlling Recurrent Neural Networks by Conceptors",
                "publication_date_yy_mm": "2014-03"
            }
        },
        {
            "name_short": "Autoconceptor CAM",
            "name_full": "Autoconceptor-based Content-Addressable Memory (conceptor CAM)",
            "brief_description": "A content-addressable memory mechanism where conceptors are created and adapted online (autoconceptors) from short cues and used to drive a reservoir to accurately re-generate stored dynamical patterns without storing full matrix conceptors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Autoconceptor content-addressable memory",
            "system_description": "Two-stage recall: (1) brief cueing of target pattern drives online formation of a preliminary conceptor C_cue, enabling imperfect regeneration; (2) autonomous running with continuous online adaptation of the conceptor (autoconceptor update rule) converges to C_auto and accurate reconstruction. Autoconceptor adaptation tends to nullify small singular values, suppressing noise components and increasing robustness.",
            "declarative_component": "The declarative aspect is implicit in the conceptor representation (ellipsoid/regularized identity) and the logical operations used to combine/compare conceptors for memory management and recognition; conceptor singular-value structure provides interpretable decomposition of stored content.",
            "imperative_component": "Reservoir computing dynamics with online adaptation rules applied to the conceptor in the loop (procedural autoconceptor update), and readout-based pattern observation for reconstruction evaluation.",
            "integration_method": "Online adaptive loop: conceptor is both derived from and constrains neural dynamics in real time. The declarative structure (conceptor) emerges from imperative dynamics and is then used imperatively to shape future dynamics—tight bidirectional coupling.",
            "emergent_properties": "Enables (a) robust cue-triggered recall of dynamical patterns even under strong reservoir noise (SNR=1 used in experiments), (b) progressive noise suppression via singular-value nulling during adaptation, and (c) class-learning effect where with many stored members the system forms a class-level representation enabling recall of novel family members.",
            "task_or_benchmark": "Content-addressable recall experiments with a 200-neuron reservoir loaded with 5 two-sine irrational-period patterns (2-parameter family); memory load scaling experiments up to 100 patterns showing class-learning transition.",
            "hybrid_performance": "Autoconceptor recall: average log10 NRMSE improved from ≈ -0.4 immediately after cueing to ≈ -1.1 after autoadaptation (units: log10 normalized RMSE). Robust recall demonstrated under strong injected state noise (SNR = 1).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Class-learning emergent: when many patterns from a parametric family are stored, the system generalizes to novel members from the family almost as well as to the loaded instances; with few stored patterns the system performs rote memorization and generalization to novel samples is worse.",
            "interpretability_properties": "Conceptor singular-values and ellipsoid geometry provide interpretable diagnostics of which state-space directions are used (quota metric), and autoconceptor singular-value collapse explains noise-suppression behavior in interpretable linear-algebraic terms.",
            "limitations_or_failures": "Performance degrades with increasing memory load; there is a finite capacity and a measurable 'quota' metric indicating reservoir fullness; storing many diverse patterns eventually reduces accurate recall; autoconceptor adaptation requires sufficiently long autonomous adaptation periods (e.g., 10k timesteps used in experiments).",
            "theoretical_framework": "Autoconceptor update dynamics analyzed mathematically (singular-value dynamics, convergence results) and linked to robustification via nulling of small singular directions; formal analyses provided in the paper (section on autoconceptor adaptation).",
            "uuid": "e629.2",
            "source_info": {
                "paper_title": "Controlling Recurrent Neural Networks by Conceptors",
                "publication_date_yy_mm": "2014-03"
            }
        },
        {
            "name_short": "Hierarchical conceptor architecture",
            "name_full": "Hierarchical filtering and classification architecture using random-feature conceptors",
            "brief_description": "A multilayer hybrid architecture that combines bottom-up procedural denoising with top-down symbolic-like guidance: higher-layer conceptors represent weighted OR combinations of stored prototypes and pass context downward while lower layers provide evidence upward and trust variables arbitrate top-down influence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hierarchical conceptor filtering & classification system",
            "system_description": "Multi-layer system (demonstrated with three layers) where each layer is a random-feature conceptor system. Prototype conceptor vectors for known patterns are stored per layer. Bottom-up pathway progressively denoises an externally noisy input; top-down pathway passes conceptor constraints (weighted OR combinations of prototypes) to lower layers. Each layer maintains hypothesis weights γ^j(n) summing to 1 representing belief over prototypes; trust variables τ_[l,l+1](n) (range 0–1) modulate how much top-down conceptor guidance influences the lower layer. The top layer autoadapts a conceptor constrained to be a weighted OR of prototypes, closing the inference-and-generation loop.",
            "declarative_component": "Prototype conceptor vectors (stored declarative prototypes), algebraic OR-combination for hypothesis-based conceptor construction, explicit hypothesis vectors γ representing symbolic-like beliefs, and trust variables functioning as meta-declarative gating.",
            "imperative_component": "Layered random-feature reservoirs with procedural denoising dynamics, local conception-weight adaptation, and online inference/update procedures for γ and τ variables.",
            "integration_method": "Tightly coupled hierarchical modular integration: declarative prototype combinations (top-down) constrain procedural neural denoising (bottom-up); online adaptation of top-layer conceptor (autoadaptation) updates symbolic hypothesis weights which in turn modulate lower-layer procedural processing via multiplicative gating; trust variables mediate the relative influence of bottom-up evidence and top-down priors.",
            "emergent_properties": "Simultaneous denoising and online classification: system dynamically converges to correct hypothesis and produces denoised reconstructions; robustness to strong input noise; interpretable belief dynamics (γ) and trust adaptation allow context-sensitive top-down correction; supports combination of positive and negative evidence through conceptor logic in classification.",
            "task_or_benchmark": "Simultaneous denoising and classification demonstration: noisy inputs drawn from one of four prototype patterns (previously used patterns), evaluate denoised outputs y_[l] and hypothesis convergence γ over time.",
            "hybrid_performance": "Reported qualitatively and via plots: multi-layer denoising reduces NRMSE progressively across layers (plots show log10 NRMSE decreasing from layer 1 to layer 3); precise numeric aggregate is not reported in the excerpt. Performance includes successful hypothesis convergence and improved denoising compared to a linear baseline (plotted).",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "The hierarchical scheme leverages top-down prototype guidance to improve denoising and classification under noise; generalization to unseen prototypes not quantified in excerpt but the architecture can use OR-weighted combinations to represent interpolated/extrapolated pattern hypotheses.",
            "interpretability_properties": "High: hypothesis vectors γ provide explicit, time-varying interpretable beliefs about which prototype generates the input; trust variables τ explicitly encode confidence in higher-layer representations; conceptor prototypes are interpretable geometric objects.",
            "limitations_or_failures": "Quantitative sensitivity to aperture and prototype selection in RFC layers noted elsewhere applies here; exact numeric performance depends on aperture tuning and reservoir randomness; no head-to-head comparison with alternative hierarchical denoising-classification systems provided in the paper.",
            "theoretical_framework": "Operational combination of RFC approximations, conceptor Boolean algebra and online adaptation rules; rationale and formal properties of conceptor operations underpin the architecture, but no single formal end-to-end learning-theoretic proof for hierarchical performance is claimed in the excerpt.",
            "uuid": "e629.3",
            "source_info": {
                "paper_title": "Controlling Recurrent Neural Networks by Conceptors",
                "publication_date_yy_mm": "2014-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The 'echo state' approach to analysing and training recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Neural networks and physical systems with emergent collective computational abilities",
            "rating": 2
        },
        {
            "paper_title": "Institutions: Abstract model theory for specification and programming",
            "rating": 2
        }
    ],
    "cost": 0.01540625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Herbert Jaeger</p>
<h1>Controlling Recurrent Neural Networks by Conceptors</h1>
<p>Technical Report No. 31
March 2014
School of Engineering and Science</p>
<h1>Controlling Recurrent Neural Networks by Conceptors (Revision 4)</h1>
<p>Herbert Jaeger</p>
<p>Original affiliation: Jacobs University Bremen
School of Engineering and Science
Campus Ring, 28759 Bremen, Germany
Current affiliation (since 2019): University of Groningen
Department of AI and Groningen Cognitive Systems and Materials Center (CogniGron)
Nijenborgh 9, 9747 AG Groningen, Netherlands
email: h.jaeger@rug.nl
web: https://www.ai.rug.nl/minds</p>
<h4>Abstract</h4>
<p>The human brain is a dynamical system whose extremely complex sensordriven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.</p>
<p>Changelog for Revision 1: Besides correcting trivial typos, the following edits / corrections have been made after the first publication on arXiv, leading to the revision 1 version on arXiv:</p>
<ul>
<li>Apr 7, 2014: updated algorithm description for incremental memory management by documenting $W^{\text {out }}$ component.</li>
<li>Jun 15, 2014: added missing parentheses in $C$ update formula given at beginning of Section 3.14 .</li>
<li>Oct 11, 2014: added missing Frobenious norms in similarity formulas (3) and (10) (pointed out by Dennis Hamester)</li>
<li>May 2016: re-wrote and extended the section on incremental memory management (Section 3.11)</li>
<li>July 2016: deleted the subsubsection "To be or not to be an attractor" from Section 3.14.3 because this material is repeated later in the text in more detail.</li>
</ul>
<h1>Changelog for Revision 2:</h1>
<ul>
<li>April 22, 2017: Proposition 12 was removed - it was wrong.</li>
</ul>
<p>Changelog for Revision 4: November 17, 2024: There were a number of inconsistencies concerning the use of terminology 'input simulation weights' versus 'input recreation weights', where text wording was at odds with notation in formulas. This has been corrected. I was alerted to these problems by my Master student Otto Bervoets. Furthermore, two notes pointing out recent developments with regards to using conceptors for continual deep learning and a formal conceptor logic were added.</p>
<p>Notes on the Structure of this Report. This report introduces several novel analytical concepts describing neural dynamics; develops the corresponding mathematical theory under aspects of linear algebra, dynamical systems theory, and formal logic; introduces a number of novel learning, adaptation and control algorithms for recurrent neural networks; demonstrates these in a number of case studies; proposes biologically (not too im-)plausible realizations of the dynamical mechanisms; and discusses relationships to other work. Said shortly, it's long. Not all parts will be of interest to all readers. In order to facilitate navigation through the text and selection of relevant components, I start with an overview section which gives an intuitive explanation of the novel concepts and informal sketches of the main results and demonstrations (Section 1). After this overview, the material is presented in detail, starting with an introduction (Section 2) which relates this contribution to other research. The main part is Section 3, where I systematically develop the theory and algorithms, interspersed with simulation demos. A graphical dependency map for this section is given at the beginning of Section 3. The technical documentation of the computer simulations is provided in Section 4, and mathematical proofs are collected in Section 5. The detailed presentation in Sections $2-5$ is self-contained. Reading the overview in Section 1 may be helpful but is not necessary for reading these sections. For convenience some figures from the overview section are repeated in Section 3.</p>
<p>Acknowledgements. The work described in this report was partly funded through the European FP7 project AMARSi (www.amarsi-project.eu). The author is indebted to Dr. Mathieu Galtier and Dr. Manjunath Ghandi for careful proofreading (not an easy task).</p>
<h1>Contents</h1>
<p>1 Overview ..... 7
2 Introduction ..... 25
2.1 Motivation ..... 25
2.2 Mathematical Preliminaries ..... 27
3 Theory and Demonstrations ..... 29
3.1 Networks and Signals ..... 29
3.2 Driving a Reservoir with Different Patterns ..... 30
3.3 Storing Patterns in a Reservoir, and Training the Readout ..... 34
3.4 Conceptors: Introduction and Basic Usage in Retrieval ..... 34
3.5 A Similarity Measure for Excited Network Dynamics ..... 38
3.6 Online Learning of Conceptor Matrices ..... 38
3.7 Morphing Patterns ..... 39
3.8 Understanding Aperture ..... 43
3.8.1 The Semantics of $\alpha$ as "Aperture" ..... 43
3.8.2 Aperture Adaptation and Final Definition of Conceptor Ma- trices ..... 44
3.8.3 Aperture Adaptation: Example ..... 46
3.8.4 Guides for Aperture Adjustment ..... 46
3.9 Boolean Operations on Conceptors ..... 50
3.9.1 Motivation ..... 50
3.9.2 Preliminary Definition of Boolean Operations ..... 51
3.9.3 Final Definition of Boolean Operations ..... 53
3.9.4 Facts Concerning Subspaces ..... 55
3.9.5 Boolean Operators and Aperture Adaptation ..... 56
3.9.6 Logic Laws ..... 56
3.10 An Abstraction Relationship between Conceptors ..... 58
3.11 Example: Memory Management in RNNs ..... 59
3.11.1 General Principle and Algorithm ..... 60
3.11.2 Demonstration 1: Incremental Loading of Integer-Periodic Patterns ..... 63
3.12 Incremental Loading of Integer-Periodic Patterns: Detailed Analysis ..... 65
3.12.1 Demonstration 2: Incremental Loading of Irrational-Period Patterns ..... 67
3.12.2 Integer-Periodic Versus Parametrized Patterns: Close-up In- spection ..... 69
3.12.3 Incremental Loading of Arbitrary Patterns ..... 72
3.13 Example: Dynamical Pattern Recognition ..... 74
3.14 Autoconceptors ..... 80
3.14.1 Motivation and Overview ..... 80
3.14.2 Basic Equations ..... 82</p>
<p>3.14.3 Example: Autoconceptive Reservoirs as Content-Addressable Memories ..... 84
3.14.4 Analysis of Autoconceptor Adaptation Dynamics ..... 93
3.15 Toward Biologically Plausible Neural Circuits: Random Feature Conceptors ..... 104
3.16 A Hierarchical Filtering and Classification Architecture ..... 121
3.17 Toward a Formal Marriage of Dynamics with Logic ..... 137
3.18 Conceptor Logic as Institutions: Category-Theoretical Detail ..... 148
3.19 Final Summary and Outlook ..... 158
4 Documentation of Experiments and Methods ..... 161
4.1 General Set-Up, Initial Demonstrations (Section 1 and Section 3.2 - 3.4) ..... 161
4.2 Aperture Adaptation (Sections 3.8.3 and 3.8.4) ..... 162
4.3 Memory Management, Demo 1 (Section 3.11.2) ..... 164
4.4 Memory Management, Demo 2 (Section 3.12.1) ..... 165
4.5 Memory Management, Close-Up Inspection (Section 3.12.2) ..... 165
4.6 Memory Management, Arbitrary Patterns (Section 3.12.3) ..... 165
4.7 Content-Addressable Memory (Section 3.14.3) ..... 166
4.8 The Japanese Vowels Classification (Section 3.13) ..... 166
4.9 Conceptor Dynamics Based on RFC Conceptors (Section 3.15) ..... 167
4.10 Hierarchical Classification and Filtering Architecture (Section 3.16) ..... 168
5 Proofs and Algorithms ..... 171
5.1 Proof of Proposition 1 (Section 3.4) ..... 171
5.2 Proof of Proposition 6 (Section 3.9.3) ..... 172
5.3 Proof of Proposition 7 (Section 3.9.3) ..... 175
5.4 Proof of Proposition 8 (Section 3.9.3) ..... 175
5.5 Proof of Proposition 9 (Section 3.9.4) ..... 176
5.6 Proof of Proposition 10 (Section 3.9.5) ..... 178
5.7 Proof of Proposition 11 (Section 3.9.6) ..... 181
5.8 Proof of Proposition 12 (Section 3.9.6) ..... 181
5.9 Proof of Proposition 13 (Section 3.10) ..... 182
5.10 Proof of Proposition 15 (Section 3.14.4) ..... 186
5.11 Proof of Proposition 17 (Section 3.18) ..... 191
References ..... 192</p>
<h1>1 Overview</h1>
<p>Scientific context. Research on brains and cognition unfolds in two directions. Top-down oriented research starts from the "higher" levels of cognitive performance, like rational reasoning, conceptual knowledge representation, command of language. These phenomena are typically described in symbolic formalisms developed in mathematical logic, artificial intelligence (AI), computer science and linguistics. In the bottom-up direction, one departs from "low-level" sensor data processing and motor control, using the analytical tools offered by dynamical systems theory, signal processing and control theory, statistics and information theory. The human brain obviously has found a way to implement high-level logical reasoning on the basis of low-level neuro-dynamical processes. How this is possible, and how the top-down and bottom-up research directions can be united, has largely remained an open question despite long-standing efforts in neural networks research and computational neuroscience [85, 92, 33, 1, 36, 43], machine learning [35, 49], robotics [10, 86], artificial intelligence [88, 109, 7, 9], dynamical systems modeling of cognitive processes [99, 103, 110], cognitive science and linguistics $[22,101]$, or cognitive neuroscience $[4,26]$.</p>
<p>Summary of contribution. Here I establish a fresh view on the neuro-symbolic integration problem. I show how dynamical neural activation patterns can be characterized by certain neural filters which I call conceptors. Conceptors derive naturally from the following key observation. When a recurrent neural network (RNN) is actively generating, or is passively being driven by different dynamical patterns (say $a, b, c, \ldots$ ), its neural states populate different regions $R_{a}, R_{b}, R_{c}, \ldots$ of neural state space. These regions are characteristic of the respective patterns. For these regions, neural filters $C_{a}, C_{b}, C_{c}, \ldots$ (the conceptors) can be incrementally learnt. A conceptor $C_{x}$ representing a pattern $x$ can then be invoked after learning to constrain the neural dynamics to the state region $R_{x}$, and the network will select and re-generate pattern $x$. Learnt conceptors can be blended, combined by Boolean operations, specialized or abstracted in various ways, yielding novel patterns on the fly. The logical operations on conceptors admit a rigorous semantical interpretation; conceptors can be arranged in conceptual hierarchies which are structured like semantic networks known from artificial intelligence. Conceptors can be economically implemented by single neurons (addressing patterns by neurons, leading to explicit command over pattern generation), or they may self-organize spontaneously and quickly upon the presentation of cue patterns (content-addressing, leading to pattern imitation). Conceptors can also be employed to "allocate free memory space" when new patterns are learnt and stored in long-term memory, enabling incremental life-long learning without the danger of freshly learnt patterns disrupting already acquired ones. Conceptors are robust against neural noise and parameter variations. The basic mechanisms are generic and can be realized in any kind of dynamical neural network. All taken together,</p>
<p>conceptors offer a principled, transparent, and computationally efficient account of how neural dynamics can self-organize in conceptual structures.</p>
<p>Going bottom-up: from neural dynamics to conceptors. The neural model system in this report are standard recurrent neural networks (RNNs, Figure $1 \mathbf{A})$ whose dynamics is mathematically described be the state update equations</p>
<p>$$
\begin{aligned}
x(n+1) &amp; =\tanh \left(W^{*} x(n)+W^{\text {in }} p(n)\right) \
y(n) &amp; =W^{\text {out }} x(n)
\end{aligned}
$$</p>
<p>Time here progresses in unit steps $n=1,2, \ldots$ The network consists of $N$ neurons (typically in the order of a hundred in this report), whose activations $x_{1}(n), \ldots, x_{N}(n)$ at time $n$ are collected in an $N$-dimensional state vector $x(n)$. The neurons are linked by random synaptic connections, whose strengths are collected in a weight matrix $W^{*}$ of size $N \times N$. An input signal $p(n)$ is fed to the network through synaptic input connections assembled in the input weight matrix $W^{\text {in }}$. The "S-shaped" function tanh squashes the neuronal activation values into a range between -1 and 1 . The second equation specifies that an ouput signal $y(n)$ can be read from the network activation state $x(n)$ by means of output weights $W^{\text {out }}$. These weights are pre-computed such that the output signal $y(n)$ just repeats the input signal $p(n)$. The output signal plays no functional role in what follows; it merely serves as a convenient 1-dimensional observer of the high-dimensional network dynamics.</p>
<p>The network-internal neuron-to-neuron connections $W^{*}$ are created at random. This will lead to the existence of cyclic ("recurrent") connection pathways inside the network. Neural activation can reverberate inside the network along these cyclic pathways. The network therefore can autonomously generate complex neurodynamical patterns even when it receives no input. Following the terminology of the reservoir computing $[58,5]$, I refer to such randomly connected neural networks as reservoirs.</p>
<p>For the sake of introducing conceptors by way of an example, consider a reservoir with $N=100$ neurons. I drive this system with a simple sinewave input $p(n)$ (first panel in first row in Fig. 1 B). The reservoir becomes entrained to this input, each neuron showing individual variations thereof (Fig. 1 B second panel). The resulting reservoir state sequence $x(1), x(2), \ldots$ can be represented as a cloud of points in the 100-dimensional reservoir state space. The dots in the first panel of Fig. 1 C show a 2-dimensional projection of this point cloud. By a statistical method known as principal component analysis, the shape of this point cloud can be captured by an $N$-dimensional ellipsoid whose main axes point in the main scattering directions of the point cloud. This ellipsoid is a geometrical representation of the correlation matrix $R$ of the state points. The lengths $\sigma_{1}, \ldots, \sigma_{N}$ of the ellipsoid axes are known as the singular values of $R$. The directions and lengths of these axes provide a succinct characterization of the geometry of the state point</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Deriving conceptors from network dynamics. A. Network layout. Arrows indicate synaptic links. B. Driving the reservoir with four different input patterns. Left panels: 20 timesteps of input pattern $p(n)$ (black thin line) and conceptor-controlled output $y(n)$ (bold light gray). Second column: 20 timesteps of traces $x_{i}(n), x_{j}(n)$ of two randomly picked reservoir neurons. Third column: the singular values $\sigma_{i}$ of the reservoir state correlation matrix $R$ in logarithmic scale. Last column: the singular values $s_{i}$ of the conceptors $C$ in linear plotting scale. C. From pattern to conceptor. Left: plots of value pairs $x_{i}(n), x_{j}(n)$ (dots) of the two neurons shown in first row of $\mathbf{B}$ and the resulting ellipse with axis lengths $\sigma_{1}, \sigma_{2}$. Right: from $R$ (thin light gray) to conceptor $C$ (bold dark gray) by normalizing axis lengths $\sigma_{1}, \sigma_{2}$ to $s_{1}, s_{2}$.</p>
<p>cloud. The $N=100$ lengths $\sigma_{i}$ resulting in this example are log-plotted in Fig. 1 B, third column, revealing an exponential fall-off in this case.</p>
<p>As a next step, these lengths $\sigma_{i}$ are normalized to become $s_{i}=\sigma_{i} /\left(\sigma_{i}+\alpha^{-2}\right)$, where $\alpha \geq 0$ is a design parameter that I call aperture. This normalization ensures that all $s_{i}$ are not larger than 1 (last column in Fig. 1 B). A new ellipsoid is obtained (Fig. 1 C right) which is located inside the unit sphere. The normalized ellipsoid can be described by a $N$-dimensional matrix $C$, which I call a conceptor matrix. $C$ can be directly expressed in terms of $R$ by $C=R\left(R+\alpha^{-2} I\right)^{-1}$, where $I$ is the identity matrix.</p>
<p>When a different driving pattern $p$ is used, the shape of the state point cloud, and subsequently the conceptor matrix $C$, will be characteristically different. In the example, I drove the reservoir with four patterns $p^{1}-p^{4}$ (rows in Fig. 1B). The first two patterns were sines of slightly different frequencies, the last two patterns were minor variations of a 5-periodic random pattern. The conceptors derived from the two sine patterns differ considerably from the conceptors induced by the two 5-periodic patterns (last column in Fig. 1B). Within each of these two pairs, the conceptor differences are too small to become visible in the plots.</p>
<p>There is an instructive alternative way to define conceptors. Given a sequence of reservoir states $x(1), \ldots, x(L)$, the conceptor $C$ which characterizes this state point cloud is the unique matrix which minimizes the cost function $\sum_{n=1, \ldots, L}|x(n)-C x(n)|^{2} / L+\alpha^{-2}|C|^{2}$, where $|C|^{2}$ is the sum of all squared matrix entries. The first term in this cost would become minimal if $C$ were the identity map, the second term would become minimal if $C$ would be the all-zero map. The aperture $\alpha$ strikes a balance between these two competing cost components. For increasing apertures, $C$ will tend toward the identity matrix $I$; for shrinking apertures it will come out closer to the zero matrix. In the terminology of machine learning, $C$ is hereby defined as a regularized identity map. The explicit solution to this minimization problem is again given by the formula $C=R\left(R+\alpha^{-2} I\right)^{-1}$.</p>
<p>Summing up: if a reservoir is driven by a pattern $p(n)$, a conceptor matrix $C$ can be obtained from the driven reservoir states $x(n)$ as the regularized identity map on these states. $C$ can be likewise seen as a normalized ellipsoid characterization of the shape of the $x(n)$ point cloud. I write $C(p, \alpha)$ to denote a conceptor derived from a pattern $p$ using aperture $\alpha$, or $C(R, \alpha)$ to denote that $C$ was obtained from a state correlation matrix $R$.</p>
<p>Loading a reservoir. With the aid of conceptors a reservoir can re-generate a number of different patterns $p^{1}, \ldots, p^{K}$ that it has previously been driven with. For this to work, these patterns have to be learnt by the reservoir in a special sense, which I call loading a reservoir with patterns. The loading procedure works as follows. First, drive the reservoir with the patterns $p^{1}, \ldots, p^{K}$ in turn, collecting reservoir states $x^{j}(n)$ (where $j=1, \ldots, K)$. Then, recompute the reservoir connection weights $W^{*}$ into $W$ such that $W$ optimally balances between the following</p>
<p>two goals. First, $W$ should be such that $W x^{j}(n) \approx W^{*} x^{j}(n)+W^{\text {in }} p^{j}(n)$ for all times $n$ and patterns $j$. That is, $W$ should allow the reservoir to "internalize" the driving input in the absence of the same. I call such $W$ an input internalizing matrix. Second, $W$ should be such that the weights collected in this matrix become as small as possible. Technically this compromise-seeking learning task amounts to computing what is known as a regularized linear regression, a standard and simple computational task. This idea of "internalizing" a driven dynamics into a reservoir has been independently (re-)introduced under different names and for a variety of purposes (self-prediction [74], equilibration [57], reservoir regularization [95], self-sensing networks [105], innate training [63]) and appears to be a fundamental RNN adaptation principle.</p>
<p>Going top-down: from conceptors to neural dynamics. Assume that conceptors $C^{j}=C\left(p^{j}, \alpha\right)$ have been derived for patterns $p^{1}, \ldots, p^{K}$, and that these patterns have been loaded into the reservoir, replacing the original random weights $W^{<em>}$ by $W$. Intuitively, the loaded reservoir, when it is run using $x(n+1)=$ $\tanh (W x(n))$ (no input!) should behave exactly as when it was driven with input earlier, because $W$ has been trained such that $W x(n) \approx W^{</em>} x(n)+W^{\text {in }} p^{j}(n)$. In fact, if only a single pattern had been loaded, the loaded reservoir would readily re-generate it. But if more than one patter had been loaded, the autonomous (input-free) update $x(n+1)=\tanh (W x(n))$ will lead to an entirely unpredictable dynamics: the network can't "decide" which of the loaded patterns it should re-generate! This is where conceptors come in. The reservoir dynamics is filtered through $C^{j}$. This is effected by using the augmented update rule $x(n+1)=C^{j} \tanh (W x(n))$. By virtue of inserting $C^{j}$ into the feedback loop, the reservoir states become clipped to fall within the ellipsoid associated with $C^{j}$. As a result, the pattern $p^{j}$ will be re-generated: when the reservoir is observed through the previously trained output weights, one gets $y(n)=W^{\text {out }} x(n) \approx p^{j}(n)$. The first column of panels in Fig. 1 B shows an overlay of the four autonomously re-generated patterns $y(n)$ with the original drivers $p^{j}$ used in that example. The recovery of the originals is quite accurate (mean square errors $3.3 \mathrm{e}-05,1.4 \mathrm{e}-05$, $0.0040,0.0019$ for the four loaded patterns). Note that the first two and the last two patterns are rather similar to each other. The filtering afforded by the respective conceptors is "sharp" enough to separate these twin pairs. I will later demonstrate that in this way a remarkably large number of patterns can be faithfully re-generated by a single reservoir.</p>
<p>Morphing and generalization. Given a reservoir loaded with $K$ patterns $p^{j}$, the associated conceptors $C^{j}$ can be linearly combined by creating mixture conceptors $M=\mu^{1} C^{1}+\ldots+\mu^{K} C^{K}$, where the mixing coefficients $\mu^{j}$ must sum to 1 . When the reservoir is run under the control of such a morphed conceptor $M$, the resulting generated pattern is a morph between the original "pure" patterns $p^{j}$. If all $\mu^{j}$ are non-negative, the morph can be considered an interpolation between the</p>
<p>pure patterns; if some $\mu^{j}$ are negative, the morph extrapolates beyond the loaded pure patterns. I demonstrate this with the four patterns used in the example above, setting $\mu^{1}=(1-a) b, \mu^{2}=a b, \mu^{3}=(1-a)(1-b), \mu^{4}=a(1-b)$, and letting $a, b$ vary from -0.5 to 1.5 in increments of 0.25 . Fig. 2 shows plots of observer signals $y(n)$ obtained when the reservoir is generating patterns under the control of these morphed conceptors. The innermost 5 by 5 panels show interpolations between the four pure patterns, all other panels show extrapolations.</p>
<p>In machine learning terms, both interpolation and extrapolation are cases of generalization. A standard opinion in the field states that generalization by interpolation is what one may expect from learning algorithms, while extrapolation beyond the training data is hard to achieve.</p>
<p>Morphing and generalizing dynamical patterns is a common but nontrivial task for training motor patterns in robots. It typically requires training demonstrations of numerous interpolating patterns [93, 16, 70]. Conceptor-based pattern morphing appears promising for flexible robot motor pattern learning from a very small number of demonstrations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Morphing between, and generalizing beyond, four loaded patterns. Each panel shows a 15 -step autonomously generated pattern (plot range between -1 and +1 ). Panels with bold frames: the four loaded prototype patterns (same patterns as in Fig. 1 B.)</p>
<p>Aperture adaptation. Choosing the aperture $\alpha$ appropriately is crucial for regenerating patterns in a stable and accurate way. To demonstrate this, I loaded a 500 -neuron reservoir with signals $p^{1}-p^{4}$ derived from four classical chaotic attractors: the Lorenz, Rössler, Mackey-Glass, and Hénon attractors. Note that it used to be a challenging task to make an RNN learn any single of these attractors [58]; to my knowledge, training a single RNN to generate several different chaotic attractors has not been attempted before. After loading the reservoir, the regeneration was tested using conceptors $C\left(p^{j}, \alpha\right)$ where for each attractor pattern $p^{j}$ a number of different values for $\alpha$ were tried. Fig. 3 A shows the resulting regenerated patterns for five apertures for the Lorenz attractor. When the aperture is too small, the reservoir-conceptor feedback loop becomes too constrained and the produced patterns de-differentiate. When the aperture is too large, the feedback loop becomes over-excited.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Aperture adaptation for re-generating four chaotic attractors. A Lorenz attractor. Five versions re-generated with different apertures (values inserted in panels) and original attractor (green). B Best re-generations of the other three attractors (from left to right: Rössler, Mackey-Glass, and Hénon, originals in green). C Log10 of the attenuation criterion plotted against the $\log 10$ of aperture. Dots mark the apertures used for plots in $\mathbf{A}$ and $\mathbf{B}$.</p>
<p>An optimal aperture can be found by experimentation, but this will not be an option in many engineering applications or in biological neural systems. An intrinsic criterion for optimizing $\alpha$ is afforded by a quantity that I call attenuation: the damping ratio which the conceptor imposes on the reservoir signal. Fig. 3 C plots the attenuation against the aperture for the four chaotic signals. The</p>
<p>minimum of this curve marks a good aperture value: when the conceptor dampens out a minimal fraction of the reservoir signal, conceptor and reservoir are in good "resonance". The chaotic attractor re-generations shown in Fig. 3 B were obtained by using this minimum-attenuation criterion.</p>
<p>The aperture range which yields visibly good attractor re-generations in this demonstration spans about one order of magnitude. With further refinements (zeroing small singular values in conceptors is particularly effective), the viable aperture range can be expanded to about three orders of magnitude. While setting the aperture right is generally important, fine-tuning is unnecessary.</p>
<p>Boolean operations and conceptor abstraction. Assume that a reservoir is driven by a pattern $r$ which consists of randomly alternating epochs of two patterns $p$ and $q$. If one doesn't know which of the two patterns is active at a given time, all one can say is that the pattern $r$ currently is $p$ OR it is $q$. Let $C\left(R_{p}, 1\right), C\left(R_{q}, 1\right), C\left(R_{r}, 1\right)$ be conceptors derived from the two partial patterns $p, q$ and the "OR" pattern $r$, respectively. Then it holds that $C\left(R_{r}, 1\right)=C\left(\left(R_{p}+\right.\right.$ $\left.\left.R_{q}\right) / 2,1\right)$. Dropping the division by 2 , this motivates to define an OR (mathematical notation: $\vee$ ) operation on conceptors $C_{1}\left(R_{1}, 1\right), C_{2}\left(R_{2}, 1\right)$ by putting $C_{1} \vee C_{2}:=\left(R_{1}+R_{2}\right)\left(R_{1}+R_{2}+I\right)^{-1}$. The logical operations NOT $(\neg)$ and AND $(\wedge)$ can be defined along similar lines. Fig. 4 shows two-dimensional examples of applying the three operations.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Boolean operations on conceptors. Red/blue (thin) ellipses represent source conceptors $C_{1}, C_{2}$. Magenta (thick) ellipses show $C_{1} \vee C_{2}, C_{1} \wedge C_{2}, \neg C_{1}$ (from left to right).</p>
<p>Boolean logic is the mathematical theory of $\vee, \wedge, \neg$. Many laws of Boolean logic also hold for the $\vee, \wedge, \neg$ operations on conceptors: the laws of associativity, commutativity, double negation, de Morgan's rules, some absorption rules. Furthermore, numerous simple laws connect aperture adaptation to Boolean operations. Last but not least, by defining $C_{1} \leq C_{2}$ if and only if there exists a conceptor $B$ such that $C_{2}=C_{1} \vee B$, an abstraction ordering is created on the set of all conceptors of dimension $N$.</p>
<p>Neural memory management. Boolean conceptor operations afford unprecedented flexibility of organizing and controlling the nonlinear dynamics of recurrent neural networks. Here I demonstrate how a sequence of patterns $p^{1}, p^{2}, \ldots$ can be incrementally loaded into a reservoir, such that (i) loading a new pattern $p^{j+1}$ does not interfere with previously loaded $p^{1}, \ldots, p^{j}$; (ii) if a new pattern $p^{j+1}$ is similar to already loaded ones, the redundancies are automatically detected and exploited, saving memory capacity; (iii) the amount of still "free" memory space can be logged.</p>
<p>Let $C^{j}$ be the conceptor associated with pattern $p^{j}$. Three ideas are combined to implement the memory management scheme. First, keep track of the "already used" memory space by maintaining a conceptor $A^{j}=C^{1} \vee \ldots \vee C^{j}$. The sum of all singular values of $A^{j}$, divided by the reservoir size, gives a number that ranges between 0 and 1 . It is an indicator of the portion of reservoir "space" which has been used up by loading $C^{1}, \ldots, C^{j}$, and I call it the quota claimed by $C^{1}, \ldots, C^{j}$. Second, characterize what is "new" about $C^{j+1}$ (not being already represented by previously loaded patterns) by considering the conceptor $N^{j+1}=C^{j+1} \backslash A^{j}$. The logical difference operator $\backslash$ can be re-written as $A \backslash B=A \wedge \neg B$. Third, load only that which is new about $C^{j+1}$ into the still unclaimed reservoir space, that is, into $\neg A^{j}$. These three ideas can be straightforwardly turned into a modification of the basic pattern loading algorithm.</p>
<p>For a demonstration, I created a series of periodic patterns $p^{1}, p^{2}, \ldots$ whose integer period lengths were picked randomly between 3 and 15 , some of these patterns being sines, others random patterns. These patterns were incrementally loaded in a 100 -neuron reservoir, one by one. Fig. 5 shows the result. Since patterns $j=5,6,7$ were identical replicas of patterns $j=1,2,3$, no additional quota space was consumed when these patterns were (re-)loaded. Accuracy was measured by the normalized root mean square error (NRMSE). The NRMSE jumps from very small values to a high value when the last pattern is loaded; the quota of 0.99 at this point indicates that the reservoir is "full". The re-generation testing and NRMSE computation was done after all patterns had been loaded. An attempt to load further patterns would be unsuccessful, but it also would not harm the re-generation quality of the already loaded ones.</p>
<p>This ability to load patterns incrementally suggests a solution to a notorious problem in neural network training, known as catastrophic forgetting, which manifests itself in a disruption of previously learnt functionality when learning new functionality. Although a number of proposals have been made which partially alleviate the problem in special circumstances [32, 42], catastrophic forgetting was still listed as an open challenge in an expert's report solicited by the NSF in 2007 [21] which collected the main future challenges in learning theory.</p>
<p>Recognizing dynamical patterns. Boolean conceptor operations enable the combination of positive and negative evidence in a neural architecture for dynamical pattern recognition. For a demonstration I use a common benchmark, the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Incremental pattern storing in a neural memory. Each panel shows a 20 -timestep sample of the correct training pattern $p^{j}$ (black line) overlaid on its reproduction (green line). The memory fraction used up until pattern $j$ is indicated by the panel fraction filled in red; the quota value is printed in the left bottom corner of each panel.</p>
<p>Japanese vowel recognition task [62]. The data of this benchmark consist in preprocessed audiorecordings of nine male native speakers pronouncing the Japanese di-vowel /ae/. The training data consist of 30 recordings per speaker, the test data consist of altogether 370 recordings, and the task is to train a recognizer which has to recognize the speakers of the test recordings. This kind of data differs from the periodic or chaotic patterns that I have been using so far, in that the patterns are non-stationary (changing in their structure from beginning to end), multidimensional (each recording consisting of 12 frequency band signals), stochastic, and of finite duration. This example thus also demonstrates that conceptors can be put to work with data other than single-channel stationary patterns.</p>
<p>A small (10 neurons) reservoir was created. It was driven with all training recordings from each speaker $j$ in turn $(j=1, \ldots, 9)$, collecting reservoir response signals, from which a conceptor $C^{j}$ characteristic of speaker $j$ was computed. In addition, for each speaker $j$, a conceptor $N^{j}=\neg\left(C^{1} \vee \ldots \vee C^{j-1} \vee C^{j+1} \vee \ldots C^{9}\right)$ was computed. $N^{j}$ characterizes the condition "this speaker is not any of the other eight speakers". Patterns need not to be loaded into the reservoir for this application, because they need not be re-generated.</p>
<p>In testing, a recording $p$ from the test set was fed to the reservoir, collecting a reservoir response signal $x$. For each of the conceptors, a positive evidence $E^{+}(p, j)=x^{\prime} C^{j} x$ was computed. $E^{+}(p, j)$ is a non-negative number indicating how well the signal $x$ fits into the ellipsoid of $C^{j}$. Likewise, the negative evidence $E^{-}(p, j)=x^{\prime} N^{j} x$ that the sample $p$ was not uttered by any of the eight speakers other than speaker $j$ was computed. Finally, the combined evidence $E(p, j)=E^{+}(p, i)+E^{-}(p, i)$ was computed. This gave nine combined evidences</p>
<p>$E(p, 1), \ldots, E(p, 9)$. The pattern $p$ was then classified as speaker $j$ by choosing the speaker index $j$ whose combined evidence $E(p, j)$ was the greatest among the nine collected evidences.</p>
<p>In order to check for the impact of the random selection of the underlying reservoir, this whole procedure was repeated 50 times, using a freshly created random reservoir in each trial. Averaged over these 50 trials, the number of test misclassifications was 3.4. If the classification would have been based solely on the positive or negative evidences, the average test misclassification numbers would have been 8.4 and 5.9 respectively. The combination of positive and negative evidence, which was enabled by Boolean operations, was crucial.</p>
<p>State-of-the-art machine learning methods achieve between 4 and 10 misclassifications on the test set (for instance [96, 102, 84, 14]). The Boolean-logic-conceptor-based classifier thus compares favorably with existing methods in terms of classification performance. The method is computationally cheap, with the entire learning procedure taking a fraction of a second only on a standard notebook computer. The most distinctive benefit however is incremental extensibility. If new training data become available, or if a new speaker would be incorporated into the recognition repertoire, the additional training can be done using only the new data without having to re-run previous training data. This feature is highly relevant in engineering applications and in cognitive modeling and missing from almost all state-of-the-art classification methods.</p>
<p>Autoconceptors and content-addressable memories. So far I have been describing examples where conceptors $C^{j}$ associated with patterns $p^{j}$ were computed at training time, to be later plugged in to re-generate or classify patterns. A conceptor $C$ matrix has the same size as the reservoir connection matrix $W$. Storing conceptor matrices means to store network-sized objects. This is implausible under aspects of biological modeling. Here I describe how conceptors can be created on the fly, without having to store them, leading to content-addressable neural memories.</p>
<p>If the system has no pre-computed conceptors at its disposal, loaded patterns can still be re-generated in a two-stage process. First, the target pattern $p$ is selected by driving the system with a brief initial "cueing" presentation of the pattern (possibly in a noisy version). During this phase, a preliminary conceptor $C^{\text {cue }}$ is created by an online adaptation process. This preliminary $C^{\text {cue }}$ already enables the system to re-generate an imperfect version of the pattern $p$. Second, after the cueing phase has ended, the system continues to run in an autonomous mode (no external cue signal), initially using $C^{\text {cue }}$, to continuously generate a pattern. While this process is running, the conceptor in the loop is continuously adapted by a simple online adaptation rule. This rule can be described in geometrical terms as "adapt the current conceptor $C(n)$ such that its ellipsoid matches better the shape of the point cloud of the current reservoir state dynamics". Under this rule one obtains a reliable convergence of the generated pattern toward a highly</p>
<p>accurate replica of the target pattern $p$ that was given as a cue.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Content-addressable memory. A First three of five loaded patterns. Left panels show the leading 20 singular values of $C^{\text {cue }}$ (black) and $C^{\text {auto }}$ (gray). Right panels show an overlay of the original driver pattern (black, thin) and the reconstruction at the end of auto-adaptation (gray, thick). B Pattern reconstruction errors directly after cueing (black squares) and at end of auto-adaptation (gray crosses). C Reconstruction error of loaded patterns (black) and novel patterns drawn from the same parametric family (gray) versus the number of loaded patterns, averaged over 5 repetitions of the entire experiment and 10 patterns per plotting point. Error bars indicate standard deviations.</p>
<p>Results of a demonstration are illustrated in Figure 6. A 200-neuron reservoir was loaded with 5 patterns consisting of a weighted sum of two irrational-period sines, sampled at integer timesteps. The weight ratio and the phaseshift were chosen at random; the patterns thus came from a family of patterns parametrized by two parameters. The cueing time was 30 timesteps, the free-running autoadaptation time was 10,000 timesteps, leading to an auto-adapted conceptor $C^{\text {auto }}$ at the end of this process. On average, the reconstruction error improved from about -0.4 (log10 NRMSE measured directly after the cueing) to -1.1 (at the end of auto-adaptation). It can be shown analytically that the auto-adaptation process pulls many singular values down to zero. This effect renders the combined reservoir-conceptor loop very robust against noise, because all noise components in the directions of the nulled singular values become completely suppressed. In fact,</p>
<p>all results shown in Figure 6 were obtained with strong state noise (signal-to-noise ratio equal to 1) inserted into the reservoir during the post-cue auto-adaptation.</p>
<p>The system functions as a content-addressable memory (CAM): loaded items can be recalled by cueing them. The paradigmatic example of a neural CAM are auto-associative neural networks (AANNs), pioneered by Palm [85] and Hopfield [50]. In contrast to conceptor-based CAM, which store and re-generate dynamical patterns, AANNs store and cue-recall static patterns. Furthermore, AANNs do not admit an incremental storing of new patterns, which is possible in conceptor-based CAMs. The latter thus represent an advance in neural CAMs in two fundamental aspects.</p>
<p>To further elucidate the properties of conceptor CAMs, I ran a suite of simulations where the same reservoir was loaded with increasing numbers of patterns, chosen at random from the same 2-parametric family (Figure 6 C). After loading with $k=2,3,5, \ldots, 100$ patterns, the reconstruction accuracy was measured at the end of the auto-adaptation. Not surprisingly, it deteriorated with increasing memory load $k$ (black line). In addition, I also cued the loaded reservoir with patterns that were not loaded, but were drawn from the same family. As one would expect, the re-construction accuracy of these novel patterns was worse than for the loaded patterns - but only for small $k$. When the number of loaded patterns exceeded a certain threshold, recall accuracy became essentially equal for loaded and novel patterns. These findings can be explained in intuitive terms as follows. When few patterns are loaded, the network memorizes individual patterns by "rote learning", and subsequently can recall these patterns better than other patterns from the family. When more patterns are loaded, the network learns a representation of the entire parametric class of patterns. I call this the class learning effect.</p>
<p>Toward biological feasibility: random feature conceptors. Several computations involved in adapting conceptor matrices are non-local and therefore biologically infeasible. It is however possible to approximate matrix conceptors with another mechanism which only requires local computations. The idea is to project (via random projection weights $F$ ) the reservoir state into a random feature space which is populated by a large number of neurons $z_{i}$; execute the conceptor operations individually on each of these neurons by multiplying a conception weight $c_{i}$ into its state; and finally to project back to the reservoir by another set of random projection weights $G$ (Figure 7).</p>
<p>The original reservoir-internal random connection weigths $W$ are replaced by a dyade of two random projections of first $F$, then $G$, and the original reservoir state $x$ segregates into a reservoir state $r$ and a random feature state $z$. The conception weights $c_{i}$ assume the role of conceptors. They can be learnt and adapted by procedures which are directly analog to the matrix conceptor case. What had to be non-local matrix computations before now turns into local, one-dimensional (scalar) operations. These operations are biologically feasible in the modest sense</p>
<p>that any information needed to adapt a synaptic weight is locally available at that synapse. All laws and constructions concerning Boolean operations and aperture carry over.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Random feature conceptors. This neural architecture has two pools of neurons, the reservoir and the feature space.</p>
<p>A set of conception weights $c_{i}$ corresponding to a particular pattern can be neurally represented and "stored" in the form of the connections of a single neuron to the feature space. A dynamical pattern thus can be represented by a single neuron. This enables a highly compact neural representation of dynamical patterns. A machine learning application is presented below.</p>
<p>I re-ran with such random feature conceptors a choice of the simulations that I did with matrix conceptors, using a number of random features that was two to five times as large as the reservoir. The outcome of these simulations: the accuracy of pattern re-generation is essentially the same as with matrix conceptors, but setting the aperture is more sensitive.</p>
<p>A hierarchical classification and de-noising architecture. Here I present a system which combines in a multi-layer neural architecture many of the items introduced so far. The input to this system is a (very) noisy signal which at a given time is being generated by one out of a number of possible candidate pattern generators. The task is to recognize the current generator, and simultaneously to re-generate a clean version of the noisy input pattern.</p>
<p>I explain the architecture with an example. It uses three processing layers to de-noise an input signal $u_{[1]}(n)=p^{j}(n)+$ noise, with $p^{j}$ being one of the four patterns $p^{1}, \ldots, p^{4}$ used before in this report (shown for instance in Figure 1 B). The architecture implements the following design principles (Figure 8 A). (i) Each layer is a random feature based conceptor system (as in Figure 7 B). The four patterns $p^{1}, \ldots, p^{4}$ are initially loaded into each of the layers, and four prototype conceptor weight vectors $c^{1}, \ldots, c^{4}$ corresponding to the patterns are computed and stored. (ii) In a bottom-up processing pathway, the noisy external input signal $u_{[1]}(n)=p^{j}(n)+$ noise is stagewise de-noised, leading to signals $y_{[1]}, y_{[2]}, y_{[3]}$ on layers $l=1,2,3$, where $y_{[3]}$ should be a highly cleaned-up version of the input</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Simultaneous signal de-noising and classification. A. Schema of architecture. B. Simulation results. Panels from above: first three panels: hypothesis vectors $\gamma_{[l]}^{j}(n)$ in the three layers. Color coding: $p^{1}$ blue, $p^{2}$ green, $p^{3}$ red, $p^{4}$ cyan. Fourth panel: trust variables $\tau_{[1,2]}(n)$ (blue) and $\tau_{[2,3]}(n)$ (green). Fifth panel: signal reconstruction errors (log10 NRMSE) of $y_{[1]}$ (blue), $y_{[2]}$ (green) and $y_{[3]}$ (red) versus clean signal $p^{j}$. Black line: linear baseline filter. Bottom panels: 20-step samples from the end of the two presentation periods. Red: noisy input; black: clean input; thick gray: cleaned output signal $y_{[3]}$.
(subscripts $[l]$ refer to layers, bottom layer is $l=1$ ). (iii) The top layer autoadapts a conceptor $c_{[3]}$ which is constrained to be a weighted OR combination of the four prototype conceptors. In a suggestive notation this can be written as $c_{[3]}(n)=\gamma_{[3]}^{1}(n) c^{1} \vee \ldots \vee \gamma_{[3]}^{4}(n) c^{4}$. The four weights $\gamma_{[3]}^{j}$ sum to one and represent a hypothesis vector expressing the system's current belief about the current driver $p^{j}$. If one of these $\gamma_{[3]}^{j}$ approaches 1 , the system has settled on a firm classification of the current driving pattern. (iv) In a top-down pathway, conceptors $c_{[l]}$ from layers $l$ are passed down to the respective layers $l-1$ below. Because higher layers should have a clearer conception of the current noisy driver pattern than lower layers, this passing-down of conceptors "primes" the processing in layer $l-1$ with valuable contextual information. (v) Between each pair of layers $l, l+1$, a trust variable $\tau_{[l, l+1]}(n)$ is adapted by an online procedure. These trust variables range between 0 and 1 . A value of $\tau_{[l, l+1]}(n)=1$ indicates maximal confidence that the signal $y_{[l+1]}(n)$ comes closer to the clean driver $p^{j}(n)$ than the signal $y_{[l]}(n)$ does,</p>            </div>
        </div>

    </div>
</body>
</html>
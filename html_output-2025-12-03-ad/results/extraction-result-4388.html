<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4388 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4388</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4388</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267412619</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.01788v2.pdf" target="_blank">LitLLM: A Toolkit for Scientific Literature Review</a></p>
                <p><strong>Paper Abstract:</strong> Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-factual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our project page including the demo and toolkit can be accessed here: https://litllm.github.io</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4388.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4388.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM Toolkit for Scientific Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive, modular toolkit that uses retrieval-augmented generation (RAG) and LLM-based re-ranking and generation to produce literature-review / related-work text grounded in retrieved academic papers from APIs (Semantic Scholar, OpenAlex).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular pipeline: (1) an LLM summarizes a user-provided abstract into up to ~5 keywords to form a query; (2) query is sent to academic search APIs (Semantic Scholar, OpenAlex) to retrieve candidate papers; (3) an LLM-based re-ranker (listwise permutation generation and optional debate-ranking with attribution) reorders top-k candidates; (4) an LLM generator produces the related work / literature review using the re-ranked abstracts as context, optionally controlled by a sentence-plan (plan-based generation). The system is implemented as a web UI (React) and uses API-based retrieval plus OpenAI GPT-3.5/GPT-4 for LLM components in the implementation, but the pipeline is LLM-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI GPT-3.5-turbo and GPT-4 (implementation); pipeline supports other proprietary or open-source LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM keyword summarization to form search queries + API-based retrieval (Semantic Scholar, OpenAlex) of abstracts/metadata; LLM-based re-ranking of retrieved candidate abstracts (listwise permutation / debate-ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-Augmented Generation (RAG): augment LLM generator with re-ranked retrieved abstracts as context and optionally apply sentence-plan (plan-based) generation for controllable multi-document synthesis into a cohesive related-work narrative.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Configurable top-k retrieved per query (demo/example used 4); module supports retrieving larger numbers via API limits (num_papers_api parameter).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature across multiple research areas (uses Semantic Scholar / OpenAlex corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Related work / literature-review sections (multi-document synthesized narrative with citations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative human researcher feedback (small pilot with 5 researchers); demo examples checking recovered citations; no standardized automatic metrics (ROUGE, etc.) reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative: users found zero-shot generation informative and plan-based generation more concise/useful; demo example recovered a top recommended paper that matched an original paper's citation. Authors report substantial reduction in time/effort but provide no quantitative numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual literature review / traditional search engines (Semantic Scholar default ranking); no large-scale quantitative baseline study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No quantitative head-to-head reported; qualitative user feedback suggests LitLLM speeds up review creation and plan-based outputs are more tailored than zero-shot outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining LLM keyword-based querying, LLM re-ranking, and RAG-style generation grounded in retrieved abstracts produces more fact-grounded related-work text and reduces hallucination risk; sentence-plan conditioning improves controllability and reduces hallucination in generated sections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on abstracts only (not full-text), which limits depth; LLM hallucination risk remains if retrieval is insufficient; dependency on external APIs and up-to-date indexes; no large-scale automatic evaluation reported; disclosure and human oversight recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors note limitations from long-context constraints and propose future ingestion of full papers when longer-context LLMs are available; no empirical scaling curves or numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4388.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that augments LLM generation by retrieving external documents (here, retrieved paper abstracts) and conditioning the LLM on that retrieved context to improve factuality and recency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retrieve a set of relevant external documents (paper abstracts/metadata) and provide them as context to an LLM generator so outputs are grounded in retrieved evidence rather than solely the model's parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Conceptual â€” applied in this work with GPT-3.5/GPT-4 (implementation); RAG is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>API-based retrieval of candidate papers combined with LLM-summarized queries; retrieved documents appended to generation prompt as evidence/context.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Contextual grounding of single LLM generation across multiple retrieved documents to create multi-document summaries/literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Variable; depends on retriever/top-k selection; paper demonstrates small top-k (e.g., 4 in demo) but pipeline supports larger retrieved sets limited by prompt/context size.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General knowledge- and literature-intensive NLP tasks; here applied to scientific literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded natural-language summaries / literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported in literature as reducing hallucinations and improving factuality (cited surveys); this paper uses qualitative feedback rather than numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative statement: RAG 'drastically reduces hallucinations' (cited survey); no numerical results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pure parametric LLM generation without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described qualitatively as superior in factual grounding; no quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG enables up-to-date, evidence-grounded literature review generation and mitigates some hallucination issues of LLM-only generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompt/context length limits how many retrieved documents can be fully incorporated; retrieval quality heavily influences output factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors discuss need for longer-context LLMs to scale to full-paper ingestion; no quantitative scaling analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4388.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Listwise Reranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Listwise Re-Ranking (Instructional Permutation Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A re-ranking approach which prompts an LLM to output a permutation (ranked list) of candidate papers in descending relevance to a query abstract (listwise ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is chatgpt good at search? investigating large language models as re-ranking agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Instructional permutation generation (listwise LLM reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After an initial retrieval yields top-k candidate papers, the system feeds the set plus the query abstract to an LLM with a prompt that requests a permutation ordering (e.g., output only ranks like [] > [] > []). The LLM returns a listwise ranking rather than independent pairwise scores.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied here with GPT-3.5/GPT-4 in implementation; approach demonstrated in literature with ChatGPT/GPT-4 and some open-source LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Listwise prompt-based ranking using full candidate abstracts as input to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Reorders retrieved papers to prioritize which documents will be used as context for final generation; this is a pre-synthesis ranking step rather than a cross-document synthesis method.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on the retriever's top-k candidates (configurable); typical k small (tens) due to prompt size limits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Information retrieval and academic search / literature review re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked list of retrieved papers (re-ordered retrieval results).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Literature reports relevance-ranking metrics; this paper does not report numeric IR metrics but cites prior work showing competitive results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper cites prior work showing generative LLMs can deliver competitive reranking results; no new numeric results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional supervised rerankers or the original search engine ranking (Semantic Scholar default).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited prior work indicates LLM rerankers can be competitive; no head-to-head numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompting LLMs for listwise permutations is an effective zero-shot reranking strategy that integrates easily into the RAG pipeline for literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompt length constraints limit the number of candidates; potential instability / sensitivity to prompt design and ordering; compute/API costs for multiple LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Effectiveness depends on candidate set size and LLM context window; authors note practical limits and use small top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4388.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debate-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Debate-ranking with Attribution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A re-ranking variant that prompts an LLM to produce pros/cons arguments for each candidate paper and then output an inclusion probability, improving interpretability of ranking decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Debate-ranking with attribution</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each candidate paper the LLM is prompted to (1) generate arguments for and against including that paper given the query abstract and (2) output a final probability or score of inclusion; scores are used to produce a ranked list with human-readable attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with GPT-3.5/GPT-4 in the LitLLM pipeline (as reported); concept referenced to recent literature.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Argument-generation and scoring via LLM prompt per candidate using abstracts as input.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates LLM-generated pros/cons and probabilities across candidates to form a ranked set for downstream synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per candidate in top-k retrieval; practical top-k limited by cost and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic literature re-ranking and selection for literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked candidate list with argumentation and inclusion probabilities (interpretable signals).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not systematically evaluated in this paper; proposed to improve interpretability and selection quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No quantitative results in this paper; presented as an explored reranking strategy within LitLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Listwise permutation reranking and default search rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No numeric comparisons reported; described as providing attribution that may reduce blind acceptance of ranked items.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attribution via generated arguments can make re-ranking decisions more transparent and may help authors vet included papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Extra cost and latency since the LLM must generate argumentation per candidate; argument quality depends on input abstracts and LLM factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales linearly with number of candidates (per-candidate LLM calls); requires careful budget management for larger candidate sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4388.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan-based Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence Plan-Based Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controllable generation method that supplies an LLM with a sentence-level plan (number of sentences, which citations to place on which line) to reduce hallucination and produce more usable, structured related-work text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sentence plan-based generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors prompt the LLM with an explicit plan template (e.g., 'Please generate {num_sentences} sentences... Cite {cite_x} at line {line_x}.') to control length, citation placement, and structure; the re-ranked abstracts serve as the factual context for each planned sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with GPT-3.5/GPT-4 in LitLLM; general technique applicable to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Uses retrieved abstracts as evidence; plan constrains how the LLM should map evidence to surface sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Guided sentence-level synthesis: each sentence is constructed to align with the provided plan and the relevant retrieved abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dependent on re-ranked set used as context; plans in demo used 4 retrieved papers in example.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Controllable generation for literature-review text across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise, structured related-work paragraphs with per-line citation placement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human qualitative feedback from pilot users; concurrent work cited (Agarwal et al., 2024) indicates reduced hallucinations with sentence plans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>User feedback: plan-based generation produced more succinct and readily usable literature reviews versus zero-shot generation; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot generation (no explicit sentence plan).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitatively preferred by users; reported to reduce hallucinations and increase usability versus zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sentence plans provide controllability and reduce hallucinations, yielding outputs more aligned with author preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires designing appropriate plans (author input or automatic plan generation); plan rigidity might reduce creativity or omit nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Plan effectiveness interacts with context size and number of retrieved documents; authors suggest plans help manage longer multi-document contexts but do not provide scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4388.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot LitGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Generation for Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation strategy where the LLM produces a related-work synthesis without an explicit sentence plan, relying on prompts and retrieved context (RAG) to guide output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-shot generation (RAG-backed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The LLM is prompted to create a cohesive related-work section using only provided abstracts and retrieved context; the prompt directs the model to compare strengths/weaknesses and cite provided papers, but no sentence-level plan is enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5/GPT-4 in implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM consumes retrieved abstracts in prompt context (RAG) and synthesizes directly.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct multi-document synthesis via a single LLM generation step guided by prompt instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over re-ranked set provided in prompt; example used 4 retrieved papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form related work / narrative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>User qualitative feedback in pilot study; no automated metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Users found zero-shot outputs informative about literature in general but less tailored than plan-based outputs; no numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Plan-based generation and manual review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Informative but less concise and less controllable than plan-based outputs per user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot generation is valuable for broad exploration but benefits from plan-based control to produce usable text for a paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Greater risk of hallucination and less adherence to author preferences than plan-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Quality constrained by prompt/context size; longer context LLMs could enable richer zero-shot multi-paper synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4388.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Lit Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using ChatGPT for Literature Review / LLM-powered writing assistants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior lines of work and anecdotal studies that apply ChatGPT (and similar LLMs) directly to generate literature reviews, abstracts, and other scientific writing tasks, often prone to hallucination unless grounded by retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using chatgpt to conduct a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT-based literature review approaches</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Researchers have experimented with using ChatGPT/GPT-4 to summarize, synthesize, and draft literature reviews from prompts; such approaches may be high-quality in language but are vulnerable to hallucination and outdated knowledge without retrieval grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (GPT-3.5 family) and GPT-4 as reported in related studies.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based querying of model's parametric memory (no guaranteed external retrieval unless explicitly combined).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-model synthesis from internal knowledge; sometimes combined with human-provided citations or retrieved documents in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Implicitly depends on model's parametric knowledge; not explicit retrieval of multiple papers unless user provides them.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific writing and literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Draft literature reviews / summaries / abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative assessments, case studies; prior works note hallucination issues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>High-quality fluent text but documented hallucination risks and outdated knowledge in practice; no uniform numeric evaluation in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual literature reviews and RAG-backed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Language quality often high but factual reliability lower than RAG-backed approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-only approaches can draft fluent reviews but must be used cautiously due to hallucination and recency limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination, outdated knowledge (training cutoff), untraceable reasoning, and lack of provenance for claims.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Model improvements (e.g., GPT-4) improve fluency and some factuality but do not eliminate hallucinations without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4388.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM trained and evaluated on scientific tasks intended to reason about scientific knowledge, but observed to hallucinate (invent citations, papers) in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large LLM trained with scientific corpora to perform tasks like question answering and summarization in scientific contexts; discussed here as an example of an LLM that, despite strong performance, produced non-factual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica model (size/architecture noted in original Galactica paper; referenced here qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Parametric: model relies on internal knowledge from training data rather than external retrieval (as discussed in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct LLM synthesis from parametric memory across many scientific documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Implicit (encoded in training corpus); not a retrieval-based multi-document pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific knowledge across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers, summaries, and draft scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmarks on scientific tasks; qualitative examination of hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Outperformed contemporary models on many tasks but generated made-up content like inaccurate citations and imaginary papers (qualitative critique reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Contemporary LLMs at time of Galactica evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Strong benchmark performance yet problematic factuality in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large parametric LLMs can appear knowledgeable but may produce non-factual, hallucinatory outputs without retrieval grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination, invented citations; motivates RAG approaches for grounded outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scaling parametric model size improved many capabilities but did not eliminate hallucination issues without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4388.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4388.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Listwise Rerank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Listwise Document Reranking with LLMs (RankVicuna / RankZephyr / Ma et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent work demonstrating open-source and proprietary LLMs can perform zero-shot listwise reranking of retrieved documents competitively with supervised rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot listwise document reranking with a large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-shot listwise LLM rerankers (RankVicuna / RankZephyr / Ma et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that feed a list of retrieved documents into an LLM and prompt it to output a reranked permutation (listwise), with demonstrations both on proprietary LLMs (ChatGPT/GPT-4) and open-source LLMs (RankVicuna et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT/GPT-4 in some studies; open-source LLMs (e.g., Vicuna family) in others as per referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Listwise prompt-based re-ranking of retrieved candidate documents.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Pre-selection / ordering of documents for downstream synthesis (not a synthesis method per se).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on retriever top-k; practical k limited by prompt/context size.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Information retrieval and literature search contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Re-ranked lists for downstream consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>IR relevance metrics (in cited studies); this paper references these findings qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited prior work reports competitive reranking performance in zero-shot settings; no numbers reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Supervised rerankers, traditional IR ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Prior work shows competitive performance; specifics vary by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can serve as effective zero-shot rerankers, enabling simpler pipelines without additional supervised reranker training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Context-size limits and sensitivity to prompt design; model and instruction variance affects stability.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance depends on LLM capability and context window; open-source models show promise but may lag top proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLM: A Toolkit for Scientific Literature Review', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Is chatgpt good at search? investigating large language models as re-ranking agent. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented controllable review generation. <em>(Rating: 2)</em></li>
                <li>Zero-shot listwise document reranking with a large language model. <em>(Rating: 2)</em></li>
                <li>Rankvicuna: Zero-shot listwise document reranking with open-source large language models. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey. <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science. <em>(Rating: 1)</em></li>
                <li>Using chatgpt to conduct a literature review. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4388",
    "paper_id": "paper-267412619",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM Toolkit for Scientific Literature Review",
            "brief_description": "An interactive, modular toolkit that uses retrieval-augmented generation (RAG) and LLM-based re-ranking and generation to produce literature-review / related-work text grounded in retrieved academic papers from APIs (Semantic Scholar, OpenAlex).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LitLLM",
            "system_description": "Modular pipeline: (1) an LLM summarizes a user-provided abstract into up to ~5 keywords to form a query; (2) query is sent to academic search APIs (Semantic Scholar, OpenAlex) to retrieve candidate papers; (3) an LLM-based re-ranker (listwise permutation generation and optional debate-ranking with attribution) reorders top-k candidates; (4) an LLM generator produces the related work / literature review using the re-ranked abstracts as context, optionally controlled by a sentence-plan (plan-based generation). The system is implemented as a web UI (React) and uses API-based retrieval plus OpenAI GPT-3.5/GPT-4 for LLM components in the implementation, but the pipeline is LLM-agnostic.",
            "llm_model_used": "OpenAI GPT-3.5-turbo and GPT-4 (implementation); pipeline supports other proprietary or open-source LLMs",
            "extraction_technique": "LLM keyword summarization to form search queries + API-based retrieval (Semantic Scholar, OpenAlex) of abstracts/metadata; LLM-based re-ranking of retrieved candidate abstracts (listwise permutation / debate-ranking).",
            "synthesis_technique": "Retrieval-Augmented Generation (RAG): augment LLM generator with re-ranked retrieved abstracts as context and optionally apply sentence-plan (plan-based) generation for controllable multi-document synthesis into a cohesive related-work narrative.",
            "number_of_papers": "Configurable top-k retrieved per query (demo/example used 4); module supports retrieving larger numbers via API limits (num_papers_api parameter).",
            "domain_or_topic": "General scientific literature across multiple research areas (uses Semantic Scholar / OpenAlex corpora).",
            "output_type": "Related work / literature-review sections (multi-document synthesized narrative with citations).",
            "evaluation_metrics": "Qualitative human researcher feedback (small pilot with 5 researchers); demo examples checking recovered citations; no standardized automatic metrics (ROUGE, etc.) reported in this paper.",
            "performance_results": "Qualitative: users found zero-shot generation informative and plan-based generation more concise/useful; demo example recovered a top recommended paper that matched an original paper's citation. Authors report substantial reduction in time/effort but provide no quantitative numbers.",
            "comparison_baseline": "Manual literature review / traditional search engines (Semantic Scholar default ranking); no large-scale quantitative baseline study reported.",
            "performance_vs_baseline": "No quantitative head-to-head reported; qualitative user feedback suggests LitLLM speeds up review creation and plan-based outputs are more tailored than zero-shot outputs.",
            "key_findings": "Combining LLM keyword-based querying, LLM re-ranking, and RAG-style generation grounded in retrieved abstracts produces more fact-grounded related-work text and reduces hallucination risk; sentence-plan conditioning improves controllability and reduces hallucination in generated sections.",
            "limitations_challenges": "Relies on abstracts only (not full-text), which limits depth; LLM hallucination risk remains if retrieval is insufficient; dependency on external APIs and up-to-date indexes; no large-scale automatic evaluation reported; disclosure and human oversight recommended.",
            "scaling_behavior": "Authors note limitations from long-context constraints and propose future ingestion of full papers when longer-context LLMs are available; no empirical scaling curves or numbers provided.",
            "uuid": "e4388.0",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A paradigm that augments LLM generation by retrieving external documents (here, retrieved paper abstracts) and conditioning the LLM on that retrieved context to improve factuality and recency.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Retrieve a set of relevant external documents (paper abstracts/metadata) and provide them as context to an LLM generator so outputs are grounded in retrieved evidence rather than solely the model's parametric memory.",
            "llm_model_used": "Conceptual â€” applied in this work with GPT-3.5/GPT-4 (implementation); RAG is model-agnostic.",
            "extraction_technique": "API-based retrieval of candidate papers combined with LLM-summarized queries; retrieved documents appended to generation prompt as evidence/context.",
            "synthesis_technique": "Contextual grounding of single LLM generation across multiple retrieved documents to create multi-document summaries/literature reviews.",
            "number_of_papers": "Variable; depends on retriever/top-k selection; paper demonstrates small top-k (e.g., 4 in demo) but pipeline supports larger retrieved sets limited by prompt/context size.",
            "domain_or_topic": "General knowledge- and literature-intensive NLP tasks; here applied to scientific literature review.",
            "output_type": "Grounded natural-language summaries / literature reviews.",
            "evaluation_metrics": "Reported in literature as reducing hallucinations and improving factuality (cited surveys); this paper uses qualitative feedback rather than numeric metrics.",
            "performance_results": "Qualitative statement: RAG 'drastically reduces hallucinations' (cited survey); no numerical results provided here.",
            "comparison_baseline": "Pure parametric LLM generation without retrieval.",
            "performance_vs_baseline": "Described qualitatively as superior in factual grounding; no quantitative comparisons in this paper.",
            "key_findings": "RAG enables up-to-date, evidence-grounded literature review generation and mitigates some hallucination issues of LLM-only generations.",
            "limitations_challenges": "Prompt/context length limits how many retrieved documents can be fully incorporated; retrieval quality heavily influences output factuality.",
            "scaling_behavior": "Authors discuss need for longer-context LLMs to scale to full-paper ingestion; no quantitative scaling analysis.",
            "uuid": "e4388.1",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM Listwise Reranking",
            "name_full": "LLM-based Listwise Re-Ranking (Instructional Permutation Generation)",
            "brief_description": "A re-ranking approach which prompts an LLM to output a permutation (ranked list) of candidate papers in descending relevance to a query abstract (listwise ranking).",
            "citation_title": "Is chatgpt good at search? investigating large language models as re-ranking agent.",
            "mention_or_use": "use",
            "system_name": "Instructional permutation generation (listwise LLM reranker)",
            "system_description": "After an initial retrieval yields top-k candidate papers, the system feeds the set plus the query abstract to an LLM with a prompt that requests a permutation ordering (e.g., output only ranks like [] &gt; [] &gt; []). The LLM returns a listwise ranking rather than independent pairwise scores.",
            "llm_model_used": "Applied here with GPT-3.5/GPT-4 in implementation; approach demonstrated in literature with ChatGPT/GPT-4 and some open-source LLMs.",
            "extraction_technique": "Listwise prompt-based ranking using full candidate abstracts as input to the LLM.",
            "synthesis_technique": "Reorders retrieved papers to prioritize which documents will be used as context for final generation; this is a pre-synthesis ranking step rather than a cross-document synthesis method.",
            "number_of_papers": "Operates on the retriever's top-k candidates (configurable); typical k small (tens) due to prompt size limits.",
            "domain_or_topic": "Information retrieval and academic search / literature review re-ranking.",
            "output_type": "Ranked list of retrieved papers (re-ordered retrieval results).",
            "evaluation_metrics": "Literature reports relevance-ranking metrics; this paper does not report numeric IR metrics but cites prior work showing competitive results.",
            "performance_results": "Paper cites prior work showing generative LLMs can deliver competitive reranking results; no new numeric results reported here.",
            "comparison_baseline": "Traditional supervised rerankers or the original search engine ranking (Semantic Scholar default).",
            "performance_vs_baseline": "Cited prior work indicates LLM rerankers can be competitive; no head-to-head numbers in this paper.",
            "key_findings": "Prompting LLMs for listwise permutations is an effective zero-shot reranking strategy that integrates easily into the RAG pipeline for literature review.",
            "limitations_challenges": "Prompt length constraints limit the number of candidates; potential instability / sensitivity to prompt design and ordering; compute/API costs for multiple LLM calls.",
            "scaling_behavior": "Effectiveness depends on candidate set size and LLM context window; authors note practical limits and use small top-k.",
            "uuid": "e4388.2",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Debate-ranking",
            "name_full": "Debate-ranking with Attribution",
            "brief_description": "A re-ranking variant that prompts an LLM to produce pros/cons arguments for each candidate paper and then output an inclusion probability, improving interpretability of ranking decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Debate-ranking with attribution",
            "system_description": "For each candidate paper the LLM is prompted to (1) generate arguments for and against including that paper given the query abstract and (2) output a final probability or score of inclusion; scores are used to produce a ranked list with human-readable attribution.",
            "llm_model_used": "Implemented with GPT-3.5/GPT-4 in the LitLLM pipeline (as reported); concept referenced to recent literature.",
            "extraction_technique": "Argument-generation and scoring via LLM prompt per candidate using abstracts as input.",
            "synthesis_technique": "Aggregates LLM-generated pros/cons and probabilities across candidates to form a ranked set for downstream synthesis.",
            "number_of_papers": "Applied per candidate in top-k retrieval; practical top-k limited by cost and latency.",
            "domain_or_topic": "Academic literature re-ranking and selection for literature review generation.",
            "output_type": "Ranked candidate list with argumentation and inclusion probabilities (interpretable signals).",
            "evaluation_metrics": "Not systematically evaluated in this paper; proposed to improve interpretability and selection quality.",
            "performance_results": "No quantitative results in this paper; presented as an explored reranking strategy within LitLLM.",
            "comparison_baseline": "Listwise permutation reranking and default search rankings.",
            "performance_vs_baseline": "No numeric comparisons reported; described as providing attribution that may reduce blind acceptance of ranked items.",
            "key_findings": "Attribution via generated arguments can make re-ranking decisions more transparent and may help authors vet included papers.",
            "limitations_challenges": "Extra cost and latency since the LLM must generate argumentation per candidate; argument quality depends on input abstracts and LLM factuality.",
            "scaling_behavior": "Scales linearly with number of candidates (per-candidate LLM calls); requires careful budget management for larger candidate sets.",
            "uuid": "e4388.3",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Plan-based Gen",
            "name_full": "Sentence Plan-Based Generation",
            "brief_description": "A controllable generation method that supplies an LLM with a sentence-level plan (number of sentences, which citations to place on which line) to reduce hallucination and produce more usable, structured related-work text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sentence plan-based generation",
            "system_description": "Authors prompt the LLM with an explicit plan template (e.g., 'Please generate {num_sentences} sentences... Cite {cite_x} at line {line_x}.') to control length, citation placement, and structure; the re-ranked abstracts serve as the factual context for each planned sentence.",
            "llm_model_used": "Implemented with GPT-3.5/GPT-4 in LitLLM; general technique applicable to other LLMs.",
            "extraction_technique": "Uses retrieved abstracts as evidence; plan constrains how the LLM should map evidence to surface sentences.",
            "synthesis_technique": "Guided sentence-level synthesis: each sentence is constructed to align with the provided plan and the relevant retrieved abstracts.",
            "number_of_papers": "Dependent on re-ranked set used as context; plans in demo used 4 retrieved papers in example.",
            "domain_or_topic": "Controllable generation for literature-review text across domains.",
            "output_type": "Concise, structured related-work paragraphs with per-line citation placement.",
            "evaluation_metrics": "Human qualitative feedback from pilot users; concurrent work cited (Agarwal et al., 2024) indicates reduced hallucinations with sentence plans.",
            "performance_results": "User feedback: plan-based generation produced more succinct and readily usable literature reviews versus zero-shot generation; no numeric metrics provided.",
            "comparison_baseline": "Zero-shot generation (no explicit sentence plan).",
            "performance_vs_baseline": "Qualitatively preferred by users; reported to reduce hallucinations and increase usability versus zero-shot.",
            "key_findings": "Sentence plans provide controllability and reduce hallucinations, yielding outputs more aligned with author preferences.",
            "limitations_challenges": "Requires designing appropriate plans (author input or automatic plan generation); plan rigidity might reduce creativity or omit nuance.",
            "scaling_behavior": "Plan effectiveness interacts with context size and number of retrieved documents; authors suggest plans help manage longer multi-document contexts but do not provide scaling curves.",
            "uuid": "e4388.4",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zero-shot LitGen",
            "name_full": "Zero-shot Generation for Literature Review",
            "brief_description": "A generation strategy where the LLM produces a related-work synthesis without an explicit sentence plan, relying on prompts and retrieved context (RAG) to guide output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Zero-shot generation (RAG-backed)",
            "system_description": "The LLM is prompted to create a cohesive related-work section using only provided abstracts and retrieved context; the prompt directs the model to compare strengths/weaknesses and cite provided papers, but no sentence-level plan is enforced.",
            "llm_model_used": "GPT-3.5/GPT-4 in implementation.",
            "extraction_technique": "LLM consumes retrieved abstracts in prompt context (RAG) and synthesizes directly.",
            "synthesis_technique": "Direct multi-document synthesis via a single LLM generation step guided by prompt instructions.",
            "number_of_papers": "Operates over re-ranked set provided in prompt; example used 4 retrieved papers.",
            "domain_or_topic": "General scientific literature review generation.",
            "output_type": "Long-form related work / narrative summaries.",
            "evaluation_metrics": "User qualitative feedback in pilot study; no automated metrics reported.",
            "performance_results": "Users found zero-shot outputs informative about literature in general but less tailored than plan-based outputs; no numeric results.",
            "comparison_baseline": "Plan-based generation and manual review.",
            "performance_vs_baseline": "Informative but less concise and less controllable than plan-based outputs per user feedback.",
            "key_findings": "Zero-shot generation is valuable for broad exploration but benefits from plan-based control to produce usable text for a paper.",
            "limitations_challenges": "Greater risk of hallucination and less adherence to author preferences than plan-based generation.",
            "scaling_behavior": "Quality constrained by prompt/context size; longer context LLMs could enable richer zero-shot multi-paper synthesis.",
            "uuid": "e4388.5",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChatGPT Lit Review",
            "name_full": "Using ChatGPT for Literature Review / LLM-powered writing assistants",
            "brief_description": "Prior lines of work and anecdotal studies that apply ChatGPT (and similar LLMs) directly to generate literature reviews, abstracts, and other scientific writing tasks, often prone to hallucination unless grounded by retrieval.",
            "citation_title": "Using chatgpt to conduct a literature review",
            "mention_or_use": "mention",
            "system_name": "ChatGPT-based literature review approaches",
            "system_description": "Researchers have experimented with using ChatGPT/GPT-4 to summarize, synthesize, and draft literature reviews from prompts; such approaches may be high-quality in language but are vulnerable to hallucination and outdated knowledge without retrieval grounding.",
            "llm_model_used": "ChatGPT (GPT-3.5 family) and GPT-4 as reported in related studies.",
            "extraction_technique": "Prompt-based querying of model's parametric memory (no guaranteed external retrieval unless explicitly combined).",
            "synthesis_technique": "Single-model synthesis from internal knowledge; sometimes combined with human-provided citations or retrieved documents in practice.",
            "number_of_papers": "Implicitly depends on model's parametric knowledge; not explicit retrieval of multiple papers unless user provides them.",
            "domain_or_topic": "General scientific writing and literature reviews.",
            "output_type": "Draft literature reviews / summaries / abstracts.",
            "evaluation_metrics": "Qualitative assessments, case studies; prior works note hallucination issues.",
            "performance_results": "High-quality fluent text but documented hallucination risks and outdated knowledge in practice; no uniform numeric evaluation in cited works.",
            "comparison_baseline": "Manual literature reviews and RAG-backed systems.",
            "performance_vs_baseline": "Language quality often high but factual reliability lower than RAG-backed approaches.",
            "key_findings": "LLM-only approaches can draft fluent reviews but must be used cautiously due to hallucination and recency limits.",
            "limitations_challenges": "Hallucination, outdated knowledge (training cutoff), untraceable reasoning, and lack of provenance for claims.",
            "scaling_behavior": "Model improvements (e.g., GPT-4) improve fluency and some factuality but do not eliminate hallucinations without retrieval.",
            "uuid": "e4388.6",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A Large Language Model for Science",
            "brief_description": "An LLM trained and evaluated on scientific tasks intended to reason about scientific knowledge, but observed to hallucinate (invent citations, papers) in practice.",
            "citation_title": "Galactica: A large language model for science.",
            "mention_or_use": "mention",
            "system_name": "Galactica",
            "system_description": "A large LLM trained with scientific corpora to perform tasks like question answering and summarization in scientific contexts; discussed here as an example of an LLM that, despite strong performance, produced non-factual outputs.",
            "llm_model_used": "Galactica model (size/architecture noted in original Galactica paper; referenced here qualitatively).",
            "extraction_technique": "Parametric: model relies on internal knowledge from training data rather than external retrieval (as discussed in the paper).",
            "synthesis_technique": "Direct LLM synthesis from parametric memory across many scientific documents.",
            "number_of_papers": "Implicit (encoded in training corpus); not a retrieval-based multi-document pipeline.",
            "domain_or_topic": "Scientific knowledge across domains.",
            "output_type": "Answers, summaries, and draft scientific text.",
            "evaluation_metrics": "Benchmarks on scientific tasks; qualitative examination of hallucinations.",
            "performance_results": "Outperformed contemporary models on many tasks but generated made-up content like inaccurate citations and imaginary papers (qualitative critique reported).",
            "comparison_baseline": "Contemporary LLMs at time of Galactica evaluation.",
            "performance_vs_baseline": "Strong benchmark performance yet problematic factuality in outputs.",
            "key_findings": "Large parametric LLMs can appear knowledgeable but may produce non-factual, hallucinatory outputs without retrieval grounding.",
            "limitations_challenges": "Hallucination, invented citations; motivates RAG approaches for grounded outputs.",
            "scaling_behavior": "Scaling parametric model size improved many capabilities but did not eliminate hallucination issues without retrieval.",
            "uuid": "e4388.7",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zero-shot Listwise Rerank",
            "name_full": "Zero-shot Listwise Document Reranking with LLMs (RankVicuna / RankZephyr / Ma et al.)",
            "brief_description": "Recent work demonstrating open-source and proprietary LLMs can perform zero-shot listwise reranking of retrieved documents competitively with supervised rerankers.",
            "citation_title": "Zero-shot listwise document reranking with a large language model.",
            "mention_or_use": "mention",
            "system_name": "Zero-shot listwise LLM rerankers (RankVicuna / RankZephyr / Ma et al.)",
            "system_description": "Approaches that feed a list of retrieved documents into an LLM and prompt it to output a reranked permutation (listwise), with demonstrations both on proprietary LLMs (ChatGPT/GPT-4) and open-source LLMs (RankVicuna et al.).",
            "llm_model_used": "ChatGPT/GPT-4 in some studies; open-source LLMs (e.g., Vicuna family) in others as per referenced works.",
            "extraction_technique": "Listwise prompt-based re-ranking of retrieved candidate documents.",
            "synthesis_technique": "Pre-selection / ordering of documents for downstream synthesis (not a synthesis method per se).",
            "number_of_papers": "Operates on retriever top-k; practical k limited by prompt/context size.",
            "domain_or_topic": "Information retrieval and literature search contexts.",
            "output_type": "Re-ranked lists for downstream consumption.",
            "evaluation_metrics": "IR relevance metrics (in cited studies); this paper references these findings qualitatively.",
            "performance_results": "Cited prior work reports competitive reranking performance in zero-shot settings; no numbers reproduced here.",
            "comparison_baseline": "Supervised rerankers, traditional IR ranking.",
            "performance_vs_baseline": "Prior work shows competitive performance; specifics vary by dataset and model.",
            "key_findings": "LLMs can serve as effective zero-shot rerankers, enabling simpler pipelines without additional supervised reranker training.",
            "limitations_challenges": "Context-size limits and sensitivity to prompt design; model and instruction variance affects stability.",
            "scaling_behavior": "Performance depends on LLM capability and context window; open-source models show promise but may lag top proprietary models.",
            "uuid": "e4388.8",
            "source_info": {
                "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Is chatgpt good at search? investigating large language models as re-ranking agent.",
            "rating": 2,
            "sanitized_title": "is_chatgpt_good_at_search_investigating_large_language_models_as_reranking_agent"
        },
        {
            "paper_title": "Retrieval-augmented controllable review generation.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_controllable_review_generation"
        },
        {
            "paper_title": "Zero-shot listwise document reranking with a large language model.",
            "rating": 2,
            "sanitized_title": "zeroshot_listwise_document_reranking_with_a_large_language_model"
        },
        {
            "paper_title": "Rankvicuna: Zero-shot listwise document reranking with open-source large language models.",
            "rating": 2,
            "sanitized_title": "rankvicuna_zeroshot_listwise_document_reranking_with_opensource_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Galactica: A large language model for science.",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Using chatgpt to conduct a literature review.",
            "rating": 1,
            "sanitized_title": "using_chatgpt_to_conduct_a_literature_review"
        }
    ],
    "cost": 0.019252,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LitLLM: A Toolkit for Scientific Literature Review
21 Mar 2025</p>
<p>Shubham Agarwal shubham.agarwal@mila.quebec 
ServiceNow Research</p>
<p>Mila -Quebec AI Institute
3 HEC MontrealCanada</p>
<p>Gaurav Sahu gaurav.sahu@mila.quebec 
ServiceNow Research</p>
<p>Abhay Puri 
ServiceNow Research</p>
<p>Issam H Laradji 
ServiceNow Research</p>
<p>UBC
VancouverCanada</p>
<p>Krishnamurthy Dj Dvijotham 
ServiceNow Research</p>
<p>Jason Stanley 
ServiceNow Research</p>
<p>Laurent Charlin 
Mila -Quebec AI Institute
3 HEC MontrealCanada</p>
<p>CIFAR AI Chair
Canada</p>
<p>Christopher Pal 
ServiceNow Research</p>
<p>Mila -Quebec AI Institute
3 HEC MontrealCanada</p>
<p>CIFAR AI Chair
Canada</p>
<p>University of Waterloo</p>
<p>LitLLM: A Toolkit for Scientific Literature Review
21 Mar 20252FFC5800F5E9B8B1B9BB4B893D95063EarXiv:2402.01788v2[cs.CL]
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work.It is a tedious task which makes an automatic literature review generator appealing.Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations.They tend to hallucinate-generate non-factual information-and ignore the latest research they have not been trained on.To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs.Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM.Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process.Second, the system re-ranks the retrieved papers based on the user-provided abstract.Finally, the related work section is generated based on the re-ranked results and the abstract.There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative.Our project page including the demo and toolkit can be accessed here: https://litllm.github.io.</p>
<p>Introduction</p>
<p>Scientists have long used NLP systems like search engines to find and retrieve relevant papers.Scholarly engines, including Google Scholar, Microsoft Academic Graph, and Semantic Scholar, provide additional tools and structure to help researchers further.Following recent advances in large language models (LLMs), a new set of systems provides even more advanced features.For example, * Equal contribution.</p>
<p>Explainpaper1 helps explain the contents of papers, and Writefull2 helps with several writing tasks, including abstract and title generation.There are, of course, many other tasks where similar technologies could be helpful.</p>
<p>Systems that help researchers with literature reviews hold promising prospects.The literature review is a difficult task that can be decomposed into several sub-tasks, including retrieving relevant papers and generating a related works section that contextualizes the proposed work compared to the existing literature.It is also a task where factual correctness is essential.In that sense, it is a challenging task for current LLMs, which are known to hallucinate.Overall, creating tools to help researchers more rapidly identify, summarize and contextualize relevant prior work could significantly help the research community.</p>
<p>Recent works explore the task of literature review in parts or in full.For example, Lu et al. (2020) proposes generating the related works section of a paper using its abstract and a list of (relevant) references.Researchers also look at the whole task and build systems using LLMs like ChatGPT for literature review (Haman and Å kolnÃ­k, 2023;Huang and Tan, 2023).While these LLMs tend to generate high-quality text, they are prone to hallucinations (Athaluri et al., 2023).For example, the Galactica system was developed to reason about scientific knowledge (Taylor et al., 2022).While it outperforms contemporary models on various scientific tasks, it generates made-up content like inaccurate citations and imaginary papers. 3s a step forward, we explore retrievalaugmented-generation (RAG) to improve factual correctness (Lewis et al., 2020).The idea is to use the retrieval mechanism to obtain a relevant list of existing papers to be cited which provides relevant Figure 1: LitLLM Interface.Our system works on the Retrieval Augmented Generation (RAG) principle to generate the literature review grounded in retrieved relevant papers.User needs to provide the abstract in the textbox (in purple) and press send to get the generated related work (in red).First, the abstract is summarized into keywords (Section 3.1), which are used to query a search engine.Retrieved results are re-ranked (in blue) using the Paper Re-Ranking module (Section 3.2), which is then used as context to generate the related work (Section 3.3).Users could also provide a sentence plan (in green) according to their preference to generate a concise, readily usable literature review (See Section 3.3.2).
(Section 3.2) (Section 3.3) (Section 3.3.2) (Section 3.1)
Figure 2: Schematic diagram of the modular pipeline used in our system.In the default setup, we summarize the research abstract into a keyword query, which is used to retrieve relevant papers from an academic search engine.We use an LLM-based reranker to select the most relevant paper relative to the provided abstract.</p>
<p>Based on the re-ranked results and the user-provided summary of their work, we use an LLM-based generative model to generate the literature review, optionally controlled by a sentence plan.</p>
<p>contextual knowledge for LLM based generation.</p>
<p>LitLLM is an interactive tool to help scientists write the literature review or related work section of a scientific paper starting from a user-provided abstract (see Figure 1).The specific objectives of this work are to create a system to help users navigate through research papers and write a literature review for a given paper or project.Our main contributions are:</p>
<p>â€¢ We provide a system based on a modular pipeline that conducts a literature review based on a user-proposed abstract.</p>
<p>Related Work</p>
<p>LLMs have demonstrated significant capabilities in storing factual knowledge and achieving stateof-the-art results when fine-tuned on downstream Natural Language Processing (NLP) tasks (Lewis et al., 2020).However, they also face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes (Huang et al., 2023;Gao et al., 2023;Li et al., 2024).These limitations have motivated the development of RAG (Retrieval Augmented Generation), which incorporates knowledge from external databases to enhance the accuracy and credibility of the models, particularly for knowledge-intensive tasks (Gao et al., 2023).RAG has emerged as a promising solution to the challenges faced by LLMs.It synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases (Gao et al., 2023).This approach allows for continuous knowledge updates and integration of domainspecific information in an attempt to limit the effect of outdated knowledge.The proposed work builds upon the advancements around RAG to provide a more efficient solution for academic writing.</p>
<p>On the other hand, there has been a notable emphasis on utilizing Large Language Models (LLMs) for tasks related to information retrieval and ranking (Zhu et al., 2023).The work by Sun et al. (2023) leverages generative LLMs such as Chat-GPT and GPT-4 for relevance ranking in information retrieval, demonstrating that these models can deliver competitive results to state-of-the-art supervised methods.Pradeep et al. (2023b,a) introduce different open-source LLM for listwise zeroshot reranking, further motivating the proposed approach of using LLMs for reranking in our work.</p>
<p>The exploration of large language models (LLMs) and their zero-shot abilities has been a significant focus in recent research.For instance, one study investigated using LLMs in recommender systems, demonstrating their promising zero-shot ranking abilities, although they struggled with the order of historical interactions and position bias (Hou et al., 2023).Another study improved the zero-shot learning abilities of LLMs through instruction tuning, which led to substantial improvements in performance on unseen tasks (Wei et al., 2021).A similar approach was taken to enhance the zero-shot reasoning abilities of LLMs, with the introduction of an autonomous agent to instruct the reasoning process, resulting in significant performance boosts (Crispino et al., 2023).The application of LLMs has also been explored in the context of natural language generation (NLG) assessment, with comparative assessment found to be superior to prompt scoring (Liusie et al., 2023).In the domain of Open-Domain Question Answering (ODQA), a Self-Prompting framework was proposed to utilize the massive knowledge stored in LLMs, leading to significant improvements over previous methods (Li et al., 2022).Prompt engineering has been identified as a key technique for enhancing the abilities of LLMs, with various strategies being explored (Shi et al., 2023).4</p>
<p>Pipeline</p>
<p>Figure 2 provides an overview of the pipeline.The user provides a draft of the abstract or a research idea.We use LLM to first summarize the abstract in keywords that can be used as a query for search engines.Optionally, the users could provide relevant keywords to improve search results.This query is passed to the search engine, which retrieves relevant papers with the corresponding information, such as abstracts and open-access PDF URLs.These retrieved abstracts with the original query abstract are used as input to the other LLM Re-ranker, which provides a listwise ranking of the papers based on the relevance to the query abstract.These re-ranked abstracts with the original query are finally passed to the LLM generator, which generates the related work section of the paper.Recently, Agarwal et al. (2024) showed that prompting the LLMs with the sentence plans results in reduced hallucinations in the generation outputs.These plans contain information about the number of sentences and the citation description on each line, providing control to meet author preferences.We include this sentence-based planning in the LLM generator as part of this system.In the following, we provide more details about each of the modules.</p>
<p>Paper Retrieval Module</p>
<p>In our toolkit, we retrieve relevant papers using the Semantic Scholar API (Kinney et al., 2023) and OpenAlex API (Priem et al., 2022).Other platforms could be used, but the S2 and OpenAlex platforms are well-adapted to this use case.Combined, they constitue a large-scale academic corpus comprising 300M+ metadata records across multiple research areas, providing information about papers' metadata, authors, paper embedding, etc.The S2 Recommendations API also provides relevant papers similar to any seed paper.Figure 3 Abstract LLM based keyword summarization</p>
<p>User provided keywords</p>
<p>Seed paper for recommendation shows our system's different strategies.We describe these three settings that we use to search for references:</p>
<p>â€¢ User provides an abstract or a research idea (roughly the length of the abstract).We prompt an LLM (see Figure 4) to summarize this abstract in keywords which can be used as a search query with most APIs.â€¢ Users can optionally also provide keywords that can improve search results.This is similar (in spirit) to how researchers search for related work with a search engine.This is particularly useful in interdisciplinary research, and authors would like to include the latest research from a particular domain, which could not be captured much in the abstract.â€¢ Lastly, any seed paper the user finds relevant enough to their idea could be used with the Recommendations API from search engines to provide other closely related papers.</p>
<p>Keyword summarization prompt</p>
<p>You are a helpful research assistant who is helping with literature review of a research idea.You will be provided with an abstract of a scientific document.Your task is to summarize the abstract in max 5 keywords to search for related papers using API of academic search engine.</p>
<p><code>`Abstract: {abstract}</code>F igure 4: Prompt used to summarize the research idea by LLM to search an academic engine</p>
<p>Paper Re-Ranking Module</p>
<p>Recent efforts have explored the application of proprietary LLMs for ranking (Sun et al., 2023;Ma et al., 2023) as well as open-source models like (Pradeep et al., 2023a,b).These approaches provide a combined list of passages directly as input to the model and retrieve the re-ordered ranking list (Zhang et al., 2023).Typically, a retriever first filters top-k potential candidates, which are then re-ranked by an LLM to provide the final output list.In our work, we explore the instructional permutation generation approach (Sun et al., 2023) where the model is prompted to generate a permutation of the different papers in descending order based on the relevance to the user-provided abstract, and debate-ranking with attribution (Rahaman et al., 2024;Agarwal et al., 2024) where an LLM is prompted to (1) generate arguments for and against including the candidate paper and (2) output a final probability of including the candidate based on the arguments.Figure 5 showcases the prompt we used for LLM-based re-ranking.</p>
<p>Ranking prompt</p>
<p>You are a helpful research assistant who is helping with literature review of a research idea.You will be provided</p>
<p>Summary Generation Module</p>
<p>We explore two strategies for generation: (1) Zeroshot generation and (2) Plan-based generation, which relies on sentence plans for controllable generation, described in the following menting the generation model with an information retrieval module.The RAG principles have been subsequently used for dialogue generation in taskoriented settings (Thulke et al., 2021), code generation (Liu et al., 2020;Parvez et al., 2021) and product review generation (Kim et al., 2020).RAG drastically reduces hallucinations in the generated output (Gao et al., 2023;Tonmoy et al., 2024).</p>
<p>Our work builds upon the principles of RAG, where we retrieve the relevant papers based on the query and augment them as context for generating the literature review.This also allows the system to be grounded in the retrieved information and be updated with the latest research where the training data limits the parametric knowledge of the LLM. Figure 6 shows our system's prompt for effective Retrieval Augmented Generation (RAG).</p>
<p>Plan based generation</p>
<p>To get the best results from LLM, recent research shifts focus on designing better prompts (Prompt Engineering) including 0-shot chain-of-thought prompting (Kojima et al., 2022;Zhou et al., 2022), few-shot prompting (Brown et al., 2020) techniques, few-shot Chain-of-thought prompting (Wei et al., 2022) and in-context prompting (Li and Liang, 2021;Qin and Eisner, 2021).However, the longer context of our problem statement (query paper and multiple relevant papers) hinders the application of these techniques for response generation.</p>
<p>We utilized sentence plan-based prompting techniques drawing upon insights from the literature of traditional modular Natural Language Generation (NLG) pipelines with intermediary steps of sentence planning and surface realization (Reiter and Dale, 1997;Stent et al., 2004).These plans provide a sentence structure of the expected output, which efficiently guides the LLM in generating the literature review in a controllable fashion as demonstrated in concurrent work (Agarwal et al., 2024).Figure 7 (in Appendix) shows the prompt for planbased generation with an example template as:</p>
<p>Please generate {num_sentences} sentences in {num_words} words.Cite {cite_x} at line {line_x}.Cite {cite_y} at line {line_y}.</p>
<p>Implementation Details</p>
<p>We build our system using React, which provides a nice interface to quickly and efficiently build system demos.We query the Semantic Scholar API available through the Semantic Scholar Open Data Platform (Lo et al., 2020;Kinney et al., 2023) to search for the relevant papers.Specifically, we use the Academic Graph5 and Recommendations6 API endpoint.For OpenAlex, we query the search endpoint. 7In this work, we use OpenAI API8 to generate results for LLM using GPT-3.5-turboand GPT-4 model.At the same time, our modular pipeline allows using any LLM (proprietary or open-sourced) for different components.We also allow the end-user to sort the retrieved papers by relevance (default S2 results), citation count, or year.</p>
<p>User Experience</p>
<p>As a preliminary study, we provided access to our user interface to 5 different researchers who worked through the demo to write literature reviews and validate the system's efficacy.We also provide an example in the demo with an abstract for a quick start.Particularly, the users found the 0-shot generation to be more informative about the literature in general while the plan-based generation to be more accessible and tailored for their research paper, as also evident in our demo video. 9.Table 1 (in Appendix) shows the output-related work for a recent paper (Li et al., 2023) that was randomly chosen with a number of cited papers as 4. Our system generated an informative query Multimodal Research: Image-Text Model Interaction and retrieved relevant papers where the top recommended paper was also cited in the original paper.While zero-shot generation provides valuable insights into existing literature, plan-based generation produces a more succinct and readily usable literature review.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we introduce and describe LitLLM, a system which can generate literature reviews in a few clicks from an abstract using off-the-shelf LLMs.This LLM-powered toolkit relies on the RAG with a re-ranking strategy to generate a literature review with attribution.Our auxiliary tool allows researchers to actively search for related work based on a preliminary research idea, research proposal or even a full abstract.We present a modular pipeline that can be easily adapted to include the next generation of LLMs and other domains, such as news, by changing the source of retrieval information.</p>
<p>Given the growing impact of different LLMbased writing assistants, we are optimistic that our system may aid researchers in searching relevant papers and improve the quality of automatically generated related work sections of a paper.While our system shows promise as a helpful research assistant, we believe that their usage should be disclosed to the readers, and authors should also observe caution in eliminating any possible hallucinations.</p>
<p>In the future, we would also like to explore academic search through multiple APIs, such as Google Scholar.This work only considered abstracts of the query paper and the retrieved papers, which creates a bottleneck in effective literature review generation.With the advent of longer context LLMs, we envision our system ingesting the whole paper (potentially leveraging an efficient LLM-based PDF parser) to provide a more relevant background of the related research.We consider our approach as an initial step for building intelligent research assistants which could help academi-cians through an interactive setting (Dwivedi-Yu et al., 2022).In the following, we provide snippets of code to retrieve results from the Semantic Scholar API for both recommendation and query-based search:</p>
<h1>QUERY BASED SEARCH def query_search_s2 ( query : str , num_papers_api : int , fields : str ): rsp = requests .get (" https :// api .semanticscholar .org / graph / v1 / paper / search " , headers ={ "X -API -KEY ": S2_API_KEY }, params ={ " query ": query , " limit ": num_papers_api , " fields ": fields }) rsp .raise_for_status () results = rsp .json () # Total papers found total = results [" total "] papers = results [" data "] return papers # PAPER SEARCH def get_paper_data ( paper_url : str , fields : str ): """ Retrieves data of one paper based on URL """ rsp = requests .get (f" https :// api .semanticscholar .org / graph / v1 / paper / URL :{ paper_url }" , headers ={ "X -API -KEY ": S2_API_KEY }, params ={ " fields ": fields }) results = rsp .json () return rewsults # RECOMMENDATION API def get_recommendations_from_s2 ( arxiv_id : str , num_papers_api : int , fields : str ): """ Get recommendations from S2 API """ query_id = f" ArXiv :{ arxiv_id }" rsp = requests .post (" https :// api .semanticscholar .org / recommendations / v1 / papers /" , json ={ " positivePaperIds ": [ query_id ]} , params ={ " fields ": fields , " limit ": num_papers_api }) results = rsp .json () papers = results [" recommendedPapers " ] return papers Abstract for paper: Do DALL-E and Flamingo Understand Each Other?(Li et al., 2023) The field of multimodal research focusing on the comprehension and creation of both images and text has witnessed significant strides.This progress is exemplified by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example.An interesting question worth exploring in this domain is whether Flamingo and DALL-E understand each other.To study this question, we propose a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image.We argue that these models understand each other if the generated image is similar to the given image.Specifically, we study the relationship between the quality of the image reconstruction and that of the text generation.We find that an optimal description of an image is one that gives rise to a generated image similar to the original one.The finding motivates us to propose a unified framework to finetune the text-to-image and image-to-text models.Concretely, the reconstruction part forms a regularization loss to guide the tuning of the models  (Yu et al., 2022) [2] MAMO: Fine-Grained Vision-Language Representations Learning with Masked Multimodal Modeling (2022) Cited by 0 (Zhao et al., 2022) [3] Dynamic Modality Interaction Modeling for Image-Text Retrieval (2021) Cited by 88 (Qu et al., 2021) [4] WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning (2021) Cited by 185 (Srinivasan et al., 2021) Zero shot Generation (Section 3.3.1)The field of multimodal research has seen significant advancements in recent years, with the development of models such as Flamingo and DALL-E that focus on image captioning and text-to-image generation respectively.However, the question of whether these models can understand each other and work in harmony is a topic of interest.In this context, the work of [1] presents the Contrastive Captioner (CoCa), a model that combines contrastive loss and captioning loss to pretrain an image-text encoder-decoder foundation model.This model, while efficient, does not fully address the interaction between image and text modalities at a fine-grained level.The work of [2] addresses this gap by proposing a jointly masked multimodal modeling method that focuses on fine-grained multimodal representations.This method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover.However, this approach, while effective, does not fully address the challenges of intra-modal reasoning and cross-modal alignment that are inherent in image-text retrieval.The work of [3] presents a novel modality interaction modeling network based on the routing mechanism, which is the first unified and dynamic multimodal interaction framework towards image-text retrieval.This model can dynamically learn different activated paths for different data, providing a more flexible approach to modality interaction.However, the design of interaction patterns in this model still relies heavily on expert experience and empirical feedback, which may limit its applicability in different contexts.The work of [4] introduces the Wikipedia-based Image Text (WIT) Dataset, a large-scale dataset for multimodal, multilingual learning.This dataset, while providing a rich resource for multimodal learning, does not directly address the question of how different models can understand each other and work together.In light of these previous works, our study proposes a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image.We argue that these models understand each other if the generated image is similar to the given image.This approach allows us to study the relationship between the quality of the image reconstruction and that of the text generation, and to propose a unified framework to finetune the text-to-image and image-to-text models.Our extensive experiments validate our findings and demonstrate the effectiveness of our proposed unified framework.Plan based generation (Section 3.3.2):Generate the output using 5 sentences.Cite [1] on line 2. Cite [2], [3] on line 3. Cite [4] on line 5.The field of multimodal research has seen significant advancements in the comprehension and creation of both images and text, with models like Flamingo and DALL-E leading the way.In a similar vein, the Contrastive Captioner (CoCa) model presented in [1] pretrains an image-text encoder-decoder foundation model with contrastive loss and captioning loss, achieving state-of-the-art performance on a broad range of downstream tasks.Other works have also focused on improving the fine-grained image-text interaction, with [2] proposing a jointly masked multimodal modeling method and [3] developing a dynamic multimodal interaction framework for image-text retrieval.These models, while effective, often rely on expert experience and empirical feedback, which may limit their flexibility.The introduction of the Wikipedia-based Image Text (WIT) Dataset in [4] provides a large, multilingual dataset for pretraining multimodal models, offering a more diverse set of concepts and real-world entities for model training.This diversity and scale could potentially enhance the performance of models like Flamingo and DALL-E, further advancing the field of multimodal research.</h1>
<p>Table 1: We show an example generated related work for a randomly chosen recent paper (Li et al., 2023) with LLM summarized query and retrieved papers.We show the generated related work from our system using both zero-shot and plan-based generation, producing a more succinct and readily usable literature review.Note: The number of citations is retrieved by Semantic Scholar at the date of submission of this work.</p>
<p>Figure 3 :
3
Figure 3: Different retrieval strategies as discussed in Section 3.1</p>
<p>with an abstract or an idea of a scientific document and abstracts of some other relevant papers.Your task is to rank the papers based on the relevance to the query abstract.Provide only the ranks as [] &gt; [] &gt; [].Do not output anything apart from the ranks.Ranking prompt based on the permutation generation method</p>
<p>3.3.1 Zero-shot generation While LLMs can potentially search and generate relevant papers from their parametric memory and trained data, they are prone to hallucinating and generating non-factual content.Retrieval augmented generation, first introduced in Parvez et al. (2021) for knowledge tasks, addresses this by aug-RAG prompt You will be provided with an abstract of a scientific document and other references papers in triple quotes.Your task is to write the related work section of the document using only the provided abstracts and other references papers.Please write the related work section creating a cohesive storyline by doing a critical analysis of prior work comparing the strengths and weaknesses while also motivating the proposed approach.You should cite the other related documents as [#] whenever you are referring it in the related work.Do not write it as Reference #.Do not cite abstract.Do not include any extra notes or newline characters at the end.Do not copy the abstracts of reference papers directly but compare and contrast to the main work concisely.Do not provide the output in bullet points.Do not provide references at the end.Please cite all the provided reference papers.</p>
<p>igure 7 :
7
Prompt for sentence plan-based generation</p>
<p>. Extensive experiments on multiple datasets with different image captioning and image generation models validate our findings and demonstrate the effectiveness of our proposed unified framework.As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work.Project website: https://dalleflamingo.github.io.LLM summarized query: Multimodal Research: Image-Text Model Interaction Retrieved papers: (User Input: 4) [1] CoCa: Contrastive Captioners are Image-Text Foundation Models (2022) Cited by 702</p>
<p>https://www.explainpaper.com/
https://x.writefull.com/
see e.g., What Meta Learned from Galactica
Relevant papers Relevant papersRelevant papersRe-ranked papers Re-ranked papers
This paragraph was generated using our platform with some minor modifications based on a slightly different version of our abstract.
https://api.semanticscholar.org/api-docs/ graph
https://api.semanticscholar.org/api-docs/ recommendations
https://api.openalex.org/works
https://platform.openai.com/docs/guides/gpt
Appendix
Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2412.15249Litllms for literature review: Are we there yet?. 2024arXiv preprint</p>
<p>Exploring the boundaries of reality: Investigating the phenomenon of artificial intelligence hallucination in scientific writing through chatgpt references. Cureus, 15. Sandeep Sai Anirudh Athaluri, V Varma Manthena, Manoj S R Krishna, Vineel Kesapragada, Tirth Yarlagadda, Rama Dave, Siri Tulasi, ; Duddumpudi, B Tom, Benjamin Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Gretchen Herbert-Voss, Tom Krueger, Rewon Henighan, Aditya Child, Daniel M Ramesh, Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Litwin, 10.48550/ARXIV.2005.141652023Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Agent instructs large language models to be general zeroshot reasoners. Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, Chenguang Wang, ArXiv, abs/2310.037102023</p>
<p>Editeval: An instruction-based benchmark for text improvements. Jane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria Lomeli, Patrick Lewis, Gautier Izacard, Edouard Grave, Sebastian Riedel, Fabio Petroni, arXiv:2209.133312022arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Using chatgpt to conduct a literature review. Michael Haman, Milan Å kolnÃ­k, Accountability in Research. 2023</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, ArXiv, abs/2305.088452023</p>
<p>The role of chatgpt in scientific communication: writing better scientific review articles. Jingshan Huang, Ming Tan, American Journal of Cancer Research. 13411482023</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.05232A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 2023arXiv preprint</p>
<p>Retrieval-augmented controllable review generation. Jihyeok Kim, Seungtaek Choi, Reinald Kim Amplayo, Seung-Won Hwang, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, arXiv:2301.10140The semantic scholar open data platform. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-Tau Yih, Tim RocktÃ¤schel, Advances in Neural Information Processing Systems. 202033</p>
<p>Do dall-e and flamingo understand each other?. Hang Li, Jindong Gu, Rajat Koner, Sahand Sharifzadeh, Volker Tresp, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Self-prompting large language models for zero-shot open-domain qa. Junlong Li, Zhuosheng Zhang, Hai Zhao, 2022</p>
<p>Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, arXiv:2401.03205The dawn after the dark: An empirical study on factuality hallucination in large language models. 2024arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Retrieval-augmented generation for code summarization via hybrid gnn. Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, Yang Liu, arXiv:2006.054052020arXiv preprint</p>
<p>Adian Liusie, Potsawee Manakul, Mark John, Francis Gales, Llm comparative assessment: Zeroshot nlg evaluation through pairwise comparisons using large language models. 2023</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Multi-XScience: A large-scale dataset for extreme multidocument summarization of scientific articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/v1/2020.emnlp-main.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020</p>
<p>Zero-shot listwise document reranking with a large language model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.021562023arXiv preprint</p>
<p>Md Rizwan Parvez, Uddin Wasi, Saikat Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, arXiv:2108.11601Retrieval augmented code generation and summarization. 2021arXiv preprint</p>
<p>Rankvicuna: Zero-shot listwise document reranking with open-source large language models. Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin, arXiv:2309.150882023aarXiv preprint</p>
<p>Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin, arXiv:2312.02724Rankzephyr: Effective robust zeroshot listwise reranking is a breeze!. 2023barXiv preprint</p>
<p>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, arXiv:2205.018332022arXiv preprint</p>
<p>Learning how to ask: Querying LMs with mixtures of soft prompts. Guanghui Qin, Jason Eisner, arXiv:2104.065992021arXiv preprint</p>
<p>Dynamic modality interaction modeling for image-text retrieval. Leigang Qu, Meng Liu, Jianlong Wu, Zan Gao, Liqiang Nie, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval2021</p>
<p>Nasim Rahaman, Martin Weiss, Manuel WÃ¼thrich, Yoshua Bengio, Li Erran Li, Chris Pal, Bernhard SchÃ¶lkopf, arXiv:2403.14443Language models can reduce asymmetry in information markets. 2024arXiv preprint</p>
<p>Building applied natural language generation systems. Ehud Reiter, Robert Dale, Natural Language Engineering. 311997</p>
<p>Prompt space optimizing few-shot reasoning success with large language models. Fobo Shi, Peijun Qing, D Yang, Nan Wang, Youbo Lei, H Lu, Xiaodong Lin, ArXiv, abs/2306.037992023</p>
<p>Trainable sentence planning for complex information presentations in spoken dialog systems. Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork, 10.3115/1218955.1218966Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. Amanda Stent, Rashmi Prasad, Marilyn Walker, the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalBarcelona, Spain2021. 2004Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, arXiv:2304.09542Is chatgpt good at search? investigating large language models as re-ranking agent. 2023arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Efficient retrieval augmented generation from unstructured knowledge for taskoriented dialog. David Thulke, Nico Daheim, Christian Dugast, Hermann Ney, arXiv:2102.046432021arXiv preprint</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, V Quoc, Le, ArXiv, abs/2109.016522021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Coca: Contrastive captioners are image-text foundation models. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, Trans. Mach. Learn. Res. 2022. 2022</p>
<p>Xinyu Zhang, Sebastian HofstÃ¤tter, Patrick Lewis, Raphael Tang, Jimmy Lin, arXiv:2312.02969Rank-withoutgpt: Building gpt-independent listwise rerankers on open-source large language models. 2023arXiv preprint</p>
<p>Mamo: Finegrained vision-language representations learning with masked multimodal modeling. Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023arXiv preprintReferences: [1]: {text} [2]: {text} Plan: {plan} ``F</p>            </div>
        </div>

    </div>
</body>
</html>
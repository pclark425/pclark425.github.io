<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1080 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1080</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1080</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-b9c5bf8c367a68002a56ce1bc7eae479aa7ba14d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b9c5bf8c367a68002a56ce1bc7eae479aa7ba14d" target="_blank">R-IAC: Robust Intrinsically Motivated Exploration and Active Learning</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Autonomous Mental Development</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a novel formulation of IAC, called robust intelligent adaptive curiosity (R-IAC), and shows that its performances as an intrinsically motivated active learning algorithm are far superior to IAC in a complex sensorimotor space where only a small subspace is neither unlearnable nor trivial.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1080.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1080.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple 2-DOF arm (one-pixel camera)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated two-axis robotic arm with a one-pixel camera in a painted cubic room</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 2-DOF redundant arm whose tip carries a single-pixel camera; the agent incrementally learns the forward mapping p = V(q1,q2) using an incremental local GMR predictor while exploration is driven by intrinsic-motivation heuristics (IAC or R-IAC) or by random babbling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>2-DOF arm with one-pixel camera (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated redundant 2-DOF robotic arm. The learning algorithm is incremental regression (ILO-GMR) that learns a forward model mapping joint angles (q1,q2) to a single pixel intensity p; exploration policies compared are random babbling, IAC (learning-progress based), and R-IAC (robust IAC with probabilistic region selection, multiresolution monitoring, and an error-maximization mode).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Painted cubic room with varied wallpapers (checkerboard front wall, noisy ceiling, white walls)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A cubic room whose walls produce different sensorimotor contingencies for the one-pixel camera: (1) a front wall containing an increasing-precision checkerboard pattern (spatially-varying, graded complexity), (2) a ceiling with white noise (stochastic/unlearnable outputs), and (3) white walls/floor (trivial constant outputs). Because the arm is redundant, the same visual consequence may be produced by multiple joint configurations; this creates a sensorimotor space with interleaved trivial, learnable (graded-complexity), and unlearnable regions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by learnability as measured by prediction error and learning progress (LP = negative derivative of prediction error over a sliding window ζ). Spatial complexity is induced by the checkerboard gradient (increasing visual complexity left→right). Quantitative experiment parameters: ζ (learning-progress window) = 50, T_split = 250, exploration runs = 20,000 sensorimotor experiments per run; input space dimensionality = 2 motor dims, 1 sensory dim.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>mixed (contains low/trivial regions, medium learnable regions with graded complexity, and locally high complexity areas); overall environment purposefully heterogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation manifested as (i) stochastic variation in the noisy ceiling (unlearnable/random outputs), (ii) structured variation across the checkerboard gradient (gradual change in mapping complexity), and (iii) low variation on white walls (constant outputs). The paper operationalizes variation by presence/absence of motor→sensory correlations and by heterogeneity of subregions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>mixed (high variation/unpredictability in ceiling; medium/structured variation in checkerboard; low variation in white walls)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Generalization prediction error on a held-out test set (predict p from (q1,q2)); measured over 30 runs, error evaluated periodically (every 5000 experiments) on test queries sampled uniformly over learnable subspace (excludes noisy zone).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: R-IAC produced statistically significantly lower generalization error than IAC, and IAC was significantly better than random; experiments: 30 runs, 20,000 interactions per run (error snapshots every 5000). (No absolute numerical error values are reported in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: the paper argues and empirically demonstrates that intrinsic-motivation driven exploration based on learning progress directs exploration to subregions of intermediate and increasing complexity while avoiding trivial (low-complexity) and unlearnable (high-entropy) regions; in heterogeneous environments with small learnable subspaces amid large unlearnable/trivial areas, R-IAC locates and focuses on the learnable regions more efficiently than IAC or random exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active intrinsic-motivation driven exploration using probabilistic mixture of modes: random babbling (mode 1, p1≈0.3), learning-progress maximization (mode 2, p2≈0.6–0.7), and (for R-IAC) an error-maximization mode (mode 3, p3≈0.1). Region splitting (T_split=250) with multiresolution monitoring in R-IAC; prediction machine updated incrementally (ILO-GMR).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — generalization to held-out uniform test queries in the learnable portion of the sensorimotor space was measured: R-IAC reduced generalization error faster and to a lower level than IAC and random; inclusion of multiresolution monitoring provided clear improvement especially in early-to-mid learning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Learning curves collected over 20,000 sensorimotor experiments per run; results averaged over 30 runs; performance snapshots every 5000 interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a small simulated embodied sensorimotor system with interleaved trivial, learnable, and unlearnable zones, intrinsic-motivation active learning (IAC) outperforms random exploration; R-IAC (probabilistic selection + multiresolution LP monitoring + error-mode) outperforms IAC substantially, focusing exploration on the graded-checkerboard region and avoiding noise and trivial zones, yielding faster reduction in generalization error.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'R-IAC: Robust Intrinsically Motivated Exploration and Active Learning', 'publication_date_yy_mm': '2009-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1080.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1080.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hand-Eye-Clouds (two-arm 2D)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated two 2-D arms with an eye/camera observing either the other arm, moving clouds, or white wall (hand-eye-clouds experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated embodied agent with two planar 2-link arms (4 motor DOFs) and a camera on one arm; the camera observations can be: (i) the tip of the other arm (learnable correlated mapping), (ii) randomly moving clouds (unlearnable/stochastic), or (iii) white wall (trivial constant), producing a heterogeneous 6-D sensorimotor learning problem used to compare IAC, R-IAC, and random exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Two 2D arms with eye/camera (hand-eye-clouds simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated embodied system with 4 motor inputs (q11,q12,q21,q22) and 2 sensory outputs (x,y) when the hand is visible in the camera frame; uses incremental local GMR (ILO-GMR) to learn forward mapping E(q's)→(x,y). Exploration controlled by random, IAC, or R-IAC policies as in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hand-eye-clouds environment (white wall, window with clouds, and potential hand-in-view region)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>The camera's field of view changes with right-arm configuration: when looking at the left arm tip, motor→sensory mapping is structured and learnable; when looking at a ‘window’ area, outputs are random clouds (stochastic/unlearnable); much of the joint-space corresponds to looking at white wall (trivial constant output). The learnable region is small compared to the large unlearnable and trivial subspaces, creating a high-dimensional, inhomogeneous sensorimotor learning challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity characterized by the size and learnability of subregions: mapping dimensionality = 4 motor + 2 sensory = 6-D joint sensorimotor relation. Measured by prediction/generalization error and by empirical statistics such as distribution of distances between eye center and hand tip (distance used as proxy for whether camera sees hand). Experiment parameters: T_split=250, ζ=50 (learning-progress window), exploration length = 100,000 experiments per run, averaged over 30 simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high overall (because the learnable subset is small and embedded in large unlearnable/trivial regions), i.e., heterogeneous high complexity</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is operationalized by stochastic outputs when observing clouds (high-output randomness), deterministic constant outputs on white wall (low variation), and structured deterministic mapping when viewing the other arm (structured variation). The paper also quantifies exploration distribution via the histogram of eye-hand distances.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (predominantly due to large stochastic cloud region and heterogeneity across subregions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Multiple metrics: (1) generalization prediction error on a test set of 1,000 random motor configurations where the hand is within the field of view, and (2) control performance measured as Euclidean distance between reached hand position and visual target when using the learnt forward model to infer inverse control (via ILO-GMR SLSE approach). Also measured time spent with eye-hand distance < 8 (distributional statistic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantitative/qualitative reported: R-IAC spends approximately 3× more time with eye-hand distance < 8 compared to IAC and random (i.e., spends three times more time exploring the interesting 'hand-in-view' region). In terms of learning/control performance: R-IAC yields clearly better prediction and control performance than IAC and random over 100,000 interactions (curves averaged over 30 runs); exact numerical prediction/control-error values are shown in figures but not reported as numeric values in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: authors emphasize that when only a small subspace is interesting (learnable) and large parts of the space are unlearnable or trivial, exploration heuristics that maximize learning progress and incorporate robustness (R-IAC) locate and concentrate sampling on the small learnable region more effectively than heuristics focusing on uncertainty. They argue that uncertainty-maximization alone would waste resources in unlearnable regions; R-IAC trades off exploration by probabilistically sampling regions according to learning progress and by multiresolution monitoring to avoid over-focusing on spurious small regions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active intrinsic-motivation exploration with R-IAC/IAC: random babbling (p1≈0.3), learning-progress driven region selection (p2≈0.6 for R-IAC, ≈0.7 for IAC), and error-maximization exploration mode (p3≈0.1 in R-IAC). Incremental learning via ILO-GMR (k=2, N=100). 100,000 interactions per run; results averaged over 30 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — evaluated on a held-out test set of 1,000 motor configurations with the hand in view: R-IAC achieved lower prediction error and superior control performance (smaller Euclidean control errors) than IAC and random; R-IAC also allocated substantially more sampling budget to the learnable region (3× more time with eye-hand distance <8).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experiments used 100,000 sensorimotor trials per run; statistics averaged over 30 simulation runs; R-IAC achieved superior generalization/control performance within that interaction budget compared to IAC and random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a higher-dimensional, highly heterogeneous embodied sensorimotor environment with a small interesting subspace embedded in large unlearnable/trivial regions, R-IAC is markedly more effective than IAC and random exploration: it allocates far more interactions to the learnable region (empirically ~3× more), achieves lower generalization error on predicting hand-in-view positions, and produces better inverse-control results when the learnt forward model is reused for control. The paper highlights the trade-off that naive uncertainty-driven exploration would sample unlearnable stochastic zones, whereas learning-progress-driven and robust selection strategies avoid such waste.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'R-IAC: Robust Intrinsically Motivated Exploration and Active Learning', 'publication_date_yy_mm': '2009-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1080.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1080.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R-IAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust Intelligent Adaptive Curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-motivation active-learning algorithm that directs exploration towards sensorimotor subregions where the learner's forward model shows maximal learning progress; R-IAC improves on IAC by probabilistic region selection, multiresolution monitoring of learning progress, a split mechanism that maximizes LP dissimilarity, and an optional error-maximization exploration mode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R-IAC (intrinsic-motivation exploration controller)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Algorithmic embodied-learning controller that chooses motor experiments online: measures prediction error e(t+1) from a prediction machine (incremental regressor, e.g., ILO-GMR), computes learning progress LP as negative derivative of prediction error over a sliding window ζ, stores exemplars in recursively split regions, selects regions probabilistically proportional to LP (multiresolution), and executes actions sampled within selected regions; includes a mode that selects actions expected to maximize prediction error (active sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>algorithmic exploration strategy applied to embodied agents (used with simulated robotic agents in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General unprepared sensorimotor spaces with interleaved learnable, trivial, and unlearnable subregions (applied to the two experiments above)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>R-IAC is evaluated in sensorimotor environments that are unprepared by the experimenter and contain large unlearnable subspaces (stochastic outputs), trivial subspaces (constant outputs), and small learnable subspaces (structured deterministic mappings). The method is intended for embodied systems with high-dimensional sensors and motors where only parts of the space are learnable.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity is operationalized via learnability: prediction error magnitude and its temporal derivative (learning progress). Region splitting uses T_split exemplars, learning-progress window ζ (e.g., ζ=50), and a split quality metric maximizing (LP_child1 - LP_child2)^2. Complexity of environment also assessed by proportion of sensorimotor space that is learnable vs unlearnable/trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>targeted at environments with heterogeneous complexity; particularly effective when the overall space contains a small learnable subspace embedded in large unlearnable/trivial regions (i.e., high apparent complexity but small actual learnable fraction).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is treated as heterogeneity of subregions (stochastic vs deterministic mappings) and is implicitly measured by how learning progress differs across regions. Split machine aims to separate regions with differing LP trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>designed to handle high variation (large unlearnable/stochastic regions) through LP-based avoidance and probabilistic selection; multiresolution monitoring reduces sensitivity to over-splitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Measured by downstream prediction/generalization error of the prediction machine (ILO-GMR) on held-out data, distributional statistics of exploration (e.g., fraction of time spent in learnable region, eye-hand distance histograms), and ultimately control error when the forward model is reused for inverse control.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Empirical results: R-IAC yields statistically significantly lower generalization error than IAC and random in the 2-DOF experiment (20k interactions) and achieves markedly better allocation of exploration and lower prediction/control errors in the hand-eye-clouds experiment (100k interactions); e.g., spends ≈3× more time in the learnable region (eye-hand distance < 8) than IAC/random. Exact numeric error curves are presented in figures but not tabulated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly frames LP-driven exploration as a mechanism to regulate exploration so that agents focus on subregions of intermediate/increasing complexity and avoid both trivial and unlearnable regions; it argues that heuristics maximizing uncertainty would push sampling into unlearnable (high-entropy) zones and thus be inefficient in such heterogeneous environments; R-IAC mitigates this by probabilistic LP selection and multiresolution LP estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active intrinsic-motivation driven sampling (mixture of random babbling, LP-based probabilistic selection across multiresolution regions, and an error-maximization sampling mode), incremental updating of the prediction machine (ILO-GMR), and recursive region splitting driven by LP dissimilarity.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>When used to guide exploration of embodied simulators, R-IAC leads to faster and better generalization of the learned forward models than IAC or random exploration; multiresolution monitoring further improves early performance. The method also yields better control when learned models are reused for inverse control.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated improvements in sample efficiency in the paper's experiments (20k and 100k interaction budgets respectively); R-IAC concentrates samples on informative learnable regions, achieving lower errors for the same number of interactions compared with baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>R-IAC's key contributions are (1) probabilistic region selection proportional to learning progress (avoids winner-take-all brittleness), (2) multiresolution LP monitoring (retains parent regions so LP is tracked at multiple scales), (3) region splitting optimized for LP dissimilarity, and (4) an auxiliary error-maximization mode. These modifications make R-IAC more robust to poor splits and far more efficient at discovering and exploiting small learnable subspaces in environments with large unlearnable or trivial regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'R-IAC: Robust Intrinsically Motivated Exploration and Active Learning', 'publication_date_yy_mm': '2009-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intrinsic motivation systems for autonomous mental development <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated machines <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated learning of hierarchical collections of skills <em>(Rating: 1)</em></li>
                <li>Discovering communication <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1080",
    "paper_id": "paper-b9c5bf8c367a68002a56ce1bc7eae479aa7ba14d",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Simple 2-DOF arm (one-pixel camera)",
            "name_full": "Simulated two-axis robotic arm with a one-pixel camera in a painted cubic room",
            "brief_description": "A simulated 2-DOF redundant arm whose tip carries a single-pixel camera; the agent incrementally learns the forward mapping p = V(q1,q2) using an incremental local GMR predictor while exploration is driven by intrinsic-motivation heuristics (IAC or R-IAC) or by random babbling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "2-DOF arm with one-pixel camera (simulated)",
            "agent_description": "A simulated redundant 2-DOF robotic arm. The learning algorithm is incremental regression (ILO-GMR) that learns a forward model mapping joint angles (q1,q2) to a single pixel intensity p; exploration policies compared are random babbling, IAC (learning-progress based), and R-IAC (robust IAC with probabilistic region selection, multiresolution monitoring, and an error-maximization mode).",
            "agent_type": "simulated agent",
            "environment_name": "Painted cubic room with varied wallpapers (checkerboard front wall, noisy ceiling, white walls)",
            "environment_description": "A cubic room whose walls produce different sensorimotor contingencies for the one-pixel camera: (1) a front wall containing an increasing-precision checkerboard pattern (spatially-varying, graded complexity), (2) a ceiling with white noise (stochastic/unlearnable outputs), and (3) white walls/floor (trivial constant outputs). Because the arm is redundant, the same visual consequence may be produced by multiple joint configurations; this creates a sensorimotor space with interleaved trivial, learnable (graded-complexity), and unlearnable regions.",
            "complexity_measure": "Characterized by learnability as measured by prediction error and learning progress (LP = negative derivative of prediction error over a sliding window ζ). Spatial complexity is induced by the checkerboard gradient (increasing visual complexity left→right). Quantitative experiment parameters: ζ (learning-progress window) = 50, T_split = 250, exploration runs = 20,000 sensorimotor experiments per run; input space dimensionality = 2 motor dims, 1 sensory dim.",
            "complexity_level": "mixed (contains low/trivial regions, medium learnable regions with graded complexity, and locally high complexity areas); overall environment purposefully heterogeneous",
            "variation_measure": "Variation manifested as (i) stochastic variation in the noisy ceiling (unlearnable/random outputs), (ii) structured variation across the checkerboard gradient (gradual change in mapping complexity), and (iii) low variation on white walls (constant outputs). The paper operationalizes variation by presence/absence of motor→sensory correlations and by heterogeneity of subregions.",
            "variation_level": "mixed (high variation/unpredictability in ceiling; medium/structured variation in checkerboard; low variation in white walls)",
            "performance_metric": "Generalization prediction error on a held-out test set (predict p from (q1,q2)); measured over 30 runs, error evaluated periodically (every 5000 experiments) on test queries sampled uniformly over learnable subspace (excludes noisy zone).",
            "performance_value": "Qualitative: R-IAC produced statistically significantly lower generalization error than IAC, and IAC was significantly better than random; experiments: 30 runs, 20,000 interactions per run (error snapshots every 5000). (No absolute numerical error values are reported in text.)",
            "complexity_variation_relationship": "Explicit: the paper argues and empirically demonstrates that intrinsic-motivation driven exploration based on learning progress directs exploration to subregions of intermediate and increasing complexity while avoiding trivial (low-complexity) and unlearnable (high-entropy) regions; in heterogeneous environments with small learnable subspaces amid large unlearnable/trivial areas, R-IAC locates and focuses on the learnable regions more efficiently than IAC or random exploration.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active intrinsic-motivation driven exploration using probabilistic mixture of modes: random babbling (mode 1, p1≈0.3), learning-progress maximization (mode 2, p2≈0.6–0.7), and (for R-IAC) an error-maximization mode (mode 3, p3≈0.1). Region splitting (T_split=250) with multiresolution monitoring in R-IAC; prediction machine updated incrementally (ILO-GMR).",
            "generalization_tested": true,
            "generalization_results": "Yes — generalization to held-out uniform test queries in the learnable portion of the sensorimotor space was measured: R-IAC reduced generalization error faster and to a lower level than IAC and random; inclusion of multiresolution monitoring provided clear improvement especially in early-to-mid learning.",
            "sample_efficiency": "Learning curves collected over 20,000 sensorimotor experiments per run; results averaged over 30 runs; performance snapshots every 5000 interactions.",
            "key_findings": "In a small simulated embodied sensorimotor system with interleaved trivial, learnable, and unlearnable zones, intrinsic-motivation active learning (IAC) outperforms random exploration; R-IAC (probabilistic selection + multiresolution LP monitoring + error-mode) outperforms IAC substantially, focusing exploration on the graded-checkerboard region and avoiding noise and trivial zones, yielding faster reduction in generalization error.",
            "uuid": "e1080.0",
            "source_info": {
                "paper_title": "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning",
                "publication_date_yy_mm": "2009-10"
            }
        },
        {
            "name_short": "Hand-Eye-Clouds (two-arm 2D)",
            "name_full": "Simulated two 2-D arms with an eye/camera observing either the other arm, moving clouds, or white wall (hand-eye-clouds experiment)",
            "brief_description": "A simulated embodied agent with two planar 2-link arms (4 motor DOFs) and a camera on one arm; the camera observations can be: (i) the tip of the other arm (learnable correlated mapping), (ii) randomly moving clouds (unlearnable/stochastic), or (iii) white wall (trivial constant), producing a heterogeneous 6-D sensorimotor learning problem used to compare IAC, R-IAC, and random exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Two 2D arms with eye/camera (hand-eye-clouds simulated)",
            "agent_description": "Simulated embodied system with 4 motor inputs (q11,q12,q21,q22) and 2 sensory outputs (x,y) when the hand is visible in the camera frame; uses incremental local GMR (ILO-GMR) to learn forward mapping E(q's)→(x,y). Exploration controlled by random, IAC, or R-IAC policies as in the paper.",
            "agent_type": "simulated agent",
            "environment_name": "Hand-eye-clouds environment (white wall, window with clouds, and potential hand-in-view region)",
            "environment_description": "The camera's field of view changes with right-arm configuration: when looking at the left arm tip, motor→sensory mapping is structured and learnable; when looking at a ‘window’ area, outputs are random clouds (stochastic/unlearnable); much of the joint-space corresponds to looking at white wall (trivial constant output). The learnable region is small compared to the large unlearnable and trivial subspaces, creating a high-dimensional, inhomogeneous sensorimotor learning challenge.",
            "complexity_measure": "Complexity characterized by the size and learnability of subregions: mapping dimensionality = 4 motor + 2 sensory = 6-D joint sensorimotor relation. Measured by prediction/generalization error and by empirical statistics such as distribution of distances between eye center and hand tip (distance used as proxy for whether camera sees hand). Experiment parameters: T_split=250, ζ=50 (learning-progress window), exploration length = 100,000 experiments per run, averaged over 30 simulations.",
            "complexity_level": "high overall (because the learnable subset is small and embedded in large unlearnable/trivial regions), i.e., heterogeneous high complexity",
            "variation_measure": "Variation is operationalized by stochastic outputs when observing clouds (high-output randomness), deterministic constant outputs on white wall (low variation), and structured deterministic mapping when viewing the other arm (structured variation). The paper also quantifies exploration distribution via the histogram of eye-hand distances.",
            "variation_level": "high (predominantly due to large stochastic cloud region and heterogeneity across subregions)",
            "performance_metric": "Multiple metrics: (1) generalization prediction error on a test set of 1,000 random motor configurations where the hand is within the field of view, and (2) control performance measured as Euclidean distance between reached hand position and visual target when using the learnt forward model to infer inverse control (via ILO-GMR SLSE approach). Also measured time spent with eye-hand distance &lt; 8 (distributional statistic).",
            "performance_value": "Quantitative/qualitative reported: R-IAC spends approximately 3× more time with eye-hand distance &lt; 8 compared to IAC and random (i.e., spends three times more time exploring the interesting 'hand-in-view' region). In terms of learning/control performance: R-IAC yields clearly better prediction and control performance than IAC and random over 100,000 interactions (curves averaged over 30 runs); exact numerical prediction/control-error values are shown in figures but not reported as numeric values in text.",
            "complexity_variation_relationship": "Explicit: authors emphasize that when only a small subspace is interesting (learnable) and large parts of the space are unlearnable or trivial, exploration heuristics that maximize learning progress and incorporate robustness (R-IAC) locate and concentrate sampling on the small learnable region more effectively than heuristics focusing on uncertainty. They argue that uncertainty-maximization alone would waste resources in unlearnable regions; R-IAC trades off exploration by probabilistically sampling regions according to learning progress and by multiresolution monitoring to avoid over-focusing on spurious small regions.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active intrinsic-motivation exploration with R-IAC/IAC: random babbling (p1≈0.3), learning-progress driven region selection (p2≈0.6 for R-IAC, ≈0.7 for IAC), and error-maximization exploration mode (p3≈0.1 in R-IAC). Incremental learning via ILO-GMR (k=2, N=100). 100,000 interactions per run; results averaged over 30 runs.",
            "generalization_tested": true,
            "generalization_results": "Yes — evaluated on a held-out test set of 1,000 motor configurations with the hand in view: R-IAC achieved lower prediction error and superior control performance (smaller Euclidean control errors) than IAC and random; R-IAC also allocated substantially more sampling budget to the learnable region (3× more time with eye-hand distance &lt;8).",
            "sample_efficiency": "Experiments used 100,000 sensorimotor trials per run; statistics averaged over 30 simulation runs; R-IAC achieved superior generalization/control performance within that interaction budget compared to IAC and random.",
            "key_findings": "In a higher-dimensional, highly heterogeneous embodied sensorimotor environment with a small interesting subspace embedded in large unlearnable/trivial regions, R-IAC is markedly more effective than IAC and random exploration: it allocates far more interactions to the learnable region (empirically ~3× more), achieves lower generalization error on predicting hand-in-view positions, and produces better inverse-control results when the learnt forward model is reused for control. The paper highlights the trade-off that naive uncertainty-driven exploration would sample unlearnable stochastic zones, whereas learning-progress-driven and robust selection strategies avoid such waste.",
            "uuid": "e1080.1",
            "source_info": {
                "paper_title": "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning",
                "publication_date_yy_mm": "2009-10"
            }
        },
        {
            "name_short": "R-IAC",
            "name_full": "Robust Intelligent Adaptive Curiosity",
            "brief_description": "An intrinsic-motivation active-learning algorithm that directs exploration towards sensorimotor subregions where the learner's forward model shows maximal learning progress; R-IAC improves on IAC by probabilistic region selection, multiresolution monitoring of learning progress, a split mechanism that maximizes LP dissimilarity, and an optional error-maximization exploration mode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "R-IAC (intrinsic-motivation exploration controller)",
            "agent_description": "Algorithmic embodied-learning controller that chooses motor experiments online: measures prediction error e(t+1) from a prediction machine (incremental regressor, e.g., ILO-GMR), computes learning progress LP as negative derivative of prediction error over a sliding window ζ, stores exemplars in recursively split regions, selects regions probabilistically proportional to LP (multiresolution), and executes actions sampled within selected regions; includes a mode that selects actions expected to maximize prediction error (active sampling).",
            "agent_type": "algorithmic exploration strategy applied to embodied agents (used with simulated robotic agents in this paper)",
            "environment_name": "General unprepared sensorimotor spaces with interleaved learnable, trivial, and unlearnable subregions (applied to the two experiments above)",
            "environment_description": "R-IAC is evaluated in sensorimotor environments that are unprepared by the experimenter and contain large unlearnable subspaces (stochastic outputs), trivial subspaces (constant outputs), and small learnable subspaces (structured deterministic mappings). The method is intended for embodied systems with high-dimensional sensors and motors where only parts of the space are learnable.",
            "complexity_measure": "Complexity is operationalized via learnability: prediction error magnitude and its temporal derivative (learning progress). Region splitting uses T_split exemplars, learning-progress window ζ (e.g., ζ=50), and a split quality metric maximizing (LP_child1 - LP_child2)^2. Complexity of environment also assessed by proportion of sensorimotor space that is learnable vs unlearnable/trivial.",
            "complexity_level": "targeted at environments with heterogeneous complexity; particularly effective when the overall space contains a small learnable subspace embedded in large unlearnable/trivial regions (i.e., high apparent complexity but small actual learnable fraction).",
            "variation_measure": "Variation is treated as heterogeneity of subregions (stochastic vs deterministic mappings) and is implicitly measured by how learning progress differs across regions. Split machine aims to separate regions with differing LP trajectories.",
            "variation_level": "designed to handle high variation (large unlearnable/stochastic regions) through LP-based avoidance and probabilistic selection; multiresolution monitoring reduces sensitivity to over-splitting.",
            "performance_metric": "Measured by downstream prediction/generalization error of the prediction machine (ILO-GMR) on held-out data, distributional statistics of exploration (e.g., fraction of time spent in learnable region, eye-hand distance histograms), and ultimately control error when the forward model is reused for inverse control.",
            "performance_value": "Empirical results: R-IAC yields statistically significantly lower generalization error than IAC and random in the 2-DOF experiment (20k interactions) and achieves markedly better allocation of exploration and lower prediction/control errors in the hand-eye-clouds experiment (100k interactions); e.g., spends ≈3× more time in the learnable region (eye-hand distance &lt; 8) than IAC/random. Exact numeric error curves are presented in figures but not tabulated in the text.",
            "complexity_variation_relationship": "Yes — the paper explicitly frames LP-driven exploration as a mechanism to regulate exploration so that agents focus on subregions of intermediate/increasing complexity and avoid both trivial and unlearnable regions; it argues that heuristics maximizing uncertainty would push sampling into unlearnable (high-entropy) zones and thus be inefficient in such heterogeneous environments; R-IAC mitigates this by probabilistic LP selection and multiresolution LP estimation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active intrinsic-motivation driven sampling (mixture of random babbling, LP-based probabilistic selection across multiresolution regions, and an error-maximization sampling mode), incremental updating of the prediction machine (ILO-GMR), and recursive region splitting driven by LP dissimilarity.",
            "generalization_tested": true,
            "generalization_results": "When used to guide exploration of embodied simulators, R-IAC leads to faster and better generalization of the learned forward models than IAC or random exploration; multiresolution monitoring further improves early performance. The method also yields better control when learned models are reused for inverse control.",
            "sample_efficiency": "Demonstrated improvements in sample efficiency in the paper's experiments (20k and 100k interaction budgets respectively); R-IAC concentrates samples on informative learnable regions, achieving lower errors for the same number of interactions compared with baselines.",
            "key_findings": "R-IAC's key contributions are (1) probabilistic region selection proportional to learning progress (avoids winner-take-all brittleness), (2) multiresolution LP monitoring (retains parent regions so LP is tracked at multiple scales), (3) region splitting optimized for LP dissimilarity, and (4) an auxiliary error-maximization mode. These modifications make R-IAC more robust to poor splits and far more efficient at discovering and exploiting small learnable subspaces in environments with large unlearnable or trivial regions.",
            "uuid": "e1080.2",
            "source_info": {
                "paper_title": "R-IAC: Robust Intrinsically Motivated Exploration and Active Learning",
                "publication_date_yy_mm": "2009-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intrinsic motivation systems for autonomous mental development",
            "rating": 2
        },
        {
            "paper_title": "Intrinsically motivated machines",
            "rating": 2
        },
        {
            "paper_title": "Intrinsically motivated learning of hierarchical collections of skills",
            "rating": 1
        },
        {
            "paper_title": "Discovering communication",
            "rating": 1
        }
    ],
    "cost": 0.019403749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>R-IAC: Robust Intrinsically Motivated Exploration and Active Learning</h1>
<p>Adrien Baranès and Pierre-Yves Oudeyer</p>
<h4>Abstract</h4>
<p>Intelligent adaptive curiosity (IAC) was initially introduced as a developmental mechanism allowing a robot to self-organize developmental trajectories of increasing complexity without preprogramming the particular developmental stages. In this paper, we argue that IAC and other intrinsically motivated learning heuristics could be viewed as active learning algorithms that are particularly suited for learning forward models in unprepared sensorimotor spaces with large unlearnable subspaces. Then, we introduce a novel formulation of IAC, called robust intelligent adaptive curiosity (R-IAC), and show that its performances as an intrinsically motivated active learning algorithm are far superior to IAC in a complex sensorimotor space where only a small subspace is neither unlearnable nor trivial. We also show results in which the learnt forward model is reused in a control scheme. Finally, an open source accompanying software containing these algorithms as well as tools to reproduce all the experiments presented in this paper is made publicly available.</p>
<p>Index Terms-Active learning, artificial curiosity, developmental robotics, exploration, intrinsic motivation, sensorimotor learning.</p>
<h2>I. Intrinsically Motivated Exploration and Learning</h2>
<p>DEVELOPMENTAL robotics approaches are studying mechanisms that may allow a robot to continuously discover and learn new skills in unknown environments and in a life long time scale [1], [2]. A main aspect is the fact that the set of these skills and their functions are at least partially unknown to the engineer who conceived the robot initially, and are also task independent. Indeed, a desirable feature is that robots should be capable of exploring and developing various kinds of skills that they may reuse later on for tasks that they did not foresee. This is what happens in human children, and this is also why developmental robotics shall import concepts and mechanisms from human developmental psychology.</p>
<h2>A. The Problem of Exploration in Open-Ended Learning</h2>
<p>Like children, the "freedom" that is given to developmental robots to learn an open set of skills also poses a very important problem: as soon as the set of motors and sensors is rich enough, the set of potential skills becomes extremely large and complicated. This means that on the one hand, it is impossible to try to learn all skills that may potentially be learned because there is not enough time to physically practice all of them. Furthermore, there are many skills or goals that the child/robot could</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>imagine, but never be actually learnable, because they are either too difficult or just not possible (i.e., trying to learn to control the weather by producing gestures is hopeless). This kind of problem is not at all typical of the existing work in machine learning, where usually the "space" and the associated "skills" to be learned and explored are well prepared by a human engineer. For example, when learning hand-eye coordination in robots, the right input and output spaces (e.g., arm joint parameters and visual position of the hand) are typically provided, as well as the fact that hand-eye coordination is an interesting skill to learn. But a developmental robot is not supposed to be provided with the right subspaces of its rich sensorimotor space and with their association with appropriate skills: it would, for example, have to discover that arm joint parameters and visual position of the hand are related in the context of a certain skill (which we call hand-eye coordination, but which it has to conceptualize by itself) and in the middle of a complex flow of values in a richer set of sensations and actions.</p>
<h2>B. Intrinsic Motivations</h2>
<p>Developmental robots, like humans, have a sharp need for mechanisms that may drive and self-organize the exploration of new skills, as well as identify and organize useful subspaces in its complex sensorimotor experiences. Psychologists have identified two broad families of guidance mechanisms which drive exploration in children.</p>
<p>1) Social learning, which exists in different forms such as stimulus enhancement, emulation, imitation, or demonstration, and which many groups try to implement in robots (e.g., [3]-[14]).
2) Internal guiding mechanisms, also studied by many robotics research groups (e.g., see [15]-[20]) and in particular intrinsic motivation, responsible of spontaneous exploration and curiosity in humans, which is the mechanism underlying the algorithms presented in this paper.
Intrinsic motivations are mechanisms that guide curiosity driven exploration, that were initially studied in psychology [21]-[23] and are now also being approached in neuroscience [24]-[26]. Machine learning and robotics researchers have proposed that such mechanism might be crucial for self-organizing developmental trajectories, as well as for guiding the learning of general and reusable skills in machines and robots [27], [28]. A large diversity of approaches for operationalizing intrinsic motivation have been presented in the literature (e.g., [27]-[35]), and see [27] for a general overview. Several experiments have been conducted in real world robotic setups, such as in [27], [34], [36] where an intrinsic motivation system was shown to allow for the progressive discovery of skills of</p>
<p>increasing complexity, such as reaching, biting, and simple vocal imitation with an AIBO robot. In these experiments, the focus was on the study of how developmental stages could self-organize into a developmental trajectory without a direct prespecification of these stages and their number.</p>
<p>This paper aims to propose a new version of the intelligent adaptive curiosity algorithm (IAC) presented in [27], called robust intelligent adaptive curiosity (R-IAC), and to show that it can be used as an efficient active learning algorithm to learn forward and inverse models in a complex unprepared sensorimotor space with unlearnable subspaces. Furthermore, together with the complete pseudo-code, we provide access to accompanying publicly available open source software that implements the algorithm and contains tools to reproduce all the experiments presented in this paper.</p>
<h2>II. R-IAC as Active Learning</h2>
<h2>A. Developmental Active Learning</h2>
<p>In IAC, intrinsic motivation is implemented as a heuristics, which pushes a robot to explore sensorimotor activities for which learning progress is maximal, i.e., subregions of the sensorimotor space where the predictions of the learnt forward model improve fastest [27]. Thus, this mechanism actively regulates the growth of complexity in sensorimotor exploration, and can be conceptualized as a developmental active learning algorithm. This heuristics shares properties with statistical techniques in optimal experiment design (e.g., [37]) where exploration is driven by expected information gain, as well as with attention and motivation mechanisms proposed in the developmental psychology literature (e.g., [22] and [38], or see [23] for a review) where it has been proposed that exploration is preferentially focused on activities of intermediate difficulty or novelty [39], [40], but differs significantly from many active learning heuristics in machine learning in which exploration is directed towards regions where the learned model is maximally uncertain or where predictions are maximally wrong (e.g., [41], [42], and see [27] for a review). As argued in [27], developmental robots are typically faced with large sensorimotor spaces which cannot be entirely learned (because of time limits among other reasons), and/or in which subregions are not learnable (potentially because it is too complicated for the learner, or because there are no correlations between the input and output variables, see examples in the experiment section and in [27]). In these sensorimotor spaces, exploring zones of maximal uncertainty or unpredictability is bound to be an inefficient strategy since it would direct exploration towards subspaces in which there are no learnable correlations, while a heuristics based on learning progress, allows to avoid unlearnable parts, as well as to focus exploration on zones of gradually increasing complexity.</p>
<p>In [27] and [34], experiments showed how IAC allowed an AIBO robot, equipped with a set of parameterized motor primitives (in a 5 DOF motor space), as well as a set of perceptual primitives (in a 3 DOF perceptual space), to self-organize a developmental trajectory in which a variety of affordances uses of the motor primitives were learned in spite of not having been specified initially. In [36], this system allowed an AIBO robot,
equipped with parameterized central pattern generators (CPG's) in a 24 DOF motor space and 3 DOF perceptual space, to learn a variety of locomotion skills. Yet, these previous results focused on qualitative properties of the self-organized developmental trajectories, and IAC was not optimized for efficient active learning per se.</p>
<p>Here, we present a novel formulation of IAC, called R-IAC, and show that it can efficiently allow a robot to learn actively, fast, and correctly forward and inverse kinematic models in an unprepared sensorimotor space. As we will explain, R-IAC introduces four main advances compared to IAC.</p>
<p>1) Probabilistic action selection: instead of choosing actions to explore the zone of maximal learning progress at a given moment in time (except in the random action selection mode), R-IAC explores actions on sensorimotor subregions probabilistically chosen based on their individual learning progress.
2) Multiresolution monitoring of learning progress: in R-IAC, when sensorimotor regions are split into subregions, parent regions are kept, and one continues to monitor learning progress in them, and they continue to be eligible regions for action selection. As a consequence, learning progress is monitored simultaneously at various regions scales, as opposed to IAC where it was monitored only in child regions and thus, at increasing small scales.
3) A new region splitting mechanism that is based on the direct optimization of learning progress dissimilarity among regions.
4) The introduction of a third exploration mode hybridizing learning progress heuristics with more classic heuristics based on the exploration of zones of maximal unpredictability.</p>
<h2>B. Prediction Machine and Analysis of Error Rate</h2>
<p>We consider a robot as a system with motor/actions channels $\mathbf{M}$ and sensory/state channels $\mathbf{S} . \mathbf{M}$ and $\mathbf{S}$ can be low level such as torque motor values or touch sensor values, or higher level such as a "go forward one meter" motor command or "face detected" visual sensor. Furthermore, $\mathbf{S}$ can correspond to internal sensors measuring the internal state of the robot or encoding past values of the sensors. Real valued action/motor parameters are represented as a vector $\mathbf{M}(\mathbf{t})$, and sensors, as $\mathbf{S}(\mathbf{t})$, at a time $t$. $\mathbf{S M}(\mathbf{t})$ represents a sensorimotor context, i.e., the concatenation of both motors and sensors vectors.</p>
<p>We also consider a prediction machine (PM) (Fig. 1), as a system based on a learning algorithm (neural networks, KNN, etc.), which is able to create a forward model of a sensorimotor space based on learning examples collected through self-determined sensorimotor experiments. Experiments are defined as series of actions, and consideration of sensations detected after actions are performed.</p>
<p>An experiment is represented by the set $(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}))$, and denotes the sensory/state consequence $\mathbf{S}(\mathbf{t}+\mathbf{1})$ that is observed when actions encoded in $\mathbf{M}(\mathbf{t})$ are performed in the sensory/state context $\mathbf{S}(\mathbf{t})$. This set is called a "learning exemplar." After each trial, the PM gets this data and incrementally updates the forward model that it is encoding, i.e., the robot incrementally increases its knowledge of the sensorimotor space. In</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The prediction learning machine (e.g., a neural network, an SVM, or Gaussian process regression based algorithm).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Internal mechanism of the prediction analysis machine (PAM) associated to a given subregion $\boldsymbol{R}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}$ of the sensorimotor space. This module considers errors detected in prediction by the PM, and returns a value representative of the learning progress in the region. Learning progress is the derivative of errors computed over the last $\zeta$ exemplars collected in the subregion $\boldsymbol{R}</em>$.
this update process, PM is able to compare, for a given context $\mathbf{S M}(\boldsymbol{t})$, differences between predicted sensations $\hat{\mathbf{S}}(\boldsymbol{t}+\mathbf{1})$ (estimated using the created model), and real consequences $\mathbf{S}(\boldsymbol{t}+\mathbf{1})$. It is then able to produce a measure of error $\boldsymbol{e}(\boldsymbol{t}+\mathbf{1})$, which represents the quality of the model for sensorimotor context $\mathbf{S M}(\boldsymbol{t})$. This is summarized in Fig. 1.}</p>
<p>Then, we consider a module able to analyze learning progress over time, for a given subregion $\boldsymbol{R}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}$ of the sensorimotor space SM. This system, called prediction analysis machine (PAM) (Fig. 2) considers the set $\boldsymbol{E}</em>}}$ of all the $\mathbf{m}$ experimented exemplars $\operatorname{Ex<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}=\left(\mathbf{S M}</em>}}(\mathbf{t}), \mathbf{S<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}(\mathbf{t}+\mathbf{1}), \boldsymbol{e}</em>$, sorted by their execution order (from the older to the last performed)}}(\boldsymbol{t}+\mathbf{1})\right), i \in[1: m]$ collected inside $\boldsymbol{R}_{\boldsymbol{n}</p>
<p>$$
\begin{aligned}
\boldsymbol{E}<em _mathbf_1="\mathbf{1">{\boldsymbol{n}}= &amp; \left{\operatorname{Ex}</em>}}, \operatorname{Ex<em _boldsymbol_m="\boldsymbol{m">{\mathbf{2}}, \ldots, \operatorname{Ex}</em>\right}}<em _mathbf_1="\mathbf{1">{\boldsymbol{n}} \
= &amp; \left{\left(\mathbf{S M}</em>}}(\mathbf{t}), \mathbf{S<em _mathbf_1="\mathbf{1">{\mathbf{1}}(\mathbf{t}+\mathbf{1}), \boldsymbol{e}</em>)\right)\right. \
&amp; \left.\left(\mathbf{S M}}}(\boldsymbol{t}+\mathbf{1<em _mathbf_2="\mathbf{2">{\mathbf{2}}(\mathbf{t}), \mathbf{S}</em>}}(\mathbf{t}+\mathbf{1}), \boldsymbol{e<em _mathbf_i="\mathbf{i">{\mathbf{2}}(\boldsymbol{t}+\mathbf{1})\right), \ldots\right. \
&amp; \left.\left.\left(\mathbf{S M}</em>}}(\mathbf{t}), \mathbf{S<em _mathbf_i="\mathbf{i">{\mathbf{i}}(\mathbf{t}+\mathbf{1}), \boldsymbol{e}</em>
\end{aligned}
$$}}(\boldsymbol{t}+\mathbf{1})\right)\right}_{\boldsymbol{n}</p>
<p>where $\boldsymbol{e}<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}(\boldsymbol{t}+\mathbf{1})$ is the prediction errors of $\mathbf{P M}$ associated to the prediction of $\mathbf{S}</em>)$.}}(\mathbf{t}+\mathbf{1})$ given $\mathbf{S M}_{\boldsymbol{i}}(\mathbf{t</p>
<p>The PAM monitors the learning process inside $\boldsymbol{R}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}$ by analyzing the evolution of errors. More precisely, the system computes the value opposite to the derivative of errors, which is called learning progress $\left(\boldsymbol{L} \boldsymbol{P}</em>\right)$. This value is computed using a sliding window which contains the $\zeta(\zeta=2, k$,
$k&gt;1$ ) most recent exemplars of the considered region $\boldsymbol{R}}<em _boldsymbol_m="\boldsymbol{m">{\boldsymbol{n}}:\left{\mathrm{Ex}</em>}-\zeta}, \mathrm{Ex<em _boldsymbol_m="\boldsymbol{m">{\boldsymbol{m}-\zeta+1, \ldots}, \mathrm{Ex}</em>$ (if the region contains less than $\zeta$ exemplars, then the learning progress is computed over a shorter window with all the current collected exemplars).}}\right}_{\boldsymbol{n}</p>
<p>Therefore, considering $\left|E_{n}\right|$ as the cardinality of $E_{n}$ we define the learning progress $\left(\boldsymbol{L} \boldsymbol{P}\left(\boldsymbol{E}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}\right)\right)$, also noted $\boldsymbol{L} \boldsymbol{P}</em>$ and at a given moment in time as}}$, in region $\boldsymbol{R}_{\boldsymbol{n}</p>
<p>$$
\boldsymbol{L} \boldsymbol{P}\left(\boldsymbol{E}<em i="|E_{n">{\boldsymbol{n}}\right)=\frac{\sum</em>
$$}|-\zeta}^{\left|E_{n}\right|-\xi} e_{i}-\sum_{i=|E_{n}|-\xi}^{\left|E_{n}\right|} e_{i}}{\zeta</p>
<p>Because $\boldsymbol{L} \boldsymbol{P}<em _split="{split" _text="\text">{\boldsymbol{n}}$ is computed over the recent past through a sliding window, it is not necessary to memorize all past learning exemplars, and makes the whole system both computationally efficient in terms of speed and memory usage. Actually, in a given region, only the last $\boldsymbol{T}</em>}}$ exemplars need to be memorized, with $\boldsymbol{T<em _boldsymbol_n="\boldsymbol{n">{\text {split }}&gt;\zeta$ being the splitting parameter described below. $\boldsymbol{L} \boldsymbol{P}</em>$ is then used as a measure of interestingness in the action selection scheme outlined below. The more a region is characterized by learning progress, the more it is interesting, and the more the system will perform experiments and collect learning exemplars that fall into this region. Of course, as exploration goes on, the learned forward model becomes better in this region and learning progress might decrease, leading to a decrease in the interestingness of this region.}</p>
<p>To precisely represent the learning behavior inside the whole sensorimotor space and differentiate its various evolutions in various subspaces/subregions, different PAM modules, each associated to a different subregion $\boldsymbol{R}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}$ of the sensorimotor space, need to be built. Therefore, the learning progress $\boldsymbol{L} \boldsymbol{P}</em>}}$ provided as the output values of each PAM becomes representative of the interestingness of the associated region $\boldsymbol{R<em 0="0">{\boldsymbol{n}}$. Initially, the whole space is considered as one single region $\boldsymbol{R}</em>$, associated to one PAM, which will be progressively split into subregions with their own PAM as we will now describe.</p>
<h2>C. The Split Machine</h2>
<p>The split machine $\operatorname{SpM}$ (Fig. 3) is both responsible of identifying the region and PAM corresponding to a given $\mathbf{S M}(\mathbf{t})$, but also responsible of splitting (or creating in R-IAC where parent regions are kept in use) subregions from existing regions.</p>
<p>1) Region Implementation: We use a tree representation to store the list of regions as shown in Fig. 4. The main node represents the whole space, and leafs are subspaces. $\mathbf{S}(\mathbf{t})$ and $\mathbf{M}(\mathbf{t})$ are here normalized into $[0 ; 1]^{n}$. The main region (first node), called $R_{0}$, represents the whole sensorimotor space. Each region stores a first-in-first-out (FIFO) list of recently collected exemplars that it covers, with a maximum length of $\boldsymbol{T}_{\text {split }}$. When this threshold is reached, different mechanisms are triggered in IAC and R-IAC.
1) In IAC, the region is split into two daughter regions according to the mechanism described below, and the parent region is deleted.
2) In R-IAC: if the region is a leaf region, then it is split into two daughter regions with the mechanism described below, but the initial region is kept and becomes a parent region. If the region is already a parent region, then it is not split anymore and any subsequent exemplars added to the FIFO</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. General architecture of IAC and R-IAC. PM is used to create a forward model of the world, and measures the quality of its predictions (errors values). Then, a split machine cuts the sensorimotor space into different regions, whose quality of learning over time is examined by PAM. Then, an action selection system is used to choose experiments to perform.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. The sensorimotor space is iteratively and recursively split into subspaces, called "regions". Each region $R_{n}$ is responsible for monitoring the evolution of the error rate in the anticipation of consequences of the robot's actions, if the associated contexts are covered by this region.
list provokes the deletion of the oldest exemplar already in the list.
3) In both IAC and R-IAC, splitting is done with hyperplanes perpendicular to one dimension. An example of split execution is shown in Fig. 4, using a two dimensional input space. IAC and R-IAC differ in the way the hyperplane is chosen.
2) IAC Split Algorithm: In the IAC algorithm, the idea was to find a split such that the two sets of exemplars into the two subregions would minimize the sum of the variances of $\mathbf{S}(\boldsymbol{t}+\mathbf{1})$ components of exemplars of each set, weighted by the number of exemplars of each set. Hence, the split takes place in the middle of zones of maximal change in the function $\mathbf{S M}(\mathbf{t}) \rightarrow \mathbf{S}(\mathbf{t}+\mathbf{1})$. Mathematically, we consider $\varphi_{n}=\left{(\mathbf{S M}(\boldsymbol{t}), \mathbf{S}(\boldsymbol{t}+\mathbf{1}))<em n="n">{\boldsymbol{i}}\right}$ as the set of exemplars possessed by the region $\boldsymbol{R}</em>$ such that:}$. Let us denote $j$ a cutting dimension and $v_{j}$, an associated cutting value. Then, the split of $\varphi_{n}$ into $\varphi_{n+1}$ and $\varphi_{n+2}$ is done by choosing $j$ and $v_{j</p>
<p>1) all the exemplars $(\mathbf{S M}(\boldsymbol{t}), \mathbf{S}(\boldsymbol{t}+\mathbf{1}))<em n_1="n+1">{\boldsymbol{i}}$ of $\varphi</em>$;
2) all the exemplars $(\mathbf{S M}(\boldsymbol{t}), \mathbf{S}(\boldsymbol{t}+\mathbf{1}))}$ have a $j^{\text {th }}$ component of their $\mathbf{S M}(\boldsymbol{t})$ smaller than $v_{j<em n_2="n+2">{\boldsymbol{i}}$ of $\varphi</em>$;
3) the quantity}$ have a $j^{\text {th }}$ component of their $\mathbf{S M}(\boldsymbol{t})$ greater than $v_{j</p>
<p>$$
\begin{aligned}
&amp; \text { Qual }\left(j, v_{j}\right) \
&amp; \quad=\left|\varphi_{n+1}\right| \sigma\left(\left{\mathbf{S}(1 \boldsymbol{t}+\mathbf{1}) \mid\left(\mathbf{S M}(\boldsymbol{t}), \mathbf{S}(\boldsymbol{t}+\mathbf{1})) \in \varphi_{n+1}\right}\right) \
&amp; \quad+\left|\varphi_{n+2}\right| \cdot \sigma\left(\left{\mathbf{S}(\boldsymbol{t}+\mathbf{1}) \mid\left(\mathbf{S M}(\boldsymbol{t}), \mathbf{S}(\boldsymbol{t}+\mathbf{1})) \in \varphi_{n+1}\right}\right)
\end{aligned}
$$</p>
<p>is minimal, where</p>
<p>$$
\sigma(\mathrm{S})=\frac{\sum_{v \in \mathrm{~S}}\left|v-\frac{\sum_{j \in \mathrm{~S}} z}{|S|}\right|^{2}}{|\mathrm{~S}|}
$$</p>
<p>where $\mathbf{S}$ is a set of vectors, and $|\mathrm{S}|$, its cardinal. Finding the exact optimal split would be computationally too expensive. For this reason, we use the following heuristics for optimization: for each dimension $j$, we evaluate $N_{s p}$ cutting $v_{j}$ equally spaced between the extrema values of $\varphi_{n}$, thus we evaluate $N_{s p} .{{j} \mid$ splits in total, and the one with minimal $\operatorname{Qual}\left(j, v_{j}\right)$ is finally chosen. This computationally cheap heuristics has produced acceptable results in all the experiments we ran so far. It could potentially be improved by allowing region splits cutting multiple dimensions at the same time in conjunction with a Monte-Carlo based sampling of the space of possible splits.
3) R-IAC Split Algorithm: In R-IAC, the splitting mechanism is based on comparisons between the learning progresses in the two potential child regions. The principal idea is to perform the separation which maximizes the dissimilarity of learning progress comparing the two created regions. This leads to the direct detection of areas where the learning progress is maximal, and to separate them from others (see Fig. 5). This contrasts with IAC where regions were built independently of the notion of learning progress.</p>
<p>Reusing the notations of the previous section, in R-IAC, the split of $\varphi_{n}$ into $\varphi_{n+1}$ and $\varphi_{n+2}$ is then done by choosing $j$ and $v_{j}$ such that</p>
<p>$$
\operatorname{Qual}\left(j, v_{j}\right)=\left(L P_{n+1}-L P_{n+2}\right)^{2}
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Evolution of the sensorimotor regions over time. The whole space is progressively subdivided in such a way that the dissimilarity of each subregion in terms of learning progress is maximal.
is maximal, where $L P_{n+1}$ and $L P_{n+2}$ are the learning progress computed inside $\varphi_{n+1}$ and $\varphi_{n+2}$.</p>
<h2>D. Action Selection Machine</h2>
<p>We present here an implementation of action selection machine (ASM). The ASM decides on actions $\mathbf{M}(\boldsymbol{t})$ to perform, given a sensory context $\mathbf{S}(\boldsymbol{t})$. (See Fig. 3). The ASM heuristics is based on a mixture of several modes, which differ between IAC and R-IAC. Both IAC and R-IAC algorithms share the same global loop in which modes are chosen probabilistically.</p>
<h2>Outline of the global loop of IAC and $\mathrm{R}-\mathrm{IAC}$ algorithms</h2>
<ul>
<li>ASM: given $\mathbf{S}(\mathbf{t})$, execute an action $\mathbf{M}(\boldsymbol{t})$ using the mode $(\boldsymbol{n})$ with probability $\boldsymbol{p}_{\boldsymbol{n}}$ and based on data stored in the region tree, with $\boldsymbol{n} \in{\mathbf{1}, \mathbf{2}}$ for IAC and $\boldsymbol{n} \in{\mathbf{1}, \mathbf{2}, \mathbf{3}}$ for R-IAC.</li>
<li>PM: Estimate the predicted consequence $\hat{\boldsymbol{S}}_{\boldsymbol{t}+\mathbf{1}}$ using the PM.</li>
<li>External Environment: Measure the real consequence $\boldsymbol{S}_{\boldsymbol{t}+\mathbf{1}}$.</li>
<li>PM: Compute the error $\boldsymbol{e}(\boldsymbol{t}+\mathbf{1})=\boldsymbol{a} \boldsymbol{b} \boldsymbol{s}\left(\hat{\boldsymbol{S}}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\mathbf{1}}-\boldsymbol{S}</em>\right)$.}+\mathbf{1}</li>
<li>Update the PM with $(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}))$.</li>
<li>Split Machine SpM: update the region tree with $(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}))$ and $\boldsymbol{e}(\boldsymbol{t}+\mathbf{1})$.</li>
<li>PAM: update evaluation of learning progress in the regions that cover $(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}))$.</li>
</ul>
<p>We now present the different exploration modes used by the ASM, in IAC and R-IAC algorithm:</p>
<p>1) Mode 1: Random Babbling Exploration: The random babbling mode corresponds to a totally random exploration (random choice of $\mathbf{M}(\mathbf{t})$ with a uniform distribution), which does not consider previous actions and context. This mode appears in both IAC and R-IAC algorithm, with a probability $\boldsymbol{p}<em 2="2">{\mathbf{1}}$ typically equal to $30 \%$.
2) Mode 2: Learning Progress Maximization Exploration: This mode, chosen with a probability $\boldsymbol{p}</em>$ typically equal to $70 \%$ in IAC and $60 \%$ in R-IAC (which means this is the dominant mode), aims to maximize learning progress, but with two different heuristics in IAC and R-IAC:</p>
<p>IAC: In the IAC algorithm, mode 2 action selection is straightforward: among the leaf regions that cover the current state $\mathbf{S}(\mathbf{t})$ (i.e., for which there exists a $\mathbf{M}(\mathbf{t})$ such
that $\mathbf{S M}(\mathbf{t})$ is in the region-there are typically many), the leaf region which learning progress is maximal is found, and then a random action within this region is chosen;
R-IAC: In the R-IAC algorithm, we take into account the fact that many regions may have close learning progress values, and thus, should be selected roughly equally often by taking a probabilistic approach to region selection. This avoids the problems of a winner take-all strategy when the region splits do not reflect well the underlying learnability structure of the sensorimotor space. Furthermore, instead of focusing on the leaf regions like in IAC, R-IAC continues to monitor learning progress in node regions and select them if they have more learning progress; thus, learning progress is monitored simultaneously at several scales in the sensorimotor space. Let us give more details:
3) Probabilistic Approach to Region Selection: A region $R_{n}$ is chosen among all eligible regions $R=\left{R_{i}\right}$ (i.e., for which there exists a $\mathbf{M}(\mathbf{t})$ such that $\mathbf{S M}(\mathbf{t})$ is in the region $n$ ) with a probability $\boldsymbol{P}<em n="n">{\boldsymbol{n}}$ proportional to its learning progress $L P</em>$}$, stored in the associated $\mathbf{P A M}_{\boldsymbol{n}</p>
<p>$$
\boldsymbol{P}<em n="n">{\boldsymbol{n}}=\frac{\left[L P</em>}-\min \left(\mathrm{LP<em i="1">{i}\right)\right]}{\sum</em>
$$}^{\left|R_{n}\right|}\left[L P_{i}-\min \left(\mathrm{LP}_{i}\right)\right]</p>
<p>Multiresolution Monitoring of Learning Progress: In the IAC algorithm, the estimation of learning progress only happens in leaf regions, which are the only eligible regions for action selection. In R-IAC, learning progress is monitored in all regions created during the system's life time, which allows us to track learning progress at multiple resolution in the sensorimotor space. This implies that when a new exemplar is available, R-IAC updates the evaluation of learning progress in all regions that cover this exemplar, but only if the exemplar was chosen randomly, i.e., not with mode 3 as described below. Because regions are created in a top-down manner and stored in a tree structure which was already used for fast access in IAC, this new heuristics does not bring computational overload and can be implemented efficiently.</p>
<p>In R-IAC mode 2, when a region has been chosen with the probabilistic approach and the multiresolution scheme, a random action is chosen within this region.
4) Mode 3: Error Maximization Exploration: Mode 3 combines a traditional active learning heuristics with the concept of learning progress: in mode 3, a region is first chosen with the same scheme as in R-IAC mode 2. But once this region has been chosen, an action in this region is selected such that the expected error in prediction will be maximal. This is currently implemented through a k-nearest neighbor regression of the function $\boldsymbol{S M}(t) \rightarrow e(t+1)$, which allows finding the point of maximal error, to which is added small random noise (to avoid to query several times exactly the same point). Mode 3 is typically chosen with a probability $\boldsymbol{p}_{3}=10 \%$ in $\mathbf{R}-\mathbf{I A C}$ (and does not appear in IAC).</p>
<h2>E. Pseudo-Code of $\boldsymbol{R}-\boldsymbol{I A C}$</h2>
<p>$\mathbf{R}-\mathbf{I A C}\left(\boldsymbol{P M}, \boldsymbol{p}<em 2="2">{1}, \boldsymbol{p}</em>}, \boldsymbol{p<em _split="{split" _text="\text">{3}, \boldsymbol{T}</em>, \eta, \Gamma, \kappa, \zeta\right)$}}, \boldsymbol{l</p>
<h2>Init:</h2>
<ul>
<li>let $\boldsymbol{R}_{\mathbf{0}}$ be the whole space of mathematically possible values of the sensorimotor context $\mathbf{S M}(\mathbf{t})$ (typically a hypercube in $\mathbb{R}^{d}$ );</li>
<li>let $\boldsymbol{L P}<em _mathbf_0="\mathbf{0">{\mathbf{0}}=\mathbf{0}$ be the learning progress associated to $\boldsymbol{R}</em>$;}</li>
<li>let $\boldsymbol{L e x}<em 0="0">{\boldsymbol{R}</em>}}=[\emptyset]$ (later on in the algorithm, $\boldsymbol{L e x<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>}}}$ will be the FIFO list $\left[\left(\left(\mathbf{S M<em _mathbf_i="\mathbf{i">{\mathbf{i}}(\mathbf{t}), \mathbf{S}</em>}}(\mathbf{t}+\mathbf{1})\right), \mathbf{e<em _mathbf_i="\mathbf{i">{\mathbf{i}}(\mathbf{t}+\mathbf{1}), \boldsymbol{\omega}</em>}}\right)\right]$ where the set of $\left(\mathbf{S M<em _mathbf_i="\mathbf{i">{\mathbf{i}}(\mathbf{t}), \mathbf{S}</em>}}(\mathbf{t}+\mathbf{1})\right)$ components is the set of learning exemplars collected in $R_{k}$, the set of $\mathbf{e<em _boldsymbol_i="\boldsymbol{i">{\mathbf{i}}(\mathbf{t}+\mathbf{1})$ components is the set of associated prediction errors, and $\boldsymbol{\omega}</em>}}$ is a time stamp whose value is used to find the relative order in which each particular learning exemplar was collected within $\boldsymbol{R<em _boldsymbol_R="\boldsymbol{R">{\boldsymbol{k}}$ ); The size of $\boldsymbol{L e x}</em><em _split="{split" _text="\text">{\boldsymbol{k}}}$ is bounded by $\boldsymbol{T}</em>$;}</li>
<li>init the prediction/learning machine PM with an empty set of learning exemplars.</li>
</ul>
<h2>Loop:</h2>
<ul>
<li>let $\mathbf{S}(\mathbf{t})$ be the current state;</li>
<li>let $\boldsymbol{R}=\left{\boldsymbol{R}<em _mathbf_1="\mathbf{1">{\mathbf{0}}, \boldsymbol{R}</em>}}, \ldots, \boldsymbol{R<em _boldsymbol_l="\boldsymbol{l">{\boldsymbol{n}}\right}$ be the set of subregions $\boldsymbol{R}</em>$;}}$ of the sensorimotor space such that there exists a $\mathbf{M}(\mathbf{t})$ such that $\mathbf{S M}(\mathbf{t}) \in \boldsymbol{R}_{\boldsymbol{l}</li>
<li>for all $\boldsymbol{n}$, let $\boldsymbol{L P}<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{n}}$ be the learning progress associated to $\boldsymbol{R}</em>$.}</li>
</ul>
<h2>Action Selection:</h2>
<ul>
<li>select action selection mode among mode 1, mode 2 and mode 3 with probabilities $\boldsymbol{p}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{p}</em>$;}}, \boldsymbol{p}_{\mathbf{3}</li>
<li>if mode $=$ mode 1</li>
<li>Let $\mathbf{M}(\mathbf{t})$ be a random vector (uniform distribution)</li>
<li>if mode $=$ mode 2</li>
<li>for $l=0 \ldots n$, let $\boldsymbol{P}<em l="l">{l}=\left(\left|L P</em>-\right.\right.$ $\left.\left.\min <em i="i">{L P</em>\right)\right|\right)$} \in \boldsymbol{R}}\left(L P_{i}\right)\right]\right) /\left(\sum_{i=1}^{|R|}\left|L P_{i}-\min L P_{i \in \boldsymbol{R}}\left(L P_{i</li>
<li>let $\boldsymbol{R}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{k}}$ be a subregion in $\boldsymbol{R}$ chosen with probability $\boldsymbol{P}</em>$, $k \in{0, \ldots, n}$ in a roulette wheel manner;}</li>
<li>let $\mathbf{M}(\mathbf{t})$ be a random vector such that $\mathbf{S M}(\boldsymbol{t}) \in \boldsymbol{R}_{\boldsymbol{k}}$ (uniform distribution);</li>
<li>if mode $=$ mode 3</li>
<li>for $l=0 \ldots n$, let $\boldsymbol{P}<em l="l">{l}=\left(\left|L P</em>-\right.\right.$ $\left.\left.\min <em i="i">{L P</em>\right)\right|\right)$} \in \boldsymbol{R}}\left(L P_{i}\right)\right|\right) /\left(\sum_{i=1}^{|R|}\left|L P_{i}-\min L P_{i \in \boldsymbol{R}}\left(L P_{i</li>
<li>let $\boldsymbol{R}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{k}}$ be a subregion in $\boldsymbol{R}$ chosen with probability $\boldsymbol{P}</em>$, $k \in{0, \ldots, n}$ in a roulette wheel manner;}</li>
<li>let $\mathbf{E t}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>}}}$ be a model of the errors made in prediction in $\boldsymbol{R<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{k}}$ in the past, built with a $l$-nearest neighbor algorithm on the last $\eta$ learning examplars collected in $\boldsymbol{R}</em>}}$, belonging to $\boldsymbol{L e x<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>$;}}</li>
<li>Let $\mathbf{M} \max (\mathbf{t})=\operatorname{argmax}<em _boldsymbol_R="\boldsymbol{R">{\mathbf{M}(\boldsymbol{t})} \operatorname{Ext}</em>)$;}_{\boldsymbol{k}}}(\boldsymbol{S M}(\boldsymbol{t}))$ obtained by sampling uniformly randomly $\boldsymbol{\Gamma}$ candidates $\mathbf{M}(\mathbf{t</li>
<li>Let $\mathbf{M}(\mathbf{t})=\mathbf{M} \max (\mathbf{t})+\boldsymbol{\varepsilon}$ with $\boldsymbol{\varepsilon}$ a small random number between $\mathbf{0}$ and $\boldsymbol{\sigma}$ along a uniform distribution.</li>
<li>Execute $\mathbf{M}(\mathbf{t})$.</li>
</ul>
<h2>Prediction and measurement of the consequences of action:</h2>
<ul>
<li>Estimate the predicted consequence $\hat{\mathbf{S}}(\boldsymbol{t}+\mathbf{1})$ of executing $\mathbf{M}(\mathbf{t})$ in the environment with state $\mathbf{S}(\mathbf{t})$ using the prediction machine PM (e.g., ILO-GMR, LWPR, or a neural net).</li>
<li>Measure the real consequence $\mathbf{S}(\mathbf{t}+\mathbf{1})$ after execution of $\mathbf{M}(\mathbf{t})$ in the environment with state $\mathbf{S}(\mathbf{t})$.</li>
<li>Compute the error $\boldsymbol{e}(\boldsymbol{t}+\mathbf{1})=\boldsymbol{a b} \boldsymbol{s}(\hat{\mathbf{S}}(\boldsymbol{t}+\mathbf{1})-\mathbf{S}(\mathbf{t}+\mathbf{1}))$.</li>
<li>Update the prediction machine PM with the new learning exemplar $(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}))$.</li>
</ul>
<h2>Update of region models:</h2>
<ul>
<li>let $\mathrm{Ex}=(\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}), \mathbf{e}(\mathbf{t}+\mathbf{1}))$;</li>
<li>let $\gamma$ be the total number of regions created by the system so far;</li>
<li>lor all regions $\boldsymbol{R}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{k}}$ such that $\mathbf{S M}(\mathbf{t}) \in \boldsymbol{R}</em>$}</li>
<li>let $\boldsymbol{\omega}$ be the maximum $\omega_{\boldsymbol{i}}$ time stamp in $\boldsymbol{L e x}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>$;}}</li>
<li>update $\boldsymbol{L e x}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>}}}$ by adding $((\mathbf{S M}(\mathbf{t}), \mathbf{S}(\mathbf{t}+\mathbf{1}), \mathbf{e}(\mathbf{t}+$ 1), $\omega_{\mathbf{t}}$ ) (and possibly deleting the oldest exemplar if $\left|\boldsymbol{L e x<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>}}}\right|=\boldsymbol{T<em _mathbf_t="\mathbf{t">{\text {split }}$ ) where $\boldsymbol{\omega}</em>$ is a time stamp used to keep track of the order in which this learning exemplar was stored in relation to others (see below);}</li>
<li>if $\left|\boldsymbol{L e x}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>}}}\right|=\boldsymbol{T<em _boldsymbol_k="\boldsymbol{k">{\text {split }}$ AND $\boldsymbol{R}</em>}}$ is a leaf region Create two new regions $\boldsymbol{R<em _gamma_2="\gamma+2">{\gamma+1}$ and $\boldsymbol{R}</em>}$ as subregions of $\boldsymbol{R<em j="j">{\boldsymbol{k}}$ with $j$, a cutting dimension and $v</em>$, an associated cutting value optimized through random uniform sampling of $\kappa$ possible candidates and such that:</li>
</ul>
<p>1) $\boldsymbol{L e x}<em _gamma_1="\gamma+1">{\boldsymbol{R}</em>}}$ is initialized with all the elements in $\boldsymbol{L e x<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>)$ smaller than;
2) is initialized with all the elements in that have a component of their greater than $v_{j}$;
3) $\boldsymbol{L e x}}}}$ that have a $j^{\text {th }}$ component of their $\mathbf{S M}(\boldsymbol{t<em _gamma_2="\gamma+2">{\boldsymbol{R}</em>}}$ is initialized with all the elements in $\boldsymbol{L e x<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{R}</em>$;
4) the difference between learning progresses $L P_{\gamma+1}$ and $L P_{\gamma+2}$ measured in both subregions is maximal, i.e., $\left(L P_{\gamma+1}-L P_{\gamma+1}\right)^{2}$ is maximal, where errors are indexed by their relative order of measurement calculated from $\boldsymbol{\omega}}}}$ that have a $j^{\text {th }}$ component of their $\mathbf{S M}(\boldsymbol{t})$ greater than $v_{j<em _boldsymbol_n="\boldsymbol{n">{\boldsymbol{i}}$ values where $\boldsymbol{L P}\left(\boldsymbol{E}</em>\right) / \zeta$ and where $\zeta$ defines the time window used to compute learning progress achieved through the acquisition of most recent learning exemplars in each region;}}\right)=$ $\left(\sum_{i=|E_{n}|-\zeta / 2 \mid}^{\mid E_{n} \mid-\zeta / 2} e_{i}-\sum_{i=|E_{n}|-\zeta / 2 \mid}^{\mid E_{n}} e_{i</p>
<ul>
<li>store the learning progresses $\mathbf{L} \mathbf{P}<em _gamma_2="\gamma+2">{\gamma+1}$ and $\mathbf{L} \mathbf{P}</em>$ of the two newly created regions;</li>
<li>$\gamma=\gamma+1$</li>
<li>for all regions $\boldsymbol{R}<em _boldsymbol_k="\boldsymbol{k">{\boldsymbol{k}}$ such that $\mathbf{S M}(\mathbf{t}) \in \boldsymbol{R}</em>}}$ (except $\boldsymbol{R<em _gamma_2="\gamma+2">{\gamma+1}$ and $\boldsymbol{R}</em>$ and store the value;
End Loop}$ if a split was performed), recompute $\boldsymbol{L} \boldsymbol{P}_{\boldsymbol{k}</li>
</ul>
<h2>F. Software</h2>
<p>An open source MATLAB-based software library containing the source code of the IAC and R-IAC algorithms, as well as tools and a tutorial that allow reproducing all experiments presented in Sections IV and V below is made publicly available at: http://flowers.inria.fr/riac-software.zip</p>
<h2>G. Remark</h2>
<p>1) Computational Complexity of R-IAC: Because regions are stored and accessed in a binary tree, because only leaves regions can be split and only one region per new exemplar can be split, and because the number of exemplars stored in each region is bounded by $\boldsymbol{T}_{\text {split }}$ and managed by a FIFO list/stack, it follows that the total number of regions grows logarithmically with the number of collected exemplars, hence the number of stored exemplars grows also logarithmically (with a higher, but constant multiplicative factor), and thus, global memory usage grows logarithmically. Furthermore, the computational cost of updating the regions tree structure is dominated by the cost of the splitting algorithm, which is currently done through Monte-Carlo simulation; this allows to control the number of samples in the optimization process and thus, can be set to be low to ensure fast computation (at the cost of accuracy, but since multiresolution makes the system robust to suboptimal splits, this has a limited impact on the performance of the system), which is also permitted by the fact that learning progress is computed only over the last $\zeta$ exemplars in each region. Finally, the computational cost of action selection grows linearly with the number of regions, thus logarithmically with the number of collected exemplars, with a very small constant multiplicative factor since it only consists of basic probability computations based on the learning progress values of regions. Thus, the system's memory and computation time requirements grow logarithmically with time, which makes it in practice scalable to many existing robotic experimental setups.
2) Regulation of the Growth of Complexity: As argued in detail in [28], the heuristics consisting in preferentially exploring subregions of the sensorimotor space where learning progress is maximal has the practical consequence to lead the robot to explore zones of intermediate complexity/difficulty/contingency, which has been advocated by developmental psychologists (e.g., [22], [23], and [38]) as being the key property of spontaneous exploration in humans. Indeed, subregions which are trivial to learn are quickly characterized by a low plateau in prediction errors, and thus become uninteresting. On the other end of the complexity spectrum, subregions which are unlearnable are characterized with a high plateau in prediction errors and thus, are also quickly identified as uninteresting. In between, exploration first focuses on subregions where prediction errors decrease fastest, which typically correspond to lower complexity situations, and when these regions are mastered and a plateau is reached, exploration continues in more complicated subregions where large learning progress is detected.
3) Key Advances of R-IAC Over IAC and Robustness to Potential Inaccurate and Large Number of Region Splits: Among the various differences between IAC and R-IAC, the two most crucial ones are 1) the probabilistic choice of regions in R-IAC
as opposed to the winner take all strategy in IAC, and 2) the multiresolution monitoring of learning progress in R-IAC as opposed to the only lowest scale monitoring of IAC. The combination of these two innovations allows the system to cope with potentially inaccurate and supernumerary region splits. Indeed, a problem in IAC was that if for example one homogeneous region with high learning progress was split, the winner-take-all strategy typically biased the system to explore later on only one of the two subregions, which was very inefficient. Furthermore, the more regions were split, which happened continuously given the splitting mechanism, the smaller they became, and because only child regions were monitored, exploration was becoming increasingly focused on smaller and smaller subregions of the sensorimotor space, which was also often quite inefficient. While the new splitting mechanism introduced in this paper allows the system to minimize inaccurate splits, the best strategy to go around these problems was to find a global method whose efficiency depends only loosely on the particular region split mechanism. The probabilistic choice of actions makes the system robust to the potentially unnecessary split of homogeneous regions, and the multiresolution scheme allows the system to be rather insensitive to the creation of an increasing number of small regions.
4) Planning Learning Progress: The central contribution of both the IAC and R-IAC systems lie in the way rewards are defined and computed, i.e., through region-based hierarchical multiresolution evaluation of learning progress. This can be readily and efficiently reused in a traditional active learning regression context where the learning problem can be transformed into an immediate reward maximization problem, such as in the standard self-supervised regression framework in [41], and as we will do in the experiments presented in the next sections. Yet, many real world robotic sensorimotor spaces are such that a given zone of high learning progress might not be immediately reachable and thus, might require planning through a potentially uncertain intermediate path which does not necessarily provide learning progress. While the reward system of R-IAC is currently integrated into an action selection loop which is compatible with such environments, it does not include such a planning capacity and thus, the overall architecture is suboptimal in that case. Hence, the R-IAC reward system would need to be integrated with an action selection scheme that allows the system to plan and maximize the cumulated sum of future expected R-IAC rewards (i.e., future expected learning progress as defined in R-IAC) rather than immediate R-IAC rewards. This could be done like in related intrinsically motivated reinforcement learning architecture presented in [28], [33], and will be achieved and evaluated in future work.</p>
<h2>III. The Prediction Machine: Incremental Regression ALGORITHMS FOR LEARNING FORWARD AND INVERSE MODELS</h2>
<p>The R-IAC system presented above is mostly agnostic regarding the kind of learning algorithm used to implement the prediction machine, i.e., used to learn forward models. The only property that is assumed is that learning must be incremental, since exploration is driven by measures of the improvement of the learned forward models as new learning exemplars are collected. But among incremental algorithms, methods based on</p>
<p>neural networks, memory based learning algorithms, or incremental statistical learning techniques could be used [43]. This agnosticism is an interesting feature of the system since it constitutes a single method to achieve active learning with multiple learning algorithms, i.e., with multiple kinds of learning biases that can be peculiar to each application domain, as opposed to a number of statistical active learning algorithms designed specifically for particular learning methods such as support vector machines, Gaussian mixture regression, or locally weighted regression [41]. Nevertheless, what the robot will learn eventually will obviously depend both on R-IAC and on the capabilities of the prediction machine/regression algorithm for which R-IAC drives the collection of learning exemplars.</p>
<p>In robot learning, a particular important problem is to learn the forward and inverse kinematics as well as the forward and inverse dynamics of the body [44]-[47]. A number of regression algorithms have been designed and experimented in this context in the robot learning literature, and because a particularly interesting use of R-IAC is for driving exploration for the discovery of the robot's body, as will be illustrated in the experiments in the next section (and was already illustrated for IAC in [27] and [36]), it is useful to look at state-of-the-art statistical regression methods for this kind of space. An important family of such algorithms is locally weighted regression [45], among which locally weighted projection regression (LWPR) has recently shown a strong ability to learn incrementally and efficiently forward and inverse models in high dimensional sensorimotor spaces [45], [46]. Gaussian process regression has also proven to allow for very high generalization performances [48]. Another approach, based on Gaussian mixture regression [3], [49], is based on the learning of the joint probability distribution of the sensorimotor variables, instead of learning a forward or an inverse model, and can be used online for inferring specific forward or inverse models by well chosen projections of the joint density. Gaussian mixture regression (GMR) has recently shown a number of good properties for robot motor learning in a series of real world robotic experiments [3]. It is interesting to note that these techniques come from advances in statistical learning theory, and seem to allow significantly higher performances than approaches based on neural networks [50].</p>
<p>Because it is incremental and powerful, LWPR might be a good basic prediction algorithm to be used in the R-IAC framework for conducting robot experiments. Yet, LWPR is also characterized by a high number of parameters whose tuning is not straightforward and thus, makes its use not optimal for repeated experiments about R-IAC in various sensorimotor spaces. On the other hand, Gaussian processes and Gaussian mixture regression have fewer parameters (only one parameter for Gaussian mixture regression (GMR), i.e., the number of Gaussians) and are much easier to tune. Unfortunately, they are batch methods which can be computationally very demanding as the dataset grows. Thus, they cannot be used directly as prediction machines in the R-IAC framework.</p>
<p>This is why we have developed a regression algorithm, called incremental local online Gaussian mixture regression (ILO-GMR), which mixes the ease of use of GMR with the incremental memory-based approach of local learning approaches. The general idea is to compute online local few
components GMM/GMR models based on the datapoints in memory whose values in the input point dimensions are in the vicinity of this input point. This local approach allows directly taking into account any novel single datapoint/learning exemplar added to the database since regression is done locally and online. It can be done computationally efficiently thanks to the use of few GMM components (typically 2 or 3 ), and crucially thanks to the use of an incremental approximate nearest neighbor algorithm derived from recent batch mode approximate nearest neighbor algorithms [51]-[53]. A feature of ILO-GMR is that given its incremental and online nature, with a single set of parameters it can in principle approximate and adapt efficiently to a high variety of mapping to be learnt that may differ significantly in their length scale.</p>
<p>ILO-GMR has only two parameters: the number $\boldsymbol{k}$ of components for local models, and a parameter $\boldsymbol{N}$ that defines the notion of local vicinity (see the pseudo-code outline below). A related approach, based on Gaussian process regression rather than Gaussian mixture regression, has been described in [42] and in depth comparisons of these approaches will be of high interest in further work.</p>
<p>Outline of the pseudo-code of ILO - GMR</p>
<h2>ILO - GMR(Data, X, N, k)</h2>
<p>Data is the set of $(\boldsymbol{X} \boldsymbol{i}, \boldsymbol{Y} \boldsymbol{i}=\boldsymbol{f}(\boldsymbol{X} \boldsymbol{i}))$ points already observed and stored in a hierarchichal $k$-means tree structure.
$\boldsymbol{X}$ is the input point</p>
<ul>
<li>Find the approximate $\boldsymbol{N}$ closest points $\boldsymbol{X} \boldsymbol{j}$ to $\boldsymbol{X}$ in Data (e.g., with incremental hierarchical k-means).</li>
<li>Build a local $\boldsymbol{k}$-components GMM model based on the $(\boldsymbol{X} \boldsymbol{j}, \boldsymbol{Y} \boldsymbol{j})$ corresponding exemplars.</li>
<li>Predict $\boldsymbol{y}^{\prime}=\boldsymbol{f}(\boldsymbol{X})$ with GMR using a least square estimate.</li>
<li>Measure the true $\boldsymbol{y}=\boldsymbol{f}(\boldsymbol{X})$.</li>
<li>Update incrementally Data to include the novel exemplar $(\boldsymbol{y}, \boldsymbol{f}(\boldsymbol{X}))$.</li>
</ul>
<p>We have compared the performance of ILO-GMR with other state-of-the-art regression methods on the hard regression task defined in the SARCOS dataset which has been used several times in the literature as a benchmark for regression techniques in robotics, e.g., [45], [60], and [61]. This dataset encodes the inverse dynamics of the arm of the SARCOS robot, with 21 input dimensions (position, velocity and acceleration of 7 DOFs) and 7 output dimensions (corresponding torques). It contains 44484 exemplars in the training database and 4449 test exemplars. It is available at: http://www.gaussianprocess.org/gpml/data/. The regression methods to which we compared performances on this dataset are: Gaussian Mixture Regression (GMR, [3]), Gaussian Process Regression (GPR, [62]), Local Gaussian Process Regression (LGP, [61]), support vector regression ( $\nu$-SVR, [63]) and LWPR, [44]. All those algorithms were tuned with reasonable effort to obtain the best generalization results. For ILO-GMR, optimal tuning was done with $\boldsymbol{N}=\mathbf{2 0 0}$ and $\boldsymbol{k}=\mathbf{2}$, but results degrade very slowly</p>
<p>TABLE I
Performances of Regression Methods Using the SARCOS Data Set</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GMR</th>
<th style="text-align: center;">Y- <br> SVR</th>
<th style="text-align: center;">ILO- <br> GMR</th>
<th style="text-align: center;">LWPR</th>
<th style="text-align: center;">GRR</th>
<th style="text-align: center;">LGP</th>
<th style="text-align: center;">LR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.0075</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.081</td>
</tr>
</tbody>
</table>
<p>when moving away from these parameters. Table I shows the comparison of the performances of those algorithms for predicting the torques of the first joint in the SARCOS database. We observe that the performance of ILO-GMR matches nearly the best performance (GPR), is slightly better than $\nu$-SVR, LGP and GMR, and clearly better than LWPR while being also incremental but much easier to tune.</p>
<p>Furthermore, in spite of the fact that our current implementation of ILO-GMR was done in Matlab and is not optimized, it is already able to make a single prediction and incorporate a new learning exemplar in around 10 milliseconds on a standard laptop computer and when 44484 SARCOS data examples are already in memory. Furthermore, we have measured experimentally the evolution of training and prediction time per new exemplar: it increases approximately linearly with a small slope in the range $0-44484$ learning exemplars. Further work will study systematically the computational complexity and scalability of ILO-GMR.</p>
<p>Furthermore, learning forward motor models is mainly useful if they can be reused for robot control, hence for inferring inverse motor models [46], [48]. This brings up difficult challenges since most robotic systems are highly redundant, which means that the mapping from motor targets in the task space to motor commands in the joint/articulatory space is not a function: one target may correspond to many motor articulatory commands. This is why learning directly inverse models with standards regression algorithm is bound to fail in redundant robots, since when asked to find an articulatory configuration that yields a given target configuration, it will typically output the mean of accurate solutions which is itself not an accurate solutions. Fortunately, there are various approaches to go around this problem [46], [48], and one of them is specific to the GMM/GMR approach [50], called the single component least square estimate (SLSE): because this approach encodes joint distributions rather than functions, redundancies are encoded in the GMM and inverse models can be computed by projecting the joint distribution on the corresponding output dimensions and then doing regression based only a the single Gaussian component that gives the highest posterior probability at the given input point. This approach is readily applicable in ILO-GMR, which we have done for the second experiment described below.</p>
<h2>IV. EXPERIMENT WITH A SIMPLE SIMULATED ROBOT</h2>
<p>In this section, we describe the behavior of the IAC and R-IAC algorithms, in a simple sensorimotor environment that allows us to show visually significant qualitative and quantitative differences, as well as compare them with random exploration. In these experiments, the parameters of IAC and R-IAC are $\boldsymbol{T}<em _mathbf_1="\mathbf{1">{\text {split }}=250$, and the learning progress window is 50 . Also, probabilities are $\boldsymbol{p}</em>}}=0.3, \boldsymbol{p<em _mathbf_1="\mathbf{1">{\mathbf{2}}=0.7$ in IAC and $\boldsymbol{p}</em>}}=0.3, \boldsymbol{p<em _mathbf_3="\mathbf{3">{\mathbf{2}}=0.6, \boldsymbol{p}</em>=0.1$, in R-IAC, The incremental learning algorithm that is used to learn the forward model
}<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Representation of a two axes arm, with a one pixel camera mounted on its extremity. This arm is put in the center of a cubic room, with different painted walls of different complexities.
is the ILO-GMR system described in part III, with the same parameters in both IAC and R-IAC experiments ( $\boldsymbol{k}=2$ and $\boldsymbol{N}=100)$.</p>
<h2>A. Robotics Configuration</h2>
<p>We designed a simulated mechanical system, using the Matlab robotics toolbox [54]. It consists of a robotic arm using two degrees of freedom, represented by the two rotational axes $\boldsymbol{q}<em 2="2">{1}, \boldsymbol{q}</em>$ as shown on Fig. 6. The upper part of the arm has been conceived as a bow, which creates a redundancy in the system: for each position and orientation of the tip of the arm, there are two corresponding possible articulatory/joint angle configurations.</p>
<p>This system's sensory equipment consists of a one-pixel camera, returning an intensity value $\boldsymbol{p}$, set on its extremity as shown on Fig. 6. The arm is put in a cubic painted environment $V$, whose wallpapers are visible to the one-pixel camera, according to articulatory configurations. Intensity values measured by the cameras are consequences of both environment oudey $\boldsymbol{V}$ and rotational axes $\boldsymbol{q}<em 2="2">{1}, \boldsymbol{q}</em>}$. So, we can describe the system input/output mapping with two input dimensions, and one output as $\boldsymbol{p}=\boldsymbol{V}\left(\boldsymbol{q<em 2="2">{1}, \boldsymbol{q}</em>\right)$.</p>
<p>Thus, in this system the mapping to be learnt is state independent since here trajectories are not considered (only end positions are measured) and the perceptual result of applying motor joint angle commands does not depend on the starting configuration.</p>
<h2>B. Environment Configuration</h2>
<p>The front wall consists of an increasing precision checker (Fig. 7), conceived with a black and white pattern. The designed ceiling contains animated wallpaper with white noise, returning a random value to the camera when this one is watching upward bound. Finally, other walls and ground are just painted in white (Fig. 6).</p>
<p>The set up of the system is such that we can sort three kinds of subregions in the sensorimotor space.</p>
<ul>
<li>The arm is positioned such that the camera is watching the front wall: for most learning algorithm, this subregion is rather difficult to learn with an increasing level of complexity from left to right (see Fig. 7). This feature makes it</li>
</ul>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Wallpaper disposed in the front wall. For many learning algorithms, the complexity increases from left to right.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Twp-Dimensions visualization of the sensorimotor space of the robot, with two motor dimensions $\left(\boldsymbol{q}<em 2="2">{1}, \boldsymbol{q}</em>\right)$ and one sensory dimension.
particularly interesting to study whether IAC or R-IAC are able to spot these properties and control the complexity of explored sub-subregions accordingly.</p>
<ul>
<li>The arm is positioned such that the camera is watching the ceiling: the measured intensity values are random, and thus there are no correlations between motor configurations and sensory measures. Hence, once a few statistical properties of the sensory measures have potentially been learnt (such as the mean), nothing more can be learnt and thus no learning progress can happen.</li>
<li>The arm is positioned such that a white wall is in front of the camera: the measured intensity value is always 0 , so the input/output correlation is trivial. Thus, after it has been learnt that intensity values are constant in this area, nothing can be further learnt.
Because the system has just two motor dimensions and one sensory dimension, it can be visualized using a 2D projection on a plane such as in Fig. 8. This projection shows a central vertical zone corresponding to the dynamic noise projected on the ceiling. Then, we can easily distinguish the front wall, represented on both sides of the noisy area, because of the redundancy
of the arm. The remaining white parts correspond to other walls and the floor.</li>
</ul>
<h2>C. Results: Exploration Areas</h2>
<p>First, it is interesting to perform qualitative comparisons of the exploration behavior generated by random exploration, IAC exploration and R-IAC exploration methods.</p>
<p>For each exploration method, the system is allowed to explore its sensorimotor space through 20000 sensorimotor experiments, i.e., it is allowed to collect 20000 learning exemplars. During each run of a given method, every 2000 sensorimotor experiments made by the system one computes a 2-D smoothed histogram representing the distribution of explored sensorimotor configurations in the last 500 sensorimotor experiments. This allows us to visualize the evolution of the exploration focus, over time, for each system. Random exploration obviously leads to a flat histogram.</p>
<p>Fig. 9 presents typical results obtained with R-IAC (on the left) and IAC (on the right), on a grey scale histogram where darker intensities denote low exploration focus and lighter intensities denote higher exploration focus. First, we observe that R-IAC is focusing on the front wall, containing the image of the checker, using its two possible redundant exploration positions. It avoids the region which contains the white noise, and also the regions just containing a white color. In contrast, we cannot observe the same accuracy to concentrate sensorimotor experiments over interesting areas with the IAC exploration method.</p>
<p>Here, the algorithm is indeed avoiding the noise, but we cannot observe precisely some interest toward the front wall, and the system seems to find some things to learn in the back wall, as we can see, watching the bottom-right part of the two last images.</p>
<p>The histograms in Fig. 9 were smoothed with a gaussian spatial frequency filter to allow us to visualize well the global exploratory behavior. Nevertheless, it is also interesting to use a smaller spatial frequency smoother in order to zoom in and visualize the details of the exploration behavior in the front wall region. Fig. 10 shows a typical result obtained with R-IAC, just considering exemplars performed watching the front wall in the bottom-left side of the 2-D projection.</p>
<p>This sequence shows very explicitly that the system first focuses exploration on zones of lower complexity and progressively shifts its exploration focus towards zones of higher complexity. The system is thus able to evaluate accurately the different complexities of small parts of the world, and to drive the exploration based on this evaluation.</p>
<h2>D. Results: Active Learning</h2>
<p>We can now compare the performances of random exploration, IAC exploration and R-IAC exploration in terms of their efficiency for learning as fast as possible the forward model of the system. For the R-IAC method, we included here a version of R-IAC without the multiresolution scheme to assess the specific contribution of multiresolution learning progress monitoring in the results.</p>
<p>For each exploration method, 30 experiments were run in order to be able to measure means and standard deviations of the evolution of performances in generalization. In each given</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. Evolution of the exploration focus when using R-IAC as an exploration heuristics (left) or IAC (right). Each square represents the smoothed distribution of explored motor configurations at different times in a given run and over a sliding time window. Darker intensities denote low exploration focus and lighter intensities denote higher exploration focus. We observe that R-IAC leads the system to explore preferentially motor configurations such that the camera is looking at the checkerboard, while avoiding zones that are trivial to learn or unlearnable zones. On the contrary, IAC is unable to organize exploration properly and "interesting" zones are much less explored.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. A zoom into the evolution of the distribution of explored sensorimotor experiments in one of the two subregions where the camera is looking at the checkerboard when R-IAC is used. We observe that exploration is first focused on zones of the checkerboard that have a low complexity (for the given learning algorithm), and progressively shifts towards zones of increasing complexity.
experiment, every 5000 sensorimotor experiment achieved by the robot, we froze the system and tested its performances in generalization for predicting $\boldsymbol{p}$ from $\left(\boldsymbol{q}<em 2="2">{1}, \boldsymbol{q}</em>\right)$ on a test database generated beforehand and independently consisting of random uniform queries in the sensorimotor subspace where there are learnable input/output correlations (i.e., excluding the zone with white noise). Results are provided on Fig. 11. As we can easily observe, and as already shown in [27], using IAC leads to learning performances that are statistically significantly higher than with RANDOM exploration. Yet, as Fig. 11 shows, results of R-IAC are statistically significantly higher than IAC, and the difference between IAC and R-IAC is larger than between IAC and random exploration. Finally, we observe that including the multiresolution scheme into R-IAC provides a clear improvement over R-IAC without multiresolution, especially in the first half of the exploration trajectory where inappropriate or too early region splits can slow down the efficiency of exploration if only leaf regions are taken into account for region selection.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. Mean and standard deviation of prediction errors with IAC, R-IAC with only local resolution, and R-IAC with multiresolution, compared with the random exploration approach.</p>
<h2>V. The Hand-Eye-Cloods Experiment</h2>
<p>We will now compare the performances of IAC and R-IAC as active learning algorithms to learn a forward model in a more</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12. Experimental setup. The 2D robot has two arms, each with two links and two revolute joints. At the tip of the right arm is rigidly attached a square camera/eye which can sense either the position of the tip of the other arm in its own reference frame ( $\boldsymbol{X}, \boldsymbol{Y}$ ) if it is above it, but which can also sense the position of randomly moving clouds when the right arm motor configuration is such that the camera is looking over the top grey area (the «window»). When the camera senses something, the robot does not know initially whether this corresponds to the tip of its left arm or to a cloud. In subregions corresponding to the first alternative, the motor/sensor mapping is correlated and a lot can be learnt. In subregions corresponding to the second alternative, there are no correlations between motors and sensors and nothing can be learnt except some basic statistical properties of the random movement of clouds. There is a third alternative, which actually happens most of the time if the joint space is sampled randomly: the camera looks below the window but does not see its left arm tip. In this very large subregion, the motor to sensor mapping is trivial.
complex six-dimensional robotic sensorimotor space that includes large unlearnable zones. Both algorithms will also be compared with baseline random exploration.</p>
<h2>A. Robotics Configuration</h2>
<p>In this experiment, a simulated robot has two 2-D arms, each with two links and two revolute joints whose angles are controlled by motor inputs $\boldsymbol{q}<em 12="12">{11}, \boldsymbol{q}</em>}, \boldsymbol{q<em 22="22">{21}, \boldsymbol{q}</em>)$ of point-blobs relative to the square. These point-blobs can be either the tip of the other arm or clouds in the sky (see Fig. 12). This means that when the right arm is positioned such that the camera is over the clouds, which move randomly, the relation between motor configurations and perception is quasi-random. If on the contrary the arms are such that the camera is on top of the tip of the other arm, then there is an interesting sensorimotor relationship to learn. Formally, the system has the relation}$ (see Fig. 12). On the tip of one of the two arms is attached a square camera capable of detecting the sensory position $(\boldsymbol{x}, \boldsymbol{y</p>
<p>$$
(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{E}\left(\boldsymbol{q}<em 12="12">{11}, \boldsymbol{q}</em>}, \boldsymbol{q<em 22="22">{21}, \boldsymbol{q}</em>\right)
$$</p>
<p>where $(\boldsymbol{x}, \boldsymbol{y})$ is computed in the following way.</p>
<p>1) The camera is placed over the white wall: nothing has been detected $(\boldsymbol{x}, \boldsymbol{y})=(-10,-10)$.
2) The camera is on top of the left hand: the value $(\boldsymbol{x}, \boldsymbol{y})$ of the relative position of the hand in the camera reference frame $\boldsymbol{C}$ is taken. According to the camera size, the $\mathbf{x}$ and $\mathbf{y}$ values are in the interval $[0 ; 6]$.
3) The camera is looking at the window: Two random values $(\boldsymbol{x}, \boldsymbol{y})$ playing the role of random clouds displacement are chosen for output. The interval of outputs corresponds to camera size.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13. Mean distributions (and standard deviations) over 30 simulations of distances between the hand and the center of the eye when exploration is random, guided by IAC, or guided by R-IAC. We observe that while IAC pushes the system to explore slightly more than random exploration the zones of the sensorimotor space where the tip of the left arm is perceived by the camera or near the camera, R-IAC is significantly more efficient than IAC for driving exploration in the "interesting" area.
4) The camera is looking at the window and sees both hand and cloud: the output value $(\boldsymbol{x}, \boldsymbol{y})$ is random, like if just a cloud had been detected.
This setup can be thought to be similar to the problems encountered by infants discovering their body: they do not know initially that among the blobs moving in their field of view, some of them are part of their "self" and can be controlled, such as the hand, and some other are independent of the self and cannot be controlled (e.g., cars passing in the street or clouds in the sky).</p>
<p>Thus, in this sensorimotor space, the "interesting" potentially learnable subspace is next to a large unlearnable subspace, and also next to a large very simple subspace (when the camera is looking neither to the clouds nor to the tip of the other arm).</p>
<h2>B. Results</h2>
<p>In these experiments, the parameters of IAC and R-IAC are $\boldsymbol{T}<em 1="1">{\text {split }}=250$, the learning progress window is 50 . Also, probabilities are, $\boldsymbol{p}</em>}=0,3, \boldsymbol{p<em 1="1">{2}=0,7$ in IAC and $\boldsymbol{p}</em>}=0,3, \boldsymbol{p<em 3="3">{2}=0,6$, $\boldsymbol{p}</em>=100$ ).}=0,1$, in R-IAC. Experiments span a duration of 100000 sensorimotor experiments. The incremental learning algorithm that is used to learn the forward model is the ILO-GMR system described in part III, with the same parameters in both IAC and R-IAC experiments ( $\boldsymbol{k}=2$ and $\boldsymbol{N</p>
<p>A first study of what happens consists of monitoring the distance between the center of the eye (camera), and the hand (tip of the other arm). A small distance means that the eye is looking the hand, and a high, that it is focusing on clouds (noisy part) or on the white wall. Fig. 13 shows histograms of these distances. We first observe the behavior of the random exploration algorithm. The curve shows that the system is, in majority, describing actions with a distance of 22 , corresponding to the camera looking at clouds or at the white wall. Interestingly, the curve of the IAC algorithm is similar but slightly displaced towards shorter distance: this shows that IAC pushed the system to explore the "interesting" zone a little more. We finally observe that R-IAC shows a large difference with both IAC and random exploration: the system spends three times more time in a distance less than</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14. Left: evolution of the generalization capabilities of the learnt forward model with Random exploration, IAC, and R-IAC, averaged over 30 simulations. Right: evolution of performances in control based on the forward model learnt through random exploration, IAC exploration, and R-IAC exploration, averaged over 30 simulations.
eight, i.e., exploring sensorimotor configurations in which the camera is looking at the other arm's tip. Thus, the difference between R-IAC and IAC is more important than the difference between IAC and random exploration.</p>
<p>Then, we evaluated the quality of the learnt forward model using the three exploration algorithms. We considered this quality in two respects: 1) the capability of the model to predict the position of the hand in the camera given motor configurations for which the hand is within the field of view of the robot; 2) the capacity to use the forward model to control the arm: given a right arm configuration and a visual objective, we tested how far the forward model could be used to drive the left arm to reach this visual objective with the left hand. The first kind of evaluation was realized by first building independently a test database of 1000 random motor configurations for which the hand is within the field of view, and then using it for testing the learnt models built by each algorithm at various stages of their lifetime (the test consisted in predicting the position of the hand in the camera given joint configurations). Thirty simulations were run, and the evolution of mean prediction errors is shown on the right of Fig. 14. The second evaluation consisted in generating a set of $\left{(\boldsymbol{x}, \boldsymbol{y})<em 21="21">{\boldsymbol{C}}, \boldsymbol{q}</em>}, \boldsymbol{q<em 11="11">{22} \mid \boldsymbol{x}&gt;0\right.$ and $\left.\boldsymbol{y}&gt;0\right}$ values that are possible given the morphology of the robot, and then use the learnt forward models to try to move the left arm, i.e., find $\left(\boldsymbol{q}</em>}, \boldsymbol{q<em _boldsymbol_C="\boldsymbol{C">{12}\right)$ to reach the $(\boldsymbol{x}, \boldsymbol{y})</em>}}$ objectives corresponding to particular $\boldsymbol{q<em 22="22">{21}, \boldsymbol{q}</em>$ values. Control was realized through inferring an inverse model using ILO-GMR as presented in Section III. The distance between the reached point and the objective point was each time measured, and results, averaged over 30 simulations, are reported in the left graph of Fig. 14.</p>
<p>Both curves on Fig. 14 confirm clearly the qualitative results of the previous figure: IAC outperforms significantly R-IAC, which is only slighlty better than random exploration. We have thus shown that R-IAC is much more efficient in such an example of complex inhomogeneous sensorimotor space. We also illustrate on Fig. 15 configurations obtained, considering fixed
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Fig. 15. Examples of performances obtained in control. the first row shows three fixed goals (defined as joint position of the right hand, and a position of the left hand extremity in the reference frame of the eye). The robot has to reach the targets (position fixed in the eye) with the left arm. We observe that the robot which has explored its sensorimotor space with R-IAC reaches all three goals with high accuracy (fourth row), while the robot that explored its sensorimotor space through random (second row) or IAC (third row) exploration are either slightly imprecise for goals 2 and 3 and very imprecise for goal 1.
goals $\left{(\boldsymbol{x}, \boldsymbol{y})<em 21="21">{\boldsymbol{C}}, \boldsymbol{q}</em>\right}$, and estimated positioning of the left hand.}, \boldsymbol{q}_{22</p>
<h2>VI. CONCLUSION</h2>
<p>IAC was initially introduced as a developmental mechanism allowing a robot to self-organize developmental trajectories of increasing complexity without preprogramming the particular developmental stages [27]. In this paper, we have argued that IAC and other intrinsically motivated learning heuristics could be viewed as active learning algorithms, and were based on heuristics that are more suited than traditional active learning algorithms for operation in unprepared sensorimotor spaces with large unlearnable subspaces. Then, we have introduced a novel formulation of IAC, called R-IAC, and shown that its performances as an intrinsically motivated active learning algorithm were far superior to IAC in a complex sensorimotor space where only a small subspace was interesting. We have also shown results in which the learnt forward model was reused in a control scheme.</p>
<p>Further work will study extensions of the current results in several directions. First, experiments presented in this paper were achieved in simulated robots. In spite of the fact that IAC was already evaluated in high-dimensional real robotic systems [27], [34], [36], these experiments were focusing on the selforganization of patterns in developmental trajectories. Evaluating IAC and R-IAC as active learning methods in high-dimensional real sensorimotor robotic spaces remains to be achieved. Second, both IAC and R-IAC heuristics could also be conceptualized as mechanisms for generating internal immediate rewards that could serve as a reward system in a reinforcement learning framework, such as for example in intrinsically motivated reinforcement learning [28], [33], [35]. Leveraging the capabilities of advanced reinforcement learning techniques for sequential action selection to optimize cumulated rewards might</p>
<p>allow IAC and R-IAC to be successfully applied in robotic sensorimotor spaces where dynamical information is crucial, such as for example for learning the forward and inverse models of a force controlled high-dimensional robot, for which guided exploration has been identified as a key research target for the future [47], [48].</p>
<p>Also, as argued in [55], it is possible to devise "compe-tence-based" intrinsic motivation systems in which the measure of interestingness characterizes goals in the task space rather than motor configurations in the motor/joint space such as in knowledge-based intrinsic motivation systems like IAC or R-IAC. We believe that a competence based version of R-IAC would allow increasing significantly exploration efficiency in massively redundant sensorimotor spaces. Finally, an issue of central importance to be studied in the future is how intrinsically motivated exploration and learning mechanisms can be fruitfully coupled with social learning mechanisms, which would be relevant not only for motor learning [56]-[58], but also for developmental language learning grounded in sensorimotor interactions [59].</p>
<h2>ACKNOWLEDGMENT</h2>
<p>The authors would like to thank M. Li for his crucial help in the elaboration and evaluation of the ILO-GMR regression algorithm.</p>
<h2>REFERENCES</h2>
<p>[1] J. Weng, J. McClelland, A. Pentland, and O. Sporns et al., "Autonomous mental development by robots and animals," Science, vol. 291, pp. 599-600, 2001.
[2] M. Lungarella, G. Metta, R. Pfeifer, and G. Sandini, "Developmental robotics: A survey," Connect. Sci., vol. 15, no. 4, pp. 151-190, 2003.
[3] S. Calinon, F. Guenter, and A. Billard, "On learning, representing and generalizing a task in a humanoid robot," IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 37, no. 2, pp. 286-298, Mar. 2007.
[4] M. Lopes, F. S. Melo, and L. Montesano, "Affordance-based imitation learning in robots," in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., San Diego, CA, 2007, pp. 1015-1021.
[5] P. Abbeel and A. Y. Ng, "Apprenticeship learning via inverse reinforcement learning," in Proc. 21st Int. Conf. Mach. Learn., Alberta, Canada, 2004, pp. 1-8.
[6] C. G. Atkeson and S. Schaal, "Robot learning from demonstration," in Proc. 14th Int. Conf. Mach. Learn., Nashville, TN, 1997, pp. 12-20.
[7] A. Alissandrakis, C. L. Nehaniv, and K. Dautenhahn, "Action, state and effect metrics for robot imitation," in Proc. 15th IEEE Int. Symp. Robot Human Interact. Comm., Hatfield, U.K., 2006, pp. 232-237.
[8] B. Argall, S. Chernova, and M. Veloso, "A survey of robot learning from demonstration," Robot. Autonom. Syst., vol. 57, no. 5, pp. $469-483,2009$.
[9] M. Asada, M. Ogino, S. Matsuyama, and J. Oga, "Imitation learning based on visuo-somatic mapping," in Proc. 9th Int. Symp. Exp. Robot., Germany, 2006, vol. 21, pp. 269-278.
[10] P. Andry, P. Gaussier, S. Moga, J. P. Banquet, and J. Nadel, "Learning and communication via imitation: An autonomous robot perspective," IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 31, no. 5, pp. 431-442, Sep. 2001.
[11] Y. Demiris and A. Meltzoff, "The robot in the crib: A developmental analysis of imitation skills in infants and robots," Inf. Child Develop., vol. 17, pp. 43-53, 2008.
[12] M. Pardowitz, S. Knoop, R. D. Zollner, and R. Dillmann, "Incremental learning of tasks from user demonstrations, past experiences, and vocal comments," IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 37, no. 2, pp. 322-332, Mar. 2007.
[13] E. Oztop, M. Kawato, and M. Arbib, "Mirror neurons and imitation: A computationally guided review," Neural Netw., vol. 19, no. 3, pp. $254-271,2006$.
[14] R. Rao, A. Shon, and A. Meltzoff, "A Bayesian model of imitation in infants and robots," in Imitation and Social Learning in Robots, Humans, and Animals. Cambridge, U.K.: Cambridge Univ. Press, 2007, pp. 217-248.
[15] R. C. Arkin, "Moving up the food chain: Motivation and emotion in behavior-based robots," in Who Needs Emotions: The Brain Meets the Robot, J. Fellous and M. Arbib, Eds. London, U.K.: Oxford Univ. Press, 2005, pp. 245-269.
[16] , M. Fellous and M. Arbib, Eds., Who Needs Emotions: The Brain Meets the Robot. London, U.K.: Oxford Univ. Press, 2005.
[17] D. McFarland and T. Bosser, Intelligent Behavior in Animals and Robots. Cambridge, MA: MIT Press, 1993.
[18] R. Manzotti and V. Tagliasco, "From behaviour-based robots to mo-tivation-based robots," Robot. Autonom. Syst., vol. 51, no. 2-3, pp. $175-190,2005$.
[19] A. Stoytchev and R. Arkin, "Incorporating motivation in a hybrid robot architecture," J. Adv. Comput. Intel. Intell. Informat., vol. 8, no. 3, pp. 269-274, 2004.
[20] R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, "An ethological and emotional basis for human-robot interaction," Robot. Autonom. Syst., vol. 42, no. 3, pp. 191-201, 2003.
[21] R. White, "Motivation reconsidered: The concept of competence," Psychology, vol. 66, pp. 297-333, 1959.
[22] D. Berlyne, "Curiosity and exploration," Science, vol. 153, no. 3731, pp. 25-33, 1966.
[23] E. Deci and R. Ryan, Intrinsic Motivation and Self-Determination in Human Behavior. New York: Plenum, 1985.
[24] W. Schultz, "Getting formal with dopamine and reward," Neuron, vol. 36, pp. 241-263, 2002.
[25] P. Dayan and B. Balleine, "Reward, motivation and reinforcement learning," Neuron, vol. 36, pp. 285-298, 2002.
[26] P. Redgrave and K. Gurney, "The short-latency dopamine signal: A role in discovering novel actions?," Nature Rev. Neurosci., vol. 7, no. 12, pp. 967-975, 2006.
[27] P.-Y. Oudeyer, F. Kaplan, and V. Hafner, "Intrinsic motivation systems for autonomous mental development," IEEE Trans. Evol. Comput., vol. 11, no. 2, pp. 265-286, Apr. 2007.
[28] A. Barto, S. Singh, and N. Chentanez, "Intrinsically motivated learning of hierarchical collections of skills," in Proc. 3rd Int. Conf. Develop. Learn., San Diego, CA, 2004, pp. 112-119.
[29] A. Blanchard and L. Caltamero, "Modulation of exploratory behavior for adaptation to the context," in Proc. Biol. Inspired Robot. AISB'06: Adapt. Artif. Biol. Syst., Bristol, U.K., 2006.
[30] R. Der, M. Herrmann, and R. Liebscher, "Homeokinetic approach to autonomous learning in mobile robots," in Proc. Robotik, R. Dillman, R. D. Schraft, and H. Wörn, Eds., Dusseldorf, Germany, 2002, pp. 301-306, VDI.
[31] D. S. Blank, D. Kumar, L. Meeden, and J. Marshall, "Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture," Cybern. Syst., vol. 36, no. 2, pp. 125-150, 2005.
[32] X. Huang and J. Weng, "Novelty and reinforcement learning in the value system of developmental robots," in Proc. 2nd Int. Workshop Epig. Robot.: Modeling Cogn. Develop. Robot. Syst., Edinburgh, Scotland, Aug. 10-11, 2002, pp. 47-55.
[33] J. Schmidhuber, "Curious model-building control systems," in Proc. Int. Joint Conf. Neural Netw., Singapore, 1991, vol. 2, pp. 1458-1463.
[34] P.-Y. Oudeyer and F. Kaplan, "Discovering communication," Connect. Sci., vol. 18, no. 2, pp. 189-206, 2006.
[35] M. Schembri, M. Mirolli, and G. Baldassarre, "Evolution and learning in an intrinsically motivated reinforcement learning robot," in Proc. Euro. Conf. Artif. Life, 2007, pp. 294-303.
[36] F. Kaplan and P.-Y. Oudeyer, "Intrinsically motivated machines," in 50 Years of AI, Festschrift, M. Lungarella, F. Iida, J. Bongard, and R. Pfeifer, Eds. : Springer-Verlag, 2007, vol. 4850, LNAL pp. 304-315.
[37] V. Fedorov, Theory of Optimal Experiment. New York: Academic, 1972.
[38] E. J. Gibson, Principles of Perceptual Learning and Development. New York: Appleton-Century-Crofts, 1969.
[39] D. Berlyne, Conflict, Arousal, and Curiosity. New York: McGrawHill, 1960.
[40] M. Csikszentmihalyi, Creativity-Flow and the Psychology of Discovery and Invention. New York: Harper Perennial, 1996.
[41] D. Cohn, Z. Ghahramani, and M. Jordan, "Active learning with statistical models," J. Artif. Intell. Res., vol. 4, pp. 129-145, 1996.
[42] M. Hasenjager and H. Ritter, "Active learning in neural networks," in New Learning Paradigms in Soft Computing. Berlin, Germany: Physica-Verlag GmbH, 2002, pp. 137-169.</p>
<p>[43] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification. Hoboken, NJ: Wiley, 2006.
[44] S. Vijayakumar and S. Schaal, "LWPR: An O(n) algorithm for incremental real time learning in high dimensional space," in Proc. 17th Int. Conf. Mach. Learn., Stanford, CA, 2000, pp. 1079-1086.
[45] A. D’Souza, S. Vijayakumar, and S. Schaal, "Learning inverse kinematics," in IEEE Int. Conf. Intell. Robots Syst., Piscataway, NJ, 2001, pp. 298-303.
[46] J. Peters and S. Schaal, "Learning to control in operational space," Int. J. Robot. Res., vol. 27, pp. 197-212, 2008.
[47] C. Salaün, V. Padois, and O. Sigaud, "Control of redundant robots using learned models: An operational space control approach," in Proc. IEEE Int. Conf. Intell. Robots Syst., Piscataway, NJ, 2009, pp. 878-885.
[48] D. Y. Yeung and Y. Zhang, "Learning inverse dynamics by Gaussian process regression under the multi-task learning framework," in The Path to Autonomous Robots, G. S. Sukhatme, Ed. New York: Springer-Verlag, 2009, pp. 131-142.
[49] Z. Ghahramani, "Solving inverse problems using an EM approach to density estimation," in Proc. 1993 Connectionist Models Summer School, M. C. Mozer, P. Smolensky, D. S. Tourezky, J. L. Elman, and A. S. Weigend, Eds. Hillsdale, NJ: Erlbaum Associates, 1993, pp. $316-323$.
[50] C. E. Rasmussen, "Evaluation of Gaussian Process and Other Methods for Non-Linear Regression," PhD thesis, Dept. Comp. Sci., Univ. Toronto, Toronto, ON, Canada, 1996.
[51] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu, "An optimal algorithm for approximate nearest neighbor searching," $J$. Assoc. Comput. Mach., vol. 45, pp. 891-923, 1998.
[52] S. Maneewongvatana and D. M. Mount, "Analysis of approximate nearest neighbor searching with clustered point sets, data structures, near neighbor searches, and methodology," in Fifth and Sixth DIMACS Implementation Challenges, M. H. Goldwasser, D. S. Johnson, and C. C. McGeoch, Eds., 2002, vol. 59, DIMACS Series in Discr. Math. and Theor. Comput. Sci., pp. 105-123, AMS.
[53] D. Filliat, "A visual bag of words method for interactive qualitative localization and mapping," in Proc. Int. Conf. Robot. Autom., 2007, pp. 3921-3926.
[54] P. I. Corke, "A robotics toolbox for matlab," IEEE Robot. Autom. Mag., vol. 1, no. 3, pp. 24-32, Sep. 2006.
[55] P.-Y. Oudeyer and F. Kaplan, "How can we define intrinsic motivation?," in Proc. 8th Int. Conf. Epig. Robot.: Modeling Cogn. Develop. Robot. Syst., Brighton, 2008.
[56] Y. Kuniyoshi, Y. Yorozu, M. Inaba, and H. Inoue, "From visuo-motor self learning to early imitation-a neural architecture for humanoid learning," in Proc. IEEE Int. Conf. Robot. Autom., 2003, vol. 3, pp. 3132-3139.
[57] M. Lopes, F. Mello, L. Montesano, and J. Santos-Victor, "Cognitive processes in imitation: Overview and computational approaches," in From Motor to Interaction Learning in Robots, O. Sigaud and J. Peters, Eds. New York: Springer LNCS, (unpublished).
[58] A. L. Thomaz and C. Breazeal, "Experiments in socially guided exploration: Lessons learned in building robots that learn with and without human teachers," Connect. Sci., vol. 20, no. 2\&amp;3, pp. 91-110, 2008.
[59] F. Kaplan, P.-Y. Oudeyer, and B. Bergen, "Computational models," Debate Lang. Learnability, Inf. Child Develop., vol. 17, no. 1, pp. $55-80,2008$.
[60] D. Koch and A. Billard, "Gaussian mixture regression and its application in robotics," in Proc. Workshop Regress. Robot., 2009.
[61] D. Nguyen-Tuong and J. Peters, "Local Gaussian processes regression for real-time model-based robot control," in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Piscataway, NJ, 2008, pp. 380-385.
[62] D. Y. Yeung and Y. Zhang, "Learning inverse dynamics by Gaussian process regression under the multi-task learning framework," in The Path to Autonomous Robots, G. S. Sukhatme, Ed. New York: Springer-Verlag, 2009, pp. 131-142.
[63] C.-C. Chang and C.-J. Lin, LIBSVM: A Library for Support Vector Machines 2001 [Online]. Available: http://www.csie.ntu.edu.tw/ cjlin/ libsvm
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Adrien Baranès received the M.S. degree in artificial intelligence and robotics from the University Paris VI, France, in 2008. He is now working towards the Ph.D. degree at the French National Institute of Computer Sciences and Control (INRIA), Talence, France, where his research topic is the intrinsic motivations paradigm, studied in both knowledge and competence based frameworks.</p>
<p>His research interests are machine learning, psychological and developmental approaches of robotics, and computational neurosciences.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Pierre-Yves Oudeyer received the M.Sc. degree in theoretical computer science from Ecole Normale Supérieure de Lyon, France, and received the Ph.D. degree in artificial intelligence from the University Paris VI, France.</p>
<p>After eight years as a permanent researcher at Sony CSL Paris, he is now a Research Scientist at INRIA, France, where he heads the FLOWERS team. He has published a book, more than 60 papers in international journals and conferences, and received several prizes for his work in developmental robotics and on the origins of language. He is interested in the mechanisms that allow humans and robots to develop perceptual, motivational, behavioral, and social capabilities to become capable of sharing cultural representations.
Dr. Oudeyer is an Editor of the IEEE CIS NewsLETTER ON AUTONOMOUS Mental DEVELOPMENT, and Associate Editor of the IEEE Transactions ON AUTONOMOUS MENTAL DEVELOPMENT, Frontiers in Neurorobotics, and of the International Journal of Social Robotics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Manuscript received August 07, 2009; revised October 30, 2009. First published December 01, 2009; current version published February 05, 2010.
The authors are with the FLOWERS Team, French National Institute of Computer Science and Control (INRIA), Bordeaux Sud-Ouest, Talence 33640 France (e-mail: adrien.baranes@inria.fr; pierre-yves.oudeyer@inria.fr).
Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TAMD.2009.2037513&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
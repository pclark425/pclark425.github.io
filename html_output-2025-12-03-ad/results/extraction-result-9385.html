<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1748ba02aa26397c28f3182b4db089669654354a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1748ba02aa26397c28f3182b4db089669654354a" target="_blank">How do you go where?: improving next location prediction by learning travel mode information using transformers</a></p>
                <p><strong>Paper Venue:</strong> SIGSPATIAL/GIS</p>
                <p><strong>Paper TL;DR:</strong> A transformer decoder-based neural network is proposed to predict the next location an individual will visit based on historical locations, time, and travel modes, which are behaviour dimensions often overlooked in previous work.</p>
                <p><strong>Paper Abstract:</strong> Predicting the next visited location of an individual is a key problem in human mobility analysis, as it is required for the personalization and optimization of sustainable transport options. Here, we propose a transformer decoder-based neural network to predict the next location an individual will visit based on historical locations, time, and travel modes, which are behaviour dimensions often overlooked in previous work. In particular, the prediction of the next travel mode is designed as an auxiliary task to help guide the network's learning. For evaluation, we apply this approach to two large-scale and long-term GPS tracking datasets involving more than 600 individuals. Our experiments show that the proposed method significantly outperforms other state-of-the-art next location prediction methods by a large margin (8.05% and 5.60% relative increase in F1-score for the two datasets, respectively). We conduct an extensive ablation study that quantifies the influence of considering temporal features, travel mode information, and the auxiliary task on the prediction results. Moreover, we experimentally determine the performance upper bound when including the next mode prediction in our model. Finally, our analysis indicates that the performance of location prediction varies significantly with the chosen next travel mode by the individual. These results show potential for a more systematic consideration of additional dimensions of travel behaviour in human mobility prediction tasks. The source code of our model and experiments is available at https://github.com/mie-lab/location-mode-prediction.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9385",
    "paper_id": "paper-1748ba02aa26397c28f3182b4db089669654354a",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00450375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How do you go where? Improving next location prediction by learning travel mode information using transformers</h1>
<p>Ye Hong<br>hongy@ethz.ch<br>Institute of Cartography and<br>Geoinformation, ETH Zurich<br>Zurich, Switzerland</p>
<p>Henry Martin<br>martinhe@ethz.ch<br>Institute of Cartography and<br>Geoinformation, ETH Zurich<br>Zurich, Switzerland<br>Institute of Advanced Research in<br>Artificial Intelligence (IARAI)<br>Vienna, Austria</p>
<p>Martin Raubal<br>mraubal@ethz.ch<br>Institute of Cartography and<br>Geoinformation, ETH Zurich<br>Zurich, Switzerland</p>
<h2>ABSTRACT</h2>
<p>Predicting the next visited location of an individual is a key problem in human mobility analysis, as it is required for the personalization and optimization of sustainable transport options. Here, we propose a transformer decoder-based neural network to predict the next location an individual will visit based on historical locations, time, and travel modes, which are behaviour dimensions often overlooked in previous work. In particular, the prediction of the next travel mode is designed as an auxiliary task to help guide the network's learning. For evaluation, we apply this approach to two large-scale and long-term GPS tracking datasets involving more than 600 individuals. Our experiments show that the proposed method significantly outperforms other state-of-theart next location prediction methods by a large margin ( $8.05 \%$ and $5.60 \%$ relative increase in F1-score for the two datasets, respectively). We conduct an extensive ablation study that quantifies the influence of considering temporal features, travel mode information, and the auxiliary task on the prediction results. Moreover, we experimentally determine the performance upper bound when including the next mode prediction in our model. Finally, our analysis indicates that the performance of location prediction varies significantly with the chosen next travel mode by the individual. These results show potential for a more systematic consideration of additional dimensions of travel behaviour in human mobility prediction tasks. The source code of our model and experiments is available at https://github.com/mie-lab/location-mode-prediction.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Information systems $\rightarrow$ Geographic information systems; Location based services; $\cdot$ Computing methodologies $\rightarrow$ Neural networks; $\cdot$ Applied computing $\rightarrow$ Transportation.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Mobility, Deep learning, Location prediction, Travel behaviour</p>
<h2>1 INTRODUCTION</h2>
<p>The rapid urbanization process in the last decades has caused a constant increase in individual travel, imposing significant challenges in achieving sustainable cities. To meet the sustainable development goals of the United Nations [12], mobility behaviour change and new mobility concepts to promote these changes will play indispensable roles [26]. These mobility concepts, such as mobility as
a service (MaaS) [30], smart charging [43] and ride-sharing [16], all rely on the capability to proactively provide personalized services that are tailored to the travel context and individuals' characteristics [23]. Individual mobility prediction to know when and where travel will occur is a crucial technique driving the development and application of these new concepts and, therefore, a key technology for sustainable transportation.</p>
<p>The prediction of where an individual will go given her historical mobility information is central to individual mobility prediction. The problem is also known as the next location prediction and has attracted much attention over the last decade. Researchers are increasingly interested in tackling this problem using learning-based methods thanks to the booming of deep learning (DL) models [22]. As it can be formulated as a sequence prediction problem, similar to the tasks encountered in natural language processing and audio processing, models that have shown success in these two fields are often directly applied. In particular, the transformer model [39] that utilizes a multi-head self-attention mechanism has revolutionized various sequence modelling tasks due to its powerful and efficient network structure. Transformer models are also starting to gain attention in predicting individual mobility, as they tackle some challenges in mobility prediction by design: (1) Multiple periodicities co-exist in the location visitation patterns of individuals [9] (e.g., daily, weekly). These periodicities vary considerably across individuals. The multi-head self-attention module allows the network to focus on multiple steps in the input sequence, effectively capturing these periodicities. (2) The long-term dependency of mobility behaviour. Studies have shown that the current mobility depends on behaviours conducted days or weeks before [4, 37], which requires the prediction model to capture long-term dependencies. The design of the transformer model enables efficient learning of these dependencies. However, human mobility also exhibits unique characteristics, such as complex spatio-temporal dependencies [9, 20] and the inherent stochasticity of location visits [36], which hinder the performance when applying sequence learning models on raw location visit sequences. Therefore, learning to predict the next location directly from historical location visits is challenging. An accurate prediction model should consider context information that influences individuals' choice of locations.</p>
<p>Results from travel behaviour studies that aim to understand individuals' activity location choices could guide the consideration of context information. Empirical evidence suggests that the selection of activity locations is highly correlated to other aspects</p>
<p>of individual travel behaviour, such as the availability of travel modes [27] and the day of the week [7]. However, the comprehensive information regarding individuals' travel behaviour is not fully utilized in location prediction problems. To date, it is still unclear (1) how strong the influence of these long-term factors is on choosing the immediate next location and (2) whether the DL network can benefit from this knowledge and learn the complex dependency patterns directly from data.</p>
<p>To close this research gap and answer the above questions, we propose a transformer-based model that utilizes historical travel behaviour to predict individuals' next location. More precisely, the model aims to learn mobility transition patterns from historical location, temporal and travel mode sequence information. Inspired by travel behaviour studies, we encourage the model to also predict the next travel mode the individual will choose. We anticipate that this ancillary task will help the prediction of the next location. Through experiments on two real-world GPS datasets, we demonstrate the effectiveness of our model design and quantify the dependency of location prediction performance on travel mode. Our results show that careful consideration of individual travel behaviour significantly benefits human mobility prediction. In short, our contributions are summarized as follows:</p>
<ul>
<li>We propose a transformer decoder-based neural network that utilizes location, travel mode and time-related information for the next location prediction task. The proposed model achieves state-of-the-art performance.</li>
<li>We show that jointly learning the next location and next mode improves the prediction performance for both tasks.</li>
<li>We conduct extensive experiments on two real-world GPS tracking datasets and conclude that considering additional aspects of travel behaviour significantly increases the performance of next location prediction.</li>
</ul>
<p>The rest of this paper is organized as follows. We first systematically review related work in Section 2. In Section 3, we formulate the next location prediction problem. Next, we introduce details of the network architecture in Section 4. We apply our model to two real-world GPS datasets and analyze its performance in Section 5. Finally, we summarize the main findings and conclude the paper in Section 6.</p>
<h2>2 RELATED WORK</h2>
<h3>2.1 Next location prediction</h3>
<p>The next location prediction problem has found application in many different fields, such as recommendation systems [44], sensor networks [28], and mobility behaviour analysis [41, 42]. The exact definition of the problem varies across studies due to different objectives and employed datasets. For example, location-based social network (LBSN) applications focus on predicting the next checkin point-of-interest (POI) [40, 44]. In contrast, mobility behaviour studies aim to understand the next location for a user to conduct an activity [34]. Here we focus on the methods proposed for mobility applications.</p>
<p>The last decade has witnessed the expansion of studies focusing on next location prediction. Markov Chain and its variants are probably the most often employed methods for the task [22]. These
models regard locations as states and construct a transition matrix that encodes the transition probability between states for each individual. Ashbrook and Starner [1] and Gambs et al. [11] both proposed identifying significant locations from GPS data and building a Markov model to predict location transitions. Later Markov model variants that consider collective movements [3] and incorporate location importance [17] further increased the prediction performance. However, Markov-based models struggle to represent the complex sequential patterns in human mobility because of their inherent assumption that the current state only depends on the states of previously limited time steps [20].</p>
<p>Recent advances in DL have also promoted their application in location prediction. As a widely adopted sequence modelling method, recurrent neural network (RNN)-based models, such as Long Short-Term Memory (LSTM) [34] and spatial-temporal (ST)RNN [21], were reported to outperform Markov models by a large margin in the task. Still, vanilla RNN models tend to underweight long-term dependencies when the input sequence length increases. Therefore, studies employed the attention module to capture both short-term and long-term dependencies dynamically [9, 20]. Moreover, the transformer model that builds on top of the multi-head selfattention mechanism [39] have started to gain interest in the field. In particular, Xue et al. [44] proposed MobTcast for considering various contexts with a transformer-based structure and achieved state-of-the-art POI prediction results for LBSN data. Although having great potential in learning the complex spatio-temporal dependencies, limited studies have applied transformer for the location prediction problem.</p>
<h3>2.2 Factors affecting activity location choice</h3>
<p>Understanding the factors affecting activity location choice is beneficial for predicting individuals' mobility, as they can be regarded as prior knowledge and potentially guide the learning of DL models. In the travel behaviour field, the choice of locations is regarded as an integral part of individuals' activity-travel behaviour and has been studied within the activity-based framework [32]. Studies that focus on analysing travel behaviour over time suggest that both stability and variability are found in individuals' activity location choices. For example, Dharmowijoyo et al. [6] showed that the variability of location visits is much larger between weekend-weekday pairs than between weekday-weekday and weekend-weekend pairs. Empirical studies also demonstrate the correlation of different aspects of individual travel behaviour. For example, Susilo and Axhausen [38] reported high repetition in location-mode combinations, suggesting that individuals use the same travel mode to reach their locations. Similar conclusions were reported by Hong et al. [14], where they found that only a subset of all location-mode combinations is essential for describing the mobility behaviour. From this perspective, aspects of travel behaviour can be considered constraints for individuals' choice of activity locations.</p>
<p>A similar problem as the next location prediction is the formulation of an individual's location choice set, which is a crucial component in microscopic traffic simulation models [19]. Instead of predicting the exact next location, the problem aims at generating a set containing all possible locations. Based on time geography theory, potential path areas analysis has been applied to tackle</p>
<p>the problem, suggesting that the choice set is constrained by the travel time [33], time of day [45] and the available travel mode [27]. However, this empirical knowledge is not fully utilized in models for location predictions.</p>
<p>Building on the previous studies, we utilize a transformer-based model for learning spatio-temporal dependencies in individual location visits. In addition, we consider other aspects of travel behaviour that constrain an individual's activity location choices in the learning process.</p>
<h2>3 PROBLEM FORMULATION</h2>
<p>We formulate the next location prediction problem with the following definitions:</p>
<p>Definition 1 (Trajectory). Let $u^{(i)}$ be a user in a set of users $\mathcal{U}=\left{u^{(1)}, u^{(2)}, \ldots, u^{(i \mid \mathcal{U})}\right}$, a trajectory $T^{(i)}=\left[q_{1}, q_{2}, \ldots, q_{n_{u^{(i)}}}\right]$ is a time-ordered sequence composed of $n_{u^{(i)}}$ track points visited by $u^{(i)}$. A track point can be represented as a tuple of $q=\langle p, t\rangle$, where $t$ records the time when the user visits, and $p=\langle x, y\rangle$ represents spatial coordinates in a given reference system, e.g., latitude and longitude.</p>
<p>Definition 2 (Location SEQUEnce). Location is defined when a user remains within a certain geographical radius for a defined time, and is a sub-sequence of the user's trajectory. A location sequence $S^{(i)}=\left[L_{1}, L_{2}, \ldots, L_{w_{u^{(i)}}}\right]$ is a time-ordered sequence composed of $w_{u^{(i)}}$ locations visited by $u^{(i)}$. Each location $L=\langle l, t, e\rangle$ is described by the arrival time $t$, the (main) travel mode $e$ used to reach that location, and the identifier $l \in O$, where $O$ is the set containing all known locations.</p>
<p>Problem 1 (Next location Prediction). Consider the historical location sequence $S_{\text {hist }}^{(i)}=\left[L_{n-m+1}, L_{n-m+2}, \ldots, L_{n}\right]$ that $u^{(i)}$ visited, where $n$ is the current time step and $m$ is the number of considered previous locations, the goal is to predict the location identifier the same user $u^{(i)}$ will visit in the next time step, i.e., $l_{n+1} \in O$.</p>
<p>In this study, we construct the historical location sequence using locations visited in the previous 7 days. Thus, the number of considered previous locations $m$ depends on the user $u^{(i)}$ and the current time step $n$, making the next location prediction a sequence prediction problem with variable sequence length.</p>
<h2>4 METHODS</h2>
<p>Figure 1 presents the overview of the proposed model. Specifically, the model considers historical location, time and travel mode patterns as input and learns from the auxiliary task of predicting the next travel mode to improve the prediction of the next location. The model consists of three major components: (1) feature embedding (Section 4.1); (2) transformer decoder and fully connected (FC) layers (Section 4.2); and (3) loss design and prediction (Section 4.3). The detailed description of each component is presented in the following subsections.</p>
<h3>4.1 Embedding learning</h3>
<p>The representation and modelling of historical information is a vital step for accurately predicting the next location. Most existing prediction models utilize the location visitation sequence to
understand the complex spatial-temporal dependencies. We additionally consider temporal and travel mode information that can provide context for visitation patterns. Moreover, user information is beneficial for the network to identify location sequences travelled by different users and learn user-specific movement patterns. Therefore, we regard the location $l_{k}$, the start time $t_{k}$, and the travel mode $e_{k}$ at any time step $k$ as well as the user $u^{(i)}$ (represented as a unique identifier) of the current sequence as the input features (see Figure 2). Specifically, we extract the time of the day $h_{k}$ (grouped into 15-minute bins) and the day of the week $d_{k}$ from the start time $t_{k}$, aiming to separate different levels of periodicity in location visits.</p>
<p>We now introduce the embedding method to transform each feature from the categorical type to a finite-dimensional real-valued vector. This is a more efficient and effective way to represent the correlation between different categorical values compared to the commonly used one-hot encoding method [10, 20]. The embedding layers are parameter matrices that provide mappings between the original variable and the real-valued vector, whose parameters are jointly optimized with the entire network. The feature construction process can be formulated as follows:</p>
<p>$$
\begin{aligned}
e m b_{k}^{l} &amp; =l_{k} \mathbf{W}<em k="k">{1} \
e m b</em>}^{h} &amp; =h_{k} \mathbf{W<em k="k">{h} \
e m b</em>}^{d} &amp; =d_{k} \mathbf{W<em k="k">{d} \
e m b</em>}^{e} &amp; =e_{k} \mathbf{W<em u="u">{e} \
e m b^{u^{(i)}} &amp; =u^{(i)} \mathbf{W}</em>
\end{aligned}
$$</p>
<p>where $e m b_{k}^{l}, e m b_{k}^{h}, e m b_{k}^{d}, e m b_{k}^{e} \in \mathbb{R}^{d_{\text {base }}}$ and $e m b^{u^{(i)}} \in \mathbb{R}^{d_{\text {user }}}$ are the respective embedding vectors, and $l_{k}, h_{k}, d_{k}, e_{k}$ and $u^{(i)}$ are the respective one-hot encoded original categorical features. $\mathbf{W}$ terms stand for weight matrices that are optimized during training. See Figure 2 for an illustration of the embedding process. Finally, the total embedding vector $e m b_{k}^{a l l}$ for each time step $k$ is obtained by adding all sequence features together with a position encoding PE:</p>
<p>$$
e m b_{k}^{a l l}=e m b_{k}^{l}+e m b_{k}^{h}+e m b_{k}^{d}+e m b_{k}^{e}+P E
$$</p>
<p>We use the original positional encoding proposed by Vaswani et al. [39] that utilizes sine and cosine functions to encode sequence information in the embedding. Positional encoding is essential for the training as the self-attention module does not implicitly assume sequential order [39]. Note that we use addition instead of a concatenation operation to combine embedding vectors [46]. This process allows the model to flexibly balance the importance of each feature, as compared to manually assigning and tuning the sizes for different embedding vectors. After the embedding process, we obtain a single user embedding $e m b^{u^{(i)}}$ and a sequence of embedding vectors $e m b_{k}^{a l l}$ representing features at each time step $k$, which can be further processed with the transformer-based network.</p>
<h3>4.2 Transformer and fully connected layers</h3>
<p>An accurate next location prediction model should be able to extract regularities and capture the multilevel periodicity from the complex</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Structure of the proposed next location prediction model. The model learns to represent input features into embedding vectors, which are fed into a transformer decoder neural network. The network's output is concatenated with user embedding and processed through a fully connected residual block for predicting the next location. We introduce the next mode prediction as an auxiliary task to guide the learning process.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Pipeline of generating the embedding vectors.
spatio-temporal historical sequences [9]. We utilize a transformerbased network to learn location transition patterns from historical location, temporal and travel mode information encoded in the final embedding vector $e m b_{k}^{a l l}$. We adopt an architecture similar to the Generative Pre-trained Transformer (GPT) model, originally designed for language modelling [29], where only the transformer decoder part is used. The decoder consists of a stack of $N$ identical blocks (see Figure 3), each with two layers. The first is the masked multi-head attention, and the second is the fully-connected feedforward network with two linear layers and a ReLU activation function. Residual connections and layer normalization components are added to each layer to facilitate learning [13]. The input and output dimension of each block is designed to be the same as the embedding vector, i.e., $d_{\text {model }}=d_{\text {base }}$.</p>
<p>The core of the transformer structure is the multi-head selfattention mechanism. The attention function can be understood as obtaining an output value based on a query and a set of key-value pairs, all of which are vectors of size $d_{k}$. As described by Vaswani et al. [39], transformer uses the scaled dot-product attention and implements the calculation efficiently by packing the set of query, key and value values into matrices Q, K and V. This process can be
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Structure of the transformer decoder neural network and the masked multi-head attention.
formulated as follows:</p>
<p>$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>Then, multi-head attention is constructed by concatenating the results of $H$ attention functions (see Figure 3):</p>
<p>$$
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &amp; =\left(\text { head }<em H="H">{1} \oplus \ldots \oplus \text { head }</em> \
\text { where head }}\right) \boldsymbol{W}^{O<em i="i">{i} &amp; =\operatorname{Attention}\left(Q \boldsymbol{W}</em>}^{Q}, K \boldsymbol{W<em i="i">{i}^{K}, V \boldsymbol{W}</em>\right)
\end{aligned}
$$}^{V</p>
<p>where $\oplus$ represents the concatenation operation and $\boldsymbol{W}$ terms are parameter matrices learned by the network. In each masked multihead attention, key, value and query matrices are identical, all corresponding to the output of the previous block. In the first block, they are set as the embedding matrix $e m b^{a l l}$, obtained through stacking all embedding vectors according to their sequence. Note</p>
<p>that we additionally include the forward-masking operation to prevent the attention function from accessing information from "future" time steps (see Figure 3); that is, the entry at time step $i$ can only focus on the information preceding (and including) $i$. The selfattention mechanism enables the model to access information from every step in the historical sequence and evaluate its importance by learning parameter matrices. This ensures that long-term dependencies in the historical pattern can be extracted. Additionally, thanks to the multi-head design, the model retains multiple sets of parameter matrices that focus on various places in the historical sequence, efficiently capturing the multi-level periodic nature of human mobility.</p>
<p>Then, the output vector of the transformer-decoder model out $_{n}$ is concatenated with the user embedding $e m b^{u^{(i)}}$, and fed into a fully-connected residual block:</p>
<p>$$
f_{n}=F C\left(\text { out }_{n} \oplus e m b^{u^{(i)}}\right)
$$</p>
<p>where $F C(\cdot)$ represents the operation by the fully-connected residual block, whose structure is shown in Figure 4. It consists of a single linear feedforward layer and a ReLU activation function, followed by a dropout layer, residual connection and a batch normalization layer. This block learns the dependencies of the transformer decoder output and the user embedding, helping the model extract user-specific movement patterns. Finally, the aggregated vector representation $f_{n}$ encapsulates location, temporal, travel mode and user information of the entire historical sequence, and can be further used to predict the next location.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Structure of the fully-connected residual block.</p>
<h3>4.3 Loss and training</h3>
<p>The aggregated vector representation $f_{n}$ is then fed into two branches: the next location prediction branch and the next mode prediction branch. For the first task, we calculate the probability of each location through a linear projection and a softmax function:</p>
<p>$$
P\left(l_{n+1}\right)=\operatorname{Softmax}\left(\operatorname{Linear}<em n="n">{1}\left(f</em>\right)\right)
$$</p>
<p>where $P\left(l_{n+1}\right)$ of size $|O|$ contains the probability of all locations to be visited at the next time step. Therefore, the most likely visited location $l_{n+1}$ at time step $n+1$ is the location with the highest probability in $P\left(l_{n+1}\right)$.</p>
<p>Given the ground truth next location $l_{n+1}$ from the training dataset, the task can be regarded as a multi-class classification
problem. We obtain our location prediction loss $\mathcal{L}_{1}$ using the multiclass cross entropy:</p>
<p>$$
\mathcal{L}<em i="1">{1}=-\sum</em>\right)
$$}^{|O|} P\left(l_{n+1}\right)^{(i)} \log \left(P\left(l_{n+1}^{-}\right)^{(i)</p>
<p>where $P\left(l_{n+1}^{-}\right)^{(i)}$ represents the predicted probability of visiting the $i$-th location and $P\left(l_{n+1}\right)^{(i)}$ is the one-hot represented ground truth, i.e., $P\left(l_{n+1}\right)^{(i)}=1$ if the actual visited next location is the $i$-th location, and $P\left(l_{n+1}\right)^{(i)}=0$ otherwise.</p>
<p>In addition to predicting the next location, we enforce the network to predict the next travel mode and optimize the network's parameters based on the joint prediction loss. This design is motivated by the assumption that learning to correctly predict the next travel mode can improve the prediction of the next location. Analogous to the next location probability, the probability of choosing each travel mode $P\left(e_{n+1}^{-}\right)$can be obtained as follows:</p>
<p>$$
P\left(e_{n+1}^{-}\right)=\operatorname{Softmax}\left(\operatorname{Linear}<em n="n">{2}\left(f</em>\right)\right)
$$</p>
<p>Therefore, we introduce the mode prediction loss $\mathcal{L}_{2}$ using the multi-class cross entropy:</p>
<p>$$
\mathcal{L}<em i="1">{2}=-\sum</em>\right)
$$}^{|\mathcal{M}|} P\left(e_{n+1}\right)^{(i)} \log \left(P\left(e_{n+1}^{-}\right)^{(i)</p>
<p>where $|\mathcal{M}|$ is the total number of travel mode categories, $P\left(e_{n+1}^{-}\right)^{(i)}$ represents the probability of using the $i$-th mode at time step $n+1$ and $P\left(e_{n+1}\right)^{(i)}$ is the one-hot represented true next mode.</p>
<p>The final loss is the combination of the two losses. Given training samples in mini-batches of size $B$, the network's parameters are optimized by minimizing the following loss:</p>
<p>$$
\mathcal{L}=\sum_{b=1}^{B}\left(\mathcal{L}<em 2="2">{1}(b)+\theta \mathcal{L}</em>(b)\right)
$$</p>
<p>where $\mathcal{L}<em 2="2">{1}(b)$ and $\mathcal{L}</em>(b)$ correspond to the location prediction loss and the mode prediction loss of the $b$-th training sample in the mini-batch, respectively, and $\theta$ balances the relative weights of the two losses.</p>
<h2>5 EXPERIMENTS</h2>
<h3>5.1 Datasets</h3>
<p>We test the proposed method using longitudinal tracking datasets from two tracking studies performed in Switzerland.</p>
<p>The Green Class (GC) study [24]. The study contains 139 participants based in Switzerland who got access to a comprehensive mobility package consisting of a general public transport pass valid in Switzerland, as well as access to several car- and bike-sharing programs and a battery electric vehicle for their personal use. All participants were asked to record their movements using a tracking app on their phones between November 2016 and January 2018. The app pre-processed the movement of participants into staypoints (stationary behaviour) and triplegs (continuous movement without changing travel mode). The study participants provided high-level activity labels for staypoints (home, work, errand, leisure, wait, and unknown) and mode labels for triplegs (car, e-car, train, bus, tram, bicycle, e-bike, walk, airplane, boat, coach).</p>
<p>The yumuv study [30]. The study involves 498 participants based in and around the city of Z端rich, Switzerland. Participants were tracked over three months between July and September 2020, when COVID-19 case numbers, non-medical interventions and impact on people's daily lives were low in Switzerland. Participants got access to a mobility-as-a-service app called yumuv that provided simplified access to several micro-mobility modes. All participants were tracked via an app on their phones. Similar to the GC study, the app pre-processed movements into staypoints and triplegs that participants labelled with activity labels and travel modes.</p>
<p>We pre-process the raw movement data from the GPS tracking studies into corresponding analysis units for the next location prediction task using Python and the open-source Trackintel framework [25]. We only consider users tracked for more than 300 days in GC and more than 30 days in Yumuv to ensure high temporal tracking coverage. 93 users in GC and 422 users in Yumuv remain. Based on the activity label, we regard a stay point as an activity if its duration is longer than 25 minutes or if it was labelled with a non-trivial purpose (any available purpose except for wait or unknown). Then, the activity stay points are spatially aggregated into locations to account for visits to the same place at different times. We utilized the function provided in Trackintel with parameters $\epsilon=20$ and num_samples $=2$ to generate dataset locations [15]. Locations are also attached with the arrival travel mode. Since travel to a location may involve multiple stages with various travel modes, we determine the main travel mode as the mode with the longest distance [2]. We further group the main travel mode into seven groups: walk, bicycle, train, tram, bus, car and other (including ski, airplane and coach). Table 1 shows the number of locations in both datasets, categorized based on the main travel mode.</p>
<p>Table 1: Main travel mode frequency of locations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mode category</th>
<th style="text-align: center;">GC</th>
<th style="text-align: center;">Yumuv</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Walk</td>
<td style="text-align: center;">55,000</td>
<td style="text-align: center;">80,617</td>
</tr>
<tr>
<td style="text-align: center;">Bicycle</td>
<td style="text-align: center;">4,926</td>
<td style="text-align: center;">24,745</td>
</tr>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">25,495</td>
<td style="text-align: center;">20,260</td>
</tr>
<tr>
<td style="text-align: center;">Tram</td>
<td style="text-align: center;">2,182</td>
<td style="text-align: center;">10,943</td>
</tr>
<tr>
<td style="text-align: center;">Bus</td>
<td style="text-align: center;">3,465</td>
<td style="text-align: center;">10,962</td>
</tr>
<tr>
<td style="text-align: center;">Car</td>
<td style="text-align: center;">88,664</td>
<td style="text-align: center;">55,289</td>
</tr>
<tr>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">1,751</td>
<td style="text-align: center;">476</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">$\mathbf{1 8 1 , 4 8 3}$</td>
<td style="text-align: center;">$\mathbf{2 0 3 , 2 9 2}$</td>
</tr>
</tbody>
</table>
<p>We split the location visitation sequence of each user into train, validation and test sets with the ratio of 6:2:2 based on time, such that location records that occurred in the first $60 \%$ of days are considered as train and the last $20 \%$ of days as test. The hyperparameters are tuned on the validation set, and the test set is only used for reporting the prediction performance. We train a single population-level model using records from the training set of all users. We do not pre-filter the locations visited less often, which is a pre-processing step usually found in the location prediction models using GPS traces (e.g., [34]). Although preserving the original location transition patterns, this setting makes the problem a more challenging task, as a large proportion of mobility consists of exploring new locations that are difficult to predict [5].</p>
<h3>5.2 Setup</h3>
<p>The hyper-parameters for the network are reported in Table 2. We chose the best performing set of hyper-parameters using grid search: the number of layers $L$ from ${2,4,6}$, the number of heads $H$ from ${4,8}$, the size of the embedding $d_{\text {base }}$ from ${32,64,128,256}$ and the dropout of the FC layer from ${0.1,0.2,0.5}$. The $\vartheta$ in the final loss (Eq. (15)) is set to 1 . Adam optimizer with an initial learning rate of $1 e^{-3}$ and L2 penalty of $1 e^{-6}$ is used to optimize the model's parameters. We implement learning rate warm-up and decay as reported by Vaswani et al. [39]. To alleviate the model over-fit on the training dataset, we use an early stopping strategy to pause the learning if the validation loss stops decreasing for 3 epochs. We then drop the learning rate by 0.1 and continue the training from the model with the lowest validation loss. This process is repeated 3 times.</p>
<p>Table 2: Experiment settings and network parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Value</th>
<th style="text-align: center;">Network parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Learning rate (lr)</td>
<td style="text-align: center;">$1 e^{-3}$</td>
<td style="text-align: center;">Emb. $d_{\text {base }}$</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">L2 penalty</td>
<td style="text-align: center;">$1 e^{-6}$</td>
<td style="text-align: center;">User emb. $d_{\text {user }}$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">lr warm up</td>
<td style="text-align: center;">2 epochs</td>
<td style="text-align: center;"># layers $N$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">lr decay</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;"># heads $H$</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Early stop patiance</td>
<td style="text-align: center;">3 epochs</td>
<td style="text-align: center;">Feedforward</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">Early stop lr drop</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">FC dropout</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>We implement several popular methods as baselines to compare with our proposed model. The approaches we choose are as follows: (1) Markov models. The de facto standard method for this task [11, 17, 18]. We implement the $1^{\text {st }}$ order Mobility Markov Chain (1MMC) [11], as our pre-experiment shows that higher-order methods can not improve the prediction performance. (2) LSTM network, a widely adopted recurrent neural network (RNN) model for sequence data, has been successfully applied to predict the next location [34, 42]. We implement the model proposed by Solomon et al. [34]. (3) Deepmove [9]. It incorporates a historical attention module to extract historical movement patterns with RNN-based networks. We consider the most recent two days as the current sequence and the remaining five days as the historical sequence for the input to the model. (4) MobTcast [44]. It considers temporal, semantic, social, and geographical contexts with a transformer encoder-based structure to forecast the next POI. Also, the model includes two losses to encourage predicted and ground truth locations to be close in space. We incorporate all components except for the social context since the overlap between location visits for our users is low due to the high-resolution GPS tracking. (5) LSTM with selfattention (LSTM attn). Inspired by Li et al. [20], we implement an LSTM-based network with self-attentions between the current and previous hidden states.</p>
<p>We use the following metrics to evaluate the next location prediction performance of various methods: (1) Accuracy. It indicates the number of correctly predicted locations by the network. We rank the predicted probability from $P\left(I_{n+1}\right)$ in descending order and count the number of times that the ground truth location appears in the top-k predicted locations (Acc@k). Acc@1, Acc@5,</p>
<p>Table 3: Performance evaluation results for next location prediction. The mean and the standard deviation (in parentheses) are reported. Numbers marked in bold and underline represent the best and the second-best performing method respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc@1</th>
<th style="text-align: center;">Acc@5</th>
<th style="text-align: center;">Acc@10</th>
<th style="text-align: center;">MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">1-MMC [11]</td>
<td style="text-align: center;">25.56</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">59.10</td>
<td style="text-align: center;">62.95</td>
<td style="text-align: center;">46.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM [34]</td>
<td style="text-align: center;">$30.26(0.42)$</td>
<td style="text-align: center;">$36.25(0.18)$</td>
<td style="text-align: center;">$60.86(0.15)$</td>
<td style="text-align: center;">$65.90(0.27)$</td>
<td style="text-align: center;">$47.54(0.13)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Deepmove [9]</td>
<td style="text-align: center;">$30.82(0.19)$</td>
<td style="text-align: center;">$36.27(0.13)$</td>
<td style="text-align: center;">$60.96(0.13)$</td>
<td style="text-align: center;">$66.06(0.17)$</td>
<td style="text-align: center;">$47.57(0.10)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MobTcast [44]</td>
<td style="text-align: center;">$\underline{32.29}(0.28)$</td>
<td style="text-align: center;">$37.20(0.29)$</td>
<td style="text-align: center;">$60.34(0.37)$</td>
<td style="text-align: center;">$65.51(0.35)$</td>
<td style="text-align: center;">$47.76(0.30)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM attn</td>
<td style="text-align: center;">$32.16(0.48)$</td>
<td style="text-align: center;">$\underline{37.73}(0.08)$</td>
<td style="text-align: center;">$\underline{61.65}(0.21)$</td>
<td style="text-align: center;">$\underline{66.57}(0.37)$</td>
<td style="text-align: center;">$\underline{48.70}(0.12)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">$\mathbf{3 4 . 8 9}(0.16)$</td>
<td style="text-align: center;">$\mathbf{3 9 . 9 3}(0.14)$</td>
<td style="text-align: center;">$\mathbf{6 1 . 8 9}(0.17)$</td>
<td style="text-align: center;">$\mathbf{6 6 . 9 2}(0.10)$</td>
<td style="text-align: center;">$\mathbf{4 9 . 9 4}(0.11)$</td>
</tr>
<tr>
<td style="text-align: center;">Yumuv</td>
<td style="text-align: center;">1-MMC [11]</td>
<td style="text-align: center;">34.81</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">68.61</td>
<td style="text-align: center;">71.58</td>
<td style="text-align: center;">55.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM [34]</td>
<td style="text-align: center;">$41.46(0.34)$</td>
<td style="text-align: center;">$45.32(0.26)$</td>
<td style="text-align: center;">$68.72(0.23)$</td>
<td style="text-align: center;">$72.25(0.29)$</td>
<td style="text-align: center;">$56.10(0.22)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Deepmove [9]</td>
<td style="text-align: center;">$42.29(0.21)$</td>
<td style="text-align: center;">$46.11(0.26)$</td>
<td style="text-align: center;">$69.47(0.08)$</td>
<td style="text-align: center;">$73.01(0.06)$</td>
<td style="text-align: center;">$56.91(0.19)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MobTcast [44]</td>
<td style="text-align: center;">$42.65(0.28)$</td>
<td style="text-align: center;">$46.13(0.25)$</td>
<td style="text-align: center;">$69.18(0.17)$</td>
<td style="text-align: center;">$72.81(0.07)$</td>
<td style="text-align: center;">$56.69(0.16)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM attn</td>
<td style="text-align: center;">$\underline{43.02}(0.28)$</td>
<td style="text-align: center;">$\underline{46.66}(0.23)$</td>
<td style="text-align: center;">$\underline{69.87}(0.11)$</td>
<td style="text-align: center;">$\underline{73.48}(0.16)$</td>
<td style="text-align: center;">$\underline{57.38}(0.15)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">$\mathbf{4 5 . 4 3}(0.32)$</td>
<td style="text-align: center;">$\mathbf{4 8 . 6 7}(0.20)$</td>
<td style="text-align: center;">$\mathbf{7 0 . 1 7}(0.09)$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5 0}(0.08)$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4 9}(0.13)$</td>
</tr>
</tbody>
</table>
<p>and Acc@10 are reported. (2) F1 score. Since certain locations are visited more often than others [35], we employ the F1 score weighted by location visitation frequency. We argue that the F1 score is a better metric than Acc@1 as it considers the unbalanced location visits. (3) Mean Reciprocal Rank (MRR). The metric is commonly applied to measure performance in information retrieval and re-identification tasks. MRR is the harmonic mean of the ground truth label's rank in the prediction.</p>
<h3>5.3 Results</h3>
<p>5.3.1 Overall performance. Table 3 shows the performance of next place prediction methods on the two considered datasets. We train each DL model 5 times with different random seeds and report the mean and the standard deviation of the respective performance indicators.</p>
<p>The 1-MMC method provides a strong baseline for the task. However, its performance is worse than any DL model, indicating that the Markov property cannot fully capture mobility patterns. On the contrary, the na誰ve LSTM model outperforms the 1-MMC method by a relatively large margin, demonstrating the effectiveness of DL-based models in predicting mobility. The worse performance compared to other DL methods can be attributed to the insufficient consideration of long-term dependencies in location visit sequences, an inherent shortcoming of the LSTM model [39]. This deficiency is partly tackled by introducing the historical attention module in the Deepmove model that explicitly focuses on extracting historical information, which consistently outperforms the na誰ve LSTM on all indicators for both datasets. MobTcast obtains comparable performance in Yumuv and slightly better F1 and Acc@1 scores in GC compared to Deepmove, suggesting the additional consideration of context and the introduction of two losses have limited effect on predicting an individual's next location.</p>
<p>However, we observe a significant performance increase by adding a self-attention mechanism to the na誰ve LSTM model. The self-attention's ability to focus on specific steps during prediction indicates that successfully mining historical patterns can greatly
benefit location prediction. By incorporating the multi-head selfattention and introducing the next mode prediction loss, the proposed model outperforms all other models by a relatively large margin. The relative increase of the F1 score is $8.05 \%$ and $5.60 \%$ for two datasets, respectively, which is calculated using the formula $\left(F 1_{1}-F 1_{2}\right) / F 1_{2}$, where $F 1_{1}$ is the F 1 score of our model and $F 1_{2}$ represents the F1 score of the second-best performing model. The consistent pattern for both considered datasets demonstrates the effectiveness of our network architecture and loss design. Additionally, we report that the performance difference between LSTM with self-attention and our transformer decoder-based model is relatively small in Acc@5 and Acc@10 compared to the other indicators. We assume that the single head self-attention can already extract multiple likely visited next locations but lacks the ability to distinguish the importance within this set successfully.
5.3.2 Ablation study. We perform an extensive ablation study to understand the importance of each component in our proposed model. In Table 4, we show the performance of different variants of our model, considering whether or not to include temporal features $h_{k}$ and $d_{k}$, the travel mode feature $e_{k}$, and the next travel mode prediction loss $\mathcal{L}_{2}$. The performance results are consistent across both datasets. We first report that the model performs significantly better with temporal features. This increase is intuitive as the time encodes different levels of periodicity in location visits compared to merely having the sequence information. While considering the travel mode feature and the mode prediction loss are both beneficial for the model's performance, the latter is more important for the task. Moreover, we observe a further performance gain when simultaneously including the travel mode feature and the mode prediction loss, as indicated by the highest performance achieved by the complete model for both datasets. These experiments show that temporal features are indispensable for accurate next location prediction. They also confirm that both historical travel mode patterns and the ability to predict the next travel mode are essential for the task, justifying the effectiveness of our model design.</p>
<p>Since our proposed model outputs both the probability of the next location and the next travel mode, we can utilize the same</p>
<p>Table 4: Performance of the ablation study for next location prediction. We consider the model with $(\checkmark)$ and without (-) adding temporal features (T), the travel mode feature (F), and the mode prediction loss $\left(\mathcal{L}_{2}\right)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Module</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc@1</th>
<th style="text-align: center;">Acc@5</th>
<th style="text-align: center;">Acc@10</th>
<th style="text-align: center;">MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">$\mathcal{L}_{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.24 (0.18)</td>
<td style="text-align: center;">36.22 (0.22)</td>
<td style="text-align: center;">60.57 (0.25)</td>
<td style="text-align: center;">65.75 (0.23)</td>
<td style="text-align: center;">47.42 (0.24)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">31.20 (0.18)</td>
<td style="text-align: center;">36.64 (0.19)</td>
<td style="text-align: center;">60.99 (0.10)</td>
<td style="text-align: center;">66.09 (0.07)</td>
<td style="text-align: center;">47.83 (0.12)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.61 (0.23)</td>
<td style="text-align: center;">36.80 (0.18)</td>
<td style="text-align: center;">60.88 (0.07)</td>
<td style="text-align: center;">65.98 (0.10)</td>
<td style="text-align: center;">47.86 (0.09)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.55 (0.19)</td>
<td style="text-align: center;">39.10 (0.20)</td>
<td style="text-align: center;">61.10 (0.24)</td>
<td style="text-align: center;">66.18 (0.19)</td>
<td style="text-align: center;">49.07 (0.17)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">34.80 (0.29)</td>
<td style="text-align: center;">39.81 (0.12)</td>
<td style="text-align: center;">61.75 (0.11)</td>
<td style="text-align: center;">66.67 (0.10)</td>
<td style="text-align: center;">49.76 (0.07)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.60 (0.09)</td>
<td style="text-align: center;">39.39 (0.10)</td>
<td style="text-align: center;">61.23 (0.07)</td>
<td style="text-align: center;">66.28 (0.16)</td>
<td style="text-align: center;">49.34 (0.08)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">34.89 (0.16)</td>
<td style="text-align: center;">39.93 (0.14)</td>
<td style="text-align: center;">61.89 (0.17)</td>
<td style="text-align: center;">66.92 (0.10)</td>
<td style="text-align: center;">49.94 (0.11)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.11 (0.17)</td>
<td style="text-align: center;">45.90 (0.12)</td>
<td style="text-align: center;">69.47 (0.12)</td>
<td style="text-align: center;">73.12 (0.12)</td>
<td style="text-align: center;">56.73 (0.09)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">42.83 (0.24)</td>
<td style="text-align: center;">46.45 (0.19)</td>
<td style="text-align: center;">69.47 (0.17)</td>
<td style="text-align: center;">72.90 (0.11)</td>
<td style="text-align: center;">57.01 (0.13)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.68 (0.18)</td>
<td style="text-align: center;">46.30 (0.18)</td>
<td style="text-align: center;">69.68 (0.14)</td>
<td style="text-align: center;">73.15 (0.18)</td>
<td style="text-align: center;">57.01 (0.16)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.53 (0.10)</td>
<td style="text-align: center;">47.78 (0.05)</td>
<td style="text-align: center;">69.68 (0.27)</td>
<td style="text-align: center;">73.26 (0.19)</td>
<td style="text-align: center;">57.82 (0.10)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">45.04 (0.22)</td>
<td style="text-align: center;">48.37 (0.19)</td>
<td style="text-align: center;">70.08 (0.12)</td>
<td style="text-align: center;">73.41 (0.14)</td>
<td style="text-align: center;">58.28 (0.12)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.65 (0.11)</td>
<td style="text-align: center;">47.97 (0.11)</td>
<td style="text-align: center;">69.86 (0.12)</td>
<td style="text-align: center;">73.38 (0.12)</td>
<td style="text-align: center;">58.01 (0.10)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">45.43 (0.32)</td>
<td style="text-align: center;">48.67 (0.20)</td>
<td style="text-align: center;">70.17 (0.09)</td>
<td style="text-align: center;">73.50 (0.08)</td>
<td style="text-align: center;">58.49 (0.13)</td>
</tr>
</tbody>
</table>
<p>model structure to evaluate the performance of predicting the next travel mode. From this viewpoint, time and location can be regarded as additional features, and predicting the next location can be seen as the auxiliary task. We evaluate the usefulness of these components in the next mode prediction task using an ablation study. The performance result are shown in Table 5. We again observe a large performance increase after including temporal features. Furthermore, historical location visits and the additional location prediction loss benefit the prediction. The top performance is achieved by the complete model that utilizes all three components in the network. These results indicate that the next travel mode prediction performance can be improved by considering location visit information. Combined with the location prediction ablation study, we conclude that the next location and the next travel mode prediction tasks are inherently dependent and should be tackled together to achieve optimal performance.</p>
<p>Table 5: Performance for next mode prediction. We consider the model with and without adding temporal features (T), the location feature (F), and the location prediction loss $\left(\mathcal{L}_{1}\right)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Module</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc@1</th>
<th style="text-align: center;">MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">$\mathcal{L}_{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">54.29 (0.16)</td>
<td style="text-align: center;">58.17 (0.09)</td>
<td style="text-align: center;">75.95 (0.05)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.30 (0.18)</td>
<td style="text-align: center;">60.14 (0.09)</td>
<td style="text-align: center;">77.28 (0.04)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.64 (0.33)</td>
<td style="text-align: center;">61.03 (0.29)</td>
<td style="text-align: center;">77.67 (0.17)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">58.75 (0.08)</td>
<td style="text-align: center;">61.16 (0.25)</td>
<td style="text-align: center;">77.72 (0.16)</td>
</tr>
<tr>
<td style="text-align: center;">Yumuv</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.84 (0.12)</td>
<td style="text-align: center;">52.53 (0.13)</td>
<td style="text-align: center;">71.90 (0.07)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">52.59 (0.11)</td>
<td style="text-align: center;">53.96 (0.13)</td>
<td style="text-align: center;">72.74 (0.09)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.70 (0.21)</td>
<td style="text-align: center;">54.64 (0.10)</td>
<td style="text-align: center;">73.05 (0.06)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">54.03 (0.19)</td>
<td style="text-align: center;">54.84 (0.11)</td>
<td style="text-align: center;">73.05 (0.08)</td>
</tr>
</tbody>
</table>
<p>5.3.3 Importance of travel mode information. We further investigate the influence of travel mode on the next location prediction task. To understand to what extent the next location prediction result depends on the prediction of travel mode, we input the ground truth next travel mode into the model. This is achieved by feeding the travel mode at the next time step into the mode embedding layer to generate the embedding vector $e m b_{n+1}^{e}$. Then, the vector is added to the user embedding and further processed with the fully connected layer, i.e., Eq. (10) becomes $f_{n}=$ $F C\left(\left(\right.\right.$ out $\left.<em n_1="n+1">{n}+\operatorname{Linear}\left(e m b</em>$ ). We can regard this new problem as predicting the next location conditioned on the next travel mode, where the scenario is to predict the location when knowing which travel mode the user will take. Figure 5 compares the performance of the original model with the model that includes the next travel mode. When knowing the next travel mode, we observe a notable increase in all indicators for the two datasets. In line with the conclusion from the ablation study, this result illustrates that the choice of the next location depends strongly on the travel mode. The performances shown in Figure 5 can also be interpreted as the upper bound of performance gain that can be achieved by including the prediction of travel mode in our model. This result suggests that working on a better mode prediction still holds great potential to increase the prediction accuracy of the next location.}^{e}\right)\right) \oplus e m b^{u^{(1)}</p>
<p>This mode-location dependency is evident by checking differences in predicting various categories of locations. In particular, we group the location prediction result based on the ground truth next travel mode and plot the F1 score in Figure 6. Each "box" in the figure is generated by the F1 score of every user and represents the model's performance in a single travel mode. We also labelled each box with the median F1 score across users. The differences in the F1 scores suggest that location prediction depends on the travel mode: locations with specific travel modes are more challenging to predict than others. For example, the locations reached by tram and bus obtained relatively lower performance than other modes in GC (Figure 6a). For Yumuv, the mode tram achieves a relatively</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of the original model (blue) and the model that considers the ground truth next travel mode (orange).
low median F1-Score, and the mode bus has a high variance across users (Figure 6b). This observation could be attributed to the more variate choice of locations when users travel with these modes. The failure to predict the mode other (including airplane, ski and coach) is related to the exploration nature of human mobility: users often travel with these modes to new locations that are difficult for the model to predict. Moreover, the distinct pattern for the GC and Yumuv datasets can be explained by the mobility patterns of the two groups of participants. GC participants are more active in their mobility and often travel a longer distance by car and train [14], whereas Yumuv users mostly commute within the city [30] and have a more balanced use of travel modes (Table 1).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: F1 score boxplot of individual categorized by the next travel mode.</p>
<h2>6 DISCUSSION AND CONCLUSION</h2>
<p>Travel behaviour researchers have long realized that individuals' choice of locations is highly influenced by other travel behaviour dimensions, such as the day of the week and the availability of travel modes. While this dependency is evident in an increasing number of empirical studies, limited attention has been placed on integrating this information into the next location prediction task. In this study, we present a transformer-based model to predict the next location visit of an individual. In particular, the model is designed to accurately predict the next location and the next travel mode at the same time. We test our model on two large-scale real-world GPS tracking datasets and compare its performance with most recent
methods for the task. Our experiments show that the proposed model learns the dynamics of human mobility from historical time, travel mode and location sequences and obtained state-of-the-art next location prediction performance. Our extensive ablation study demonstrates that temporal and travel mode features and the ability to predict the next travel mode are essential components in an accurate location prediction model. Moreover, our analysis indicates strong mode-location dependency, which suggests the next location and the next travel mode prediction tasks should be tackled together to achieve optimal performance.</p>
<p>We conclude that considering historical travel mode patterns increases the accuracy of the next location prediction. This result connects travel mode detection models with next location prediction models. Mode detection models aim to classify a movement's travel mode given its spatio-temporal characteristics [31], which can enrich general tracking datasets that lack accurate user-provided travel mode labels. Therefore, mobility prediction can be tackled using a two-step approach: (1) identify the travel mode with detection models based on spatio-temporal movement characteristics; and (2) predict the next location with prediction models based on temporal, travel mode and location information. This approach indicates that an optimal system for mobility prediction should develop accurate models for both tasks.</p>
<p>Next location prediction is an essential backbone of many sustainable transport solutions; however, it is a challenging problem that is not yet fully tackled. Although we improve the state-of-the-art considerably compared to the other methods, the field still needs more research to achieve a more accurate prediction performance. We propose several future directions based on the results of this work. This study demonstrates the importance of travel mode for predicting an individual's immediate next location visit. Since the mode-location dependency is more evident on a longer time scale $[14,38]$, future research should consider integrating travel mode information into mobility simulation models for generating more realistic mobility sequences. Furthermore, besides the interdependent travel behaviour dimensions, studies have shown that external contexts, such as the built environment, play an indispensable role in shaping individuals' mobility [8]. This knowledge can be effectively extracted and combined with individual mobility in DL models based on land-use maps and POI data. Whether or not the additional information will improve the performance of next location prediction or mobility generation tasks is a direction worth exploring.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work is supported by the Hasler Foundation (grant number 1-008062). The yumuv dataset was collected as part of a joint study with Swiss Federal Railways that was financed through the ETH Mobility Initiative (grant number MI-01-19).</p>
<h2>REFERENCES</h2>
<p>[1] Daniel Ashbrook and Thad Starner. 2002. Learning Significant Locations and Predicting User Movement with GPS. In International Symposium on Wearable Computers (ISWC) 2002. 101-108.
[2] Kay W Axhausen. 2007. Definition of movement and activity for transport modelling: Contribution to the handbooks in transport: transport modelling. In Handbook of transport modelling: transport modelling. Vol. 1. Elsevier, 329-343.</p>
<p>[3] Meng Chen, Yang Liu, and Xiaohui Yu. 2014. NLPMM: A Next Location Predictor with Markov Modeling. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2014. 186-197.
[4] Elisabetta Cherchi, Cinzia Cirillo, and Juan de Dios Ort炭zar. 2017. Modelling correlation patterns in mode choice models estimated on multiday travel data. Transportation Research Part A: Policy and Practice 96 (2017), 146-153.
[5] Andrea Cuttone, Sune Lehmann, and Marta C. Gonz叩lez. 2018. Understanding predictability and exploration in human mobility. EPJ Data Science 7 (2018), 2.
[6] Dimas B.E. Dharmowijoyo, Yusak O. Susilo, and Anders Karlstr旦m. 2017. Analysing the complexity of day-to-day individual activity-travel patterns using a multidimensional sequence alignment model: A case study in the Bandung Metropolitan Area, Indonesia. Journal of Transport Geography 64 (2017), 1-12.
[7] Dimas B. E. Dharmowijoyo, Yusak O. Susilo, and Anders Karlstr旦m. 2016. Day-today variability in travellers' activity-travel patterns in the Jakarta metropolitan area. Transportation 43, 4 (2016), 601-621.
[8] Temitope Farinloye, Emmanuel Mogaji, Stella Aririguzoh, and Tai Anh Kieu. 2019. Qualitatively exploring the effect of change in the residential environment on travel behaviour. Travel Behaviour and Society 17 (2019), 26-35.
[9] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. 2018. DeepMove: Predicting Human Mobility with Attentional Recurrent Networks. In International World Wide Web Conference (WWW) 2018. 1459-1468.
[10] Jie Feng, Zeyu Yang, Fengli Xu, Haisu Yu, Mudan Wang, and Yong Li. 2020. Learning to Simulate Human Mobility. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2020. 3426-3433.
[11] Sebastien Gamba, Marc-Olivier Killijian, and Miguel N炭単ez del Prado Cortez. 2012. Next Place Prediction Using Mobility Markov Chains. In Proceedings of the First Workshop on Measurement, Privacy, and Mobility (MPM '12). 5:1-3:6.
[12] David Griggs, Mark Stafford-Smith, Owen Gaffney, Johan Rockstr旦m, Marcus C Ohman, Priya Shyamsundar, Will Steffen, Gisbert Glaser, Norichika Kanie, and Ian Noble. 2013. Sustainable development goals for people and planet. Nature 495, 7441 (2013), 305-307.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Conference on Computer Vision and Pattern Recognition (CVPR) 2016. 770-778.
[14] Ye Hong, Henry Martin, Yanan Xin, Dominik Bucher, Daniel J. Reck, Kay W. Axhausen, and Martin Raubal. 2022. Conserved quantities in human mobility. From locations to trips. (2022). https://doi.org/10.3929/ethz-b-000551831
[15] Ye Hong, Yanan Xin, Henry Martin, Dominik Bucher, and Martin Raubal. 2021. A Clustering-Based Framework for Individual Travel Behaviour Change Detection. In International Conference on Geographic Information Science (GIScience) 2021 Part II. 4:1-4:15.
[16] Haosheng Huang, Dominik Bucher, Julian Kissling, Robert Weibel, and Martin Raubal. 2019. Multimodal Route Planning With Public Transport and Carpooling. IEEE Transactions on Intelligent Transportation Systems 20 (2019), 3513-3525.
[17] Qunying Huang. 2017. Mining online footprints to predict user's next location. International Journal of Geographical Information Science 31, 3 (2017), 523-541.
[18] Vabhise Kulkarni and Beno樽t Garbinato. 2019. 20 Years of Mobility Modeling \&amp; Prediction: Trends, Shortcomings \&amp; Perspectives. In ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (SIGSPATIAL) 2021. 492-495.
[19] Gabriel Leite Mariante, Tai-Yu Ma, and V辿ronique Van Acker. 2018. Modeling discretionary activity location choice using detour factors and sampling of alternatives for mixed logit models. Journal of Transport Geography 72 (2018), $151-165$.
[20] Fu Li, Zhipeng Gui, Zhaoyu Zhang, Dehua Peng, Siyu Tian, Kunxiaojia Yuan, Yunzeng Sun, Huayi Wu, Jianya Gong, and Yichen Lei. 2020. A hierarchical temporal attention-based LSTM encoder-decoder model for individual mobility prediction. Neurocomputing 403 (2020), 153-166.
[21] Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. Predicting the Next Location: A Recurrent Model with Spatial and Temporal Contexts. In AAAI Conference on Artificial Intelligence (AAAI) 2016. 194-200.
[22] Massimiliano Luca, Gianni Barlacchi, Bruno Lepri, and Luca Pappalardo. 2021. A Survey on Deep Learning for Human Mobility. Comput. Surveys 55, 1 (2021), $7: 1-7: 44$.
[23] Zhenliang Ma and Pengfei Zhang. 2022. Individual mobility prediction review: Data, problem, method and application. Multimodal Transportation 1, 1 (2022), 100002.
[24] Henry Martin, Henrik Becker, Dominik Bucher, David Jonietz, Martin Raubal, and Kay W. Axhausen. 2019. Begleitstudie SBB Green Class - Abschlussbericht. Arbeitsberichte Verkehrs- und Raumplanung 1439 (2019). https://doi.org/10.3929/ ethz-b-000353337
[25] Henry Martin, Ye Hong, Nina Wiedemann, Dominik Bucher, and Martin Raubal. 2022. Trackintel: An open-source Python library for human mobility analysis. arXiv:2206.03593
[26] Henry Martin, Daniel J. Reck, and Martin Raubal. 2021. Using Information and Communication Technologies to facilitate mobility behaviour change and enable Mobility as a Service. GI_Forum Journal for Geographic Information Science 9, 1 (2021), 187-193.
[27] T. Neutens, F. Witlos, N. Van De Weghe, and P. H. De Maeyer. 2007. Space-time opportunities for multiple agents: a constraint-based approach. International Journal of Geographical Information Science 21 (2007), 1061-1076.
[28] Poria Pirozmand, Guowei Wu, Behrouz Jedari, and Feng Xia. 2014. Human mobility in opportunistic networks: Characteristics, models and prediction methods. Journal of Network and Computer Applications 42 (2014), 45-58.
[29] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.
[30] Daniel J. Reck, Henry Martin, and Kay W. Axhausen. 2022. Mode choice, substitution patterns and environmental impacts of shared and personal micro-mobility. Transportation Research Part D: Transport and Environment 102 (2022), 103134.
[31] Avipsa Roy, Daniel Fuller, Trisalyn Nelson, and Peter Kedron. 2022. Assessing the role of geographic context in transportation mode detection from GPS data. Journal of Transport Geography 100 (2022), 103330.
[32] Stefan Sch旦nfelder and K. W. Axhausen. 2016. Urban rhythms and travel behaviour: spatial and temporal phenomena of daily travel. Routledge, Farnham, England.
[33] Darren M. Scott and Sylvia Y. He. 2012. Modeling constrained destination choice for shopping: a GIS-based, time-geographic approach. Journal of Transport Geography 23 (2012), 60-71.
[34] Adir Solomon, Anit Livne, Gilad Katz, Bracha Shapira, and Lior Rokach. 2021. Analyzing movement predictability using human attributes and behavioral patterns. Computers, Environment and Urban Systems 87 (2021), 101596.
[35] Chaoming Song, Tal Koren, Pu Wang, and Albert-L叩szl坦 Barab叩si. 2010. Modelling the scaling properties of human mobility. Nature Physics 6, 10 (2010), 818-823.
[36] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-L叩szl坦 Barab叩si. 2010. Limits of predictability in human mobility. Science 327, 5968 (2010), 1018-1021.
[37] Lijun Sun, Kay W. Axhausen, Der-Horng Lee, and Xianfeng Huang. 2013. Understanding metropolitan patterns of daily encounters. Proceedings of the National Academy of Sciences 110, 34 (2013), 13774-13779.
[38] Yusak O. Susilo and Kay W. Axhausen. 2014. Repetitions in individual daily activity-travel-location patterns: a study using the Herfindahl-Hirschman Index. Transportation 41, 5 (2014), 995-1011.
[39] Ashish Vaewani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Conference on Neural Information Processing Systems (NeurIPS) 2017. 5998-6008.
[40] Dongjie Wang, Kunpeng Liu, Hui Xiong, and Yanjie Fu. 2022. Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams. arXiv:2201.10983
[41] Dongjie Wang, Pengyang Wang, Kunpeng Liu, Yuanchun Zhou, Charles E. Hughes, and Yanjie Fu. 2021. Reinforced Imitative Graph Representation Learning for Mobile User Profiling: An Adversarial Training Perspective. In AAAI Conference on Artificial Intelligence (AAAI) 2021. 4410-4417.
[42] Yang Xu, Dan Zou, Sangwon Park, Qiuping Li, Suhong Zhou, and Xinyu Li. 2022. Understanding the movement predictability of international travelers using a nationwide mobile phone dataset collected in South Korea. Computers, Environment and Urban Systems 92 (2022), 101753.
[43] Yanyan Xu, Serdar olak, Enne C. Kara, Scott J. Moura, and Marta C. Gonz叩lez. 2018. Planning for electric vehicle needs by coupling charging profiles with urban mobility. Nature Energy 3, 6 (2018), 484-493.
[44] Hao Xue, Flora D. Salim, Yongli Ren, and Nuria Oliver. 2021. MobTCast: Leveraging Auxiliary Trajectory Forecasting for Human Mobility Prediction. In Conference on Neural Information Processing Systems (NeurIPS) 2021. 30380-30391.
[45] Seo Youn Yoon, Kathleen Deutsch, Yali Chen, and Konstadinos G. Goulias. 2012. Feasibility of using time-space prism to represent available opportunities and choice sets for destination choice models in the context of dynamic urban environments. Transportation 39, 4 (2012), 807-823.
[46] Hseyi Zhou, Shangbang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. In AAAI Conference on Artificial Intelligence (AAAI) 2021. 11106-11115.</p>            </div>
        </div>

    </div>
</body>
</html>
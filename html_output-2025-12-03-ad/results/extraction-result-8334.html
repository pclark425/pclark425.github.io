<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270619922</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.13828v3.pdf" target="_blank">Neuro-symbolic Training for Reasoning over Spatial Language</a></p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning based on natural language expressions is essential for everyday human tasks. This reasoning ability is also crucial for machines to interact with their environment in a human-like manner. However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions. This is attributed to not achieving the right level of abstraction required for generalizability. To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering. Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains. We evaluate our approach on existing spatial question-answering benchmarks. Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-T+Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT fine-tuned with SpaRTUN then tuned with Q-Chain neuro-symbolic constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based encoder model first tuned on the synthetic SpaRTUN corpus and then fine-tuned with example-specific Q-Chains and soft logical constraints (via DomiKnowS) to improve multi-hop spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (bert-based-uncased) with Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-based transformer (BERT) fine-tuned for QA by concatenating question and context, using [CLS] for classification; augmented by SpaRTUN pre-fine-tuning and neuro-symbolic constraint loss implemented with DomiKnowS and t-norm Product soft logic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN (training), SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial question-answering benchmarks requiring multi-hop (up to 10) reasoning over described scenes (synthetic and realistic); relations include direction and topological relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Fine-tuned on SpaRTUN then target dataset; input formed by Q [SEP] C for encoder; YN uses binary classification, FR uses multiple binary classifiers (one per relation). Training objective combines cross-entropy with soft logical constraint violations from Q-Chains (Î»_k=1).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Q-Chain: automatic construction of reasoning chains via forward chaining on SpaRTUN logical facts; derive consistency constraints from resolution trees; convert constraints to differentiable soft logic (t-norm Product) via DomiKnowS; Primal-Dual optimization fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame accuracy by reasoning depth k (Table 2): k=1: 98.70%, k=2: 96.45%, k=3: 93.03%, k=4: 74.58%, k=5: 64.95%, k=6: 59.04%, k=7: 54.38%, k=8: 49.23%, k=9: 45.36%, k=10: 44.05%. SpartQA-Human/ReSQ: SpartQA-Human Ver.1 59.44% (vs BERT 54.54%); ReSQ overall 61.31% (compared to BERT 60.98%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Consistent and larger relative improvements vs. BERT and other baselines on datasets and on higher-hop StepGame steps (k>=6) indicate use of multi-hop spatial reasoning abstractions; qualitative examples (Figure 4) show correct multi-step derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms base BERT and BERT-T on StepGame across most hops; smaller gap vs. LLMs on realistic ReSQ where LLMs (GPT-3.5/GPT-4) have stronger commonsense. Compared to relation-extraction + external solver pipelines (GPT3(d2)+ASP, PistaQ*), BERT-T+Q-Chain achieves competitive multi-hop accuracy without needing formal representation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires chain-of-reasoning annotations (SpaRTUN) during training; when reasoning depth is shallow (k=1) or questions require commonsense/unclassified knowledge, BERT-T+Q-Chain can be less effective (negative impact observed on BERT for some k=1 / unclassified splits). Training needs GPU resources (~8 A6000 GPUs, ~100 GPU hours for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-T+Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (base) fine-tuned with SpaRTUN then constrained with Q-Chain neuro-symbolic training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative Flan-T5-base model fine-tuned first on SpaRTUN then further trained with Q-Chain derived soft logical constraints (using LoRA adapters to reduce compute) to improve multi-hop spatial FR and YN QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-base (with LoRA adapter) + Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-finetuned encoder-decoder transformer (Flan-T5-base) used as a generative QA model; for YN model compares probability of Yes/No tokens; LoRA adapters used to reduce trainable parameters; Q-Chain soft-logic constraints integrated via DomiKnowS.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN (training), SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA benchmarks requiring multi-hop spatial reasoning (YN and multi-label FR relations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prompt: 'Answer based on the context: C Q.' YN: first-token Yes/No selection; FR: generative multi-label prediction (handled via prompt engineering and training). Training objective combined cross-entropy and soft constraint losses from Q-Chains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Same Q-Chain pipeline as encoder models: forward-chaining to build reasoning chains, derive constraints, t-norm Product soft logic via DomiKnowS, Primal-Dual optimizer; LoRA for efficient fine-tuning of Flan-T5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human Ver.1/Ver.2: Flan-T5-T+Q-Chain reported 55.94% / 61.61% respectively (improvements over Flan-T5-T). ReSQ unclassified and k splits: Flan-T5-T+Q-Chain overall 64.43% (compared to Flan-T5-T 62.30%). StepGame: the paper reports consistent positive differences across reasoning hops vs LLMs; exact per-k numbers for Flan-T5 variants not listed in StepGame table but reported as improved.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Notable improvements on synthetic multi-hop datasets (SpartQA-Human, StepGame) and ReSQ k=2 split indicate enhanced multi-hop spatial reasoning; gains larger than for BERT variants on some splits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Improves more substantially from Q-Chain training than BERT variants (paper notes larger relative gains for Flan-T5), but closed-source LLMs (GPT-3.5/GPT-4) still outperform on commonsense-heavy (ReSQ unclassified) questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generative FR multi-label setting complicates direct use of constraints at inference; requires SpaRTUN chain annotations for training; LoRA reduces parameters but compute demand remains non-trivial; less effective on commonsense/unclassified splits compared to large LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT / BERT-T (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (bert-based-uncased) baseline and BERT-T (SpaRTUN pre-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-based BERT baseline fine-tuned on target datasets; BERT-T refers to the model additionally fine-tuned on synthetic SpaRTUN before task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (bert-based-uncased), BERT-T</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained BERT encoder used with classification heads for YN and multi-label FR tasks, input created by concatenating question and context; BERT-T indicates additional pre-fine-tuning on SpaRTUN synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN (training for BERT-T), SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA datasets varying from realistic (ReSQ) to synthetic multi-hop (StepGame, SpartQA-Human).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Fine-tuning on each target dataset; FR handled as multiple binary classification layers (one per relation); training uses cross-entropy only for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard supervised fine-tuning (cross-entropy) on target dataset; BERT-T uses SpaRTUN synthetic pre-fine-tuning to improve multi-hop capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame (Table 2) BERT: k=1 98.51%, k=2 95.53%, k=3 91.68%, k=4 66.71%, k=5 49.11%, k=6 41.47%, k=7 41.47%, k=8 32.09%, k=9 28.94%, k=10 28.16%. SpartQA-Human Ver.1 54.54% (BERT) vs BERT-T 55.94%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High accuracy at low hop depths (k<=3) shows ability to handle shallow spatial reasoning; performance drops rapidly with more hops, motivating Q-Chain approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>BERT-T generally outperforms BERT, showing benefit of SpaRTUN pre-fine-tuning but is still outperformed by BERT-T+Q-Chain at higher hop depths. LLMs (GPT-3.5/GPT-4) beat BERT variants on realistic ReSQ unclassified (commonsense) splits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades substantially with increasing reasoning hops; lacks logical constraint supervision leading to poorer generalization on deep multi-hop scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8334.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source autoregressive LLM from OpenAI used here as an in-context learning baseline (zero-shot, few-shot, CoT, LR, CoS).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer used in zero-shot and few-shot in-context learning experiments with different prompt styles including Chain-of-Thought (CoT), Logical Representation (LR), and Chain-of-Symbol (CoS).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA benchmarks requiring multi-hop spatial reasoning and commonsense in realistic splits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>In-context learning: zero-shot (direct prompt), few-shot (4 examples concatenated), CoT (few-shot with manually-crafted reasoning explanations), LR (logical predicate-argument rationales), CoS (symbolic chain representations), and Step-by-Step prompting (context compression).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Primarily in-context reasoning using CoT, LR, CoS; can output natural-language rationales; no fine-tuning in experiments. For some external baselines, outputs were used for relation extraction and then passed to external solvers (GPT3(d2)+ASP in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human (Table 2): zero-shot 58.04%, few-shot 62.23%, CoT 65.73% (best reported for GPT-3.5 variants). ReSQ k splits: k=1 58.03%, k=2 74.67% (table mixed across columns). StepGame raw vs step-by-step (Table 3): zero-shot 58.04% raw -> 63.64% step-by-step; few-shot 62.23% -> 64.33%; CoT 65.73% -> 67.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT and step-by-step prompting improve accuracy, indicating the model can leverage chain-of-thought style reasoning to solve spatial multi-step problems to an extent; however accuracy falls at larger hop counts (StepGame rows show steep decline as k increases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed by GPT-4 on many splits; outperforms few-shot Llama-3 on some tasks but underperforms tuned small models (BERT-T+Q-Chain) on deep StepGame hops. External pipeline GPT3(d2)+ASP (from prior work) achieves much higher StepGame accuracies (e.g., 92.6% at k=1..10 range) when combined with symbolic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades rapidly with increasing hop depth; CoS and LR prompting sometimes underperform CoT; struggles in synthetic deep-reasoning benchmarks compared to neuro-symbolically tuned small models and compared to pipelines that use explicit relation extraction + external solvers; sensitive to context length/complexity (benefits from context compression).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8334.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art closed-source LLM used as a zero-shot in-context baseline showing strong performance on spatial QA tasks with minimal prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal-capable autoregressive transformer reported in the paper as a zero-shot baseline for spatial QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA requiring multi-hop reasoning and some commonsense inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompts; also used in comparisons with in-context methods; no fine-tuning performed in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Inherent chain-of-thought-like capabilities exploited via CoT prompting where applicable; step-by-step context compression also applied to improve raw-context performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human (Table 2): zero-shot 77.62% (Ver.1) and 68.75% (Ver.2?) as reported in table; StepGame step-by-step performance shown as 78.32% (Table 3, step-by-step) which is higher than GPT-3.5 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High zero-shot performance suggests substantial general reasoning and commonsense capabilities applied to spatial QA; improvements from step-by-step prompting indicate the model benefits from decomposed context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms GPT-3.5 and open-source Llama-3 on many realistic-domain splits (ReSQ unclassified) and SpartQA-Human; however, neuro-symbolically tuned smaller models (BERT-T+Q-Chain) can outperform or match GPT-4 on some synthetic deep-hop StepGame settings according to the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite high overall accuracy, the model can still struggle on deep multi-hop synthetic benchmarks and benefits from context simplification; closed-source nature and lack of fine-tuning in this study mean it relies on in-context prompt engineering rather than training with logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8334.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Llama-3 model (8B parameters) used as an in-context learning baseline evaluated with zero-shot, few-shot, and CoT prompting variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-3 family model (8B) employed for zero-shot and few-shot in-context evaluation on spatial QA; used CoT, LR, CoS and step-by-step context compression.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ, StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA benchmarks requiring multi-hop reasoning and contextual understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot and few-shot in-context experiments (4 examples for few-shot); CoT/CoS/LR variants and step-by-step context compression used to create prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>In-context chain-of-thought, logical representation prompting, chain-of-symbol prompting, and step-by-step context splitting to simplify context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human (Table 2): zero-shot 61.54% (Ver.1), few-shot 62.94%, CoT 67.83%; StepGame few-shot row (Table 2 second block): e.g., k=1 38.01%, k=2 27.87%, k=3 24.15%, dropping to ~14-19% at higher hops (much lower than tuned models).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT and step-by-step prompting improve accuracy significantly over raw prompts, indicating the model leverages decomposition to reason spatially; however absolute performance on deep hops is low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Competitive with GPT-3.5 on some SpartQA-Human splits; substantially lower than GPT-3.5 and GPT-4 on StepGame deep hops; competitive open-source alternative overall but weaker on deep synthetic multi-hop tasks compared to neuro-symbolically trained small models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly on deep-hop StepGame tasks (accuracy drops strongly with k), indicating weak multi-hop spatial chaining when used only with in-context methods; sensitive to prompt format and context length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8334.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT3(d2)+ASP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 variant (d2) for relation extraction + Answer Set Programming external solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline baseline from prior work that uses GPT-3 to extract relations into a formal representation and then passes these to an ASP solver to perform logical spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coupling large language models with logic programming for robust and general reasoning from text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (d2) + ASP solver pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Relation extraction performed by a GPT-3 variant (d2) to produce formal logical facts; logical reasoning executed using an external Answer Set Programming (ASP) solver with engineered rules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame, SpartQA-Human (synthetic benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic spatial QA requiring precise multi-hop logical inference over formal relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Two-stage pipeline: (1) LLM maps context to formal symbolic relations at inference time; (2) external ASP solver applies engineered spatial logic rules to infer answers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Explicit symbolic reasoning via formal representations and ASP; depends on high-quality relation extraction at inference time and engineered rules in external solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame (Table 2 second block): e.g., very high accuracy reported by prior work: k=1..10 values in the table show e.g., k=1 92.60%, k=2 89.90%, k=3 89.10%, k=4 93.80%, k=5 92.90%, k=6 91.60%, k=7 91.20%, k=8 90.40%, k=9 89.00%, k=10 88.30% (reported from Yang et al. 2023 results included in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High StepGame accuracy across deep hop depths indicates that explicit symbolic pipelines can accurately perform multi-hop spatial reasoning when formal representations and engineered rules are available.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Substantially outperforms pure in-context LLMs (GPT-3.5, Llama-3) and standard fine-tuned models on StepGame in prior work; comparable or better than neuro-symbolically fine-tuned small models on synthetic benchmarks but relies on access to symbolic representations at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires accurate formal extraction at inference time and reliance on external symbolic solver with engineered rules; authors of the current paper note such pipelines are effective primarily on synthetic data and struggle on realistic text where mapping to formal facts is hard.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8334.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8334.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PistaQ*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PistaQ* relation-extraction + external reasoning pipeline (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pipeline approach that extracts relations (via LLMs) and applies external reasoning; used as a comparative baseline in this paper, showing high performance on synthetic but poor transfer to realistic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Disentangling extraction and reasoning in multi-hop spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PistaQ* (pipeline combining relation extraction with external reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline approach that separates relation extraction and symbolic reasoning; relation extraction may use LLMs and reasoning uses external tools with engineered rules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic and realistic spatial QA datasets with multi-hop reasoning demands.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Extract relations to formal representations (at inference) and run external symbolic reasoning to answer questions; evaluated on synthetic and realistic benchmarks reported in prior work and cited here for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Relation extraction followed by an external reasoning engine implementing spatial logic rules; not a single fine-tuned LLM but a hybrid pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 2: PistaQ* on SpartQA-Human Ver.1 75.52% (reported from Mirzaee and Kordjamshidi 2023); on ReSQ the paper reports poor transfer with PistaQ* ~47.21% accuracy (noted as low on realistic domain).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong performance on synthetic dataset (SpartQA-Human) shows symbolic pipelines succeed when formal facts are accessible; large drop on ReSQ indicates poor robustness to realistic/ambiguous text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms many pure LLM baselines on synthetic tasks but underperforms on realistic ReSQ compared to in-context LLMs; contrasted with the current paper's approach which uses constraints only during training and does not require formal representations at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires formal representations at test time which are difficult to obtain reliably from realistic natural language; thus performance falls sharply on realistic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SpaRTUN <em>(Rating: 2)</em></li>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 2)</em></li>
                <li>Coupling large language models with logic programming for robust and general reasoning from text <em>(Rating: 2)</em></li>
                <li>Disentangling extraction and reasoning in multi-hop spatial reasoning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Chain-of-symbol prompting for spatial reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8334",
    "paper_id": "paper-270619922",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "BERT-T+Q-Chain",
            "name_full": "BERT fine-tuned with SpaRTUN then tuned with Q-Chain neuro-symbolic constraints",
            "brief_description": "A BERT-based encoder model first tuned on the synthetic SpaRTUN corpus and then fine-tuned with example-specific Q-Chains and soft logical constraints (via DomiKnowS) to improve multi-hop spatial reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (bert-based-uncased) with Q-Chain",
            "model_description": "Encoder-based transformer (BERT) fine-tuned for QA by concatenating question and context, using [CLS] for classification; augmented by SpaRTUN pre-fine-tuning and neuro-symbolic constraint loss implemented with DomiKnowS and t-norm Product soft logic.",
            "model_size": null,
            "puzzle_name": "SpaRTUN (training), SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial question-answering benchmarks requiring multi-hop (up to 10) reasoning over described scenes (synthetic and realistic); relations include direction and topological relations.",
            "task_setup": "Fine-tuned on SpaRTUN then target dataset; input formed by Q [SEP] C for encoder; YN uses binary classification, FR uses multiple binary classifiers (one per relation). Training objective combines cross-entropy with soft logical constraint violations from Q-Chains (Î»_k=1).",
            "mechanisms_or_strategies": "Q-Chain: automatic construction of reasoning chains via forward chaining on SpaRTUN logical facts; derive consistency constraints from resolution trees; convert constraints to differentiable soft logic (t-norm Product) via DomiKnowS; Primal-Dual optimization fine-tuning.",
            "performance_metrics": "StepGame accuracy by reasoning depth k (Table 2): k=1: 98.70%, k=2: 96.45%, k=3: 93.03%, k=4: 74.58%, k=5: 64.95%, k=6: 59.04%, k=7: 54.38%, k=8: 49.23%, k=9: 45.36%, k=10: 44.05%. SpartQA-Human/ReSQ: SpartQA-Human Ver.1 59.44% (vs BERT 54.54%); ReSQ overall 61.31% (compared to BERT 60.98%).",
            "evidence_of_spatial_reasoning": "Consistent and larger relative improvements vs. BERT and other baselines on datasets and on higher-hop StepGame steps (k&gt;=6) indicate use of multi-hop spatial reasoning abstractions; qualitative examples (Figure 4) show correct multi-step derivations.",
            "comparisons": "Outperforms base BERT and BERT-T on StepGame across most hops; smaller gap vs. LLMs on realistic ReSQ where LLMs (GPT-3.5/GPT-4) have stronger commonsense. Compared to relation-extraction + external solver pipelines (GPT3(d2)+ASP, PistaQ*), BERT-T+Q-Chain achieves competitive multi-hop accuracy without needing formal representation at inference.",
            "limitations_or_failure_cases": "Requires chain-of-reasoning annotations (SpaRTUN) during training; when reasoning depth is shallow (k=1) or questions require commonsense/unclassified knowledge, BERT-T+Q-Chain can be less effective (negative impact observed on BERT for some k=1 / unclassified splits). Training needs GPU resources (~8 A6000 GPUs, ~100 GPU hours for experiments).",
            "uuid": "e8334.0",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Flan-T5-T+Q-Chain",
            "name_full": "Flan-T5 (base) fine-tuned with SpaRTUN then constrained with Q-Chain neuro-symbolic training",
            "brief_description": "Generative Flan-T5-base model fine-tuned first on SpaRTUN then further trained with Q-Chain derived soft logical constraints (using LoRA adapters to reduce compute) to improve multi-hop spatial FR and YN QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-base (with LoRA adapter) + Q-Chain",
            "model_description": "Open-source instruction-finetuned encoder-decoder transformer (Flan-T5-base) used as a generative QA model; for YN model compares probability of Yes/No tokens; LoRA adapters used to reduce trainable parameters; Q-Chain soft-logic constraints integrated via DomiKnowS.",
            "model_size": null,
            "puzzle_name": "SpaRTUN (training), SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial QA benchmarks requiring multi-hop spatial reasoning (YN and multi-label FR relations).",
            "task_setup": "Prompt: 'Answer based on the context: C Q.' YN: first-token Yes/No selection; FR: generative multi-label prediction (handled via prompt engineering and training). Training objective combined cross-entropy and soft constraint losses from Q-Chains.",
            "mechanisms_or_strategies": "Same Q-Chain pipeline as encoder models: forward-chaining to build reasoning chains, derive constraints, t-norm Product soft logic via DomiKnowS, Primal-Dual optimizer; LoRA for efficient fine-tuning of Flan-T5.",
            "performance_metrics": "SpartQA-Human Ver.1/Ver.2: Flan-T5-T+Q-Chain reported 55.94% / 61.61% respectively (improvements over Flan-T5-T). ReSQ unclassified and k splits: Flan-T5-T+Q-Chain overall 64.43% (compared to Flan-T5-T 62.30%). StepGame: the paper reports consistent positive differences across reasoning hops vs LLMs; exact per-k numbers for Flan-T5 variants not listed in StepGame table but reported as improved.",
            "evidence_of_spatial_reasoning": "Notable improvements on synthetic multi-hop datasets (SpartQA-Human, StepGame) and ReSQ k=2 split indicate enhanced multi-hop spatial reasoning; gains larger than for BERT variants on some splits.",
            "comparisons": "Improves more substantially from Q-Chain training than BERT variants (paper notes larger relative gains for Flan-T5), but closed-source LLMs (GPT-3.5/GPT-4) still outperform on commonsense-heavy (ReSQ unclassified) questions.",
            "limitations_or_failure_cases": "Generative FR multi-label setting complicates direct use of constraints at inference; requires SpaRTUN chain annotations for training; LoRA reduces parameters but compute demand remains non-trivial; less effective on commonsense/unclassified splits compared to large LLMs.",
            "uuid": "e8334.1",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BERT / BERT-T (baselines)",
            "name_full": "BERT (bert-based-uncased) baseline and BERT-T (SpaRTUN pre-tuned)",
            "brief_description": "Encoder-based BERT baseline fine-tuned on target datasets; BERT-T refers to the model additionally fine-tuned on synthetic SpaRTUN before task-specific fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT (bert-based-uncased), BERT-T",
            "model_description": "Pre-trained BERT encoder used with classification heads for YN and multi-label FR tasks, input created by concatenating question and context; BERT-T indicates additional pre-fine-tuning on SpaRTUN synthetic dataset.",
            "model_size": null,
            "puzzle_name": "SpaRTUN (training for BERT-T), SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial QA datasets varying from realistic (ReSQ) to synthetic multi-hop (StepGame, SpartQA-Human).",
            "task_setup": "Fine-tuning on each target dataset; FR handled as multiple binary classification layers (one per relation); training uses cross-entropy only for baselines.",
            "mechanisms_or_strategies": "Standard supervised fine-tuning (cross-entropy) on target dataset; BERT-T uses SpaRTUN synthetic pre-fine-tuning to improve multi-hop capability.",
            "performance_metrics": "StepGame (Table 2) BERT: k=1 98.51%, k=2 95.53%, k=3 91.68%, k=4 66.71%, k=5 49.11%, k=6 41.47%, k=7 41.47%, k=8 32.09%, k=9 28.94%, k=10 28.16%. SpartQA-Human Ver.1 54.54% (BERT) vs BERT-T 55.94%.",
            "evidence_of_spatial_reasoning": "High accuracy at low hop depths (k&lt;=3) shows ability to handle shallow spatial reasoning; performance drops rapidly with more hops, motivating Q-Chain approach.",
            "comparisons": "BERT-T generally outperforms BERT, showing benefit of SpaRTUN pre-fine-tuning but is still outperformed by BERT-T+Q-Chain at higher hop depths. LLMs (GPT-3.5/GPT-4) beat BERT variants on realistic ReSQ unclassified (commonsense) splits.",
            "limitations_or_failure_cases": "Performance degrades substantially with increasing reasoning hops; lacks logical constraint supervision leading to poorer generalization on deep multi-hop scenarios.",
            "uuid": "e8334.2",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "A closed-source autoregressive LLM from OpenAI used here as an in-context learning baseline (zero-shot, few-shot, CoT, LR, CoS).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Large autoregressive transformer used in zero-shot and few-shot in-context learning experiments with different prompt styles including Chain-of-Thought (CoT), Logical Representation (LR), and Chain-of-Symbol (CoS).",
            "model_size": null,
            "puzzle_name": "SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial QA benchmarks requiring multi-hop spatial reasoning and commonsense in realistic splits.",
            "task_setup": "In-context learning: zero-shot (direct prompt), few-shot (4 examples concatenated), CoT (few-shot with manually-crafted reasoning explanations), LR (logical predicate-argument rationales), CoS (symbolic chain representations), and Step-by-Step prompting (context compression).",
            "mechanisms_or_strategies": "Primarily in-context reasoning using CoT, LR, CoS; can output natural-language rationales; no fine-tuning in experiments. For some external baselines, outputs were used for relation extraction and then passed to external solvers (GPT3(d2)+ASP in prior work).",
            "performance_metrics": "SpartQA-Human (Table 2): zero-shot 58.04%, few-shot 62.23%, CoT 65.73% (best reported for GPT-3.5 variants). ReSQ k splits: k=1 58.03%, k=2 74.67% (table mixed across columns). StepGame raw vs step-by-step (Table 3): zero-shot 58.04% raw -&gt; 63.64% step-by-step; few-shot 62.23% -&gt; 64.33%; CoT 65.73% -&gt; 67.83%.",
            "evidence_of_spatial_reasoning": "CoT and step-by-step prompting improve accuracy, indicating the model can leverage chain-of-thought style reasoning to solve spatial multi-step problems to an extent; however accuracy falls at larger hop counts (StepGame rows show steep decline as k increases).",
            "comparisons": "Outperformed by GPT-4 on many splits; outperforms few-shot Llama-3 on some tasks but underperforms tuned small models (BERT-T+Q-Chain) on deep StepGame hops. External pipeline GPT3(d2)+ASP (from prior work) achieves much higher StepGame accuracies (e.g., 92.6% at k=1..10 range) when combined with symbolic solver.",
            "limitations_or_failure_cases": "Performance degrades rapidly with increasing hop depth; CoS and LR prompting sometimes underperform CoT; struggles in synthetic deep-reasoning benchmarks compared to neuro-symbolically tuned small models and compared to pipelines that use explicit relation extraction + external solvers; sensitive to context length/complexity (benefits from context compression).",
            "uuid": "e8334.3",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art closed-source LLM used as a zero-shot in-context baseline showing strong performance on spatial QA tasks with minimal prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large multimodal-capable autoregressive transformer reported in the paper as a zero-shot baseline for spatial QA tasks.",
            "model_size": null,
            "puzzle_name": "SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial QA requiring multi-hop reasoning and some commonsense inference.",
            "task_setup": "Zero-shot prompts; also used in comparisons with in-context methods; no fine-tuning performed in this paper's experiments.",
            "mechanisms_or_strategies": "Inherent chain-of-thought-like capabilities exploited via CoT prompting where applicable; step-by-step context compression also applied to improve raw-context performance.",
            "performance_metrics": "SpartQA-Human (Table 2): zero-shot 77.62% (Ver.1) and 68.75% (Ver.2?) as reported in table; StepGame step-by-step performance shown as 78.32% (Table 3, step-by-step) which is higher than GPT-3.5 variants.",
            "evidence_of_spatial_reasoning": "High zero-shot performance suggests substantial general reasoning and commonsense capabilities applied to spatial QA; improvements from step-by-step prompting indicate the model benefits from decomposed context.",
            "comparisons": "Outperforms GPT-3.5 and open-source Llama-3 on many realistic-domain splits (ReSQ unclassified) and SpartQA-Human; however, neuro-symbolically tuned smaller models (BERT-T+Q-Chain) can outperform or match GPT-4 on some synthetic deep-hop StepGame settings according to the paper's analysis.",
            "limitations_or_failure_cases": "Despite high overall accuracy, the model can still struggle on deep multi-hop synthetic benchmarks and benefits from context simplification; closed-source nature and lack of fine-tuning in this study mean it relies on in-context prompt engineering rather than training with logical constraints.",
            "uuid": "e8334.4",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-3-8B",
            "name_full": "Llama 3 (8B)",
            "brief_description": "An open-source Llama-3 model (8B parameters) used as an in-context learning baseline evaluated with zero-shot, few-shot, and CoT prompting variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B",
            "model_description": "Open-source Llama-3 family model (8B) employed for zero-shot and few-shot in-context evaluation on spatial QA; used CoT, LR, CoS and step-by-step context compression.",
            "model_size": "8B",
            "puzzle_name": "SpartQA-Human, ReSQ, StepGame",
            "puzzle_type": "Textual spatial QA benchmarks requiring multi-hop reasoning and contextual understanding.",
            "task_setup": "Zero-shot and few-shot in-context experiments (4 examples for few-shot); CoT/CoS/LR variants and step-by-step context compression used to create prompts.",
            "mechanisms_or_strategies": "In-context chain-of-thought, logical representation prompting, chain-of-symbol prompting, and step-by-step context splitting to simplify context.",
            "performance_metrics": "SpartQA-Human (Table 2): zero-shot 61.54% (Ver.1), few-shot 62.94%, CoT 67.83%; StepGame few-shot row (Table 2 second block): e.g., k=1 38.01%, k=2 27.87%, k=3 24.15%, dropping to ~14-19% at higher hops (much lower than tuned models).",
            "evidence_of_spatial_reasoning": "CoT and step-by-step prompting improve accuracy significantly over raw prompts, indicating the model leverages decomposition to reason spatially; however absolute performance on deep hops is low.",
            "comparisons": "Competitive with GPT-3.5 on some SpartQA-Human splits; substantially lower than GPT-3.5 and GPT-4 on StepGame deep hops; competitive open-source alternative overall but weaker on deep synthetic multi-hop tasks compared to neuro-symbolically trained small models.",
            "limitations_or_failure_cases": "Performs poorly on deep-hop StepGame tasks (accuracy drops strongly with k), indicating weak multi-hop spatial chaining when used only with in-context methods; sensitive to prompt format and context length.",
            "uuid": "e8334.5",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT3(d2)+ASP",
            "name_full": "GPT-3 variant (d2) for relation extraction + Answer Set Programming external solver",
            "brief_description": "A pipeline baseline from prior work that uses GPT-3 to extract relations into a formal representation and then passes these to an ASP solver to perform logical spatial reasoning.",
            "citation_title": "Coupling large language models with logic programming for robust and general reasoning from text",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (d2) + ASP solver pipeline",
            "model_description": "Relation extraction performed by a GPT-3 variant (d2) to produce formal logical facts; logical reasoning executed using an external Answer Set Programming (ASP) solver with engineered rules.",
            "model_size": null,
            "puzzle_name": "StepGame, SpartQA-Human (synthetic benchmarks)",
            "puzzle_type": "Synthetic spatial QA requiring precise multi-hop logical inference over formal relations.",
            "task_setup": "Two-stage pipeline: (1) LLM maps context to formal symbolic relations at inference time; (2) external ASP solver applies engineered spatial logic rules to infer answers.",
            "mechanisms_or_strategies": "Explicit symbolic reasoning via formal representations and ASP; depends on high-quality relation extraction at inference time and engineered rules in external solver.",
            "performance_metrics": "StepGame (Table 2 second block): e.g., very high accuracy reported by prior work: k=1..10 values in the table show e.g., k=1 92.60%, k=2 89.90%, k=3 89.10%, k=4 93.80%, k=5 92.90%, k=6 91.60%, k=7 91.20%, k=8 90.40%, k=9 89.00%, k=10 88.30% (reported from Yang et al. 2023 results included in paper).",
            "evidence_of_spatial_reasoning": "High StepGame accuracy across deep hop depths indicates that explicit symbolic pipelines can accurately perform multi-hop spatial reasoning when formal representations and engineered rules are available.",
            "comparisons": "Substantially outperforms pure in-context LLMs (GPT-3.5, Llama-3) and standard fine-tuned models on StepGame in prior work; comparable or better than neuro-symbolically fine-tuned small models on synthetic benchmarks but relies on access to symbolic representations at inference.",
            "limitations_or_failure_cases": "Requires accurate formal extraction at inference time and reliance on external symbolic solver with engineered rules; authors of the current paper note such pipelines are effective primarily on synthetic data and struggle on realistic text where mapping to formal facts is hard.",
            "uuid": "e8334.6",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "PistaQ*",
            "name_full": "PistaQ* relation-extraction + external reasoning pipeline (prior work)",
            "brief_description": "A prior pipeline approach that extracts relations (via LLMs) and applies external reasoning; used as a comparative baseline in this paper, showing high performance on synthetic but poor transfer to realistic domains.",
            "citation_title": "Disentangling extraction and reasoning in multi-hop spatial reasoning",
            "mention_or_use": "mention",
            "model_name": "PistaQ* (pipeline combining relation extraction with external reasoning)",
            "model_description": "Pipeline approach that separates relation extraction and symbolic reasoning; relation extraction may use LLMs and reasoning uses external tools with engineered rules.",
            "model_size": null,
            "puzzle_name": "SpartQA-Human, ReSQ",
            "puzzle_type": "Synthetic and realistic spatial QA datasets with multi-hop reasoning demands.",
            "task_setup": "Extract relations to formal representations (at inference) and run external symbolic reasoning to answer questions; evaluated on synthetic and realistic benchmarks reported in prior work and cited here for comparison.",
            "mechanisms_or_strategies": "Relation extraction followed by an external reasoning engine implementing spatial logic rules; not a single fine-tuned LLM but a hybrid pipeline.",
            "performance_metrics": "Reported in Table 2: PistaQ* on SpartQA-Human Ver.1 75.52% (reported from Mirzaee and Kordjamshidi 2023); on ReSQ the paper reports poor transfer with PistaQ* ~47.21% accuracy (noted as low on realistic domain).",
            "evidence_of_spatial_reasoning": "Strong performance on synthetic dataset (SpartQA-Human) shows symbolic pipelines succeed when formal facts are accessible; large drop on ReSQ indicates poor robustness to realistic/ambiguous text.",
            "comparisons": "Outperforms many pure LLM baselines on synthetic tasks but underperforms on realistic ReSQ compared to in-context LLMs; contrasted with the current paper's approach which uses constraints only during training and does not require formal representations at inference.",
            "limitations_or_failure_cases": "Requires formal representations at test time which are difficult to obtain reliably from realistic natural language; thus performance falls sharply on realistic datasets.",
            "uuid": "e8334.7",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SpaRTUN",
            "rating": 2
        },
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2,
            "sanitized_title": "spartqa_a_textual_question_answering_benchmark_for_spatial_reasoning"
        },
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 2,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        },
        {
            "paper_title": "Coupling large language models with logic programming for robust and general reasoning from text",
            "rating": 2,
            "sanitized_title": "coupling_large_language_models_with_logic_programming_for_robust_and_general_reasoning_from_text"
        },
        {
            "paper_title": "Disentangling extraction and reasoning in multi-hop spatial reasoning",
            "rating": 2,
            "sanitized_title": "disentangling_extraction_and_reasoning_in_multihop_spatial_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-symbol prompting for spatial reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofsymbol_prompting_for_spatial_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01936175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Neuro-symbolic Training for Spatial Reasoning over Natural Language
29 May 2025</p>
<p>Tanawan Premsri premsrit@msu.edu 
Department of Computer Science
Michigan State University</p>
<p>Parisa Kordjamshidi 
Department of Computer Science
Michigan State University</p>
<p>Neuro-symbolic Training for Spatial Reasoning over Natural Language
29 May 20255CF3B0D20A4A1392BF4BFC4D97D255F3arXiv:2406.13828v3[cs.CL]Context + Question Language Model Answer
Spatial reasoning based on natural language expressions is essential for everyday human tasks.This reasoning ability is also crucial for machines to interact with their environment in a human-like manner.However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions.This is attributed to not achieving the right level of abstraction required for generalizability.To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering.Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains.We evaluate our approach on existing spatial question-answering benchmarks.Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</p>
<p>Introduction</p>
<p>Spatial language conveys information about the location and translocation of objects/entities in space.Reasoning over spatial language is required for natural language processing systems to answer "Where" questions and identify the relative spatial relations between objects explained in the text, such as left and right.Figure 1, demonstrates a spatial language comprehension and questionanswering problem.Understanding spatial language turns out to be crucial for many AI applications, including question-answering systems, grounding language into visual modality (Zhang et al., 2021;Zhang and Kordjamshidi, 2022;Liu et al., 2023), navigating robots (Sisbot et al., 2007;Yadollahi et al., 2023) and even in more specific</p>
<p>SpartQA-Human</p>
<p>Context: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a small yellow square near and to the left of a large yellow circle.The large circle is near and to the left of a large blue triangle.In B, there is a small yellow square near and below a large blue triangle.There is a medium black triangle near and above the large blue triangle.In C, there is a small black square far to the left and above a small blue circle.The blue circle is near and to the left of a large yellow square.YN Question: Is the small black thing to the right of medium black thing?Answer: Yes FR Question: What is the position of small black thing relative to medium black thing?Answer: Right ReSQ Context: A tourist group is visiting a school.The school kids are sitting at their desks, singing a song and clapping their hands, with their teacher clapping as well.Six of the tourists are standing behind them and are watching them, one tourist is taking a photo.YN Question: Are the tourists in the school?Answer: Yes  fields such as medical domain (Atif et al., 2007;Datta et al., 2020;Gong et al., 2024).</p>
<p>Despite the high performance of Large Language models (LLMs) in many NLP and vision problems (Brown et al., 2020;OpenAI, 2023), recent studies highlight the shortcomings in complex spatial reasoning over text (Bang et al., 2023;Yang et al., 2023;Mirzaee and Kordjamshidi, 2023;Liu et al., 2023;Chen et al., 2024).This paper focuses on complex, that is, multi-hop spatial reasoning over text.However, our research findings can advance complex reasoning over text in general.Our main idea is to exploit spatial logical rules of reasoning in training language models.</p>
<p>Explicit logic, in combination with LLMs, has been used in some related works.In particular, Yang et al. 2023 integrated explicit symbolic reasoning with LLMs in a pipeline.The LLMs map natural language to symbolic representation, and this formal representation is passed to an Answer Set Programming (ASP) solver to conduct logical reasoning.Similarly, Mirzaee and Kordjamshidi 2023 passed formal representations obtained from LLMs to Prolog to conduct logical reasoning and answer spatial questions.These approaches rely on external reasoning tools and require accurate formal representations generated at inference time.Obtaining such formal representations for realistic domain natural language is hard and, as shown in (Mirzaee and Kordjamshidi, 2023), limits their effectiveness only to synthetic benchmarks.</p>
<p>Here, we aim to address the limitations of utilizing spatial logic in realistic domains.In our proposed technique, we fine-tune language models using spatial logical rules to guide them in capturing the level of abstraction needed for general spatial reasoning.In particular, we train the models to minimize both cross-entropy loss and violation from logical constraints.We hypothesize that incorporating logical knowledge as a source of supervision, even using synthetic data, enriches the models' abstract reasoning over spatial concepts, further enhancing their generalizability.A key advantage of our approach is that it does not require full coverage of logical knowledge.Any partially available logical knowledge can be utilized during training.Additionally, formal knowledge representation is not needed at inference time, which is crucial because generating formal representations during inference is a challenging task in itself.Our technique also avoids reliance on external reasoning tools, which can add computational complexity in real-time applications.</p>
<p>We use the available three benchmarks for spatial question answering, SpartQA-Human (Mirzaee et al., 2021), ReSQ (Mirzaee and Kordjamshidi, 2022), and StepGame (Shi et al., 2022) to evaluate our proposed method.Our improved results confirm our hypothesis about the impact of neurosymbolic training on generalizability.The contribution of this paper can be summarized as follows1 , (1) We exploit the available logical spatial knowledge in tuning language models to improve their spatial reasoning.(2) We provide extensive experimental results and analysis on the existing benchmarks.Our results consistently confirm the effectiveness of our proposed approach in both encoderbased and generative language models and their generalizability for transferring knowledge to different domains; (3) We show that tuning smaller models with our proposed method generalizes better on out-of-domain and complex reasoning compared to using prompt-engineered, very large language models.</p>
<p>Investigating the reasoning capability of NLP models has become an important research topic given the instability in the performance of LLMs (Gendron et al., 2024;Valmeekam et al., 2022;Feng et al., 2024;Chen et al., 2024;Bang et al., 2023).Typically, this ability is assessed through questionanswering (QA) benchmarks (Weston et al., 2016;Zhou et al., 2020;Tan et al., 2023).However, fewer studies are focused on spatial reasoning over text.Recent benchmarks, such as SpaRTUN (Mirzaee and Kordjamshidi, 2022) and StepGame (Shi et al., 2022;Li et al., 2024), have been developed to address this issue by providing comprehensive evaluation resources for spatial question answering.The current results using the existing benchmarks indicate that LLMs struggle with spatial reasoning over text, even on synthetic datasets (Mirzaee and Kordjamshidi, 2022;Yang et al., 2023;Bang et al., 2023).</p>
<p>Several works have attempted to improve LLMs' spatial reasoning capabilities.Mirzaee and Kordjamshidi 2022 fine-tunes LLMs with synthetic data, which improves their multi-hop reasoning abilities, even when applied to realistic domains.Another approach in (Shi et al., 2022) designed a specialized memory network based on a recurrent neural network for deep reasoning tasks, but it was less effective than tuning pre-trained language models (PLMs).Some papers focus on enhancing spatial reasoning by in-context learning.For instance, Sharma 2023 proposed a prompting technique that decomposes the target spatial question into simpler questions and infers the answer to the final query based on the information gained from them.Another approach, proposed by Hu et al. 2024, replaced textual explanations in the Chain-of-Thought (CoT) by symbolic representations.While this method improved spatial understanding in simple environments, it was less applicable to diverse and complex spatial relations settings.Lastly, some studies employ neuro-symbolic modeling that involves LLMs in the pipeline (Yang et al., 2023;Mirzaee and Kordjamshidi, 2023).These approaches extract relations and obtain formal representations using LLMs.Then, the formal representation is passed to an external solver that includes engineered rules of reasoning to conduct logical reasoning and answer spatial questions.While this method significantly improved multihop reasoning, it required formal representations at inference time, which posed a challenge.Furthermore, formalizing commonsense reasoning is difficult, if not impossible, for realistic domains.</p>
<p>Unlike the previous studies, we exploit available spatial logical knowledge as a set of constraints in fine-tuning language models.Compared to hard pipelines, this approach has the advantage that logical knowledge does not need to be complete and cover all possible situations.Our approach utilizes synthetic data and logical representations during training only.Numerous research studies have highlighted the utility of logical knowledge in solving question-answering tasks.For instance, Prager et al. 2004 demonstrates that answering sets of questions jointly using their logical relationships enhances the performance in reasoning tasks.Amizadeh et al. 2020 directly used the logical formalization of the question as input to the model to improve the logical reasoning of vision and language models.Sun et al. 2022 proposed framework with two main components.The first component is for encoding logical knowledge, and the second is for entailment of that knowledge.Lastly, Kamali et al. 2024 proposed a framework that integrates logical reasoning with generalizable grounding interpretation to enhance both grounding and reasoning in vision and language models.</p>
<p>Utilizing logical knowledge has been proven effective in solving many other NLP problems.For example, Lu et al. 2022 uses a heuristic function inspired by the A* algorithm to restrict the output of generative language models with a set of constraints, while Qin et al. 2022 uses a sampling method looking for the most optimal solution that satisfies the constraints.Both of these techniques apply the constraints during inference, which differs from our proposed method for using them during the training.Lastly, Rajaby Faghihi et al. 2023 provided a benchmark to evaluate the benefits of logical constraints in several NLP problems under the DomiKnowS framework (Rajaby Faghihi et al., 2021).In this paper, we propose a novel approach for formulating spatial reasoning knowledge as logical constraints and exploit the existing techniques for the actual integration.We utilize constraints solely during training, eliminating the need for access to logical constraints during inference where using them might be problematic for realistic domains and real-time applications.</p>
<p>Methodology</p>
<p>Problem Definition</p>
<p>For the spatial QA task, the input includes a textual context C that describes a scene and a question Q about the spatial relations between objects in the scene.The output is the answer(s) to the question, denoted as A. There are two types of questions/answers: Yes/No (YN) and find-relation (FR).The answer domain is restricted to A = {Y es, N o} for YN questions.The answers to FR questions depend on the dataset and the types of relations covered.For example, the answer domain for FR can be A = {lef t, right, above, below,. . .}.</p>
<p>Backbone Language Models</p>
<p>We use both encoder-based models from the BERT family (Devlin et al., 2019) as well as generative models like Flan-T5 (Chung et al., 2024) to assess the effectiveness of our proposed fine-tuning approach.We used other LLMs, such as GPT family and Llama3-8B, as comparison baselines without fine-tuning.</p>
<p>Encoder-based Language Models</p>
<p>We utilize BERT as the backbone architecture to consistently build on top of previous results in Mirzaee and Kordjamshidi (2022).We fine-tune BERT by adding classification layers for QA tasks.We employ a binary classification layer for the YN domain while using multiple binary classification layers for the FR domain.The motivation behind using multiple binary classification layers instead of one multi-label classification layer is that one question can have multiple labels as responses.The number of binary classification layers in the FR domain depends on the number of possible answers, |A|, in the problem setting.The input to the BERT model is constructed by concatenating the question, Q, separator token, [SEP ], and the context C.After feeding the input to BERT, we use the [CLS] token from the last layer as the input to the classification layer(s) for final answer prediction.</p>
<p>Generative Language Models</p>
<p>We select Flan-T5 (Chung et al., 2024) as the opensource generative baseline for fine-tuning.In the YN setting, processing the output is straightforward, as the first token can represent the response directly generated by the model.We compare the probability of the Yes and No tokens and select the one with the highest probability as the answer.Obtaining consistency constraints from spatial logical rules and answers inferred from questions generated using intermediate facts in Q-Chain.3. Incorporating obtained consistency constraints into the loss function using differentiable soft logic during tuning.For more details, refer to Section 3.3.</p>
<p>However, this approach is not feasible in the FR setting due to the multi-label questions requiring multiple answers.The input prompt to the Flan-T5 model is structured as follows: "Answer based on the context: C Q."</p>
<p>For In-context Learning, we select Llama3-8B (AI@Meta, 2024), GPT3.5 (Brown et al., 2020), and GPT-4 (OpenAI, 2023) as the baseline LLMs using Zero-shot, and Few-shot settings.Zero-shot.We prompt the LLMs to provide answers based on the scene description and the question without including any examples.Few-shot.We randomly sample four pairs of questions and answers from the training dataset and concatenate them to the context and target question to form the few-shot setting.</p>
<p>Training with Spatial Logic</p>
<p>To address the challenge of multi-hop spatial reasoning, we employ symbolic knowledge during training.We assume a formal knowledge base (KB) of rules, referred to as spatial logic, is given during training time.This spatial logic does not need to be exhaustive or cover all reasoning aspects; any available knowledge can be used to enhance the reasoning capabilities of the language model.This work uses 79 reasoning rules taken from Mirzaee and Kordjamshidi (2022).These rules are divided into four categories: converse, inverse, transitive, and transitive + topological.The KB covers rules between 15 spatial relations, including directional and topological relations, such as "If x is above y, then y below x." denoted formally as Above(x, y) â Below(y, x).</p>
<p>Our main hypothesis is that supervision from high-level logical knowledge allows the model to capture more abstract patterns, thereby improving generalization to other domains.To exploit the spatial logic in the training phase, we follow three steps: (1) Creating the chain of reasoning for each training example.(2) Obtaining consistency constraints from the chain of reasoning.(3) Incorporating constraints in the loss function.Please note that we will need the reasoning chain annotations for fine-tuning with spatial logic.SpaRTUN is the only synthetic dataset that provides such annotated formal representations, making it a suitable source of supervision for our tuning purpose.Creating example-specific rules.We exploit Spatial Logic by automatically augmenting each training example with a chain of questions, denoted as Q-Chain.This process is illustrated in Figure 2.</p>
<p>We infer the answer to the final query from the input context using the logical resolution tree, which outlines the logical steps for implication.Our synthetic training data (i.e., SpaRTUN) provides logi-cal representations of the initial facts expressed in the context.We build the resolution tree using the forward chaining algorithm to infer a specific target fact given a set of initial facts.In our example, the initial facts are "The white is above the orange", denoted as q 1 , and "The red is above the white", denoted as q 2 .The target fact is "The orange below the red," denoted as t.The forward chaining approach is a reasoning method that starts with the initial facts in the knowledge base.It performs an exhaustive search for all applicable inference rules and applies them to the current facts to iteratively infer new ones until the target fact is reached or no new facts are inferred (Nickerson, 1994).In the example of Figure 2, we apply the spatial converse rules to the initial fact q 1 and q 2 .This approach yields the intermediate inferred facts, q 3 , "The orange is below the white," from q 1 , and q 4 , "The white is below the red," from q 2 .Finally, we search for the applicable spatial rule to combine q 3 and q 4 , resulting in the target fact, t, "The orange is below the red."Obtaining consistency constraints.In this step, we obtain a set of consistency constraints based on the given resolution tree.We denote the truth value of a fact q i in the Q-Chain as T ruth(q i ), which will be True if the fact is True and otherwise is False.We create a consistency constraint for each step of the Q-Chain.For example, in Figure 2, step R 1 is applied to q 1 to infer q 3 .This step implies that the T ruth(q 1 ) should be consistent with the T ruth(q 3 ).In other words, q 1 , "white above orange," implies q 3 : "orange below white,".The corresponding formal constraint for this example will be T ruth(q 1 ) â T ruth(q 3 ).Incorporating Constraints.Our fine-tuning objective is to minimize the violation from the set of logical consistency constraints given for each training example.However, the logical expressions are not differentiable.To form a differentiable loss function, we use a soft logical surrogate of the constraints in the loss.There are various types of conversions for mapping logical constraints into differentiable soft logic, including t-norm Product, t-norm Åukasiewicz, and t-norm GÃ¶del (Li et al., 2019).We use the t-norm Product to convert, which means, Â¬A becomes 1 â a, A â§ B becomes ab, A â¨ B becomes a + b â ab, and A â B becomes min(1, b a ), where A and B are concepts with probabilities a and b respectively.During the training, the probability of the truth of the facts (T ruth(q i )) involved in the constraints is obtained from the model prediction for computing the loss.We build on previous studies of optimizing an objective that combines the task performance and soft constraints (Li et al., 2019;Asai and Hajishirzi, 2020;Bamas et al., 2020) and form the following objective,
â w L(w) = â w L(w) + K k=1 Î» k â w h k (w), (1)
where L(w) is the task-performance loss function, i.e., Cross-Entropy loss, K is the number of logical constraints related to the current Q-Chain, h k measures the violation from a given logical constraint in soft logic differentiable form, and Î» k is a learning parameter indicating the importance of obeying k th rule.We set the Î» k to 1 for all constraints to ensure equal importance.</p>
<p>In fact, in this work, we utilize the implementation in the DomiKnowS framework (Rajaby Faghihi et al., 2021) to integrate the logical constraints.This framework provides a declarative language for incorporating symbolic knowledge as logical constraints.We present the original logical forms in the program, and DomiKnowS automatically converts them into a differentiable form for inclusion in the loss function.We employ an optimization technique referred to as the Primal-Dual (PD) program that uses a dual formulation of the original objective (Bamas et al., 2020) for fine-tuning.Programming for building a model in DomiKnowS requires defining a graph of concepts and relationships, along with logical rules between concepts.The program for our problem setting is provided in Appendix E.</p>
<p>In-context Learning with Spatial Logic</p>
<p>We design a set of experiments to analyze and compare the LLMs' performance against our proposed fine-tuning method.We utilize spatial logical reasoning to manually create the rationale of few-shot examples for our in-context learning settings, including Chain-of-Thoughts (Wei et al., 2022), Logical Representation, and Chain-of-Symbols (Hu et al., 2024).Chain-of-Thought (CoT).To enable CoT, we provide instructions that require an explanation before answering the question.For each example response, we include a manually crafted rationale based on spatial logical rules.We provide these examples with instructions and target questions as input to the model to generate the answer.Multiple examples of this CoT setting are demonstrated in Appendix C.2. Logical Representation (LR).In this setting, we follow the same prompt structure as CoT but use logical representations instead of natural language.We obtain the first order logical representations (predicate-argument structures) by manually converting the CoT explanations.The predicateargument structure R(obj i , obj j ) represents the relation R between obj i and obj j , where obj i and obj j denote object i and object j, respectively.An example of the LR counterpart of the CoT phrase is in Table 1.Chain of Symbol (CoS).This symbolic representation was introduced by Hu et al. (2024).We explicitly follow the proposed process in Hu et al. (2024)  Step-by-Step Prompting.From the SpartQA-Human dataset, we observed that scene descriptions are often complex and lengthy.To deal with this issue, we introduce an additional prompting step using LLMs to break down the context before answering the question.Each generated line of context contains a simple spatial description.For example, given the original context, "There exists a big red square and a medium red square in a block called A.  Mirzaee and Kordjamshidi (2022).</p>
<p>SpartQA-Human (Ver.2) We follow the methodologies from (Mirzaee et al., 2021) to increase the size of the human annotated dataset.The objective is to enrich the SpartQA-Human with a more extensive test using diverse patterns and questions.</p>
<p>Annotators included authors and an undergraduate student who was paid as a research assistant.We extended both types of YN and FR questions.</p>
<p>ReSQ (Kordjamshidi et al., 2017;Mirzaee and Kordjamshidi, 2022) is a small realistic SQA dataset with multi-hop reasoning.The depth of reasoning is smaller than other synthetic datasets, but it usually requires commonsense knowledge to infer the answer.The annotations include the depth of reasoning denoted as k, which can be 1 or 2. The questions that required complex reasoning and commonsense knowledge are annotated as unclassified depth.12.30% of the test examples fall under the k=1 split, 23.93% fall under the k=2 split, and the rest are unclassified.This dataset only has YN questions.We use this dataset to evaluate the performance of our models in realistic domains.</p>
<p>StepGame (Shi et al., 2022) is a synthetic SQA dataset that contains a diverse set of multi-hop reasoning questions involving spatial relations, with annotated reasoning depth denoted as k .The questions in this dataset include up to 10 hops of reasoning, allowing for a comprehensive evaluation of models' reasoning capability at varying depths.This dataset contains only FR questions.</p>
<p>The domain of answer is a = {lef t, right, above, below, overlap, lower-lef t, lower-right, upperlef t, upper-right}.</p>
<p>Experimental Models for Tuning</p>
<p>BERT-family.We use bert-based-uncased as the initial checkpoint, referred to as BERT.This model is only fine-tuned with the target dataset in each experiment.In some of the initial experiments we used RoBERTa as well.However, due to insignificance of the differences, we continued other model variations with BERT.The results of RoBERTa can be found in Appendix B. BERT-T.We fine-tune the model with SpaRTUN first.Then, we further fine-tune with the target dataset of each experiment.</p>
<p>BERT-T + Q-Chain.We adhere to the methodology explained in Section 3.3 to integrate logical knowledge into the BERT-T model via finetuning with constraints using the DomiKnowS framework (Rajaby Faghihi et al., 2021).Flan-T5.We follow the methodology described in Section 3.2.2.We selected the flan-t5-base in this experiment as the starting checkpoint.To reduce computational costs by decreasing the number of training parameters, we employ the LoRA adapter (Hu et al., 2022).</p>
<p>Flan-T5-T.We utilize the same setting as the Flan-T5 model.We fine-tune the model first with SpaR-TUN and then with the target dataset.</p>
<p>Flan-T5-T + Q-Chain.We integrate the logical knowledge into Flan-T5-T model using proposed method in section 3.3.The backbone is the same as the baseline model without any changes.</p>
<p>Experimental Setup.We used the same hyperparameters across all experiments.The learning rate was selected from {1e â5 , 8e â6 , 1e â6 }.For SpartQA-Human and ReSQ, the number of epochs was 100, while for StepGame, it was 30.For fine-tuning with SpaRTUN, we used 12 epochs for BERT and 8 epochs for Flan-T5.The final hyperparameters were chosen using the development set of the target dataset.In all experiments, we used Cross-Entropy Loss as the loss function and Adam as the optimizer, with the amsgrad parameter set to True.The experiments were run on 8 A6000 GPUs, requiring approximately 100 GPU hours.</p>
<p>Experimental Models for In-context</p>
<p>We evaluate the performance of three selected LLMs for in-context learning setting including GPT3.5, GPT-4, and Llama3-8B, using ReSQ and SpartQA-Human to compare with our fine-tuning approach.</p>
<p>The selected examples and prompts for each in-context learning method can be found in the Appendix C. Zero-shot.We directly ask LLMs to answer the question given the corresponding context.Few-shot.We provide LLMs with four randomly selected examples from the training data.</p>
<p>CoT.We augment the few-shot setting with reasoning explanations along with the CoT examples.</p>
<p>LR.We provide the few-shot examples with the reasoning explanation in the logical format described in Section 3.4.</p>
<p>CoS.We further alter the few-shot examples with the CoS symbolic forms explained in Section 3.4.</p>
<p>Results and Discussion</p>
<p>Realistic Domain.ReSQ serves as a realistic SQA domain.Table 2 shows that using the Q-chain is effective for both BERT and Flan-T5, with notable improvement on Flan-T5.To better understand these results, we analyzed the performance across different splits of ReSQ based on the manually annotated depth of reasoning required to answer the questions.The first two splits include questions requiring one or two hops of reasoning, denoted as k = 1 and k = 2.The unclassified split covers questions where reasoning depth is hard to determine and rely more on commonsense knowledge.An example of each case is provided in Figure 3.Our observation from Table 2 reveals that our model consistently improves performance on k = 2 but negatively impacts BERT on k = 1 and the unclassified categories.This suggests that our finetuning approach is more effective when deeper reasoning is needed but is less effective for questions requiring commonsense knowledge.On the other hand, LLMs outperform fine-tuning approaches on ReSQ, specifically in the unclassified subset.They consistently show 2-13% higher performance than Flan-T5+T+Q-chain (lines 7-13).These results indicate that one of LLMs' superior performances is due to their commonsense knowledge while our approach performs better in multi-hop reasoning.Nevertheless, we observe that logicbased fine-tuning enhances Flan-T5 more significantly than BERT on the unclassified subset.This    PLMs and LLMs is much smaller in this domain compared to the realistic domain (ReSQ).This is expected, as we concluded in the results of the realistic domain in Table 2, that LLMs are better at commonsense reasoning than complex multi-hop spatial reasoning.The hypothesis is further supported when assessing the proposed method on StepGame.As can be observed in Table 3, the finetuning method consistently demonstrates significant positive differences in all reasoning steps compared to LLMs.The struggle of GPT3 on reasoning  over StepGame is also investigated in (Yang et al., 2023).Our proposed method consistently improves by 1-4% on a higher number of reasoning hops (k=6 to k=10), similar to the observation results of ReSQ.These results confirm our primary hypothesis that our proposed method equips the models with a higher level of logical abstraction to conduct more complex logical reasoning.We provide three examples of responses from this dataset to qualitatively demonstrate the improvements in Figure 4. We also included results from GPT-3(d2) + ASP and PistaQ*, both of which demonstrate significant effectiveness in multi-hop reasoning.These models perform relation extraction to obtain a formal logical representation using LLMs, while reasoning is handled separately using external tools with engineered rules of reasoning.However, these pipelines face two key limitations.First, they require formal representations at test time, effective only on synthetic data.Second, they do not fine-tune the language models for better reasoning, relying instead on external tools.This approach contrasts with ours, which focuses on directly improving the language models' reasoning capabilities.</p>
<p>In-context Learning.This section focuses only on comparing the variations of in-context learning for spatial reasoning.As shown in Table 2 and Table 4, CoT proved to be the most effective approach for spatial reasoning in both GPT and Llama.We also evaluated the impact of integrating formal representations in in-context learning, referred to as the LR approach.As evident in Table 4 (line 4), using formal representations marginally degraded performance.In most cases, utilizing natural language in the CoT proved to be more effective.Additionally, we evaluate CoS symbolic representations, finding that CoT and LR outperformed CoS using the same examples.This highlights the advantage of natural language reasoning over symbolic reasoning for in-context learning.We further applied step-bystep prompting explained in Section 3.4 to evaluate more sophisticated prompting techniques on LLMs.Table 4 shows that this technique achieves 2-5% better results than zero-shot, few-shot, and CoT using raw context.The results demonstrate that more straightforward and shorter contexts improve LLMs' spatial understanding.This is consistent with previous results on other domains and tasks (Li et al., 2023;Liu et al., 2024).Lastly, we compare open-source Llama and closed-source GPT models.Our results indicate that Llama-3 is competitive with GPT-3.5, showing only minor differences in the YN domain as shown in Table 2. Thus, we plan to prioritize open-source models in future research.</p>
<p>Conclusion</p>
<p>Given the importance of spatial reasoning in many real-world applications, we focus on improving this reasoning skill in language models.We equip LLMs with neuro-symbolic supervision to leverage logical knowledge during fine-tuning.This practice boosts their capability of capturing the necessary level of abstraction for spatial reasoning and enhances the generalizability for transferring knowledge across domains.Our constraint-based training technique achieves higher accuracy than other competitive Spatial Question-answering baselines across various benchmarks.Furthermore, the results indicate that our model performs better in scenarios requiring more reasoning steps.Lastly, we compare our models with state-of-the-art LLMs.Experimental comparisons show that while large LLMs like GPT3.5 perform better in commonsense reasoning, we achieve higher performance in multihop spatial question-answering with tuning comparatively smaller language models like BERT with spatial logic.</p>
<p>Limitations</p>
<p>While we improve the reasoning capability of the models, our approach by no means solves the general reasoning problem.The trustworthiness and reliability of the LLMs are still a research challenge.Our models will need GPU resources to run, which might be expensive.Our research is based on open-source data and code, and the results will be reproducible.We do not see any ethical con-cerns in our research approach or the impact of our results.Our data, being limited to the spatial reasoning domain, does not include any specific type of bias that can harm minority people belonging to any particular gender or race.user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching block B's left edge and below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B? assistant : Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Aove block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Yes user : There is a block called A. Inside A, a black triangle above and to the left of a black circle.There is another block called B which is to the right of A. B has two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and above the other yellow circle, Which also touches the right edge of B. Does block B have all of the circles inside it?assistant : No user : There is a block called A. A has a large blue square near and below a medium blue triangle.</p>
<p>Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A?</p>
<p>assistant : No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A, and below that, a red oval is touching the right edge of A. There is a second block called B, to the left of A. Inside B are 3 squares: a brown, a green, and a red.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?</p>
<p>C.2 Chain of Thought system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No] user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A.</p>
<p>A medium green square is touching block B's left edge and below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B? assistant : From information provided:The medium red square is above a small green square in B.Therefore, the small green square is below the medium red square in B. The medium red triangle is below the small square in B, and the small green square is below the medium red square in B.Therefore, the medium red triangle is below the medium red square in B. Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Aove block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : From given information: A medium white triangle is below a small green rectangle in C.Then, small green rectangle is above medium white triangle in C.A small red rectangle in block A, a medium white triangle in C, and block C is below block A.Then, a medium white triangle is below a A small red rectangle.A medium white triangle is below a A small red rectangle.Therefore, a small red rectangle is above a medium white triangle.A medium white rectangle is in A,a medium white triangle in C, and block C is below block A.Then, a medium white triangle is below a A medium white rectangle.A medium white triangle is below a A medium white rectangle.Therefore, a medium white rectangle is above a medium white triangle.A medium white rectangle is above a medium white triangle.Thus, all rectangles from given story are above a white triangle.Answer: Yes' user : There is a block called A. Inside A, a black triangle above and to the left of a black circle.There is another block called B which is to the right of A. B has two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.</p>
<p>The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : From given relation, A black triangle is above a black circle in A.Then, there is black circle in A.Therefore, not all circle are in B. Answer: No user : There is a block called A. A has a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Based on the provided content.Block B is to the right if block A.Block C is to the right of block B.Therefore, block C is to the right of block A.In block C, the yellow triangle is near and to the left of large blue triangle.Therefore, there is large blue triangle in C.Block A has a large blue square near and below a medium blue triangle.Therefore, there is large blue square in A.Then, large blue triangle in C is to the right of large blue square in A.So, large blue triangle in C is not to the left of large blue square in A. Answer: No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A, and below that, a red oval is touching the right edge of A. There is a second block call B Which is to the left of A. Inside B there are 3 squares, a brown one, a green one, and a red one.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?</p>
<p>C.3 Logical Representation (LR)</p>
<p>system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No] user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B?</p>
<p>assistant : Below(Medium red triangle in B, red square in block B) Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Above block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Below(C, A) =&gt; Above(A, C), Contain(A, Small red rectangle) + Above(A, C) + Contain(C, Medium white triangle)=&gt;Above(Small red rectangle, Medium White triangle), Above(A, C) &amp; Contain(A, Medium white rectangle) &amp; Contain(C, Medium white triangle)=&gt;Above(Medium white rectangle, Medium White triangle), Below(Medium white triangle in C, small green rectangle in C) =&gt;Above(small green rectangle, Medium white triangle)Answer: Yes user : There is a block called A. Inside A, there is a black triangle that is above and to the left of a black circle.There is another block called B which is to the right of A. In B, there are two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : Contain(A, Black Circle) =&gt; Â¬Contain(B, Black Circle), Â¬Contain(B, Black Circle) =&gt; Some x, Â¬Contain(B, x) where x is Circle.Answer: No user : There is a block called A. In A, there is a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B?</p>
<p>assistant : Let analyze the relations given from context to answer question.B: 1. (medium, red, triangle) â, &lt; (small, red, square)Based on the information, B: 1. (medium, red, triangle) â, &lt; (small, red, square)2.(medium, red, triangle) â (small, red, square)Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Above block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Let consider the related relations to answer the questions, A: 1. (small, red, rectangle) 2. (medium, white, rectangle) C: 1. (small, green, rectangle) = edge 2. (medium, white, triangle) ) â (small, green, rectangle) C â A Based on the information, let determined the relation of all rectangle and white triangles, 1. C â A. Therefore, A â C 2. A â C, A: (small, red, rectangle), and C: (medium, white, triangle).Therefore, A: (small, red, rectangle) â C: (medium, white, triangle) 3. A â C, A: (medium, white, rectangle), and C: (medium, white, triangle).Therefore, A: (medium, white, rectangle) â C: (medium, white, triangle).4. C: (medium, white, triangle) ) â (small, green, rectangle).Therefore, (small, green, rectangle) â (medium, white, triangle).5.A: (small, red, rectangle) â C: (medium, white, triangle), A: (medium, white, rectangle) â C: (medium, white, triangle), and C: (small, green, rectangle) â (medium, white, triangle), therefore, all rectangle above a white triangle.Answer: Yes user : There is a block called A. Inside A, there is a black triangle that is above and to the left of a black circle.There is another block called B which is to the right of A. In B, there are two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : Let consider the related relations to answer the questions,A: 1. (black, triangle) â &lt; (black, circle)B: 1. (yellow, circle1) right (blue, circle)2.(blue, square) â (yellow, circle2) 3.(blue, circle) =left edge of B Based on the information, let determined the relation of all rectangle and white triangles, 1. A: 1. (black, triangle) â&lt; (black, circle).Therefore, A: (black, circle)2.A: (black, circle).Therefore, B doesn't have all circle.Answer: No user : There is a block called A. In A, there is a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Let consider the related relations to answer the questions,B &gt; A,C &gt; BA: 1. (large, blue square) â¼ â (medium, blue square)C: 1. (large, yellow, triangle)Based on the information, let determined the relation large blue things in C and the large blue thing in A,1.C &gt; B and B &gt; A. Therefore, C &gt; A2.A: 1. (large, blue, square) â¼â (medium, blue square).Therefore, A: (large, blue, square)3.C &gt; A, A: (large, blue, square), and C: (large, yellow, triangle).Therefore, C: (large, yellow, triangle) &gt; A: (large, blue, square).4.C: (large, yellow, triangle) &gt; A: (large, blue, square).Therefore, C: (large, yellow, triangle) !&lt; A: (large, blue, square)large blue things in C is not to the left of the large blue thing in A. Answer: No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A and below that there is a red oval is touching the right edge of A.</p>
<p>Rule Type</p>
<p>Rule</p>
<p>Logical Constraints above(x, y) â below(y, x) below(x, y) â above(y, x) lef t(x, y) â right(y, x) right(x, y) â lef t(y, x) f ront(x, y) â behind(y, x) Converse behind(x, y) â f ront(y, x) T ruth(q i ) â T ruth(q j ) coveredby(x, y) â cover(y, x) cover(x, y) â coveredby(y, x) inside(x, y) â contain(y, x) contain(x, y) â inside(y, x) near(x, y) â near(y, x) f ar(x, y) â f ar(y, x) Symmetric touch(x, y) â touch(y, x) T ruth(q i ) â T ruth(q j ) disconnected(x, y) â disconnected(y, x) overlap(x, y) â overlap(y, x) lef t(x, y) â§ lef t(y, z) â lef t(x, z) right(x, y) â§ right(y, z) â right(x, z) above(x, y) â§ above(y, z) â above(x, z) below(x, y) â§ below(y, z) â below(x, z) behind(x, y) â§ behind(y, z) â behind(x, z) f ront(x, y) â§ f ront(y, z) â f ront(x, z) inside(x, y) â§ inside(y, z) â inside(x, z) contain(x, y) â§ contain(y, z) â contain(x, z) inside(x, y) â§ coveredby(y, z) â inside(x, z) contain(x, y) â§ cover(y, z) â contain(x, z) inside(x, y) â§ lef t(y, z) â lef t(x, z) inside(x, y) â§ right(y, z) â right(x, z) inside(x, y) â§ above(y, z) â above(x, z) inside(x, y) â§ below(y, z) â below(x, z) Transitivity inside(x, y) â§ behind(y, z) â behind(x, z) T ruth(q i ) â§ T ruth(q j ) â T ruth(q k ) inside(x, y) â§ f ront(y, z) â f ront(x, z) inside(x, y) â§ near(y, z) â near(x, z) inside(x, y) â§ f ar(y, z) â f ar(x, z) inside(x, y) â§ disconnected(y, z) â disconnected(x, z) coveredby(x, y) â§ lef t(y, z) â lef t(x, z) coveredby(x, y) â§ right(y, z) â right(x, z) coveredby(x, y) â§ above(y, z) â above(x, z) coveredby(x, y) â§ below(y, z) â below(x, z) coveredby(x, y) â§ behind(y, z) â behind(x, z) coveredby(x, y) â§ f ront(y, z) â f ront(x, z) coveredby(x, y) â§ near(y, z) â near(x, z) coveredby(x, y) â§ f ar(y, z) â f ar(x, z) coveredby(x, y) â§ disconnected(y, z) â disconnected(x, z) Table 7: The conversion from spatial logical rules proposed in (Mirzaee and Kordjamshidi, 2022) to logical constraints used in our experiment.</p>
<p>E DomiKnowS Concepts and Graphs</p>
<p>Graph and concepts declaration of our work inside the DomiKnowS framework.</p>
<p>StepGame Context: S is positioned below C. The object B is upper and slightly to the left of the object O. U is below S and to the right of S. V and C are next to each other with C at the bottom V on the top.A and U are parallel, and A on the right of U. FR Question: What is the relation of the agent S to the agent C? Answer: below</p>
<p>Figure 1 :
1
Figure 1: Examples of story and questions in Spatial Question Answering (SQA).YN is Yes-No, and FR is Find relation type of questions.</p>
<p>Figure 2 :
2
Figure 2: The pipeline for exploiting spatial logic in the training phase involves three steps: 1. Creating the chain of reasoning (Q-Chain) for each training example using annotated facts and rules from the training dataset.2.Obtaining consistency constraints from spatial logical rules and answers inferred from questions generated using intermediate facts in Q-Chain.3. Incorporating obtained consistency constraints into the loss function using differentiable soft logic during tuning.For more details, refer to Section 3.3.</p>
<p>Figure 3 :
3
Figure 3: Three examples of each category in the ResQ dataset with responses from Llama3 (Few-shot) and Flan-T5 + Q-Chain (Ours).</p>
<p>Figure 4 :
4
Figure 4: Three examples of each category in the StepGame dataset with responses from Llama3 (Fewshot) and BERT-T + Q-Chain (Our).</p>
<p>The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Question: Left(large blue thing in C, large blue thing in A).Related relation from story: Right(B, A), Right(C, B), Contain(C, large blue triangle), Contain(A, large blue square) Steps: Right(C, B) + Right(B, A) =&gt; Right(C, A) Contain(C, large blue triangle) + Right(C, A) + Contain(A, large blue square) =&gt; Right(large blue triangle in C, large blue square in A)Right(large blue triangle in C, large blue square in A) =&gt; !Left(large blue thing in C, large blue thing in A) Answer: No C.4 Chain of Symbol system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No]</p>
<p>E. 1
1
Graph for YN story = Concept ( name =" story ") question = Concept ( name =" question ") story_contain , = story .contains ( question ) answer_class = question ( name =" answer_class " , ConceptClass = EnumConcept , values =[ " yes " , " no " ]) symmetric = Concept ( name =" symmetric ") s_quest1 , s_quest2 = symmetric .has_a ( arg1 = question , arg2 = question ) ifL ( andL ( answer_class .yes ( 'x ') , existsL ( symmetric ( 's ' , path =( 'x ' , symmetric ) ))) , answer_class .yes ( path =( 's ' , s_quest2 ))) reverse = Concept ( name =" reverse ")</p>
<p>for converting spatial expressions from natural language into symbolic representation.An example of the CoS counterpart of CoT expression is shown in Table1.</p>
<p>DomainSpatial context CoT large red square is to the left of a small green square LR Left(large red square, small green square) CoS (large, red, square) &lt; (small, green, square) Table 1: An example of a spatial relation used in Chain of Thought (CoT), Logical Representation (LR), and Chain of Symbol (CoS).</p>
<p>(Mirzaee and Kordjamshidi, 2022ht edge of the block." the expected context generated by the LLM would be "The big red square in block A.SpaRTUN(Mirzaee and Kordjamshidi, 2022) is a synthetic SQA dataset.This dataset contains both YN and FR types of questions that require multi-hop reasoning to answer.It covers a wide range of spatial relations.2Theanswer for YN is A = {Y es, N o}, while the answer for the FR is a subset of 15 spatial relations, e.g., {lef t, behind}, and {above, inside}.The dataset provides the chain of reasoning annotations that we use to create the Q-Chains for our proposed training method.
This dataset is used only for training because it issynthetic and the baselines performance reportedin (Mirzaee and Kordjamshidi, 2022) on its testsplit is already high.SpartQA-Human (Ver.1) (Mirzaee et al., 2021) isa small human-annotated SQA dataset. The datasetcontains both YN and FR types of questions. Weonly use the YN question types for fine-tuning andevaluation. The YN questions require more rea-soning steps and are shown to be more challengingbased on previous research results in<newline> The big red square is touching the rightedge of block A...". More examples are provided inAppendix C. We refer to this prompting techniqueas Step-by-Step, which can be applied before anyICL method.4 Experiments4.1 DatasetsOur experiments are conducted on all existingdatasets, including SpaRTUN, SpartQA-Human,ReSQ, and StepGame.</p>
<p>Table 2 :
2
Mirzaee and Kordjamshidi (2023)eSQ with various models.For ReSQ, k is the number of the reasoning steps required to answer the questions.Unclassified indicates the cases in which k was challenging for human annotators to determine.The results from PistaQ<em> reported fromMirzaee and Kordjamshidi (2023).
SpartQA-HumanReSQModelVer.1Ver.2k=1k=2unclassifiedAll#BERT54.5453.5770.67 56.8560.6660.98 1BERT-T55.9458.0376.00 54.7961.1861.15 2BERT-T+Q-Chain (Our)59.44 58.9272.00 58.9059.9061.31 3Flan-T554.5460.7174.67 56.1661.4461.80 4Flan-T5-T49.6557.1481.33 54.7961.4462.30 5Flan-T5-T+Q-Chain (Our) 55.94 61.61 81.33 57.5363.7564.43 6GPT3.5 (zero-shot)58.0458.0374.67 60.9566.5866.22 7GPT3.5 (few-shot)62.2358.9284.00 68.4968.1270.16 8GPT3.5 (LR)64.3359.82----9GPT3.5 (CoS)60.1464.28----10GPT3.5 (CoT)65.7371.4386.67 67.1268.6470.49 11GPT-4 (zero-shot)77.6268.7584.00 73.9776.8677.05 12Llama-3 (zero-shot)61.5450.8980.00 64.3867.3568.20 13Llama-3 (few-shot)62.9460.7182.67 69.8671.4672.46 14Llama-3 (LR)60.1472.32----15Llama-3 (CoS)67.1361.60----16Llama-3 (CoT)67.8370.5482.76 76.0367.1071.15 17PistaQ</em>75.52----47.21 18Modelk=1k=2k=3k=4k=5k=6k=7k=8k=9k=10BERT98.51 95.53 91.68 66.71 49.11 41.47 41.47 32.09 28.94 28.16BERT-T98.50 95.32 93.26 76.78 66.36 58.76 53.70 46.27 42.71 40.12BERT-T+Q-Chain (Our) 98.70 96.45 93.03 74.58 64.95 59.04 54.38 49.23 45.36 44.05Llama-3 (few-shot)38.01 27.87 24.15 21.27 19.75 18.03 16.88 15.52 15.17 14.70GPT3 (few-shot)55.00 37.00 25.00 30.00 32.00 29.00 21.00 22.00 34.00 31.00GPT3 (CoT)61.00 45.00 30.00 35.00 35.00 27.00 22.00 24.00 23.00 25.00GPT3(d2) + ASP92.60 89.90 89.10 93.80 92.90 91.60 91.20 90.40 89.00 88.30</p>
<p>Table 3 :
3
Yang et al. 2023.ame on several models including results of GPT3 reported fromYang et al. 2023.
ModelRaw Context Step by stepGPT3.5 (zero-shot)58.0463.64GPT3.5 (few-shot)62.2364.33GPT3.5 (CoT)65.7367.83GPT3.5 (LR)64.3359.44GPT3.5 (CoS)60.1458.74GPT-4 (zero-shot)77.6278.32</p>
<p>Table 4 :
4
The accuracy of LLMs on SpartQA-Human.
suggests that the Q-chain can facilitate complexreasoning, particularly when applied to a modelwith stronger commonsense capabilities. Note thatapplying LR and CoS is not straightforward onReSQ due to the challenges associated with ex-tracting logical representations from realistic do-mains text. This challenge is evident in the resultsreported for PistaQ<em>, which employs a similar ap-proach. PistaQ</em> achieved only 47.21% accuracy onReSQ, despite significant improvements observedin synthetic domains such as SpartQA-Human.Synthetic Domain with More Complex LogicalReasoning. SpartQA-Human and StepGame aresynthetic domains used in our experiments. Ac-
cording to Table2, we consistently observe improvements with our proposed Q-chain in the synthetic domain which typically requires considerable reasoning hops.Moreover, the gap between small</p>
<p>K is at the bottom of T. V is to the right of G horizontally.W and L are next to each other with W on the left and L on the right.Z is positioned below and to the right of X. T is positioned in the lower right corner of O. X and N are parallel, and X is under N. G is diagonally to the bottom left of M. V is below and to the left of O. N is on the lower right of M. T is diagonally to the bottom left of I. H and I are parallel, and H on the left of I. L is at the bottom of I. K is on the top of Z and is on the same vertical plane.U and B are vertical and U is below B.
StepGame: k = 1 Context: T is below C. H and M are vertical and H is above M. J is below and to the left of H. Question: What is the relation of the agent M to the agent H? Label: Below Llama3 (few-shot): Below(â) BERT-T+Q-Chain (Our): Below(â) StepGame: k = 4 Context: R is at the bottom and A is on the top. R is below E at 4 o'clock. W is positioned right to M. M and R are parallel, and M is under R. S is on the left side and above W. Z is above J. Question: What is the relation of the agent M to the agent W? Label: Left Llama3 (few-shot): Right(â³) BERT-T+Q-Chain (Our): Left(â)StepGame: k = 10 Context: Question: What is the relation of the agent N to the agent X? Label: Above Llama3 (few-shot): Below(â³) BERT-T+Q-Chain (Our): Above(â)</p>
<p>Table 6 :
6
Accuracy of SpartQA-Human and ReSQ with BERT and RoBERTa for comparison between these models.
Lianhui Qin, Sean Welleck, Daniel Khashabi, and YejinChoi. 2022. Cold decoding: Energy-based con-strained text generation with langevin dynamics. InAdvances in Neural Information Processing Systems,volume 35, pages 9538-9551. Curran Associates,Inc.Hossein Rajaby Faghihi, Quan Guo, Andrzej Uszok, Ali-akbar Nafar, and Parisa Kordjamshidi. 2021. Domi-
KnowS: A library for integration of symbolic domain knowledge in deep learning.In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 231-241, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics.</p>
<p>The code, trained models, and data are publicly available at https://github.com/HLR/SpaRTUNQChain
Related Work
The dataset also includes Don't Know questions, but for simplicity, we apply the closed-world assumption and label those questions as No.
AcknowledgmentThis project is partially supported by the Office of Naval Research (ONR) grant N00014-23-1-2417.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Office of Naval Research.We thank anonymous reviewers for their constructive feedback, which greatly helped us improve this manuscript.A Data StatisticThe data statistic of each dataset is shown in Table 5.DatasetTrainB RoBERTa resultsWe provide results from the same setting as the BERT using RoBERTa in Table6and found that the overall results are slightly lower than BERT for typical transfer learning.So, we decided to use only BERT for the remaining experiments.C In-context Learning ExamplesThe example of each in-context learning prompt and example is indicated below.C.1 Few-Shotsystem : You will be given a story and question.Then, you will answer either only Yes or No based on given story.Candidate answer:[Yes, No]There is a second block call B Which is to the left of A. Inside B there are 3 squares, a brown one, a green one, and a red one.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?C.5 Step-by-Step PromptingRaw context : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Spliting context : A big red square in block A.A big red triangle in block A.A medium red square in block A.A small green circle in block A.The triangle is touching the bottom edge of block A.The medium square is touching the bottom edge of block A.The big square is touching the right edge of block A.The medium square is touching the right edge of block A.The circle is above the big square in A.The big square is above the medium square in A.Block B is to the left of block A.A medium green square is touching the left edge of block B.A medium green square is below a medium red square in B.The medium red square is above a small green square in B.The medium red square is left of a small green square in B.The medium red triangle is below the small square in B.The medium red triangle is left the small square in B.D Spatial Logical RulesThe conversion from spatial logical rules proposed in(Mirzaee and Kordjamshidi, 2022)to logical constraints used in our experiment is shown in Table 7.r_quest1 , r_quest2 = reverse .has_a ( arg10 = question , arg20 = question ) ifL ( andL ( answer_class .yes ( 'x ') , existsL ( reverse ( 'r ' , path =( 'x ' , reverse )))) , answer_class .no ( path =( 'r ', r_quest2 ) ) ) ifL ( andL ( answer_class .no ( 'x ') , existsL ( reverse ( 'r ' , path =( 'x ' , reverse )))) , answer_class .yes ( path =( 'r ' , r_quest2) for rel in transitive_1 : ifL ( andL ( rel ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ) ) ) , rel ( path =( 't ' , tran_quest2 ))) , rel ( path =( 't ' , tran_quest3 ))) # Transitive of cover and contain transitive_2 = [( coveredby , inside ) , ( cover , contain )] for rel1 , rel2 in transitive_2 : ifL ( andL ( rel2 ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ) ) ) , rel1 ( path =( 't ', tran_quest2))) , rel2 ( path =( 't ' , tran_quest3 ))) # Transitive of inside / cover with position transitive_3_1 = [ inside , coveredby ] transitive_3_2 =[ left , right , above , below , behind , front , near , far ,disconnected ] for rel1 in transitive_3_1 :for rel2 in transitive_3_2 : ifL ( andL ( rel1 ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ))) , rel2 ( path =( 't ' , tran_quest2 ))) , rel2 ( path =( 't ' , tran_quest3 )))#
A I , Meta , Llama 3 model card. 2024</p>
<p>Neurosymbolic visual reasoning: Disentangling "Visual" from "Reasoning. Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, Kazuhito Koishida, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020119</p>
<p>Logicguided data augmentation and regularization for consistent question answering. Akari Asai, Hannaneh Hajishirzi, 10.18653/v1/2020.acl-main.499Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>From generic knowledge to specific reasoning for medical image interpretation using graph based representations. Jamal Atif, CÃ©line Hudelot, Geoffroy Fouquier, Isabelle Bloch, Elsa D Angelini, IJCAI. 2007</p>
<p>The primal-dual method for learning augmented algorithms. Etienne Bamas, Andreas Maggiori, Ola Svensson, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.18653/v1/2023.ijcnlp-main.45Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua20231Association for Computational Linguistics</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Yu, Journal of Machine Learning Research. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei25702024</p>
<p>Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest x-ray reports using deep learning. Surabhi Datta, Yuqi Si, Laritza Rodriguez, Sonya E Shooshan, Dina Demner-Fushman, Kirk Roberts, 10.1016/j.jbi.2020.103473Journal of Biomedical Informatics. 1081034732020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Naturally supervised 3d visual grounding with language-regularized concept learners. Chun Feng, Joy Hsu, Weiyu Liu, Jiajun Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Pheng-Ann Heng, and Qi Dou. 2024. 3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable tumor segmentation. GaÃ«l Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie ; Yuan, Wenao Zhong, Jinpeng Ma, Zhao Li, Jingyang Wang, Zhang, 10.1016/j.media.2024.103324Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24202498103324Medical Image Analysis</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Chain-of-symbol prompting for spatial reasoning in large language models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, First Conference on Language Modeling. 2024</p>
<p>Nesycoco: A neuro-symbolic concept composer for compositional generalization. Danial Kamali, J Elham, Parisa Barezi, Kordjamshidi, arXiv:2412.155882024Preprint</p>
<p>Multimodal spatial role labeling (msprl) task overview. Parisa Kordjamshidi, Taher Rahgooy, Marie-Francine Moens, James Pustejovsky, Umar Manzoor, Kirk Roberts, Experimental IR Meets Multilinguality, Multimodality, and Interaction. ChamSpringer International Publishing2017. 2017</p>
<p>Advancing spatial reasoning in large language models: An in-depth evaluation and enhancement using the stepgame benchmark. Fangjun Li, David C Hogg, Anthony G Cohn, 10.1609/aaai.v38i17.29811Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A logic-driven framework for consistency of neural models. Tao Li, Vivek Gupta, Maitrey Mehta, Vivek, 10.18653/v1/D19-1405Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsSrikumar. 2019</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin, 10.18653/v1/2023.emnlp-main.391Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>NeuroLogic a*esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Le Ronan, Lianhui Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 10.18653/v1/2022.naacl-main.57Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, 10.18653/v1/2022.emnlp-main.413Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Disentangling extraction and reasoning in multi-hop spatial reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, 10.18653/v1/2023.findings-emnlp.221Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>SPARTQA: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjamshidi, 10.18653/v1/2021.naacl-main.364Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Chapter 13 -the teaching of thinking and problem solving. Raymond S Nickerson, 10.1016/B978-0-08-057299-4.50019-0Thinking and Problem Solving. Robert J Sternberg, San DiegoAcademic Press1994of Handbook of Perception and Cognition</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Question answering using constraint satisfaction: QA-by-dossier-with-contraints. John Prager, Jennifer Chu-Carroll, Krzysztof Czuba, 10.3115/1218955.1219028Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, Spain2004</p>
<p>Gluecons: A generic benchmark for learning under constraints. Aliakbar Hossein Rajaby Faghihi, Chen Nafar, Roshanak Zheng, Yue Mirzaee, Andrzej Zhang, Alexander Uszok, Tanawan Wan, Dan Premsri, Parisa Roth, Kordjamshidi, 10.1609/aaai.v37i8.26143Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Exploring and improving the spatial reasoning abilities of large language models. Manasi Sharma, 2023</p>
<p>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, 10.1609/aaai.v36i10.21383Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Spatial reasoning for human robot interaction. Luis F Emrah Akin Sisbot, Rachid Marin, Alami, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2007</p>
<p>Reasoning over logically interacted conditions for question answering. Haitian Sun, William W Cohen, Ruslan Salakhutdinov, arXiv:2205.128982022Preprint</p>
<p>Towards benchmarking and improving the temporal reasoning capability of large language models. Qingyu Tan, Hwee Tou Ng, Lidong Bing, 10.18653/v1/2023.acl-long.828Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van MerriÃ«nboer, Armand Joulin, Tomas Mikolov, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2016</p>
<p>Learning spatial reasoning in virtual vs. physical games with robots. Elmira Yadollahi, Miguel Alexandre Monteiro, Ana Paiva, 10.1145/3623809.3623830Proceedings of the 11th International Conference on Human-Agent Interaction, HAI '23. the 11th International Conference on Human-Agent Interaction, HAI '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Zhun Yang, Adam Ishay, Joohyung Lee, 10.18653/v1/2023.findings-acl.321Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Towards navigation by reasoning over spatial configurations. Yue Zhang, Quan Guo, Parisa Kordjamshidi, 10.18653/v1/2021.splurobonlp-1.5Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics. Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for RoboticsOnline. Association for Computational Linguistics2021</p>
<p>LOViS: Learning orientation and visual signals for vision and language navigation. Yue Zhang, Parisa Kordjamshidi, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Ming Zhou, Nan Duan, Shujie Liu, Heung-Yeung Shum, 10.1016/j.eng.2019.12.014Progress in neural nlp: Modeling, learning, and reasoning. Engineering. 20206</p>            </div>
        </div>

    </div>
</body>
</html>
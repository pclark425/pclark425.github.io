<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8662 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8662</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8662</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-cac8c4f79077c74aa059d1c58c021be1c89f1178</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cac8c4f79077c74aa059d1c58c021be1c89f1178" target="_blank">A review of large language models and autonomous agents in chemistry</a></p>
                <p><strong>Paper Venue:</strong> Chemical Science</p>
                <p><strong>Paper TL;DR:</strong> This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8662.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8662.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only, GPT-style causal transformer trained on molecular SMILES for de novo molecule generation; emphasizes chemically valid SMILES and interpretability via token salience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6 M</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES and GuacaMol datasets (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery / general molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive SMILES generation (masked self-attention), direct generation from learned SMILES distribution</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported to generate novel molecules with strong validity and uniqueness metrics; specific percentages not provided in review but stated to outperform many VAE-based approaches on generative quality metrics</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not inherently target-conditioned; generates molecules from learned distribution (general-purpose molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, uniqueness, Frechet ChemNet Distance (FCD), KL divergence, novelty metrics commonly used in MOSES/GuacaMol benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Outperformed many VAE-based approaches on standard generative metrics (validity/uniqueness/FCD/KL) for SMILES-based generation; produced high-quality novel molecules in benchmark evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reported superior performance vs several VAE-based generative models; compared using MOSES/GuacaMol benchmarks and generative metrics (FCD, KL divergence).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Higher computational demands relative to some VAEs (though authors argue quality justifies cost); SMILES-based generation can struggle with stereochemistry/chirality representation and constrained exploration of chemical space unless further conditioning applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8662.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cMolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>cMolGPT (conditional MolGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional autoregressive SMILES generator that conditions molecule generation on protein/target embeddings to produce target-specific ligands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cMolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only autoregressive transformer (GPT-style) with conditional embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on MOSES (no target info) then fine-tuned with protein-binder pair embeddings from datasets for EGFR, HTR1A, S1PR1</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>target-specific drug discovery (protein–ligand generation for EGFR, HTR1A, S1PR1 examples)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>conditional SMILES generation guided by embeddings of protein-binder pairs; uses self-supervised pretraining plus fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates molecules mainly within the sub-chemical space represented in training data; tends to produce plausible binders but with limited broad exploration beyond dataset space</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioned on protein/target embeddings; evaluated for target-binding relevance using downstream QSAR models</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Activity prediction via QSAR (Pearson correlation reported), diversity and validity implicit via generative benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Successfully generated target-specific compound libraries and candidate binders; downstream QSAR achieved Pearson correlation > 0.75 for activity prediction of generated compounds, indicating reasonable predictive alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Demonstrates target-conditioning advantage over unconstrained generators; relies on QSAR for activity evaluation rather than experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Generation biased toward chemical subspaces present in original dataset (limited chemical-space exploration); strong reliance on QSAR models (which have their own limitations and can propagate error); limited experimental validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8662.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taiga</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taiga (autoregressive RL for molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive SMILES-based generative framework that first maps molecules to a vector space then refines generation for specific properties using reinforcement learning (REINFORCE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Taiga</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only autoregressive transformer with reinforcement learning (REINFORCE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on large unlabeled SMILES corpora (SMILES-to-vector mapping); then fine-tuned on smaller labeled datasets for property conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>property-optimized molecule generation (examples: drug-likeness QED, IC50 targeting, BACE inhibitors, anti-cancer activity)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>two-stage: (1) learn SMILES vector space autoregressively, (2) apply reinforcement learning (REINFORCE) with property-based rewards to bias generation toward desired properties</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates molecules optimized for target properties; reinforcement learning improves property scores though can slightly reduce validity—novelty metrics not numerically reported in review</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimizes for specific property metrics (e.g., QED, IC50 proxies) via RL reward shaping and fine-tuning on labeled property datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Quantitative Estimate of Drug-likeness (QED), targeted IC50 proxies, BACE benchmark performance, property improvement under RL; validity reported to decline slightly under property optimization</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Taiga improved property-optimization outcomes (higher QED, better IC50 proxies, promising BACE/anti-cancer results) at the cost of a modest decrease in generated molecule validity; showed utility in moving from unlabeled pretraining to property-directed design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Combines generative pretraining and RL, contrasting with conditional-embedding methods and purely supervised generative approaches; demonstrates property gains versus non-RL baselines but incurs some loss in validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Reinforcement learning tuning can reduce chemical validity; benchmarks like MoleculeNet have data quality issues (e.g., undefined stereocenters) complicating evaluation; RL reliance on proxy metrics rather than experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8662.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale GPT-like model trained on molecular data to explore effects of dataset scale and hyperparameters on generative performance and domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only GPT-style transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~1 B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>~10M molecules from PubChem (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule generation and exploration of scaling/hyperparameter effects for chemical generative models</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive SMILES generation; study focuses on scaling, hyperparameters, and dataset effects</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Designed to produce diverse molecular candidates; specific novelty metrics not detailed in review, but work studies impact of scale on generative quality</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>General molecule generation; not primarily target-conditioned in the described work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Standard generative metrics (validity, uniqueness, distributional similarity) and analyses of how scaling and hyperparameters affect outputs</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Work emphasized how dataset scale and hyperparameter tuning affect generative performance in chemistry; used to refine understanding of data/scale tradeoffs for molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positions large decoder models against smaller/generic models and demonstrates benefits of larger-scale pretraining for generative quality relative to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires substantial data and compute; chemical training corpora remain orders of magnitude smaller than general-language corpora, limiting absolute scale advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8662.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adilov2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative pretraining from Molecules (Adilov 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early GPT-2-like causal transformer pretrained on SMILES with adapters for parameter-efficient fine-tuning, enabling both molecule generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative pretraining from Molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-like causal transformer with adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT-2 style) with adapter modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13.4 M (reported training setup)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>~5M SMILES from PubChem-derived PubChem-10M</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule generation and property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>self-supervised causal language modeling on SMILES; adapter modules inserted for task-specific fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Enables novel molecule generation through learned SMILES distribution; quantitative novelty metrics not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Adapters permit task-specific tuning for downstream property prediction or targeted generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property prediction benchmarks and standard generative metrics (not detailed numerically in review)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrated that GPT-style pretraining on SMILES plus adapter-based fine-tuning is a flexible approach for molecule generation and property prediction with efficient parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with encoder-only models like ChemBERTa; argued for scalability and resource-efficiency in causal decoder approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Adapter/fine-tuning performance depends on quality and representativeness of downstream datasets; SMILES representation limitations persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8662.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iupacGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iupacGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only model that generates and manipulates molecules using IUPAC names rather than SMILES, aiming for more human-readable and semantically rich generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>iupacGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT-style) trained on IUPAC name sequences</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on a large dataset of IUPAC names and fine-tuning with lightweight networks (exact corpus size not specified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>molecule generation, classification, and regression with outputs in IUPAC name format</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive generation of IUPAC names conditioned on tasks or prompts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates chemically valid outputs represented as IUPAC names; novelty metrics not specified but claimed to excel in generation and downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>IUPAC outputs improve human interpretability and ease integration into workflows; not explicitly described as target-conditioned</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-specific generation, classification, and regression performance; numerical metrics not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrated that using IUPAC names as a generation modality can produce useful, interpretable molecular outputs enabling generation, classification, and regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Represents an alternative to SMILES-based generation, with potential advantages in human-readability and ease of validation; empirical comparisons not deeply quantified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>IUPAC name complexity and variability can be challenging; mapping between IUPAC strings and structures requires robust parsers and may hide stereochemical detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8662.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that leverages large pre-trained generalist LLMs (e.g., Galactica, LLaMA, Mistral) and parameter-efficient fine-tuning (PEFT/LoRA) for molecular tasks, achieving state-of-the-art property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (using Galactica, LLaMA, Mistral + PEFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language models with parameter-efficient fine-tuning (PEFT/LoRa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~7 B (examples of base models used)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMolInstruct and other domain-specific instruction datasets; fine-tuning on MoleculeNet benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>property prediction, molecule generation, conversion and captioning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>parameter-efficient fine-tuning of large pre-trained LLMs to perform molecular generation/prediction tasks via instruction-style prompting and PEFT</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Primary focus on property prediction and transformation tasks; generation novelty not quantified in review but reported high predictive performance</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Fine-tuning on molecular benchmarks (MoleculeNet) yields task-specific performance; PEFT enables domain adaptation with fewer parameters</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MoleculeNet benchmark metrics for property prediction; reported state-of-the-art performance on those benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LlaSMol achieved state-of-the-art property prediction performance on MoleculeNet when using PEFT approaches on large pre-trained LLMs, demonstrating effective domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Shows that adapting large generalist LLMs via PEFT can match or exceed specialized mol-LLMs on property-prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance dependent on quality and relevance of fine-tuning datasets; large base models still require substantial compute for base pretraining even if PEFT reduces fine-tuning cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8662.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemSpaceAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemSpaceAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning framework for protein-specific molecular generation that iteratively queries and refines models to efficiently find molecules with desired target properties without prior inhibitor knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemSpaceAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only GPT-style generative model integrated with active learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ChEMBL 33, GuacaMol v1, MOSES, BindingDB (as reported in review's table)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>protein-specific drug discovery (targeted ligand generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>active learning-driven generation: propose molecules, evaluate (in silico), update model/selection strategy to focus on promising areas</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Efficiently identifies molecules with desired characteristics; novelty relative to training set not numerically specified but method emphasizes efficient discovery beyond initial dataset</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to identify protein-specific molecules without prior knowledge of inhibitors—focuses exploration toward target-relevant space via active selection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-specific success rates in finding target-characteristic molecules; in-review descriptions are qualitative rather than numerically specific</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported effective identification of molecules with desired target characteristics using active learning, improving efficiency of protein-specific generation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positions active learning as more efficient than unguided generation for target discovery, leveraging iterative evaluation to focus sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies on in-silico evaluation proxies for activity; success depends on evaluation fidelity (QSAR/simulations) and initial dataset coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8662.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoMolDesigner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoMolDesigner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source automation tool for small-molecule antibiotic design that integrates generative models and automated design workflows to accelerate candidate identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AutoMolDesigner</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>toolchain combining generative models and automation (details: generative + workflow automation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in review (open-source tool that integrates various data and models)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>antibiotic small-molecule design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>automated generation + selection workflows; integrates generative modeling with downstream evaluation to propose antibiotic candidates</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Intended to propose small-molecule antibiotic candidates; specifics on novelty metrics not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specifically aimed at antibiotic discovery, combining generation and property filters relevant to antibiotic activity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Tool-level demonstrations and design case studies (review does not list quantitative benchmarks in detail)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as an open-source example of applying generative and automation approaches to antibiotic design, illustrating automation's role in discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as an applied automation complement to standalone generative models, emphasizing end-to-end workflows rather than raw generative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Success depends on underlying evaluation/assay fidelity and dataset quality; details of experimental validation not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8662.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yoshikai2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer+VAE contrastive approach for chirality and novelty (Yoshikai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Addresses transformer limitations in recognizing chirality from SMILES by coupling transformers with a VAE and applying contrastive learning across multiple SMILES representations to improve novelty and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer + VAE with contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid transformer and variational autoencoder with contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not explicitly detailed in review; uses multiple SMILES representations per molecule for contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecule generation with improved stereochemical/chirality representation and enhanced novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>coupled transformer and VAE; contrastive learning across different SMILES for the same molecule to boost novelty/validity</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Contrastive learning increases molecular novelty and validity (qualitative claim in review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Aims to better capture stereochemistry and produce more chemically meaningful diverse molecules</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity and novelty improvements reported qualitatively; no specific numeric values provided in the review</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Combining transformer architectures with VAEs and contrastive learning mitigates chirality-recognition issues in SMILES-based generation and enhances novelty/validity of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Proposes hybrid approach to overcome pure-transformer weaknesses in stereochemistry handling; compared qualitatively against transformer-only generators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Complex hybrid training; SMILES-based chirality representation remains a core challenge; specifics on scalability and quantitative gains require consulting original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8662.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8662.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jablonka-GPT3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-only GPT fine-tuned for property prediction and conditional molecule generation (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An example where GPT-3 (a general natural-language decoder-only model) is fine-tuned to predict molecular properties and to conditionally generate molecules, demonstrating competitive performance with domain-specific mol-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned for chemical tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT-3) fine-tuned for chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175 B (GPT-3 base mentioned elsewhere in review context)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned using a curation of multiple classification and regression benchmarks and chemical datasets (review references curated benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>property prediction and inverse molecular design (conditional generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>fine-tuning a large general-purpose decoder model to perform conditional molecule generation and property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Demonstrated conditional generation capability; novelty statistics not provided in the review, but claim competitive performance versus domain-specific mol-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditionally generates molecules for desired properties using fine-tuned prompts or conditioning strategies</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property-prediction and generative benchmarks (benchmarks aggregated from multiple classification/regression tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned GPT-3 could predict properties and generate molecules conditionally, achieving competitive results with dedicated mol-LLMs in the review's cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Illustrates that large general-purpose LLMs (properly fine-tuned) can compete with specialized molecular LLMs like MolGPT/cMolGPT for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Dependence on curated benchmarks and fine-tuning data quality; mapping between natural-language-tuned decoders and strict chemical syntax (SMILES/IUPAC) requires care to avoid syntactic/chemical invalidity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A review of large language models and autonomous agents in chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative pretraining from Molecules <em>(Rating: 2)</em></li>
                <li>MolGPT <em>(Rating: 2)</em></li>
                <li>cMolGPT <em>(Rating: 2)</em></li>
                <li>Taiga <em>(Rating: 2)</em></li>
                <li>ChemGPT <em>(Rating: 2)</em></li>
                <li>AutoMolDesigner <em>(Rating: 2)</em></li>
                <li>iupacGPT <em>(Rating: 1)</em></li>
                <li>LlaSMol <em>(Rating: 1)</em></li>
                <li>ChemSpaceAL <em>(Rating: 1)</em></li>
                <li>Transformer+VAE contrastive approach for chirality and novelty <em>(Rating: 1)</em></li>
                <li>Decoder-only GPT fine-tuned for property prediction and conditional molecule generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8662",
    "paper_id": "paper-cac8c4f79077c74aa059d1c58c021be1c89f1178",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT",
            "brief_description": "A decoder-only, GPT-style causal transformer trained on molecular SMILES for de novo molecule generation; emphasizes chemically valid SMILES and interpretability via token salience.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "decoder-only transformer (GPT-style)",
            "model_size": "6 M",
            "training_data": "MOSES and GuacaMol datasets (SMILES)",
            "application_domain": "de novo molecular generation for drug discovery / general molecule design",
            "generation_method": "autoregressive SMILES generation (masked self-attention), direct generation from learned SMILES distribution",
            "novelty_of_chemicals": "Reported to generate novel molecules with strong validity and uniqueness metrics; specific percentages not provided in review but stated to outperform many VAE-based approaches on generative quality metrics",
            "application_specificity": "Not inherently target-conditioned; generates molecules from learned distribution (general-purpose molecular generation)",
            "evaluation_metrics": "Validity, uniqueness, Frechet ChemNet Distance (FCD), KL divergence, novelty metrics commonly used in MOSES/GuacaMol benchmarks",
            "results_summary": "Outperformed many VAE-based approaches on standard generative metrics (validity/uniqueness/FCD/KL) for SMILES-based generation; produced high-quality novel molecules in benchmark evaluations.",
            "comparison_to_other_methods": "Reported superior performance vs several VAE-based generative models; compared using MOSES/GuacaMol benchmarks and generative metrics (FCD, KL divergence).",
            "limitations_and_challenges": "Higher computational demands relative to some VAEs (though authors argue quality justifies cost); SMILES-based generation can struggle with stereochemistry/chirality representation and constrained exploration of chemical space unless further conditioning applied.",
            "uuid": "e8662.0",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "cMolGPT",
            "name_full": "cMolGPT (conditional MolGPT)",
            "brief_description": "A conditional autoregressive SMILES generator that conditions molecule generation on protein/target embeddings to produce target-specific ligands.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "cMolGPT",
            "model_type": "decoder-only autoregressive transformer (GPT-style) with conditional embeddings",
            "model_size": null,
            "training_data": "Pretrained on MOSES (no target info) then fine-tuned with protein-binder pair embeddings from datasets for EGFR, HTR1A, S1PR1",
            "application_domain": "target-specific drug discovery (protein–ligand generation for EGFR, HTR1A, S1PR1 examples)",
            "generation_method": "conditional SMILES generation guided by embeddings of protein-binder pairs; uses self-supervised pretraining plus fine-tuning",
            "novelty_of_chemicals": "Generates molecules mainly within the sub-chemical space represented in training data; tends to produce plausible binders but with limited broad exploration beyond dataset space",
            "application_specificity": "Conditioned on protein/target embeddings; evaluated for target-binding relevance using downstream QSAR models",
            "evaluation_metrics": "Activity prediction via QSAR (Pearson correlation reported), diversity and validity implicit via generative benchmarks",
            "results_summary": "Successfully generated target-specific compound libraries and candidate binders; downstream QSAR achieved Pearson correlation &gt; 0.75 for activity prediction of generated compounds, indicating reasonable predictive alignment.",
            "comparison_to_other_methods": "Demonstrates target-conditioning advantage over unconstrained generators; relies on QSAR for activity evaluation rather than experimental validation.",
            "limitations_and_challenges": "Generation biased toward chemical subspaces present in original dataset (limited chemical-space exploration); strong reliance on QSAR models (which have their own limitations and can propagate error); limited experimental validation reported.",
            "uuid": "e8662.1",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Taiga",
            "name_full": "Taiga (autoregressive RL for molecules)",
            "brief_description": "An autoregressive SMILES-based generative framework that first maps molecules to a vector space then refines generation for specific properties using reinforcement learning (REINFORCE).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Taiga",
            "model_type": "decoder-only autoregressive transformer with reinforcement learning (REINFORCE)",
            "model_size": null,
            "training_data": "Pretraining on large unlabeled SMILES corpora (SMILES-to-vector mapping); then fine-tuned on smaller labeled datasets for property conditioning",
            "application_domain": "property-optimized molecule generation (examples: drug-likeness QED, IC50 targeting, BACE inhibitors, anti-cancer activity)",
            "generation_method": "two-stage: (1) learn SMILES vector space autoregressively, (2) apply reinforcement learning (REINFORCE) with property-based rewards to bias generation toward desired properties",
            "novelty_of_chemicals": "Generates molecules optimized for target properties; reinforcement learning improves property scores though can slightly reduce validity—novelty metrics not numerically reported in review",
            "application_specificity": "Optimizes for specific property metrics (e.g., QED, IC50 proxies) via RL reward shaping and fine-tuning on labeled property datasets",
            "evaluation_metrics": "Quantitative Estimate of Drug-likeness (QED), targeted IC50 proxies, BACE benchmark performance, property improvement under RL; validity reported to decline slightly under property optimization",
            "results_summary": "Taiga improved property-optimization outcomes (higher QED, better IC50 proxies, promising BACE/anti-cancer results) at the cost of a modest decrease in generated molecule validity; showed utility in moving from unlabeled pretraining to property-directed design.",
            "comparison_to_other_methods": "Combines generative pretraining and RL, contrasting with conditional-embedding methods and purely supervised generative approaches; demonstrates property gains versus non-RL baselines but incurs some loss in validity.",
            "limitations_and_challenges": "Reinforcement learning tuning can reduce chemical validity; benchmarks like MoleculeNet have data quality issues (e.g., undefined stereocenters) complicating evaluation; RL reliance on proxy metrics rather than experimental validation.",
            "uuid": "e8662.2",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemGPT",
            "name_full": "ChemGPT",
            "brief_description": "A large-scale GPT-like model trained on molecular data to explore effects of dataset scale and hyperparameters on generative performance and domain adaptation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemGPT",
            "model_type": "decoder-only GPT-style transformer",
            "model_size": "~1 B",
            "training_data": "~10M molecules from PubChem (SMILES)",
            "application_domain": "molecule generation and exploration of scaling/hyperparameter effects for chemical generative models",
            "generation_method": "autoregressive SMILES generation; study focuses on scaling, hyperparameters, and dataset effects",
            "novelty_of_chemicals": "Designed to produce diverse molecular candidates; specific novelty metrics not detailed in review, but work studies impact of scale on generative quality",
            "application_specificity": "General molecule generation; not primarily target-conditioned in the described work",
            "evaluation_metrics": "Standard generative metrics (validity, uniqueness, distributional similarity) and analyses of how scaling and hyperparameters affect outputs",
            "results_summary": "Work emphasized how dataset scale and hyperparameter tuning affect generative performance in chemistry; used to refine understanding of data/scale tradeoffs for molecular generation.",
            "comparison_to_other_methods": "Positions large decoder models against smaller/generic models and demonstrates benefits of larger-scale pretraining for generative quality relative to smaller models.",
            "limitations_and_challenges": "Requires substantial data and compute; chemical training corpora remain orders of magnitude smaller than general-language corpora, limiting absolute scale advantages.",
            "uuid": "e8662.3",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Adilov2021",
            "name_full": "Generative pretraining from Molecules (Adilov 2021)",
            "brief_description": "An early GPT-2-like causal transformer pretrained on SMILES with adapters for parameter-efficient fine-tuning, enabling both molecule generation and property prediction.",
            "citation_title": "Generative pretraining from Molecules",
            "mention_or_use": "mention",
            "model_name": "GPT-2-like causal transformer with adapters",
            "model_type": "decoder-only transformer (GPT-2 style) with adapter modules",
            "model_size": "13.4 M (reported training setup)",
            "training_data": "~5M SMILES from PubChem-derived PubChem-10M",
            "application_domain": "molecule generation and property prediction",
            "generation_method": "self-supervised causal language modeling on SMILES; adapter modules inserted for task-specific fine-tuning",
            "novelty_of_chemicals": "Enables novel molecule generation through learned SMILES distribution; quantitative novelty metrics not provided in review",
            "application_specificity": "Adapters permit task-specific tuning for downstream property prediction or targeted generation",
            "evaluation_metrics": "Property prediction benchmarks and standard generative metrics (not detailed numerically in review)",
            "results_summary": "Demonstrated that GPT-style pretraining on SMILES plus adapter-based fine-tuning is a flexible approach for molecule generation and property prediction with efficient parameter updates.",
            "comparison_to_other_methods": "Contrasted with encoder-only models like ChemBERTa; argued for scalability and resource-efficiency in causal decoder approaches.",
            "limitations_and_challenges": "Adapter/fine-tuning performance depends on quality and representativeness of downstream datasets; SMILES representation limitations persist.",
            "uuid": "e8662.4",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "iupacGPT",
            "name_full": "iupacGPT",
            "brief_description": "A decoder-only model that generates and manipulates molecules using IUPAC names rather than SMILES, aiming for more human-readable and semantically rich generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "iupacGPT",
            "model_type": "decoder-only transformer (GPT-style) trained on IUPAC name sequences",
            "model_size": null,
            "training_data": "Pretraining on a large dataset of IUPAC names and fine-tuning with lightweight networks (exact corpus size not specified in review)",
            "application_domain": "molecule generation, classification, and regression with outputs in IUPAC name format",
            "generation_method": "autoregressive generation of IUPAC names conditioned on tasks or prompts",
            "novelty_of_chemicals": "Generates chemically valid outputs represented as IUPAC names; novelty metrics not specified but claimed to excel in generation and downstream tasks",
            "application_specificity": "IUPAC outputs improve human interpretability and ease integration into workflows; not explicitly described as target-conditioned",
            "evaluation_metrics": "Task-specific generation, classification, and regression performance; numerical metrics not provided in review",
            "results_summary": "Demonstrated that using IUPAC names as a generation modality can produce useful, interpretable molecular outputs enabling generation, classification, and regression tasks.",
            "comparison_to_other_methods": "Represents an alternative to SMILES-based generation, with potential advantages in human-readability and ease of validation; empirical comparisons not deeply quantified in review.",
            "limitations_and_challenges": "IUPAC name complexity and variability can be challenging; mapping between IUPAC strings and structures requires robust parsers and may hide stereochemical detail.",
            "uuid": "e8662.5",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LlaSMol",
            "name_full": "LlaSMol",
            "brief_description": "A strategy that leverages large pre-trained generalist LLMs (e.g., Galactica, LLaMA, Mistral) and parameter-efficient fine-tuning (PEFT/LoRA) for molecular tasks, achieving state-of-the-art property prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LlaSMol (using Galactica, LLaMA, Mistral + PEFT)",
            "model_type": "decoder-only large language models with parameter-efficient fine-tuning (PEFT/LoRa)",
            "model_size": "~7 B (examples of base models used)",
            "training_data": "SMolInstruct and other domain-specific instruction datasets; fine-tuning on MoleculeNet benchmarks",
            "application_domain": "property prediction, molecule generation, conversion and captioning tasks",
            "generation_method": "parameter-efficient fine-tuning of large pre-trained LLMs to perform molecular generation/prediction tasks via instruction-style prompting and PEFT",
            "novelty_of_chemicals": "Primary focus on property prediction and transformation tasks; generation novelty not quantified in review but reported high predictive performance",
            "application_specificity": "Fine-tuning on molecular benchmarks (MoleculeNet) yields task-specific performance; PEFT enables domain adaptation with fewer parameters",
            "evaluation_metrics": "MoleculeNet benchmark metrics for property prediction; reported state-of-the-art performance on those benchmarks",
            "results_summary": "LlaSMol achieved state-of-the-art property prediction performance on MoleculeNet when using PEFT approaches on large pre-trained LLMs, demonstrating effective domain adaptation.",
            "comparison_to_other_methods": "Shows that adapting large generalist LLMs via PEFT can match or exceed specialized mol-LLMs on property-prediction tasks.",
            "limitations_and_challenges": "Performance dependent on quality and relevance of fine-tuning datasets; large base models still require substantial compute for base pretraining even if PEFT reduces fine-tuning cost.",
            "uuid": "e8662.6",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemSpaceAL",
            "name_full": "ChemSpaceAL",
            "brief_description": "An active-learning framework for protein-specific molecular generation that iteratively queries and refines models to efficiently find molecules with desired target properties without prior inhibitor knowledge.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemSpaceAL",
            "model_type": "decoder-only GPT-style generative model integrated with active learning loop",
            "model_size": null,
            "training_data": "ChEMBL 33, GuacaMol v1, MOSES, BindingDB (as reported in review's table)",
            "application_domain": "protein-specific drug discovery (targeted ligand generation)",
            "generation_method": "active learning-driven generation: propose molecules, evaluate (in silico), update model/selection strategy to focus on promising areas",
            "novelty_of_chemicals": "Efficiently identifies molecules with desired characteristics; novelty relative to training set not numerically specified but method emphasizes efficient discovery beyond initial dataset",
            "application_specificity": "Designed to identify protein-specific molecules without prior knowledge of inhibitors—focuses exploration toward target-relevant space via active selection",
            "evaluation_metrics": "Task-specific success rates in finding target-characteristic molecules; in-review descriptions are qualitative rather than numerically specific",
            "results_summary": "Reported effective identification of molecules with desired target characteristics using active learning, improving efficiency of protein-specific generation workflows.",
            "comparison_to_other_methods": "Positions active learning as more efficient than unguided generation for target discovery, leveraging iterative evaluation to focus sampling.",
            "limitations_and_challenges": "Relies on in-silico evaluation proxies for activity; success depends on evaluation fidelity (QSAR/simulations) and initial dataset coverage.",
            "uuid": "e8662.7",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AutoMolDesigner",
            "name_full": "AutoMolDesigner",
            "brief_description": "An open-source automation tool for small-molecule antibiotic design that integrates generative models and automated design workflows to accelerate candidate identification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AutoMolDesigner",
            "model_type": "toolchain combining generative models and automation (details: generative + workflow automation)",
            "model_size": null,
            "training_data": "Not specified in review (open-source tool that integrates various data and models)",
            "application_domain": "antibiotic small-molecule design",
            "generation_method": "automated generation + selection workflows; integrates generative modeling with downstream evaluation to propose antibiotic candidates",
            "novelty_of_chemicals": "Intended to propose small-molecule antibiotic candidates; specifics on novelty metrics not provided in review",
            "application_specificity": "Specifically aimed at antibiotic discovery, combining generation and property filters relevant to antibiotic activity",
            "evaluation_metrics": "Tool-level demonstrations and design case studies (review does not list quantitative benchmarks in detail)",
            "results_summary": "Presented as an open-source example of applying generative and automation approaches to antibiotic design, illustrating automation's role in discovery pipelines.",
            "comparison_to_other_methods": "Serves as an applied automation complement to standalone generative models, emphasizing end-to-end workflows rather than raw generative performance.",
            "limitations_and_challenges": "Success depends on underlying evaluation/assay fidelity and dataset quality; details of experimental validation not provided in review.",
            "uuid": "e8662.8",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Yoshikai2024",
            "name_full": "Transformer+VAE contrastive approach for chirality and novelty (Yoshikai et al.)",
            "brief_description": "Addresses transformer limitations in recognizing chirality from SMILES by coupling transformers with a VAE and applying contrastive learning across multiple SMILES representations to improve novelty and validity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Transformer + VAE with contrastive learning",
            "model_type": "hybrid transformer and variational autoencoder with contrastive learning",
            "model_size": null,
            "training_data": "Not explicitly detailed in review; uses multiple SMILES representations per molecule for contrastive learning",
            "application_domain": "de novo molecule generation with improved stereochemical/chirality representation and enhanced novelty",
            "generation_method": "coupled transformer and VAE; contrastive learning across different SMILES for the same molecule to boost novelty/validity",
            "novelty_of_chemicals": "Contrastive learning increases molecular novelty and validity (qualitative claim in review)",
            "application_specificity": "Aims to better capture stereochemistry and produce more chemically meaningful diverse molecules",
            "evaluation_metrics": "Validity and novelty improvements reported qualitatively; no specific numeric values provided in the review",
            "results_summary": "Combining transformer architectures with VAEs and contrastive learning mitigates chirality-recognition issues in SMILES-based generation and enhances novelty/validity of outputs.",
            "comparison_to_other_methods": "Proposes hybrid approach to overcome pure-transformer weaknesses in stereochemistry handling; compared qualitatively against transformer-only generators.",
            "limitations_and_challenges": "Complex hybrid training; SMILES-based chirality representation remains a core challenge; specifics on scalability and quantitative gains require consulting original paper.",
            "uuid": "e8662.9",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Jablonka-GPT3",
            "name_full": "Decoder-only GPT fine-tuned for property prediction and conditional molecule generation (Jablonka et al.)",
            "brief_description": "An example where GPT-3 (a general natural-language decoder-only model) is fine-tuned to predict molecular properties and to conditionally generate molecules, demonstrating competitive performance with domain-specific mol-LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned for chemical tasks)",
            "model_type": "decoder-only transformer (GPT-3) fine-tuned for chemistry",
            "model_size": "175 B (GPT-3 base mentioned elsewhere in review context)",
            "training_data": "Fine-tuned using a curation of multiple classification and regression benchmarks and chemical datasets (review references curated benchmarks)",
            "application_domain": "property prediction and inverse molecular design (conditional generation)",
            "generation_method": "fine-tuning a large general-purpose decoder model to perform conditional molecule generation and property prediction",
            "novelty_of_chemicals": "Demonstrated conditional generation capability; novelty statistics not provided in the review, but claim competitive performance versus domain-specific mol-LLMs",
            "application_specificity": "Conditionally generates molecules for desired properties using fine-tuned prompts or conditioning strategies",
            "evaluation_metrics": "Property-prediction and generative benchmarks (benchmarks aggregated from multiple classification/regression tasks)",
            "results_summary": "Fine-tuned GPT-3 could predict properties and generate molecules conditionally, achieving competitive results with dedicated mol-LLMs in the review's cited work.",
            "comparison_to_other_methods": "Illustrates that large general-purpose LLMs (properly fine-tuned) can compete with specialized molecular LLMs like MolGPT/cMolGPT for certain tasks.",
            "limitations_and_challenges": "Dependence on curated benchmarks and fine-tuning data quality; mapping between natural-language-tuned decoders and strict chemical syntax (SMILES/IUPAC) requires care to avoid syntactic/chemical invalidity.",
            "uuid": "e8662.10",
            "source_info": {
                "paper_title": "A review of large language models and autonomous agents in chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative pretraining from Molecules",
            "rating": 2,
            "sanitized_title": "generative_pretraining_from_molecules"
        },
        {
            "paper_title": "MolGPT",
            "rating": 2
        },
        {
            "paper_title": "cMolGPT",
            "rating": 2
        },
        {
            "paper_title": "Taiga",
            "rating": 2
        },
        {
            "paper_title": "ChemGPT",
            "rating": 2
        },
        {
            "paper_title": "AutoMolDesigner",
            "rating": 2,
            "sanitized_title": "automoldesigner"
        },
        {
            "paper_title": "iupacGPT",
            "rating": 1
        },
        {
            "paper_title": "LlaSMol",
            "rating": 1
        },
        {
            "paper_title": "ChemSpaceAL",
            "rating": 1,
            "sanitized_title": "chemspaceal"
        },
        {
            "paper_title": "Transformer+VAE contrastive approach for chirality and novelty",
            "rating": 1,
            "sanitized_title": "transformervae_contrastive_approach_for_chirality_and_novelty"
        },
        {
            "paper_title": "Decoder-only GPT fine-tuned for property prediction and conditional molecule generation",
            "rating": 1,
            "sanitized_title": "decoderonly_gpt_finetuned_for_property_prediction_and_conditional_molecule_generation"
        }
    ],
    "cost": 0.019612249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A ReView of Large Language Models and Autonomous Agents in Chemistry</h1>
<p>A Preprint</p>
<p>Mayk Caldas Ramos<br>FutureHouse Inc., San Francisco, CA<br>Department of Chemical Engineering<br>University of Rochester, Rochester, NY<br>mcaldasr@ur.rochester.edu</p>
<p>Christopher J. Collison<br>School of Chemistry and Materials Science<br>Rochester Institute of Technology, Rochester, NY<br>cjcscha@rit.edu<br>Andrew D. White ${ }^{*}$<br>FutureHouse Inc., San Francisco, CA<br>Department of Chemical Engineering<br>University of Rochester, Rochester, NY<br>andrew@futurehouse.org</p>
<p>November 18, 2024</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.</p>
<p>Keywords Large Language Model, LLM, LLM agent, agent, science, chemistry</p>
<h2>Contents</h2>
<p>1 Introduction
1.1 Challenges in Chemistry
2 Large Language Models
2.1 The Transformer</p>
<p>2.2 Model training ..... 6
2.3 Model types ..... 7
2.3.1 Encoder-only Models ..... 7
2.3.2 Decoder-only Models ..... 8
2.3.3 Encoder-decoder Models ..... 8
2.3.4 Multi-task and Multi-modal Models ..... 8
3 LLMs for Chemistry and Biochemistry ..... 9
3.1 Molecular Representations, Datasets, and Benchmarks ..... 11
3.2 Property Prediction and Encoder-only Mol-LLMs ..... 12
3.2.1 Property Prediction ..... 13
3.2.2 Encoder-only Mol-LLMs ..... 14
3.3 Property Directed Inverse Design and Decoder-only mol-LLMs ..... 16
3.3.1 Property Directed Inverse Design ..... 17
3.3.2 Decoder-only Mol-LLMs ..... 18
3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs ..... 20
3.4.1 Synthesis Prediction ..... 20
3.4.2 Encoder-decoder mol-LLMs ..... 21
3.5 Multi-Modal LLMs ..... 23
3.6 Textual Scientific LLMs ..... 24
3.6.1 Text Classification ..... 25
3.6.2 Text Generation ..... 26
3.7 The use of ChatGPT in Chemistry ..... 27
3.7.1 Automation ..... 28
4 LLM-based Autonomous Agents ..... 28
4.1 Memory Module ..... 29
4.2 Planning and Reasoning Modules ..... 30
4.3 Profiling Module ..... 30
4.4 Perception ..... 31
4.5 Tools ..... 31
5 LLM-Based Autonomous Agents in Scientific Research ..... 31
5.1 Agents for Literature Review ..... 34
5.2 Agents for Chemical Innovation ..... 35
5.3 Agents for Experiments Planning ..... 36
5.4 Agents for Automating Cheminformatics Tasks ..... 37
5.5 Agents for Hypothesis Creation ..... 38
6 Challenges and Opportunities ..... 39</p>
<h1>1 Introduction</h1>
<p>The integration of Machine Learning (ML) and Artificial Intelligence (AI) into chemistry has spanned several decades. ${ }^{1-10}$ Although applications of computational methods in quantum chemistry and molecular modeling from the 1950s-1970s were not considered AI, they laid the groundwork. Subsequently in the 1980s expert systems like DENDRAL ${ }^{11,12}$ were expanded to infer molecular structures from mass spectrometry data. ${ }^{13}$ At the same time, Quantitative Structure-Activity Relationship (QSAR) Models were developed ${ }^{5}$ that would use statistical methods to predict the effects of chemical structure on activity. ${ }^{14-17}$ In the 1990s, neural networks, and associated Kohonen Self-Organizing Maps were introduced to domains such as drug design, ${ }^{18,19}$ as summarized well by Yang et al. ${ }^{5}$ and Goldman and Walters ${ }^{20}$, although they were limited by the computational resources of the time. With an explosion of data from High-Throughput Screening (HTS), ${ }^{21,22}$ models then started to benefit from vast datasets of molecular structures and their biological activities. Furthermore, ML algorithms such as Support Vector Machines and Random Forests became popular for classification and regression tasks in cheminformatics, ${ }^{1}$ offering improved performance over traditional statistical methods. ${ }^{23}$
Deep learning transformed the landscape of ML in chemistry and materials science in the 2010s. ${ }^{24}$ Recurrent Neural Networks (RNNs), ${ }^{25-29}$ Convolutional Neural Networks (CNNs) ${ }^{30-32}$ and later, Graph Neural Networks (GNNs), ${ }^{33-38}$ made great gains in their application to molecular property prediction, drug discovery, ${ }^{39}$ and synthesis prediction. ${ }^{40}$ Such methods were able to capture complex patterns in data, and therefore enabled the identification of novel materials for high-impact needs such as energy storage and conversion. ${ }^{41,42}$
In this review, we explore the next phase of AI in chemistry, namely the use of Large Language Models (LLMs) and autonomous agents. Inspired by successes in natural language processing (NLP), LLMs were adapted for chemical language (e.g., Simplified Molecular Input Line Entry System (SMILES) ${ }^{43}$ ) to tackle tasks from synthesis prediction to molecule generation. ${ }^{44-46}$ We will then explore the integration of LLMs into autonomous agents as illustrated by M. Bran et al. ${ }^{47}$ and Boiko et al. ${ }^{48}$, which may be used for data interpretation or, for example, to experiment with robotic systems. We are at a crossroads where AI enables chemists to solve major global problems faster and streamline routine lab tasks. This enables, for instance, the development of larger, consistent experimental datasets and shorter lead times for drug and material commercialization. As such, language has been the preferred mechanism for describing and disseminating research results and protocols in chemistry for hundreds of years. ${ }^{49}$</p>
<h3>1.1 Challenges in Chemistry</h3>
<p>We categorize some key challenges that can be addressed by AI in chemistry as: Property Prediction, Property-Directed Molecule Generation, and Synthesis Prediction. These categories, as illustrated in Figure 1 can be connected to a fourth challenge in automation. The first task is to predict a property for a given compound to decide if it should be synthesized for a specific application, such as an indicator, ${ }^{50}$ light harvester, ${ }^{51}$ or catalyst. ${ }^{52}$ To achieve better models for property prediction, high-quality data is crucial. We discuss the caveats and issues with the current datasets in Section 3.1 and illustrate state-of-the-art findings in Section 3.2.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AI-powered LLMs accelerate chemical discovery with models that address key challenges in Property Prediction, Property Directed Molecule Generation, and Synthesis Prediction. Autonomous agents connect these models and additional tools thereby enabling rapid exploration of vast chemical spaces.</p>
<p>The second task is to generate novel chemical structures that meet desired chemical profiles or exhibit properties. ${ }^{53}$ Success in this area would accelerate progress in various chemical applications, but reliable reverse engineering (inverse</p>
<p>design) ${ }^{54}$ is not yet feasible over the vast chemical space. ${ }^{55}$ For instance, inverse design, when coupled with automatic selection of novel structures (de novo molecular design) could lead to the development of drugs targeting specific proteins while retaining properties like solubility, toxicity, and blood-brain barrier permeability. ${ }^{56}$ The complexity of connecting de novo design with property prediction is high and we show how state-of-the-art models currently perform in Section 3.3.</p>
<p>Once a potential target molecule has been identified, the next challenge is predicting its optimal synthesis using inexpensive, readily available, and non-toxic starting materials. In a vast chemical space, there will always be an alternative molecule "B" that has similar properties to molecule "A" but is easier to synthesize. Exploring this space to find a new molecule with the right properties and a high-yield synthesis route brings together these challenges. The number of possible stable chemicals is estimated to be up to $10^{150}$. ${ }^{57-66}$ Exploring this vast space requires significant acceleration beyond current methods. ${ }^{61}$ As Restrepo ${ }^{57}$ emphasizes, cataloguing failed syntheses is essential to building a comprehensive dataset of chemical features. Autonomous chemical resources can accelerate database growth and tackle this challenge. Thus, automation is considered a fourth major task in chemistry. ${ }^{62-65}$ The following discussion explores how LLMs and autonomous agents can provide the most value. Relevant papers are discussed in Section 3.4
This review is organized within the context of these categories. The structure of the review is as follows. Section 2 provides an introduction to transformers, including a brief description of encoder-only, decoder-only and encoderdecoder architectures. Section 3 provides a detailed survey of work with LLMs, where we connect each transformer architecture to the areas of chemistry that it is best suited to support. We then progress into a description of autonomous agents in section 4, and a survey of how such LLM-based agents are finding application in chemistry-centered scientific research, section 5. After providing some perspective on future challenges and opportunities in section 6, and we conclude in section 7. We distinguish between "text-based" and "mol-based" inputs and outputs, with "text" referring to natural language and "mol" referring to the chemical syntax for material structures, as introduced by Zhang et al. ${ }^{66}$.</p>
<h1>2 Large Language Models</h1>
<p>The prior state-of-the-art for sequence-to-sequence (seq2seq) tasks had been the Recurrent Neural Network (RNN), ${ }^{67}$ typically as implemented by Hochreiter and Schmidhuber ${ }^{68}$. In a seq2seq task, an input sequence, such as a paragraph in English, is processed to generate a corresponding output sequence, such as a translation into French. The RNN retains "memory" of previous steps in a sequence to predict later parts. However, as sequence length increases, gradients can become vanishingly small or explosively large, ${ }^{69,70}$ preventing effective use of earlier information in long sequences. Due to these limitations, RNNs have thus fallen behind Large Language Models (LLMs), which primarily implement transformer architectures, introduced by Vaswani et al. ${ }^{71}$. LLMs are deep neural networks (NN) characterized by their vast number of parameters and, though transformers dominate, other architectures for handling longer input sequences are being actively explored. ${ }^{72-75}$ A detailed discussion of more generally applied LLMs can be found elsewhere. ${ }^{76}$ Since transformers are well-developed in chemistry and are the dominant paradigm behind nearly all state-of-the-art sequence modeling results, they are a focus in this review.</p>
<h3>2.1 The Transformer</h3>
<p>The transformer was introduced in, "Attention is all you need" by Vaswani et al. ${ }^{71}$ in 2017. A careful line-by-line review of the model can be found in "The Annotated Transformer". ${ }^{77}$ The transformer was the first seq2seq model based entirely on attention mechanisms, although attention had been a feature for RNNs some years prior. ${ }^{78}$ The concept of "attention" is a focus applied to certain words of the input, which would convey the most importance, or the context of the passage, and thereby would allow for better decision-making and greater accuracy. However, in a practical sense, "attention" is implemented simply as the dot-product between token embeddings and a learned non-linear function, which will be described further below.</p>
<p>Context Window Large language models are limited by the size of their context window, which represents the maximum number of input tokens they can process at once. This constraint arises from the quadratic computational cost of the transformer's attention mechanism, which restricts effective input to a few thousand tokens. ${ }^{79}$ Hence, LLM-based agents struggle to maintain coherence and capture long-range dependencies in extensive texts or complex dialogues, impacting their performance in applications requiring deep contextual understanding. ${ }^{80}$ These limitations and strategies to overcome them are better discussed in Section 4.</p>
<p>Tokenization In NLP tasks, the natural language text sequence, provided in the context window, is first converted to a list of tokens, which are integers that each represent a fragment of the sequence. Hence the input is numericized according to the model's vocabulary following a specific tokenization scheme. ${ }^{81-85}$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: a) The generalized encoder-decoder transformer: The encoder on the left converts an input into a vector, while the decoder on the right predicts the next token in a sequence. b) Encoder-decoder transformers are traditionally used for translation tasks and, in chemistry, for reaction prediction, translating reactants into products. c) Encoder-only transformers provide a vector output and are typically used for sentiment analysis. In chemistry, they are used for property prediction or classification tasks. d) Decoder-only transformers generate likely next tokens in a sequence. In chemistry, they are used to generate new molecules given an instruction and description of molecules.</p>
<p>Input Embeddings Each token is then converted into a vector in a process called input embedding. This vector is a learned representation that positions tokens in a continuous space based on their semantic relationships. This process allows the model to capture similarities between tokens, which is further refined through mechanisms like attention (discussed below) that weigh and enhance these semantic connections.</p>
<p>Positional Encoding A positional encoding is then added, which plays a major role in transformer success. It is added to the input embeddings to provide information about the order of elements in a sequence, as transformers lack a built-in notion of sequence position. Vaswani et al. ${ }^{71}$ reported similar performance with both fixed positional encoding based on sine and cosine functions, and learned encodings. However, many options for positional embeddings exist. ${ }^{86}$ In fixed positional encoding, the position of each element in a sequence is encoded using sine and cosine functions with different frequencies, depending on the element's position. This encoding is then added to the word's vector representation (generated during the tokenization and embedding process). The result is a modified vector that encodes both the meaning of the word and its position within the sequence. These sine and cosine functions generate values within a manageable range of -1 to 1 , ensuring that each positional encoding is unique and that the encoding is unaffected by sequence length.</p>
<p>Attention The concept of "attention" is central to the transformer's success, especially during training. Attention enables the model to focus on the most relevant parts of the input data. It operates by comparing each element in a sequence, such as a word, to every other element. Each element serves as a query, compared against other elements called keys, each associated with a corresponding value. The alignment between a query and a keys, determines the strength of their connection, represented by an attention weight. ${ }^{87}$ These weights highlight the importance of certain elements by scaling their associated values accordingly. During training, the model learns to adjust these weights, capturing relationships and contextual information within the sequence. Once trained, the model uses these learned weights to integrate information from different parts of the sequence, ensuring that its output remains coherent and contextually aligned with the input.</p>
<p>The transformer architecture is built around two key modules: the encoder and the decoder. Figure 2a provides a simplified diagram of the general encoder-decoder transformer architecture. The input is The input is tokenized, from the model's vocabulary, ${ }^{81-85}$ embedded and positionally encoded, as described above. The encoder consists of multiple stacked layers (six layers in the original model), ${ }^{71}$ with each layer building on the outputs of the previous one. Each token is represented as a vector, that gets passed through these layers. At each encoder layer, a self-attention mechanism is applied, which calculates the attention between tokens, as discussed earlier. Afterward, the model uses normalization and adds the output back to the input through what's called a residual connection. Residual connection is represented in Figure 2a by the "by-passing" arrow. This bypass helps prevent issues with vanishing gradients, ${ }^{69,70}$ ensuring that information flows smoothly through the model. The final step in each encoder layer is a feed-forward neural network with an activation function (such as ReLU, ${ }^{88}$ SwiGLU, ${ }^{89}$ GELU, ${ }^{90}$ etc) that further refines the representation of the input.
The decoder works similarly to the encoder but with key differences. It starts with an initial input token - usually a special start token-embedded into a numerical vector. This token initiates the output sequence generation. Positional encodings are applied to preserve the token order. The decoder is composed of stacked layers, each containing a masked self-attention mechanism that ensures the model only attends to the current and previous tokens, preventing access to future tokens. Additionally, an encoder-decoder attention mechanism aligns the decoder's output with relevant encoder inputs, as depicted by the connecting arrows in Figure 2a. This alignment helps the model focus on the most critical information from the input sequence. Each layer also employs normalization, residual connections, and a feed-forward network. The final layer applies a softmax function, converting the scores into a probability density over the vocabulary of tokens. The decoder generates the sequence autoregressively, predicting each token based on prior outputs until an end token signals termination.</p>
<h1>2.2 Model training</h1>
<p>The common lifetime of an LLM consists of being first pretrained using self-supervised techniques, generating what is called a base model. Effective prompt engineering may lead to successful task completion but this base model is often fine-tuned for specific applications using supervised techniques and this creates the "instruct model." It is called the "instruct model" because the fine-tuning is usually done for it to follow arbitrary instructions, removing the need to specialize fine-tuning for each downstream task. ${ }^{91}$ Finally, the instruct model can be further tuned with reward models to improve human preference or some other non-differentiable and sparse desired character. ${ }^{92}$ These concepts are expanded on below.</p>
<p>Self-supervised Pretraining A significant benefit implied in all the transformer models described in this review is that self-supervised learning takes place with a vast corpus of text. Thus, the algorithm learns patterns from unlabeled data, which opens up the model to larger datasets that may not have been explicitly annotated by humans. The advantage is to discover underlying structures or distributions without being provided with explicit instructions on what to predict, nor with labels that might indicate the correct answer.</p>
<p>Prompt Engineering The model's behavior can be guided by carefully crafting input prompts that leverage the pretrained capabilities of LLMs. Since the original LLM remains unchanged, it retains its generality and can be applied across various tasks. ${ }^{93}$ However, this approach relies heavily on the assumption that the model has adequately learned the necessary domain knowledge during pretraining to achieve an appropriate level of accuracy in a specific domain. Prompt engineering can be sensitive to subtle choices of language; small changes in wording can lead to significantly different outputs, making it challenging to achieve consistent results and to quantify the accuracy of the outputs. ${ }^{94}$</p>
<p>Supervised Fine-tuning After this pretraining, many models described herein are fine-tuned on specific downstream tasks (e.g., text classification, question answering) using supervised learning. In supervised learning, models learn from labeled data, and map inputs to known outputs. Such fine-tuning allows the model to be adjusted with a smaller, task-specific dataset to perform well on that downstream task.</p>
<p>LLM Alignment A key step after model training is aligning the output with human preferences. This process is critical to ensure that the large language model (LLM) produces outputs that are not only accurate but also reflect appropriate style, tone, and ethical considerations. Pretraining and fine-tuning often do not incorporate human values, so alignment methods are essential to adjust the model's behavior, including reducing harmful outputs. ${ }^{95}$
One important technique for LLM alignment is instruction tuning. This method refines the model by training it on datasets that contain specific instructions and examples of preferred responses. By doing so, the model learns to generalize from these examples and follow user instructions more effectively, leading to outputs that are more relevant</p>
<p>and safer for real-world applications. ${ }^{96,97}$ Instruction tuning establishes a baseline alignment, which can then be further improved in the next phase using reinforcement learning (RL). ${ }^{98}$</p>
<p>In RL-based alignment, the model generates tokens as actions and receives rewards based on the quality of the output, guiding the model to optimize its behavior over time. Unlike post-hoc human evaluations, RL actively integrates preference feedback during training, refining the model to maximize cumulative rewards. This approach eliminates the need for token-by-token supervised fine-tuning by focusing on complete outputs, which better capture human preferences. ${ }^{99-101}$</p>
<p>The text generation process in RL is typically modeled as a Markov Decision Process (MDP), where actions are tokens, and rewards reflect how well the final output aligns with human intent. ${ }^{102}$ A popular method, Reinforcement Learning with Human Feedback (RLHF), ${ }^{103}$ leverages human input to shape the reward system, ensuring alignment with user preferences. Variants such as reinforcement learning with synthetic feedback (RLSF), ${ }^{104}$ Proximal Policy Optimization (PPO), ${ }^{105}$ and REINFORCE ${ }^{106}$ offer alternative strategies for assigning rewards and refining model policies. ${ }^{99,102,107,108}$ A broader exploration of RL's potential in fine-tuning LLMs is available in works by Cao et al. ${ }^{109}$ and Shen et al. ${ }^{95}$
There are ways to reformulate the RLHF process into a direct optimization problem with a different loss. This is known as reward-free metods. Among the main examples of reward-free methods, we have the direct preference optimization (DPO), ${ }^{110}$ Rank Responses to align Human Feedback (RRHF), ${ }^{111}$ and Preference Ranking Optimization (PRO). ${ }^{112}$ These models are popular competitors to PPO and other reward-based methods due to its simplicity. It overcomes the lack of token-by-token loss signal by comparing two completions at a time. The discussions about which technique is superior remain very active in the literature. ${ }^{113}$
Finally, the alignment may not be to human preferences but to downstream tasks that do not provide token-by-token rewards. For example, Bou et al. ${ }^{114}$ and Hayes et al. ${ }^{115}$ both use RL on a language model for improving its outputs on a downstream scientific task.</p>
<h1>2.3 Model types</h1>
<p>While the Vaswani Transformer ${ }^{71}$ employed an encoder-decoder structure for sequence-to-sequence tasks, the encoder and decoder were ultimately seen as independent models, leading to "encoder-only", and "decoder-only" models described below.</p>
<p>Examples of how such models can be used are provided in Figures 2b, c, and d. Figure 2b illustrates the encoder-decoder model's capability to transform sequences, such as translating from English to Spanish or predicting reaction products by mapping atoms from reactants (amino acids) to product positions (a dipeptide and water). This architecture has large potential on sequence-to-sequence transformations. ${ }^{116,117}$ Figure 2c highlights the strengths of an encoder-only model in extracting properties or insights directly from input sequences. For example, in text analysis, it can assign sentiment scores or labels, such as tagging the phrase "Chemistry is great" with a positive sentiment. In chemistry, it can predict molecular properties, like hydrophobicity or pKa , from amino acid representations, demonstrating its applications in material science and cheminformatics. ${ }^{118-120}$ Finally, Figure 2d depicts a decoder-only architecture, ideal for tasks requiring sequence generation or completion. This model excels at inferring new outputs from input prompts. For instance, given that "chemistry is great," it can propose broader implications or solutions. It can also generate new peptide sequences from smaller amino acid fragments, showcasing its ability to create novel compounds. This generative capacity is particularly valuable in drug design, where the goal is to discover new molecules or expand chemical libraries. ${ }^{44,121-123}$</p>
<h3>2.3.1 Encoder-only Models</h3>
<p>Beyond Vaswani's transformer, ${ }^{71}$ used for sequence-to-sequence tasks, another significant evolutionary step forward came in the guise of the Bidirectional Encoder Representations from Transformers, or "BERT", described in October 2018 by Devlin et al. ${ }^{87}$ BERT utilized only the encoder component, achieving state-of-the-art performance on sentence-level and token-level tasks, outperforming prior task-specific architectures. ${ }^{87}$ The key difference was BERT's bidirectional transformer pretraining on unlabeled text, meaning the model processes the context both to the left and right of the word in question, facilitated by a Masked Language Model (MLM). This encoder-only design allowed BERT to develop more comprehensive representations of input sequences, rather than mapping input sequences to output sequences. In pretraining, BERT also uses Next Sentence Prediction (NSP). "Sentence" here means an arbitrary span of contiguous text. The MLM task randomly masks tokens and predicts them by considering both preceding and following contexts simultaneously, inspired by Taylor. ${ }^{124}$ NSP predicts whether one sentence logically follows another, training the model to understand sentence relationships. This bidirectional approach allows BERT to recognize greater nuance and richness in the input data.</p>
<p>Subsequent evolutions of BERT include, for example, RoBERTa, (Robustly optimized BERT approach), described in 2019 by Liu et al. ${ }^{125}$. RoBERTa was trained on a larger corpus, for more iterations, with larger mini-batches, and longer sequences, improving model understanding and generalization. By removing the NSP task and focusing on the MLM task, performance improved. RoBERTa dynamically changed masked positions during training and used different hyperparameters. Evolutions of BERT also include domain-specific pretraining and creating specialist LLMs for fields like chemistry, as described below (see Section 3).</p>
<h1>2.3.2 Decoder-only Models</h1>
<p>In June 2018, Radford et al. ${ }^{126}$ proposed the Generative Pretrained Transformer (GPT) in their paper, "Improving Language Understanding by Generative Pretraining". GPT used a decoder-only, left-to-right unidirectional language model to predict the next word in a sequence based on previous words, without an encoder. Unlike earlier models, GPT could predict the next sequence, applying a general language understanding to specific tasks with smaller annotated datasets.
GPT employed positional encodings to maintain word order in its predictions. Its self-attention mechanism prevented tokens from attending to future tokens, ensuring each word prediction depended only on preceding words. Hence a decoder-only architecture represents a so-called causal language model, one that generates each item in a sequence based on the previous items. This approach is also referred to as "autoregressive", meaning that each new word is predicted based on the previously generated words, with no influence from future words. The generation of each subsequent output is causally linked to the history of generated outputs and nothing ahead of the current word affects its generation.</p>
<h3>2.3.3 Encoder-decoder Models</h3>
<p>Evolving further, BART (Bidirectional and Auto-Regressive Transformers) was introduced by Lewis et al. in 2019. ${ }^{127}$ BART combined the context learning strengths of the bidirectional BERT, and the autoregressive capabilities of models like GPT, which excel at generating coherent text. BART was thus a hybrid seq2seq model, consisting of a BERT-like bidirectional encoder and a GPT-like autoregressive decoder. This is nearly the same architecture as Vaswani et al. ${ }^{71}$; the differences are in the pretraining. BART was pretrained using a task that corrupted text by, for example, deleting tokens, and shuffling sentences. It then learned to reconstruct the original text with left-to-right autoregressive decoding as in GPT models.</p>
<h3>2.3.4 Multi-task and Multi-modal Models</h3>
<p>In previous sections, we discussed LLMs that take natural language text as input and then output either a learned representation or another text sequence. These models traditionally perform tasks like translation, summarization, and classification. However, multi-task models are capable of performing several different tasks using the same model, even if those tasks are unrelated. This allows a single model to be trained on multiple objectives, enhancing its versatility and efficiency, as it can generalize across various tasks during inference.
Multi-task models, such as the Text-to-Text Transfer Transformer (T5) developed by Raffel et al. ${ }^{128}$ demonstrate that various tasks can be reframed into a text-to-text format, allowing the same model architecture and training procedure to be applied universally. By doing so, the model can be used for diverse tasks, but all with the same set of weights. This reduces the need for task-specific models and increases the model's adaptability to new problems. The relevance of this approach is particularly significant as it enables researchers to tackle multiple tasks without needing to retrain separate models, saving both computational resources and time. For instance, Flan-T5 ${ }^{129}$ used instruction fine-tuning with chain-of-thought prompts, enabling it to generalize to unseen tasks, such as generating rationales before answering. This fine-tuning expands the model's ability to tackle more complex problems. More advanced approaches have since been proposed to build robust multi-task models that can flexibly switch between tasks at inference time. ${ }^{130-133}$
Additionally, LLMs have been extended to process different input modalities, such as image and sound, even though they initially only processed text. For example, Fuyu ${ }^{134}$ uses linear projection to adapt image representations into the token space of an LLM, allowing a decoder-only model to generate captions for figures. Expanding on this, next-GPT ${ }^{135}$ was developed as an "any-to-any" model, capable of processing multiple modalities, such as text, audio, image, and video, through modality-specific encoders. The encoded representation is projected into a decoder-only token space, and the LLM's output is processed by a domain-specific diffusion model to generate each modality's output. Multitask or multimodel methods are further described below as these methods start to connect LLMs with autonomous agents.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Classification of LLMs in chemistry and biochemistry according to their application.</p>
<h1>3 LLMs for Chemistry and Biochemistry</h1>
<p>The integration of large language models (LLMs) into chemistry and biochemistry is opening new frontiers in molecular design, property prediction, and synthesis. As these models evolve, they increasingly align with specific chemical tasks, capitalizing on the strengths of their architectures. Specifically, encoder-only models excel at property prediction, ${ }^{118}$ decoder-only models are suited for inverse design, ${ }^{136}$ and encoder-decoder models are applied to synthesis prediction. ${ }^{137}$ However, with the development improvement of decoder-only models ${ }^{138}$ and the suggestion that regression tasks can be reformulated as a text completion task, ${ }^{139}$ decoder-only models started being also applied for property prediction. ${ }^{140-143}$ This section surveys key LLMs that interpret chemical languages like SMILES and InChI, as well as those that process natural language descriptions relevant to chemistry.
We provide a chronological perspective on the evolution of LLMs in this field (Figure 4), presenting broadly on the design, functionality, and value of each model. Our approach primarily centers on models that use chemical representations like SMILES strings as inputs, but we also examine how natural language models extract valuable data from scientific literature to enhance chemical research.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of how Large Language Models (LLMs) evolved chronologically. The dates display the first publication of each model.</p>
<p>Ultimately, this discussion underscores the potential for mol-based and text-based LLMs to work together, addressing the growing opportunity for automation in chemistry. This sets the stage for a broader application of autonomous agents in scientific discovery. Figure 3 illustrates the capabilities of different LLMs available currently, while Figure 4 presents a chronological map of LLM development in chemistry and biology.
Of critical importance, this section starts by emphasizing the role of trustworthy datasets and robust benchmarks. Without well-curated, diverse datasets, models may fail to generalize across real-world applications. Benchmarks that are too narrowly focused can limit the model's applicability, preventing a true measure of its potential. While natural language models take up a smaller fraction of this section, these models will be increasingly used to curate these datasets, ensuring data quality becomes a key part of advancing LLM capabilities in chemistry.</p>
<h1>3.1 Molecular Representations, Datasets, and Benchmarks</h1>
<p>Molecules can be described in a variety of ways, ranging from two-dimensional structural formulas to more complex three-dimensional models that capture electrostatic potentials. Additionally, molecules can be characterized through properties such as solubility, reactivity, or spectral data from techniques like NMR or mass spectrometry. However, to leverage these descriptions in machine learning, they must be converted into a numerical form that a computer can process. Given the diversity of data in chemistry-based machine learning, multiple methods exist for representing molecules, ${ }^{144-149}$ highlighting this heterogeneity. Common representations include molecular graphs, ${ }^{150-152}$ 3D point clouds, ${ }^{153-156}$ and quantitative feature descriptors. ${ }^{145,157-160}$ In this review, we focus specifically on string-based representations of molecules, given the interest in language models. Among the known string representations, we can cite IUPAC names, SMILES, ${ }^{43}$ DeepSMILES, ${ }^{161}$ SELFIES, ${ }^{162}$ and InChI, ${ }^{163}$ as recently reviewed by Das et al. ${ }^{164}$
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Number of training tokens (on log scale) available from various chemical sources compared with typical LLM training runs. The numbers are drawn from ZINC, ${ }^{165}$ PubChem, ${ }^{166}$ Touvron et al. ${ }^{167}$, ChEMBL, ${ }^{168}$ and Kinney et al. ${ }^{169}$</p>
<p>Regarding datasets, there are two types of data used for training LLMs, namely training data and evaluation data. Training data should be grounded in real molecular structures to ensure the model develops an accurate representation of what constitutes a valid molecule. This is similar to how natural language training data, such as that used in models like GPT-4, must be based on real sentences or code to avoid generating nonsensical outputs. Figure 5 shows a comparison of the number of tokens in common chemistry datasets with those used to train LLaMA2, based on literature data. ${ }^{165-169}$ With this in mind, we note the largest chemical training corpus, which largely comprises hypothetical chemical structures, amounts to billions of tokens, almost two orders of magnitude fewer than the trillions of tokens used to train LLaMA2. When excluding hypothetical structures from datasets like ZINC, ${ }^{165}$ (Figure 5), the number of tokens associated with verifiably synthesized compounds is over five orders of magnitude lower than that of LLaMA2's training data. To address this gap, efforts such as the Mol-instructions dataset, curated by Fang et al. ${ }^{170}$, prioritize quality over quantity, providing $\sim 2 \mathrm{M}$ biomolecular and protein-related instructions. Mol-instructions ${ }^{170}$ was selectively built from multiple data sources, ${ }^{56,171-180}$ with rigorous quality control. Given the success of literature-based LLMs, one may naturally assume that large datasets are of paramount importance for chemistry. However, it is crucial not to overlook the importance of data quality. Segler et al. ${ }^{181}$ demonstrated that even using the Reaxys dataset, a very small, human-curated collection of chemical reactions, was sufficient to achieve state-of-the-art results in retrosynthesis. Therefore, the issue is not merely a lack of data, but rather a lack of high-quality data that may be the pivotal factor</p>
<p>holding back the development of better scientific LLMs. Ultimately, the focus must shift from sheer quantity to the curation of higher-quality datasets to advance these models.
To evaluate the accuracy of these models, we compare their performance against well-established benchmarks. However, if the benchmarks are not truly representative of the broader chemistry field, it becomes difficult to gauge the expected impact of these models. Numerous datasets, curated by the scientific community, are available for this benchmarking. ${ }^{182,183}$ Among them, MoleculeNet, ${ }^{56}$ first published in 2017, is the most commonly used labeled dataset for chemistry. However, MoleculeNet has several limitations: it is small, contains errors and inconsistencies, and lacks relevance to a larger number of real-world chemistry problems. ${ }^{184-187}$ Pat Walters, a leader in ML for drug discovery, has emphasized, "I think the best way to make progress on applications of machine learning to drug discovery is to fund a large public effort that will generate high-quality data and make this data available to the community". ${ }^{188}$
Walters provides several constructive critiques noting, for example, that the QM7, QM8, and QM9 datasets, intended for predicting quantum properties from 3D structures, are often misused with predictions based incorrectly on their 1D SMILES strings, which inadequately represent 3D molecular conformations. He also suggests more relevant benchmarks and also datasets with more valid entries. For example, he points to the Absorption, Distribution, Metabolism, and Excretion (ADME) data curated by Fang et al. ${ }^{189}$, as well as the Therapeutic Data Commons (TDC) ${ }^{190,191}$ and TDC2. ${ }^{192}$ These datasets contain measurements of real compounds, making them grounded in reality. Moreover, ADME is crucial for determining a drug candidate's success, while therapeutic results in diverse modalities align with metrics used in drug development.
Here, we hypothesize that the lack of easily accessible, high-quality data in the correct format for training foundational chemical language models is a major bottleneck to the development of the highly desired "super-human" AI-powered digital chemist. A more optimistic view is presented by Rich and Birnbaum ${ }^{193}$ They argue that we do not need to wait for the creation of new benchmarks. Instead, they suggest that even the currently available, messy public data can be carefully curated to create benchmarks that approximate real-world applications. In addition, we argue that extracting data from scientific chemistry papers might be an interesting commitment to generating data of high quality, grounded to the truth, and on a large scale. ${ }^{194}$ Some work has been done in using LLMs for data extraction. ${ }^{195,196}$ Recently, a few benchmarks following these ideas were created for evaluating LLMs' performance in biology (LAB-Bench ${ }^{197}$ ) and material science (MatText, ${ }^{198}$ MatSci-NLP ${ }^{199}$ and MaScQA ${ }^{200}$ ).</p>
<h1>3.2 Property Prediction and Encoder-only Mol-LLMs</h1>
<p>Encoder-only transformer architectures are primarily composed of an encoder, making them well-suited for chemistry tasks that require extracting meaningful information from input sequences, such as classification and property prediction. Since encoder-only architectures are mostly applied to capturing the underlying structure-property relationships, we describe here the relative importance of the property prediction task. Sultan et al. ${ }^{201}$ also discussed the high importance of this task, the knowledge obtained in the last years, and the remaining challenges regarding molecular property prediction using LLMs.</p>
<p>Table 1: Encoder-only scientific LLMs. The release date column displays the date of the first publication for each paper. When available, the publication date of the last updated version is displayed between parentheses. $a$ :"Model Size" is reported as the number of parameters. $b$ : The authors report they not used as many encoder layers as it was used in the original BERT paper. But the total number of parameters was not reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Model <br> Size $^{\text {a }}$</th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Release date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CatBERTa ${ }^{202}$</td>
<td style="text-align: center;">355 M</td>
<td style="text-align: center;">OpenCatalyst2020 (OC20)</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2023.09 \ &amp; (2023.11) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">SELFormer ${ }^{203}$</td>
<td style="text-align: center;">$\sim 86 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 2 \mathrm{M}$ compounds from ChEMBL</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2023.04 \ &amp; (2023.06) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ChemBERTa$2^{122}$</td>
<td style="text-align: center;">5 M - 46 M</td>
<td style="text-align: center;">77M SMILES from PubChem</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2022.09</td>
</tr>
<tr>
<td style="text-align: center;">MaterialsBERT ${ }^{204}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">2.4M material science abstracts +750 annotated abstract for NER</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER and property extraction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.09 \ &amp; (2023.04) \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 1 - continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Model <br> Size $^{a}$</th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Release date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SolvBERT ${ }^{205}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">1M SMILES of solutesolvent pairs from CombiSolv-QM and LogS from Boobier et al. ${ }^{206}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.07 \ &amp; (2023.01) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ScholarBERT ${ }^{207}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 340 \mathrm{M}, \ &amp; 770 \mathrm{M} \end{aligned}$</td>
<td style="text-align: center;">Public.Resource.Org, Inc</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.05 \ &amp; (2023.05) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">BatteryBERT ${ }^{208}$</td>
<td style="text-align: center;">$\sim 110 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 400 \mathrm{k}$ papers from RSC, Elsevier and Springer</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Document classification</td>
<td style="text-align: center;">2022.05</td>
</tr>
<tr>
<td style="text-align: center;">MatBERT ${ }^{209}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">Abstracts from solid state articles and abstracts and methods from gold nanoparticle articles</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">2022.04</td>
</tr>
<tr>
<td style="text-align: center;">MatSciBERT ${ }^{210}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">$\sim 150 \mathrm{~K}$ material science paper downloaded from Elsevier</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER and text classification</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2021.09 \ &amp; (2022.05) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Mol-BERT ${ }^{118}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">$\sim 4 \mathrm{~B} \quad$ SMILES from ZINC15 and ChEMBL27</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2021.09</td>
</tr>
<tr>
<td style="text-align: center;">MolFormer ${ }^{211}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">PubChem and ZINC</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2021.06 \ &amp; (2022.12) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ChemBERT ${ }^{212}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">$\sim 200 \mathrm{k}$ extracted using ChemDataExtractor</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">2021.06</td>
</tr>
<tr>
<td style="text-align: center;">MolBERT ${ }^{213}$</td>
<td style="text-align: center;">$\sim 85 \mathrm{M}$</td>
<td style="text-align: center;">ChemBench</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2020.11</td>
</tr>
<tr>
<td style="text-align: center;">ChemBERTa ${ }^{44}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10M SMILES from PubChem</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2020.10</td>
</tr>
<tr>
<td style="text-align: center;">BioMegatron ${ }^{214}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 345 \mathrm{M}, \ &amp; 800 \mathrm{M}, \ &amp; 1.2 \mathrm{~B} \end{aligned}$</td>
<td style="text-align: center;">Wikipedia, CC-Stories, Real-News, and OpenWebtext</td>
<td style="text-align: center;">Megatron- <br> LM</td>
<td style="text-align: center;">NER and QA</td>
<td style="text-align: center;">2020-10</td>
</tr>
<tr>
<td style="text-align: center;">PubMedBERT ${ }^{215}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">14M abstracts from PubMed</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER, QA, and document classification</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2020.07 \ &amp; (2021.10) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{array}{ll} \text { Molecule } &amp; \text { Attention } \ \text { former } \end{array}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">ZINC15</td>
<td style="text-align: center;">Encoder with GCN features</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2020.02</td>
</tr>
<tr>
<td style="text-align: center;">SMILESBERT ${ }^{217}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">$\begin{array}{ll} \sim 18 \mathrm{M} &amp; \text { SMILES } \ \text { ZINC } \end{array}$</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2019.09</td>
</tr>
<tr>
<td style="text-align: center;">BlueBERT ${ }^{218}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">PubMed and MIMIC-III</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER, and document classification</td>
<td style="text-align: center;">2019.06</td>
</tr>
<tr>
<td style="text-align: center;">ClinicalBERT ${ }^{219}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Patient readmission probability</td>
<td style="text-align: center;">2019.04</td>
</tr>
<tr>
<td style="text-align: center;">SciBERT ${ }^{220}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">1.14M papers from Semantic Scholar</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER and sentence classification</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2019.03 \ &amp; (2019.11) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">BioBERT ${ }^{221}$</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">PubMed and PMC</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">NER and QA</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2019.01 \ &amp; (2019.09) \end{aligned}$</td>
</tr>
</tbody>
</table>
<h1>3.2.1 Property Prediction</h1>
<p>The universal value of chemistry lies in identifying and understanding the properties of compounds to optimize their practical applications. In the pharmaceutical industry, therapeutic molecules interact with the body in profound</p>
<p>ways. ${ }^{222-224}$ Understanding these interactions and modifying molecular structures to enhance those therapeutic benefits can lead to significant medical advancements. ${ }^{225}$ Similarly, in polymer science, material properties depend on chemical structure, polymer chain length, and packing, ${ }^{226}$ and a protein's function similarly depends on its structure and folding. Historically, chemists have identified new molecules from natural products ${ }^{227}$ and screened them against potential targets ${ }^{228}$ to test their properties for diseases. Once a natural product shows potential, chemists synthesize scaled-up quantities for further testing or derivatization, ${ }^{229-231}$ a costly and labor-intensive process. ${ }^{232,233}$ Traditionally, chemists have used their expertise to hypothesize the properties of new molecules derived from those natural products, hence aiming for the best investment of synthesis time and labor. Computational chemistry has evolved to support the chemical industry in more accurate property prediction. ${ }^{234}$ Techniques such as quantum theoretical calculations and force-field-based molecular dynamics offer great support for property prediction and the investigation of molecular systems, though both require substantial computational resources. ${ }^{235-239}$ Property prediction can now be enhanced through machine learning tools, ${ }^{159,240-242}$ and more recent advancements in LLMs lead to effective property prediction without the extensive computational demands of quantum mechanics and MD calculations. Combined with human insight, AI can revolutionize material development, enabling the synthesis of new materials with a high likelihood of possessing desired properties for specific applications.</p>
<h1>3.2.2 Encoder-only Mol-LLMs</h1>
<p>Encoder-only models are exemplified by the BERT architecture, which is commonly applied in natural language sentiment analysis to extract deeper patterns from prose. ${ }^{243}$ The human chemist has been taught to look at a 2D image of a molecular structure and to recognize its chemical properties or classify the compound. Therefore, encoder-only models would ideally convert SMILES strings, empty of inherent chemical essence, into a vector representation, or latent space, which would reflect those chemical properties. This vector representation can then be used directly for various downstream tasks.
While encoder-only LLMs are predominantly used for property prediction, they are also applicable for synthesis classification.Schwaller et al. ${ }^{244}$ used a BERT model to more accurately classify complex synthesis reactions by generating reaction fingerprints from raw SMILES strings, without the need to separate reactants from reagents in the input data, thereby simplifying data preparation. The BERT model achieved higher accuracy ( $98.2 \%$ ) compared to the encoder-decoder model ( $95.2 \%$ ) for classifying reactions. Accurate classification aids in understanding reaction mechanisms, vital for reaction design, optimization, and retrosynthesis. Toniato et al. ${ }^{245}$ also used a BERT architecture to classify reaction types for downstream retrosynthesis tasks that would enable the manufacture of any molecular target. Further examples of BERT use include self-supervised reaction atom-to-atom mapping. ${ }^{246,247}$ These chemical classifications would accelerate research and development in organic synthesis, described further below.
Beyond synthesis classification, encoder-only models like BERT have shown great promise for molecular property prediction, especially when labeled data is limited. Recognizing this, Wang et al. introduced a semi-supervised SMILESBERT model, which was pretrained on a large unlabeled dataset with a Masked SMILES Recovery task. ${ }^{248}$ The model was then fine-tuned for various molecular property prediction tasks, outperforming state-of-the-art methods in 2019 on three chosen datasets varying in size and property. This marked a shift from using BERT for reaction classification towards property prediction and drug discovery. Maziarka et al. ${ }^{216}$ also claimed state-of-the-art performance in property prediction after self-supervised pretraining in their Molecule Attention Transformer (MAT), which adapted BERT to chemical molecules by augmenting the self-attention with inter-atomic distances and molecular graph structure.
Zhang et al. ${ }^{249}$ also tackled the issue of limited property-labeled data and the lack of correlation between any two datasets labeled for different properties, hindering generalizability. They introduced multitask learning BERT (MTLBERT), which used large-scale pretraining and multitask learning with unlabeled SMILES strings from ChEMBL, ${ }^{168}$ which is a widely-used database containing bioactive molecules with drug-like properties, designed to aid drug discovery. The MTL-BERT approach mined contextual information and extracted key patterns from complex SMILES strings, improving model interpretability. The model was fine-tuned for relevant downstream tasks, achieving better performance than state-of-the-art methods in 2022 on 60 molecular datasets from ADMETlab ${ }^{250}$ and MoleculeNet. ${ }^{56}$
In 2021, Li and Jiang ${ }^{118}$ introduced Mol-BERT, pretrained on four million unlabeled drug SMILES from the ZINC15 ${ }^{251}$ and ChEMBL27 ${ }^{168}$ databases to capture molecular substructure information for property prediction. Their work leveraged the underutilized potential of large unlabeled datasets like ZINC, which contains over 230 million commercially available compounds, and is designed for virtual screening and drug discovery. Mol-BERT consisted of three components: a PretrainingExtractor, Pretraining Mol-BERT, and Fine-Tuning Mol-BERT. It treated Morgan fingerprint fragments as "words" and complete molecular compounds as "sentences," using RDKit and the Morgan algorithm for canonicalization and substructure identification. This approach generated comprehensive molecular fingerprints from SMILES strings, used in a Masked Language Model (MLM) task for pretraining. Mol-BERT was fine-tuned on labeled samples, providing outputs as binary values or continuous scores for classification or regression, and it</p>
<p>outperformed existing sequence and graph-based methods by at least $2 \%$ in ROC-AUC scores on Tox21, SIDER, and ClinTox benchmark datasets. ${ }^{56}$
Ross et al. ${ }^{252}$ introduced MoLFormer, a large-scale self-supervised BERT model, with the intention to provide molecular property predictions with competitive accuracy and speed when compared to Density Functional Theory calculations or wet-lab experiments. They trained MoLFormer with rotary positional embeddings on SMILES sequences of 1.1 billion unlabeled molecules from ZINC, ${ }^{251}$ and PubChem, ${ }^{166}$ another database of chemical properties and activities of millions of small molecules, widely used in drug discovery and chemical research. The rotary positional encoding captures token positions more effectively than traditional methods, ${ }^{71}$ improving modeling of sequence relationships. MoLFormer outperformed state-of-the-art GNNs on several classification and regression tasks from ten MoleculeNet ${ }^{56}$ datasets, while performing competitively on two others. It effectively learned spatial relationships between atoms, predicting various molecular properties, including quantum-chemical properties. Additionally, the authors stated how MoLFormer represents an efficient and environment-friendly use of computational resources, claiming a reduced GPU usage in training by a factor of 60 (16 GPUs instead of 1000).
With ChemBERTa, Chithrananda et al. ${ }^{44}$ explored the impact of pretraining dataset size, tokenization strategy, and the use of SMILES or SELFIES, distinguishing their work from other BERT studies. They used HuggingFace’s RoBERTa transformer, ${ }^{253}$ and referenced a DeepChem ${ }^{56}$ tutorial for accessibility. Their results showed improved performance on downstream tasks (BBBP, ClinTox, HIV, Tox21 from MoleculeNet ${ }^{56}$ ) as the pretraining dataset size increased from 100 K to 10 M . Although ChemBERTa did not surpass state-of-the-art GNN-based baselines like Chemprop (which used 2048-bit Morgan Fingerprints from RDKit), ${ }^{254}$ the authors suggested that with expansion to larger datasets they would eventually beat those baselines. The authors compared Byte-Pair Encoder (BPE) with a custom SmilesTokenizer and its regular expression developed by ${ }^{255}$ while exploring tokenization strategies. They found the SmilesTokenizer slightly outperformed BPE, suggesting more relevant sub-word tokenization is beneficial. No difference was found between SMILES and SELFIES, but the paper highlighted how attention heads in transformers could be visualized with BertViz, ${ }^{256}$ showing certain neurons selective for functional groups. This study underscored the importance of appropriate benchmarking and addresses the carbon footprint of AI in molecular property prediction.
In ChemBERTa-2, Ahmad et al. ${ }^{122}$ aimed to create a foundational model applicable across various tasks. They addressed a criticism that LLMs were not so generalizable because the training data was biased or non-representative. They addressed this criticism by training on 77 M samples and adding a Multi-Task Regression component to the pretraining. ChemBERTa-2 matched state-of-the-art architectures on MoleculeNet. ${ }^{56}$ As with ChemBERTa, the work was valuable because of additional exploration, in this case into how pretraining improvements affected certain downstream tasks more than others, depending on the type of fine-tuning task, the structural features of the molecules in the fine-tuning task data set, or the size of that fine-tuning dataset. The result was that pretraining the encoder-only model is important, but gains could be made by considering the chemical application itself, and the associated fine-tuning dataset.
In June 2023, Yuksel et al. ${ }^{203}$ introduced SELFormer, building on ideas from ChemBERTa2 ${ }^{122}$ and using SELFIES for large data input. Yuksel et al. ${ }^{203}$ argue that SMILES strings have validity and robustness issues, hindering effective chemical interpretation of the data, although this perspective is not universally held. ${ }^{257}$ SELFormer uses SELFIES and is pretrained on two million drug-like compounds, fine-tuned for diverse molecular property prediction tasks (BBBP, SIDER, Tox21, HIV, BACE, FreeSolv, ESOL, PDBbind from MoleculeNet). ${ }^{56}$ SELFormer outperformed all competing methods for some tasks and produced comparable results for the rest. It could also discriminate molecules with different structural properties. The paper suggests future directions in multimodal models combining structural data with other types of molecular information, including text-based annotations. We will discuss such multimodal models below.
In 2022, Yu et al. ${ }^{205}$ published SolvBERT, a multi-task BERT-based regression model that could predict both solvation free energy and solubility from the SMILES notations of solute-solvent complexes. It was trained on the CombiSolvQM dataset, ${ }^{258}$ a curation of experimental solvent free energy data called CombiSolv-Exp-8780, ${ }^{259-262}$ and the solubility dataset from Boobier et al. ${ }^{206}$. SolvBERT's performance was benchmarked against advanced graph-based models ${ }^{263,264}$ This work is powerful because there is an expectation that solvation free energy depends on 3-dimensional conformational properties of the molecules, or at least 2D properties that would be well characterized by graph-based molecular representations. It shows an overachieving utility of using SMILES strings in property prediction, and aligns with other work by Winter et al. ${ }^{265}$, regarding activity coefficients. SolvBERT showed comparable performance to a Directed Message Passing Neural Network (DMPNN) in predicting solvation free energy, largely due to its effective clustering feature in the pretraining phase as shown by TMAP (Tree Map of All Possible) visualizations. Furthermore, SolvBERT outperformed Graph Representation Of Molecular Data with Self-supervision (GROVER) ${ }^{264}$ in predicting experimentally evaluated solubility data for new solute-solvent combinations. This underscores the significance of SolvBERT's ability to capture the dynamic and spatial complexities of solvation interactions in a text-based model.
While models like SolvBERT have achieved impressive results in solvation free energy prediction, challenges such as limited labeled data continue to restrict the broader application of transformer models in chemistry. Recognizing</p>
<p>this issue, Jiang et al. introduced INTransformer in 2024, ${ }^{266}$ a method designed to enhance property prediction by capturing global molecular information more effectively, even when data is scarce. By incorporating perturbing noise and using contrastive learning to artificially augment smaller datasets, INTransformer delivered improved performance on several tasks. Ongoing work continues to explore various transformer strategies for smaller datasets. Again using contrastive learning, which maximizes the difference between representations of similar and dissimilar data points, but in a different context, MoleculeSTM ${ }^{267}$ uses LLM encoders to create representations for SMILES and for descriptions of molecules extracted from PubChem. ${ }^{268}$ Similar work was performed by Xu et al. ${ }^{269}$ The authors curated a dataset with descriptions of proteins. Subsequently, to train ProtST, a protein language model (PLM) was used to encode amino acid sequences and LLMs to encode the descriptions.
In this section, we outlined the advancements of encoder-only models like BERT and their evolution for property prediction and synthesis classification. Chemists traditionally hypothesize molecular properties, but these models, ranging from Mol-BERT to SolvBERT, showcase the growing efficiency of machine learning in property prediction. Approaches such as multitask learning and contrastive learning, as seen in INTransformer, offer solutions to challenges posed by limited labeled data.</p>
<h1>3.3 Property Directed Inverse Design and Decoder-only mol-LLMs</h1>
<p>Decoder-only GPT-like architectures offer significant value for property-directed molecule generation and de novo chemistry applications because they excel at generating novel molecular structures by learning from vast datasets of chemical compounds. These models can capture intricate patterns and relationships within molecular sequences, proposing viable new compounds that adhere to desired chemical properties and constraints. This enables rapid exploration and innovation within an almost infinite chemical space. Moreover, such large general-purpose models can be fine-tuned with small amounts of domain-specific scientific data, ${ }^{142,270}$ allowing them to support specific applications efficiently. In this section, we first describe property-directed inverse design from a chemistry perspective and then examine how decoder-only LLMs have propelled inverse design forward.</p>
<p>Table 2: Decoder-only scientific LLMs. The release date column displays the date of the first publication for each paper. When available, the publication date of the last updated version is displayed between parentheses. a:"Model Size" is reported as the number of parameters. "PubMed" refer to the PubMed abstracts dataset, while PMC (PubMed Corpus) refers to the full-text corpus dataset. $b$ : The total number of parameters was not reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Model <br> Size $^{a}$</th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Release date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tx-LLM ${ }^{271}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">TDC datasets</td>
<td style="text-align: center;">PaLM-2</td>
<td style="text-align: center;">Property prediction and retrosynthesis</td>
<td style="text-align: center;">2024.06</td>
</tr>
<tr>
<td style="text-align: center;">BioMedLM ${ }^{272}$</td>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">PubMed abstracts and full articles</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">2024.03</td>
</tr>
<tr>
<td style="text-align: center;">LlasMol ${ }^{273}$</td>
<td style="text-align: center;">$\sim 7 B$</td>
<td style="text-align: center;">SMolInstruct</td>
<td style="text-align: center;"><code>Galactica, LLaMa, Mistral</code></td>
<td style="text-align: center;">Property <br> molecule <br> molecule <br> retrosynthesis, <br> conversion</td>
<td style="text-align: center;">$\begin{gathered} \text { prediction, } \ \text { captioning, } \ \text { generation, } \ \text { name } \ \text { conversion } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">BioMistral ${ }^{274}$</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">PubMed Central (PMC)</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">$\begin{gathered} 2024.02 \ (2024.08) \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">BiMediX ${ }^{275}$</td>
<td style="text-align: center;">8x7B</td>
<td style="text-align: center;">1.3M Arabic-English instructions (BiMed)</td>
<td style="text-align: center;">Mixtral</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">2024.02</td>
</tr>
<tr>
<td style="text-align: center;">EpilepsyLLM ${ }^{276}$</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">Data from the Japan Epilepsy Association, Epilepsy Information Center, and Tenkan Net</td>
<td style="text-align: center;">LLaMa</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">2024.01</td>
</tr>
</tbody>
</table>
<p>Table 2 - continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Model <br> Size $^{a}$</th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Release date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CheXagent ${ }^{277}$</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">28 publicly available datasets, including PMC, MIMIC, wikipedia, PadChest, and BIMCV-COVID-19</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">QA, Image understanding</td>
<td style="text-align: center;">2024.01</td>
</tr>
<tr>
<td style="text-align: center;">ChemSpaceAL ${ }^{278}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">ChEMBL 33, GuacaMol v1, MOSES, and BindingDB 08-2023</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2023.09 \ &amp; (2024.02) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">BioMedGPT$\mathrm{LM}^{279}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 7 \mathrm{~B} \ &amp; 10 \mathrm{~B} \end{aligned}$</td>
<td style="text-align: center;">5.5M biomedical papers from S2ORC</td>
<td style="text-align: center;">LLaMA2</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">2023.08</td>
</tr>
<tr>
<td style="text-align: center;">Darwin ${ }^{280}$</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">SciQ and Web of Science</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">QA, Property prediction, NER, and Molecule Generation</td>
<td style="text-align: center;">2023.08</td>
</tr>
<tr>
<td style="text-align: center;">cMolGPT ${ }^{46}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">MOSES</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">2023.05</td>
</tr>
<tr>
<td style="text-align: center;">PMC-LLaMA ${ }^{281}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 7 \mathrm{~B} \ &amp; 13 \mathrm{~B} \end{aligned}$</td>
<td style="text-align: center;">MedC-k and MedC-I</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2023.04 \ &amp; (2024.04) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">GPTChem ${ }^{142}$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">Curation of multiple classification and regression benchmarks</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Property prediction and inverse design</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2023.02 \ &amp; (2024.02) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Galactica ${ }^{123}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 125 \mathrm{M} \ &amp; 1.3 \mathrm{~B}, 6.7 \mathrm{~B} \ &amp; 30 \mathrm{~B}, 120 \mathrm{~B} \end{aligned}$</td>
<td style="text-align: center;">The galactica corpus, a curation with 62B scientific documents</td>
<td style="text-align: center;">Decoderonly</td>
<td style="text-align: center;">QA, NER, Document Summarization, Property Prediction</td>
<td style="text-align: center;">2022.11</td>
</tr>
<tr>
<td style="text-align: center;">BioGPT ${ }^{282}$</td>
<td style="text-align: center;">355 M</td>
<td style="text-align: center;">15M of Title and abstract from PubMed</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">QA, NER, and Document Classification</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022-09 \ &amp; (2023.04) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">SMILES-to-properties-transformer ${ }^{265}$</td>
<td style="text-align: center;">6.5 M</td>
<td style="text-align: center;">Synthetic data generated with the thermodynamic model COSMO-RS</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.06 \ &amp; (2022.09) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ChemGPT ${ }^{283}$</td>
<td style="text-align: center;">$\sim 1 B$</td>
<td style="text-align: center;">10M molecules from PubChem</td>
<td style="text-align: center;">GPT-neo</td>
<td style="text-align: center;">Molecule generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.05 \ &amp; (2023.11) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Regression Transformer ${ }^{139}$</td>
<td style="text-align: center;">$\sim 27 \mathrm{M}$</td>
<td style="text-align: center;">ChEMBL, MoleculeNet, USPTO, etc</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">Property prediction, Molecule tuning, Molecule generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2022.02 \ &amp; (2023.04) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MolGPT ${ }^{284}$</td>
<td style="text-align: center;">6 M</td>
<td style="text-align: center;">MOSES and GuacaMol</td>
<td style="text-align: center;">GPT</td>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">2021.10</td>
</tr>
<tr>
<td style="text-align: center;">Adilov2021 ${ }^{285}$</td>
<td style="text-align: center;">13.4 M</td>
<td style="text-align: center;">5M SMILES from ChemBERTa's PubChem-10M.</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Property prediction and molecule generation</td>
<td style="text-align: center;">2021.09</td>
</tr>
</tbody>
</table>
<h1>3.3.1 Property Directed Inverse Design</h1>
<p>Nature has long been a rich source of molecules that inhibit disease proliferation, because organisms have evolved chemicals for self-defense. Historically, most pharmaceuticals are derived from these natural products, ${ }^{286,287}$ which offer benefits such as cell permeability, target specificity, and a vast chemical diversity. ${ }^{288}$ However, the high costs and complexities associated with high-throughput screening and synthesizing natural products limit the exploration of this space. ${ }^{286,288}$
While natural products have been a valuable starting point, we are not confined to their derivatives. AI, particularly generative LLMs, allows us to go beyond nature and explore a much larger chemical space. In-silico molecular design enables rapid modification, akin to random mutation, ${ }^{289}$ where only valid, synthesizable molecules that meet predefined property criteria remain in the generated set. ${ }^{242,290}$ This approach allows us to test modifications in-silico, expanding exploration beyond the boundaries of natural products.</p>
<p>The true innovation of AI-driven molecular design, however, lies in its ability to directly generate candidate molecules based on desired properties, without the need for iterative stepwise modifications. ${ }^{291}$ This "inverse design" capability allows us to start with a target property and directly generate candidate molecules that meet the predefined property requirements. Generative LLMs applied to sequences of atoms and functional groups offer a powerful opportunity for out-of-the-box exploration, tapping into the vast chemical space that extends far beyond the confines of nature. This accelerates the path from concept to viable therapeutic agents, aligning seamlessly with decoder-only LLM architectures.</p>
<h1>3.3.2 Decoder-only Mol-LLMs</h1>
<p>One of the first applications of decoder-only models in chemistry was Adilov's (2021) "Generative pretraining from Molecules". ${ }^{285}$ This work pretrained a GPT-2-like causal transformer for self-supervised learning using SMILES strings. By introducing "adapters" between attention blocks for task-specific fine-tuning, ${ }^{292}$ this method provided a versatile approach for both molecule generation and property prediction, requiring minimal architectural changes. It aimed to surpass encoder-only models, such as ChemBERTa, ${ }^{44}$ with a more scalable and resource-efficient approach, demonstrating the power of decoder-only models in chemical generation.
A key advancement then came with MolGPT, ${ }^{284}$ a 6-million-parameter decoder-only model designed for molecular generation. MolGPT introduced masked self-attention, enabling the learning of long-range dependencies in SMILES strings. The model ensured chemically valid SMILES representations, respecting structural rules like valency and ring closures. It also utilized salience measures for interpretability, aiding in predicting SMILES tokens and understanding which parts of the molecule were most influential in the model's predictions. MolGPT outperformed many existing Variational Auto-Encoder (VAE)-based approaches, ${ }^{293-300}$ in predicting novel molecules with specified properties, being trained on datasets like MOSES ${ }^{301}$ and GuacaMol. ${ }^{302}$
While MolGPT's computational demands may be higher than traditional VAEs, its ability to generate high-quality, novel molecules justifies this trade-off. MolGPT demonstrated strong performance on key metrics such as validity, which measures the percentage of generated molecules that are chemically valid according to bonding rules; uniqueness, the proportion of generated molecules that are distinct from one another; Frechet ChemNet Distance (FCD), ${ }^{303}$ which compares the distribution of generated molecules to that of real molecules in the training set, indicating how closely the generated molecules resemble real-world compounds; and KL divergence, ${ }^{302}$ a measure of how the probability distribution of generated molecules deviates from the true distribution of the training data. These metrics illustrate MolGPT's ability to generate high-quality, novel molecules while maintaining a balance between diversity and similarity to known chemical spaces. A brief summary of advancements in transformer-based models for de-novo molecule generation from 2023 and 2024 follows, which continue to refine and expand upon the foundational work laid by models like MolGPT.
Haroon et al. ${ }^{304}$ further developed a GPT-based model with relative attention for de novo drug design, showing improved validity, uniqueness, and novelty. This work was followed by Frey et al. ${ }^{283}$, who introduced ChemGPT to explore hyperparameter tuning and dataset scaling in new domains. ChemGPT's contribution lies in refining generative models to better fit specific chemical domains, advancing the understanding of how data scale impacts generative performance. Both Wang et al. ${ }^{305}$ and Mao et al. ${ }^{306}$ presented work that surpassed MolGPT. Furthermore, Mao et al. ${ }^{140}$ showed that decoder-only models could generate novel compounds using IUPAC names directly.
This marked a departure from typical SMILES-based molecular representations, as IUPAC names offer a standardized, human-readable format that aligns with how chemists conceptualize molecular structures. By integrating these chemical semantics into the model, iupacGPT ${ }^{140}$ bridges the gap between computational predictions and real-world chemical applications. The IUPAC name outputs are easier to understand, validate, and apply, facilitating smoother integration into workflows like regulatory filings, chemical databases, and drug design. Focusing on pretraining with a vast dataset of IUPAC names and fine-tuning with lightweight networks, iupacGPT excels in molecule generation, classification, and regression tasks, providing an intuitive interface for chemists in both drug discovery and material science.
In a similar vein, Zhang et al. ${ }^{307}$ proposed including target 3D structural information in molecular generative models, even though their approach is not LLM-based. However, it serves as a noteworthy contribution to the field of structure-based drug design. Integrating biological data, such as 3D protein structures, can significantly improve the relevance and specificity of generated molecules, making this method valuable for future LLM-based drug design. Similarly, Wang et al. ${ }^{308}$ discussed PETrans, a deep learning method that generates target-specific ligands using protein-specific encoding and transfer learning. This study further emphasizes the importance of using transformer models for generating molecules with high binding affinity to specific protein targets. The significance of these works lies in their demonstration that integrating both human-readable formats (like IUPAC names) and biological context (such as protein structures) into generative models can lead to more relevant, interpretable, and target-specific drug</p>
<p>candidates. This reflects a broader trend in AI-driven chemistry to combine multiple data sources for more precise molecular generation, accelerating the drug discovery process.
In 2024, Yoshikai et al. ${ }^{309}$ discussed the limitations of transformer architectures in recognizing chirality from SMILES representations, which impacts the prediction accuracy of molecular properties. To address this, they coupled a transformer with a VAE. Using contrastive learning from NLP to generate new molecules with multiple SMILES representations, enhancing molecular novelty and validity.Kyro et al. ${ }^{278}$ presented ChemSpaceAL, an active learning method for protein-specific molecular generation, efficiently identifying molecules with desired characteristics without prior knowledge of inhibitors. Yan et al. ${ }^{310}$ proposed the GMIA framework, which improves prediction accuracy and interpretability in drug-drug interactions through a graph mutual interaction attention decoder. These innovations represent significant strides in addressing key challenges in molecular generation, such as chirality recognition, molecular novelty, and drug-drug interaction prediction. By integrating new techniques like VAEs, contrastive learning, and active learning into transformer-based models, they have improved both the accuracy and interpretability of molecular design.
Building on these developments, Shen et al. ${ }^{311}$ reported on AutoMolDesigner, an open-source tool for small-molecule antibiotic design, further emphasizing the role of automation in molecular generation. This work serves as a precursor to more complex models, such as Taiga ${ }^{101}$ and cMolGPT, ${ }^{46}$ which employ advanced methods like autoregressive mechanisms and reinforcement learning for molecular generation and property optimization.
For a deeper dive into decoder-only transformer architecture in chemistry, we highlight the May 2023 "Taiga" model by Mazuz et al. ${ }^{101}$, and cMolGPT by Wang et al. ${ }^{46}$. Taiga first learns to map SMILES strings to a vector space, and then refines that space using a smaller, labeled dataset to generate molecules with targeted attributes. It uses an autoregressive mechanism, predicting each SMILES character in sequence based on the preceding ones. For property optimization, Taiga employs the REINFORCE algorithm, ${ }^{106}$ which helps refine molecules to enhance specific features. While this reinforcement learning (RL) approach may slightly reduce molecular validity, it significantly improves the practical applicability of the generated compounds. Initially evaluated using the Quantitative Estimate of Drug-likeness (QED) metric, ${ }^{312}$ Taiga has also demonstrated promising results in targeting IC50 values, ${ }^{168}$ the BACE protein, ${ }^{313}$ and anti-cancer activities they collected from a variety of sources. This work underscores the importance of using new models to address applications that require a higher level of chemical sophistication, to illustrate how such models could ultimately be applied outside of the available benchmark datasets. It also builds on the necessary use of standardized datasets and train-validation-test splitting, to demonstrate progress, as explained by Wu et al. ${ }^{56}$. Yet, even the MoleculeNet benchmarks ${ }^{56}$ are flawed, and we point the reader here to a more detailed discussion on benchmarking, ${ }^{188}$ given that a significant portion of molecules in the BACE dataset have undefined stereo centers, which, at a deeper level, complicates the modeling and prediction accuracy.
While models like Taiga demonstrate the power of autoregressive learning and reinforcement strategies to generate molecules with optimized properties, the next step in molecular design incorporates deeper chemical domain knowledge. This approach is exemplified by Wang et al. ${ }^{46}$. They introduced cMolGPT, a conditional generative model that brings a more targeted focus to drug discovery by integrating specific protein-ligand interactions, which underscores the importance of incorporating chemical domain knowledge to effectively navigate the vast landscape of drug-like molecules. Using self-supervised learning and an auto-regressive approach, cMolGPT generates SMILES guided by predefined conditions based on target proteins and binding molecules. Initially trained on the MOSES dataset ${ }^{301}$ without target information, the model is fine-tuned with embeddings of protein-binder pairs, focusing on generating compound libraries and target-specific molecules for the EGFR, HTR1A, and S1PR1 protein datasets. ${ }^{314-317}$
Their approach employs a QSAR model ${ }^{5}$ to predict the activity of generated compounds, achieving a Pearson correlation coefficient over 0.75 . However, despite the strong predictive capabilities, this reliance on a QSAR model, with its own inherent limitations, highlights the need for more extensive experimental datasets. cMolGPT ${ }^{46}$ tends to generate molecules within the sub-chemical space represented in the original dataset, successfully identifying potential binders but struggling to broadly explore the chemical space for novel solutions. This underscores the challenge of generating diverse molecules with varying structural characteristics while maintaining high binding affinity to specific targets. While cMolGPT advances the integration of biological data and fine-tuned embeddings for more precise molecular generation, models like Taiga and cMolGPT differ in their approach. Taiga ${ }^{101}$ employs reinforcement learning to optimize generative models for molecule generation, while cMolGPT uses target-specific embeddings to guide the design process. Both highlight the strengths of decoder-only models but emphasize distinct strategies; Taiga optimizes molecular properties through autoregressive learning, and cMolGPT focuses on conditional generation based on protein-ligand interactions.
In contrast, Yu et al. ${ }^{273}$ follow a different approach with LlaSMol, ${ }^{273}$ which utilizes pretrained models (for instance Galactica, LlaMa2, and Mistral) and performs parameter efficient fine-tuning (PEFT) techniques ${ }^{318,319}$ such as LoRa. ${ }^{320}$ PEFT enables fine-tuning large language models with fewer parameters, making the process more resource-efficient</p>
<p>while maintaining high performance. LlaSMol demonstrated its potential by achieving state-of-the-art performance in property prediction tasks, particularly when fine-tuned on benchmark datasets like MoleculeNet. ${ }^{56}$
There continue to be significant advancements being made in using transformer-based models to tackle chemical prediction tasks with optimized computational resources, including more generalist models, such as Tx-LLM, ${ }^{271}$ designed to streamline the complex process of drug discovery. For additional insights on how these models are shaping the field, we refer the reader to several excellent reviews, ${ }^{164,321-323}$ with Goel et al. ${ }^{324}$ highlighting the efficiency of modern machine learning methods in sampling drug-like chemical space for virtual screening and molecular design. Goel et al. ${ }^{324}$ discussed the effectiveness of generative models, including large language models (LLMs), in approximating the vast chemical space, particularly when conditioned on specific properties or receptor structures.
We provide a segue from this section by introducing the work by Jablonka et al. ${ }^{142}$, which showcases a decoderonly GPT model that, despite its training on natural language rather than specialized chemical languages, competes effectively with decoder-only LLMs tailored to chemical languages. The authors finetuned GPT-3 to predict properties and conditionally generate molecules and, therefore, highlight its potential as a foundational tool in the field. This work sets the stage for integrating natural language decoder-only LLMs, like GPT, into chemical research, where they could serve as central hubs for knowledge discovery.
Looking ahead, this integration foreshadows future developments that pair LLMs with specialized tools to enhance their capabilities, paving the way for the creation of autonomous agents that leverage deep language understanding in scientific domains. Decoder-only models have already significantly advanced inverse molecular design, from improving property prediction to enabling target-specific molecular generation. Their adaptability to various chemical tasks demonstrates their value in optimizing drug discovery processes and beyond. As models like LlaSMol and cMolGPT continue to evolve, integrating chemical domain knowledge and biological data, they offer exciting opportunities for more precise molecular generation. The growing potential for combining large language models like GPT-4 with specialized chemical tools signals a future where AI-driven autonomous agents could revolutionize chemical research, making these models indispensable to scientific discovery.</p>
<h1>3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs</h1>
<p>The encoder-decoder architecture is designed for tasks involving the translation of one sequence into another, making it ideal for predicting chemical reaction outcomes or generating synthesis pathways from given reactants. We begin with a background on optimal synthesis prediction and describe how earlier machine learning has approached this challenge. Following that, we explain how LLMs have enhanced chemical synthesis prediction and optimization. Although, our context below is aptly chosen to be synthesis prediction, other applications exist. For example, SMILES Transformer $(\mathrm{ST})^{325}$ is worth a mention, historically, because it explored the benefits of self-supervised pretraining to produce continuous, data-driven molecular fingerprints from large SMILES-based datasets.</p>
<h3>3.4.1 Synthesis Prediction</h3>
<p>Once a molecule has been identified through property-directed inverse design, the next challenge is to predict its optimal synthesis, including yield. Shenvi ${ }^{332}$ describe how the demanding and elegant syntheses of natural products has contributed greatly to organic chemistry. However, in the past 20 years, the focus has shifted away from complex natural product synthesis towards developing new reactions applicable for a broader range of compounds, especially in reaction catalysis. ${ }^{332}$ Yet, complex synthesis is becoming relevant again as it can be digitally encoded, mined by LLMs, ${ }^{333}$ and applied to new challenges. Unlike property prediction, reaction prediction is particularly challenging due to the involvement of multiple molecules. Modifying one reactant requires adjusting all others, with different synthesis mechanisms or conditions likely involved. Higher-level challenges exist for catalytic reactions and complex natural product synthesis. Synthesis can be approached in two ways. Forward synthesis involves building complex target molecules from simple, readily available substances, planning the steps progressively. Retrosynthesis, introduced by E.J. Corey in 1988, ${ }^{334}$ is more common. It involves working backward from the target molecule, breaking it into smaller fragments whose re-connection is most effective. Chemists choose small, inexpensive, and readily available starting materials to achieve the greatest yield and cost-effectiveness. As a broad illustration, the first total synthesis of discodermolide ${ }^{335}$ involved 36 such steps, a 24 -step longest linear sequence, and a $3.2 \%$ yield. There are many possible combinations for the total synthesis of the target molecule, and the synthetic chemist must choose the most sensible approach based on their expertise and knowledge. However, this approach to total synthesis takes many years. LLMs can now transform synthesis such that structure-activity relationship predictions can be coupled in lock-step with molecule selection based on easier synthetic routes. This third challenge of predicting the optimal synthesis can also lead to the creation of innovative, non-natural compounds, chosen because of such an easier predicted synthesis but for which the properties are still predicted to meet the needs of the application. Thus, these three challenges introduced above are interconnected.</p>
<p>Table 3: Encoder-decoder scientific LLMs. The release date column displays the date of the first publication for each paper. When available, the publication date of the last updated version is displayed between parentheses. $a$ : "Model Size" is reported as the number of parameters. $b$ : The total number of parameters was not reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Model <br> Size $^{a}$</th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Release date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BioT5+ ${ }^{117}$</td>
<td style="text-align: center;">252 M</td>
<td style="text-align: center;">ZINC20, UniRef50, 33M PubMed articles, 339 K mol-text pairs from PubChem, 569K FASTA-text pairs from Swiss-prot</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Molecule Captioning, <br> Molecule Generation, <br> Property Prediction,</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2024.02 } \ &amp; \text { (2024.08) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">nach0 ${ }^{187}$</td>
<td style="text-align: center;">250 M</td>
<td style="text-align: center;">MoleculeNet, USPTO, ZINC</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Property prediction, <br> Molecule generation, <br> Question answering, NER</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2023.11 } \ &amp; \text { (2024.05) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ReactionT5 ${ }^{326}$</td>
<td style="text-align: center;">220 M</td>
<td style="text-align: center;">ZINC and ORD</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Property prediction and Reaction prediction</td>
<td style="text-align: center;">2023.11</td>
</tr>
<tr>
<td style="text-align: center;">BioT5 ${ }^{116}$</td>
<td style="text-align: center;">252 M</td>
<td style="text-align: center;">ZINC20, UniRef50, full-articles from BioRxiv and PubMed, mol-text-IUPAC information from PubChem</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Molecule Captioning, Property Prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2023-10 } \ &amp; \text { (2024.12) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MOLGEN ${ }^{327}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">ZINC15</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2023.01 } \ &amp; \text { (2024.03) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Text+Chem T5 ${ }^{328}$</td>
<td style="text-align: center;">60M, 220M</td>
<td style="text-align: center;">11.5 M or 33.5 M samples curated from Vaucher et al. ${ }^{329}$, Toniato et al. ${ }^{247}$, and CheBI-20</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Molecule Captioning, Product Prediction, Retrosynthesis, Molecule Generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2023.01 } \ &amp; \text { (2023.06) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MolT5 ${ }^{330}$</td>
<td style="text-align: center;">60M, 770M</td>
<td style="text-align: center;">C4 dataset</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Molecule Captioning and Molecule Generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2022.04 } \ &amp; \text { (2022.12) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">T5Chem ${ }^{179}$</td>
<td style="text-align: center;">220 M</td>
<td style="text-align: center;">USPTO</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Product Prediction, Retrosynthesis, Property Prediction</td>
<td style="text-align: center;">2022.03</td>
</tr>
<tr>
<td style="text-align: center;">Text2Mol ${ }^{331}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">CheBI-20</td>
<td style="text-align: center;">SciBERT w/ decoder</td>
<td style="text-align: center;">Molecule captioning and conditional molecule generation</td>
<td style="text-align: center;">2021.11</td>
</tr>
<tr>
<td style="text-align: center;">ChemFormer ${ }^{185}$</td>
<td style="text-align: center;">45M, 230M</td>
<td style="text-align: center;">100M SMILES from ZINC-15</td>
<td style="text-align: center;">BART</td>
<td style="text-align: center;">Product Prediction, Property Prediction, Molecular Generation</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2021.07 } \ &amp; \text { (2022.01) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">SMILES transformer ${ }^{325}$</td>
<td style="text-align: center;">$b$</td>
<td style="text-align: center;">ChEMBL24</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">Property prediction</td>
<td style="text-align: center;">2019.11</td>
</tr>
<tr>
<td style="text-align: center;">Molecular Transformer ${ }^{255}$</td>
<td style="text-align: center;">12 M</td>
<td style="text-align: center;">USPTO</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">Product prediction</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { 2018.11 } \ &amp; \text { (2019.08) } \end{aligned}$</td>
</tr>
</tbody>
</table>
<h1>3.4.2 Encoder-decoder mol-LLMs</h1>
<p>Before we focus on transformer use, some description is provided on the evolution from RNN and Gated Recurrent Unit (GRU) approaches in concert with the move from template-based to semi-template-based to template-free models. Nam and Kim ${ }^{336}$ pioneered forward synthesis prediction using a GRU-based translation model. In contrast, Liu et al. ${ }^{337}$ reported retro-synthesis prediction with a Long Short-Term Memory (LSTM) based seq2seq model incorporating an attention mechanism, achieving $37.4 \%$ accuracy on the USPTO-50K dataset. The reported accuracies of these early models highlighted the challenges of synthesis prediction, particularly retrosynthesis. Schneider et al. ${ }^{338}$ further advanced retrosynthesis by assigning reaction roles to reagents and reactants based on the product.
Building on RNNs and GRUs, the field advanced with the introduction of template-based models. In parallel with the development of the Chematica tool ${ }^{339,340}$ for synthesis mapping, Segler and Waller ${ }^{341}$ highlighted that traditional rule-based systems often failed by neglecting molecular context, leading to "reactivity conflicts." Their approach emphasized transformation rules that capture atomic and bond changes, applied in reverse for retrosynthesis. Trained on 3.5 million reactions, their model achieved $95 \%$ top-10 accuracy in retrosynthesis and $97 \%$ for reaction prediction on a validation set of nearly 1 million reactions from the Reaxys database (1771-2015). Although not transformer-based, this work laid the foundation for large language models (LLMs) in synthesis. However, template-based models depend on</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6499 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6499</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6499</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277501721</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01857v1.pdf" target="_blank">Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLC's superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6499.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6499.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLC (bilingual) - DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual Consistency (bilingual) applied to DeepSeek-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time ensemble that translates the problem into multiple languages, samples chain-of-thought reasoning paths per language, and uses majority voting across language-specific candidate answers; here applied to DeepSeek-Math-7B-Instruct on CMATH (Chinese math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cross-Lingual Consistency (bilingual-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CMATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical problem solving (math reasoning, original dataset in Chinese)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>86.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>self-consistency (monolingual Chinese and English)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>9.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Bilingual CLC integrates Chinese and English reasoning paths, increasing coverage of correct reasoning patterns and neutralizing monolingual linguistic biases; at the 10th epoch CLC achieved 86.8% (reported +1.0% vs Chinese self-consistency and +9.5% vs English self-consistency). The method is described as enabling exploration of a broader multilingual solution space and overriding incorrect monolingual predictions via majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6499.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6499.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLC (bilingual) - Qwen2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual Consistency (bilingual) applied to Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bilingual cross-lingual ensemble using Chinese and English CoT samples with majority voting, applied to Qwen2.5-Math-7B-Instruct on CMATH to mitigate language-specific bias and improve math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cross-Lingual Consistency (bilingual-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CMATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical problem solving (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>87.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>self-consistency (monolingual Chinese and English)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Qwen2.5 showed stronger English self-consistency than Chinese; bilingual CLC further improved results when monolingual self-consistency had plateaued, attributed to ensemble pattern-coverage across languages reducing the impact of monolingual local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6499.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6499.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLC (bilingual) - Gemma2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual Consistency (bilingual) applied to Gemma2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bilingual cross-lingual ensemble (Chinese+English) sampling CoT answers and majority-voting to select final math answers; evaluated on CMATH and MGSM for Gemma2-9B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cross-Lingual Consistency (bilingual-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CMATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>77.6</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>self-consistency (monolingual Chinese and English)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On CMATH at the 10th epoch bilingual CLC reached 77.6%, which is +2.4% over Chinese self-consistency (75.2%) and +6.0% over English self-consistency (71.6%). The authors emphasize that even when monolingual performances are similar across languages, multilingual/bilingual ensembles can still yield gains by escaping monolingual reasoning traps and neutralizing language-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6499.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6499.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLC (multilingual) - Gemma2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual Consistency (multilingual) applied to Gemma2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilingual CLC ensembles CoT reasoning across up to 11 languages (English, Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, Telugu), aggregates candidate answers with majority voting, and was exhaustively analyzed for language-set effects on MGSM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cross-Lingual Consistency (multilingual-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MGSM</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multilingual multi-step mathematical problem solving (MGSM dataset written in 11 languages)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (mean over post-20th-epoch iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>92.09</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>monolingual self-consistency (per-language)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors conducted exhaustive enumeration of language combinations (2,047 non-empty subsets) and report that mean accuracy peaks at an optimal 6-language set (Chinese, English, Bengali, Spanish, Russian, Thai) with mean accuracy 92.09%; this yields absolute gains of 4.1% to 18.5% over monolingual self-consistency (which ranged 73.6%–88.0%). They explain gains by 'pattern coverage enhancement' (multilingual ensemble overrides incorrect monolingual answers) and note a 'probabilistic consensus degradation' beyond a critical cardinality (n>6) where added languages can introduce conflicting predictions that reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6499.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6499.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled chain-of-thought reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed method that improves CoT by sampling multiple reasoning chains (via non-greedy decoding) and aggregating candidate final answers via majority voting; used here as the primary monolingual baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Math-7B-Instruct / Qwen2.5-Math-7B-Instruct / Gemma2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 9B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous (monolingual sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CMATH / MGSM (per-language evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical reasoning (per-language CoT sampling aggregated via majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Cross-Lingual Consistency (CLC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-consistency improves over single-sample CoT by sampling multiple chains, but is limited by monolingual training data biases and local optima in sampled reasoning paths—limitations that CLC is designed to mitigate by ensembling across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Universal Self-Consistency for Large Language Model Generation <em>(Rating: 2)</em></li>
                <li>Ensemble methods in machine learning <em>(Rating: 2)</em></li>
                <li>Probabilistic Consensus through Ensemble Validation: A Framework for LLM Reliability <em>(Rating: 2)</em></li>
                <li>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6499",
    "paper_id": "paper-277501721",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "CLC (bilingual) - DeepSeek",
            "name_full": "Cross-Lingual Consistency (bilingual) applied to DeepSeek-Math-7B-Instruct",
            "brief_description": "An inference-time ensemble that translates the problem into multiple languages, samples chain-of-thought reasoning paths per language, and uses majority voting across language-specific candidate answers; here applied to DeepSeek-Math-7B-Instruct on CMATH (Chinese math problems).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Math-7B-Instruct",
            "model_size": "7B",
            "reasoning_method_name": "Cross-Lingual Consistency (bilingual-consistency)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "CMATH",
            "task_description": "multi-step mathematical problem solving (math reasoning, original dataset in Chinese)",
            "performance_metric": "accuracy",
            "performance_value": 86.8,
            "comparison_target_method": "self-consistency (monolingual Chinese and English)",
            "performance_difference": 9.5,
            "statistical_significance": false,
            "analysis_notes": "Bilingual CLC integrates Chinese and English reasoning paths, increasing coverage of correct reasoning patterns and neutralizing monolingual linguistic biases; at the 10th epoch CLC achieved 86.8% (reported +1.0% vs Chinese self-consistency and +9.5% vs English self-consistency). The method is described as enabling exploration of a broader multilingual solution space and overriding incorrect monolingual predictions via majority voting.",
            "ablation_study_present": false,
            "uuid": "e6499.0",
            "source_info": {
                "paper_title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CLC (bilingual) - Qwen2.5",
            "name_full": "Cross-Lingual Consistency (bilingual) applied to Qwen2.5-Math-7B-Instruct",
            "brief_description": "Bilingual cross-lingual ensemble using Chinese and English CoT samples with majority voting, applied to Qwen2.5-Math-7B-Instruct on CMATH to mitigate language-specific bias and improve math reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-7B-Instruct",
            "model_size": "7B",
            "reasoning_method_name": "Cross-Lingual Consistency (bilingual-consistency)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "CMATH",
            "task_description": "multi-step mathematical problem solving (math reasoning)",
            "performance_metric": "accuracy",
            "performance_value": 87.3,
            "comparison_target_method": "self-consistency (monolingual Chinese and English)",
            "performance_difference": 6.5,
            "statistical_significance": false,
            "analysis_notes": "Qwen2.5 showed stronger English self-consistency than Chinese; bilingual CLC further improved results when monolingual self-consistency had plateaued, attributed to ensemble pattern-coverage across languages reducing the impact of monolingual local optima.",
            "ablation_study_present": false,
            "uuid": "e6499.1",
            "source_info": {
                "paper_title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CLC (bilingual) - Gemma2",
            "name_full": "Cross-Lingual Consistency (bilingual) applied to Gemma2-9B-Instruct",
            "brief_description": "Bilingual cross-lingual ensemble (Chinese+English) sampling CoT answers and majority-voting to select final math answers; evaluated on CMATH and MGSM for Gemma2-9B-Instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma2-9B-Instruct",
            "model_size": "9B",
            "reasoning_method_name": "Cross-Lingual Consistency (bilingual-consistency)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "CMATH",
            "task_description": "multi-step mathematical problem solving",
            "performance_metric": "accuracy",
            "performance_value": 77.6,
            "comparison_target_method": "self-consistency (monolingual Chinese and English)",
            "performance_difference": 6.0,
            "statistical_significance": false,
            "analysis_notes": "On CMATH at the 10th epoch bilingual CLC reached 77.6%, which is +2.4% over Chinese self-consistency (75.2%) and +6.0% over English self-consistency (71.6%). The authors emphasize that even when monolingual performances are similar across languages, multilingual/bilingual ensembles can still yield gains by escaping monolingual reasoning traps and neutralizing language-specific biases.",
            "ablation_study_present": false,
            "uuid": "e6499.2",
            "source_info": {
                "paper_title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CLC (multilingual) - Gemma2",
            "name_full": "Cross-Lingual Consistency (multilingual) applied to Gemma2-9B-Instruct",
            "brief_description": "Multilingual CLC ensembles CoT reasoning across up to 11 languages (English, Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, Telugu), aggregates candidate answers with majority voting, and was exhaustively analyzed for language-set effects on MGSM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma2-9B-Instruct",
            "model_size": "9B",
            "reasoning_method_name": "Cross-Lingual Consistency (multilingual-consistency)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "MGSM",
            "task_description": "multilingual multi-step mathematical problem solving (MGSM dataset written in 11 languages)",
            "performance_metric": "accuracy (mean over post-20th-epoch iterations)",
            "performance_value": 92.09,
            "comparison_target_method": "monolingual self-consistency (per-language)",
            "performance_difference": 4.1,
            "statistical_significance": false,
            "analysis_notes": "Authors conducted exhaustive enumeration of language combinations (2,047 non-empty subsets) and report that mean accuracy peaks at an optimal 6-language set (Chinese, English, Bengali, Spanish, Russian, Thai) with mean accuracy 92.09%; this yields absolute gains of 4.1% to 18.5% over monolingual self-consistency (which ranged 73.6%–88.0%). They explain gains by 'pattern coverage enhancement' (multilingual ensemble overrides incorrect monolingual answers) and note a 'probabilistic consensus degradation' beyond a critical cardinality (n&gt;6) where added languages can introduce conflicting predictions that reduce performance.",
            "ablation_study_present": true,
            "uuid": "e6499.3",
            "source_info": {
                "paper_title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-consistency (baseline)",
            "name_full": "Self-Consistency (majority-vote over sampled chain-of-thought reasoning paths)",
            "brief_description": "A previously proposed method that improves CoT by sampling multiple reasoning chains (via non-greedy decoding) and aggregating candidate final answers via majority voting; used here as the primary monolingual baseline.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Math-7B-Instruct / Qwen2.5-Math-7B-Instruct / Gemma2-9B-Instruct",
            "model_size": "7B / 9B",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "homogeneous (monolingual sampling)",
            "benchmark_name": "CMATH / MGSM (per-language evaluations)",
            "task_description": "multi-step mathematical reasoning (per-language CoT sampling aggregated via majority vote)",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "Cross-Lingual Consistency (CLC)",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Self-consistency improves over single-sample CoT by sampling multiple chains, but is limited by monolingual training data biases and local optima in sampled reasoning paths—limitations that CLC is designed to mitigate by ensembling across languages.",
            "ablation_study_present": false,
            "uuid": "e6499.4",
            "source_info": {
                "paper_title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Universal Self-Consistency for Large Language Model Generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Ensemble methods in machine learning",
            "rating": 2,
            "sanitized_title": "ensemble_methods_in_machine_learning"
        },
        {
            "paper_title": "Probabilistic Consensus through Ensemble Validation: A Framework for LLM Reliability",
            "rating": 2,
            "sanitized_title": "probabilistic_consensus_through_ensemble_validation_a_framework_for_llm_reliability"
        },
        {
            "paper_title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            "rating": 1,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        }
    ],
    "cost": 0.01317775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models</p>
<p>Zhiwei Yu yuzhiwei01@inspur.com 
Shandong Inspur Artificial Intelligence Research Institute Co., Ltd. Jinan 100089
ShandongChina</p>
<p>Shandong Yunhai Guochuang Cloud Computing Equipment Industry Innovation Center Co., Ltd
100089Jinan, ShandongChina</p>
<p>Tuo Li 
Shandong Inspur Artificial Intelligence Research Institute Co., Ltd. Jinan 100089
ShandongChina</p>
<p>Shandong Yunhai Guochuang Cloud Computing Equipment Industry Innovation Center Co., Ltd
100089Jinan, ShandongChina</p>
<p>Changhong Wang 
Shandong Inspur Artificial Intelligence Research Institute Co., Ltd. Jinan 100089
ShandongChina</p>
<p>Shandong Yunhai Guochuang Cloud Computing Equipment Industry Innovation Center Co., Ltd
100089Jinan, ShandongChina</p>
<p>Hui Chen 
BNRist
Tsinghua University
Haidian District100084BeijingChina</p>
<p>Lang Zhou 
Shandong Inspur Artificial Intelligence Research Institute Co., Ltd. Jinan 100089
ShandongChina</p>
<p>Shandong Yunhai Guochuang Cloud Computing Equipment Industry Innovation Center Co., Ltd
100089Jinan, ShandongChina</p>
<p>Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models
F92EDF1AF84B41DE39728E5D263D79DD
Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance.However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks.To overcome these constraints, we propose the cross-lingual consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMs' reasoning capabilities.Empirical evaluations on the CMATH dataset reveal CLC's superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively.Expanding CLC's linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space.This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines,</p>
<p>as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset.</p>
<p>Introduction</p>
<p>The emergence of large language models (LLMs) has revolutionized natural language processing (NLP) capabilities across diverse applications (Minaee et al., 2024;Naveed et al., 2024).A critical component maximizing LLM efficacy lies in prompt engineering (Brown et al., 2020;Radford et al., 2019), which involves designing and refining the input prompts to elicit more accurate, relevant, and focused outputs.Among recent breakthroughs, chain-of-thought (CoT) prompting (Wei et al., 2023) has emerged as a transformative strategy.</p>
<p>CoT prompting has proven particularly valuable in reasoning tasks that inherently demand multi-step logical inference to derive accurate conclusions.Traditional large language models (LLMs) demonstrate the capability to generate immediate responses, yet fundamentally lack the capacity to externalize their internal reasoning pathways or justify conclusions through explicit logical progression.The CoT methodology bridges this capability gap by structurally guiding models to articulate intermediate reasoning steps, thereby significantly enhancing performance on complex multi-step tasks requiring systematic analysis.Furthermore, the self-consistency technique (Wang X. et al., 2023) employs sampling strategies such as beam search and nucleus sampling (Holtzman et al., 2020) instead of greedy decoding, which has been empirically shown to amplify CoT's effectiveness in domains requiring rigorous deductive processesincluding mathematical reasoning, symbolic logic operations, and causal inference (Chen et al., 2023;P. Wang et al., 2023;Shao et al., 2024).Despite these advancements, several challenges remain in the reasoning domain.</p>
<p>Firstly, although CoT prompting has demonstrated substantial improvements in multistep reasoning tasks, models exhibit constrained generalization capacity when handling novel complexity scenarios.LLMs still face difficulties when tasked with reasoning about unfamiliar domains or situations not well-represented in the training data (Stechly et al., 2025).Secondly, while the self-consistency methodology enhances models' performance by sampling the reasoning paths over breadth, issues of reasoning incorrectness persist (Chen et al., 2023;P. Wang et al., 2023), particularly when most of the sampled reasoning paths trapped in local optima.Notably, small-scale LLMs are particularly susceptible to the inherent linguistic biases present in multilingual training corpora.As a result, they often exhibit semantic drift and logical inconsistencies, leading to poor performance on low-resource languages (Lample &amp; Conneau, 2019;Xu et al., 2024;Bajpai &amp; Chakraborty, 2025).This correlates with evidence that sub-10B parameters LLMs have limited capacity and struggle with sampling deep reasoning paths (Kaplan et al., 2020;Wei et al., 2023).This paper aims to address these limitations by proposing a novel inference framework, named cross-lingual consistency (CLC), to improve the reasoning capabilities of LLMs.Specifically, we substitute the traditional CoT prompts in monolingual self-consistency with multilingual CoT ensembles, where parallel reasoning paths across diverse languages undergo majority voting.Empirical results demonstrate that CLC achieves 4.1%-18.5% absolute accuracy gains on the MGSM benchmark compared to monolingual self-consistency baselines, with particularly pronounced improvements in low-resource language scenarios.Through the CLC inference framework, we hope to push the boundaries of LLMs' reasoning capabilities and pave the way for more accurate, reliable, and interpretable AI systems.</p>
<p>Method</p>
<p>We propose an inference framework named CLC, as shown in Figure 1.Given the original textual sequence x representing the problem to be reasoned, the proposed framework consists of three steps as follows: (1) Initially, the problem x is translated and expanded into multiple language versions.For instance, if the input problem is in Chinese, it is translated into English and French.The selection of target languages primarily depends on the language repertoire of the specified LLM.It is noteworthy that the prompt templates employed in this study are also translated into corresponding languages to ensure compatibility with the input problems.(2) Subsequently, the LLM is prompted to generate multiple candidate answers for the problem in each selected language, thereby yielding a diverse set of potential solutions.(3) The generated candidate answers are then aggregated and statistically analyzed to determine the final answer.For deterministic tasks, where the problem has a unique solution (e.g., mathematical problems), majority vote is used as the decision rule.Specifically, all generated candidate answers are consolidated into a single set.The frequency of each candidate answer within this set is tallied, and the answer with the highest frequency is selected as the final solution.For non-deterministic tasks, where the problem admits multiple valid solutions (e.g., text summarization), a semantic clustering approach or an LLM can be employed to determine the final answer.Taking semantic clustering as an example, the process involves clustering the semantic vectors of the candidate answers and selecting the answer that is semantically closest to the centroid of the most populous cluster as the final solution.</p>
<p>Figure 1.Overview of the cross-lingual consistency framework.The question x that requires reasoning is firstly expanded into different language versions through translation.For example, if the original input question is in Chinese, it will be translated into English and French.Secondly, the LLM explores diverse reasoning paths and generate candidate answers in diverse languages.Finally, the Decision maker decides the final answer.</p>
<p>Experiment</p>
<p>Experiment Setup</p>
<p>Datasets.We evaluated CLC inference framework on two math datasets, including CMATH and MGSM.The original CMATH dataset is in Chinese, so we used Google Translate to translate the questions of CMATH in English and evaluated the bilingual-consistency inference framework.We later expanded bilingual-consistency to multilingual-consistency framework on the MGSM dataset with 11 languages, including English, Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu.</p>
<p>Models.Since small-scale LLMs face challenges in exploring reasoning paths at sufficient depth and are sensitive to linguistic biases in multilingual training corpora, it is likely to boost performance of small-scale LLMs using CLC which samples the reasoning paths over breadth and makes full use of training data in different languages.</p>
<p>Therefore, this paper mainly focuses on small-scale open-source models, including Qwen2.5-Math-7B-Instruct,Gemma2-9B-Instruct, and DeepSeek-Math-7B-Instruct.</p>
<p>In order to accelerate inference, vLLM framework was deployed and model weights were quantized to 16-bit precision (using float16 for DeepSeek-Math-7B-Instruct and bfloat16 for Qwen2.5-Math-7B-Instruct and Gemma2-9B-Instruct). (2) Chinese Prompt:</p>
<p>Prompts
Question\n 请通过逐步推理来解答问题， 并把最终答案放置于\boxed{}中。 Gemma2-9B-</p>
<p>Instruct</p>
<p>(1) English Prompt: Question\n Please reason step by step, and put your final answer within \boxed{}.</p>
<p>(2) Chinese Prompt: Question\n 请通过逐步推理来解答问题， 并把最终答案放置于\boxed{}中。</p>
<p>(3) Spanish Prompt: Question\n Razone paso a paso y coloque su respuesta final dentro de \boxed{}.Table 1.Zero-shot CoT prompts for DeepSeek-Math-7B-Instruct and Gemma2-9B-Instruct.</p>
<p>Results</p>
<p>Bilingual-Consistency</p>
<p>The effectiveness of CLC was initially verified in the case of two languages (Chinese and English), i.e., bilingual-consistency.Figure 2 shows the performance of DeepSeek-Math-7B-Instruct on CMATH using bilingual-consistency, traditional selfconsistency and sampling without consistency.Sampling Chinese reasoning paths without consistency shows an accuracy range of 76.3% to 78.5% over 10 epochs, while sampling English reasoning paths without consistency shows an accuracy range of 50.0% to 54.7% over 10 epochs.Hence, compared with English reasoning, DeepSeek-Math-7B-Instruct is much better at Chinese reasoning on CMATH.The sampling answers over 10 epochs were then integrated to obtain the traditional self-consistency results using majority voting.The accuracy curves of self-consistency in Chinese and English exhibit a similar trend: a rapid initial rise during the early training epochs, followed by a gradual deceleration in improvement.As demonstrated in previous research (Wang X. et al., 2023), self-consistency does boost the performance of CoT prompts, leading to 85.8% and 77.3% accuracy at the 10th epoch in Chinese and English, respectively.In other words, during odd epochs of bilingual-consistency, the newly added answers come from Chinese-sampling, while during even epochs, the newly added answers come from English-sampling.</p>
<p>As the number of epochs increases, the accuracy of bilingual-consistency exhibits a two-stage pattern.In the first stage (from the 1st to the 7th epoch), the accuracy increases quickly from 78.0% to 87.0%, while in the second stage (from the 7th to the 20th epoch), the accuracy basically stabilizes, fluctuating between 86.7% and 87.5%.</p>
<p>Take the 10th epoch as a comparison point, bilingual-consistency achieves an accuracy of 86.8%, demonstrating a significant accuracy improvement compared to selfconsistency in Chinese (+1.0%) and self-consistency in English (+9.5%) on CMATH using DeepSeek-Math-7B-Instruct. The effectiveness of bilingual-consistency was validated on a broader range of LLMs and datasets.Figure 3 shows the accuracy results of bilingual-consistency, traditional self-consistency and sampling without consistency using Qwen2.5-Math-7B-Instruct on CMATH.In contrast to DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instructexhibits better reasoning ability on CMATH in English than in Chinese.</p>
<p>When using Chinese-sampling without consistency, Qwen2.5-Math-7B-Instructscores 67.2%-71.3%over 10 epochs, whereas it achieves 84.8-86.7%over 10 epochs with English-sampling without consistency.Similar to DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct can achieve higher accuracy when self-consistency is deployed, leading to 83.8% and 87.5% accuracy at the 10th epoch in Chinese and English, respectively.More importantly, the effectiveness of self-consistency in English is quite weak with only +0.8% improvement compared to English-sampling without consistency.Therefore, it is quite impressive that bilingual-consistency can further enhance the reasoning ability of Qwen2.5-Math-7B-Instruct when selfconsistency in English appears to have hit its performance bottleneck.As shown in   It is intuitive that the smaller the performance gap between Chinese and English is, the less effective bilingual-consistency will be.However, the results of Gemma2-9B-Instruct on MGSM show that this is not the case.As shown in Figure 5, Chinese-sampling without consistency and English-sampling without consistency exhibit similar reasoning performance on MGSM using Gemma2-9B-Instruct.Specifically, the average accuracy of Chinese-sampling without consistency is 69.4%, while the average accuracy of English-sampling without consistency is 69.5%.As expected, selfconsistency can increase the accuracy compared with sampling.Take the 10th epoch as a comparison point, self-consistency in Chinese achieves an accuracy of 75.2% which is +4.0%higher than that of Chinese-sampling without consistency.Self-consistency in English scores 71.6% which is +2.8% higher than English-sampling without consistency.Moreover, it is surprising that bilingual-consistency is still effective given that Chinese-sampling and English-sampling without consistency score almost the same.Bilingual-consistency achieves a accuracy of 77.6% at the 10th epoch, which is +2.4% higher than self-consistency in Chinese (75.2%) and +6.0% higher than selfconsistency in English (71.6%).</p>
<p>Multilingual-Consistency</p>
<p>The effectiveness of CLC was also verified under the condition of multiple languages.MGSM, a dataset written in 11 languages, is suitable for the multilingualconsistency task.Figure 6(a) shows the accuracy comparison between multilingualconsistency and traditional self-consistency in each individual language, including Chinese, English, Bengali, German, Spanish, French, Japanese, Russian, Swahili, Telugu, and Thai, on MGSM using Gemma2-9B-Instruct.Among the 11 languages, self-consistency in English achieves the highest accuracy of 88.0% at the 10th epoch, followed by self-consistency in Spanish (87.6%),Russian (86.8%),French (82.4%),German (82.4%),Thai (81.6%),Chinese (80.0%),Swahili (80.0%),Bengali (76.8%),Japanese (75.6%), and Telugu (73.6%).The accuracy disparity likely stems from data sparsity challenges in low-resource languages like Telugu and Bengali, where limited training corpora and insufficient multilingual alignment hinder effective learning, whereas high-resource languages (English/Spanish) benefit from richer linguistic data to establish good performance.</p>
<p>The language integration sequence for multilingual-consistency was randomly chosen, which is Chinese, English, Bengali, German, Spanish, French, Japanese, Russian, Swahili, Telugu, and Thai.Therefore, the accuracy of Multilingualconsistency at the 1st epoch was 80.0%, the same as that of self-consistency in Chinese at the 1st epoch.Moreover, multilingual-consistency integrating 11 languages achieves 90.8% at the 11th epoch, which is 2.8% higher than self-consistency in English at the 10th epoch.</p>
<p>As shown in Figure 6(b), An accuracy of 91.6% is achieved by multilingualconsistency at the 110th epoch where data from 110 epochs (11 languages, 10 epochs for each language) were all used for majority voting.Interestingly, multilingualconsistency achieves its best accuracy, that is 92.0%, at the 24th epoch, which occurs at the early stage of the overall 110 epochs.However, the accuracy values of multilingual-consistency from the 25th epoch to 110th epoch range from 90.4% to 91.6%, never reaching 92.0% again.This implies that the performance of multilingualconsistency does not always increase as more reasoning languages are added.In other words, some reasoning languages have negative effects on the overall performance of multilingual-consistency.</p>
<p>Discussion</p>
<p>It remains unclear how to predict and decide the optimal set of languages to achieve the best performance.Given the limited scale of the MGSM benchmark (250 samples), an exhaustive enumeration approach can be employed to compute accuracy metrics across all possible linguistic configurations.This methodology facilitates systematic identification of optimal language combinations for enhancing mathematical reasoning capabilities in large language models (LLMs).Given the combinatorial explosion of language configurations that generates 2,047 potential combinations (derived from ∑ (11, ) 11 =2</p>
<p>), Table 2  based on the mean accuracy metric across post-20th-epoch iterations.</p>
<p>Through exhaustive enumeration of language configurations, we identify an optimal combination comprising six languages (Chinese, English, Bengali, Spanish, Russian, and Thai), which achieves a mean accuracy of 92.09%.Therefore, compared to monolingual self-consistency with each language (73.6%-88.0%),CLC with the optimal combination achieves 4.1%-18.5% absolute accuracy gains on the MGSM benchmark using Gemma2-9B-Instruct, with particularly pronounced improvements in low-resource language scenarios.</p>
<p>As demonstrated in Table 2, the mean accuracy initially increases with the expansion of linguistic diversity, peaking at n=6, followed by a gradual decline.This phenomenon aligns with two competing mechanisms:</p>
<p>Pattern coverage enhancement.Multilingual ensemble voting mechanisms leverage parallel per-language inference pipelines to maximize coverage of training data patterns.Incorrect predictions from a single language can be overridden by correct responses from other languages through majority voting, effectively achieving pattern coverage via probabilistic consensus (Dietterich, 2000;Naik, 2024;Zhou, 2025).There are two synergistic benefits behind the probabilistic consensus of multilingual ensemble voting mechanisms: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space.</p>
<p>Probabilistic consensus degradation.Beyond a critical cardinality (n&gt;6), the system enters a phase of probabilistic consensus degradation (Dietterich, 2000;Naik, 2024;Zhou, 2025).Newly introduced languages may introduce conflicting predictions, overriding previously correct predictions through majority voting mechanisms.</p>
<p>While self-consistency mechanisms enhance CoT reasoning in large language models (LLMs), their efficacy is fundamentally constrained by multilingual training data biases-manifesting as semantic drift and logical inconsistencies, particularly in sub-10B parameter models.To overcome these limitations, we propose the CLC inference framework, which innovatively integrates multilingual CoT paths by majority voting to achieve performance gains.In the case of bilingual consistency, CLC achieves 9.5%, 6.5%, and 6.0% absolute accuracy gains on CMATH for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively, significantly outperforming traditional self-consistency baselines.Moreover, in the case of multilingual consistency, CLC achieves 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset compared to monolingual self-consistency with each language, with particularly pronounced improvements in low-resource language scenarios.There are two synergistic benefits behind CLC: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting,</p>
<p>2) escaping monolingual reasoning traps by exploring the broader multilingual solution space.</p>
<p>Future work should explore integration of CLC with retrieval-augmented generation, validate its efficacy in large-parameter models (&gt;100B) like GPT-4 and PaLM-2, and extend the CLC inference framework to low-resourced language families.</p>
<p>.</p>
<p>Zero-shot CoT prompts are utilized to instruct LLMs to reason step by step.The Prompts for DeepSeek-Math-7B-Instruct are the same as those officially provided by DeepSeek, as shown in Table 1.Qwen2.5-Math-7B-Instruct and Gemma2-9B-Instruct also use the same prompts as DeepSeek-Math-7B-Instruct, since there is no officially recommended CoT prompts for them.Question\n Please reason step by step, and put your final answer within \boxed{}.</p>
<p>(4) French Prompt: Question\n Veuillez raisonner é tape par é tape et mettre votre ré ponse finale dans \boxed{}.(5) German Prompt: Question\n Bitte begründen Sie Schritt für Schritt und geben Sie Ihre endgültige Antwort in \boxed{} ein.(6) Russian Prompt: Question\n Пожалуйста, рассуждайте шаг за шагом и поместите окончательный ответ в \boxed{}.(7) Japanese Prompt: Question\n 段階的に推論して、最終的な答えを \boxed{} 内に入力して ください。 (8) Thai Prompt: Question\n โปรดให ้เหตุ ผลที ละขั ้ นตอน และใส่ ค ำตอบสุ ดท ้ำยของคุ ณไว ้ใน \boxed{} (9) Swahili Prompt: Question\n Tafadhali sababu hatua kwa hatua, na uweke jibu lako la mwisho ndani ya \boxed{}.(10) Bengali Prompt: Question\n অনু গ্রহ করে ধারে ধারে যু ক্ত ি দিন এবং আেনাে চূ ডান্ত উত্তেটি \boxed{}-এে মরধে োখু ন। (11) Telugu Prompt: Question\n దయచేసి దశల వారీగా వాదించిండి మరియు మీ తుద సమాధానాన్ని \ బాక్స ్డ్{}లో ఉించిండి.</p>
<p>Finally, bilingual-consistency results were obtained by integrating the results of the 10 Chinese reasoning epochs and 10 English reasoning epochs via majority voting, as shown in Figure2.Specifically, the bilingual-consistency answers at the first epoch are based on those generated by Chinese-sampling without consistency at the first epoch.At the second epoch, the bilingual-consistency incorporates answers from bothChinese-sampling at the first epoch and English-sampling at the first epoch, and so on.</p>
<p>Figure 2 .
2
Figure 2. A comparison among bilingual-consistency, traditional self-consistency and</p>
<p>Figure 3 ,
3
Figure3, the accuracy of bilingual-consistency increases quickly from 69.7% to 87.3%</p>
<p>Figure 3 .
3
Figure 3.A comparison among bilingual-consistency, traditional self-consistency and</p>
<p>Figure 4 .
4
Figure 4.A comparison among bilingual-consistency, traditional self-consistency and</p>
<p>Figure 5 .
5
Figure 5.A comparison among bilingual-consistency, traditional self-consistency and</p>
<p>Figure 6 .
6
Figure 6.(a): A comparison between multilingual-consistency and traditional self-</p>
<p>To ensure robust performance evaluation, we adopt the mean accuracy across post-20th-epoch iterations as the primary metric.This design intentionally mitigates two confounding factors: (1) convergence-induced accuracy instability during pre-20th-epoch iterations, and (2) post-convergence accuracy oscillation during post-20th-epoch iterations that may distort model capability assessment.</p>
<p>Table 2 .
2
Top-performing language configurations within each group size (n=2 to 11)
selectively presents</p>
<p>A Bajpai, T Chakraborty, 10.48550/arXiv.2412.08090arXiv:2412.08090Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages. 2025</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, D Amodei, 10.48550/arXiv.2005.14165arXiv:2005.14165Language Models are Few-Shot Learners. 2020</p>
<p>X Chen, R Aksitov, U Alon, J Ren, K Xiao, P Yin, S Prakash, C Sutton, X Wang, D Zhou, 10.48550/arXiv.2311.17311arXiv:2311.17311Universal Self-Consistency for Large Language Model Generation. 2023</p>
<p>Ensemble methods in machine learning. T G Dietterich, International Workshop on Multiple Classifier Systems. 2000</p>
<p>A Holtzman, J Buys, L Du, M Forbes, Y Choi, 10.48550/arXiv.1904.09751arXiv:1904.09751The Curious Case of Neural Text Degeneration. 2020</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, 10.48550/arXiv.2001.08361arXiv:2001.08361Scaling Laws for Neural Language Models. 2020</p>
<p>G Lample, A Conneau, 10.48550/arXiv.1901.07291arXiv:1901.07291Cross-lingual Language Model Pretraining. 2019</p>
<p>S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, arXiv:2402.06196Large Language Models: A Survey. 2024</p>
<p>N Naik, 10.48550/arXiv.2411.06535arXiv:2411.06535Probabilistic Consensus through Ensemble Validation: A Framework for LLM Reliability. 2024</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, arXiv:2307.06435A Comprehensive Overview of Large Language Models. 2024</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Others, OpenAI Blog. 892019</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, M Zhang, Y K Li, Y Wu, D Guo, arXiv:2402.03300DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024</p>
<p>K Stechly, K Valmeekam, S Kambhampati, 10.48550/arXiv.2405.04776arXiv:2405.04776Chain of Thoughtlessness? An Analysis of CoT in Planning. 2025</p>
<p>P Wang, Z Wang, Z Li, Y Gao, B Yin, X Ren, 10.48550/arXiv.2305.01879arXiv:2305.01879SCOTT: Self-Consistent Chain-of-Thought Distillation. 2023</p>
<p>X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.11171Self-Consistency Improves Chain of Thought Reasoning in Language Models. 2023</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>Y Xu, L Hu, J Zhao, Z Qiu, K Xu, Y Ye, H Gu, 10.1007/s11704-024-40579-4A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias. 2024</p>
<p>Ensemble methods: Foundations and algorithms. Z.-H Zhou, 2025CRC press</p>            </div>
        </div>

    </div>
</body>
</html>
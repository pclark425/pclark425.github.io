<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-142ebbf4760145f591166bde2564ac70c001e927</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/142ebbf4760145f591166bde2564ac70c001e927" target="_blank">Language Models (Mostly) Know What They Know</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format and investigated whether models can be trained to predict P(IK), the probability that "I know" the answer to a question.</p>
                <p><strong>Paper Abstract:</strong> We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability"P(True)"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict"P(IK)", the probability that"I know"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic in LMs (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic performance, mechanisms, and interventions as reported in "Language Models (Mostly) Know What They Know"</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper evaluates how Anthropic-trained transformer language models (800M, 3B, 12B, 52B) behave on arithmetic-style tasks (basic arithmetic, mixed-arithmetic, GSM8K word problems) and what mechanisms and interventions affect arithmetic performance, focusing on self-evaluation (P(True)) and model-knowing (P(IK)). It finds improved discrimination and calibration with model scale and with interventions (few-shot, comparison samples, P(IK) finetuning, hints, source context) but does not identify a specific internal symbolic arithmetic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anthropic pretrained transformer models (studied at 800M, 3B, 12B, 52B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer language models (architecture as in Bai et al., 2022) pretrained for ~850B tokens; studied model sizes: 800M, 3B, 12B, 52B parameters. Pretraining distribution contains ~10% code; models are primarily pure LMs though RLHF-trained policies are also examined in a limited way.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Basic arithmetic short-answer problems, Mixed-Arithmetic (synthetic mixed-ops), GSM8K math word problems with chain-of-thought/scratchpad reasoning, and evaluation of numeric reasoning in Python function synthesis (web-scraped).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>No single internal symbolic arithmetic algorithm is claimed; evidence suggests models rely on pattern recognition/memorization and contextual retrieval, combined with a separable verification capability (models can estimate P(True) or P(IK) that reflects confidence). Models use context and hints to boost apparent knowledge; generation and internal verification appear to be distinct processes (verification often easier than generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) P(IK) (value-head) scores rise substantially when relevant source documents or correct hints are added to context (example: P(IK) from 18% to 78% when a relevant Wikipedia passage is prepended). 2) Showing multiple T=1 samples ('brainstorming' comparison samples) before evaluating a candidate improves P(True) calibration and discrimination across arithmetic tasks. 3) Training a P(IK) value head on task data yields high AUROC and good calibration in-distribution (Table 1: Mixed-Arithmetic AUROC 0.987 / Brier 0.042 when trained on all tasks). 4) AUROC and calibration generally improve with model size (figures and size trends reported). 5) Correct chain-of-thought hints increase P(IK) more than incorrect or distracting hints (GSM8K experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>1) The paper finds no direct interpretability or probing evidence for algorithmic, stepwise arithmetic representations (no internal arithmetic algorithm identified). 2) P(IK) generalizes only partially OOD: classifiers trained on TriviaQA alone are underconfident and calibrate poorly on arithmetic or GSM8K (GSM8K AUROC 0.624 / Brier 0.200 when trained only on TriviaQA). 3) The authors caution P(IK) may partly capture question difficulty rather than strictly 'what this model knows'. 4) Models show biases (e.g., strong bias against 'none of the above') and can be partially fooled by bad or distracting hints.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting; showing multiple sampled T=1 responses (comparison / brainstorming samples); chain-of-thought / scratchpad few-shot prompts for GSM8K; training a value-head classifier (P(IK)) via finetuning; including relevant source documents in context; binarization thresholds for GSM8K ground truth (0.25 used sometimes); temperature tuning for RLHF policies.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Few-shot prompts and showing multiple T=1 samples improve calibration of P(True) and help discriminate correct vs incorrect numerical answers; P(IK) finetuning markedly increases AUROC and decreases Brier score in-distribution and when trained on diverse tasks; including relevant background materials or correct hints raises P(IK) substantially; training on the arithmetic distributions (instead of only TriviaQA) reduces underconfidence and increases calibration and AUROC (e.g. Mixed-Arithmetic AUROC improves from 0.928 to 0.987 when trained on all tasks). Temperature adjustment (T=2.5) can remediate RLHF miscalibration but is not an arithmetic-specific intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key reported numbers (52B classifier unless otherwise noted): - Mixed-Arithmetic: AUROC 0.928 / Brier 0.194 when P(IK) trained on TriviaQA only; AUROC 0.987 / Brier 0.042 when trained on all tasks. - GSM8K (treated specially; threshold sometimes 0.25): AUROC 0.624 / Brier 0.200 when trained on TriviaQA only; AUROC 0.752 / Brier 0.121 when trained on all tasks. - TriviaQA (in-distribution): AUROC 0.864 / Brier 0.151 (trained on TriviaQA only) and 0.873 / 0.145 (trained on all tasks). - General trend: AUROC and calibration of P(IK) improve with model size (800M -> 3B -> 12B -> 52B). - Self-evaluation P(True): conditional accuracy (accuracy among samples labeled P(True)>0.5) is substantially higher than baseline accuracy, with separation increasing by model size; exact per-task numeric conditional accuracies are reported in figures but not tabulated numerically in main text. - GSM8K ground-truth P(IK) values are rare above 0.5; authors sometimes use a 0.25 threshold for binarization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor out-of-distribution calibration (underconfidence) when P(IK) is trained on a different task distribution; zero-shot self-evaluation tends to give ~50% P(True) and is poorly calibrated; bias against 'none of the above' harming arithmetic (and other) evaluations; hints can partially mislead models (bad or distracting hints sometimes reduce or spuriously increase P(IK)); no evidence of robust algorithmic, systematic multi-step arithmetic reasoning for larger problems (GSM8K remains hard). Smaller models produce answers that are easier to categorize (but are less correct overall).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>The paper does not perform direct comparisons to human arithmetic performance or to symbolic calculators/algorithms. Authors note a qualitative point: verification (asking the model to judge answers) often appears easier than generation, but no formal benchmark comparisons to symbolic methods are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models (Mostly) Know What They Know', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2981",
    "paper_id": "paper-142ebbf4760145f591166bde2564ac70c001e927",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Arithmetic in LMs (this paper)",
            "name_full": "Arithmetic performance, mechanisms, and interventions as reported in \"Language Models (Mostly) Know What They Know\"",
            "brief_description": "This paper evaluates how Anthropic-trained transformer language models (800M, 3B, 12B, 52B) behave on arithmetic-style tasks (basic arithmetic, mixed-arithmetic, GSM8K word problems) and what mechanisms and interventions affect arithmetic performance, focusing on self-evaluation (P(True)) and model-knowing (P(IK)). It finds improved discrimination and calibration with model scale and with interventions (few-shot, comparison samples, P(IK) finetuning, hints, source context) but does not identify a specific internal symbolic arithmetic algorithm.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Anthropic pretrained transformer models (studied at 800M, 3B, 12B, 52B)",
            "model_description": "Decoder-only transformer language models (architecture as in Bai et al., 2022) pretrained for ~850B tokens; studied model sizes: 800M, 3B, 12B, 52B parameters. Pretraining distribution contains ~10% code; models are primarily pure LMs though RLHF-trained policies are also examined in a limited way.",
            "arithmetic_task_type": "Basic arithmetic short-answer problems, Mixed-Arithmetic (synthetic mixed-ops), GSM8K math word problems with chain-of-thought/scratchpad reasoning, and evaluation of numeric reasoning in Python function synthesis (web-scraped).",
            "reported_mechanism": "No single internal symbolic arithmetic algorithm is claimed; evidence suggests models rely on pattern recognition/memorization and contextual retrieval, combined with a separable verification capability (models can estimate P(True) or P(IK) that reflects confidence). Models use context and hints to boost apparent knowledge; generation and internal verification appear to be distinct processes (verification often easier than generation).",
            "evidence_for_mechanism": "1) P(IK) (value-head) scores rise substantially when relevant source documents or correct hints are added to context (example: P(IK) from 18% to 78% when a relevant Wikipedia passage is prepended). 2) Showing multiple T=1 samples ('brainstorming' comparison samples) before evaluating a candidate improves P(True) calibration and discrimination across arithmetic tasks. 3) Training a P(IK) value head on task data yields high AUROC and good calibration in-distribution (Table 1: Mixed-Arithmetic AUROC 0.987 / Brier 0.042 when trained on all tasks). 4) AUROC and calibration generally improve with model size (figures and size trends reported). 5) Correct chain-of-thought hints increase P(IK) more than incorrect or distracting hints (GSM8K experiments).",
            "evidence_against_mechanism": "1) The paper finds no direct interpretability or probing evidence for algorithmic, stepwise arithmetic representations (no internal arithmetic algorithm identified). 2) P(IK) generalizes only partially OOD: classifiers trained on TriviaQA alone are underconfident and calibrate poorly on arithmetic or GSM8K (GSM8K AUROC 0.624 / Brier 0.200 when trained only on TriviaQA). 3) The authors caution P(IK) may partly capture question difficulty rather than strictly 'what this model knows'. 4) Models show biases (e.g., strong bias against 'none of the above') and can be partially fooled by bad or distracting hints.",
            "intervention_type": "Few-shot prompting; showing multiple sampled T=1 responses (comparison / brainstorming samples); chain-of-thought / scratchpad few-shot prompts for GSM8K; training a value-head classifier (P(IK)) via finetuning; including relevant source documents in context; binarization thresholds for GSM8K ground truth (0.25 used sometimes); temperature tuning for RLHF policies.",
            "effect_of_intervention": "Few-shot prompts and showing multiple T=1 samples improve calibration of P(True) and help discriminate correct vs incorrect numerical answers; P(IK) finetuning markedly increases AUROC and decreases Brier score in-distribution and when trained on diverse tasks; including relevant background materials or correct hints raises P(IK) substantially; training on the arithmetic distributions (instead of only TriviaQA) reduces underconfidence and increases calibration and AUROC (e.g. Mixed-Arithmetic AUROC improves from 0.928 to 0.987 when trained on all tasks). Temperature adjustment (T=2.5) can remediate RLHF miscalibration but is not an arithmetic-specific intervention.",
            "performance_metrics": "Key reported numbers (52B classifier unless otherwise noted): - Mixed-Arithmetic: AUROC 0.928 / Brier 0.194 when P(IK) trained on TriviaQA only; AUROC 0.987 / Brier 0.042 when trained on all tasks. - GSM8K (treated specially; threshold sometimes 0.25): AUROC 0.624 / Brier 0.200 when trained on TriviaQA only; AUROC 0.752 / Brier 0.121 when trained on all tasks. - TriviaQA (in-distribution): AUROC 0.864 / Brier 0.151 (trained on TriviaQA only) and 0.873 / 0.145 (trained on all tasks). - General trend: AUROC and calibration of P(IK) improve with model size (800M -&gt; 3B -&gt; 12B -&gt; 52B). - Self-evaluation P(True): conditional accuracy (accuracy among samples labeled P(True)&gt;0.5) is substantially higher than baseline accuracy, with separation increasing by model size; exact per-task numeric conditional accuracies are reported in figures but not tabulated numerically in main text. - GSM8K ground-truth P(IK) values are rare above 0.5; authors sometimes use a 0.25 threshold for binarization.",
            "notable_failure_modes": "Poor out-of-distribution calibration (underconfidence) when P(IK) is trained on a different task distribution; zero-shot self-evaluation tends to give ~50% P(True) and is poorly calibrated; bias against 'none of the above' harming arithmetic (and other) evaluations; hints can partially mislead models (bad or distracting hints sometimes reduce or spuriously increase P(IK)); no evidence of robust algorithmic, systematic multi-step arithmetic reasoning for larger problems (GSM8K remains hard). Smaller models produce answers that are easier to categorize (but are less correct overall).",
            "comparison_to_humans_or_symbolic": "The paper does not perform direct comparisons to human arithmetic performance or to symbolic calculators/algorithms. Authors note a qualitative point: verification (asking the model to judge answers) often appears easier than generation, but no formal benchmark comparisons to symbolic methods are provided.",
            "uuid": "e2981.0",
            "source_info": {
                "paper_title": "Language Models (Mostly) Know What They Know",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.011639499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models (Mostly) Know What They Know</h1>
<p>Saurav Kadavath; Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai,<br>Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson,<br>Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph,<br>Ben Mann, Sam McCandlish, Chris Olah, Jared Kaplan*</p>
<h2>Anthropic</h2>
<h4>Abstract</h4>
<p>We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for $\mathrm{P}($ True $)$ on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting $\mathrm{P}(\mathrm{IK})$ and partially generalize across tasks, though they struggle with calibration of $\mathrm{P}(\mathrm{IK})$ on new tasks. The predicted $\mathrm{P}(\mathrm{IK})$ probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 3
1.1 Contributions ..... 4
1.2 Models and Evaluation Tasks ..... 5
1.3 Related Work ..... 6
2 Larger Models are Calibrated on Diverse Multiple Choice Questions ..... 7
3 From Calibration to Knowing What You Know ..... 8
3.1 Replacing an Option with 'None of the Above' Harms Performance and Calibration ..... 8
3.2 Models are Well-Calibrated on True/False Tasks ..... 10
3.3 RLHF Policy Miscalibration Can Be Remediated with a Temperature Tuning ..... 11
4 Ask the AI: Is your proposed answer True or False? ..... 11
4.1 Basic Self-Evaluation ..... 11
4.2 Showing Many $T=1$ Samples Improves Self-Evaluation ..... 12
5 Training Models to Predict Whether They Can Answer Questions Correctly ..... 14
5.1 Evaluating P(IK) Training and Model Size Trends ..... 14
5.2 Out of Distribution Generalization of P(IK) ..... 14
5.3 P(IK) Generalizes to Account for Source Materials ..... 18
5.4 P(IK) Generalizes to Account for Hints Towards GSM8k Solutions ..... 19
5.5 Comparing Models Trained with Distinct Pretraining Distributions ..... 20
6 Discussion ..... 22
6.1 Limitations and Future Work ..... 22
6.2 Broader Impacts ..... 23
7 Contribution Statement ..... 23
A Metrics, Formatting Details, and P(IK) Training ..... 24
B Discriminating What Models Know with Entropy or Loss ..... 28
C More P(True) Evaluation Results and Details ..... 30
D Mixed-Arithmetic and Function Synthesis Dataset Descriptions ..... 30</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 (left) We show the overall ability of a 52B language model to evaluate its own proposed answers (sampled at unit temperature) to questions from TriviaQA, Lambada, Arithmetic, GSM8k, and Codex Hu manEval. We have weighted the overall contribution from each of these five datasets equally. We evaluate 20-shot using the method of section 4, where we show the model several of its own samples and then ask for the probability P(True) that a specific sample is correct. (right) We show the improvement in the accuracy on each sampling task when only including questions where a randomly sampled (unit temperature) response achieved P(True) &gt; 0.5.</p>
<h1>1 Introduction</h1>
<p>We would eventually like to train AI systems that are honest, which requires that these systems accurately and faithfully evaluate their level of confidence in their own knowledge and reasoning. So AI systems must be able to recognize what they do and do not know, as a prerequisite. In this work, we study the extent to which Language Models (LMs) possess this ability and how it can be elicited and imparted.
As a starting point, we examine calibration: do the probabilistic predictions from language models match up with frequencies of occurrence? Language models can produce well-calibrated predictions for token probabilities on-distribution [Guo et al., 2017]. We show that large language models are also well-calibrated on a diverse array of multiple choice questions, as long as the questions are formatted appropriately. In particular, calibration improves with model size and few-shot prompting.
Good calibration opens up the possibility for using models to evaluate the accuracy of their own outputs ("self-evaluation"). For example, given any open-ended query, we can sample an answer from the model and then have the model evaluate P(True), the probability that its answer is correct. We may expect selfevaluation to be challenging, because the model may be overconfident that its own samples ${ }^{1}$ are correct. Our self-evaluation procedure nevertheless distinguishes correct and incorrect samples, as summarized in Figure 1. Furthermore, as model size and capabilities increase, models improve at self-evaluation, which suggests that verification improves faster than generation quality in this context.
We also show that self-evaluation can be improved if we provide a model with many of its own samples, before asking it to evaluate any single sample. That is, 'brainstorming other possibilities' helps large models to evaluate the validity of a given answer option.
These techniques address a question about the world, as they ask models to evaluate "according to accepted truth in the wider world (i.e. according to humans), is a particular answer to a question correct?" In the case of self-evaluation, the proposed answer was provided by the model, but its validity is nevertheless an external fact.
But we are also interested in having language models attempt to directly evaluate their own state of knowledge. To this purpose, we investigate what happens when we train models to predict whether or not they can correctly answer questions themselves. This is really a question about the model ${ }^{2}$ since we are training the model to learn what sorts of questions it, in particular, can answer.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 Left: We train a P(IK) classifier to predict whether or not a model knows the answer to TriviaQA questions, and then evaluate on Arithmetic, Python Function Synthesis, and Lambada questions. This histogram shows P(IK) scores exclusively from OOD questions. Right: We train a P(IK) classifier on TriviaQA, Arithmetic, Python Function Synthesis, and Lambada and histogram P(IK) scores for their test sets.</p>
<p>We find that language models can easily learn to perform well at evaluating P(IK), the probability that they know the answer to a question, on a given distribution (see Figure 3 for an illustration). More intriguingly, we also find some generalization across tasks, for example from trivia to story completion, math, and code, though models struggle with calibration OOD. We also observe some other types of generalization: although P(IK) was only trained on bare questions, it generalizes in such a way that it increases when we provide source materials that address these questions (for trivia) and when we provide hints for math word problems.</p>
<h3>1.1 Contributions</h3>
<h4>Calibration: Multiple Choice, None of the Above, and True/False</h4>
<ul>
<li>We show that when we use a format with visible lettered answer options, large models are very well calibrated on diverse multiple choice questions (e.g., from BIG Bench [Srivastava et al., 2022], MMLU [Hendrycks et al., 2021], and many other evaluations); see Figures 4, 5, and 6.</li>
<li>Calibration improves with model size, and it also improves when we pass from zero-shot to few-shot evaluation; see Figure 4.</li>
<li>Replacing an option with 'none of the above' reduces accuracy and calibration significantly with our models (see Figure 7). However, our models are also well calibrated on True/False distinctions (see Figure 8), with accuracy and calibration also increasing with model capability.</li>
<li>We also show that RLHF policies [Bai et al., 2022] naively seem miscalibrated, but with a simple temperature adjustment they become fairly well-calibrated on several evaluations (Figure 9).</li>
</ul>
<h4>Self-Evaluation of Model-Generated Samples, without Finetuning</h4>
<p>We evaluate on the generative tasks TriviaQA [Joshi et al., 2017], Lambada [Paperno et al., 2016], GSM8k [Cobbe et al., 2021], the Codex HumanEval [Chen et al., 2021], arithmetic problems, and natural function synthesis problems scraped from GitHub. See Figure 1 for a brief overview of results.</p>
<ul>
<li>Models can self-evaluate whether their own samples are True or False, though this tends to be a more challenging task (since models tend to find their own samples more plausible). Self-evaluations are well-calibrated few-shot, though models aren't as well-calibrated zero-shot. In particular, larger <em>k</em> for <em>k</em>-shot self-evaluation seems to primarily help by improving calibration, rather than by improving the AUROC for separating correct and incorrect responses. See Figures 10 and 11 in the main text for representative results, and Figures 28, 29, and 30 in the appendix for complete results.</li>
<li>Showing models many of their own <em>T</em> = 1 samples, along with a single sample to evaluate as True/False, can significantly improve their performance (this is somewhat reminiscent of self-consistency prompting [Wang et al., 2022]).</li>
</ul>
<p>We conclude that language models can perform well at this task few-shot, with most measures of performance improving with model size, even though models are being asked to evaluate their own samples.</p>
<h1>Finetuning to Identify the Questions Models Can Correctly Answer</h1>
<p>We are also interested in whether models know, or can be taught, to specifically identify questions that they can and cannot answer, rather than simply evaluating whether answers to questions are in fact correct. Figure 2 provides a brief overview of results.</p>
<ul>
<li>We train models with a value head to predict the probability that they can answer a question correctly, which we refer to as $\mathrm{P}(\mathrm{IK})$. We find that models trained on TriviaQA have significant power to differentiate between math, lambada, and code questions that they can answer; see Figure 12. However, $\mathrm{P}(\mathrm{IK})$ tends to be poorly calibrated on these other distributions (see Figure 13). Generalization is perhaps most clearly illustrated in Figures 16 and 17.</li>
<li>We study generalization of $\mathrm{P}(\mathrm{IK})$ to the inclusion of source materials and mathematical hints, see Figures 18 and 19. We find that $\mathrm{P}(\mathrm{IK})$ responds appropriately to sources, correct hints, and incorrect or distracting hints.</li>
<li>We compare two models with roughly equal capability, but different pretraining data, in Figure 20. These models are somewhat better at predicting their own $\mathrm{P}(\mathrm{IK})$ rather than that for another model. We also try "crossing" the $\mathrm{P}(\mathrm{IK})$ training data for these models, and find a small effect suggesting that models generalize better when trained on what they know, rather than on what the other model knew.</li>
</ul>
<p>Thus we conclude that we only find partial generalization on this task.</p>
<h2>Glossary: Observables and Metrics</h2>
<ul>
<li>P(True) - The probability a model assigns to the proposition that a specific sample is the correct answer to a question.</li>
<li>P(IK) - The probability a model assigns to "I know", i.e. the proposition that it will answer a given question correctly when samples are generated at unit temperature. In this work, $\mathrm{P}(\mathrm{IK})$ is usually computed using a binary classification head on top of a language model.</li>
<li>Ground Truth P(IK) - The fraction of unit temperature samples to a question that are correct.</li>
<li>Calibration Charts - We often plot prediction probability vs frequency that a prediction was correct, see Figure 4 as an example. We use all predictions (not just predictions for the correct answer) and put the same number of predictions in each bin (rather than using equally spaced bins).</li>
<li>Expected or RMS Calibration Error - To obtain a single number summarizing calibration, we compute the ECE as the mean of the absolute difference between probabilistic predictions and frequencies. For ECE we always use 10 bins, with an equal number of predictions in each. What we call the ECE has also been called the mean absolute deviation calibration error [Lin et al., 2022]. To more closely match other works, we only use the most likely predictions (for multiple choice) when computing the ECE, instead of using all predictions. We also include the RMS calibration error, which is better motivated theoretically, but seems to be less widely used in the literature.</li>
<li>AUROC - We sometimes share the area under the receiver operating characteristic (AUROC) discriminating between questions models do or do not know the answer to, or discriminating between samples a model does or does not identify as correct. This captures discriminative power but is indifferent to calibration (note chance AUROC is 0.5 , and larger scores are better).</li>
<li>(Conditional) Accuracy - When models self-evaluate the probability P(True) that their own samples are valid, we are very interested in the accuracy of the samples that models label as 'True' (i.e. correct), and how this compares to the accuracy on the full distribution of problems in the task.</li>
<li>Brier Score - In some cases we observe tradeoffs between discrimination (e.g. best AUROC) and calibration. Brier scores combine the discriminative power of self-evaluation with calibration (note the chance Brier score on binary choice tasks is 0.25 , and smaller scores are better).</li>
</ul>
<h3>1.2 Models and Evaluation Tasks</h3>
<p>Our goal in this study is to evaluate calibration and generalization on a diverse range of tasks. As such we include all of the multiple choice evaluations in BIG Bench [Srivastava et al., 2022], the MMLU</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 Examples of P(IK) scores from a 52B model. Token sequences that ask harder questions have lower P(IK) scores on the last token. To evaluate P(IK) on a specific full sequence, we simply take the P(IK) score at the last token. Note that we only train P(IK) on final tokens (and not on partial questions).</p>
<p>Evaluation [Hendrycks et al., 2021], TruthfulQA [Lin et al., 2021], LogiQA [Liu et al., 2020], and QuALITY [Pang et al., 2021]. We are most interested in open-ended generation, so we study the sampling-based evaluations TriviaQA [Joshi et al., 2017], Lambada [Paperno et al., 2016], the Codex HumanEval [Chen et al., 2021], GSM8k [Cobbe et al., 2021], some basic arithmetic problems, and a dataset of additional web-scraped Python function synthesis problems. See Appendix D for more information on our arithmetic and function synthesis datasets. Ultimately, we would like to train models to express calibrated confidence levels when generating long-form responses in natural language dialogue.</p>
<p>When we perform few-shot evaluation, we simply stuff the context with randomly chosen examples from the (test) evaluation itself. For BIG Bench this means all few-shot examples come from within the specific subtask we are evaluating, though in the case of MMLU we randomize across the entire evaluation, without respecting subject boundaries.</p>
<p>We study a series of language models with 800M, 3B, 12B, 52B parameters. We do not include smaller models because they perform poorly on many of the evaluations we consider. The architecture and training setup for these models is identical to that in [Bai et al., 2022], except that the models we consider here were pretrained for 850B tokens, rather than the 400B tokens used in that work. For simplicity, we do not study models that have been finetuned on Python code, though our pretraining distribution includes about 10% code. In section 3.3 we briefly study helpful and harmless RLHF policies finetuned (via the process in [Bai et al., 2022]) from these language models, but otherwise we only study pure language models. We show accuracies for our models on a few of the multiple choice tasks we study in Figure 37 in the appendix.</p>
<h3>1.3 Related Work</h3>
<p>Calibration for general ML predictions, and interventions to improve calibration, have been studied [Nguyen and O'Connor, 2015; Hendrycks and Gimpel, 2016; Nalisnick et al., 2019; Guo et al., 2017; Hendrycks et al., 2018; Ovadia et al., 2019; Minderer et al., 2021] for some time. Calibration for language models and QA has also been studied [Desai and Durrett, 2020; Jiang et al., 2021], but typically it has been found that to achieve good calibration predictions must be adjusted. Selective prediction, where models abstain from answering certain questions, has been studied as well [Varshney et al., 2022]. Recently, the calibration of a wide range of models was analyzed on the diverse BIG Bench suite of tasks [Srivastava et al., 2022], where it was shown that language model calibration improves with model size. We are indebted to the BIG Bench collaboration for providing a convenient, huge, and diverse evaluation set. The authors of Gopher [Rae et al., 2021] briefly studied calibration on MMLU [Hendrycks et al., 2021] and found promising results, which led us to experiment with a variety of multiple choice formats.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 (left) We show calibration curves for various model sizes on all of the multiple choice tasks in BIG Bench, in the format described in section 2. We include a dashed line indicating perfect calibration. (right) Here we show trends in the expected calibration error on BIG Bench, for both multiple choice and a separate True/False format (see Section 3.2). We show the RMS calibration error in Figure 21 in the appendix.</p>
<p>Truthfulness [Evans et al., 2021] has been a recent focus of various works, including benchmarks [Lin et al., 2021] and the incorporation of web search and citation [Nakano et al., 2021, Menick et al., 2022] into language models. That said, truthfulness focuses primarily on factual accuracy "in the world", rather than on self-knowledge, or eliciting latent knowledge [Christiano et al., 2021]. We use "honesty" [Askell et al., 2021] as an umbrella term for ideas including truthfulness, calibration, self-knowledge, explainability, and non-deceptiveness. Language models finetuned to perform non-language tasks [Ahn et al., 2022, Dinh et al., 2022] might provide an interesting test-bed for honesty in the future.</p>
<p>Perhaps the work most similar to ours is [Mielke et al., 2020], which is a very interesting application of metacognition/self-evaluation to improve natural language calibration. Another quite similar work is the very recent [Lin et al., 2022], where the authors train language models to express their calibration on arithmetic in words, and also study a signal analogous to P(True).</p>
<h1>2 Larger Models are Calibrated on Diverse Multiple Choice Questions</h1>
<p>A model makes calibrated predictions if the probability it assigns to outcomes coincides with the frequency with which these outcomes actually occur. Language models are known to produce calibrated token-level probabilities. In this section we will see that language models can produce well-calibrated probabilities when they are asked to choose the correct answer from among several explicit options. We believe calibration is interesting on its own, but it is especially relevant to honesty, since a model that can produce calibrated answers to meta-questions like 'do you know the answer to X?' must know something about what it knows. Generally we can use calibration as a way to bootstrap towards self-knowledge.
We find that when multiple choice problems are formatted in this way (as used by e.g. [Rae et al., 2021]):</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Choices</span><span class="o">:</span>
<span class="w">    </span><span class="o">(</span><span class="n">A</span><span class="o">)</span><span class="w"> </span><span class="n">Barack</span><span class="w"> </span><span class="n">Obama</span>
<span class="w">    </span><span class="o">(</span><span class="n">B</span><span class="o">)</span><span class="w"> </span><span class="n">George</span><span class="w"> </span><span class="n">Washington</span>
<span class="w">    </span><span class="o">(</span><span class="n">C</span><span class="o">)</span><span class="w"> </span><span class="n">Michael</span><span class="w"> </span><span class="n">Jackson</span>
<span class="n">Answer</span><span class="o">:</span>
</code></pre></div>

<p>and we identify the answers only by their labels, as e.g. ' (B)', our largest models tend to produce a wellcalibrated probability distribution among the available options. We show the calibration chart for all multiple choice BIG Bench tasks, in this format, in Figure 4. As can be seen in Figure 6, models are well-calibrated (in this format) even for somewhat adversarial datasets like TruthfulQA ${ }^{3}$ [Lin et al., 2021], as well as for QuALITY [Pang et al., 2021] and LogiQA [Liu et al., 2020].</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 (left) We show expected calibration error versus normalized accuracy for all BIG Bench tasks; the number of problems in each task is represented by the marker sizes. We do not find any noticeable correlation between accuracy and calibration within BIG Bench. To normalize accuracies we linearly map chance accuracy to 0, keeping perfect accuracy at 1. (right) We compare calibration for several variations on BIG Bench evaluations: we vary between 0-shot and 5-shot, replace an answer option with "none of the above", and compare our format with letter-labeled choices to the default BIG Bench formatting.</p>
<p>It is crucial that the model gets to see the answer choices explicitly before choosing amongst them; without this, we would not expect a calibrated response, due to ambiguities and degeneracies among possible paraphrases and specializations of any given answer option (e.g. "Washington" vs "George Washington, the first US president"). As can be seen in figure 5, task formatting is important for achieving excellent calibration, and calibration improves as we pass from 0-shot to 5-shot evaluation. We expect calibration is also easier to achieve with this format because each answer option corresponds to a single token (this isn't the case in BIG Bench by default, see appendix A.4).</p>
<p>To simplify the interpretation of results, we reduce calibration curves to a single number by computing the expected calibration error (ECE), after binning the predictions in 10 equally-represented bins. On the right of Figure 4 we show scaling trends for calibration on BIG Bench. We typically find good calibration 0-shot, without any other prompt, though calibration improves as we pass from 0-shot to few-shot. We find that in almost all cases, calibration improves with model size and capability. Accuracy on the tasks also improves with model size, but as can be seen in Figure 5, we do not observe any obvious causal connection between accuracy and calibration. For details on how we obtain calibration charts and ECE see Appendix A.</p>
<p>In what follows we will work to leverage these calibration results to ask language models to evaluate what they do and do not know.</p>
<h1>3 From Calibration to Knowing What You Know</h1>
<p>If language models can answer multiple choice questions in a calibrated way, then one might hope that they can apply this ability to evaluate their own outputs. That is, we can simply ask models to generate potential answers to questions, and then have the model evaluate whether any of them are correct. In this section we will begin to explore this idea by reformulating existing tasks. Then in section 4 we will study self-evaluation.</p>
<h3>3.1 Replacing an Option with 'None of the Above' Harms Performance and Calibration</h3>
<p>We have seen that language models can produce calibrated probabilities for multiple choice questions, at least when the questions and choices are provided in the right format. However, to achieve this feat the model only needs to determine the relative weight for several concrete options, when they are compared to each other.</p>
<p>We are more interested in whether the model actually knows whether each of the answer options is correct, when judged independently. To probe this question, we modified our multiple choice evaluations by replacing their final option with "none of the above". This procedure can make questions that do not actually have a unique correct answer ambiguous or impossible, but for many tasks it should result in a well-defined new evaluation. In particular this procedure appears to be sensible for the vast majority of questions in MMLU. Concretely this means that we took questions such as the example in section 2 and replaced them with:</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 We show calibration curves for various model sizes on MMLU, TruthfulQA, QuALITY, and LogiQA in the format described in section 2. We emphasize that even on tasks that are difficult for LMs, like LogiQA, and on a somewhat adversarial evaluation like TruthfulQA, large models can achieve good calibration when the available answer options are presented to them in a favorable format. We include a dashed line indicating perfect calibration.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 (left) We show accuracy on MMLU in the standard format, and after replacing option (D) with "none of the above". This replacement decreases accuracy very significantly. (right) We show calibration specifically for the (D) answer option, in the standard form of MMLU and with (D) as "none of the above". The latter makes calibration much worse, and in particular the model seems strongly biased against using this option, which also harms accuracy.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 We show calibration curves for various model sizes on all of the multiple choice tasks in BIG Bench, reformulated as True/False questions on a mix of the correct answers, and randomly chosen incorrect answer options. The 52B model is very well-calibrated except near the tails, where it is overconfident.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Choices</span><span class="o">:</span>
<span class="w">    </span><span class="o">(</span><span class="n">A</span><span class="o">)</span><span class="w"> </span><span class="n">Barack</span><span class="w"> </span><span class="n">Obama</span>
<span class="w">    </span><span class="o">(</span><span class="n">B</span><span class="o">)</span><span class="w"> </span><span class="n">George</span><span class="w"> </span><span class="n">Washington</span>
<span class="w">    </span><span class="o">(</span><span class="n">C</span><span class="o">)</span><span class="w"> </span><span class="n">none</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">above</span>
<span class="n">Answer</span><span class="o">:</span>
</code></pre></div>

<p>We found that this procedure degraded performance very significantly on evaluations; results for MMLU are shown in Figure 36 in the appendix. Furthermore, adding "none of the above" also harms calibration, as can be seen in Figures 5 and 7. It seems that even the 52B model is biased against using the "none of the above" option and failed to use it with appropriate frequency. This is particularly surprising for 5 -shot evaluations; we also tried evaluating 20 -shot and this also did not improve performance.</p>
<h1>3.2 Models are Well-Calibrated on True/False Tasks</h1>
<p>We saw in section 3.1 that language models seem to be confused by a "none of the above" option. Here we take a different approach, and simply ask models if a given answer is true or false. So we use the format:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Proposed</span><span class="w"> </span><span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="n">George</span><span class="w"> </span><span class="n">Washington</span>
<span class="n">Is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">answer</span><span class="o">:</span>
<span class="w">    </span><span class="o">(</span><span class="n">A</span><span class="o">)</span><span class="w"> </span><span class="n">True</span>
<span class="w">    </span><span class="o">(</span><span class="n">B</span><span class="o">)</span><span class="w"> </span><span class="n">False</span>
<span class="n">The</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="o">:</span>
</code></pre></div>

<p>where we expect either ' $(\mathrm{A})^{\prime}$ or ' $(\mathrm{B})^{\prime}$ ' as an answer. If the model responses are correct at more than chance level, and especially if they are calibrated, then the probability $\mathrm{P}($ True $)$ indicates whether the model believes a response is valid.</p>
<p>As a first test of this approach, we can use answer options from existing multiple choice tasks. For this purpose, we take the correct answer and a randomly chosen incorrect answer, and create a new evaluation set with twice as many problems in the format above, asking models to determine if each answer is correct. In Figure 8 we show the calibration results from this method on BIG Bench. We see that the 52B model is quite well-calibrated in this context. We show similar results for MMLU in Figure 36 in the appendix, and scaling trends comparing a variety of evaluation methods in Figure 7.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9 We show calibration curves for RLHF policies finetuned from our language models. Calibration of these models appears to be very poor, but simply adjusting the temperature of their probability distributions to $T=2.5$ largely fixes calibration issues for three different evaluations.</p>
<h1>3.3 RLHF Policy Miscalibration Can Be Remediated with a Temperature Tuning</h1>
<p>Our focus in this paper is on pure language models, but as a quick experiment we also looked at calibration for a helpful and harmless RLHF policy, trained exactly as in [Bai et al., 2022] using the base language models we are studying here. We find that these policies naively appear very miscalibrated, which is not surprising, since RL finetuning tends to collapse language model predictions towards behaviors that receive the most reward. However, a simple temperature adjustment (with the same temperature $T=2.5$ for all evaluations) largely fixes calibration issues with several independent evaluation tasks, with results shown in Figure 9. This also matches what was found for gender bias evaluations in [Bai et al., 2022], where RLHF policies had a bias score very similar to a basic language model evaluated at lower temperature. Of course more intensive RL training might distort calibration in ways that cannot be remedied in this way, but this at least provides hope that good calibration can survive some forms of finetuning.</p>
<h2>4 Ask the AI: Is your proposed answer True or False?</h2>
<p>We will now apply the True/False approach from section 3.2 to the samples models generated when trying to answer questions, including the short answer tasks arithmetic, Lambada, and TriviaQA, and the longform answer tasks Codex HumanEval and GSM8k (technically GSM8k calls for a short answer, but we will be evaluating full written solution attempts, which have been solicited via chain-of-thought/scratchpad [Nye et al., 2021, Wei et al., 2022] prompting). We show the accuracies when sampling at a temperature of one in Figure 1.</p>
<p>In almost all cases self-evaluation performance improves with model size, and for our 52B models answers labeled with $\mathrm{P}($ True $)&gt;50 \%$ are far more likely to be correct as compared to generic responses (see the summary histogram and comparisons in Figure 1). We also find that showing the model many $T=1$ samples for a single question significantly improves its ability to evaluate whether any given sample is correct. This is somewhat reminiscent of self-consistency [Wang et al., 2022], though here we are showing models their own samples, rather than externally judging self-consistency ourselves. One way to summarize these results is via the Brier scores for self-evaluation, which are shown in Figure 11.</p>
<p>We provide complete results in appendix C; since there are quite a few figures we only show representative examples from Lambada and Codex in the main text.</p>
<h3>4.1 Basic Self-Evaluation</h3>
<p>In section 3.2 we saw that large language models are well-calibrated on True/False questions. Our primary motivation for studying this issue was to ask language models about their own outputs. That is, we are interested in a process where we first ask a question like:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Answer</span><span class="o">:</span>
</code></pre></div>

<p>and sample a response from the model. Then we ask the model about its own sample:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Proposed</span><span class="w"> </span><span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="n">George</span><span class="w"> </span><span class="n">Washington</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">president</span><span class="o">.</span>
</code></pre></div>

<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10 Models self-evaluate their own samples by producing a probability $\mathrm{P}(\mathrm{True})$ that the samples are in fact correct. Here we show histograms of $\mathrm{P}(\mathrm{True})$ for the correct and incorrect samples, in the evaluation paradigm where models also see five $T=1$ samples for the same question, in order to improve their judgment. Here we show results only for Lambada and Codex, as these are fairly representative of short-answer and long-answer behavior; for full results see Figure 28 in the appendix.</p>
<div class="codehilite"><pre><span></span><code>Is the proposed answer:
(A) True
(B) False
The proposed answer is:
</code></pre></div>

<p>and evaluate the probabilities it assigns to the ' $(\mathrm{A})^{\prime}$ ' and ' $(\mathrm{B})^{\prime}$ ' options. We apply this approach to model generated answers from TriviaQA [Joshi et al., 2017], Lambada [Paperno et al., 2016], the Codex HumanEval [Chen et al., 2021], GSM8k [Cobbe et al., 2021], and basic arithmetic problems.</p>
<p>This True/False self-evaluation may be considerably more difficult than the tasks we studied in section 3.2, because there the model was presented with human-written possibilities, whereas here the model is forced to evaluate ${ }^{4}$ its own samples. These samples may lie close to the model's own decision boundary for validity (or the model may simply be overconfident about its own samples), whereas options presented to the model by a third party may be easier to categorize. We observe this effect in several ways. Zero-shot, $\mathrm{P}(\mathrm{True})$ is poorly calibrated, and typically it lies close to $50 \%$ for typical samples (see Figure 38 in the appendix for illustrations). Furthermore, we show in Figure 33 in the appendix that samples from smaller models are universally easier to categorize as correct or incorrect, for models of all sizes.</p>
<h1>4.2 Showing Many $T=1$ Samples Improves Self-Evaluation</h1>
<p>We can improve performance further by showing the model other $T=1$ samples, for comparison. That is, we generate 5 samples in total, and then ask the model about the validity of one of these samples:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">president</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">?</span>
<span class="n">Here</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">brainstormed</span><span class="w"> </span><span class="n">ideas</span><span class="o">:</span><span class="w"> </span><span class="n">James</span><span class="w"> </span><span class="n">Monroe</span>
<span class="n">Thomas</span><span class="w"> </span><span class="n">Jefferson</span>
<span class="n">John</span><span class="w"> </span><span class="n">Adams</span>
<span class="n">Thomas</span><span class="w"> </span><span class="n">Jefferson</span>
<span class="n">George</span><span class="w"> </span><span class="n">Washington</span>
<span class="n">Possible</span><span class="w"> </span><span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="n">James</span><span class="w"> </span><span class="n">Monroe</span>
<span class="n">Is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">answer</span><span class="o">:</span>
<span class="w">    </span><span class="o">(</span><span class="n">A</span><span class="o">)</span><span class="w"> </span><span class="n">True</span>
<span class="w">    </span><span class="o">(</span><span class="n">B</span><span class="o">)</span><span class="w"> </span><span class="n">False</span>
<span class="n">The</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">is</span><span class="o">:</span>
</code></pre></div>

<p>With this format, performance improves significantly on all of the short-form answer tasks, as compared to the version where we only show models a single proposed answer, as summarized in Figure 11. However,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11 Here we show results only for Lambada and Codex, as these are fairly representative of shortanswer and long-answer behavior; for full results see Figures 28, 29, 30, and 31 in the appendix. Top: Here we show the Brier scores for model self-evaluation with three methods: basic self-evaluation with a prompt, and self-evaluation with comparison samples, either with a fixed prompt or 20-shot. Note that the Brier score combines accuracy of the True/False determination with calibration, and 20-shot evaluation with comparison samples performs best in every case. Brier scores do not decrease with model size on evaluations like Codex because small model samples are almost always invalid, so that it's relatively trivial to achieve a small Brier score. Bottom: We show the base accuracy of our models on various sampling tasks, and then the accuracy among the responses where via self-evaluation we have $\mathrm{P}($ True $)&gt;0.5$. For $\mathrm{P}($ True $)$ we evaluate with a single example and a prompt, and then both 20 -shot and with a prompt with five comparison examples. Few-shot evaluation is important for obtaining good calibration.
models benefit less from this approach on tasks requiring long-form answers (Codex and GMS8k). As noted in the last section, calibration of $\mathrm{P}($ True $)$ is relatively poor zero-shot, but it improves few-shot. In Figure 32 in the appendix we show the full calibration curves for 20-shot True/False self-evaluation.</p>
<p>A pragmatic comparison of self-evaluation techniques can be seen on the bottom of Figure 11. There we directly compare the overall accuracy of our models at the task, to the accuracies for the subset of responses where the models chose $\mathrm{P}($ True $)&gt;0.5$. We see that these conditional accuracies are substantially higher than the overall accuracy, with the separation between base and conditional accuracies growing with model size. This suggests that as capabilities improve and samples become more sophisticated, models seem to demonstrate relative improvement at self-evaluation, compared to their generative ability. This accords with the intuitive sense that verification is often easier than generation.</p>
<p>Overall, if given a few examples from a given distribution, models can generate samples and then self-evaluate them to productively differentiate correct and incorrect samples, with reasonably calibrated confidence.</p>
<h1>5 Training Models to Predict Whether They Can Answer Questions Correctly</h1>
<p>In this section, we will train models to predict whether they know the answer to any given free-form question, denoting the probability they assign as ' $\mathrm{P}(\mathrm{IK}$ )' (for Probability that I Know the answer). This is fundamentally a question about the model itself, as different models will know the answers to different questions.
We considered two approaches:</p>
<ul>
<li>Value Head: We train $\mathrm{P}(\mathrm{IK})$ as the logit from an additional value 'head' added to the model (independent of the logits for language modeling). An advantage of this approach is that we can easily probe $\mathrm{P}(\mathrm{IK})$ at general token positions.</li>
<li>Natural Language: We train $\mathrm{P}(\mathrm{IK})$ by asking the model to literally address "With what confidence could you answer this question?", and output an answer like $0 \%, 10 \%, 20 \%, \cdots 100 \%$.</li>
</ul>
<p>We had hoped to observe large benefits from few-shot evaluation out-of-distribution with the natural language approach. In early experiments we did not observe major gains, and so we will use the value head approach in what follows. But we believe it would be worth returning to the natural language approach in the future.
For each question, we generated 30 answer samples at $T=1$. For a given question $Q$, if 20 of the model's sampled answers were correct, and 10 were incorrect, our training set contained 20 copies of the $(Q, \mathrm{IK})$ datapoint and 10 copies of the $(Q, \mathrm{IDK})$ datapoint. This was a convenience to allow us to easily approximate ${ }^{5}$ a soft label for the ground-truth $\mathrm{P}(\mathrm{IK})$ by using many hard labels. We trained with a cross-entropy loss on these labels.</p>
<p>Figure 3 gives some examples of $\mathrm{P}(\mathrm{IK})$ scores from a 52 billion parameter model on a few example questions where the model obviously should or should not know the answers. Note that the value head is technically present at every token position. However, during training, we only backpropagate the loss through the head at the last token position of the input sequence. At test time, we also read off the $\mathrm{P}(\mathrm{IK})$ score for any given sequence as the output from our head at the final token position.</p>
<h3>5.1 Evaluating P(IK) Training and Model Size Trends</h3>
<p>Because we're adding a new untrained head, language models do not perform well zero or few-shot at predicting $\mathrm{P}(\mathrm{IK})$, so we need to finetune them. Since TriviaQA has a large training set, we explore generalization by finetuning $\mathrm{P}(\mathrm{IK})$ only on trivia questions. In order to obtain ground-truth $\mathrm{P}(\mathrm{IK})$ scores that we use for training, we sample 30 answers (at temperature $=1$ ) from each TriviaQA question with a 10 -shot prompt constructed from other random TriviaQA questions and answers. We use a 10 -shot prompt simply to ensure that the models almost always output answers in the correct format, which is important because correctness on TriviaQA questions is judged based on a string comparison.
Our dataset for training and evaluating the classifier then contains datapoints of the form (Few-Shot Prompt + Question, Ground Truth Label). During P(IK) training, we finetune the entire model along with the value head. In Figure 12, we then evaluate this classifier on a held-out set of TriviaQA test questions, and see that the model is able to separate the questions it got correct and incorrect quite well. In particular, $\mathrm{P}(\mathrm{IK})$ is very well calibrated on TriviaQA. As a note, in a later section we train $\mathrm{P}(\mathrm{IK})$ on Lambada, Arithmetic, and Python Function Synthesis problems as well. We used a 10 -shot prompt for both Lambada and Arithmetic, while Python Function Synthesis was done 0 -shot.</p>
<h3>5.2 Out of Distribution Generalization of P(IK)</h3>
<p>Next, we study the generalization of $\mathrm{P}(\mathrm{IK})$ when training only on TriviaQA and then evaluating on Lambada, Arithmetic, GSM8k, Codex HumanEval, and a set of 'naturally occurring' python function synthesis tasks scraped from GitHub (we added this test set because the Codex evaluation is not very large). We are interested in how generalization scales across model sizes, and how performance compares to the case where we do train $\mathrm{P}(\mathrm{IK})$ on these distributions.
We treat GSM8k slightly differently, since it was harder than other tasks. As for TriviaQA, we generated 30 samples @ $T=1$ per question from GSM8k using a 10 -shot prompt of examples to ensure proper formatting.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12 Testing a 52B classifier on a held-out set of TriviaQA questions. We see that the classifier predicts lower values of $\mathrm{P}(\mathrm{IK})$ for the questions it gets incorrect, and higher values of $\mathrm{P}(\mathrm{IK})$ for the questions it gets correct. We set the ground truth $\mathrm{P}(\mathrm{IK})$ as the fraction of samples at $T=1$ that the model gets correct.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13 Left: Full calibration plot of $\mathrm{P}(\mathrm{IK})$ classifiers on TriviaQA over a range of model sizes. We see that the smallest models have higher calibration error than the biggest models. The larger classifiers are very well calibrated in-distribution. Right: We show calibration curves for $\mathrm{P}(\mathrm{IK})$ on three other sampling-based datasets, both in-distribution and out-of-distribution (trained only on TriviaQA). We see that OOD calibration of $\mathrm{P}(\mathrm{IK})$ is often quite poor, and for the most part models are underconfident.</p>
<p>However, the 10-shot prompt for GSM8k included reasoning, as well as the final answer. This meant that each of the 30 samples included model-generated reasoning. Similarly to other tasks, we calculate a ground-truth $\mathrm{P}(\mathrm{IK})$ for each question as just the fraction of $T=1$ samples that are correct. However, since there aren't many problems with a ground truth $P(I K)&gt;0.5$, we sometimes binarize (i.e. split the distribution) using a threshold of 0.25 , instead of 0.5 . We only evaluate the largest model (52B) on GSM8k due to its difficulty.</p>
<p>Figure 14 gives an overview of generalization performance for $\mathrm{P}(\mathrm{IK})$ classifiers that are only trained on TriviaQA. Specifically, we see that generalization gets better as model size increases. Figures 39, 40, and 41 show details of generalization to Mixed-Arithmetic, Python Function Synthesis, and Lambada. We see that there is a general trend where the AUROC of $\mathrm{P}(\mathrm{IK})$ increases with model size, and calibration gets better with model size. However, when testing on Lambada, calibration was terrible, because the model produces uniformly low $\mathrm{P}(\mathrm{IK})$ scores. However, training on all 4 tasks resolves this issue, as shown on the right side of Figure 41.</p>
<p>Table 1 gives an overview comparing generalization to in-distribution performance of $\mathrm{P}(\mathrm{IK})$ scores. Figure 16 gives a more detailed view of how the distributions of $\mathrm{P}(\mathrm{IK})$ changes depending on training data on the 52B model. We see that training on specific $\mathrm{P}(\mathrm{IK})$ distributions helps performance, indicating that there is a significant generalization gap to fill.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14 Model size trend of AUROC with P(IK), when training on only TriviaQA. We generally observe increasing AUROC as model size increases for all three out-of-distribution evals, suggesting that larger P(IK) classifiers are better at generalization.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15 Calibration on GSM8k when training on only TriviaQA vs training on TriviaQA, LAMBADA, Mixed-Arithmetic, and Python Function Synthesis. We see that calibration does improve when training on more diverse data, rather instead of being mostly underconfident the model becomes overconfident. Note that GSM8k was significantly out of the training distribution in all cases.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16 Generalization of $\mathrm{P}(\mathrm{IK})$. We trained $\mathrm{P}(\mathrm{IK})$ classifiers on just TriviaQA and on TriviaQA, Arithmetic, Python Function Synthesis, and LAMBADA. The left side of this figure includes distributions of $\mathrm{P}(\mathrm{IK})$ from a 52B classifier that was trained on just TriviaQA, while the right side includes distributions of $\mathrm{P}(\mathrm{IK})$ from a 52B classifier that was trained on all of these data distributions. We observe nontrivial generalization from TriviaQA to the other tasks, but training on the other tasks improves performance greatly.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Training on TriviaQA <br> (AUROC / Brier Score)</th>
<th style="text-align: center;">Training on All Tasks Except GSM8K <br> (AUROC / Brier Score)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TriviaQA</td>
<td style="text-align: center;">$0.864 / 0.151$</td>
<td style="text-align: center;">$0.873 / 0.145$</td>
</tr>
<tr>
<td style="text-align: center;">Mixed-Arithmetic</td>
<td style="text-align: center;">$0.928 / 0.194$</td>
<td style="text-align: center;">$0.987 / 0.042$</td>
</tr>
<tr>
<td style="text-align: center;">LAMBADA</td>
<td style="text-align: center;">$0.606 / 0.431$</td>
<td style="text-align: center;">$0.853 / 0.108$</td>
</tr>
<tr>
<td style="text-align: center;">Python Func Synth.</td>
<td style="text-align: center;">$0.687 / 0.164$</td>
<td style="text-align: center;">$0.881 / 0.109$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K ${ }^{6}$</td>
<td style="text-align: center;">$0.624 / 0.200$</td>
<td style="text-align: center;">$0.752 / 0.121$</td>
</tr>
</tbody>
</table>
<p>Table 1 AUROC and Brier Scores of $\mathrm{P}(\mathrm{IK})$ on a variety of tasks, comparing training on just TriviaQA and training on everything. All values are computed using 52B parameter classifiers. All AUROC scores here are computed by comparing the model's predicted $\mathrm{P}(\mathrm{IK})$ against the binarized label: 'IK' if the ground truth $\mathrm{P}(\mathrm{IK})&gt;0.5$, and 'IDK' otherwise. Even when we only train on TriviaQA, we see decent generalization to other tasks. However, training on everything does help across all tasks.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17 Left: $\mathrm{P}(\mathrm{IK})$ scores on GSM8k when training on only TriviaQA. Right: $\mathrm{P}(\mathrm{IK})$ scores on GSM8k when training on TriviaQA, LAMBADA, Mixed-Arithmetic, and Python Function Synthesis. Note the threshold for Ground Truth P(IK) is 0.25 here, in contrast to 0.5 in the other histograms from Figure 16 and Table 1</p>
<h1>5.3 P(IK) Generalizes to Account for Source Materials</h1>
<p>If we consider a fairly obscure question like
What state's rodeo hall of fame was established in 2013?
then $\mathrm{P}(\mathrm{IK})$ appropriately predicts a low value, specifically $18 \%$ for a 52 B model. However, if we prepend a Wikipedia article on the Idaho Rodeo Hall of Fame to the context:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Wikipedia</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Idaho</span><span class="w"> </span><span class="n">Rodeo</span><span class="w"> </span><span class="n">Hall</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Fame</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">established</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="mi">501</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="p">)(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">non</span><span class="o">-</span>
<span class="w">    </span><span class="n">profit</span><span class="w"> </span><span class="n">organization</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">May</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mf">2013.</span><span class="w"> </span><span class="n">Lonnie</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Charmy</span><span class="w"> </span><span class="n">LeaVell</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">founders</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">organization</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">charitable</span><span class="w"> </span><span class="n">nonprofit</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="n">was</span>
<span class="w">    </span><span class="n">received</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">IRS</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">February</span><span class="w"> </span><span class="mi">19</span><span class="p">,</span><span class="w"> </span><span class="mf">2014.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">IRHF</span><span class="w"> </span><span class="n">hosts</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">reunion</span><span class="w"> </span><span class="ow">and</span>
<span class="w">    </span><span class="n">induction</span><span class="w"> </span><span class="n">ceremony</span><span class="w"> </span><span class="n">annually</span><span class="w"> </span><span class="n">every</span><span class="w"> </span><span class="n">October</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Idaho</span><span class="w"> </span><span class="n">Hall</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Fame</span><span class="w"> </span><span class="n">preserves</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">promotes</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Western</span><span class="w"> </span><span class="n">lifestyle</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">heritage</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">hall</span><span class="w"> </span><span class="n">exists</span><span class="w"> </span><span class="n">to</span>
<span class="w">    </span><span class="n">dedicate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">men</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">women</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">rodeo</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">contribute</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">ranching</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">farming</span>
<span class="w">    </span><span class="n">through</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">sport</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="k">extends</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">reach</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="k">continue</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">western</span>
<span class="w">    </span><span class="n">ways</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">youth</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">communities</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">ensure</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">traditions</span>
<span class="w">    </span><span class="k">continue</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">generations</span><span class="o">.</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="mi">2015</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">hall</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">awarded</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Historic</span>
<span class="w">    </span><span class="n">Preservation</span><span class="w"> </span><span class="n">Recognition</span><span class="w"> </span><span class="n">Award</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">National</span><span class="w"> </span><span class="n">Society</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Daughters</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">American</span><span class="w"> </span><span class="n">Revolution</span><span class="o">.</span>
</code></pre></div>

<p>What state's rodeo hall of fame was established in 2013?</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18 Effect of including Wikipedia article on $\mathrm{P}(\mathrm{IK})$ for TriviaQA Questions. We see that including a relevant Wikipedia article in the context boosts the average $\mathrm{P}(\mathrm{IK})$ on TriviaQA Questions. $\mathrm{P}(\mathrm{IK})$ increases more for shorter Wikipedia articles, from which it is presumably easier to identify the relevant facts.
then the $\mathrm{P}(\mathrm{IK})$ score rises to $78 \%$. In other words, without any further training, $\mathrm{P}(\mathrm{IK})$ generalizes to address whether the language model can find answers to questions in source materials within its context.</p>
<p>We demonstrate this phenomenon quantitatively using questions from TriviaQA, by comparing $\mathrm{P}(\mathrm{IK})$ evaluated both with and without accompanying reference material. Questions from TriviaQA come with Wikipedia documents that are relevant to each questions. For each question, we then compute $\mathrm{P}(\mathrm{IK})$ both with an without the reference document. Specifically, the format for the prompts are as follows:</p>
<h1>With Background Material:</h1>
<div class="codehilite"><pre><span></span><code>Here is some background information material: &lt;BACKGROUND_MATERIAL&gt;
Now, answer the following question.
Question: Which Lloyd Webber musical premiered in the US on 10th
    December 1993?
</code></pre></div>

<p>Answer:</p>
<h2>Without Background Material:</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">Lloyd</span><span class="w"> </span><span class="n">Webber</span><span class="w"> </span><span class="n">musical</span><span class="w"> </span><span class="n">premiered</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">US</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="mi">10</span><span class="n">th</span>
<span class="w">    </span><span class="n">December</span><span class="w"> </span><span class="mi">1993</span><span class="o">?</span>
</code></pre></div>

<h2>Answer:</h2>
<p>Whenever the full article was excessively long, we needed to truncate it to 7000 tokens so that the article and the question would always fit in the model context. Figure 18 summarizes the effects of including Wikipedia articles on $\mathrm{P}(\mathrm{IK})$. We see that including the article increases $\mathrm{P}(\mathrm{IK})$. Furthermore, shorter articles increase $\mathrm{P}(\mathrm{IK})$ more. Presumably this is because the correct answer is easier to extract from shorter articles than longer articles.</p>
<h2>5.4 P(IK) Generalizes to Account for Hints Towards GSM8k Solutions</h2>
<p>In this section we study how hints towards the solution of GSM8k problems affect P(IK) scores. Specifically, we add hints to the problem statement using the following format:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Students</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="kd">class</span><span class="w"> </span><span class="mi">3</span><span class="n">B</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">collecting</span><span class="w"> </span><span class="n">school</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">behavior</span>
<span class="w">    </span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="kd">get</span><span class="w"> </span><span class="n">enough</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">total</span><span class="o">,</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">go</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">trip</span><span class="o">.</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="kd">class</span><span class="o">,</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">Adam</span><span class="o">,</span><span class="w"> </span><span class="n">Martha</span><span class="o">,</span><span class="w"> </span><span class="n">Betty</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Tom</span><span class="o">.</span><span class="w"> </span><span class="n">Adam</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">collected</span><span class="w"> </span><span class="mi">50</span>
<span class="w">        </span><span class="n">points</span><span class="o">.</span><span class="w"> </span><span class="n">Betty</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">better</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">Adam</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">collected</span><span class="w"> </span><span class="mi">30</span><span class="o">%</span><span class="w"> </span><span class="n">more</span><span class="o">.</span><span class="w"> </span><span class="n">Marta</span>
<span class="w">    </span><span class="n">managed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">collect</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">Tom</span><span class="o">,</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">30</span><span class="w"> </span><span class="n">points</span>
<span class="w">    </span><span class="n">less</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">Betty</span><span class="o">.</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="kd">class</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">go</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">trip</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">minimum</span><span class="w"> </span><span class="n">threshold</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">400</span><span class="w"> </span><span class="n">points</span><span class="o">?</span>
</code></pre></div>

<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19 Effect of hints on $\mathrm{P}(\mathrm{IK})$ applied to GSM8k - all of these results represent generalization, as the models were not trained on GSM8k. Left: We see that showing more of the GSM8k hint results in higher $\mathrm{P}(\mathrm{IK})$. The effect is more consistent for the model trained on all 4 tasks (TriviaQA, LAMBADA, Arithmetic, and Python Function Synthesis), rather than the one trained on just TriviaQA. Right: We evaluate the model trained on all 4 other tasks on various hints. We see lower $\mathrm{P}(\mathrm{IK})$ scores for bad hints (though the models are partially fooled), and actual decreases in the $\mathrm{P}(\mathrm{IK})$ score when the hints are irrelevant because they come from other questions.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Here</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nl">hint</span><span class="p">:</span><span class="w"> </span><span class="n">Betty</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">30</span><span class="o">%</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">Adam</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">it</span><span class="s1">&#39;s 30/100 * 50</span>
<span class="s1">    = &lt;&lt;30/100*50=15&gt;&gt;15 points more.</span>
<span class="s1">Betty&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">therefore</span><span class="w"> </span><span class="mi">50</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">15</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="mi">50</span><span class="o">+</span><span class="mi">15</span><span class="o">=</span><span class="mi">65</span><span class="o">&gt;&gt;</span><span class="mi">65</span><span class="w"> </span><span class="n">points</span><span class="p">.</span>
<span class="n">Tom</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">30</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">less</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">Betty</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">he</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">65</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">30</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="mi">65</span><span class="o">-</span><span class="mi">30</span><span class="o">=</span><span class="mi">35</span><span class="o">&gt;&gt;</span><span class="mi">35</span>
<span class="w">    </span><span class="n">points</span><span class="p">.</span>
<span class="n">Marta</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">Tom</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">she</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">35</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="o">&lt;&lt;</span><span class="mi">3</span><span class="o">*</span><span class="mi">35</span><span class="o">=</span><span class="mi">105</span><span class="o">&gt;&gt;</span><span class="mi">105</span><span class="w"> </span><span class="n">points</span><span class="p">.</span>
<span class="ow">In</span><span class="w"> </span><span class="n">total</span><span class="p">,</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">students</span><span class="w"> </span><span class="n">collected</span><span class="w"> </span><span class="mi">50</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">65</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">35</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">105</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="o">&lt;&lt;</span><span class="mi">50</span><span class="o">+</span><span class="mi">65</span><span class="o">+</span><span class="mi">35</span><span class="o">+</span><span class="mi">105</span><span class="o">=</span><span class="mi">255</span><span class="o">&gt;&gt;</span><span class="mi">255</span><span class="w"> </span><span class="n">points</span><span class="p">.</span>
<span class="n">So</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="mi">400</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">255</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="mi">400</span><span class="o">-</span><span class="mi">255</span><span class="o">=</span><span class="mi">145</span><span class="o">&gt;&gt;</span><span class="mi">145</span><span class="w"> </span><span class="n">points</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">on</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">trip</span><span class="p">.</span>
</code></pre></div>

<h1>Answer:</h1>
<p>We test two variations on hints. First, we vary the amount of information given away by the hint by truncating hints at fixed fractions of their total length. Second, we compare how $\mathrm{P}(\mathrm{IK})$ is affected by good, incorrect, and distracting hints. Bad hints are acquired by generating chain-of-thought samples from our 52B model and selecting only the samples that lead to an incorrect answer. Distracting hints are simply hints that were generated for a different question, so that they are irrelevant to the question at hand.
Figure 19 shows the result of evaluating GSM8k problems with hints using our 52B P(IK) model. We see that (1) showing more of the hint generally leads to higher $\mathrm{P}(\mathrm{IK})$, (2) good hints that lead to the correct answer result in higher $\mathrm{P}(\mathrm{IK})$ scores than bad hints, and (3) the $\mathrm{P}(\mathrm{IK})$ model that was trained on TriviaQA, LAMBADA, Arithmetic, and Python Function Synthesis performs better and more consistently, especially with partial hints.</p>
<h3>5.5 Comparing Models Trained with Distinct Pretraining Distributions</h3>
<p>We would like $\mathrm{P}(\mathrm{IK})$ to truly capture model self-knowledge. Thus we would like to distinguish between this and an alternative hypothesis, that $\mathrm{P}(\mathrm{IK})$ is merely capturing something like the intrinsic difficulty of tasks. In order to try to disentangle these explanations, we studied two 12B models (with identical architecture) that were pretrained on distinct data distributions - one was trained with four repetitions of a high quality dataset, while the other uses a single copy of that dataset mixed with a single copy of a larger but lowerquality distribution of webdata. We refer to these pretrained language models as $A$ and $B$ in this section. We finetuned both models for $\mathrm{P}(\mathrm{IK})$ on the questions from the TriviaQA training set that each model got correct.
These models get many of the same questions correct and incorrect. So our focus here will be on the subset of questions that one model answers correctly and the other incorrectly. In the TriviaQA test set there were</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Questions that only A gets right</th>
<th style="text-align: center;">Questions only B gets right</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A's average P(IK)</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.408</td>
</tr>
<tr>
<td style="text-align: left;">B's average P(IK)</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.477</td>
</tr>
</tbody>
</table>
<p>Table 2 'Cross-Experiments': Average $\mathrm{P}(\mathrm{IK})$ from for two distinct models on subsets of TriviaQA questions where one model is correct and the other is wrong. We see that the entries in the major diagonal are larger than the entries in the minor diagonal, showing that there is some signal that $\mathrm{P}(\mathrm{IK})$ is encoding model-specific information. However, the difference in $\mathrm{P}(\mathrm{IK})$ is only around $6 \%$, so there is room for improvement from future work.
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20 Scatterplot that disambiguates numbers in Table 2. We evaluate 2 distinct 12B models on TriviaQA, separating the data into questions each model gets correct and the other gets incorrect. The scatterplot depicts the $\mathrm{P}(\mathrm{IK})$ scores from each model on both of these data subsets.</p>
<p>650 and 826 such questions, respectively, that model A or B got correct but that the other got wrong. Table 2 shows that each model has a noticeably higher $\mathrm{P}(\mathrm{IK})$ for the questions that they get correct, compared to the questions that the other model gets right. In Figure 20 we show a scatter plot of $\mathrm{P}(\mathrm{IK})$ for models A and B, colored by which model got the questions right. The differences between the distributions are small but noticeable.</p>
<p>We also tried a second experiment, in order to try to get insight as to whether $\mathrm{P}(\mathrm{IK})$ leverages features specifically relevant to self-knowledge. For this purpose we finetuned both the $A$ and $B$ pretrained models on the ground-truth $\mathrm{P}(\mathrm{IK})$ data from each model. So for example, we started from model $A$, and finetuned it on the ground-truth $\mathrm{P}(\mathrm{IK})$ data from model $A$ and, separately, on the data from model $B$ (resulting in two distinct finetuned snapshots). We might hope to find that training a $\mathrm{P}(\mathrm{IK})$ classifier to predict whether or not model $A$ knows the answer to a question would work better when starting from model $A$ itself as the initial checkpoint, as compared to if we start from model $B$. Table 3 contains results for this experiment.</p>
<p>We find mixed but encouraging results here: when testing on the ground-truth $\mathrm{P}(\mathrm{IK})$ data from model A, both starting from model A and starting from model B seems to give comparable performance. However, when testing on ground-truth $\mathrm{P}(\mathrm{IK})$ data from model B , starting from model B seems to do better than starting from model A. We also tried focusing on only the questions where the two models differ, but we found similar results in that case.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ See section XYZ for details on the setup for GSM8k. In this table, we binarized the ground-truth $\mathrm{P}(\mathrm{IK})$ with a threshold of 0.5 , like all other evals. Also, note that the Brier score for GSM8k tasks are somewhat misleading, since it is overwhelmingly dominated by questions the model gets incorrect and has low $\mathrm{P}(\mathrm{IK})$ for.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
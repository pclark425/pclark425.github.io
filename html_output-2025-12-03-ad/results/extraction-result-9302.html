<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9302 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9302</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9302</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-253420466</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.04668v1.pdf" target="_blank">Zero-Label Prompt Selection</a></p>
                <p><strong>Paper Abstract:</strong> Natural language prompts have been shown to facilitate cross-task generalization for large language models. However, with no or limited labeled examples, the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels. To address the issue, we propose a Zero-Label Prompt Selection (ZPS) method that selects prompts without any labeled data or gradient update. Specifically, given the candidate human-written prompts for a task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the pseudo-labels for prompt selection. Experiments show that ZPS improves over prior methods by a sizeable margin in zero-label performance. We also extend ZPS to a few-shot setting and show its advantages over strong baselines such as prompt tuning and model tuning.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9302.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9302.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZPS_zero_label</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Label Prompt Selection (ZPS) - zero-label use</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tuning-free method that selects a single high-performing natural-language prompt from a candidate set using pseudo-labels produced by an ensemble of prompts applied to unlabeled task data; evaluated in a zero-label setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks: NLI, coreference, sentence completion, WSD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of 11 held-out tasks (RTE, CB, ANLI R1-R3, COPA, HellaSwag, StoryCloze, WSC, Winogrande, WiC) used to measure cross-task generalization of prompted LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label prompting: apply human-written prompts (templates + verbalizers) with no labeled examples or gradient updates; use the prompt ensemble to pseudo-label unlabeled data and compute per-prompt 'pseudo accuracy' against ensemble labels to select one prompt to use at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Average over all candidate prompts (T0 baseline); self-training; ensemble distillation. (i.e., selection-free and label-using baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: ZPS outperforms the T0 baseline average of candidate prompts in zero-label setting across the 11 test tasks (numerical ensemble-strategy table suggests ensemble-based methods yield average accuracies in the low-to-mid 60s; ZPS selection improves over mean prompt performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to T0 baseline (mean over candidate prompts) ZPS yields higher zero-label performance (paper reports 'ZPS outperforms the T0 baseline' across the 11 tasks); also shows advantage versus some few-shot methods (GRIPS, ICL, PT) in zero-label setting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Selecting a single prompt via agreement with a pseudo-label ensemble yields better performance than using the average of prompts; pseudo labels from an ensemble are assumed to have higher accuracy than single-prompt predictions and thus provide a useful zero-label signal for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Unlabeled data constructed by removing labels of the training split for each test task; candidate prompts taken from PromptSource/T0 prompt candidates; prompt templates and verbalizers defined per task; selection pipeline: prompt filtering via confidence clustering (k-means on per-prompt confidence), ensemble pseudo-labeling, compute pseudo-accuracy (proportion agreement) and select top prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9302.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble_strategy_comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ensemble aggregation strategies (log-probability mean, probability mean, majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of three ways to combine prompt outputs into ensemble pseudo-labels: mean of log probabilities, mean of probabilities, and majority vote, showing log-probability mean performs best in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 11 test tasks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label ensemble pseudo-labeling using three aggregation schemes applied to prompt outputs on unlabeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Majority vote; probability mean; log-probability mean (the proposed best strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracies reported per-ensemble: Majority vote avg = 63.04; Probability mean avg = 62.74; Log-probability mean (Ours) avg = 63.54 (per Table 3 in paper). Per-task numbers also reported (see paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Log-probability mean outperforms probability mean by ~0.8 points (63.54 vs 62.74 avg) and majority vote by ~0.5 points (63.54 vs 63.04 avg) across the 11 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.5 to +0.8 percentage points average accuracy for log-prob mean vs other ensemble strategies</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Log probabilities can penalize choices that are very unlikely, making the aggregated score more robust to extreme low-probability outputs from some prompts; this yields slightly better ensemble pseudo-label quality and downstream selection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ensemble over filtered prompts after confidence-based filtering; per-example aggregation using either average of log P(y), average of P(y), or majority vote over per-prompt argmax predictions; ensemble outputs used to compute pseudo labels on unlabeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9302.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_filtering_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt filtering (confidence clustering) effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Filtering candidate prompts by clustering per-prompt confidence scores (k-means into two clusters) and discarding the lower-confidence cluster improves ensemble and selected-prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 11 test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label pipeline with an initial prompt filtering step: compute per-prompt confidence (difference between top and second top LM probabilities across unlabeled samples), cluster prompts into high/low-confidence and discard low-confidence prompts before ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZPS with filtering vs ZPS without filtering (no clustering-based removal).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: prompt filtering 'boosts the performance of ZPS for almost all tasks' as reported; even without filtering ZPS still outperforms other baselines but filtering increases ensemble accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Filtering yields consistent improvements across tasks (paper reports tables demonstrating higher accuracies with filtering; exact per-task deltas are in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Removing low-confidence prompts increases the quality of the ensemble pseudo-labels, reducing noise and improving the signal used for prompt selection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Confidence per prompt computed as average margin between top and second top predicted verbalizer-probabilities across unlabeled data; prompts clustered using k-means into two groups and the lower-confidence group discarded; remaining prompts ensembled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9302.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robustness_adversarial_prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robustness to adversarial/low-performing prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ZPS remains beneficial when candidate prompt sets are contaminated with adversarial low-performing prompts generated via a genetic method: replacing varying fractions of candidates with low-performing prompts still yields consistent improvements over selecting the mean of candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 11 test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label prompt selection with candidate sets contaminated by adversarial low-performing prompts at ratios from 0.1 to 0.8.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZPS-selected prompt performance vs mean performance of candidate prompts (no selection) under varying contamination ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: ZPS yields consistent improvements across contamination ratios from 0.1 to 0.8; paper reports aggregated Table 5 demonstrating this robustness (exact numeric entries not reproduced in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt filtering and ensemble-based pseudo-labeling let ZPS identify and avoid low-quality prompts even when many such prompts are present; selection relies on consensus signals that remain informative under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Adversarial low-performing prompts were generated via GPS (genetic prompt search) by selecting the worst-performing prompts on a validation set; random replacements of original prompts repeated with 5 seeds; contamination ratio varied from 0.1 to 0.8.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9302.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fewshot_pseudo_val</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using pseudo-labeled validation sets in few-shot model tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In few-shot (32 labeled examples) model tuning, using pseudo-labeled unlabeled data for checkpoint and prompt selection (so that all labeled data can be used for training) yields higher performance than the classic splitting of labeled data into train/val.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (model tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot tuning with 32 labeled examples per task and access to unlabeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot model tuning where D32 (all 32 labeled) used for training and pseudo-labeled unlabeled examples (top confidently pseudo-labeled samples, expanded) used for checkpoint and prompt selection ('more pseudo val' strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Classic 16+16 labeled split for train/validation; using 32 pseudo_train (train only on pseudo-labeled samples and labeled data used only for validation); 32 pseudo_val (train on all labeled, use small pseudo-labeled val); 'more pseudo val' (larger pseudo-labeled val set) â€” the paper adopts 'more pseudo val'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported average accuracies: '16 + 16' mean = 63.88; '32 pseudo train' mean = 62.53; '32 pseudo val' mean = 66.00; 'More pseudo val' (adopted ZPS approach) mean = 66.32 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>'More pseudo val' improves average accuracy by +2.44 points over 16+16 (66.32 vs 63.88) and improves modestly over '32 pseudo val' (+0.32). '32 pseudo train' performs worst (62.53) indicating pseudo labels are noisy for training.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.44 percentage points average accuracy for 'more pseudo val' vs classic 16+16 few-shot split</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using pseudo-labeled data for checkpoint/prompt selection frees all scarce labeled data for training, yielding better trained models; however, pseudo-labeled data is noisy so it should not be used as primary training data (hence '32 pseudo train' performs poorly).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot set consists of 32 labeled examples; pseudo-labels produced by ensemble; top-K confident pseudo-labeled examples (e.g., top-32) used for val; training uses all labeled examples; optimizer Adafactor, batch size 4, learning rates reported (5e-5 for MT and self-training).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9302.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL_limit_T0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning (ICL) performance with T0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ICL (priming with a few labeled examples) performs substantially worse with T0 on the evaluated tasks, likely because the priming style differs from the multitask training prompts used for T0.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot in-context learning where 2 priming examples per task are concatenated to test inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>In-context learning with 2 demonstration examples per prompt (ICL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against parameter-tuning methods (MT, PT), search-based prompt methods (GRIPS, GPS), and MT+ZPS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy for ICL reported as 55.86 (Table in paper); substantially below other few-shot methods (e.g., MT avg 65.27, MT+ZPS avg 66.45).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ICL avg 55.86 vs MT avg 65.27 (approx -9.4 points lower) and vs MT+ZPS 66.45 (approx -10.6 points lower).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-9 to -11 percentage points average accuracy (ICL underperforms tuned methods and ZPS-augmented tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The priming prompt used for ICL likely differs substantially from the task descriptions T0 was multitask-trained with, reducing the usefulness of priming demonstrations for T0.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For ICL, 2 examples randomly selected from the training set composed the priming prompt; results reported averaged over candidate prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9302.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scale_effects_ZPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZPS effect across model scales</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ZPS improves performance across different T0 model sizes (T0-Large ~770M, T0-3B, T0-11B), indicating the method's effectiveness is not limited to a single model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M / 3B / 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>T0 benchmark (11 test tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-label prompt selection experiments repeated on different T0 model sizes to test scale robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label ZPS prompt selection applied to different pretrained T0 model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Per-scale: ZPS vs non-selection baseline (mean over prompts) for each model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Table 6 indicates ZPS boosts performance on T0-3B and T0-Large (770M) as well as on T0-11B; exact per-scale numeric values are in Table 6 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The ZPS selection mechanism relies on ensemble agreement signals and is robust to model capacity differences; gains persist though absolute accuracies vary with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments replicated on multiple T0 scales; prompt candidate pools and unlabeled data constructed in same manner as primary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9302.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9302.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RTE_case_study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RTE prompt selection case study (pseudo-accuracy vs small labeled val)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In a detailed case study on RTE prompts, ZPS pseudo-accuracy correlates better with true test accuracy than selection based on a small labeled validation set (16 examples), demonstrating superior prompt-ranking capability without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RTE (Recognizing Textual Entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural language inference binary classification task; case study uses 9 human-written RTE prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-label pseudo-accuracy computed by ZPS vs D16 Acc. (accuracy computed on 16 labeled validation examples) used for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompt ranking by D16 labeled validation accuracy vs ranking by ZPS pseudo-accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-prompt comparisons in Table 8 show pseudo-accuracy from ZPS has better correlation with post-hoc test accuracy than D16 Acc.; examples show some prompts with higher pseudo-acc correspond to higher test accuracy where D16 did not indicate that.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>D16 (16-label validation) shows poor prompt selection ability and low correlation with true test accuracy; ZPS pseudo-accuracy better distinguishes prompt quality in absence of labels.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Small labeled validation sets provide noisy and unreliable ranking of prompts; ensemble-based pseudo-label agreement provides a more stable proxy for true performance, enabling better zero-label prompt selection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Nine RTE prompts evaluated; reported quantities: D16 Acc (accuracy on 16 labeled examples), Pseudo Acc (ZPS pseudo-accuracy), Posthoc Acc (true test set accuracy); table demonstrates relative correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-Label Prompt Selection', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>Grips: Gradient-free, edit-based instruction search for prompting large language models <em>(Rating: 2)</em></li>
                <li>GPS <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>Promptsource: An integrated development environment and repository for natural language prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9302",
    "paper_id": "paper-253420466",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "ZPS_zero_label",
            "name_full": "Zero-Label Prompt Selection (ZPS) - zero-label use",
            "brief_description": "A tuning-free method that selects a single high-performing natural-language prompt from a candidate set using pseudo-labels produced by an ensemble of prompts applied to unlabeled task data; evaluated in a zero-label setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks: NLI, coreference, sentence completion, WSD)",
            "task_description": "A set of 11 held-out tasks (RTE, CB, ANLI R1-R3, COPA, HellaSwag, StoryCloze, WSC, Winogrande, WiC) used to measure cross-task generalization of prompted LLMs.",
            "presentation_format": "Zero-label prompting: apply human-written prompts (templates + verbalizers) with no labeled examples or gradient updates; use the prompt ensemble to pseudo-label unlabeled data and compute per-prompt 'pseudo accuracy' against ensemble labels to select one prompt to use at inference.",
            "comparison_format": "Average over all candidate prompts (T0 baseline); self-training; ensemble distillation. (i.e., selection-free and label-using baselines)",
            "performance": "Qualitative: ZPS outperforms the T0 baseline average of candidate prompts in zero-label setting across the 11 test tasks (numerical ensemble-strategy table suggests ensemble-based methods yield average accuracies in the low-to-mid 60s; ZPS selection improves over mean prompt performance).",
            "performance_comparison": "Compared to T0 baseline (mean over candidate prompts) ZPS yields higher zero-label performance (paper reports 'ZPS outperforms the T0 baseline' across the 11 tasks); also shows advantage versus some few-shot methods (GRIPS, ICL, PT) in zero-label setting.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Selecting a single prompt via agreement with a pseudo-label ensemble yields better performance than using the average of prompts; pseudo labels from an ensemble are assumed to have higher accuracy than single-prompt predictions and thus provide a useful zero-label signal for selection.",
            "null_or_negative_result": false,
            "experimental_details": "Unlabeled data constructed by removing labels of the training split for each test task; candidate prompts taken from PromptSource/T0 prompt candidates; prompt templates and verbalizers defined per task; selection pipeline: prompt filtering via confidence clustering (k-means on per-prompt confidence), ensemble pseudo-labeling, compute pseudo-accuracy (proportion agreement) and select top prompt.",
            "uuid": "e9302.0",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Ensemble_strategy_comparison",
            "name_full": "Comparison of ensemble aggregation strategies (log-probability mean, probability mean, majority vote)",
            "brief_description": "Empirical comparison of three ways to combine prompt outputs into ensemble pseudo-labels: mean of log probabilities, mean of probabilities, and majority vote, showing log-probability mean performs best in this setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Same 11 test tasks as above.",
            "presentation_format": "Zero-label ensemble pseudo-labeling using three aggregation schemes applied to prompt outputs on unlabeled data.",
            "comparison_format": "Majority vote; probability mean; log-probability mean (the proposed best strategy).",
            "performance": "Average accuracies reported per-ensemble: Majority vote avg = 63.04; Probability mean avg = 62.74; Log-probability mean (Ours) avg = 63.54 (per Table 3 in paper). Per-task numbers also reported (see paper table).",
            "performance_comparison": "Log-probability mean outperforms probability mean by ~0.8 points (63.54 vs 62.74 avg) and majority vote by ~0.5 points (63.54 vs 63.04 avg) across the 11 tasks.",
            "format_effect_size": "+0.5 to +0.8 percentage points average accuracy for log-prob mean vs other ensemble strategies",
            "explanation_or_hypothesis": "Log probabilities can penalize choices that are very unlikely, making the aggregated score more robust to extreme low-probability outputs from some prompts; this yields slightly better ensemble pseudo-label quality and downstream selection.",
            "null_or_negative_result": false,
            "experimental_details": "Ensemble over filtered prompts after confidence-based filtering; per-example aggregation using either average of log P(y), average of P(y), or majority vote over per-prompt argmax predictions; ensemble outputs used to compute pseudo labels on unlabeled examples.",
            "uuid": "e9302.1",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Prompt_filtering_effect",
            "name_full": "Prompt filtering (confidence clustering) effect",
            "brief_description": "Filtering candidate prompts by clustering per-prompt confidence scores (k-means into two clusters) and discarding the lower-confidence cluster improves ensemble and selected-prompt performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Same 11 test tasks.",
            "presentation_format": "Zero-label pipeline with an initial prompt filtering step: compute per-prompt confidence (difference between top and second top LM probabilities across unlabeled samples), cluster prompts into high/low-confidence and discard low-confidence prompts before ensemble.",
            "comparison_format": "ZPS with filtering vs ZPS without filtering (no clustering-based removal).",
            "performance": "Qualitative: prompt filtering 'boosts the performance of ZPS for almost all tasks' as reported; even without filtering ZPS still outperforms other baselines but filtering increases ensemble accuracy.",
            "performance_comparison": "Filtering yields consistent improvements across tasks (paper reports tables demonstrating higher accuracies with filtering; exact per-task deltas are in Table 4).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Removing low-confidence prompts increases the quality of the ensemble pseudo-labels, reducing noise and improving the signal used for prompt selection.",
            "null_or_negative_result": false,
            "experimental_details": "Confidence per prompt computed as average margin between top and second top predicted verbalizer-probabilities across unlabeled data; prompts clustered using k-means into two groups and the lower-confidence group discarded; remaining prompts ensembled.",
            "uuid": "e9302.2",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Robustness_adversarial_prompts",
            "name_full": "Robustness to adversarial/low-performing prompts",
            "brief_description": "ZPS remains beneficial when candidate prompt sets are contaminated with adversarial low-performing prompts generated via a genetic method: replacing varying fractions of candidates with low-performing prompts still yields consistent improvements over selecting the mean of candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Same 11 test tasks.",
            "presentation_format": "Zero-label prompt selection with candidate sets contaminated by adversarial low-performing prompts at ratios from 0.1 to 0.8.",
            "comparison_format": "ZPS-selected prompt performance vs mean performance of candidate prompts (no selection) under varying contamination ratios.",
            "performance": "Qualitative: ZPS yields consistent improvements across contamination ratios from 0.1 to 0.8; paper reports aggregated Table 5 demonstrating this robustness (exact numeric entries not reproduced in text).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Prompt filtering and ensemble-based pseudo-labeling let ZPS identify and avoid low-quality prompts even when many such prompts are present; selection relies on consensus signals that remain informative under noise.",
            "null_or_negative_result": false,
            "experimental_details": "Adversarial low-performing prompts were generated via GPS (genetic prompt search) by selecting the worst-performing prompts on a validation set; random replacements of original prompts repeated with 5 seeds; contamination ratio varied from 0.1 to 0.8.",
            "uuid": "e9302.3",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Fewshot_pseudo_val",
            "name_full": "Using pseudo-labeled validation sets in few-shot model tuning",
            "brief_description": "In few-shot (32 labeled examples) model tuning, using pseudo-labeled unlabeled data for checkpoint and prompt selection (so that all labeled data can be used for training) yields higher performance than the classic splitting of labeled data into train/val.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (model tuning)",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Few-shot tuning with 32 labeled examples per task and access to unlabeled examples.",
            "presentation_format": "Few-shot model tuning where D32 (all 32 labeled) used for training and pseudo-labeled unlabeled examples (top confidently pseudo-labeled samples, expanded) used for checkpoint and prompt selection ('more pseudo val' strategy).",
            "comparison_format": "Classic 16+16 labeled split for train/validation; using 32 pseudo_train (train only on pseudo-labeled samples and labeled data used only for validation); 32 pseudo_val (train on all labeled, use small pseudo-labeled val); 'more pseudo val' (larger pseudo-labeled val set) â€” the paper adopts 'more pseudo val'.",
            "performance": "Reported average accuracies: '16 + 16' mean = 63.88; '32 pseudo train' mean = 62.53; '32 pseudo val' mean = 66.00; 'More pseudo val' (adopted ZPS approach) mean = 66.32 (Table 7).",
            "performance_comparison": "'More pseudo val' improves average accuracy by +2.44 points over 16+16 (66.32 vs 63.88) and improves modestly over '32 pseudo val' (+0.32). '32 pseudo train' performs worst (62.53) indicating pseudo labels are noisy for training.",
            "format_effect_size": "+2.44 percentage points average accuracy for 'more pseudo val' vs classic 16+16 few-shot split",
            "explanation_or_hypothesis": "Using pseudo-labeled data for checkpoint/prompt selection frees all scarce labeled data for training, yielding better trained models; however, pseudo-labeled data is noisy so it should not be used as primary training data (hence '32 pseudo train' performs poorly).",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot set consists of 32 labeled examples; pseudo-labels produced by ensemble; top-K confident pseudo-labeled examples (e.g., top-32) used for val; training uses all labeled examples; optimizer Adafactor, batch size 4, learning rates reported (5e-5 for MT and self-training).",
            "uuid": "e9302.4",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "ICL_limit_T0",
            "name_full": "In-Context Learning (ICL) performance with T0",
            "brief_description": "ICL (priming with a few labeled examples) performs substantially worse with T0 on the evaluated tasks, likely because the priming style differs from the multitask training prompts used for T0.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Few-shot in-context learning where 2 priming examples per task are concatenated to test inputs.",
            "presentation_format": "In-context learning with 2 demonstration examples per prompt (ICL).",
            "comparison_format": "Compared against parameter-tuning methods (MT, PT), search-based prompt methods (GRIPS, GPS), and MT+ZPS.",
            "performance": "Average accuracy for ICL reported as 55.86 (Table in paper); substantially below other few-shot methods (e.g., MT avg 65.27, MT+ZPS avg 66.45).",
            "performance_comparison": "ICL avg 55.86 vs MT avg 65.27 (approx -9.4 points lower) and vs MT+ZPS 66.45 (approx -10.6 points lower).",
            "format_effect_size": "-9 to -11 percentage points average accuracy (ICL underperforms tuned methods and ZPS-augmented tuning)",
            "explanation_or_hypothesis": "The priming prompt used for ICL likely differs substantially from the task descriptions T0 was multitask-trained with, reducing the usefulness of priming demonstrations for T0.",
            "null_or_negative_result": false,
            "experimental_details": "For ICL, 2 examples randomly selected from the training set composed the priming prompt; results reported averaged over candidate prompts.",
            "uuid": "e9302.5",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Scale_effects_ZPS",
            "name_full": "ZPS effect across model scales",
            "brief_description": "ZPS improves performance across different T0 model sizes (T0-Large ~770M, T0-3B, T0-11B), indicating the method's effectiveness is not limited to a single model scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 family",
            "model_size": "770M / 3B / 11B",
            "task_name": "T0 benchmark (11 test tasks)",
            "task_description": "Zero-label prompt selection experiments repeated on different T0 model sizes to test scale robustness.",
            "presentation_format": "Zero-label ZPS prompt selection applied to different pretrained T0 model sizes.",
            "comparison_format": "Per-scale: ZPS vs non-selection baseline (mean over prompts) for each model size.",
            "performance": "Qualitative: Table 6 indicates ZPS boosts performance on T0-3B and T0-Large (770M) as well as on T0-11B; exact per-scale numeric values are in Table 6 of the paper.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The ZPS selection mechanism relies on ensemble agreement signals and is robust to model capacity differences; gains persist though absolute accuracies vary with model size.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments replicated on multiple T0 scales; prompt candidate pools and unlabeled data constructed in same manner as primary experiments.",
            "uuid": "e9302.6",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "RTE_case_study",
            "name_full": "RTE prompt selection case study (pseudo-accuracy vs small labeled val)",
            "brief_description": "In a detailed case study on RTE prompts, ZPS pseudo-accuracy correlates better with true test accuracy than selection based on a small labeled validation set (16 examples), demonstrating superior prompt-ranking capability without labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "11B",
            "task_name": "RTE (Recognizing Textual Entailment)",
            "task_description": "Natural language inference binary classification task; case study uses 9 human-written RTE prompts.",
            "presentation_format": "Zero-label pseudo-accuracy computed by ZPS vs D16 Acc. (accuracy computed on 16 labeled validation examples) used for selection.",
            "comparison_format": "Prompt ranking by D16 labeled validation accuracy vs ranking by ZPS pseudo-accuracy.",
            "performance": "Per-prompt comparisons in Table 8 show pseudo-accuracy from ZPS has better correlation with post-hoc test accuracy than D16 Acc.; examples show some prompts with higher pseudo-acc correspond to higher test accuracy where D16 did not indicate that.",
            "performance_comparison": "D16 (16-label validation) shows poor prompt selection ability and low correlation with true test accuracy; ZPS pseudo-accuracy better distinguishes prompt quality in absence of labels.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Small labeled validation sets provide noisy and unreliable ranking of prompts; ensemble-based pseudo-label agreement provides a more stable proxy for true performance, enabling better zero-label prompt selection.",
            "null_or_negative_result": false,
            "experimental_details": "Nine RTE prompts evaluated; reported quantities: D16 Acc (accuracy on 16 labeled examples), Pseudo Acc (ZPS pseudo-accuracy), Posthoc Acc (true test set accuracy); table demonstrates relative correlations.",
            "uuid": "e9302.7",
            "source_info": {
                "paper_title": "Zero-Label Prompt Selection",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2,
            "sanitized_title": "multitask_prompted_training_enables_zeroshot_task_generalization"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "rating": 2,
            "sanitized_title": "grips_gradientfree_editbased_instruction_search_for_prompting_large_language_models"
        },
        {
            "paper_title": "GPS",
            "rating": 1
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "Promptsource: An integrated development environment and repository for natural language prompts",
            "rating": 1,
            "sanitized_title": "promptsource_an_integrated_development_environment_and_repository_for_natural_language_prompts"
        }
    ],
    "cost": 0.015105999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ZERO-LABEL PROMPT SELECTION</p>
<p>Chonghua Liao 
Tsinghua University</p>
<p>Yanan Zheng 
Tsinghua University</p>
<p>Zhilin Yang zhiliny@tsinghua.edu.cn </p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qizhi Institute</p>
<p>ZERO-LABEL PROMPT SELECTION</p>
<p>Natural language prompts have been shown to facilitate cross-task generalization for large language models. However, with no or limited labeled examples, the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels. To address the issue, we propose a Zero-Label Prompt Selection (ZPS) method that selects prompts without any labeled data or gradient update. Specifically, given the candidate human-written prompts for a task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the pseudo-labels for prompt selection. Experiments show that ZPS improves over prior methods by a sizeable margin in zero-label performance. We also extend ZPS to a few-shot setting and show its advantages over strong baselines such as prompt tuning and model tuning.</p>
<p>INTRODUCTION</p>
<p>Recently, extensive studies have shown that large language models (LLMs) have promising performance for few-shot learning (Brown et al., 2020;Schick &amp; SchÃ¼tze, 2021;Gao et al., 2021), and they even show strong generalization abilities to new tasks without any annotated data (Brown et al., 2020;Sanh et al., 2021). Different from conventional fine-tuning methods that require expensive parameter updates for each downstream task, prompts are employed to provide in-context information or task instructions, which is helpful for guiding models to perform each task. Manually-written prompts are often used to specify the task and unify the format of inputs.</p>
<p>However, the performance of different prompts during evaluation can vary from near state-of-the-art to random guess; e.g., using a non-optimal prompt can cause a performance drop of up to 60 points on the CB task . Previous work mainly relies on using multiple prompts (Brown et al., 2020;Sanh et al., 2021) or a prompt ensemble  to enhance the performance and robustness when generalizing to test tasks, while omitting the fact that using multiple prompts leads to a substantially increased computational cost, which hinders the practical deployment of LLMs. These challenges make prompt selection an important problem.</p>
<p>There have been efforts on improving model performance via searching for a better prompt. For example, Jiang et al. (2020) proposed two automatic methods to augment prompts. They further explored combining the generated diverse prompts with ensemble methods. Shin et al. (2020) designed a gradient-based search method to find trigger words in a prompt. Gao et al. (2021) developed a way to use a span-corruption pretraining objective for prompt generation. Deng et al. (2022) presented RLprompt, a prompt search method with reinforcement learning which relies on a policy network trained with a carefully designed reward function. Prasad et al. (2022) designed an iterative prompt search algorithm that relies on human-defined edit rules to improve the few-shot performance.  proposed GPS, a genetic prompt searching algorithm that leveraged generative language models for prompt augmentation. Nevertheless, the main drawback of such methods is that they all require an additional labeled set to serve as a prompt scoring set or to provide the rewards or gradient signals. It remains challenging when no labeled samples are available. Thus, a crucial question arises:  (1) A number of prompted unlabeled data are fed into the pretrained language model to get logits and predictions.</p>
<p>(2) The pseudo labels are obtained from the prompt ensemble.</p>
<p>(3) The pseudo labels are used to calculate the pseudo accuracy of prompts. And the one with the highest pseudo accuracy is selected. Some details like prompt filtering are omitted for brevity and can be found in the text.</p>
<p>Is it possible to select a high-performing prompt without any labeled data or gradient update?</p>
<p>In this paper, we answer this question affirmatively. To tackle the aforementioned problem, we propose ZPS-Zero Label Prompt Selection-a simple-yet-effective technique for selecting a highperforming prompt in a zero-label setting. As illustrated in Figure 1, given a set of candidate humanwritten prompts P and an unlabeled dataset X for a task, the ensemble of prompts is used to annotate the unlabeled data. Finally, the pseudo-labels generated from the prompt ensemble are used for prompt selection. We also extend the idea of ZPS to a few-shot setting and validate our advantages over strong baselines such as prompt tuning and model tuning. In the few-shot setting, we further explore the role of pseudo-labeled data: pseudo-labeled data can not only be used for prompt selection, but checkpoint selection as well. By using pseudo-labeled data for checkpoint selection, there is no need to split a subset from limited labeled data as a validation set, which means more labeled data can be used for training to boost model performance.</p>
<p>Our contributions are summarized as follows.</p>
<p>â€¢ We propose a novel Zero-Label Prompt Selection (ZPS) algorithm to select high-performing prompts without an extra validation set or any parameter update. â€¢ We show that ZPS can be used in a plug-and-play manner to boost zero-label and few-shot performance. Extensive experiments show that ZPS leads to a substantial performance boost for both the zero-label and few-shot settings.</p>
<p>RELATED WORK</p>
<p>Pseudo-labeling. Recently, there have been many advances in deep learning with pseudo-labeling. Pseudo-labeling (Lee et al., 2013;Reed et al., 2015;Shi et al., 2018) employs a model to make predictions for unlabeled samples (Yarowsky, 1995b;McClosky et al., 2006). Iscen et al. (2019) showed that pseudo-labels can also be created by label propagation instead of direct network predictions. Shi et al. (2018) incorporate the idea of confidence levels for unlabeled samples to discount influences from uncertain samples. Another line of work is self-training (III, 1965;Yarowsky, 1995a;Riloff, 1996), which trains a model with both labeled and pseudo-labeled data for a few iterations. Some modifications like strong data augmentation (Zoph et al., 2020), learning an equal-or-larger student model (Xie et al., 2020), and using additional noise (Xie et al., 2020;He et al., 2020) are shown to be beneficial for self-training. Another popular technique is ensemble distillation (Hinton et al., 2015), which means distilling knowledge in an ensemble into a single model.</p>
<p>Zero-label Learning. Different from pseudo-labeling where labeled data is usually available, zero-label learning transfers a pretrained model to unseen tasks with unlabeled data only. Instead of directly predicting labels for unlabeled data, Wang et al. (2021) proposed to use unlabeled data to generate synthetic data without tuning the generative models.  designed a promptconsistency loss for finetuning on unlabeled data. Lang et al. (2022) used co-training to fine-tune both a large language model and a smaller task-specific model, where the different views of the data are obtained from different prompts. These methods study generating synthetic data or improved training techniques, while our ZPS focuses on an orthogonal aspect-how to select high-performing prompts.</p>
<p>Prompt Search. Past works attempt to improve prompt quality via prompt tuning (Liu et al., 2021b;Li &amp; Liang, 2021;Vu et al., 2021;Gu et al., 2021;Mokady et al., 2021;. Soft-prompts are optimized via gradient descent. However, the continuous nature of soft-prompts makes them hard to be interpreted. Other prior work has looked into optimizing discrete manual prompts in various aspects, such as selecting priming examples Liu et al., 2021a), ordering examples Lu et al., 2022;Kumar &amp; Talukdar, 2021), and prompt search (Jiang et al., 2020;Gao et al., 2021;Shin et al., 2020;Prasad et al., 2022;. The aforementioned methods require updating parameters and a validation set. Our ZPS, on the contrary, is a tuning-free method while no labeled data is required.</p>
<p>METHOD</p>
<p>In this section, we provide a detailed description of the presented approach, Zero-Label Prompt Selection (ZPS). ZPS mainly consists of three parts: prompt filtering, ensemble, and selection. After filtering out some low-performing prompts, the ensemble of the remaining prompts is used for prompt selection. We first start with our problem definition of zero-label learning.</p>
<p>ZERO-LABEL LEARNING</p>
<p>Suppose an LLM has been pretrained. Given a new task and a set of unlabeled data X associated with the task, we use the LLM to perform the new task without any label. Zero-label learning is a reasonable setting since task-relevant unlabeled data are usually accessible at test time. This setting has also been adopted in previous work  to test the ability of cross-task generalization.</p>
<p>PROMPT FILTERING</p>
<p>In this part, we introduce prompt filtering. We assume that we are given the pretrained model M, a set of prompts P i = (T i , V i ) âˆˆ P, and a set of unlabeled data X = {x 1 , . . . , x n } with size n. Formally, a prompt P = (T , V) is a pair composed of two parts: a prompt template and the corresponding verbalizer. The prompt template is defined as a function that transforms the unlabeled data into a sequence. For example, given a prompt template T of a sentiment classification task "Based on this review, would the user recommend this product? Review:{{content}} Answer:", we will fill a predicted text sample into the "{{content}}" blank. Further, the verbalizer V is defined as a function that maps each label to a phrase. For instance, the verbalizer corresponding to T will map 1 to "Yes", and 0 to "No". As a result, the LLM performs the tasks by predicting the mapped phrases.</p>
<p>Now we want to filter out possible low-performing prompts before we proceed to the next step.</p>
<p>Given an unlabeled sample x k , the LLM outputs the conditional probability for a given label y j :
P (y j | x k , P i ) = P M (V i (y j ) | T i (x k )) ,
where we use P M (Â· | Â·) to denote the LLM probability. We define the confidence score of a prompt as
c i = x k âˆˆX p(y (1) | x k , P i ) âˆ’ p(y (2) | x k , P i ) ,
where y (1) and y (2) denote the choices with the highest and the second highest LM probability, separately. The confidence scores represent how confident the LLM is about the predictions with a specific prompt. Let (c 1 , Â· Â· Â· , c p ) denote the confidence scores of all p prompts. We use k-means to cluster these scores into two sets and discard the set of prompts with lower scores.</p>
<p>PROMPT ENSEMBLE AND SELECTION</p>
<p>In this subsection, after clustering and filtering, we will introduce how to use the rest of the prompts to form a prompt ensemble, and utilize the prompt ensemble for prompt selection. We then provide three possible ways to combine prompts as a prompt ensemble. We note that directly serving a prompt ensemble at inference time will increase the computational cost or latency substantially, which is not practical for many applications. In this work, we mainly focus on the setting where only one prompt can be served.</p>
<p>The key idea of our approach is from one of most classic techniques-ensemble (Hansen &amp; Salamon, 1990;Krogh &amp; Vedelsby, 1994). We ensemble all available prompts to obtain a pseudo label for each sample. Since we cannot directly calculate accuracy on a labeled validation set, we obtain a "pseudo accuracy" by calculating the agreement between model predictions and pseudo labels, as illustrated in Figure 1. Specifically, the prediction of an input x k given the prompt P i iÅ
y i,k = argmax yâˆˆY P (y | x k , P i ) .
We use the notation y i = (Å· i1 , . . . ,Å· in ) to denote all the predictions of prompt P i , and we use y e to denote the prediction from the prompt ensemble. The "pseudo accuracy" of the prompt P i is computed as
acc i = f acc (y i , y e ) .
where we use f acc to denote the accuracy function that calculates the proportion of identical predictions between the two vectors. Then we select the prompt with the highest "pseudo accuracy" as the prompt we use for the target task. Intuitively, the pseudo labels are assumed to have higher accuracy compared to the prediction from a single prompt, which provides a stronger signal for selection.</p>
<p>There are many possible ways to form a prompt ensemble for obtaining y e . Here we discuss three possible ensemble strategies.</p>
<p>Log-probability mean. Given an unlabeled example x k , we use the average of log probabilities to calculate the score of a choice y, and obtain the ensemble predictionÅ· e,k :
s(x k , y) = 1 p PiâˆˆP log P (y | x k , P i ) ,Å· e,k = argmax yâˆˆY s(x k , y) .
Probability mean. It is also possible to get the score by averaging the LLM probabilities from different prompts. Specifically, we have
s(x k , y) = 1 p PiâˆˆP P (y | x k , P i ) .
Majority vote. This method aggregates the prediction of each prompt given an input x k and outputs the choice with the maximum occurrences. Formally,
s(x k , y) = PiâˆˆP 1(Å· i,k = y) ,
where 1(Â·) is the indicator function andÅ· i,k denotes the prediction of prompt P i given the input x k .</p>
<p>ZPS FOR FEW-SHOT LEARNING</p>
<p>In addition to zero-label learning, we extend ZPS to the few-shot learning setting. In this setting, we are given a labeled set with m samples, say D m , in addition to the unlabeled data X and the LLM M.</p>
<p>Our goal is to develop a strategy to boost the performance of the model. Traditional model tuning often splits D m into to parts of the same size, D m/2 and D m/2 , as a training set and a validation set. The model is first trained on D m/2 , and the validation set D m/2 is then used for checkpoint selection and prompt selection. However, since the number of labeled examples is limited, using a validation set means less data is used for training. With ZPS, we propose to use pseudo-labeled data for both checkpoint and prompt selection. As a result, all labeled examples are used for tuning the models, which leads to better generalization performance.</p>
<p>EXPERIMENTS</p>
<p>In this section, we conduct extensive quantitative experiments to evaluate our ZPS against baseline methods. We mainly consider two experimental settings, zero-label learning and few-shot learning, as described in Section 3. In addition, we also investigate and analyze several influential factors and hyper-parameters in ZPS.  )). We also use the prompt candidates provided in T0, which are constructed using PromptSource (Bach et al., 2022). For the zero-label setting, we construct the unlabeled data by removing labels of the training-split data for each test task. For the few-shot setting, we randomly sample 32 labeled data from the training split.</p>
<p>BASELINES</p>
<p>For the zero-label setting, we compare our ZPS with the following baseline methods.</p>
<p>â€¢ T0 T0 (Sanh et al., 2021) employs prompted multi-task pretraining for cross-task generalization, which provides a framework for all methods considered in our experiments. T0 does not provide a way to select prompts without labels, and the average performance of all candidate prompts is used.</p>
<p>â€¢ Self-Training Self-training first uses a trained T0 to label the unlabeled data, and then uses the pseudo-labeled data to further finetune the T0 model (Sanh et al., 2021). The whole process is repeated multiple times. The self-training method also reports average performance over multiple prompts.</p>
<p>â€¢ Ensemble Distillation We use a prompt ensemble (Hinton et al., 2015) to pseudo label the data and distill the knowledge into multiple prompts using prompt tuning . The tuned soft prompt embeddings are concatenated to different prompts and the mean performance of all prompts is reported.</p>
<p>For the few-shot setting, we mainly compare ZPS with the following two categories of baseline methods, respectively methods with parameter tuning, including model tuning and prompt tuning, and methods without parameter tuning, including in-context learning, GRIPS, and GPS.</p>
<p>â€¢ In-Context Learning (ICL) ICL (Brown et al., 2020) is a few-shot method which requires a few input-output pairs as priming demonstrations to guide models to perform the test task. Given a prompt P and a test sample, a few prompted labeled samples are concatenated to the prompted test sample to form an input. The average performance of all prompts is reported.</p>
<p>â€¢ GRIPS GRIPS (Prasad et al., 2022) is an edit-based prompt search approach. It iteratively mutates the prompts with human-defined edit rules. Then, the augmented prompts with the highest scores on the validation set are selected. It also reports the average performance on multiple augmented prompts.</p>
<p>â€¢ GPS GPS  is also a gradient-free prompt search method that adapts the genetic algorithm and uses generative models to augment prompts. We report the average performance of the selected prompts.</p>
<p>â€¢ Model Tuning (MT) Model tuning finetunes all parameters of a language model for each task.</p>
<p>We follow the few-shot setting in Zheng et al. (2021) to use half of the data for training and the other half for model selection.</p>
<p>â€¢ Prompt Tuning (PT) Prompt tuning (Liu et al., 2021b;) tunes a continuous embedding while the LLM is frozen. Training and validation splits are identical to model tuning.</p>
<p>TRAINING DETAILS</p>
<p>For fair comparison, we keep the number of labeled samples as 32 for all few-shot methods. For all methods that require gradient update (PT, MT, self-training), we use the Adafactor Optimizer and set the batch size as 4. We set the learning rate as 5e-5 for MT and self-training, and 0.05 for PT. For PT, we set the number of soft prompt tokens as 3 for zero-label ensemble distillation and 1 for few-shot learning. For ICL, we randomly select 2 examples from the training set of each task to compose the priming prompt. The above hyperparameters are chosen based on validation results. For GRIPS and GPS, we also follow the hyper-parameters reported in Prasad et al. (2022) and .</p>
<p>MAIN RESULTS AND ANALYSIS</p>
<p>Zero-Label Performance. In this section, we compare ZPS with the baselines mentioned in the last subsection on the 11 test tasks. As shown in Table 1, our ZPS outperforms the T0 baseline, which shows the effectiveness of prompt selection. Our zero-label ZPS even has a considerable advantage over some few-shot methods (GRIPS, ICL, PT) in Table 2, which shows that the selection of prompts is a crucial factor in model performance. Moreover, combining ZPS with ensemble distillation further boosts performance and outperforms using ensemble distillation alone, which indicates that ZPS is able to select high-performing prompts in a prompt tuning scenario.  Table 1: Main results on zero-label performance of different methods. All methods are based on the T0-11B model. "EnsD" denotes ensemble distillation.</p>
<p>Few-Shot Performance. Next, we turn to experiments where 32 labeled samples are available for each task. Table 2 shows that MT + ZPS outperforms all other strong few-shot baselines on the average performance of all test tasks. This reveals the effectiveness of our ZPS. We notice that the performance of ICL with T0 is significantly worse than other few-shot baselines, which is probably because the priming prompt used in ICL is quite different from task descriptions used in the multitask training stage of T0. The performance of GRIPS and PT are similar, slightly better than the T0 zeroshot baseline, while GPS achieves the strongest performance among all few-shot methods that do not require parameter updating. In our MT + GPS, with the help of pseudo labels, more labeled data can be freed to perform training instead of model selection (traditional MT requires a portion of the labeled set to select checkpoints (Zheng et al., 2021)). The superior effectiveness of ZPS validates that pseudo-labeled data can not only select prompts, but select model checkpoints as well.</p>
<p>ABLATION STUDY</p>
<p>In this section, we perform several ablation experiments to explore the influential factors of our method. We keep the other factors fixed while studying the effect of a specific factor.  Table 2: Main results on few-shot performance of different methods. All methods are based on the T0-11B model. For a fair comparison, we keep the size of the labeled set for each task as 32. Model Tuning (MT) and Prompt Tuning (PT) tune the full model parameters and the continuous prompt embedding separately. In-Context Learning (ICL), GRIPS (Prasad et al., 2022) and GPS  do not require any parameter update.</p>
<p>ENSEMBLE STRATEGIES</p>
<p>As shown in Table 3, among these three ensemble strategies, log-probability mean attains the best performance. This also echoes with the finding in Jiang et al. (2020) that given any prompt, log probabilities can penalize the choice that is very unlikely.  </p>
<p>PROMPT FILTERING</p>
<p>Prompt filtering plays an important role in our method. We compare the performance of using and not using filtering in Table 4. It shows that prompt filtering boosts the performance of ZPS for almost all tasks. Since prompt filtering is followed by a prompt ensemble, filtering out possible low-performing prompts contributes to a higher accuracy of the ensemble and leads to better performance. Moreover, even without prompt filtering, our ZPS still holds strong advantages over the other baselines in Table 1, indicating the stability and robustness of our method.  </p>
<p>ROBUSTNESS TO ADVERSARIAL PROMPTS</p>
<p>Since ZPS relies on a prompt ensemble, it is possible that the quality of the candidate prompts will affect its performance. We want to test whether ZPS will still outperform the average performance of candidate prompts when there are more low-performing prompts in the candidates. To this end, we create adversarial low-performing prompts by adapting GPS . In the genetic algorithm, the generated prompts with the lowest performance on the validation set are selected as the low-performing prompts. A portion of the original prompts are replaced with the same number of low-performing prompts. We then report the performance of ZPS-selected prompts and the average of candidates. The replacement process is repeated with 5 different random seeds. We also vary the ratio of low-performing prompts from 0.1 to 0.8. As shown in Table 5, ZPS yields consistent improvements over not using prompt selection (i.e., the mean of candidate prompts). This reflects that ZPS is useful consistently under different levels of adversary and noise.  Table 5: Robustness to adversarial prompts. We replace the original candidate prompts with adversarial low-performing prompts generated by GPS . We vary the ratio of lowperforming prompts from 0.1 to 0.8. ZPS yields consistent improvements over not using prompt selection.</p>
<p>SCALE</p>
<p>Another important factor is the model scale. We conduct experiments on T0-3B and T0-Large(770M) to verify the effectiveness of ZPS on different models. Table 6 shows that ZPS is able to boost the performance on models with different scales.  </p>
<p>USAGE OF LABELED AND PSEUDO-LABELED DATA</p>
<p>In this part, we examine different ways of using labeled and pseudo-labeled data in the few-shot setting. Given 32 labeled examples D 32 from the training set and all unlabeled examples X for each task, we consider the following cases:</p>
<p>â€¢ 16 + 16: This is a classic approach to use labeled data in model tuning. The labeled set is splited into two equal-sized halves, D 16 and D 16 , as a training set and a validation set. The validation set is then used to perform checkpoint selection and prompt selection.  Table 7: An ablation study on different strategies to use labeled and pseudo-labeled data. "Mean" denotes the mean of prompts, "val select" means selecting the prompt on the labeled validation set, and "pseudo select" means prompt selection on the pseudo-labeled data. We come to the conclusion that it is best to use labeled data for training and use pseudo-labeled data for checkpoint and prompt selection. "More pseudo val" with "pseudo select" is what we adopt in ZPS.</p>
<p>â€¢ 32 pseudo train: We first annotate the unlabeled test data X by T0 to get pseudo labels. Then, we select the pseudo-labeled samples with the top-32 highest confidence. This set is denoted as X 32 . The entire labeled set D 32 is only used as a validation set.</p>
<p>â€¢ 32 pseudo val: This case is similar to "32 pseudo train", despite that the entire labeled set D 32 is used as a training set for model tuning. In this case, we use X 32 for checkpoint selection.</p>
<p>â€¢ More pseudo val (the approach we adopted in our ZPS): The only difference between this case and "32 pseudo val" is that we increase the size of the pseudo-labeled validation set. Table 7 presents the results of the aforementioned cases. These four cases all show significant improvements compared with the T0 zero-label baseline, which demonstrated the effectiveness of pseudo-labeling. "32 pseudo train" performs the worst among all cases, indicating that the pseudolabeled samples still contain much noise that is harmful for training. By comparing "16 + 16" and "32 pseudo val", we show that the prompt selecting ability of ZPS is comparable to that of a labeled validation set. Moreover, "Pseudo val" methods perform much better than others, which validates our hypothesis. These results indicate that the gain of training with the labeled set is greater than using the labeled set for checkpoint selection. However, since the pseudo-labeled data is noisy, increasing the size of pseudo-labeled validation data can only provide marginal performance gain. Overall, we adopt "more pseudo val" with "pseudo select" in our ZPS for the best performance.  Table 8: The performance of RTE prompts illustrated by: (1) D 16 Acc., the accuracy of a validation set with 16 labels (2) Pseudo Acc., the pseudo accuracy obtained by ZPS, and (3) Posthoc Acc., the true accuracy on the test set.</p>
<p>CASE STUDY</p>
<p>In Table 8, we present 9 RTE prompts and the following quantities (1) D 16 Acc., which is the accuracy of a prompt on a small validation set with 16 labeled samples.</p>
<p>(2) Pseudo Acc., which is calculated by ZPS and (3) Posthoc Acc., which is the real performance on the test set. The results show that D 16 has poor prompt selection abilities and there are no obvious correlations between D 16 Acc. and Posthoc Acc. On the contrary, our ZPS can distinguish the difference in prompt performances and have better correlations with the real test performance.</p>
<p>CONCLUSIONS</p>
<p>We propose a novel Zero-Label Prompt Selection (ZPS) algorithm to select a high-performing prompt in the challenging setting where no labeled data is available. We demonstrate that ZPS can be used in a plug-and-play manner for different cases (zero-label generalization, ensemble distillation, and few-shot model tuning). We also perform detailed ablation studies to verify the robustness and effectiveness with respect to different factors.</p>
<p>Figure 1 :
1The main pipeline of ZPS.</p>
<p>the T0 benchmark(Sanh et al., 2021) that consists of 39 training tasks of 8 task types, and 11 test tasks of 4 task types. Both the training and the test sets are disjoint in task types. Specifically, the test tasks include natural language inference(RTE (Dagan et al., 2006), CB (De Marneffe et al., 2019), ANLI/R1-R3 (Nie et al., 2020)), coreference resolution (WSC (Levesque et al., 2012), Winogrande (Sakaguchi et al., 2020)), sentence completion (COPA (Roemmele et al., 2011), Sto-ryCloze (Mostafazadeh et al., 2017), Hellaswag (Zellers et al., 2019)), and word sense disambiguation (WiC (Pilehvar &amp; Camacho-Collados, 2019</p>
<p>Natural Language InferenceSentence Completion Coreference WSD Avg. RTE CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiCGPS 
83.86 80.00 46.47 39.91 43.01 93.43 43.96 95.03 65.48 61.96 58.82 64.72 
GRIPS 
81.59 76.07 44.41 39.32 42.10 91.41 33.11 94.16 61.54 58.61 57.23 61.78 
ICL 
72.42 65.24 37.58 33.05 37.17 84.07 26.77 90.18 64.42 54.21 49.33 55.86 
PT 
81.48 70.24 42.67 38.67 40.55 92.59 39.12 95.46 60.87 59.79 58.90 61.85 
MT 
78.34 81.25 47.10 38.43 45.75 91.45 52.91 94.55 65.38 69.53 53.29 65.27 </p>
<p>MT + ZPS (Ours) 80.87 85.71 45.50 41.60 50.33 93.00 58.54 93.80 57.69 69.93 53.92 66.45 </p>
<p>Natural Language InferenceSentence Completion Coreference WSD Avg. RTE CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiCMajority vote 
86.28 78.57 44.30 39.30 42.92 92.00 33.03 95.35 62.50 59.75 59.40 63.04 
Probability mean 
86.28 78.57 44.30 39.30 42.92 92.00 33.79 95.83 56.73 59.75 59.40 62.74 
Ours (log prob mean) 86.28 80.36 45.10 40.50 42.92 92.00 34.33 95.83 62.50 60.54 58.78 63.54 </p>
<p>Table 3 :
3Performances of different ensemble strategies.</p>
<p>Table 4 :
4Results of ZPS with and without filtering.</p>
<p>Table 6 :
6The performance of ZPS with different scales.</p>
<p>Natural Language Inference Sentence Completion Coreference WSD Avg. RTE CB ANLI1 ANLI2 ANLI3 COPA Hella. Story. WSC Wino. WiC Pseudo select 80.51 73.21 48.20 39.50 46.25 91.00 52.91 94.23 64.42 70.09 52.51 64.80 Pseudo select 81.95 82.14 39.00 36.50 43.17 95.00 51.51 94.33 57.69 57.46 55.33 63.10 Pseudo select 83.75 78.57 49.40 37.50 45.58 93.00 56.10 96.26 62.50 70.40 55.96 66.28 Pseudo select 80.87 85.71 45.50 41.60 50.33 93.00 58.54 93.80 57.69 69.93 53.92 66.4516 + 16 </p>
<p>Mean 
78.19 72.38 49.16 38.87 45.94 90.45 46.36 94.55 63.65 70.04 53.18 63.88 
Val select 
78.34 81.25 47.10 38.43 45.75 91.45 52.91 94.55 65.38 69.53 53.29 65.27 
32 pseudo 
train </p>
<p>Mean 
81.91 79.88 38.89 36.20 42.13 93.65 51.94 94.12 56.54 57.47 55.11 62.53 
Val select 
82.43 79.76 38.80 36.50 42.30 96.00 51.51 94.07 61.54 57.73 55.33 63.27 
32 pseudo 
val </p>
<p>Mean 
83.72 80.60 48.97 37.98 46.01 92.55 48.06 95.79 65.48 70.54 55.38 66.00 
More 
pseudo val </p>
<p>Mean 
79.64 85.00 46.01 40.74 50.16 92.80 51.62 93.67 62.88 70.12 56.54 66.32 </p>
<p>PromptD16 Acc. Pseudo Acc. Acc.Given{{premise}} Is it guaranteed true that "{{hypothesis}}"? Yes or no? 87.88 92.06 81.95 Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes or no? 87.88 91.34 81.95 {{premise}}\n Question: {{hypothesis}} True or False? 84.85 94.58 82.31 {{premise}}\n\n Question: Does this imply that "{{hypothesis}}"? Yes or no? 90.91 90.97 82.31 Given {{premise}} Should we assume that "{{hypothesis}}" is true? Yes or no? 90.91 94.22 81.95 Given that {{premise }} Does it follow that {{hypothesis}} Yes or no? Based on the previous passage, is it true that "{{hypothesis}}"? Yes or no? 90.91 94.95 86.28 {{premise}} Are we justified in saying that "{{hypothesis}}"? Yes or no? Given that {{premise}} Therefore, it must be true that "{{hypothesis}}"? Yes or no?90.91 
81.95 
73.29 
{{premise}} 84.85 
86.64 
75.81 
90.91 
94.58 
82.31 </p>
<p>Yifei Shengnan An, Zeqi Li, Qian Lin, Bei Liu, Qiang Chen, Weizhu Fu, Nanning Chen, Jian-Guang Zheng, Lou, arXiv:2203.03131Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprintShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.</p>
<p>Promptsource: An integrated development environment and repository for natural language prompts. H Stephen, Victor Bach, Zheng Xin Sanh, Albert Yong, Colin Webson, Raffel, V Nihal, Abheesht Nayak, Taewoon Sharma, M Kim, Thibault Bari, Zaid FÃ©vry, Manan Alyafeai, Andrea Dey, Zhiqing Santilli, Srulik Sun, Canwen Ben-David, Gunjan Xu, Han Chhablani, Wang, abs/2202.01279Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. RushCoRRJason Alan FriesStephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault FÃ©vry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. CoRR, abs/2202.01279, 2022.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>The pascal recognising textual entailment challenge. Oren Ido Dagan, Bernardo Glickman, Magnini, 978-3-540-33428-6Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment. Joaquin QuiÃ±onero-Candela, Ido Dagan, Bernardo Magnini, and Florence d'AlchÃ© BucBerlin, Heidelberg; Berlin HeidelbergSpringerIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment chal- lenge. In Joaquin QuiÃ±onero-Candela, Ido Dagan, Bernardo Magnini, and Florence d'AlchÃ© Buc (eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classi- fication, and Recognising Tectual Entailment, pp. 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.</p>
<p>The commitmentbank: Investigating projection in naturally occurring discourse. Marie-Catherine De Marneffe, Mandy Simons, Judith Tonhauser, proceedings of Sinn und Bedeutung. Sinn und Bedeutung23Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: In- vestigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107-124, 2019.</p>
<p>Rlprompt: Optimizing discrete text prompts with reinforcement learning. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, P Eric, Zhiting Xing, Hu, arXiv:2205.12548arXiv preprintMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022.</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, 10.18653/v1/2021.acl-long.295Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline1Association for Computational LinguisticsTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816-3830, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021. acl-long.295.</p>
<p>Ppt: Pre-trained prompt tuning for few-shot learning. Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang, arXiv:2109.04332arXiv preprintYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.</p>
<p>Neural network ensembles. Lars Kai Hansen, Peter Salamon, IEEE transactions on pattern analysis and machine intelligence. 1210Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10):993-1001, 1990.</p>
<p>Revisiting self-training for neural sequence generation. Junxian He, Jiatao Gu, Jiajun Shen, Marc&apos;aurelio Ranzato, ICLR. OpenReview.net. Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. Revisiting self-training for neural sequence generation. In ICLR. OpenReview.net, 2020.</p>
<p>Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.025312arXiv preprintGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.</p>
<p>Probability of error of some adaptive pattern-recognition machines. H J Scudder, Iii , IEEE Trans. Inf. Theory. 113H. J. Scudder III. Probability of error of some adaptive pattern-recognition machines. IEEE Trans. Inf. Theory, 11(3):363-371, 1965.</p>
<p>Label propagation for deep semisupervised learning. Ahmet Iscen, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Giorgos Tolias, Yannis Avrithis, and OndÅ™ej ChumAhmet Iscen, Giorgos Tolias, Yannis Avrithis, and OndÅ™ej Chum. Label propagation for deep semi- supervised learning. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5065-5074, 2019.</p>
<p>How can we know what language models know?. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 10.1162/tacla00324Transactions of the Association for Computational Linguistics. 8Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020. doi: 10.1162/tacl a 00324. URL https://aclanthology.org/2020.tacl-1.28.</p>
<p>Neural network ensembles, cross validation, and active learning. Anders Krogh, Jesper Vedelsby, Advances in neural information processing systems. 7Anders Krogh and Jesper Vedelsby. Neural network ensembles, cross validation, and active learning. Advances in neural information processing systems, 7, 1994.</p>
<p>Reordering examples helps during priming-based few-shot learning. Sawan Kumar, Partha Talukdar, doi: 10.18653/ v1/2021.findings-acl.395Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. OnlineAssociation for Computational LinguisticsSawan Kumar and Partha Talukdar. Reordering examples helps during priming-based few-shot learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 4507-4518, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.findings-acl.395. URL https://aclanthology.org/2021.findings-acl.</p>
<p>Co-training improves promptbased learning for large language models. Hunter Lang, Monica N Agrawal, Yoon Kim, David Sontag, International Conference on Machine Learning. PMLRHunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt- based learning for large language models. In International Conference on Machine Learning, pp. 11985-12003. PMLR, 2022.</p>
<p>Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 3896Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013.</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.08691arXiv preprintBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. CiteseerHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thir- teenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer, 2012.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.00190arXiv preprintXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.</p>
<p>What makes good in-context examples for. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804gpt-3? arXiv preprintJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021a. URL https://arxiv.org/abs/2101.06804.</p>
<p>. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, arXiv:2103.10385Gpt understands, too. arXiv preprintXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021b.</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2022. URL https://arxiv.org/abs/2104.08786.</p>
<p>Effective self-training for parsing. David Mcclosky, Eugene Charniak, Mark Johnson, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. the Human Language Technology Conference of the NAACL, Main ConferenceNew York City, USAAssociation for Computational LinguisticsDavid McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 152-159, New York City, USA, June 2006. Association for Computational Linguistics.</p>
<p>Generating training data with language models: Towards zero-shot language understanding. Yu Meng, Jiaxin Huang, Yu Zhang, Jiawei Han, abs/2202.04538ArXiv. Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models: Towards zero-shot language understanding. ArXiv, abs/2202.04538, 2022.</p>
<p>Ron Mokady, Amir Hertz, Amit Bermano, Clipcap, arXiv:2111.09734Clip prefix for image captioning. arXiv preprintRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.</p>
<p>LS-DSem 2017 shared task: The story cloze test. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James Allen, 10.18653/v1/W17-0906Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics. the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level SemanticsValencia, SpainAssociation for Computational LinguisticsNasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. LS- DSem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Link- ing Models of Lexical, Sentential and Discourse-level Semantics, pp. 46-51, Valencia, Spain, April 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-0906. URL https://aclanthology.org/W17-0906.</p>
<p>Adversarial NLI: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the 58th. the 58thYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adver- sarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th</p>
<p>10.18653/v1/2020.acl-main.441Annual Meeting of the Association for Computational Linguistics. OnlineAssociation for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics, pp. 4885-4901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441.</p>
<p>WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. Mohammad Taher Pilehvar, Jose Camacho-Collados, 10.18653/v1/N19-1128Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pp. 1267-1273, Minneapolis, Min- nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128.</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, arXiv:2203.07281arXiv preprintArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based in- struction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.</p>
<p>Jing Qian, Li Dong, Yelong Shen, Furu Wei, Weizhu Chen, arXiv:2202.13257Controllable natural language generation with contrastive prefixes. arXiv preprintJing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu Chen. Controllable natural language generation with contrastive prefixes. arXiv preprint arXiv:2202.13257, 2022.</p>
<p>Training deep neural networks on noisy labels with bootstrapping. Scott E Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich, ICLR (Workshop). Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR (Work- shop), 2015.</p>
<p>Automatically generating extraction patterns from untagged text. Ellen Riloff, AAAI/IAAI. AAAI Press / The MIT Press2Ellen Riloff. Automatically generating extraction patterns from untagged text. In AAAI/IAAI, Vol. 2, pp. 1044-1049. AAAI Press / The MIT Press, 1996.</p>
<p>Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Melissa Roemmele, Andrew S Cosmin Adrian Bejan, Gordon, AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alterna- tives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pp. 90-95, 2011.</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, AAAI. AAAI PressKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver- sarial winograd schema challenge at scale. In AAAI, pp. 8732-8740. AAAI Press, 2020.</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, H Stephen, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Teven Le Stiegler, Arun Scao, Raja, arXiv:2110.08207arXiv preprintVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, An- toine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>Exploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich SchÃ¼tze, 10.18653/v1/2021.eacl-main.20Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeAssociation for Computational LinguisticsTimo Schick and Hinrich SchÃ¼tze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255-269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https: //aclanthology.org/2021.eacl-main.20.</p>
<p>Transductive semi-supervised deep learning using min-max features. Weiwei Shi, Yihong Gong, Chris Ding, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Zhiheng MaXiaoyu Tao, and Nanning ZhengWeiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning Zheng. Transductive semi-supervised deep learning using min-max features. In Proceedings of the European Confer- ence on Computer Vision (ECCV), pp. 299-315, 2018.</p>
<p>Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto- Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pp. 4222-4235, Online, November 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020. emnlp-main.346.</p>
<p>Spot: Better frozen model adaptation through soft prompt transfer. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, Daniel Cer, arXiv:2110.07904arXiv preprintTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.</p>
<p>Towards zero-label language learning. Zirui Wang, Adams Wei Yu, Orhan Firat, Yuan Cao, arXiv:2109.09193arXiv preprintZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. Towards zero-label language learning. arXiv preprint arXiv:2109.09193, 2021.</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Self-training with noisy student improves imagenet classification. Qizhe Xie, Minh-Thang Luong, Eduard H Hovy, Quoc V Le, CVPR. IEEEQizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In CVPR, pp. 10684-10695. IEEE, 2020.</p>
<p>Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang, arXiv:2201.06910arXiv preprintHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Ze- roprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. arXiv preprint arXiv:2201.06910, 2022.</p>
<p>Unsupervised word sense disambiguation rivaling supervised methods. David Yarowsky, ACL. Morgan Kaufmann Publishers / ACLDavid Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL, pp. 189-196. Morgan Kaufmann Publishers / ACL, 1995a.</p>
<p>Unsupervised word sense disambiguation rivaling supervised methods. David Yarowsky, 10.3115/981658.98168433rd Annual Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, USAAssociation for Computational LinguisticsDavid Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189-196, Cam- bridge, Massachusetts, USA, June 1995b. Association for Computational Linguistics. doi: 10.3115/981658.981684.</p>
<p>HellaSwag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.18653/v1/P19-1472Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a ma- chine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, Florence, Italy, July 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/ P19-1472.</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLRZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697-12706. PMLR, 2021.</p>
<p>Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding. Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, Zhilin Yang, abs/2109.12742CoRRYanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, and Zhilin Yang. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding. CoRR, abs/2109.12742, 2021.</p>
<p>Prompt consistency for zero-shot task generalization. Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig, arXiv:2205.00049arXiv preprintChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049, 2022.</p>
<p>Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, NeurIPS. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. In NeurIPS, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
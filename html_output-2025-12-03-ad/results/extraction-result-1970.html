<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1970 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1970</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1970</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278904532</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.19080v1.pdf" target="_blank">ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models have gained much attention from the research community thanks to their strength in translating multimodal observations with linguistic instructions into robotic actions. Despite their recent advancements, VLAs often overlook the explicit reasoning and only learn the functional input-action mappings, omitting these crucial logical steps for interpretability and generalization for complex, long-horizon manipulation tasks. In this work, we propose \textit{ReFineVLA}, a multimodal reasoning-aware framework that fine-tunes VLAs with teacher-guided reasons. We first augment robotic datasets with reasoning rationales generated by an expert teacher model, guiding VLA models to learn to reason about their actions. Then, we use \textit{ReFineVLA} to fine-tune pre-trained VLAs with the reasoning-enriched datasets, while maintaining their inherent generalization abilities and boosting reasoning capabilities. In addition, we conduct an attention map visualization to analyze the alignment among visual attention, linguistic prompts, and to-be-executed actions of \textit{ReFineVLA}, showcasing its ability to focus on relevant tasks and actions. Through the latter step, we explore that \textit{ReFineVLA}-trained models exhibit a meaningful attention shift towards relevant objects, highlighting the enhanced multimodal understanding and improved generalization. Evaluated across manipulation tasks, \textit{ReFineVLA} outperforms the state-of-the-art baselines. Specifically, it achieves an average increase of $5.0\%$ success rate on SimplerEnv WidowX Robot tasks, improves by an average of $8.6\%$ in variant aggregation settings, and by $1.7\%$ in visual matching settings for SimplerEnv Google Robot tasks. The source code will be publicly available.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1970.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1970.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReFineVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReFineVLA: Multimodal Reasoning-Aware Generalist Robotic Policies via Teacher-Guided Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selective fine-tuning framework that injects teacher-generated multimodal chain-of-thought rationales into pre-trained vision-language-action models so the learner jointly predicts discretized action tokens and generates step-by-step natural-language reasoning, improving robustness and interpretability on manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReFineVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tunes a 2B-parameter VLA backbone (SpatialVLA / PaliGemma2 VLM) to jointly output discretized action tokens and natural-language rationales via a multi-objective loss (L_action + λ_r L_reasoning). Uses selective fine-tuning (freeze lower layers) and targets upper transformer blocks and the policy head.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>starts from vision-language pretraining (VLM backbone) and VLA pretraining on large cross-embodiment robot datasets; fine-tuned with reasoning supervision</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Backbone (SpatialVLA / PaliGemma2) pretrained on large robot manipulation corpora (Open X-Embodiment, RHT20) which contain multi-embodiment manipulation trajectories, spatial encodings and discretized action tokens; teacher reasoning annotations generated on BridgeData-v2 and Google RT1 robot datasets (125,000 trajectories) added for fine-tuning. Paper describes those datasets as containing object interactions, spatial relations and action sequences; exact token-level contents of pretraining data not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (simulated SimplerEnv: WidowX Robot & Google Robot tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulated manipulation tasks in SimplerEnv including Put Spoon on Towel, Put Carrot on Plate, Stack Green Block on Yellow Block, Put Eggplant in Yellow Basket, Move Near, Open/Close Drawer. Actions are represented as discretized action tokens for motion / gripper commands; evaluation measures grasp-correction success and full task completion. Environments include visual-matching and variant-aggregation settings (lighting, distractors, textures, camera poses) to test robustness; experiments are primarily in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper states high semantic alignment between pretraining (Open X-Embodiment/RHT20) and target tasks because pretrained VLA backbones already encode multi-embodiment manipulation and spatial relations. ReFineVLA preserves low-level pretrained perceptual grounding (by freezing lower layers) while injecting higher-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported fine-tuned performance (ReFineVLA): SimplerEnv WidowX tasks average success rate 47.7% (per Table 1). Improvements over SpatialVLA baseline: +5.0% average on WidowX. On SimplerEnv Google Robot tasks: visual matching average success 71.8% (Move Near 84.6%, Open/Close Drawer 59.0%), variant aggregation average success 61.2% (Move Near 82.8%, Open/Close Drawer 39.5%) — reported as +1.7% (visual matching) and +8.6% (variant aggregation) over SpatialVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper argues selective transfer fine-tuning (freezing most lower layers) reduces compute/memory/FLOPs and is more efficient than full fine-tuning, but it does not report quantitative sample-efficiency comparisons (no episode / demonstration counts vs baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Yes. The paper shows attention-map visualizations comparing pre-trained SpatialVLA and ReFineVLA: pre-fine-tuned VLAs focus narrowly on immediate action targets, while ReFineVLA shows attention shifts to semantically relevant objects and spatial anchors aligned with instructions, supporting improved multimodal grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported. The paper does not provide PCA/clustering or quantitative analyses of embedding geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Provides qualitative and indirect evidence: teacher-provided chain-of-thought rationales paired with improved attention alignment and task success indicate stronger grounding of action semantics (e.g., 'grasp', 'move towards') to perceived objects and spatial relations. The paper shows example rationales and improved success rates consistent with improved grounding, but no direct probing linking verb tokens to motor primitives with statistical measures.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Yes — empirical freezing experiments show lower layers should be frozen to preserve perceptual grounding while upper layers absorb high-level reasoning: peak performance when freezing first 24 transformer layers, indicating high-level reasoning resides in upper layers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer benefits when: (1) preserving pretrained low-level layers (freezing many lower layers), (2) balancing reasoning loss weight (λ_r ≈ 0.3), (3) pretraining distributions contain spatial/object interaction data (Open X-Embodiment, RHT20). Variant-aggregation (domain shift) benefits more from reasoning fine-tuning than simple visual-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly quantified. The experiments include environmental and object variation, but the paper does not present per-object 'seen vs novel' breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>ReFineVLA is evaluated as a fine-tuned agent; the paper provides zero-shot numbers only for some baseline pre-trained models (e.g., SpatialVLA zero-shot), not for ReFineVLA itself.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Yes — ablations show best performance when freezing the first 24 transformer layers; selective tuning of later blocks and policy head injects reasoning without destroying pretrained generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — the paper reports that (a) overly large reasoning loss weight λ_r harms performance (noisy outputs, misaligned attention), and (b) freezing too few or too many layers can degrade performance; optimal λ_r reported as 0.3 (gives +9.7% task success in their ablation). Exact negative-transfer magnitudes for extreme settings are described qualitatively but not fully quantified for all conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct experimental comparison to vision-only pretraining (e.g., ImageNet-only) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No detailed time-course analysis of representation change over fine-tuning epochs beyond ablation observations (e.g., effects of λ_r and freezing).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1970.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA (Exploring spatial representations for visual-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2B-parameter VLA model built on a PaliGemma2 VLM backbone that emphasizes spatial representations (Ego3D position encodings, Adaptive Action Grids) to improve 3D scene understanding and transfer for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring spatial representations for visual-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action transformer using a PaliGemma2 VLM backbone with spatial representation modules (Ego3D Position Encoding, Adaptive Action Grids) to model 3D spatial relations; outputs discretized action tokens for robot policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on cross-embodiment robotic datasets (Open X-Embodiment, RHT20) and VLM pretraining for the backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on large-scale robotic datasets (Open X-Embodiment, RHT20) containing multi-embodiment trajectories, spatial cues and action tokens; described as containing object interaction and spatial relationship information tailored for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv: WidowX & Google Robot tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same SimplerEnv tasks as in ReFineVLA experiments (Put Spoon on Towel, Put Carrot on Plate, Stack blocks, Put Eggplant in Basket; Move Near, Open/Close Drawer) with both visual-matching and variant-aggregation settings; evaluated both zero-shot and after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper implies good semantic alignment due to pretraining on robot-focused datasets with spatial information; authors treat SpatialVLA as a strong baseline with pre-aligned object-action semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported (from paper tables): WidowX tasks — SpatialVLA (zero-shot) average 34.4%; SpatialVLA (fine-tuning) average 42.7%. Google Robot tasks — visual matching zero-shot average 64.5% (Move Near 69.6%, Open/Close Drawer 59.3%), visual matching fine-tuning average 70.1% (Move Near 85.7%, Open/Close Drawer 54.6%). Variant aggregation zero-shot average 54.0% (Move Near 71.7%, Open/Close Drawer 36.2%), variant aggregation fine-tuning average 52.6% (Move Near 79.1%, Open/Close Drawer 26.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Yes — baseline attention maps show a narrow focus on immediate action targets and less holistic attention to contextual objects and spatial relationships compared to ReFineVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SpatialVLA includes spatial encodings intended to ground actions in 3D, but the paper presents only indirect evidence (performance and attention maps) rather than explicit probing of verb-to-affordance grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not directly analyzed in the paper (no layer-freezing ablation specific to SpatialVLA beyond being the pre-trained backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Fine-tuning can improve some metrics but can also lower performance on particular subtasks (paper shows mixed task-level changes), indicating transfer sensitivity to task and fine-tuning regime.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly broken out in the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes — SpatialVLA zero-shot evaluations are reported (see above numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported in isolation (but used as the frozen backbone in ReFineVLA experiments which show best results when freezing lower layers of the pretrained VLA).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Some tasks show decreased per-task performance after fine-tuning (e.g., certain WidowX tasks), indicating possible negative effects of naive fine-tuning for specific tasks or settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1970.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaliGemma2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paligemma 2: A family of versatile VLMs for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of vision-language models (VLMs) used as backbones for VLA models (e.g., SpatialVLA) designed for transfer across downstream multimodal tasks, including embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paligemma 2: A family of versatile vlms for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaliGemma 2 (VLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model family used as the multimodal encoder backbone for SpatialVLA; provides joint visual-text representations amenable to downstream fine-tuning for action token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on image-text data (VLM style); used as the backbone for further VLA pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper beyond being a VLM family intended for transfer; likely trained on large image-text corpora (paper does not enumerate exact datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Backbone for robotic manipulation VLA models</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used within VLA architectures to perform discretized action prediction and multimodal reasoning for SimplerEnv robotic tasks when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Implied to provide semantic visual-text alignment that facilitates downstream grounding; not quantitatively characterized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported for PaliGemma2 specifically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used as VLM backbone enabling downstream models to map language to visual regions and action tokens, but no direct probing in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Used successfully as a backbone for SpatialVLA and downstream ReFineVLA fine-tuning; specifics not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1970.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open X-Embodiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open X-Embodiment: Robotic learning datasets and RT-X models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale cross-embodiment robot dataset used to pretrain vision-language-action models to support generalist transfer across multiple robot platforms and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open X-Embodiment: Robotic learning datasets and RT-X models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open X-Embodiment (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset collection spanning many embodiments and manipulation tasks used to pretrain VLA backbones (enables cross-embodiment transfer and generalist policy pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Dataset used for VLA pretraining (multimodal robot trajectories across embodiments).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Contains multi-embodiment manipulation trajectories, diverse object interactions and action tokens; intended to provide cross-robot and cross-scene coverage for transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Pretraining source for generalist robotic manipulation policies</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used by SpatialVLA and other VLA backbones to transfer to SimplerEnv tasks; not itself a target task.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment with downstream embodied tasks because dataset is manipulation-centric and multi-embodiment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable for dataset per se.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Dataset content (manipulation trajectories) is designed to provide affordance and action-context co-occurrence needed for grounding, but no direct probing in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Authors treat Open X-Embodiment as a primary pretraining resource whose similarity to downstream tasks supports transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1970.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BridgeData-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BridgeData-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale robotic dataset used in this paper to generate teacher-guided multimodal reasoning annotations for fine-tuning (part of the 125k annotated trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridgedata v2: A dataset for robot learning at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BridgeData-v2 (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-task robotic dataset with trajectories used for policy learning and for producing reasoning-enriched training examples when prompted with an expert teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Dataset for robotic policy pretraining and fine-tuning; contains demonstration trajectories and multimodal observations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Contains expert demonstrations across robotic tasks and environments; used here as one of the two sources for generating 125k reasoning-annotated trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Source data for fine-tuning VLA models for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used to produce multimodal (image + instruction + action) samples augmented with teacher rationales for ReFineVLA fine-tuning; simulation/real mixtures in original dataset (not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High — dataset is manipulation-focused, so contains objects, actions and spatial relations relevant to target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used as source of action demonstrations that, when paired with teacher rationales, support grounding studies; no direct quantitative probing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Used together with Google RT1 dataset to create a large reasoning-annotated fine-tuning corpus (125k trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1970.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google RT-1 dataset / RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1: Robotics transformer for real-world control at scale (and associated RT-1 robot datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RT-1 is a transformer-based vision-to-action model trained on large real-world robot datasets; the related RT-1 robot datasets are used here as a source (Brohan et al.) for fine-tuning / evaluation samples and are also included in baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1 (and RT-1 robot datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based vision-to-action model trained on (real-world) robot demonstration data to map visual observations and instructions to action tokens; the dataset contains many manipulation demonstrations used both for RT-1 training and as a data source for other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretraining on large real-world robotic demonstration datasets (robot trajectories), not pure image-text web-scale VLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale robot demonstration data (image + action + instruction), emphasizing real-world control sequences and action tokens; contains object interactions and motor behaviors but not necessarily web-scale image-text pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (same SimplerEnv-style tasks and real-world tasks in original RT-1 work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>RT-1 targets real-world manipulation and is used as a baseline in SimplerEnv evaluations (paper reports RT-1 variants' success rates in visual matching and variant aggregation settings). Action space discretized as tokens; evaluated on Move Near and Open/Close Drawer tasks in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High with manipulation tasks because training data are robot demonstration trajectories; paper uses RT-1 as a baseline to show transfer capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table 2 (paper): RT-1 (converged) visual matching average 58.6% (Move Near 44.2%, Open/Close Drawer 73.0%); variant aggregation average 41.2% (Move Near 50.0%, Open/Close Drawer 32.3%). Additional RT-1 variants (begin / 15% / converged) appear in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper for RT-1 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-1's design and reported baseline performance indicate grounding from demonstration data to action tokens, but detailed grounding analyses are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>RT-1 variants in Table 2 show that different training regimes (begin/15%/converged) yield different transfer performance; details in original RT-1 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RT-1 is included as a baseline; zero-shot abilities not explicitly highlighted in this paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1970.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1-X / RT-2-X</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1-X / RT-2-X (extended RT models / X-Embodiment variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions of RT-style VLA models that further explore cross-embodiment generalization and larger pretraining / fine-tuning regimes; used as baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1-x (O'Neill et al., 2024) and Rt-2-x (O'Neill et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1-X / RT-2-X</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RT-family vision-language-action transformers adapted to cross-embodiment datasets (RT-X / OXE). Models treat discretized actions as language tokens and are trained/fine-tuned on large robot datasets to produce robot policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretraining on large robot-dataset corpora spanning multiple embodiments (Open X-Embodiment / OXE), plus vision-language components.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Open X-Embodiment / OXE datasets (multi-embodiment robot trajectories, object interactions, action tokens); RT-2 explicitly described as transferring web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv style tasks referenced in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as baselines evaluated on SimplerEnv Google Robot tasks; action token output for manipulation tasks in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed for high semantic alignment via OXE pretraining; paper uses them as strong baselines for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 2 reports RT-1-X: visual matching average 45.7% and variant aggregation average 30.9% (per table row RT-1-X). RT-2-X reported 77.9%/25.0% for visual matching/variant aggregation Move Near/Open-Close Drawer entries leading to an average 51.5% for visual matching and 57.3% for variant aggregation (per table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Architectural choice to tokenize actions links language modality and action but explicit grounding analyses not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Designed to leverage OXE pretraining for cross-embodiment transfer; performance varies by task and setting.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not explicitly described here (treated as pre-trained / fine-tuned baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1970.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA that tokenizes discretized robot actions as language tokens and fine-tunes vision-language backbones for robot control; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action model that treats actions as tokens and fine-tunes large VLM backbones for robotic tasks; open-source baseline used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Fine-tuned from vision-language backbones; pretraining details vary by implementation in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Original OpenVLA leverages VLM backbones and robot demonstration datasets for fine-tuning; this paper uses OpenVLA as a baseline but does not enumerate its pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated as a baseline on SimplerEnv WidowX and Google Robot tasks; action tokens represent robot commands over discrete action space.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align vision-language backbones to action token spaces; paper uses it as a baseline to compare transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1 (WidowX tasks) OpenVLA average 1.1% (very low in that table). Table 2 (Google tasks) OpenVLA visual matching avg 40.9% and variant aggregation avg 32.7% (per the reported rows).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper beyond baseline performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1970.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo (An open-source generalist robot policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flexible transformer architecture to unify multiple robot embodiments for generalist policies; provided as baseline variants (Base, Small) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An open-source generalist robot policy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo (Base / Small)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based generalist policy designed to handle multiple embodiments via a unified architecture; variants of different sizes reported as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretraining on multi-task / cross-embodiment robot datasets (details in original Octo work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Designed for heterogeneous embodiment data alignment; includes multi-task robot demonstrations, but exact content not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv WidowX tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated as Octo-Base and Octo-Small on WidowX tasks with discrete action token outputs; Table 1 reports their per-task success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Architected for cross-embodiment transfer; treated as a competitive baseline for generalist transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1: Octo-Base average 16.0% on WidowX tasks; Octo-Small average 30.0% (per Table 1 rows).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1970.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboVLM (What matters in employing vision language models for tokenizing actions in robot control?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language approach to tokenizing robot actions and adapting VLMs for robotic manipulation; included as a baseline in both zero-shot and fine-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What matters in employing vision language models for tokenizing actions in robot control?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action approach that investigates action tokenization choices for robotic control; evaluated as zero-shot and fine-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLM-based pretraining followed by adaptation to robotic datasets; specifics in original RoboVLM work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Uses VLM pretraining (image-text) and robot trajectory fine-tuning (details in original work); contains object descriptions and action tokens during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Reported in Table 1 (WidowX tasks) and Table 2 (Google tasks) in both zero-shot and fine-tuned settings; action token outputs used for grasp/placement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align VLM semantics to action tokens; considered a baseline for measuring transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 1 (WidowX): RoboVLM zero-shot average 13.5%; RoboVLM (fine-tuning) average 31.3%. Table 2 (Google): RoboVLM (zero-shot) visual matching Move Near 66.3% Open/Close Drawer 26.8% avg 46.6%; RoboVLM (fine-tuning) visual matching Move Near 61.7% Open/Close Drawer 43.5% avg 52.6% (per table rows).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RoboVLM's design focuses on tokenizing actions which implicitly links language and actions; this paper reports only empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performance varies by visual-matching vs variant-aggregation; fine-tuning improves some metrics over zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot and fine-tuned numbers are reported for RoboVLM in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1970.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1970.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TraceVLA (Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA variant that explores using trace / temporal prompting to enhance spatial-temporal awareness for generalist robotic policies; cited as related work and baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA approach that augments inputs with visual traces to improve spatial-temporal reasoning for manipulation policies; intended to strengthen temporal awareness in VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Built on VLA pretraining paradigms and uses manipulation datasets; exact pretraining details in original TraceVLA work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Uses temporally-structured manipulation data with spatial and temporal cues; paper references it as recent work improving spatial-temporal awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / generalist policies (related domain)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Designed for embodied tasks that require temporal planning and spatial awareness; cited as related work rather than an evaluated baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Trace-based prompting targets alignment of temporal/spatial representations with task requirements; this paper cites it as relevant prior art.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper references TraceVLA as improving spatial-temporal awareness, implying stronger grounding, but does not provide its internal evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>TraceVLA specifically addresses temporal cues but this paper does not include its dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring spatial representations for visual-language-action model <em>(Rating: 2)</em></li>
                <li>Open X-Embodiment: Robotic learning datasets and RT-X models <em>(Rating: 2)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Paligemma 2: A family of versatile vlms for transfer <em>(Rating: 1)</em></li>
                <li>What matters in employing vision language models for tokenizing actions in robot control? <em>(Rating: 2)</em></li>
                <li>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1970",
    "paper_id": "paper-278904532",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "ReFineVLA",
            "name_full": "ReFineVLA: Multimodal Reasoning-Aware Generalist Robotic Policies via Teacher-Guided Fine-Tuning",
            "brief_description": "A selective fine-tuning framework that injects teacher-generated multimodal chain-of-thought rationales into pre-trained vision-language-action models so the learner jointly predicts discretized action tokens and generates step-by-step natural-language reasoning, improving robustness and interpretability on manipulation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ReFineVLA",
            "model_description": "Fine-tunes a 2B-parameter VLA backbone (SpatialVLA / PaliGemma2 VLM) to jointly output discretized action tokens and natural-language rationales via a multi-objective loss (L_action + λ_r L_reasoning). Uses selective fine-tuning (freeze lower layers) and targets upper transformer blocks and the policy head.",
            "pretraining_type": "starts from vision-language pretraining (VLM backbone) and VLA pretraining on large cross-embodiment robot datasets; fine-tuned with reasoning supervision",
            "pretraining_data_description": "Backbone (SpatialVLA / PaliGemma2) pretrained on large robot manipulation corpora (Open X-Embodiment, RHT20) which contain multi-embodiment manipulation trajectories, spatial encodings and discretized action tokens; teacher reasoning annotations generated on BridgeData-v2 and Google RT1 robot datasets (125,000 trajectories) added for fine-tuning. Paper describes those datasets as containing object interactions, spatial relations and action sequences; exact token-level contents of pretraining data not enumerated.",
            "target_task_name": "Robotic manipulation (simulated SimplerEnv: WidowX Robot & Google Robot tasks)",
            "target_task_description": "Simulated manipulation tasks in SimplerEnv including Put Spoon on Towel, Put Carrot on Plate, Stack Green Block on Yellow Block, Put Eggplant in Yellow Basket, Move Near, Open/Close Drawer. Actions are represented as discretized action tokens for motion / gripper commands; evaluation measures grasp-correction success and full task completion. Environments include visual-matching and variant-aggregation settings (lighting, distractors, textures, camera poses) to test robustness; experiments are primarily in simulation.",
            "semantic_alignment": "The paper states high semantic alignment between pretraining (Open X-Embodiment/RHT20) and target tasks because pretrained VLA backbones already encode multi-embodiment manipulation and spatial relations. ReFineVLA preserves low-level pretrained perceptual grounding (by freezing lower layers) while injecting higher-level reasoning.",
            "performance_with_language_pretraining": "Reported fine-tuned performance (ReFineVLA): SimplerEnv WidowX tasks average success rate 47.7% (per Table 1). Improvements over SpatialVLA baseline: +5.0% average on WidowX. On SimplerEnv Google Robot tasks: visual matching average success 71.8% (Move Near 84.6%, Open/Close Drawer 59.0%), variant aggregation average success 61.2% (Move Near 82.8%, Open/Close Drawer 39.5%) — reported as +1.7% (visual matching) and +8.6% (variant aggregation) over SpatialVLA.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "The paper argues selective transfer fine-tuning (freezing most lower layers) reduces compute/memory/FLOPs and is more efficient than full fine-tuning, but it does not report quantitative sample-efficiency comparisons (no episode / demonstration counts vs baselines).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Yes. The paper shows attention-map visualizations comparing pre-trained SpatialVLA and ReFineVLA: pre-fine-tuned VLAs focus narrowly on immediate action targets, while ReFineVLA shows attention shifts to semantically relevant objects and spatial anchors aligned with instructions, supporting improved multimodal grounding.",
            "embedding_space_analysis": "Not reported. The paper does not provide PCA/clustering or quantitative analyses of embedding geometry.",
            "action_grounding_evidence": "Provides qualitative and indirect evidence: teacher-provided chain-of-thought rationales paired with improved attention alignment and task success indicate stronger grounding of action semantics (e.g., 'grasp', 'move towards') to perceived objects and spatial relations. The paper shows example rationales and improved success rates consistent with improved grounding, but no direct probing linking verb tokens to motor primitives with statistical measures.",
            "hierarchical_features_evidence": "Yes — empirical freezing experiments show lower layers should be frozen to preserve perceptual grounding while upper layers absorb high-level reasoning: peak performance when freezing first 24 transformer layers, indicating high-level reasoning resides in upper layers.",
            "transfer_conditions": "Transfer benefits when: (1) preserving pretrained low-level layers (freezing many lower layers), (2) balancing reasoning loss weight (λ_r ≈ 0.3), (3) pretraining distributions contain spatial/object interaction data (Open X-Embodiment, RHT20). Variant-aggregation (domain shift) benefits more from reasoning fine-tuning than simple visual-matching.",
            "novel_vs_familiar_objects": "Not explicitly quantified. The experiments include environmental and object variation, but the paper does not present per-object 'seen vs novel' breakdowns.",
            "zero_shot_or_few_shot": "ReFineVLA is evaluated as a fine-tuned agent; the paper provides zero-shot numbers only for some baseline pre-trained models (e.g., SpatialVLA zero-shot), not for ReFineVLA itself.",
            "layer_analysis": "Yes — ablations show best performance when freezing the first 24 transformer layers; selective tuning of later blocks and policy head injects reasoning without destroying pretrained generalization.",
            "negative_transfer_evidence": "Yes — the paper reports that (a) overly large reasoning loss weight λ_r harms performance (noisy outputs, misaligned attention), and (b) freezing too few or too many layers can degrade performance; optimal λ_r reported as 0.3 (gives +9.7% task success in their ablation). Exact negative-transfer magnitudes for extreme settings are described qualitatively but not fully quantified for all conditions.",
            "comparison_to_vision_only": "No direct experimental comparison to vision-only pretraining (e.g., ImageNet-only) is reported.",
            "temporal_dynamics": "No detailed time-course analysis of representation change over fine-tuning epochs beyond ablation observations (e.g., effects of λ_r and freezing).",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.0"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA (Exploring spatial representations for visual-language-action model)",
            "brief_description": "A 2B-parameter VLA model built on a PaliGemma2 VLM backbone that emphasizes spatial representations (Ego3D position encodings, Adaptive Action Grids) to improve 3D scene understanding and transfer for robotic manipulation.",
            "citation_title": "Exploring spatial representations for visual-language-action model",
            "mention_or_use": "use",
            "model_name": "SpatialVLA",
            "model_description": "Vision-language-action transformer using a PaliGemma2 VLM backbone with spatial representation modules (Ego3D Position Encoding, Adaptive Action Grids) to model 3D spatial relations; outputs discretized action tokens for robot policies.",
            "pretraining_type": "Vision-language pretraining on cross-embodiment robotic datasets (Open X-Embodiment, RHT20) and VLM pretraining for the backbone.",
            "pretraining_data_description": "Pretrained on large-scale robotic datasets (Open X-Embodiment, RHT20) containing multi-embodiment trajectories, spatial cues and action tokens; described as containing object interaction and spatial relationship information tailored for transfer.",
            "target_task_name": "Robotic manipulation (SimplerEnv: WidowX & Google Robot tasks)",
            "target_task_description": "Same SimplerEnv tasks as in ReFineVLA experiments (Put Spoon on Towel, Put Carrot on Plate, Stack blocks, Put Eggplant in Basket; Move Near, Open/Close Drawer) with both visual-matching and variant-aggregation settings; evaluated both zero-shot and after fine-tuning.",
            "semantic_alignment": "Paper implies good semantic alignment due to pretraining on robot-focused datasets with spatial information; authors treat SpatialVLA as a strong baseline with pre-aligned object-action semantics.",
            "performance_with_language_pretraining": "Reported (from paper tables): WidowX tasks — SpatialVLA (zero-shot) average 34.4%; SpatialVLA (fine-tuning) average 42.7%. Google Robot tasks — visual matching zero-shot average 64.5% (Move Near 69.6%, Open/Close Drawer 59.3%), visual matching fine-tuning average 70.1% (Move Near 85.7%, Open/Close Drawer 54.6%). Variant aggregation zero-shot average 54.0% (Move Near 71.7%, Open/Close Drawer 36.2%), variant aggregation fine-tuning average 52.6% (Move Near 79.1%, Open/Close Drawer 26.2%).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported quantitatively.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Yes — baseline attention maps show a narrow focus on immediate action targets and less holistic attention to contextual objects and spatial relationships compared to ReFineVLA.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "SpatialVLA includes spatial encodings intended to ground actions in 3D, but the paper presents only indirect evidence (performance and attention maps) rather than explicit probing of verb-to-affordance grounding.",
            "hierarchical_features_evidence": "Not directly analyzed in the paper (no layer-freezing ablation specific to SpatialVLA beyond being the pre-trained backbone).",
            "transfer_conditions": "Fine-tuning can improve some metrics but can also lower performance on particular subtasks (paper shows mixed task-level changes), indicating transfer sensitivity to task and fine-tuning regime.",
            "novel_vs_familiar_objects": "Not explicitly broken out in the reported results.",
            "zero_shot_or_few_shot": "Yes — SpatialVLA zero-shot evaluations are reported (see above numbers).",
            "layer_analysis": "Not reported in isolation (but used as the frozen backbone in ReFineVLA experiments which show best results when freezing lower layers of the pretrained VLA).",
            "negative_transfer_evidence": "Some tasks show decreased per-task performance after fine-tuning (e.g., certain WidowX tasks), indicating possible negative effects of naive fine-tuning for specific tasks or settings.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.1"
        },
        {
            "name_short": "PaliGemma2",
            "name_full": "Paligemma 2: A family of versatile VLMs for transfer",
            "brief_description": "A family of vision-language models (VLMs) used as backbones for VLA models (e.g., SpatialVLA) designed for transfer across downstream multimodal tasks, including embodied control.",
            "citation_title": "Paligemma 2: A family of versatile vlms for transfer",
            "mention_or_use": "use",
            "model_name": "PaliGemma 2 (VLM backbone)",
            "model_description": "Vision-language model family used as the multimodal encoder backbone for SpatialVLA; provides joint visual-text representations amenable to downstream fine-tuning for action token prediction.",
            "pretraining_type": "Vision-language pretraining on image-text data (VLM style); used as the backbone for further VLA pretraining.",
            "pretraining_data_description": "Not specified in detail in this paper beyond being a VLM family intended for transfer; likely trained on large image-text corpora (paper does not enumerate exact datasets).",
            "target_task_name": "Backbone for robotic manipulation VLA models",
            "target_task_description": "Used within VLA architectures to perform discretized action prediction and multimodal reasoning for SimplerEnv robotic tasks when fine-tuned.",
            "semantic_alignment": "Implied to provide semantic visual-text alignment that facilitates downstream grounding; not quantitatively characterized in this paper.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported for PaliGemma2 specifically in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Used as VLM backbone enabling downstream models to map language to visual regions and action tokens, but no direct probing in this paper.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Used successfully as a backbone for SpatialVLA and downstream ReFineVLA fine-tuning; specifics not quantified.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.2"
        },
        {
            "name_short": "Open X-Embodiment",
            "name_full": "Open X-Embodiment: Robotic learning datasets and RT-X models",
            "brief_description": "A large-scale cross-embodiment robot dataset used to pretrain vision-language-action models to support generalist transfer across multiple robot platforms and tasks.",
            "citation_title": "Open X-Embodiment: Robotic learning datasets and RT-X models",
            "mention_or_use": "use",
            "model_name": "Open X-Embodiment (dataset)",
            "model_description": "Dataset collection spanning many embodiments and manipulation tasks used to pretrain VLA backbones (enables cross-embodiment transfer and generalist policy pretraining).",
            "pretraining_type": "Dataset used for VLA pretraining (multimodal robot trajectories across embodiments).",
            "pretraining_data_description": "Contains multi-embodiment manipulation trajectories, diverse object interactions and action tokens; intended to provide cross-robot and cross-scene coverage for transfer learning.",
            "target_task_name": "Pretraining source for generalist robotic manipulation policies",
            "target_task_description": "Used by SpatialVLA and other VLA backbones to transfer to SimplerEnv tasks; not itself a target task.",
            "semantic_alignment": "High alignment with downstream embodied tasks because dataset is manipulation-centric and multi-embodiment.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable for dataset per se.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Dataset content (manipulation trajectories) is designed to provide affordance and action-context co-occurrence needed for grounding, but no direct probing in this paper.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Authors treat Open X-Embodiment as a primary pretraining resource whose similarity to downstream tasks supports transfer.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not applicable.",
            "layer_analysis": "Not applicable.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.3"
        },
        {
            "name_short": "BridgeData-v2",
            "name_full": "BridgeData-v2",
            "brief_description": "A large-scale robotic dataset used in this paper to generate teacher-guided multimodal reasoning annotations for fine-tuning (part of the 125k annotated trajectories).",
            "citation_title": "Bridgedata v2: A dataset for robot learning at scale",
            "mention_or_use": "use",
            "model_name": "BridgeData-v2 (dataset)",
            "model_description": "A multi-task robotic dataset with trajectories used for policy learning and for producing reasoning-enriched training examples when prompted with an expert teacher.",
            "pretraining_type": "Dataset for robotic policy pretraining and fine-tuning; contains demonstration trajectories and multimodal observations.",
            "pretraining_data_description": "Contains expert demonstrations across robotic tasks and environments; used here as one of the two sources for generating 125k reasoning-annotated trajectories.",
            "target_task_name": "Source data for fine-tuning VLA models for robotic manipulation",
            "target_task_description": "Used to produce multimodal (image + instruction + action) samples augmented with teacher rationales for ReFineVLA fine-tuning; simulation/real mixtures in original dataset (not enumerated here).",
            "semantic_alignment": "High — dataset is manipulation-focused, so contains objects, actions and spatial relations relevant to target tasks.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Used as source of action demonstrations that, when paired with teacher rationales, support grounding studies; no direct quantitative probing reported.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Used together with Google RT1 dataset to create a large reasoning-annotated fine-tuning corpus (125k trajectories).",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not applicable.",
            "layer_analysis": "Not applicable.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.4"
        },
        {
            "name_short": "Google RT-1 dataset / RT-1",
            "name_full": "RT-1: Robotics transformer for real-world control at scale (and associated RT-1 robot datasets)",
            "brief_description": "RT-1 is a transformer-based vision-to-action model trained on large real-world robot datasets; the related RT-1 robot datasets are used here as a source (Brohan et al.) for fine-tuning / evaluation samples and are also included in baseline comparisons.",
            "citation_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "use",
            "model_name": "RT-1 (and RT-1 robot datasets)",
            "model_description": "Transformer-based vision-to-action model trained on (real-world) robot demonstration data to map visual observations and instructions to action tokens; the dataset contains many manipulation demonstrations used both for RT-1 training and as a data source for other methods.",
            "pretraining_type": "Pretraining on large real-world robotic demonstration datasets (robot trajectories), not pure image-text web-scale VLM pretraining.",
            "pretraining_data_description": "Large-scale robot demonstration data (image + action + instruction), emphasizing real-world control sequences and action tokens; contains object interactions and motor behaviors but not necessarily web-scale image-text pairs.",
            "target_task_name": "Robotic manipulation (same SimplerEnv-style tasks and real-world tasks in original RT-1 work)",
            "target_task_description": "RT-1 targets real-world manipulation and is used as a baseline in SimplerEnv evaluations (paper reports RT-1 variants' success rates in visual matching and variant aggregation settings). Action space discretized as tokens; evaluated on Move Near and Open/Close Drawer tasks in Table 2.",
            "semantic_alignment": "High with manipulation tasks because training data are robot demonstration trajectories; paper uses RT-1 as a baseline to show transfer capability.",
            "performance_with_language_pretraining": "Reported in Table 2 (paper): RT-1 (converged) visual matching average 58.6% (Move Near 44.2%, Open/Close Drawer 73.0%); variant aggregation average 41.2% (Move Near 50.0%, Open/Close Drawer 32.3%). Additional RT-1 variants (begin / 15% / converged) appear in the table.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper for RT-1 specifically.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "RT-1's design and reported baseline performance indicate grounding from demonstration data to action tokens, but detailed grounding analyses are not provided in this paper.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "RT-1 variants in Table 2 show that different training regimes (begin/15%/converged) yield different transfer performance; details in original RT-1 paper.",
            "novel_vs_familiar_objects": "Not analyzed in this paper.",
            "zero_shot_or_few_shot": "RT-1 is included as a baseline; zero-shot abilities not explicitly highlighted in this paper's tables.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.5"
        },
        {
            "name_short": "RT-1-X / RT-2-X",
            "name_full": "RT-1-X / RT-2-X (extended RT models / X-Embodiment variants)",
            "brief_description": "Extensions of RT-style VLA models that further explore cross-embodiment generalization and larger pretraining / fine-tuning regimes; used as baselines in the paper.",
            "citation_title": "Rt-1-x (O'Neill et al., 2024) and Rt-2-x (O'Neill et al., 2024)",
            "mention_or_use": "mention",
            "model_name": "RT-1-X / RT-2-X",
            "model_description": "RT-family vision-language-action transformers adapted to cross-embodiment datasets (RT-X / OXE). Models treat discretized actions as language tokens and are trained/fine-tuned on large robot datasets to produce robot policies.",
            "pretraining_type": "Pretraining on large robot-dataset corpora spanning multiple embodiments (Open X-Embodiment / OXE), plus vision-language components.",
            "pretraining_data_description": "Open X-Embodiment / OXE datasets (multi-embodiment robot trajectories, object interactions, action tokens); RT-2 explicitly described as transferring web knowledge to robotic control.",
            "target_task_name": "Robotic manipulation (SimplerEnv style tasks referenced in comparisons)",
            "target_task_description": "Used as baselines evaluated on SimplerEnv Google Robot tasks; action token output for manipulation tasks in simulation.",
            "semantic_alignment": "Designed for high semantic alignment via OXE pretraining; paper uses them as strong baselines for transfer.",
            "performance_with_language_pretraining": "Table 2 reports RT-1-X: visual matching average 45.7% and variant aggregation average 30.9% (per table row RT-1-X). RT-2-X reported 77.9%/25.0% for visual matching/variant aggregation Move Near/Open-Close Drawer entries leading to an average 51.5% for visual matching and 57.3% for variant aggregation (per table).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed here.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Architectural choice to tokenize actions links language modality and action but explicit grounding analyses not given here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Designed to leverage OXE pretraining for cross-embodiment transfer; performance varies by task and setting.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not explicitly described here (treated as pre-trained / fine-tuned baselines).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported in this paper.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.6"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA: An open-source vision-language-action model",
            "brief_description": "An open-source VLA that tokenizes discretized robot actions as language tokens and fine-tunes vision-language backbones for robot control; used as a baseline in experiments.",
            "citation_title": "Openvla: An open-source vision-language-action model",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Vision-language-action model that treats actions as tokens and fine-tunes large VLM backbones for robotic tasks; open-source baseline used in comparisons.",
            "pretraining_type": "Fine-tuned from vision-language backbones; pretraining details vary by implementation in original work.",
            "pretraining_data_description": "Original OpenVLA leverages VLM backbones and robot demonstration datasets for fine-tuning; this paper uses OpenVLA as a baseline but does not enumerate its pretraining corpus.",
            "target_task_name": "Robotic manipulation (SimplerEnv tasks)",
            "target_task_description": "Evaluated as a baseline on SimplerEnv WidowX and Google Robot tasks; action tokens represent robot commands over discrete action space.",
            "semantic_alignment": "Designed to align vision-language backbones to action token spaces; paper uses it as a baseline to compare transfer/generalization.",
            "performance_with_language_pretraining": "Table 1 (WidowX tasks) OpenVLA average 1.1% (very low in that table). Table 2 (Google tasks) OpenVLA visual matching avg 40.9% and variant aggregation avg 32.7% (per the reported rows).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not analyzed in this paper beyond baseline performance numbers.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not analyzed here.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.7"
        },
        {
            "name_short": "Octo",
            "name_full": "Octo (An open-source generalist robot policy)",
            "brief_description": "A flexible transformer architecture to unify multiple robot embodiments for generalist policies; provided as baseline variants (Base, Small) in experiments.",
            "citation_title": "An open-source generalist robot policy",
            "mention_or_use": "use",
            "model_name": "Octo (Base / Small)",
            "model_description": "Transformer-based generalist policy designed to handle multiple embodiments via a unified architecture; variants of different sizes reported as baselines.",
            "pretraining_type": "Pretraining on multi-task / cross-embodiment robot datasets (details in original Octo work).",
            "pretraining_data_description": "Designed for heterogeneous embodiment data alignment; includes multi-task robot demonstrations, but exact content not enumerated in this paper.",
            "target_task_name": "Robotic manipulation (SimplerEnv WidowX tasks)",
            "target_task_description": "Evaluated as Octo-Base and Octo-Small on WidowX tasks with discrete action token outputs; Table 1 reports their per-task success rates.",
            "semantic_alignment": "Architected for cross-embodiment transfer; treated as a competitive baseline for generalist transfer.",
            "performance_with_language_pretraining": "Table 1: Octo-Base average 16.0% on WidowX tasks; Octo-Small average 30.0% (per Table 1 rows).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not analyzed in this paper.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not analyzed here.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.8"
        },
        {
            "name_short": "RoboVLM",
            "name_full": "RoboVLM (What matters in employing vision language models for tokenizing actions in robot control?)",
            "brief_description": "A vision-language approach to tokenizing robot actions and adapting VLMs for robotic manipulation; included as a baseline in both zero-shot and fine-tuned variants.",
            "citation_title": "What matters in employing vision language models for tokenizing actions in robot control?",
            "mention_or_use": "use",
            "model_name": "RoboVLM",
            "model_description": "Vision-language-action approach that investigates action tokenization choices for robotic control; evaluated as zero-shot and fine-tuned baseline.",
            "pretraining_type": "VLM-based pretraining followed by adaptation to robotic datasets; specifics in original RoboVLM work.",
            "pretraining_data_description": "Uses VLM pretraining (image-text) and robot trajectory fine-tuning (details in original work); contains object descriptions and action tokens during fine-tuning.",
            "target_task_name": "Robotic manipulation (SimplerEnv tasks)",
            "target_task_description": "Reported in Table 1 (WidowX tasks) and Table 2 (Google tasks) in both zero-shot and fine-tuned settings; action token outputs used for grasp/placement tasks.",
            "semantic_alignment": "Designed to align VLM semantics to action tokens; considered a baseline for measuring transfer.",
            "performance_with_language_pretraining": "Table 1 (WidowX): RoboVLM zero-shot average 13.5%; RoboVLM (fine-tuning) average 31.3%. Table 2 (Google): RoboVLM (zero-shot) visual matching Move Near 66.3% Open/Close Drawer 26.8% avg 46.6%; RoboVLM (fine-tuning) visual matching Move Near 61.7% Open/Close Drawer 43.5% avg 52.6% (per table rows).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "RoboVLM's design focuses on tokenizing actions which implicitly links language and actions; this paper reports only empirical performance.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Performance varies by visual-matching vs variant-aggregation; fine-tuning improves some metrics over zero-shot.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Zero-shot and fine-tuned numbers are reported for RoboVLM in tables.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.9"
        },
        {
            "name_short": "TraceVLA",
            "name_full": "TraceVLA (Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies)",
            "brief_description": "A VLA variant that explores using trace / temporal prompting to enhance spatial-temporal awareness for generalist robotic policies; cited as related work and baseline.",
            "citation_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies",
            "mention_or_use": "mention",
            "model_name": "TraceVLA",
            "model_description": "A VLA approach that augments inputs with visual traces to improve spatial-temporal reasoning for manipulation policies; intended to strengthen temporal awareness in VLA models.",
            "pretraining_type": "Built on VLA pretraining paradigms and uses manipulation datasets; exact pretraining details in original TraceVLA work.",
            "pretraining_data_description": "Uses temporally-structured manipulation data with spatial and temporal cues; paper references it as recent work improving spatial-temporal awareness.",
            "target_task_name": "Robotic manipulation / generalist policies (related domain)",
            "target_task_description": "Designed for embodied tasks that require temporal planning and spatial awareness; cited as related work rather than an evaluated baseline in this paper.",
            "semantic_alignment": "Trace-based prompting targets alignment of temporal/spatial representations with task requirements; this paper cites it as relevant prior art.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Paper references TraceVLA as improving spatial-temporal awareness, implying stronger grounding, but does not provide its internal evidence.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not discussed in detail in this paper.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "TraceVLA specifically addresses temporal cues but this paper does not include its dynamics.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1970.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring spatial representations for visual-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Open X-Embodiment: Robotic learning datasets and RT-X models",
            "rating": 2
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Paligemma 2: A family of versatile vlms for transfer",
            "rating": 1
        },
        {
            "paper_title": "What matters in employing vision language models for tokenizing actions in robot control?",
            "rating": 2
        },
        {
            "paper_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies",
            "rating": 1
        }
    ],
    "cost": 0.02977075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ReFineVLA: Multimodal Reasoning-Aware Generalist Robotic Policies via Teacher-Guided Fine-Tuning
25 May 2025</p>
<p>Tuan V Vo 
VinRobotics
Vietnam</p>
<p>Tan Q Nguyen 
VinRobotics
Vietnam</p>
<p>Khang M Nguyen 
Max Planck Research School for Intelligent Systems (IMPRS-IS)
Germany</p>
<p>Duy H M Nguyen 
University of Texas
Arlington</p>
<p>Minh N Vu minh.vu@ait.ac.at 
Automation &amp; Control Institute, TU
WienAustria</p>
<p>Austrian Institute of Technology (AIT)
ViennaAustria</p>
<p>ReFineVLA: Multimodal Reasoning-Aware Generalist Robotic Policies via Teacher-Guided Fine-Tuning
25 May 20254357659522E9C128B83FD455EB6706CBarXiv:2505.19080v1[cs.RO]
Vision-Language-Action (VLA) models have gained much attention from the research community thanks to their strength in translating multimodal observations with linguistic instructions into robotic actions.Despite their recent advancements, VLAs often overlook the explicit reasoning and only learn the functional input-action mappings, omitting these crucial logical steps for interpretability and generalization for complex, long-horizon manipulation tasks.In this work, we propose ReFineVLA, a multimodal reasoning-aware framework that fine-tunes VLAs with teacher-guided reasons.We first augment robotic datasets with reasoning rationales generated by an expert teacher model, guiding VLA models to learn to reason about their actions.Then, we use ReFineVLA to fine-tune pre-trained VLAs with the reasoning-enriched datasets, while maintaining their inherent generalization abilities and boosting reasoning capabilities.In addition, we conduct an attention map visualization to analyze the alignment among visual attention, linguistic prompts, and to-be-executed actions of ReFineVLA, showcasing its ability to focus on relevant tasks and actions.Through the latter step, we explore that ReFineVLA-trained models exhibit a meaningful attention shift towards relevant objects, highlighting the enhanced multimodal understanding and improved generalization.Evaluated across manipulation tasks, ReFineVLA outperforms the state-of-the-art baselines.Specifically, it achieves an average increase of 5.0% success rate on SimplerEnv WidowX Robot tasks, improves by an average of 8.6% in variant aggregation settings, and by 1.7% in visual matching settings for SimplerEnv Google Robot tasks.The source code will be publicly available.</p>
<p>Introduction</p>
<p>Robotic manipulation policies trained on task-specific demonstrations often struggle to generalize beyond their training data, limiting their effectiveness when faced with novel objects, environments, instructions, and embodiments.Recent progress in foundation models for vision and language, such as CLIP (Radford et al., 2021), LLaVA (Liu et al., 2024a), Phi-3-Vision (Abdin et al., 2024), along with powerful large language models (LLMs) and Vision-Language Models (VLMs), has showcased remarkable generalization across a broad spectrum of multimodality.Building on top of these, Vision-Language-Action (VLA) models (Brohan et al., 2022;Wen et al., 2024;Zhen et al., 2024a;Zheng et al., 2024;Zhou et al., 2025;Wu et al., 2024) has been proposed to aim for the fusion of broad generalization for foundation models with the specific expertise training on large-scale robotic datasets (Chen et al.; Ebert et al., 2021;Zhu et al., 2023b;O'Neill et al., 2023;Jiang et al., 2024;Khazatsky et al., 2024).</p>
<p>Despite inheriting the robustness of VLMs and being trained on large-scale datasets, VLA models often lack sophisticated and adaptive multimodal reasoning (Kim et al., 2024;Zhao et al., 2025;Zheng et al., 2024;Qu et al., 2025).They typically learn a direct functional mapping from multimodal observations to actions, without explicit step-by-step reasoning over the action horizon.This limitation becomes especially pronounced under out-of-distribution conditions, where robust performance requires a nuanced understanding of and adaptability to novel environmental variations.To address this problem, we propose ReFineVLA to inject explicit multimodal reasoning into pre-trained VLAs.Our approach leverages an expert teacher to generate detailed natural-language rationales that articulate the sequential logic as physical intelligence behind robotic actions in the context of language-based instructions.We hypothesize that this reasoning supervision enhances the model's understanding of observation-action relationships, thereby improving its performance on complex and compositional manipulation tasks (Lu et al., 2024;Zhang et al., 2025;Zhou et al., 2025).</p>
<p>Our ReFineVLA framework fine-tunes a VLA backbone and embeds it with reasoning-augmented trajectories using a multi-objective loss to predict actions and generate multimodal reasoning rationales.By doing this, the learning of policy and explicit reasoning adaptation is jointly optimized rather than mere input-output mappings, preserving the pre-trained generalization while enhancing reasoning capacity.Specifically, we instantiate ReFineVLA by fine-tuning a 2B-parameter VLA model on 125, 000 annotated manipulation trajectories with multimodal reasoning annotation.To evaluate generalization ability, we test our models on diverse SimplerEnv scenarios for Google and WidowX robots, which closely replicate real-world conditions.While comparing ReFineVLA's performance against the state-of-the-art methods, our model consistently outperforms existing VLA baselines across all embodiments and environments, demonstrating exceptional robustness under environmental variation, reflecting its deeper multimodal understanding and reasoning.To sum up, our key contributions are summarized below:</p>
<p>• Methodology: We propose ReFineVLA, a transfer-based fine-tuning framework that injects explicit multimodal reasoning into pre-trained VLAs via teacher-generated natural-language rationales, aligning policy learning with structured "chain-of-thought" supervision.</p>
<p>• Dataset &amp; Models: We curate a 125, 000-trajectory dataset by prompting an expert reasoning teacher to produce step-by-step multimodal reasoning rationales for each demonstration.We fine-tune 2B VLA backbones to learn this reasoning-enriched data, leading to improved performance and inference efficiency.</p>
<p>• Attention Visualization &amp; Validation: We validate our method via extensive simulations on WidowX and Google robots across diverse embodiments.We also conduct attention-map visualizations that reveal that the model's focus shifts from narrow action targets to semantically relevant objects and spatial anchors post-ReFineVLA fine-tuning.</p>
<p>Related Work</p>
<p>Vision-Language-Action Models: Building on the success of VLMs in understanding multimodal data (Karamcheti et al., 2023;Gadre et al., 2022;Driess et al., 2023;Du et al., 2023), recent work has extended these models to robotic control, giving rise to Vision-Language-Action (VLA) models.These models aim to develop generalist robot policies by enabling pre-trained VLMs to output robot actions.Methods like RT-2 (Brohan et al., 2023), RT-2-X (Collaboration et al., 2023a), OpenVLA (Kim et al., 2024), and RoboPoint (Yuan et al., 2024) treat discretized actions as language tokens and fine-tune large VLMs on robot datasets.Grounding a domain-general vision-language backbone in domain-specific constraints through hybrid explicit-implicit representation learning and task-centric adaptation, as presented by Robotic-CLIP (Nguyen et al., 2024), enables robots to inherit broad visual generalization while accurately modeling action-specific dynamics.Other approaches, like MobilityVLA (Chiang et al., 2024), CogACT (Li et al., 2024a), TraceVLA (Zheng et al., 2024), and π 0 (Black et al., 2024), explore reasoning traces and continuous action spaces.Meanwhile, SpatialVLA (Qu et al., 2025) enhances spatial representations in VLA models.</p>
<p>Despite their strong task performance and zero-shot generalization, these models typically require complex fine-tuning for new tasks or novel robot configurations.More importantly, their action-focused training objectives often bypass explicit, step-by-step multimodal reasoning, limiting robustness in tasks that demand deeper understanding and planning (Lu et al., 2024).</p>
<p>Generalist Robot Policies:</p>
<p>Recent advancements in robotic learning have witnessed a significant trend towards developing multi-task "generalist" robot policies capable of performing a wide array of tasks across diverse environments and embodiments, moving beyond task-specific controllers (Reed et al., 2022;Brohan et al., 2022;Haldar &amp; Pinto, 2023;Zhu et al., 2023a).Early efforts often focused on learning languageconditioned visual policies on a single embodiment using pretrained visual/text encoders (Parisotto et al., 2015;Rusu et al., 2015;Shridhar et al., 2023;Haldar et al., 2024), which limits their adaptability to new robot platforms.More recent research leverages large-scale, cross-embodiment robot datasets (O 'Neill et al., 2024) for pre-training generalist policies, facilitating effective fine-tuning to new robot setups (Team et al., 2024;Liu et al., 2024b;Wang et al., 2024).Notable example, Octo (Team et al., 2024), utilizes a flexible transformer architecture to unify embodiments.Diffusion-based generalist models, RPT (Liu et al., 2024b) and HPT (Wang et al., 2024), propose modular architectures to align data from heterogeneous embodiments.In this research line, VLAs represent a prominent direction within generalist policies by directly adapting powerful VLMs for action generation.Nevertheless, they lack the step-by-step reasoning of more sophisticated generalist robot policies.Thus, we aim to solve this problem via an expert-based fine-tuning framework that can work along with and complement generating robot policies.</p>
<p>Chain-of-Thought Reasoning for Robotics: Chain-of-Thought (CoT) reasoning has gained prominence in LLMs and VLMs, enabling them to break down complex problems into intermediate steps and improve performance on challenging reasoning tasks (Wei et al., 2022;Kojima et al., 2022;Lyu et al., 2023;Chia et al., 2023;Yao et al., 2023).Methods like fine-tuning smaller models on rationales generated by larger "teacher" models have shown promise in transferring reasoning abilities efficiently (Wei et al., 2021).CoT has also been explored in the visual domain for tasks like visual question answering and reasoning about future states (Shao et al., 2024;Rose et al., 2023;Hu et al., 2024;Harvey &amp; Wood, 2023).More recently, CoT-inspired ideas have appeared in embodied AI, including generating textual plans (Mu et al., 2024;Michał et al., 2024), generating robotic trajectories (Wen et al., 2023;Lu et al., 2023;Zawalski et al., 2024), or generating intermediate visual states (Ni et al., 2024;Liang et al., 2024).Nevertheless, the explicit application of multimodal reasoning to guide the policy learning process in VLA models for robotic manipulation, by training the VLA to jointly generate actions and explanatory multimodal reasoning rationales derived from a teacher, remains relatively underexplored.Our work, ReFineVLA, also bridges this gap by explicitly leveraging multimodal reasoning rationales generated by an expert LLM teacher to guide the fine-tuning of student VLA models, thereby instilling explicit reasoning capabilities directly into the learned robotic policies.</p>
<p>A Closer Look at Vision-Language-Action with Multimodal Reasoning</p>
<p>Background: Traditional approaches to robotic policy learning often rely on datasets of task-specific demonstrations D = {τ 1 , τ 2 , ..., τ n }, where each trajectory τ i = {(o t , s t , a t )} T t=1 records expert observations, states, and actions for a single task.These methods (Jain et al., 2024;Zhang et al., 2025;Tschannen et al., 2025) commonly employ a visual encoder F ϕ to extract features z i = F ϕ (o i ) from image observations o i , which are then fed into a policy network π θ to output action distributions â ∼ π θ (•|z, s).Training typically involves minimizing the discrepancy between the predicted actions â and the expert actions a (Zhang et al., 2024;Zhen et al., 2024b;Xiang et al., 2025).While effective for specific tasks, this paradigm often struggles with generalization to new tasks, environments, or robot embodiments.</p>
<p>Limitations of Vision-Language-Action with Multimodal</p>
<p>Reasoning Understanding : Despite their great foundation in VLMs and training on large robot datasets, standard VLA models can exhibit limitations in deep multimodal understanding and reasoning since the standard VLA fine-tuning objective primarily optimizes for accurate next-action prediction, as a direct functional mapping without any presence of reasoning-based CoT (Ni et al., 2024;Liang et al., 2024;Mu et al., 2024;Michał et al., 2024).Given an observation o (which includes both image and instruction) and an expert action a, the training process aims to minimize the negative log-likelihood of the action tokens, effectively learning the conditional probability p(a | o), where the standard loss formulation is L action = − log p(a | o).Consequently, during fine-tuning, current VLA models primarily learn a direct functional mapping from visual and linguistic inputs to the corresponding action outputs.While this direct mapping is sufficient for tasks where the required action is a simple, reactive response to the immediate observation and instruction, it often fails to infuse the step-by-step multimodal reasoning needed for more complex scenarios or tasks demanding compositional physical logic (Lu et al., 2024;Zhang et al., 2025;Zhou et al., 2025).This restriction impacts their ability for robust multimodal understanding for reasoning tasks, such as observation-situation analysis, i.e., interpreting the current state based on visual cues, instructions &amp; spatial reasoning, i.e., understanding spatial relationships between objects, and the robot &amp; task planning, i.e., breaking down a high-level goal into sequential steps.The action prediction loss, focused solely on the final action outcome, might implicitly encourage the model to find superficial correlations sufficient for seen tasks, but without explicitly learning the underlying causal or sequential logic, leading to a deficit in robust multimodal understanding beyond direct action mapping.</p>
<p>Prompt: Pick the bottle.</p>
<p>Prompt: Pick mushroom into the pot.</p>
<p>Prompt: Put small spoon from basket to tray.</p>
<p>Prompt: Put the small spoon onto the towel.4 ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning</p>
<p>To enrich VLA models' capability regarding interpretability and generalization, we incorporate ReFineVLA with an explicit multimodal reasoning.Specifically, we first present the generation of multimodal reasoning annotations in Section 4.1 from an expert reasoning teacher.Section 4.2 outlines the selective transfer fine-tuning strategy.Thus, the training objectives are formulated in Section 4.3 with the training algorithm (Algorithm 1) and the implementation details (Section 4.4).</p>
<p>Multimodal Reasoning Annotation Generation</p>
<p>VLA models effectively map multimodal inputs to robotic actions, yet typically overlook explicit multimodal reasoning for interpretability and robust generalization.Standard robotic datasets, denoted as
D = {(o i , a i )} N i=1
, contain multimodal observations o i , including visual images I i and language instructions l i , paired with actions a i .These datasets generally lack explicit reasoning annotations that explain why specific actions are suitable given the multimodal inputs.</p>
<p>To explicitly incorporate multimodal reasoning, we utilize powerful reasoning-based teacher models (i.e., Gemini (Team et al., 2023)).We generate reasoning annotations by prompting the teacher with the structured multimodal reasoning prompt (Figure 2).For each observation-action pair (o i , a i ), the teacher model generates a detailed multimodal reasoning annotation r i , explicitly elucidating the rationale behind the chosen action given the visual and linguistic context.The enriched dataset thus becomes
D ′ = {(o i , a i , r i )} N i=1
with r i denotes as the reasoning for i th pair in the original dataset D. These annotations act as structured multimodal reasoning signals, bridging perception and action.Explicit rationales enable models to understand and reason through complex task requirements, significantly enhancing their ability to generalize across tasks and interpret their decision-making processes.</p>
<p>Selective Transfer Fine-Tuning for Efficient Adaptation</p>
<p>ReFineVLA builds upon robust features learned by large-scale VLA models pretrained on extensive general robotic manipulation data.Rather than fully fine-tuning the model, which can be computationally expensive, ReFineVLA employs selective transfer fine-tuning, outlined as follows:</p>
<p>• Preservation of General Features: Pre-trained VLA models encode diverse and generalizable features.Full fine-tuning on specialized reasoning-enriched datasets risks overfitting, thereby losing foundational generalization.Therefore, selective fine-tuning preserves these foundational representa- -Identify and locate the spoon: The spoon is identifiable and its position in the sink is known.</p>
<p>-Identify and locate the cup: The cup is identifiable and its position in the sink is known.</p>
<p>-Approach the spoon: The robot needs to move its gripper closer to the spoon.</p>
<p>-Grasp the spoon: The robot must successfully pick up the spoon with its gripper.</p>
<p>-Move the spoon towards the cup: Once grasped, the robot needs to lift the spoon and maneuver it over the cup.</p>
<p>-Place the spoon on the cup: The robot must lower the spoon onto the rim or opening of the cup.</p>
<p>-Release the spoon: The robot will then open its gripper to release the spoon onto the cup.sub_action : Move its arm to a position suitable for grasping the spoon.tions by freezing most parameters, particularly the lower layers involved in basic feature extraction.</p>
<p>Visual Inputs &amp; Linguistic Instruction</p>
<p>Chain-of-Thought Reasoning</p>
<p>• Efficient Adaptation: Selectively tuning a subset of model parameters significantly reduces computational resource demands, such as time, memory, and FLOPs, making ReFineVLA practical and scalable for diverse robotic tasks and embodiments.</p>
<p>• Targeted Knowledge Infusion: Explicit multimodal reasoning predominantly involves higher-level cognitive processing.We hypothesize that such abstract reasoning capabilities reside in the upper layers of the VLA model, typically associated with decision-making and complex feature integration.Selective fine-tuning targets these upper layers, enabling the model to effectively integrate explicit reasoning capabilities without compromising foundational low-level feature extraction.</p>
<p>Empirically determined layers, such as later transformer blocks in vision and language encoders and the policy head, are selected to ensure targeted reasoning infusion, preserving general pretrained features while efficiently adapting the model for explicit reasoning.</p>
<p>Reasoning Teacher Downstream Manipulation with Instruction closed-loop control reasoning tokens action tokens</p>
<p>Learning Objective</p>
<p>To successfully realize the shortcomings in robotic multimodal reasoning, we define the training objective of ReFineVLA that jointly optimizes action prediction and reasoning generation as follows:
L ReFineVLA = L action + λ r L reasoning , (1)
where L action presents the behavioral cloning loss ensuring accurate action prediction, L reasoning guides rationale generation, fostering structured multimodal reasoning, and λ r serves as a hyperparameter controlling the penalty of ReFineVLA's reasoning performance.</p>
<p>Action Prediction Loss (L action ):</p>
<p>The model predicts ground-truth actions a i given multimodal inputs o i , optimizing the negative log-likelihood:
L action = − t log P(a i,t | o i , a i,&lt;t ; θ). (2)
Reasoning Generation Loss (L reasoning ): This loss trains the model to produce rationales r i , supervised through standard language modeling negative log-likelihood:
L reasoning = − j log P(r i,j | o i , r i,&lt;j ; θ). (3)
In brief.Equation 2 ensures the model learns the primary task as an objective for actions; meanwhile, Equation 3 governs the model to learn thinking step-by-step, akin to a reasoning objective.</p>
<p>Implementation Details</p>
<p>The training of ReFineVLA in Algorithm 1 follows a supervised learning paradigm, iteratively updating the selected parameters θ of the pretrained VLA model.Through this, we are able to refine the VLA model's // action objective (Equation 2)
8 L (j)
reasoning ← − log P(r j | x j ; θ)</p>
<p>// reasoning objective (Equation 3)
9 L batch ← 1 B B b=1 L (b) action + λ r L (b) reasoning
// model objective (Equation 1) ability to jointly predict actions and generate rationales, leveraging pretrained knowledge while efficiently adapting to the reasoning-augmented data, as illustrated in Figure 3.
10 θ ← θ − η∇ θ L batch //
We apply our multimodal reasoning annotation generation pipeline to the BridgeData-v2 (Walke et al., 2023) and Google RT1 Robot datasets (Brohan et al., 2022) to create our datasets with reasoning annotations.Specifically, we gathered approximately 125, 000 robot trajectories annotated with multimodal reasoning annotation, forming our fine-tuning dataset for ReFineVLA.For VLA models, we started with SpatialVLA (Qu et al., 2025), a 2B VLA model based on the PaliGemma 2 VLM backbone (Steiner et al., 2024), trained on the Open X-Embodiment (O 'Neill et al., 2024) andRHT20 datasets (Fang et al., 2024).SpatialVLA is also pre-trained on a large dataset, exploring effective spatial representations through techniques like Ego3D Position Encoding and Adaptive Action Grids, which enhance 3D scene understanding and transferability.These models provide robust starting points with established capabilities in generalist policies and spatial awareness, respectively, making them suitable backbones for learning enhanced reasoning.</p>
<p>Experiments &amp; Ablation Studies</p>
<p>Baselines: To evaluate our method, we run experiments on diverse environments, including 137 configurations across SimplerEnv Google Robot tasks and WidowX Robot tasks that closely mimic real-world conditions.We compare against state-of-the-art open-source generalist policies with varying model sizes and training paradigms.The baselines include OpenVLA (Kim et al., 2024), Octo (Team et al., 2024), RT1-X (O 'Neill et al., 2024), RoboVLM (Dorka et al., 2024), TraceVLA (Zheng et al., 2024), and SpatialVLA (Qu et al., 2025).</p>
<p>Simulation Evaluation</p>
<p>SimplerEnv: Our simulation evaluation utilizes SimplerEnv Google Robot tasks, which incorporates two distinct settings: visual matching and variant aggregation.The visual matching setting aims to minimize the visual appearance gap between real environments and raw simulation, significantly enhancing the correlation between policy performance in simulation and real-world scenarios.Complementing this, variant aggregation covers a wide range of environmental variations, including backgrounds of different rooms, lighter and darker lighting conditions, varying numbers of distractors, solid color and complex table textures, and camera poses.We also evaluate across different manipulation policies on the SimplerEnv WidowX Robot tasks setup with tasks: Put Spoon on Towel, Put Carrot on Plate, Stack Green Block on Yellow Block, and Put Eggplant in Yellow Basket, measuring both grasp correction success and full task completion rates.These variations allow us to assess the robustness and adaptability of ReFineVLA in handling diverse manipulation scenarios, particularly in evaluating the spatial and temporal awareness brought by multimodal reasoning understanding.</p>
<p>Ablation Studies</p>
<p>To analyze the performance gain of ReFineVLA, we further study the following key questions:</p>
<p>1) How does attention behavior change before and after ReFineVLA fine-tuning?To understand how reasoning supervision impacts model behavior, we analyze attention maps before and after fine-tuning with ReFineVLA.As shown in Figure 4, before fine-tuning, VLA models tend to focus narrowly on immediate action targets -often overlooking contextual elements such as the spatial arrangement of objects or instruction -relevant cues.This behavior reflects the model's tendency to learn direct input-to-action mappings without deeper scene understanding.</p>
<p>Prompt: Pick the bottle.</p>
<p>Prompt: Pick mushroom into the pot.</p>
<p>Prompt: Put small spoon from basket to tray.Prompt: Put the small spoon onto the towel.</p>
<p>SpatialVLA</p>
<p>ReFineVLA (Ours) After applying ReFineVLA, we observe a consistent shift in attention toward semantically meaningful regions, empirically revealing that the model learns to reason more holistically about the task by integrating visual and linguistic information over time.Figure 4 shows that the model's attention aligns better with the task instruction, supporting more robust and interpretable action decisions.These findings prove that ReFineVLA improves performance and promotes more structured, multimodal representations and human-understandable decision-making processes.2) How does ReFineVLA reason on complex or long-horizon robotic manipulation tasks? Figure 5 illustrates a reasoning procedure of ReFineVLA for a robotic actuator actions alongside the step-by-step task planning.For complex tasks that require visual and instruction understanding, like Close the drawer, ReFineVLA shows a fine-grained instruction and corresponding actions to achieve the goal.ReFineVLA's capability continues with long-horizon tasks, such as Move the coke can near the orange.The model can still generate proper actions with corresponding reasoning steps, not limited to observation, situation analysis, and spatial reasoning.It can be observed that ReFineVLA can identify the objects present in embodied scenes and estimate their approximate relative positions, which is critical for action learning.Furthermore, ReFineVLA is able to guide the robot to the next sub-actions, eventually achieving the goal task.3) How does ReFineVLA perform with different weighting of the reasoning loss, guided by the teacher's multimodal supervision?An important hyperparameter in ReFineVLA is the reasoning loss weight λ r , which determines the strength of the reasoning loss guided by the teacher's multimodal supervision.A higher λ r encourages the model to focus more on generating detailed step-by-step rationales, potentially improving task-level understanding and interpretability.However, when λ r is set too high, it leads to overemphasis on reasoning generation, resulting in noisy outputs or misaligned attention that distracts from critical visual features such as the robot end-effector or target object.In contrast, a smaller λ r reduces the risk of distraction but may produce shallow or underdeveloped reasoning traces, weakening the benefit of teacher-guided supervision.As shown in Figure 6, setting λ r = 0.3 achieves the best performance, yielding a 9.7% improvement in task success, empirically showing that integrating teacher-provided reasoning can meaningfully enhance decision quality.However, excessively low and high values of λ r decrease performance, indicating the need to balance action prediction and reasoning learning.</p>
<p>4) How does</p>
<p>ReFineVLA perform under different numbers of frozen layers during learning of teacher-guided multimodal reasoning supervision?We find that freezing lower layers helps preserve general visual and linguistic representations learned from large-scale pretraining, while allowing upper layers to specialize for structured reasoning.</p>
<p>The performance peaks when freezing the first 24 transformer layers, resulting in an 8.2% improvement of task success rate, as shown in Figure 7, which suggests that the upper layers of the model are most critical for absorbing high-level reasoning supervision, while the lower layers should retain pretrained perceptual grounding.</p>
<p>In contrast, freezing too few layers can potentially degrade these generalizable features, leading to noisy attention and less stable reasoning outputs.Freezing too many layers restricts the model's learning capacity and limits its ability to align with teacher-guided rationales.</p>
<p>Conclusions and Discussions</p>
<p>In this work, we proposed ReFineVLA, a teacher-guided fine-tuning framework that enhances VLA models with explicit multimodal reasoning.By leveraging structured rationales from an expert reasoning teacher, ReFineVLA trains policies that jointly predict actions and generate step-by-step reasoning, enabling more profound understanding of complex and long-horizon robotic tasks.Through selective layer tuning and penalty on weights for reasoning-governed loss, our method preserves generalizable features while injecting high-level reasoning capabilities.Moreover, experiments across simulated and real-world robotic settings in SimplerEnv with WindowX Robot and Google Robot tasks demonstrate that ReFineVLA outperforms existing baselines in performance and interpretability, establishing a promising direction for reasoning-driven generalist VLA-based robot policies.</p>
<p>Future Works: While ReFineVLA demonstrates strong performance and improved multimodal reasoning, several promising avenues remain for exploration.One direction is to scale reasoning supervision through human-in-the-loop refinement or self-improving teacher models using reinforcement learning.Besides that, extending ReFineVLA to real-world robotic systems will also help evaluate its robustness in sim-to-real transfer settings.Finally, incorporating memory mechanisms or temporal context could enable reasoning across long-horizon tasks.</p>
<p>Figure 1 :
1
Figure 1: Attention Visualization: Attention maps of action tokens in standard VLAs, illustrating the narrow focus on visual cues.To investigate this limitation, we conduct a closer look at the attention mechanisms within standard VLAs, specifically examining the attention heatmap of action tokens.Motivated by work in visual grounding(Kang et al., 2025), which shows that certain attention heads in VLMs can localize image regions corresponding to text, we analyze where the VLA model attends in the input image when predicting actions.Our attention map visualizations in Figure1further reveal that traditional VLAs focus narrowly on specific visual cues directly associated with the immediate action, often disregarding crucial broader multimodal context and spatial relationships necessary for deeper reasoning.This narrow focus supports the hypothesis that standard action-only training encourages a reactive, direct mapping rather than a comprehensive multimodal understanding grounded in explicit rationale.</p>
<p>Input Prompt: You are a robot.Given the image and the instruction to put the spoon into the cup, scan the scene and reason step-by-step about what you see, what each object is, what are the planning steps for a robotic controller, and what actions might be possible?Think carefully and describe the steps followed the key questions.#1 <Observation>: What do you see in the images?#2 <Situation Analysis>: What is happening in the scene?#3 <Spatial Reasoning>: How are the objects arranged, and what spatial relationships matter for completing the task?#4 <Task Planning>: What are the logical steps to achieve the task, and what should be the robot's sequential actions?#1 <Observation>: I observe a toy kitchen sink area.Inside the sink basin, there are several toy items: a yellow banana, a green cucumber, a purple eggplant, a light blue cup, and a light blue spoon with a fork attached.To the right of the sink, on what appears to be a stovetop, there is a metal pot and an orange object (possibly a carrot).Above the sink, there is a robotic arm with a gripper extended downwards, positioned near the back of the sink.There are also other toy items visible on the counter surrounding the sink.#2 <Situation Analysis>: The scene depicts a toy kitchen environment.The robotic arm is positioned over the sink, which contains several toy items.The task is to put the spoon on the cup.#3 <Spatial Reasoning>: The spoon and the cup are both located inside the sink basin.The cup is positioned upright and closer to the front right corner of the sink from the robot's perspective.The spoon, which appears to be part of a spoon-fork utensil, is lying flat next to the cup, slightly to its left.The robotic arm's gripper is currently positioned above the back of the sink, not directly over the spoon or cup.To complete the task, the robot needs to interact with both the spoon and the cup.The spoon is currently not on the cup.#4 <Task Planning>: The robot has to move its arm to a position suitable for grasping the spoon.logical_steps :</p>
<p>Figure 2 :
2
Figure 2: Multimodal Robotic Instruction Understanding with Chain-of-Thought Reasoning: An illustrative example depicts a single annotated data sample from a robotic manipulator grounded task planning.The task is for a robot to place a spoon into a cup, given an observation of a cluttered scene and natural language instructions.The input prompt guides the robot to reason through a sequence of structured questions: (1) Observation -identifying objects in the image; (2) Situation Analysis -understanding the context; (3) Spatial Reasoning -analyzing object relationships; and (4) Task Planning -formulating an action plan in logical steps.The annotated response includes step-by-step reasoning under each category, leading to a detailed plan of robot actions involving position, rotation, and gripper control for low-level motor commands.</p>
<p>observed several items: a yellow banana, a light blue cup, a light blue spoon-fork Situation Analysis: The scene depicts ... The task is put the spoon into the cup.Spatial Reasoning: The spoon, which appears to be part of a spoon-fork utensil, is lying flat next to the cup ... Task Planning: The robot needs to logical_steps : Move its gripper closer to the spoon.Grasp the spoon.Move the spoon towards the cup.Place the spoon on the cup.Release the spoon.sub_action : Move its arm to a position suitable for grasping the spoon.</p>
<p>Figure 3 :
3
Figure 3: ReFineVLA's Training Flow: A fine-tuning framework that enhances VLA models with explicit multimodal reasoning, guided by rationales from a teacher model.These rationales cover visual cues, spatial reasoning, and task planning and are injected during training via action and reasoning losses.The learner integrates visuallinguistic inputs, infuses reasoning, and outputs interpretable actions for closed-loop control.</p>
<p>Algorithm 1 :3
1
ReFineVLA: Reasoning-Aware Fine-Tuning of VLA Input: Reasoning dataset D ′ = {(x i , r i , a i )} N i=1 ; pre-trained parameters θ full ; batch size B Output: Fine-tuned subset of parameters θ 1 Freeze θ full \ θ // only fine-tune upper layers 2 repeat Sample mini-batch {(o j , a j , r j )} B j=1 ∼ D ′ 4 for j = 1 to B do 5 âj ← VLA θ (o j ) log P(a j | x j ; θ)</p>
<p>Figure 4 :
4
Figure 4: Attention Visualization of ReFineVLA compared to SpatialVLA: RefineVLA shows better attention to related entities within the given observations conditioned by the input prompts than what SpatialVLA does.</p>
<p><Observation>:Figure 5 :
5
Figure 5: Chain-of-Thought Reasoning of ReFineVLA: RefineVLA shows step-by-step reasoning to accomplish the prompted task given the initial observation.The examples illustrate the queries (1) for placing the coke can near the orange positioned in the tabletop settings and (2) for closing the drawer while it is opening.</p>
<p>Figure 6 :
6
Figure 6: Average success rate of ReFineVLA with different weights of reasoning loss λr.</p>
<p>Figure 7 :
7
Figure 7: Average success rate of ReFineVLA with different numbers of frozen layers during teacher-guided finetuning for multimodal reasoning.</p>
<p>Table 1 : Evaluated performances of different VLA baselines on SimplerEnv with WidowX Robot tasks:
1
The zero-shot and fine-tuning results denote the performance of the pre-trained models on the OXE dataset (O'Neill  et al., 2024)and fine-tuned models on theBridgeData-v2 (Walke et al., 2023), respectively.
Put SpoonPut CarrotStack Green BlockPut EggplantVLA BaselineRobotic Taskon Towel Grasp Success Grasp Success Grasp on Plate on Yellow Block Successin Yellow Basket Grasp SuccessAverageRT-1-X (Collaboration et al., 2023b) 16.7%0.0%20.8%4.2%8.3%0.0%0.0%0.0%1.1%Octo-Base (Team et al., 2024) 34.7%12.5%52.8%8.3%31.9%0.0%66.7%43.1%16.0%Octo-Small (Team et al., 2024) 77.8% 47.2%27.8%9.7%40.3%4.2%87.5%56.9%30.0%OpenVLA (Kim et al., 2024)4.1%0.0%33.3%0.0%12.5%0.0%8.3%4.1%1.1%RoboVLM (zero-shot) (Dorka et al., 2024) 37.5%20.8%33.3%25.0%8.3%8.3%0.0%0.0%13.5%RoboVLM (fine-tuning) (Dorka et al., 2024) 54.2%29.2%25.0%25.0%45.8%12.5%58.3%58.3%31.3%SpatialVLA (zero-shot) (Qu et al., 2025) 25.0%20.8%41.7%20.8%58.3%25.0%79.2%70.8%34.4%SpatialVLA (fine-tuning) (Qu et al., 2025) 20.8%16.7%29.2%25.0%62.5%29.2%100.0% 100.0%42.7%ReFineVLA (Ours) 42.9%38.1%33.3%33.3% 71.4%23.8%100.0%95.2%47.7%</p>
<p>Table 2 : Evaluated performances of different VLA baselines on SimplerEnv with Google Robot tasks:
2
(Brohan et al., 2022)e-tuning results denote the performance of the pre-traineđ models on the OXE dataset (O'Neill  et al., 2024)and the fine-tuned models on the Fractal dataset(Brohan et al., 2022), respectively.As shown in Table1 and Table 2, ReFineVLA consistently outperforms state-ofthe-art baselines across SimplerEnv tasks (WindowX and Google Robot).On the SimplerEnv WindowX tasks (Table1), ReFineVLA attains a 47.7% average success rate, a 5.0% improvement over SpatialVLA.Specifically, ReFineVLA shows gains of 21.4% on the Put Spoon on Towel task and 8.3% on the Put Carrot on Plate task.For the SimplerEnv Google Robot tasks (Table2), ReFineVLA excels in both settings: in variant aggregation, it achieves a 61.2% average success rate (8.6% over SpatialVLA); in visual matching, it reaches 71.8% average success (1.7% over SpatialVLA).On the Move Near and Open/Close Drawer tasks under variant aggregation, ReFineVLA yields improvements of 3.7% and 13.3%, respectively.These results highlight ReFineVLA's effectiveness in incorporating explicit multimodal reasoning and enhancing generalization and robustness across diverse robotic manipulation tasks and challenging environments.
Robotic TaskVisual MatchingVariant AggregationVLA BaselineMove NearOpen/Close DrawerAverage Move NearOpen/Close DrawerAverageRT-1 (begin) (Brohan et al., 2022)5.0%13.9%9.5%4.0%6.9%5.5%RT-1 (15%) (Brohan et al., 2022)35.4%56.5%46.0%44.6%26.7%35.7%RT-1 (converged) (Brohan et al., 2022)44.2%73.0%58.6%50.0%32.3%41.2%HPT (Wang et al., 2024)60.0%24.0%42.0%---TraceVLA (Zheng et al., 2024)53.7%57.0%55.4%56.4%31.0%43.7%RT-1-X (O'Neill et al., 2024)31.7%59.7%45.7%32.3%29.4%30.9%RT-2-X (O'Neill et al., 2024)77.9%25.0%51.5%79.2%35.3%57.3%Octo-Base (Team et al., 2024)4.2%22.7%13.5%3.1%1.1%2.1%OpenVLA (Kim et al., 2024)46.2%35.6%40.9%47.7%17.7%32.7%RoboVLM (zero-shot) (Li et al., 2024b)66.3%26.8%46.6%56.0%8.5%32.3%RoboVLM (fine-tuning) (Li et al., 2024b)61.7%43.5%52.6%60.0%10.6%35.3%SpatialVLA (zero-shot) (Qu et al., 2025)69.6%59.3%64.5%71.7%36.2%54.0%SpatialVLA (fine-tuning) (Qu et al., 2025)85.7%54.6%70.1%79.1%26.2%52.6%ReFineVLA (Ours)84.6%59.0%71.8%82.8%39.5%61.2%Overall Performance:</p>
<p>Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, arXiv:2404.14219Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Lawrence Yunliang, Chen , Simeon Adebola, and Ken Goldberg. Berkeley UR5 demonstration dataset. </p>
<p>. Ken Yew, Guizhen Chia, Chen, Anh Luu, Soujanya Tuan, Lidong Poria, Bing, arXiv:2311.092772023Contrastive chain-of-thought prompting. arXiv preprint</p>
<p>Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. Lewis Hao-Tien, Zhuo Chiang, Zipeng Xu, Mithun Fu, George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, arXiv:2407.077752024arXiv preprint</p>
<p>Abby O' Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Pratap Kunal, Kuo-Hao Singh, Kyle Zeng, Kyle Hatch, Laurent Hsu, Lawrence Itti, Lerrel Yunliang Chen, Li Pinto, Liam Fei-Fei, Tan, " Linxi, Lionel Jim" Fan, Lisa Ott, Luca Lee, Magnum Weihs, Marion Chen, Marius Lepert, Masayoshi Memmel, Masha Tomizuka, Mateo Guaman Itkina, Max Castro, Maximilian Spero, Michael Du, Michael C Ahn, Mingtong Yip, Mingyu Zhang, Minho Ding, Mohan Heo, Mohit Kumar Srirama, Sharma, Jin Moo, Naoaki Kim, Nicklas Kanazawa, Nicolas Hansen, Heess, J Nikhil, Niko Joshi, Ning Suenderhauf, Norman Liu, Nur Di Palo, Muhammad Mahi, Oier Shafiullah, Oliver Mees, Osbert Kroemer, Bastani, Patrick "tree" Pannag R Sanketi, Patrick Miller, Paul Yin, Peng Wohlhart, Peter Xu, Peter David Fagan, Pierre Mitrano, Pieter Sermanet, Priya Abbeel, Qiuyu Sundaresan, Quan Chen, Rafael Vuong, Ran Rafailov, Ria Tian, Roberto Doshi, Runjia Mart ; Roy Lin, Ruohan Qian, Russell Zhang, Rutav Mendonca, Ryan Shah, Ryan Hoque, Tianhe Julian ; Thomas Kollar, Tianli Yu, Todor Ding, Tony Z Davchev, Travis Zhao, Trevor Armstrong, Trinity Darrell, Vidhi Chung, Vikash Jain, Vincent Kumar, Wei Vanhoucke, Wenxuan Zhan, Wolfram Zhou, Xi Burgard, Xiangyu Chen, Xiaolong Chen, Xinghao Wang, Xinyang Zhu, Xiyuan Geng, Xu Liu, ; A Liangwei, Padalkar, Pooley, Jain, Bewley, Herzog, Irpan, Khazatsky, Rai, Singh, arXiv:2310.08864Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models. Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Mart'in, Rohan Baijal, Rosario Scalise, Rose Hendrix,; Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen ChebotarOpen X-Embodiment Collaboration2023bYifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff CuiarXiv preprintRobotic learning datasets and rt-x models</p>
<p>What matters in employing vision language models for tokenizing actions in robot control?. Nicolai Dorka, Chenguang Huang, Tim Welschehold, Wolfram Burgard, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Vision-language models as success detectors. Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando De Freitas, Serkan Cabi, arXiv:2303.072802023arXiv preprint</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, Sergey Levine, arXiv:2109.133962021arXiv preprint</p>
<p>Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot. Hongjie Hao-Shu Fang, Zhenyu Fang, Jirong Tang, Chenxi Liu, Junbo Wang, Haoyi Wang, Cewu Zhu, Lu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Yitzhak Samir, Mitchell Gadre, Gabriel Wortsman, Ludwig Ilharco, Shuran Schmidt, Song, arXiv:2203.10421Clip on wheels: Zero-shot object navigation as object localization and exploration. 202237arXiv preprint</p>
<p>Polytask: Learning unified policies through behavior distillation. Siddhant Haldar, Lerrel Pinto, arXiv:2310.085732023arXiv preprint</p>
<p>Baku: An efficient transformer for multi-task policy learning. Siddhant Haldar, Zhuoran Peng, Lerrel Pinto, arXiv:2406.075392024arXiv preprint</p>
<p>Visual chain-of-thought diffusion models. William Harvey, Frank Wood, arXiv:2303.161872023arXiv preprint</p>
<p>Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna, arXiv:2406.094032024arXiv preprint</p>
<p>Vcoder: Versatile vision encoders for multimodal large language models. Jitesh Jain, Jianwei Yang, Humphrey Shi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Robots pre-train robots: Manipulation-centric robotic representation from large-scale robot datasets. Guangqi Jiang, Yifei Sun, Tao Huang, Huanyu Li, Yongyuan Liang, Huazhe Xu, 2024</p>
<p>Your large vision-language model only needs a few attention heads for visual grounding. Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang, arXiv:2503.062872025arXiv preprint</p>
<p>Language-driven representation learning for robotics. Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang, arXiv:2302.127662023arXiv preprint</p>
<p>Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, arXiv:2411.196502024aarXiv preprint</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, arXiv:2412.140582024barXiv preprint</p>
<p>Dreamitate: Real-world visuomotor policy learning via video generation. Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, Carl Vondrick, arXiv:2406.168622024arXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2024a36</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024barXiv preprint</p>
<p>Thinkbot: Embodied instruction following with thought chain reasoning. Guanxing Lu, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang, arXiv:2312.070622023arXiv preprint</p>
<p>Research on task decomposition and motion trajectory optimization of robotic arm based on vla large model. Wentao Lu, Xiaofeng Wu, Shuyong Gao, Wei He, Qing Zhao, Lunning Zhang, Maodong Li, Wenqiang Zhang, 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA). IEEE2024</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, IJCNLP-AACL 2023The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the Association for Computational Linguistics2023</p>
<p>Robotic control via embodied chain-of-thought reasoning. Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Chelsea Finn, Levine Sergey, arXiv:2407.086932024arXiv preprint</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, Advances in Neural Information Processing Systems. 202436</p>
<p>Robotic-clip: Fine-tuning clip on action data for robotic applications. Nghia Nguyen, Minh Nhat Vu, Tung D Ta, Baoru Huang, Thieu Vo, Ngan Le, Anh Nguyen, arXiv:2409.177272024arXiv preprint</p>
<p>Generate subgoal images before act: Unlocking the chain-of-thought reasoning in diffusion model for robot manipulation with multimodal prompts. Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Jiashun Liu, Yan Zheng, Bin Wang, Yuzheng Zhuang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Abby O' Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, arXiv:2310.088642023arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Actor-mimic: Deep multitask and transfer reinforcement learning. Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov, arXiv:1511.063422015arXiv preprint</p>
<p>Exploring spatial representations for visual-language-action model. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, 2025RSS</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barthmaron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Transactions on Machine Learning Research. 2835-88562022Outstanding Certification</p>
<p>Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang, Wang , arXiv:2305.02317Visual chain of thought: bridging logical gaps with multimodal infillings. 2023arXiv preprint</p>
<p>. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell, arXiv:1511.062952015Policy distillation. arXiv preprint</p>
<p>Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li, arXiv:2403.169992024arXiv preprint</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, arXiv:2412.03555Shangbang Long, et al. Paligemma 2: A family of versatile vlms for transfer. 2024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. Alexey Michael Tschannen, Xiao Gritsenko, Muhammad Ferjad Wang, Ibrahim Naeem, Nikhil Alabdulmohsin, Talfan Parthasarathy, Lucas Evans, Ye Beyer, Basil Xia, Mustafa, arXiv:2502.147862025arXiv preprint</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023</p>
<p>Scaling image tokenizers with grouped spherical quantization. Jiangtao Wang, Zhen Qin, Yifan Zhang, Vincent Tao Hu, Björn Ommer, Rania Briq, Stefan Kesselheim, arXiv:2412.026322024arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, arXiv:2409.125142024arXiv preprint</p>
<p>Vila-u: a unified foundation model integrating visual understanding and generation. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, arXiv:2409.044292024arXiv preprint</p>
<p>Vla model-expert collaboration for bi-directional manipulation learning. Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, arXiv:2503.041632025arXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction in robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, 8th Annual Conference on Robot Learning. 2024</p>
<p>Robotic control via embodied chain-of-thought reasoning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Vla-3d: A dataset for 3d semantic scene understanding and navigation. Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, Wenshan Wang, arXiv:2411.035402024arXiv preprint</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen, arXiv:2501.188672025arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024aarXiv preprint</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024barXiv preprint</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>
<p>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, arXiv:2502.14420Unified multimodal understanding and robot control with vision-language-action model. 2025arXiv preprint</p>
<p>Learning to solve tasks with exploring prior behaviours. Ruiqi Zhu, Siyuan Li, Tianhong Dai, Chongjie Zhang, Oya Celiktutan, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023a</p>
<p>Fanuc manipulation: A dataset for learning-based manipulation with fanuc mate 200id robot. Xinghao Zhu, Ran Tian, Chenfeng Xu, Mingxiao Huo, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding, 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
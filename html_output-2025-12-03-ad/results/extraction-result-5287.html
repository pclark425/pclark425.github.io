<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5287 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5287</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5287</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-ca722c18eb6546d307f6c3a0c1efd064a53a6a29</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ca722c18eb6546d307f6c3a0c1efd064a53a6a29" target="_blank">Inverse design of 3d molecular structures with conditional generative neural networks</a></p>
                <p><strong>Paper Venue:</strong> Nature Communications</p>
                <p><strong>Paper TL;DR:</strong> The utility of the proposed conditional generative neural network for inverse design of 3d molecular structures is demonstrated by generating molecules with specified motifs or composition, discovering particularly stable molecules, and jointly targeting multiple electronic properties beyond the training regime.</p>
                <p><strong>Paper Abstract:</strong> The rational design of molecules with desired properties is a long-standing challenge in chemistry. Generative neural networks have emerged as a powerful approach to sample novel molecules from a learned distribution. Here, we propose a conditional generative neural network for 3d molecular structures with specified chemical and structural properties. This approach is agnostic to chemical bonding and enables targeted sampling of novel molecules from conditional distributions, even in domains where reference calculations are sparse. We demonstrate the utility of our method for inverse design by generating molecules with specified motifs or composition, discovering particularly stable molecules, and jointly targeting multiple electronic properties beyond the training regime. The targeted discovery of molecules with specific structural and chemical properties is an open challenge in computational chemistry. Here, the authors propose a conditional generative neural network for the inverse design of 3d molecular structures.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5287.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5287.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cG-SchNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>conditional G-SchNet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional, autoregressive generative neural network that assembles 3D molecular structures atom-by-atom conditioned on arbitrary combinations of structural and chemical properties, enabling targeted inverse design in 3D coordinate space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cG-SchNet (conditional G-SchNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive generative neural network (3D point-set generator using SchNet atom-wise features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Randomly sampled subsets (typically 55k structures for each run) from the QM9 dataset (DFT-relaxed small organic molecules with up to 9 heavy atoms). Validation on remaining QM9 structures; additional DFT relaxations performed with ORCA for generated structures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Inverse molecular design for small organic molecules (discovery of molecules with target electronic properties, motifs, compositions, low-energy isomers; applications include materials and drug-like candidate discovery and property optimization such as HOMO-LUMO gap, polarizability, stability).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional sampling: embed target properties (scalar or vector) into a conditional feature vector and autoregressively sample atom types and 3D positions by predicting type and distance distributions at each placement step; uses auxiliary tokens (origin, focus) and localized 3D candidate grids; trained via cross-entropy on type and discretized distance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>3D coordinates of atoms and atom types (3D point sets); converted to molecular graphs / SMILES via Open Babel for validity, uniqueness, and downstream analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (Open Babel valence/connectivity checks), uniqueness (canonical SMILES), novelty (not in training/test sets), recovery counts of known isomers, property matching to conditioning targets (distributions and MAE after DFT relaxation), RMSD before/after relaxation, numbers of novel constitutional/stereoisomers discovered, speed (generation ms/structure). Specific reported metrics: validity rates (~96–100% in targeted examples), RMSD medians ~0.17–0.38 Å, MAE of properties (e.g. HOMO-LUMO, polarizability) between generated predicted and relaxed values reported in Supplementary Table S2.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>QM9 (primary training/evaluation dataset); comparisons made to GDB enumeration and previously published biased G-SchNet generated sets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>cG-SchNet successfully generates 3D molecular structures conditioned on fingerprints, atomic composition, and electronic properties. It produced molecules with high fingerprint Tanimoto similarity to unseen targets, generated C7N1O1H11 isomers with HOMO-LUMO gaps outside the range seen during training, and discovered many low-energy C7O2H10 isomers despite excluding that composition from training (recovered 169 of the 200 lowest-energy test isomers and produced hundreds of novel constitutional and stereoisomers). When jointly targeting low HOMO-LUMO gap and low relative atomic energy, cG-SchNet found substantially more low-energy / low-gap unseen molecules than a previous biased G-SchNet baseline, while using fewer training examples from the target region. Generation is fast (≈9 ms per structure on an NVIDIA A100 in batches), whereas DFT relaxation is the bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to biased (fine-tuned) G-SchNet, cG-SchNet (conditioning architecture) generalizes better to sparsely sampled target regions and finds more valid unseen molecules in the low-energy, low-gap domain while using less target-specific training data. Compared to exhaustive graph enumeration + DFT (e.g., GDB/GDB→QM9 pipeline), cG-SchNet proposes far fewer candidate structures to relax while recovering a large fraction of target-region molecules and additionally finds valid molecules missed by enumeration. It leverages conditioning to combine information across compositions rather than requiring target-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designed and evaluated on small organic molecules (QM9); scalability to larger molecules/materials and more atom types requires architectural changes (e.g., cutoffs to limit considered atoms, long-range interaction corrections). Generation relies on a discrete 3D candidate grid and localized focus token; maximum-atom limit (35) in current implementation. Some downstream steps required for evaluation (bond assignment via Open Babel, DFT relaxation) can be error-prone or costly; Open Babel bond-order assignment needs custom heuristics for certain aromatic N/C motifs. Conditional targets may be incompatible (cannot all be fulfilled simultaneously), and further work is required to handle trade-offs or Pareto-front sampling. Model size and pretraining details (parameters) are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inverse design of 3d molecular structures with conditional generative neural networks', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5287.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5287.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-SchNet (biased G-SchNet baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-SchNet (unconditional / biased G-SchNet from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive deep neural network that generates 3D molecules by placing atoms sequentially (previous work); in this paper a biased/fine-tuned variant from prior work is used as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Symmetryadapted generation of 3d point sets for the targeted discovery of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>G-SchNet (and a biased, fine-tuned variant used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive generative neural network (3D point-set generator using SchNet features)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Previously trained on QM9; biased variant fine-tuned on a small subset of QM9 containing molecules with small HOMO-LUMO gap (~3.8k molecules in QM9, fine-tuned on ~3.8k or subset; in current comparison the authors used a previously published generated set).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Generation of small organic molecules in 3D, including previous targeted sampling for low HOMO-LUMO gap molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Unconditional autoregressive atom-by-atom generation; biased variant obtained by fine-tuning on a subset of training data enriched for a target (e.g., small HOMO-LUMO gap).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>3D coordinates and atom types (converted to graphs/SMILES for validation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used previous generated set filtered by HOMO-LUMO gap ≤ 4.5 eV; compared on counts of unique, unseen valid molecules, distributions of relative atomic energy vs HOMO-LUMO gap, atom/bond/ring statistics of generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>QM9; previously published generated molecule set from biased G-SchNet (available at quantum-machine.org datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Biased G-SchNet (fine-tuned on low-gap molecules) can generate low-gap molecules but tends to sample less stable motifs and more chemically strained small cycles and produces fewer low-energy / low-gap unseen molecules in the target domain than cG-SchNet when both energy and gap are jointly targeted, despite biased G-SchNet being fine-tuned on more target examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Fine-tuning (biasing) on a small target region requires many target-specific examples and can produce undesirable motifs (strained cycles, unstable motifs). cG-SchNet conditioning that leverages the whole dataset across conditions outperforms the biased fine-tuning approach for multi-property targeting and data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning requires a sufficient amount of target-region examples to be effective; biased G-SchNet can sample undesirable motifs (e.g., highly strained small rings) and is less flexible for multi-condition sampling without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inverse design of 3d molecular structures with conditional generative neural networks', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5287.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5287.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SchNet (representation network)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SchNet - A deep learning architecture for molecules and materials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-filter convolutional neural network used to extract rotation- and translation-invariant atom-wise features; employed inside cG-SchNet to represent partial molecules during generation and as pretrained property predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SchNet - A deep learning architecture for molecules and materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SchNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>continuous-filter convolutional neural network (atomistic deep learning / message-passing style representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained SchNet models used for property prediction were trained on QM9; SchNet representation used inside cG-SchNet is trained end-to-end as part of the generative model on sampled QM9 subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Atomistic feature extraction and property prediction (HOMO-LUMO gap, polarizability, internal energy) for small organic molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative language model; used as the feature-extraction backbone inside an autoregressive generative pipeline (cG-SchNet) and as pretrained property predictors (SchNetPack).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Atom-wise latent feature vectors and property predictions; not used to output molecules directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported mean absolute errors (MAE) of pretrained SchNet property predictors used in this work: HOMO-LUMO gap MAE 0.074 eV, isotropic polarizability MAE 0.124 Bohr^3, internal energy MAE 0.012 eV (as stated in Methods).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>QM9 for training/evaluation of property predictors; SchNetPack implementation used.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SchNet provides the invariant atom-wise features that enable cG-SchNet to predict next-atom type and distance distributions; pretrained SchNet models were used to predict properties of generated, unrelaxed structures and showed low MAE on QM9-derived property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SchNet is a standard atomistic representation model and provides an appropriate equivariant/invariant feature basis for 3D generative modeling; used here instead of graph- or SMILES-based embeddings to capture 3D geometry directly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SchNet is designed for atomistic systems and its use inside cG-SchNet implies computational cost for larger systems; scaling to materials or many element types may require additional data or architectural adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inverse design of 3d molecular structures with conditional generative neural networks', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Symmetryadapted generation of 3d point sets for the targeted discovery of molecules <em>(Rating: 2)</em></li>
                <li>3D-Scaffold: A deep learning framework to generate 3d coordinates of drug-like molecules with desired scaffolds <em>(Rating: 2)</em></li>
                <li>Generating equilibrium molecules with deep neural networks <em>(Rating: 1)</em></li>
                <li>E (n) equivariant normalizing flows for molecule generation in 3d <em>(Rating: 1)</em></li>
                <li>Reinforcement learning for molecular design guided by quantum mechanics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5287",
    "paper_id": "paper-ca722c18eb6546d307f6c3a0c1efd064a53a6a29",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "cG-SchNet",
            "name_full": "conditional G-SchNet",
            "brief_description": "A conditional, autoregressive generative neural network that assembles 3D molecular structures atom-by-atom conditioned on arbitrary combinations of structural and chemical properties, enabling targeted inverse design in 3D coordinate space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "cG-SchNet (conditional G-SchNet)",
            "model_type": "autoregressive generative neural network (3D point-set generator using SchNet atom-wise features)",
            "model_size": null,
            "training_data": "Randomly sampled subsets (typically 55k structures for each run) from the QM9 dataset (DFT-relaxed small organic molecules with up to 9 heavy atoms). Validation on remaining QM9 structures; additional DFT relaxations performed with ORCA for generated structures.",
            "application_domain": "Inverse molecular design for small organic molecules (discovery of molecules with target electronic properties, motifs, compositions, low-energy isomers; applications include materials and drug-like candidate discovery and property optimization such as HOMO-LUMO gap, polarizability, stability).",
            "generation_method": "Conditional sampling: embed target properties (scalar or vector) into a conditional feature vector and autoregressively sample atom types and 3D positions by predicting type and distance distributions at each placement step; uses auxiliary tokens (origin, focus) and localized 3D candidate grids; trained via cross-entropy on type and discretized distance distributions.",
            "output_representation": "3D coordinates of atoms and atom types (3D point sets); converted to molecular graphs / SMILES via Open Babel for validity, uniqueness, and downstream analysis.",
            "evaluation_metrics": "Validity (Open Babel valence/connectivity checks), uniqueness (canonical SMILES), novelty (not in training/test sets), recovery counts of known isomers, property matching to conditioning targets (distributions and MAE after DFT relaxation), RMSD before/after relaxation, numbers of novel constitutional/stereoisomers discovered, speed (generation ms/structure). Specific reported metrics: validity rates (~96–100% in targeted examples), RMSD medians ~0.17–0.38 Å, MAE of properties (e.g. HOMO-LUMO, polarizability) between generated predicted and relaxed values reported in Supplementary Table S2.",
            "benchmarks_or_datasets": "QM9 (primary training/evaluation dataset); comparisons made to GDB enumeration and previously published biased G-SchNet generated sets.",
            "results_summary": "cG-SchNet successfully generates 3D molecular structures conditioned on fingerprints, atomic composition, and electronic properties. It produced molecules with high fingerprint Tanimoto similarity to unseen targets, generated C7N1O1H11 isomers with HOMO-LUMO gaps outside the range seen during training, and discovered many low-energy C7O2H10 isomers despite excluding that composition from training (recovered 169 of the 200 lowest-energy test isomers and produced hundreds of novel constitutional and stereoisomers). When jointly targeting low HOMO-LUMO gap and low relative atomic energy, cG-SchNet found substantially more low-energy / low-gap unseen molecules than a previous biased G-SchNet baseline, while using fewer training examples from the target region. Generation is fast (≈9 ms per structure on an NVIDIA A100 in batches), whereas DFT relaxation is the bottleneck.",
            "comparison_to_other_methods": "Compared to biased (fine-tuned) G-SchNet, cG-SchNet (conditioning architecture) generalizes better to sparsely sampled target regions and finds more valid unseen molecules in the low-energy, low-gap domain while using less target-specific training data. Compared to exhaustive graph enumeration + DFT (e.g., GDB/GDB→QM9 pipeline), cG-SchNet proposes far fewer candidate structures to relax while recovering a large fraction of target-region molecules and additionally finds valid molecules missed by enumeration. It leverages conditioning to combine information across compositions rather than requiring target-specific fine-tuning.",
            "limitations_or_challenges": "Designed and evaluated on small organic molecules (QM9); scalability to larger molecules/materials and more atom types requires architectural changes (e.g., cutoffs to limit considered atoms, long-range interaction corrections). Generation relies on a discrete 3D candidate grid and localized focus token; maximum-atom limit (35) in current implementation. Some downstream steps required for evaluation (bond assignment via Open Babel, DFT relaxation) can be error-prone or costly; Open Babel bond-order assignment needs custom heuristics for certain aromatic N/C motifs. Conditional targets may be incompatible (cannot all be fulfilled simultaneously), and further work is required to handle trade-offs or Pareto-front sampling. Model size and pretraining details (parameters) are not reported.",
            "uuid": "e5287.0",
            "source_info": {
                "paper_title": "Inverse design of 3d molecular structures with conditional generative neural networks",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "G-SchNet (biased G-SchNet baseline)",
            "name_full": "G-SchNet (unconditional / biased G-SchNet from prior work)",
            "brief_description": "An autoregressive deep neural network that generates 3D molecules by placing atoms sequentially (previous work); in this paper a biased/fine-tuned variant from prior work is used as a baseline for comparison.",
            "citation_title": "Symmetryadapted generation of 3d point sets for the targeted discovery of molecules",
            "mention_or_use": "use",
            "model_name": "G-SchNet (and a biased, fine-tuned variant used as baseline)",
            "model_type": "autoregressive generative neural network (3D point-set generator using SchNet features)",
            "model_size": null,
            "training_data": "Previously trained on QM9; biased variant fine-tuned on a small subset of QM9 containing molecules with small HOMO-LUMO gap (~3.8k molecules in QM9, fine-tuned on ~3.8k or subset; in current comparison the authors used a previously published generated set).",
            "application_domain": "Generation of small organic molecules in 3D, including previous targeted sampling for low HOMO-LUMO gap molecules.",
            "generation_method": "Unconditional autoregressive atom-by-atom generation; biased variant obtained by fine-tuning on a subset of training data enriched for a target (e.g., small HOMO-LUMO gap).",
            "output_representation": "3D coordinates and atom types (converted to graphs/SMILES for validation).",
            "evaluation_metrics": "Used previous generated set filtered by HOMO-LUMO gap ≤ 4.5 eV; compared on counts of unique, unseen valid molecules, distributions of relative atomic energy vs HOMO-LUMO gap, atom/bond/ring statistics of generated molecules.",
            "benchmarks_or_datasets": "QM9; previously published generated molecule set from biased G-SchNet (available at quantum-machine.org datasets).",
            "results_summary": "Biased G-SchNet (fine-tuned on low-gap molecules) can generate low-gap molecules but tends to sample less stable motifs and more chemically strained small cycles and produces fewer low-energy / low-gap unseen molecules in the target domain than cG-SchNet when both energy and gap are jointly targeted, despite biased G-SchNet being fine-tuned on more target examples.",
            "comparison_to_other_methods": "Fine-tuning (biasing) on a small target region requires many target-specific examples and can produce undesirable motifs (strained cycles, unstable motifs). cG-SchNet conditioning that leverages the whole dataset across conditions outperforms the biased fine-tuning approach for multi-property targeting and data efficiency.",
            "limitations_or_challenges": "Fine-tuning requires a sufficient amount of target-region examples to be effective; biased G-SchNet can sample undesirable motifs (e.g., highly strained small rings) and is less flexible for multi-condition sampling without retraining.",
            "uuid": "e5287.1",
            "source_info": {
                "paper_title": "Inverse design of 3d molecular structures with conditional generative neural networks",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "SchNet (representation network)",
            "name_full": "SchNet - A deep learning architecture for molecules and materials",
            "brief_description": "A continuous-filter convolutional neural network used to extract rotation- and translation-invariant atom-wise features; employed inside cG-SchNet to represent partial molecules during generation and as pretrained property predictors.",
            "citation_title": "SchNet - A deep learning architecture for molecules and materials",
            "mention_or_use": "use",
            "model_name": "SchNet",
            "model_type": "continuous-filter convolutional neural network (atomistic deep learning / message-passing style representation)",
            "model_size": null,
            "training_data": "Pretrained SchNet models used for property prediction were trained on QM9; SchNet representation used inside cG-SchNet is trained end-to-end as part of the generative model on sampled QM9 subsets.",
            "application_domain": "Atomistic feature extraction and property prediction (HOMO-LUMO gap, polarizability, internal energy) for small organic molecules.",
            "generation_method": "Not a generative language model; used as the feature-extraction backbone inside an autoregressive generative pipeline (cG-SchNet) and as pretrained property predictors (SchNetPack).",
            "output_representation": "Atom-wise latent feature vectors and property predictions; not used to output molecules directly.",
            "evaluation_metrics": "Reported mean absolute errors (MAE) of pretrained SchNet property predictors used in this work: HOMO-LUMO gap MAE 0.074 eV, isotropic polarizability MAE 0.124 Bohr^3, internal energy MAE 0.012 eV (as stated in Methods).",
            "benchmarks_or_datasets": "QM9 for training/evaluation of property predictors; SchNetPack implementation used.",
            "results_summary": "SchNet provides the invariant atom-wise features that enable cG-SchNet to predict next-atom type and distance distributions; pretrained SchNet models were used to predict properties of generated, unrelaxed structures and showed low MAE on QM9-derived property tasks.",
            "comparison_to_other_methods": "SchNet is a standard atomistic representation model and provides an appropriate equivariant/invariant feature basis for 3D generative modeling; used here instead of graph- or SMILES-based embeddings to capture 3D geometry directly.",
            "limitations_or_challenges": "SchNet is designed for atomistic systems and its use inside cG-SchNet implies computational cost for larger systems; scaling to materials or many element types may require additional data or architectural adjustments.",
            "uuid": "e5287.2",
            "source_info": {
                "paper_title": "Inverse design of 3d molecular structures with conditional generative neural networks",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Symmetryadapted generation of 3d point sets for the targeted discovery of molecules",
            "rating": 2
        },
        {
            "paper_title": "3D-Scaffold: A deep learning framework to generate 3d coordinates of drug-like molecules with desired scaffolds",
            "rating": 2
        },
        {
            "paper_title": "Generating equilibrium molecules with deep neural networks",
            "rating": 1
        },
        {
            "paper_title": "E (n) equivariant normalizing flows for molecule generation in 3d",
            "rating": 1
        },
        {
            "paper_title": "Reinforcement learning for molecular design guided by quantum mechanics",
            "rating": 1
        }
    ],
    "cost": 0.014211249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Inverse design of 3d molecular structures with conditional generative neural networks</h1>
<p>Niklas W. A. Gebauer, ${ }^{1,2,3, *}$ Michael Gastegger, ${ }^{1,3}$ Stefaan S. P. Hessmann, ${ }^{1,2}$ Klaus-Robert Müller, ${ }^{1,2,4,5}$ and Kristof T. Schütt ${ }^{1,2, \dagger}$<br>${ }^{1}$ Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany<br>${ }^{2}$ Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany<br>${ }^{3}$ BASLEARN - TU Berlin/BASF Joint Lab for Machine Learning, Technische Universität Berlin, 10587 Berlin, Germany<br>${ }^{4}$ Department of Artificial Intelligence, Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea<br>${ }^{5}$ Max-Planck-Institut für Informatik, 66123 Saarbrücken, Germany.</p>
<p>The rational design of molecules with desired properties is a long-standing challenge in chemistry. Generative neural networks have emerged as a powerful approach to sample novel molecules from a learned distribution. Here, we propose a conditional generative neural network for 3d molecular structures with specified chemical and structural properties. This approach is agnostic to chemical bonding and enables targeted sampling of novel molecules from conditional distributions, even in domains where reference calculations are sparse. We demonstrate the utility of our method for inverse design by generating molecules with specified motifs or composition, discovering particularly stable molecules, and jointly targeting multiple electronic properties beyond the training regime.</p>
<h2>I. INTRODUCTION</h2>
<p>Identifying chemical compounds with particular properties is a critical task in many applications, ranging from drug design [1-3] over catalysis [4] to energy materials [5-8]. As an exhaustive exploration of the vast chemical compound space is infeasible, progress in these areas can benefit substantially from inverse design methods. In recent years, machine learning (ML) has been used to accelerate the exploration of chemical compound space [9-15]. A plethora of methods accurately predict chemical properties and potential energy surfaces of 3d structures at low computational cost [16-27]. Here, the number of reference calculations required for training ML models depends on the size of the domain to be explored. Thus, naive exploration schemes may still require a prohibitive number of electronic structure calculations. Instead, chemical space has to be be navigated in a guided way with fast and accurate methods to distill promising molecules.</p>
<p>This gives rise to the idea of inverse molecular design [28], where the structure-property relationship is reversed. Here, the challenge is to directly construct molecular structures corresponding to a given a set of properties. Generative ML models have recently gained traction as a powerful, data-driven approach to inverse design as they enable sampling from a learned distribution of molecular configurations [29]. By appropriately restricting the distributions, they allow to obtain sets of candidate structures with desirable characteristics for further evaluation. These methods typically represent molecules as graphs or SMILES strings [30, 31], which lack information about the three-dimensional structure of a molecule. Therefore, the same molecular graph can rep-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>resent various spatial conformations that differ in their respective properties, e.g. due to intramolecular interactions (hydrogen bonds, long-range interactions) or different orientations of structural motifs (rotamers, stereoisomers). Beyond that, connectivity-based representations are problematic in chemical systems where bonding is ambiguous, e.g. in transition metal complexes, conjugated systems or metals. Relying on these abstract representations is ultimately a limiting factor when exploring chemical space.</p>
<p>Recently, generative models that enable sampling of 3d molecular configurations have been proposed. This includes specifically designed approaches to translate given molecular graphs to 3d conformations [32-38], map from coarse-grained to fine-grained structures [39], sample unbiased equilibrium configurations of a given system [40, 41], or focus on protein folding [42-46]. In contrast, other models aim at sampling directly from distributions of 3d molecules with arbitrary composition [4756], making them suitable for general inverse design settings. These models need to be biased towards structures with properties of interest, e.g. using reinforcement learning [51, 52, 56], fine-tuning on a biased data set [48], or other heuristics [54].</p>
<p>Some of us have previously proposed G-SchNet [48], an auto-regressive deep neural network that generates diverse, small organic molecules by placing atom after atom in Euclidean space. It has been applied in the 3D-Scaffold framework to build molecules around a functional group associated with properties of interest in order to discover novel drug candidates [54]. Such an approach requires prior knowledge about the relationship between functional groups and target properties and might prevent the model from unfolding its potential by limiting sampling to very specific molecules. G-SchNet has been biased by fine-tuning on a fraction of the training data set containing all molecules with small HOMO-LUMO gap [48]. For this, a sufficient amount of training examples in the target space is required. However, the most interesting</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1. Molecule generation with cG-SchNet. a: Factorization of the conditional joint probability of atom positions and types into a chain of probabilities for placing single atoms one after another. b: Results of sampling molecules from target-dependent conditional probability distributions. Distributions of the isotropic polarizability of training structures (orange) and five sets of molecules generated by the same cG-SchNet model (blue curves) conditioned on five different isotropic polarizability target values (color-matching dots above the x-axis). The generated molecule closest to the corresponding target value and not contained in the training data (unseen) is shown above each curve. c: Schematic depiction of the atom placement loop. For visualization purposes, we show a planar molecule and a 2d slice of the actual 3d grid distributions in steps 4, 5, and 6.</p>
<p>regions for exploration are often those where reference calculations are sparse.</p>
<p>In this work, we propose conditional G-SchNet (cG-SchNet), a conditional generative neural network for the inverse design of molecules. Building on G-SchNet, the model learns conditional distributions depending on structural or chemical properties allowing us to sample corresponding 3d molecular structures. Our architecture is designed to generate molecules of arbitrary size and does not require the specification of a target composition. Consequently, it learns the relationship between the composition of molecules and their physical properties in order to sample candidates exhibiting given target properties, e.g., preferring smaller structures when targeting small polarizabilities. Previously proposed methods have been biased towards one particular set of target property values at a time by adjusting the training objective or data [48, 51]. In contrast, our conditional approach permits searching for molecules with any desirable set of target property values after training is completed. It is able to jointly target multiple properties without the need to retrain or otherwise indirectly constrain the sampling process. This provides the foundation for the model to leverage the full information of the training data resulting in increased generalization and data efficiency. We demonstrate that cG-SchNet enables the exploration of sparsely populated regions that are hardly accessible with unconditional models. To this end, we conduct extensive experiments with diverse conditioning targets including chemical properties, atomic compositions, and molecular fingerprints. In this way, we generate novel molecules with predefined structural motifs, isomers of a given composition that exhibit specific chemical properties, and novel configurations that jointly optimize HOMO-LUMO gap and energy. This demonstrates that our model enables flexible, guided exploration of chemical compound space.</p>
<h2>II. RESULTS</h2>
<h3>Targeted 3d molecule generation with cG-SchNet</h3>
<p>We represent molecules as tuples of atom positions $\mathbf{R}<em 1="1">{\leq n}=\left(\mathbf{r}</em>}, \ldots, \mathbf{r<em i="i">{n}\right)$ with $\mathbf{r}</em>} \in \mathbb{R}^3$ and corresponding atom types $\mathbf{Z<em 1="1">{\leq n}=\left(Z</em>$. cG-SchNet assembles these structures from sequences of atoms that are placed step by step in order to build the molecule in an autoregressive manner, where the placement of the next atom depends on the preceding atoms (Fig. 1a and c). In contrast to G-SchNet [48], which learns an unconditional distribution over molecules, cG-SchNet samples from target-dependent conditional probability distributions of 3d molecular structures (Fig. 1b).}, \ldots, Z_n\right)$ with $Z_i \in \mathbb{N</p>
<p>Given a tuple of $k$ conditions $\boldsymbol{\Lambda} = (\lambda_1, \ldots, \lambda_k)$, cG-SchNet learns a factorization of the conditional distribution of molecules, i.e. the joint distribution of atom positions and atom types conditioned on the target properties:</p>
<p>$$p(\mathbf{R}<em _leq="\leq" n="n">{\leq n}, \mathbf{Z}</em>}|\boldsymbol{\Lambda}) = \prod_{i=1}^{n} p(\mathbf{r<em _leq="\leq" i-1="i-1">i, Z_i | \mathbf{R}</em>$$}, \mathbf{Z}_{\leq i-1}, \boldsymbol{\Lambda}). \tag{1</p>
<p>In fact, we can split up the joint probability of the next type and the next position into the probability of the next type and the probability of the next position given the associated next type:</p>
<p>$$p\left(\mathbf{r}<em _leq="\leq" i-1="i-1">i, Z_i \right) \mathbf{R}</em>}, \mathbf{Z<em _leq="\leq" i-1="i-1">{\leq i-1}, \boldsymbol{\Lambda} = \left( p(Z_i | \mathbf{R}</em>}, \mathbf{Z<em _leq="\leq" i-1="i-1">{\leq i-1}, \boldsymbol{\Lambda}) p(\mathbf{r}_i | \mathbf{R}</em>) \right).$$}, \mathbf{Z}_{\leq i}, \boldsymbol{\Lambda</p>
<p>This allows to predict the next type before the next position. We approximate the distribution over the absolute position from distributions over distances to already placed atoms</p>
<p>$$p(\mathbf{r}<em _leq="\leq" i-1="i-1">i | \mathbf{R}</em>}, \mathbf{Z<em j="1">{\leq i}, \boldsymbol{\Lambda}) = \frac{1}{\alpha} \prod</em>}^{i-1} p(r_{ij} | \mathbf{R<em _leq="\leq" i="i">{\leq i-1}, \mathbf{Z}</em>$$}, \boldsymbol{\Lambda}) \tag{3</p>
<p>which guarantees that it is equivariant with respect to translation and rotation of the input. Here $\alpha$ is the normalization constant and $r_{ij} = ||\mathbf{r}_i - \mathbf{r}_j||$ is the distance between the new atom $i$ and a previously placed atom $j$. This approximation has previously been shown to accurately reproduce a distribution of molecular structures [48].</p>
<p>Fig. 2 shows a schematic depiction of the cG-SchNet architecture. The conditions $\lambda_1, \ldots, \lambda_k$ are each embedded into a latent vector space and concatenated, followed by a fully connected layer. In principle, any combination of properties can be used as conditions with our architecture with a suitable embedding network. In this work, we use three scalar-valued electronic properties such as the isotropic polarizability, vector-valued molecular fingerprints, and the atomic composition of molecules. Vector-valued properties are directly processed by the network while scalar-valued targets are first expanded in a Gaussian basis. To target an atomic composition, learnable</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2. Schematic depiction of the cG-SchNet architecture with inputs and outputs. "⊕" represents concatenation and "⊕" represents the Hadamard product. Left: Atom-wise feature vectors representing an unfinished molecule are extracted with SchNet [57] and conditions are individually embedded and then concatenated to extract the conditional features vector. The exact embedding depends on the type of the condition (e.g. scalar or vector-valued). Middle: The distribution for the type of the next atom is predicted from the extracted feature vectors. Right: Based on the extracted feature vectors and the sampled type of the next atom, distributions for the pairwise distances between the next atom and every atom/token in the unfinished molecule are predicted. See Methods for details on the building blocks.</p>
<p>atom type embeddings are weighted by occurrence. The embedding procedure is described in detail in the Methods section.</p>
<p>In order to localize the atom placement and stabilize the generation procedure, cG-SchNet makes use of the same two auxiliary tokens as in the unconditional setting, namely the origin and the focus token [48]. Auxiliary tokens are treated like regular atoms by the model, i.e. they possess positions and token types, which are contained in the tuples of atom positions and atom types serving as input at each step. The origin token marks the center of mass of molecules and allows the architecture to steer the growth from inside to outside. The focus token localizes the prediction of the next position in order to assure scalability and allows to break symmetries of partial structures. This avoids artifacts in the reconstruction of the positional distribution (Eq. 3) as reported by Gebauer et al. [48]. At each step, the focus token is randomly assigned to a previously placed atom. The position of the next atom is required to be close to this focus. In this way, we can use a small grid localized on the focus that does not grow with the number of atoms when predicting the distribution of the next position.</p>
<p>We train cG-SchNet on a set of molecular structures, where the values of properties used as conditions are</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>FIG. 3. Targeted exploration of chemical space with cG-SchNet. a: Generation of molecules with desired motifs by conditioning cG-SchNet on simple path-based fingerprints. First column: Four different target fingerprints of structures from the test set. For each, we conditionally sample 20k molecules with cG-SchNet. Second column: Average Tanimoto similarity of the respective target to training structures (brown) and to generated molecules without duplicates (blue) and with duplicates (grey). The amount of generated structures is noted next to the dots. Third column: Most similar training molecule. Fourth column: Three generated unseen examples with high similarity to the target. The Tanimoto similarity to the target structure is noted to the bottom-right of depicted molecules. b: Generation of C7N1O1H11 isomers with HOMO-LUMO gap targets outside the training data range by conditioning cG-SchNet on atomic composition and HOMO-LUMO gap. The training data set of 55k QM9 molecules is restricted to not contain any C7N1O1H11 isomers with gap &lt; 6 eV or gap &gt; 8 eV. The graph shows the distribution of the gap for the C7N1O1H11 isomers in QM9 (brown), the isomers in the restricted training data set (orange), and the two sets of isomers generated with cG-SchNet (blue curves) when targeting the composition C7N1O1H11 and two gap values outside the training data range (color-matching dots on the x-axis). For each target value, the two generated isomers closest to it are depicted.</p>
<p>known for each molecule. Given the conditions and the partial molecular structure at each step, cG-SchNet predicts a discrete distribution for the type of the next atom. As part of this, a stop type may be predicted that allows the model to control the termination of the sampling procedure and therefore generate molecules with variable size and composition. After sampling a type, cG-SchNet predicts distributions for the distance between the atom to be placed and each preceding atom and auxiliary token. The schematic depiction of the atom placement loop in Fig. 1c includes the auxiliary tokens, the model predictions, and the reconstruction of the localized 3d grid distribution. During training, we minimize the cross-entropy loss between the predicted distributions and the ground-truth distributions known from the reference calculations. For further details on the model architecture and training procedure, refer to the Methods section.</p>
<h3>Generating molecules with specified motifs</h3>
<p>In many applications, it is advantageous for molecules to possess specific functional groups or structural motifs. These can be correlated with desirable chemical properties, e.g., polar groups that increase solubility, or with improved synthetic accessibility. In order to sample molecules with specific motifs, we condition cG-SchNet on a path-based, 1024 bits long fingerprint that checks molecular graphs for all linear segments of up to seven atoms [58] (Supplementary Methods X C). The model is trained on a randomly selected subset of 55k molecules from the QM9 dataset consisting of ~134k organic molecules with up to nine heavy atoms from carbon, nitrogen, oxygen, and fluorine [59–61]. We condition the sampling on fingerprints of unseen molecules, i.e., structures not used during training. Fig. 3a shows results for four examples. We observe that the generated molecules have higher similarity with the target fingerprints than the training data. Furthermore, structures with high target similarity are also sampled with higher probability, as can be seen from the increased similarity score of generated duplicates. In the last column of Fig. 3a, we show sampled molecules with high similarity to each target and see that in each case various structures with perfectly matching fingerprint were found. For reference, we also show the most similar molecule in the training set. Overall, we see that the conditional sampling with cG-SchNet is sensitive to the target fingerprint and allows for generation of molecules with desired structural motifs. Although there are no molecules with</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIG. 4. Discovery of low-energy isomers for an unseen composition. We sample C7O2H10 isomers with cG-SchNet conditioned on atomic composition and relative atomic energy (see text for details), where the training data set was restricted to contain no C7O2H10 conformations. a: The distribution of the relative atomic energy for C7O2H10 isomers in the test set (orange) and for three sets of isomers generated with cG-SchNet (blue curves) when targeting the composition C7O2H10 and three different relative atomic energy values as marked with color-matching dots on the x-axis. The generated isomer closest to the respective target is depicted above each curve. b: The absolute number of C7O2H10 isomers in the test set (red dotted line) for increasing relative atomic energy thresholds. The black solid line shows how many of these were generated by cG-SchNet (target energy -0.1 eV). c: Bar plot of the absolute number of C7O2H10 isomers with relative atomic energy ≤ 0.05 eV in the test set (orange) and generated by cG-SchNet (target energy -0.1 eV, purple). The bar for generated molecules is divided into isomers that can be found in the test set (unseen isomers), isomers that have different stereochemistry but share the same bonding pattern as test set structures (novel stereoisomers), and novel constitutional isomers that are not in QM9 (novel isomers). d: Relaxed example low-energy isomers generated by cG-SchNet (target energy -0.1 eV, blue dots) and structures from the test set (orange dots) along with their relative atomic energy.</p>
<p>The same fingerprint in the training data for three of the four fingerprint targets, the ML model successfully generates perfectly matching molecules, demonstrating its ability to generalize and explore unseen regions of chemical compound space.</p>
<h3>Generalization of condition-structure relationship across compositions</h3>
<p>For inverse design tasks, integrating information gained from different structures and properties is vital to obtain previously unknown candidates with desired properties. In this experiment, we target C7N1O1H11 isomers with HOMO-LUMO gap values outside the range observed during training. To this end, the model has to learn from other compositions how molecules with particularly high or low HOMO-LUMO gap are structured, and transfer this knowledge to the target composition. There are 5859 C7N1O1H11 isomers in QM9, where 997 have a HOMO-LUMO gap smaller than 6 eV, 1612 have a HOMO-LUMO gap larger than 8 eV, and 3250 lie in between these two values. We restrict the training data consisting of 55k molecules from QM9 to contain no C7N1O1H11 isomers with HOMO-LUMO gap values outside the intermediate range (Fig. 3b). Thus, the model can only learn to generate molecules with gaps outside this range from compositions other than C7N1O1H11.</p>
<p>Fig. 3b shows examples of generated C7N1O1H11 isomers for two target values as well as the respective HOMO-LUMO gap distributions. In both cases, the majority of generated isomers exhibit gap values close to the respective target (± 1 eV), i.e., outside of the range observed for these isomers by the model during training. This demonstrates that cG-SchNet is able to transfer knowledge about the relationship between structural patterns and HOMO-LUMO gaps learned from molecules of other compositions to generate unseen C7N1O1H11 isomers with outlying gap values upon request.</p>
<h3>Discovery of low-energy conformations</h3>
<p>The ability to sample molecules that exhibit property values which are missing in the training data is a prerequisite for the targeted exploration of chemical space. A generative model needs to fill the sparsely sampled regions of the space, effectively enhancing the available data with novel structures that show property values of interest. We study this by training cG-SchNet on a randomly sampled set of 55k QM9 molecules and query our model to sample low-energy C7O2H10 isomers – the most common composition in QM9. We exclude these isomers from the training data, i.e., our model has to generalize beyond the seen compositions. The identification of low-energy conformations is desirable in many practical</p>
<p>applications, since they tend to be more stable. However, the energy of molecules is largely determined by their size and composition. Since we are mainly interested in the energy contribution of the spatial arrangement sampled by the model, we require a normalized energy value. To this end, we define the relative atomic energy, which indicates whether the internal energy per atom is relatively high or low compared to other molecules of the same composition in the data set (see Supplementary Methods X B for details). Negative values indicate comparatively low energy, and thus higher stability than the average structure of this composition. Note that a similarly normalized energy has been defined by Zubatyuk et al. [62] for their neural network potential. Using the relative atomic energy allows cG-SchNet to learn the influence of the spatial arrangement of atoms on the energy and transfer this knowledge to the unseen target composition. Examples of generated $C_{7} O_{2} H_{10}$ isomers with low, intermediate, and high relative atomic energy are shown in Fig. 4a. We observe that conformations with highly strained, small rings exhibit increased relative atomic energy values.</p>
<p>Fig. 4a shows that the trained model generalizes from the training data to sample $C_{7} O_{2} H_{10}$ isomers capturing the whole range of relative atomic energies exhibited by the QM9 test structures. We focus on stable, low-energy isomers for our analysis in the following. We sample 100k molecules with the trained cG-SchNet conditioned on the composition $C_{7} O_{2} H_{10}$ and a relative atomic energy value of -0.1 eV , i.e. close to the lowest energies occurring for these isomers in QM9. The generated molecules are filtered for valid and unique $C_{7} O_{2} H_{10}$ isomers, relaxed using density functional theory (DFT), and then matched with the test data structures. 169 of the 200 isomers with the lowest relative atomic energy in the test set have been recovered by the model as well as $67 \%$ of the 1 k isomers with relative atomic energy lower than -0.05 eV (Fig. 4b). Beyond that, cG-SchNet has generated 416 novel isomers as well as 243 novel stereoisomers that share the same bonding pattern as a test structure but show different stereochemistry (Fig. 4c). We found $32 \%$ more unique $C_{7} O_{2} H_{10}$ isomers with relative atomic energy lower than -0.05 eV with our model than already contained in QM9. Example isomers are depicted in Fig. 4d. For reference, we show additional, randomly selected generated novel isomers along with their most similar counterparts from QM9 in Supplementary Fig. S1 and depict how atoms in these structures moved during relaxation in Supplementary Fig. S4. Furthermore, we examine different conformations found for the five most often generated isomers in Supplementary Fig. S3.</p>
<p>The generated molecules include structures and motifs that are sparse or not included in the QM9 benchmark data set, which has previously been reported to suffer from decreased chemical diversity compared to real world data sets [63]. For instance, there are no $C_{7} O_{2} H_{10}$ isomers with carboxylic acid groups in QM9, while twelve of the generated novel low-energy isomers possess this functional group (e.g., Fig. 4d, top right and Supplementary</p>
<p>Fig. S2). Carboxylic acid groups are a common motif of organic compounds and feature prominently in fats and amino acids. While they are only contained in a few hundred molecules in QM9, cG-SchNet has learned to transfer this group to molecules of the targeted composition. Moreover, the model has discovered several acyclic $C_{7} O_{2} H_{10}$ isomers exhibiting a significantly lower relative atomic energy than those in QM9 (examples in Fig. 4d, bottom row). As cG-SchNet generalizes beyond the chemical diversity of QM9, this demonstrates that it can be employed to systematically enhance a database of molecular structures.</p>
<h2>Targeting multiple properties: Discovery of low-energy structures with small HOMO-LUMO gap</h2>
<p>For most applications, the search for suitable molecules is guided by multiple properties of interest. Therefore, a method for exploration needs to allow for the specification of several conditions at the same time. Here we demonstrate this ability by targeting HOMO-LUMO gap as well as relative atomic energy, i.e. two complex electronic properties at the same time. A particular challenging task is to find molecules with extreme property values, as those are often located at the sparsely populated borders of the training distribution. In previous work, we have biased an unconditioned G-SchNet in order to sample molecules with small HOMO-LUMO gap [48]. The model was fine-tuned with all $\sim 3.8 \mathrm{k}$ available molecules from QM9 with HOMO-LUMO gap smaller than 4.5 eV , a small fraction of the whole QM9 data set with $\sim 130 \mathrm{k}$ molecules. In the following, we demonstrate that improved results can be achieved with the cG-SchNet architecture while using less training samples from the target region. We further condition the sampling to particularly stable, low-energy conformations. In a fine-tuning approach, this would limit the training data to only a few molecules that are both stable and exhibit small gaps. In contrast, the conditioned model is able to learn also from reference calculations where only one of the desired properties is present.</p>
<p>We condition cG-SchNet on the HOMO-LUMO gap as well as the relative atomic energy and train it on 55 k randomly selected QM9 molecules, where only $\sim 1.6 \mathrm{k}$ of the $\sim 3.8 \mathrm{k}$ molecules with HOMO-LUMO gap smaller than 4.5 eV are contained. Then, we sample the same number of molecules as for the biased model [48] (20k) with the trained cG-SchNet using a HOMO-LUMO gap value of 4.0 eV and relative atomic energy of -0.2 eV as conditions. The generated conformations are filtered for valid and unique molecules, relaxed using DFT, and then matched with the training data structures.</p>
<p>Fig. 5 compares the sets of generated, unique, unseen molecules with HOMO-LUMO gap smaller than 4.5 eV obtained for the cG-SchNet and biased G-SchNet. For biased G-SchNet, we use the previously published [48] data set of generated molecules with low HOMO-LUMO</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>FIG. 5. Discovery of low-energy structures with small HOMO-LUMO gap. We compare cG-SchNet to the previous, biased G-SchNet approach [48]. <strong>a</strong>: The joint distributions of relative atomic energy and HOMO-LUMO gap for QM9 (left) and for unique, unseen molecules with gap ≤ 4.5 eV generated with cG-SchNet (middle) and with biased G-SchNet (right). Biased G-SchNet was fine-tuned on all molecules in QM9 below a gap threshold of 4.5 eV (red, dotted line). The conditions used for generation with cG-SchNet are marked with a blue cross. The depicted molecules are generated examples with a gap of 4 eV and different relative atomic energy values (black, dotted lines). More examples as well as the distributions close to the conditioning target for cG-SchNet and the training data can be found in Supplementary Fig. S5. <strong>b</strong>: The absolute number of unique, unseen molecules with gap ≤ 4.5 eV generated by cG-SchNet (black) and biased G-SchNet (red) for increasing relative atomic energy thresholds. For reference, we also show the amount of structures with low gap included in the training set of cG-SchNet (blue dotted line). <strong>c</strong>: The average number of atoms of different types (left), bonds of different orders (middle), and rings of different sizes (right) in unique, unseen molecules with gap ≤ 4.5 eV generated by each model.</p>
<p>gap and remove all structures with HOMO-LUMO gap larger than 4.5 eV. Since the energy range has not been restricted for the biased G-SchNet, it samples structures that capture the whole space spanned by the training data, i.e. also less stable molecules with higher relative atomic energy. The molecules generated with cG-SchNet, in contrast, are mostly structures with low relative atomic energy (Fig. 5a). Considering the total amount of unseen molecules with small gaps found by both models, we observe that cG-SchNet samples a significantly larger number of structures from the low-energy domain than the biased G-SchNet. It similarly surpasses the number of molecules from this domain in the training set, showcasing an excellent generalization performance (see Fig. 5b).</p>
<p>The statistics about the average atom, bond, and ring count of generated molecules depicted in Fig. 5c reveal further insights about the structural traits and differences of molecules with low HOMO-LUMO gap in the two sets. The molecules found with cG-SchNet contain more double bonds and a larger number of rings, mainly consisting of five or six atoms. This indicates a prevalence of aromatic rings and conjugated systems with alternating double and single bonds, which are important motifs in organic semiconductors. The same patterns can be found for molecules from biased G-SchNet, however, there is an increased number of nitrogen and oxygen atoms stemming from less stable motifs such as rings dominated by nitrogen. An example of this is the molecule with the highest energy depicted in Fig. 5a). Furthermore, the molecules of biased G-SchNet tend to contain highly strained small cycles of three or four atoms. cG-SchNet successfully averts these undesirable motifs when sampling molecules with a low relative atomic energy target.</p>
<p>We conclude that cG-SchNet has learned to build stable molecules with low HOMO-LUMO gap even though it has seen less than half of the structures that the biased model was fine-tuned on. More importantly, the training data contains only very few (~200) structures close to the target conditions at the border of the QM9 distribution, i.e. with HOMO-LUMO gap smaller than 4.5 eV and relative atomic energy smaller than -0.1 eV. However, our model is able to leverage information even</p>
<p>from structures where one of the properties is outside the targeted range. Consequently, it is able to sample a significantly higher number of unseen molecules from the target domain than there are structures in the training data that fulfill both targets. In this way, multiple properties can be targeted at once in order to efficiently explore chemical compound space.</p>
<p>The efficiency of cG-SchNet in finding molecular structures close to the target conditions is particularly evident compared to exhaustive enumeration of graphs with subsequent relaxation using DFT. In both cases, the relaxation required to obtain equilibrium coordinates and the physical properties is the computational bottleneck and takes more than 15 minutes per structure for the molecules generated in this experiment. Furthermore, the calculation of the internal energy at zero Kelvin (U0) requires additional 40 minutes per molecule. In contrast, the generation with cG-SchNet takes only 9 milliseconds per structure on a Nvidia A100 GPU when sampling in batches of 1250. The training time of about 40 hours is negligible, as it corresponds to the relaxation and calculation of U0 of only 44 structures. Thus, the efficiency is determined by the number of molecules that need to be relaxed for each method. The QM9 data set was assembled by relaxing structures from the GDB enumeration [61] of graphs for small organic compounds. Of the $\sim 78 \mathrm{k}$ molecules that we did not use for training, 354 molecules are close to the target region. Relaxing only the 5283 structures proposed by cG-SchNet, i.e. less than $10 \%$ of the computations performed by screening all graphs, we can already recover $46 \%$ of these structures. Additionally, the model has unveiled valid molecules close to the target that are not contained in the data set. More than 380 of these are larger than QM9 structures and thus not covered. However, 253 smaller structures were missed by the enumeration method. This is, again, in line with findings by Glavatskikh et al. [63] that even for these small compounds the graph-based sampling does not cover all structures of interest. Furthermore, the conditional model is not restricted to the space of low energy / low gap molecules, but can also sample low energy / high gap structures or any other combination of interest. Thus, the efficiency of the generative model becomes even more pronounced when there are multiple sets of desirable target values. Fig. 1b depicts an example where cG-SchNet has been trained on the isotropic polarizability as condition. Here, the same model is employed to sample molecules for five different target values. Again, cG-SchNet is able to generalize to isotropic polarizabilities beyond the values present in the training data.</p>
<h2>III. DISCUSSION</h2>
<p>cG-SchNet enables the targeted discovery of 3d molecular structures conditioned on arbitrary combinations of multiple structural and chemical properties. The neural network captures global and local symmetries of molec-
ular structures by design, enabling it to learn complex relationships between chemical properties and 3d structures. This makes it possible to generalize to unseen conditions and structures, as we have thoroughly evaluated in a line of experiments where we target property values not included in the training data. In contrast to previous approaches, the model does not require targetspecific biasing procedures. Instead, the explicit conditioning enables cG-SchNet to learn efficiently from all available reference calculations. Desirable values of multiple properties can be targeted simultaneously to sample from specific conditional distributions. In this way, cG-SchNet generates novel 3d candidate molecules that exhibit the target properties with high probability and thus are perfectly suited for further filtering and evaluation using ML force fields.</p>
<p>Further work is required to apply the cG-SchNet architecture to the exploration of significantly larger systems and a more diverse set of atom types. Although an unconditional G-SchNet has been trained on druglike molecules with $50+$ atoms in the 3D-Scaffold framework [54], adjustments will be necessary to ensure scalability to materials. In the current implementation, we employ all preceding atoms to predict the type and reconstruct the positional distribution of the next atom. Here, a cutoff or other heuristics to limit the number of considered atoms will need to be introduced, together with corrections for long-range interactions. While the small organic compounds considered in this work are well represented by QM9, the model might benefit from enhancing the training data using representative building blocks such as "amons" [64] or other fragmentation methods [65, 66]. This becomes increasingly important when tackling larger molecules where reference data is hard to obtain. Furthermore, additional adaptations are necessary to explore systems with periodic boundary conditions. In cases where not all targeted properties can be fulfilled simultaneously, finding suitable molecules becomes harder, if not impossible. Therefore, another important extension is to explicitly define a trade-off between multiple conditions or to sample along a Pareto front.</p>
<p>We have applied cG-SchNet to sample particularly stable, low-energy $C_{7} O_{2} H_{10}$ isomers. In this process, we have discovered molecules and motifs that are absent from the QM9 database, such as isomers with carboxylic acid groups. Furthermore, we have sampled more than 800 low-energy molecules with HOMO-LUMO gaps smaller than 4.5 eV from a domain that is only sparsely represented in the training data. Although the exploration of such small molecules with exhaustive sampling of molecular graphs and subsequent evaluation with DFT is computationally feasible, our model considerably accelerates the process by providing reasonable candidate structures. cG-SchNet thus also enables the dataefficient, systematic improvement of chemical databases, which is particularly valuable considering the computational cost and unfavourable scaling of electronic struc-</p>
<p>ture calculations. This paves the way for ML-driven, targeted exploration of chemical compound space and opens avenues for further development towards generative models for larger and more general atomistic systems.</p>
<h2>IV. METHODS</h2>
<h2>A. Training Data</h2>
<p>For each training run, 55 k reference structures are randomly sampled from the QM9 data set [59-61], a collection of 133,885 molecules with up to nine heavy atoms from carbon, nitrogen, oxygen, and fluorine. We removed 915 molecules from the training pool which are deemed invalid by our validation procedure that checks the valency and connectedness of generated structures (see Section IV F). For some runs, limited subsets of the training data pool are used, as described in the results (e.g. without $C_{7} O_{2} H_{10}$ isomers). We train the neural network using 50k randomly sampled molecules and employ the remaining 5k for validation (see Section IV D). All molecules shown in figures have been rendered with the 3d visualization package Mayavi [67].</p>
<h2>B. Details on the neural network architecture</h2>
<p>In the following, we describe the cG-SchNet architecture as depicted in Figure 2 in detail. We use the shifted softplus non-linearity</p>
<p>$$
\operatorname{ssp}(x)=\ln \left(\frac{1}{2} e^{x}+\frac{1}{2}\right)
$$</p>
<p>throughout the architecture. Successive linear neural network layers with intermediate shifted softplus activation are written as</p>
<p>$$
\operatorname{mlp}(\mathbf{x})=\mathbf{W}<em 1="1">{2}^{T} \operatorname{ssp}\left(\mathbf{W}</em>}^{T} \mathbf{x}+\mathbf{b<em 2="2">{1}\right)+\mathbf{b}</em>
$$</p>
<p>with input $\mathbf{x} \in \mathbb{R}^{n_{i n_{1}}}$, weights $\mathbf{W}<em i="i" n__1="n_{1">{1} \in \mathbb{R}^{n</em>}} \times n_{i n_{2}}}$, $\mathbf{W<em i="i" n__2="n_{2">{2} \in \mathbb{R}^{n</em>}} \times n_{\text {out }}}$, and biases $\mathbf{b<em i="i" n__2="n_{2">{1} \in \mathbb{R}^{n</em>}}}, \mathbf{b<em _out="{out" _text="\text">{2} \in \mathbb{R}^{n</em>$. While this example shows a succession of two linear layers, the notation covers any number of successive linear layers with intermediate shifted softplus activations in the following. The number of layers and neurons as well as all other hyper-parameter choices for our neural network architecture are given in Supplementary Table S1.}}</p>
<p>The inputs to cG-SchNet when placing atom $i$ is a partial molecule consisting of $i-1$ atoms including two auxiliary tokens (focus and origin) and $k$ target properties $\boldsymbol{\Lambda}=\left(\lambda_{1}, \ldots, \lambda_{k}\right)$. The atoms and tokens are given as tuples of positions $\mathbf{R}<em 1="1">{\leq i-1}=\left(\mathbf{r}</em>}, \ldots, \mathbf{r<em j="j">{i-1}\right)$ with $\mathbf{r}</em>} \in \mathbb{R}^{3}$ and types $\mathbf{Z<em 1="1">{\leq i-1}=\left(Z</em>$. The first two entries correspond to the auxiliary tokens, which are treated like ordinary atoms by the neural network. Thus, whenever we refer to atoms in the following, this also encompasses the tokens. Note that tokens do not influence
the sampling probability of a molecule in Eq. 1, since they are placed with probability $p\left(R_{\leq 2}, Z_{\leq 2} \mid \boldsymbol{\Lambda}\right)=1$.}, \ldots, Z_{i-1}\right)$ with $Z_{j} \in \mathbb{N</p>
<p>We employ SchNet [21, 57] to extract atom-wise features $\mathbf{X}<em 1="1">{\leq i-1}=\left(\mathbf{x}</em>\right)$ that are invariant to rotation and translation. We use the SchNet representation network as implemented in the SchNetPack software package [68] with $F=128$ features per atom and 9 interaction blocks.}, \ldots, \mathbf{x}_{i-1</p>
<p>Additionally, we construct a vector $\mathbf{y} \in \mathbb{R}^{D}$ of conditional features from the list of target properties. To this end, each target property is first mapped into vector space using an individual embedding network that depends on the form of the specific property. In this work, we employ different embedding networks for scalar-valued properties, vector-valued properties, and the atomic composition. Scalar-valued properties are processed by an MLP after applying a Gaussian radial basis function expansion</p>
<p>$$
\mathbf{f}<em _scal="{scal" _text="\text">{\text {scal }}=\operatorname{mlp}\left(\left[e^{-\frac{\left(\lambda</em>\right]}}-\left(\lambda_{\min } \leq\left(\Delta_{\omega}\right)\right)^{2}\right.}{2 \Delta \omega^{2}}<em _max="\max">{0 \leq l \leq \frac{\lambda</em>\right)
$$}-\lambda_{\min }}{\Delta \omega}</p>
<p>where the minimum $\lambda_{\min }$ and maximum $\lambda_{\max }$ property values and the grid spacing $\Delta \omega$ are hyper-parameters chosen per target property. Vector-valued properties such as molecular fingerprints are directly processed by an MLP:</p>
<p>$$
\mathbf{f}<em _mathrm_vec="\mathrm{vec">{\mathrm{vec}}=\operatorname{mlp}\left(\lambda</em>\right)
$$}</p>
<p>For the atomic composition, we use two embedding blocks. While the number of atoms is embedded as a scalar property, we map atom types to learnable embeddings $\mathbf{g}_{Z}^{\text {comp }} \in \mathbb{R}^{G}$. These vectors are weighted by the fraction of the corresponding atom type in the target atomic composition, concatenated, and processed by an MLP. For example, the atomic composition of hydrocarbons would be encoded as:</p>
<p>$$
\mathbf{f}<em H="H">{\text {comp }}=\operatorname{mlp}\left(\left[n</em>} \mathbf{g<em C="C">{H}^{\text {comp }} \oplus n</em>\right]\right)
$$} \mathbf{g}_{C}^{\text {comp }</p>
<p>where " $\oplus$ " is the concatenation of two vectors and $n_{H}$ and $n_{C}$ is the fraction of hydrogen and carbon atoms in the target atomic composition, respectively. Finally, the property feature vectors $\mathbf{f}<em 1="1">{\lambda</em>}}, \ldots, \mathbf{f<em k="k">{\lambda</em>$ are aggregated by an MLP}</p>
<p>$$
\mathbf{y}=\operatorname{mlp}\left(\left[\mathbf{f}<em 1="1">{\lambda</em>}} \oplus \mathbf{f<em 2="2">{\lambda</em>}} \oplus \ldots \mathbf{f<em k="k">{\lambda</em>\right]\right)
$$}</p>
<p>to obtain the combined conditional features $\mathbf{y}$.
Given the conditional features $\mathbf{y}$ representing the target properties and the atom-wise features $\mathbf{X}_{\leq i-1}$ describing the partial molecule, the cG-SchNet architecture predicts distributions for the type of the next atom and its pairwise distances to all preceding atoms with two output networks. Let $\mathbf{Z}^{\text {all }} \subset \mathbb{N}$ be the set of all atom types in the training data including an additional stop marker type. The type prediction network first computes atom-wise, $\left|\mathbf{Z}^{\text {all }}\right|$-sized vectors</p>
<p>$$
\mathbf{s}<em j="j">{j}=\operatorname{mlp}\left(\left[\mathbf{x}</em> j&lt;i
$$} \oplus \mathbf{y}\right]\right) \quad \text { with </p>
<p>containing a scalar score for each atom type. Let $\mathbf{s}_{j}^{[z]}$ be the score of type $z \in \mathbf{Z}^{\text {all }}$ predicted for preceding atom $j$. Then, the probability for the next atom being of type $z$ is obtained by taking the softmax over all types and averaging the atom-wise predictions:</p>
<p>$$
p\left(Z_{i}=z \mid \mathbf{X}<em j="1">{\leq i-1}, \mathbf{y}\right)=\frac{1}{i-1} \sum</em>
$$}^{i-1} \frac{e^{s_{j}^{[z]}}}{\sum_{z^{\prime} \in \mathbf{Z}^{[z]}} e^{s_{j}^{[z^{\prime}]}}</p>
<p>The distance distributions are discretized on a grid with $L$ bins, each covering a span of $\Delta \mu$. The bin of a distance $d \in \mathbb{R}^{+}$is given by $b: \mathbb{R}^{+} \mapsto{1, \ldots, L}$</p>
<p>$$
b(d)= \begin{cases}\left\lceil\frac{d+\frac{1}{2} \Delta \mu}{\Delta \mu}\right\rceil &amp; \text { if } d \leq(L-1) \Delta \mu \ L &amp; \text { if } d&gt;(L-1) \Delta \mu\end{cases}
$$</p>
<p>Given the type $Z_{i}$ of the next atom, the distance prediction network computes scores for each preceding atom and distance bin</p>
<p>$$
\mathbf{u}<em j="j">{j}=\operatorname{mlp}\left(\left[\left(\mathbf{x}</em>} \odot \mathbf{g<em i="i">{Z</em>\right]\right) \quad \forall j&lt;i
$$}}^{\text {next }}\right) \oplus \mathbf{y</p>
<p>where " $\odot$ " is the Hadamard product and $\mathbf{g}_{Z}^{\text {next }} \in \mathbb{R}^{F}$ is a learnable atom type embedding. The probability of any distance between the new atom and a preceding atom is obtained by applying a softmax over all bins</p>
<p>$$
p\left(r_{i j}=d \mid \mathbf{x}<em i="i">{j}, \mathbf{y}, Z</em>}\right)=\frac{e^{\mathbf{u<em l="1">{j}^{[b(d)]}}}{\sum</em> \quad \forall j&lt;i
$$}^{L} e^{\mathbf{u}_{j}^{[l]}}</p>
<p>where $\mathbf{u}_{j}^{[b(d)]}$ is the score of bin $b(d)$ predicted for preceding atom $j$.</p>
<h2>C. Sampling atom placement sequences for training</h2>
<p>The number of sequences in which a molecule can be built by placing $n$ atoms grows factorially with $n$. During training, we randomly sample a new atom placement sequence for every training molecule in each epoch. However, we use the focus and origin tokens to constrain how molecules are build by cG-SchNet and thus significantly reduce the number of possible sequences. Our approach ensures that molecules tend to grow outwards starting from the center of mass and that each new atom is placed close to one of the already placed atoms. For the first atom placement step, we set the positions of the focus and origin tokens to the center of mass of the training molecule and choose the atom closest to it as the first atom to be placed. If multiple atoms are equally close, one of them is randomly chosen as the first atom.</p>
<p>Afterwards, each atom placement step follows the same procedure. One of the already placed atoms (excluding tokens) is chosen as focus, i.e. the position of the focus token is set to the position of the chosen atom. Then, from all unplaced atoms, we select the neighbor of the
focus that is closest to the center of mass as next atom. If there are no neighbors of the focus among the unplaced atoms, we insert a step where the type prediction network shall predict the stop marker type. In this way, the focus atom is marked as finished before randomly choosing a new focus and proceeding with the next atom placement step. Marked atoms cannot be chosen as focus anymore and the atom placement sequence is complete when all placed atoms are marked as finished. Thus, the sequence ends up with $2 n$ steps, as each atom needs to be placed and furthermore marked as finished.</p>
<p>For our experiments, we consider atoms sharing a bond as neighbors. However, note that bonding information is not necessarily required as neighborhood can also be defined by a radial cutoff of e.g. $3 \AA$ centered on the focus atom.</p>
<h2>D. Neural network training</h2>
<p>We use mini-batches with $M$ molecules for training. Each mini-batch contains one atom placement sequence per molecule, randomly sampled in each epoch as explained in Section IV C. Each step of the atom placement sequence $a \in A_{m}$ consists of types $\mathbf{Z}<em _leq="\leq" i-1="i-1">{\leq i-1}$ and positions $\mathbf{R}</em>$ of the next atom.}$ of already placed atoms and the two auxiliary tokens, of the values $\boldsymbol{\Lambda}$ of molecule $m$ for the target properties of the model, and of the type $Z_{\text {next }}$ and position $\mathbf{r}_{\text {next }</p>
<p>For each atom placement, we minimize the crossentropy between the distributions predicted by the model given $\mathbf{Z}<em _leq="\leq" i-1="i-1">{\leq i-1}, \mathbf{R}</em>}$, and $\boldsymbol{\Lambda}$ and the distributions obtained from the ground truth next type $Z_{\text {next }}$ and position $\mathbf{r<em _next="{next" _text="\text">{\text {next }}$. The ground truth distribution of the next type is a one-hot encoding of $Z</em>$, thus the cross-entropy loss for the type distributions is}</p>
<p>$$
\ell^{\text {type }}(a)=-\log \left(p\left(Z_{i}=Z_{\text {next }} \mid \mathbf{X}_{\leq i-1}, \mathbf{y}\right)\right)
$$</p>
<p>The average cross-entropy loss for the distance distributions is</p>
<p>$$
\ell^{\text {dist }}(a)=-\frac{1}{i-1} \sum_{j=1}^{i-1} \sum_{l=0}^{L-1} \mathbf{q}<em j="j" l="l">{j l}^{\text {next }} \log \left(\mathbf{p}</em>\right)
$$}^{\text {next }</p>
<p>with model predictions</p>
<p>$$
\mathbf{p}<em i="i" j="j">{j l}^{\text {next }}=p\left(r</em>}=l \Delta \mu \mid \mathbf{x<em i="i">{j}, \mathbf{y}, Z</em>\right)
$$</p>
<p>and Gaussian expanded ground truth distance</p>
<p>$$
\mathbf{q}<em _next="{next" _text="\text">{j l}^{\text {next }}=\frac{e^{-\gamma\left(\left|\mathbf{r}</em>}}-\mathbf{r<em 2="2">{j}\right|</em>}-l \Delta \mu\right)^{2}}}{\sum_{l^{\prime}=0}^{l-1} e^{-\gamma\left(\left|\mathbf{r<em j="j">{\text {next }}-\mathbf{r}</em>
$$}\right|_{2}-l^{\prime} \Delta \mu\right)^{2}}</p>
<p>where $L$ is the number of bins of the distance probability grid with spacing $\Delta \mu$. The width of the Gaussian expansion can be tuned with $\gamma$, which we set to $\frac{10}{\Delta \mu}$ in our experiments.</p>
<p>The loss for a mini-batch $C$ is the average type and distance loss of all atom placement steps of all $M$ molecules in the mini-batch:</p>
<p>$$
\ell(C)=\frac{1}{M} \sum_{m=1}^{M} \sum_{a \in A_{m}}\left(\frac{\ell^{\text {type }}(a)}{\left|A_{m}\right|}+\frac{\delta(a) \ell^{\text {dist }}(a)}{0.5\left|A_{m}\right|}\right)
$$</p>
<p>where $\left|A_{m}\right|$ is the number of steps in sequence $A_{m}$ and</p>
<p>$$
\delta(a)= \begin{cases}0 &amp; \text { if } Z_{\text {next }}=\mathrm{STOP} \ 1 &amp; \text { else }\end{cases}
$$</p>
<p>The indicator function $\delta$ is zero for steps where the type to predict is the stop marker, since no position is predicted in these steps.</p>
<p>The neural networks were trained with stochastic gradient descent using the ADAM optimizer [69]. We start with a learning rate $\eta=10^{-4}$ which is reduced using a decay factor of 0.5 after 10 epochs without improvement of the validation loss. The training is stopped at $\eta \leq 10^{-6}$. We use mini-batches of 5 molecules and the model with lowest validation error is selected for generation.</p>
<h2>E. Conditional generation of molecules</h2>
<p>For the generation of molecules, conditions need to be specified covering all target properties the model was trained on, e.g. the atomic composition and the relative atomic energy. The generation is an iterative process where the type and position of each atom are sampled sequentially using the distributions predicted by cG-SchNet. Generating a molecule with $n$ atoms takes $2 n$ steps, as each atom needs to be placed and furthermore marked as finished in order to terminate the generation process.</p>
<p>At each step, we want to sample the type $Z_{\text {next }} \in$ $\mathbf{Z}^{\text {all }} \subset \mathbb{N}$ and position $\mathbf{r}_{\text {next }} \in \mathbf{G} \subset \mathbb{R}^{3}$ of the next atom given the types and positions of already placed atoms (including the two tokens) and the conditions. Here, $\mathbf{Z}^{\text {all }}$ is the set of all atom types in the training data including an additional stop marker type and $\mathbf{G}$ is a grid of candidate positions in 3d space (see Supplementary Methods X A). An unfinished atom is randomly chosen as focus at the start of each step, i.e. the position of the focus token is aligned with the position of the chosen atom. Then, we predict the distribution of the type of the next atom with the model (see Eq. 11) to sample the next type</p>
<p>$$
Z_{\text {next }} \sim p\left(Z_{i}=Z_{\text {next }} \mid \mathbf{X}_{\leq i-1}, \mathbf{y}\right)
$$</p>
<p>If the next type is the stop marker, we mark the currently focused atom as finished and proceed with the next step by choosing a new focus without sampling a position. Otherwise, we proceed to predict the distance distributions between placed atoms and the next atom with the model (see Eq. 14). Since cG-SchNet is trained to place atoms in close proximity to the focused atom, we align the local grid of candidate positions with the
focus at each step regardless of the number of atoms in the unfinished molecule. Then, the distance probabilities are aggregated to compute the distribution over 3 d candidate positions in the proximity of the focus. The position of the next atom is drawn accordingly</p>
<p>$$
\mathbf{r}<em j="1">{\text {next }}^{\prime} \sim \frac{1}{\alpha} \prod</em>}^{i-1} p\left(r_{i j}=\left|\mathbf{r<em _next="{next" _text="\text">{j}-\mathbf{r}</em>\right|}}^{\prime<em j="j">{2} \mid \mathbf{x}</em>\right)
$$}, \mathbf{y}, Z_{i</p>
<p>with</p>
<p>$$
\mathbf{r}<em _next="{next" _text="\text">{\text {next }}^{\prime}=\mathbf{r}</em>
$$}}+\mathbf{r}_{\text {focus }</p>
<p>where $\alpha$ is the normalization constant and $\mathbf{r}_{\text {focus }}$ is the position of the focus token. At the very first atom placement step, we center the focus and grid on the origin token, while for the remaining steps, only atoms will be focused.</p>
<p>The generation process terminates when all regular atoms have been marked as finished. In this work, we limit the model to a maximum number of 35 atoms. If the model attempts to place more atoms, the generation terminates and the molecule is marked as invalid.</p>
<h2>F. Checking validity and uniqueness of generated molecules</h2>
<p>We use Open Babel [58] to assess the validity of generated molecules. Open Babel assigns bonds and bond orders between atoms to translate the generated 3d representation of atom positions and types into a molecular graph. We check if the valence constraints hold for all atoms in the molecular graph and mark the molecule as invalid if not. Furthermore, the generated structure is considered invalid if it consists of multiple disconnected graphs. We found that Open Babel may struggle to assign correct bond orders even for training molecules if they contain aromatic sub-structures made of nitrogen and carbon. Thus, we use the same custom heuristic as in previous G-SchNet work [48] that catches these cases and checks whether a correct bond order can be found. The corresponding code is available in the Supplementary Material.</p>
<p>The uniqueness of generated molecules is checked using their canonical SMILES [30] string representation obtained from the molecular graph with Open Babel. If two molecules share the same string, they are considered to be equal, i.e. non-unique. Furthermore, we check the canonical SMILES string of mirror-images of generated structures, which means that mirror-image stereoisomers (enantiomers) are considered to be the same molecule in our statistics. In case of duplicates, we keep the molecule sampled first, with the exception of the search for $C_{7} O_{2} H_{10}$ isomers, where we keep the structure with the lowest predicted relative atomic energy. Molecules from the training and test data are matched with generated structures in the same way, using their canonical SMILES representations obtained with Open Babel and</p>
<p>the custom heuristic for bond order assignment. In general, we use isomeric SMILES strings that encode information about the stereochemistry of 3d structures. Only in the search for $C_{7} O_{2} H_{10}$ isomers, we also compare nonisomeric canonical SMILES obtained with RDKit [70] in order to identify novel stereoisomers, i.e. structures that share the same non-isomeric SMILES representation but differ in the isomeric variant.</p>
<h2>G. Prediction of property values of generated molecules</h2>
<p>We use pretrained SchNet [21, 57] models from SchNetPack [68] to predict the HOMO-LUMO gap, isotropic polarizability, and internal energy at zero Kelvin of generated molecules. The reported mean absolute error (MAE) of these models is $0.074 \mathrm{eV}, 0.124 \mathrm{Bohr}^{3}$, and 0.012 eV , respectively. The predicted values are used to plot the distributions of the respective property in Fig. 1b, Fig. 3b, and Fig. 4a. We relax generated molecules for every experiment in order to assess how close they are to equilibrium configurations and to calculate the MAE between predictions for generated, unrelaxed structures and the computed ground-truth property value of the relaxed structure. The relaxation procedure is described in Supplementary Methods X D), where furthermore a table with the results can be found (Supplementary Table S2). For the statistics depicted in Fig. 4b-d and Fig. 5, we use the property values computed during relaxation instead of predictions from SchNet models.</p>
<h2>V. DATA AVAILABILITY</h2>
<p>The QM9 data set is available under DOI 10.6084/m9.figshare. 978904. The set of molecules with small HOMO-LUMO gap generated by biased G-SchNet is available at http://quantum-machine. org/datasets.</p>
<h2>VI. CODE AVAILABILITY</h2>
<p>The code for cG-SchNet is available at www.github. com/atomistic-machine-learning/cG-SchNet. This includes the routines for training and deploying the model, for filtering generated structures, all hyperparameter settings used in our experiments, and the splits of the data employed to train the reported models.</p>
<h2>VII. AUTHOR CONTRIBUTIONS</h2>
<p>NWAG developed the method and carried out the experiments. MG carried out the reference computations and simulations. SSPH trained the neural networks
for predictions of molecular properties. NWAG, MG, KRM and KTS designed the experiments and analyses. NWAG, MG, and KTS wrote the paper. All authors discussed results and contributed to the final version of the manuscript.</p>
<h2>VIII. CONFLICTS OF INTERESTS</h2>
<p>There are no conflicts to declare.</p>
<h2>IX. ACKNOWLEDGEMENTS</h2>
<p>NWAG and MG work at the BASLEARN - TU Berlin/BASF Joint Lab for Machine Learning, cofinanced by TU Berlin and BASF SE. NWAG, KTS, SSPH, and KRM acknowledge support by the Federal Ministry of Education and Research (BMBF) for the Berlin Institute for the Foundations of Learning and Data (BIFOLD) (01IS18037A). KRM acknowledges financial support under the Grants 01IS14013A-E, 01GQ1115 and 01GQ0850; Deutsche Forschungsgemeinschaft (DFG) under Grant Math+, EXC 2046/1, Project ID 390685689 and KRM was partly supported by the Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP) grants funded by the Korea Government(No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University).</p>
<h2>X. SUPPLEMENTARY INFORMATION</h2>
<h2>A. 3d grid for molecule generation</h2>
<p>We use a grid of candidate positions $\mathbf{G} \subset \mathbb{R}^{3}$, with a spacing of $0.05 \AA$. The extent of the grid is limited by a minimum distance $d_{\min }$ and a maximum distance $d_{\max }$ :</p>
<p>$$
\begin{aligned}
\mathbf{G}=\left{\mathbf{r} \in \mathbb{R}^{3} \mid\right. &amp; \left.\mathbf{r}=(0.05 \cdot x, 0.05 \cdot y, 0.05 \cdot z) \wedge\right. \
&amp; \left.x, y, z \in \mathbb{Z} \wedge d_{\min } \leq|\mathbf{r}|<em _max="\max">{2} \leq d</em>\right}
\end{aligned}
$$</p>
<p>The limits should be chosen according to the minimum and maximum distances between atoms in the training set that are considered to be neighbors when building atom placement sequences. For our experiments with QM9, we choose $d_{\min }=0.9 \AA$ and $d_{\max }=1.7 \AA$.</p>
<p>Furthermore, as in previous work with G-SchNet [48], we utilize a temperature parameter $T$ to control the randomness when sampling from candidate positions:</p>
<p>$$
\mathbf{r}<em j="1">{\text {next }}^{\prime} \sim \frac{1}{\alpha} \exp \left(\frac{\sum</em>\right)
$$}^{i-1} \log \mathbf{p}_{j}^{\text {next }}}{T</p>
<p>with</p>
<p>$$
\begin{aligned}
\mathbf{p}<em i="i" j="j">{j}^{\text {next }} &amp; =p\left(r</em>}=\left|\mathbf{r<em _next="{next" _text="\text">{j}-\mathbf{r}</em>\right|}}^{\prime<em j="j">{2} \mid \mathbf{x}</em>\right) \
\mathbf{r}}, \mathbf{y}, Z_{i<em _next="{next" _text="\text">{\text {next }}^{\prime} &amp; =\mathbf{r}</em>}}+\mathbf{r<em _mathbf_r="\mathbf{r">{\text {focus }} \
\alpha &amp; =\sum</em><em j="1">{\text {cand }}^{\prime} \in \mathbf{G}} \exp \left(\frac{\sum</em>\right)
\end{aligned}
$$}^{i-1} \log \mathbf{p}_{j}^{\text {cand }}}{T</p>
<p>Increasing $T$ will increase randomness by smoothing the grid distribution. For sampling, we stick with $T=0.1$ in this work, which was found to result in accurate yet diverse sets of generated molecules [48].</p>
<p>The very first atom is placed solely based on the predicted distance to the origin token, i.e. the center of mass of the structure about to be generated. Naturally, this distance is not restricted by the same limits as neighboring atoms and thus, for this particular step, we employ a special grid $\mathbf{G}_{1} \subset \mathbb{R}^{3}$ that covers larger distances:</p>
<p>$$
\begin{aligned}
\mathbf{G}<em 2="2">{1}=\left{\mathbf{r} \in \mathbb{R}^{3} \mid \mathbf{r}\right. &amp; =(0.05 \cdot x, 0,0) \wedge \
x &amp; \left.\in \mathbb{N} \wedge|\mathbf{r}|</em>&lt;15\right}
\end{aligned}
$$</p>
<p>The maximum distance covered by the grid has been chosen to match with the maximum distance covered in the discretized distance distributions predicted by the model. Due to symmetry, the grid only needs to extend into one direction. Furthermore, the distribution is not smoothed during generation, i.e. we always set $T=1.0$ when sampling the first atom.</p>
<h2>B. Calculation of relative atomic energy</h2>
<p>We define a relative atomic energy that describes whether the energy per atom of a 3d conformation is comparatively high or low with respect to other structures in
the data set that share the same atomic composition:</p>
<p>$$
E^{\text {rel }}\left(\mathbf{R}<em _leq="\leq" n="n">{\leq n}, \mathbf{Z}</em>}\right)=E\left(\mathbf{R<em _leq="\leq" n="n">{\leq n}, \mathbf{Z}</em>\right)
$$}\right)-\hat{E}^{Z}\left(\mathbf{Z}_{\leq n</p>
<p>Here, $E\left(\mathbf{R}<em _leq="\leq" n="n">{\leq n}, \mathbf{Z}</em>}\right)$ is the internal energy per atom at zero Kelvin of a molecular structure and $\hat{E}^{Z}\left(\mathbf{Z<em _leq="\leq" n="n">{\leq n}\right)$ is the expected internal energy per atom of molecules with the same composition in the training data set. A similarly normalized energy has been defined by Zubatyuk et al. [62] for their neural network potential AIMNet. Analogous to their procedure, we predict $\hat{E}^{Z}\left(\mathbf{Z}</em>$ isomers with a model that was trained solely on other compositions (see Figure 4 in the paper).}\right)$ from the atomic composition with a linear regression model. The model maps from atomic concentration, i.e. the atomic composition divided by the total number of atoms in the system, to the internal energy per atom at zero Kelvin. In this way, we can compute the relative atomic energy even for structures with compositions that are not included in the training data and treat molecules of different size and composition in a comparable and normalized manner. This allows our model to learn a relation between 3d conformations and their energy that can be transferred across compositions, as can be seen in our experiments where we sample low-energy $C_{7} O_{2} H_{10</p>
<p>The internal energy of training structures is provided in QM9 as property "U0". For unrelaxed, generated structures we predict the internal energy with a SchNet model trained on QM9 as explained in the Methods (section IV G). For relaxed, generated molecules we use the internal energy calculated with the ORCA quantum chemistry package [71] (Supplementary Methods X D). Although we relax structures at the same level of theory as the training data, the internal energies obtained with ORCA have a systematic offset compared to the calculations used in QM9. Thus, we estimate this offset and add it to the calculated internal energy. For the relaxed low-energy $C_{7} O_{2} H_{10}$ isomers (results in Fig. 4b-d), we re-compute the internal energy of all $C_{7} O_{2} H_{10}$ isomers in QM9 with ORCA and take the average difference between the reported internal energies and the re-computed values to estimate the offset ( $\sim-0.0064 \mathrm{eV}$ per atom). For the relaxed low-energy molecules with small HOMOLUMO gap (results in Fig. 5) we re-compute the internal energy of 1000 randomly sampled structures from QM9 with ORCA and fit a linear regression model to predict the difference between reported internal energies and re-computed values from the atomic composition. This allows to estimate the offset between internal energies from ORCA and the training data for relaxed, generated molecules of arbitrary composition.</p>
<h2>C. Calculation of fingerprints</h2>
<p>We obtain 1024 bits long binary fingerprints that capture the presence of linear fragments with up to seven atoms with Open Babel [58]. We use version 2.4.1 of Open Babel, where the employed fingerprint is called</p>
<p>"FP2" and corresponds to the default choice. Fingerprints are calculated after the SMILES representation of 3d structures are obtained as described in the Methods (section IV F).</p>
<h1>D. Relaxation of generated structures with density functional theory</h1>
<p>All electronic structure computations were carried out with the ORCA quantum chemistry package [71]. SCF convergence was set to tight and integration grid levels of 4 and 5 were employed during SCF iterations and the final computation of properties, respectively.</p>
<p>Structures were first pre-optimized at the PBE/def2SVP[72, 73] level of theory and then relaxed at the final B3LYP/6-31G(2df,2p) level [74-77]. We used the same B3LYP parametrization scheme as employed in the Gaussian electronic structure packages. To further accelerate the relaxation procedure, the resolution of identity (RI) $[78,79]$ and chain of spheres (COS)[80] approximations were used.</p>
<p>The zero point vibrational energies required for the computation of the internal energies were obtained by normal mode analysis performed on the fully relaxed structures using the B3LYP/6-31G(2df,2p) level of theory.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FIG. S1. Generated novel $\mathbf{C}<em _mathbf_2="\mathbf{2">{\mathbf{7}} \mathbf{O}</em>}} \mathbf{H<em 7="7">{\mathbf{1 0}}$ isomers vs. most similar isomers in QM9. Pairs of generated, novel, low-energy $C</em>$ generated by cG-SchNet (target energy -0.1 eV ).
} O_{2} H_{10}$ isomers (left) and the corresponding most similar $C_{7} O_{2} H_{10}$ isomer in QM9 (right) according the Tanimoto similarity of path-based fingerprints (noted below each pair). In the first row, we show pairs corresponding to the novel structures depicted in the first row of Fig. 4d. The remaining structures are uniformly randomly selected from all novel isomers with relative atomic energy $\leq-0.05 \mathrm{eV<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FIG. S2. Generated novel $\mathbf{C}<em _mathbf_2="\mathbf{2">{\mathbf{7}} \mathbf{O}</em>}} \mathbf{H<em 7="7">{\mathbf{1 0}}$ isomers containing carboxylic acid. The twelve generated $C</em>$ containing a carboxylic acid group. The molecules were obtained by cG-SchNet with target relative atomic energy -0.1 eV . Relative atomic energies are denoted below each isomer.} O_{2} H_{10}$ isomers with relative atomic energy $\leq-0.05 \mathrm{eV</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>FIG. S3. Different conformations of the five most generated $\mathbf{C}<em _mathbf_2="\mathbf{2">{\mathbf{7}} \mathbf{O}</em>}} \mathbf{H<em 7="7">{\mathbf{1 0}}$ isomers. For the five most often generated $C</em>$ isomers (that share the same isomeric SMILES string) from cG-SchNet with target relative atomic energy -0.1 eV we randomly sample 50 of the generated examples. We relax them and report the resulting unique conformations along with their respective deviation from the mean energy per atom of all 50 examples. For three of the five isomers, all examples converge to the same conformation. However, we see that cG-SchNet is capable of sampling multiple conformations whenever there are degrees of freedom, e.g. in isomer number one and isomer number four. Our analysis suggests the path for a possible future adaptation and application of cG-SchNet that is particularly tailored to generative models for 3d molecules, i.e. the targeted generation of conformations for a given (possibly isomeric) graph. To this end, a proper embedding of the molecular graph and several target properties would need to be provided as conditions.
} O_{2} H_{10<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>FIG. S4. Comparison of generated novel $\mathbf{C}<em _mathbf_2="\mathbf{2">{\mathbf{7}} \mathbf{O}</em>}} \mathbf{H<em 7="7">{\mathbf{1 0}}$ isomers before and after relaxation. We show novel, low-energy $C</em>$ structures as generated by our model and the corresponding closest equilibrium conformation found by relaxation with DFT (orange structures). The root-mean-square deviation between atom positions before and after relaxation is noted below each molecule (in $\AA$ ). We show the same structures as in Fig. S1.} O_{2} H_{10</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>FIG. S5. HOMO-LUMO gap and relative atomic energy of molecules generated by cG-SchNet and in the training data. We show the HOMO-LUMO gap and relative atomic energy of molecules close to the conditioning target (black cross) for both the training data set (orange) and the set of unseen and valid structures generated by cG-SchNet (blue). Moreover, three training structures (orange, dotted lines) and four novel, generated molecules (blue, dotted lines) from the borders of the distributions are shown for reference. Here we see how cG-SchNet generalizes to larger structures (i.e. with more than 9 heavy atoms) when sampling molecules with particularly low HOMO-LUMO gap and relative atomic energy.</p>
<p>TABLE S1. Choice of neural network hyper-parameters. In this work, we trained five cG-SchNet models using different target properties. Each model uses a SchNet network with 128 features, 9 interaction blocks, a cutoff of $10 \AA$, and 25 centers for the radial basis expansion of distances. For the remaining building blocks, we report the number of neurons per layer in the MLPs and the additional hyper-parameters. Furthermore, we mark which block was used in which model, where model 1 targets isotropic polarizability (results in Fig. 1b), 2 targets molecular fingerprints (Fig. 3a), 3 targets atomic composition and HOMO-LUMO gap (Fig. 3b), 4 targets atomic composition and relative atomic energy (Fig. 4), and 5 targets relative atomic energy and HOMO-LUMO gap (Fig. 5).</p>
<table>
<thead>
<tr>
<th>Neural network block</th>
<th>Model</th>
<th>Neurons per layer</th>
<th>Additional parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>isotropic polarizability embedding</td>
<td>1</td>
<td>$64,64,64$</td>
<td>$\lambda_{\min }: 33.75 a_{0}^{3}, \lambda_{\max }: 107.95 a_{0}^{3}, \Delta \omega: 5.3 a_{0}^{3}$</td>
</tr>
<tr>
<td>fingerprint embedding</td>
<td>2</td>
<td>$725,426,128$</td>
<td>-</td>
</tr>
<tr>
<td>HOMO-LUMO gap embedding</td>
<td>3,5</td>
<td>$64,64,64$</td>
<td>$\lambda_{\min }: 2 \mathrm{eV}, \lambda_{\max }: 11 \mathrm{eV}, \Delta \omega: 2.25 \mathrm{eV}$</td>
</tr>
<tr>
<td>relative atomic energy embedding</td>
<td>4,5</td>
<td>$64,64,64$</td>
<td>$\lambda_{\min }: 0.2 \mathrm{eV}, \lambda_{\max }: 0.2 \mathrm{eV}, \Delta \omega: 0.1 \mathrm{eV}$</td>
</tr>
<tr>
<td>atom count embedding</td>
<td>3,4</td>
<td>$64,64,64$</td>
<td>$\lambda_{\min }: 0, \lambda_{\max }: 35, \Delta \omega: 8.75$</td>
</tr>
<tr>
<td>composition embedding</td>
<td>3,4</td>
<td>$64,64,64$</td>
<td>$\mathbf{g}_{Z}^{\text {comp }}: 16$ features</td>
</tr>
<tr>
<td>properties aggregation</td>
<td>all</td>
<td>$128,128,128,128,128$</td>
<td>-</td>
</tr>
<tr>
<td>type prediction</td>
<td>all</td>
<td>$206,156,106,56,6$</td>
<td>-</td>
</tr>
<tr>
<td>distance predictions</td>
<td>all</td>
<td>$264,273,282,291,300$</td>
<td>$L: 300, \Delta \mu: 0.05 \AA, \mathbf{g}_{Z}^{\text {next }}: 128$ features</td>
</tr>
</tbody>
</table>
<p>TABLE S2. Relaxation results. Results for relaxation of the 100 generated unique unseen molecules closest to the respective target electronic property values. We show the properties on which the respective model was conditioned, the targeted property values, the validity of the relaxed molecules, the median root-mean-square deviation (RMSD) between atom positions before and after relaxation for valid molecules, and the mean absolute error (MAE) between the property values before and after relaxation for valid molecules (i.e. how much the calculated property values of relaxed molecules deviate from the predicted property values of the generated molecules). For the $C_{7} O_{2} H_{10}$ isomers sampled with relative atomic energy target -0.1 eV and molecules sampled while targeting HOMO-LUMO gap and relative atomic energy simultaneously, the statistics are calculated from all generated unique unseen molecules instead of the 100 closest (since we relaxed all of them for our analyses in Fig. 4 and Fig. 5).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Conditioning</th>
<th style="text-align: center;">Target</th>
<th style="text-align: center;">Validity</th>
<th style="text-align: center;">RMSD</th>
<th style="text-align: center;">MAE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">isotropic polarizability $\left(a_{0}^{3}\right)$</td>
<td style="text-align: center;">$33.75 a_{0}^{3}$</td>
<td style="text-align: center;">$96 \%$</td>
<td style="text-align: center;">$0.20 \AA$</td>
<td style="text-align: center;">$2.23 a_{0}^{3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$54.00 a_{0}^{3}$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$0.19 \AA$</td>
<td style="text-align: center;">$2.35 a_{0}^{3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$74.25 a_{0}^{3}$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.23 \AA$</td>
<td style="text-align: center;">$1.40 a_{0}^{3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$94.50 a_{0}^{3}$</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$0.28 \AA$</td>
<td style="text-align: center;">$0.99 a_{0}^{3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$114.75 a_{0}^{3}$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.38 \AA$</td>
<td style="text-align: center;">$2.95 a_{0}^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">composition \&amp; HOMO-LUMO gap $(e V)$</td>
<td style="text-align: center;">$C_{7} N_{1} O_{1} H_{11}$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.32 \AA$</td>
<td style="text-align: center;">$0.20 e V$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$C_{7} N_{1} O_{1} H_{11}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$C_{7} O_{2} H_{10}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-0.1 e V$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.26 \AA$</td>
<td style="text-align: center;">$0.01 e V$</td>
</tr>
<tr>
<td style="text-align: center;">composition \&amp; relative atomic energy $(e V)$</td>
<td style="text-align: center;">$C_{7} O_{2} H_{10}$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.26 \AA$</td>
<td style="text-align: center;">$0.01 e V$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.0 e V$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$C_{7} O_{2} H_{10}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.1 e V$</td>
<td style="text-align: center;">$97 \%$</td>
<td style="text-align: center;">$0.17 \AA$</td>
<td style="text-align: center;">$0.02 e V$</td>
</tr>
<tr>
<td style="text-align: center;">HOMO-LUMO gap $(e V) \&amp;$</td>
<td style="text-align: center;">4.0 eV</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0.30 \AA$</td>
<td style="text-align: center;">$0.33 e V$</td>
</tr>
<tr>
<td style="text-align: center;">relative atomic energy $(e V)$</td>
<td style="text-align: center;">$-0.2 \mathrm{eV}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.03 e V$</td>
</tr>
</tbody>
</table>
<p>[1] P. J. Hajduk and J. Greer. A decade of fragment-based drug design: strategic advances and lessons learned. Nat. Rev. Drug Discovery, 6(3):211-219, 2007.
[2] S. Mandal, M. Moudgil, and S. K. Mandal. Rational drug design. Eur. J. Pharmacol., 625(1):90-100, 2009. ISSN 0014-2999. New Vistas in Anti-Cancer Therapy.
[3] P. Gantzer, B. Creton, and C. Nieto-Draghi. Inverse-qspr for de novo design: A review. Mol Inform, 39(4):1900087, 2020.
[4] J. G. Freeze, H. R. Kelly, and V. S. Batista. Search for catalysts by inverse design: Artificial intelligence, mountain climbers, and alchemists. Chem. Rev., 119(11):65956612, 2019. PMID: 31059236.
[5] K. Kang, Y. S. Meng, J. Breger, C. P. Grey, and G. Ceder. Electrodes with high power and high capacity for rechargeable lithium batteries. Science, 311(5763): $977-980,2006$.
[6] G. Hautier, A. Jain, H. Chen, C. Moore, S. P. Ong, and G. Ceder. Novel mixed polyanions lithium-ion battery cathode materials predicted by high-throughput ab initio computations. J. Mater. Chem., 21(43):17147-17153, 2011.
[7] M. C. Scharber, D. Mühlbacher, M. Koppe, P. Denk, C. Waldauf, A. J. Heeger, and C. J. Brabec. Design rules for donors in bulk-heterojunction solar cells-towards $10 \%$ energy-conversion efficiency. Adv. Mater., 18(6): $789-794,2006$.
[8] L. Yu, R. S. Kokenyesi, D. A. Keszler, and A. Zunger. Inverse design of high absorption thin-film photovoltaic materials. Adv. Energy Mater., 3(1):43-48, 2013.
[9] K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and A. Walsh. Machine learning for molecular and materials science. Nature, 559(7715):547-555, 2018.
[10] O. A. von Lilienfeld, K.-R. Müller, and A. Tkatchenko. Exploring chemical compound space with quantumbased machine learning. Nat. Rev. Chem., 4(7):347-358, 2020.
[11] K. Schütt, S. Chmiela, O. von Lilienfeld, A. Tkatchenko, K. Tsuda, and K. Müller. Machine Learning Meets Quantum Physics, volume 968 of Lecture Notes in Physics. Springer International Publishing, 2020.
[12] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schütt, A. Tkatchenko, and K.-R. Müller. Machine learning force fields. Chem. Rev., 121 (16):10142-10186, 2021. PMID: 33705118.
[13] J. Westermayr, M. Gastegger, K. T. Schütt, and R. J. Maurer. Perspective on integrating machine learning into computational chemistry and materials science. J. Chem. Phys., 154(23):230903, 2021.
[14] M. Ceriotti, C. Clementi, and O. Anatole von Lilienfeld. Machine learning meets chemical physics, 2021.
[15] J. A. Keith, V. Vassilev-Galindo, B. Cheng, S. Chmiela, M. Gastegger, K.-R. Müller, and A. Tkatchenko. Combining machine learning and computational chemistry for predictive insights into chemical systems. Chem. Rev., 121(16):9816-9872, 2021.
[16] J. Behler and M. Parrinello. Generalized neural-network representation of high-dimensional potential-energy surfaces. Phys. Rev. Lett., 98(14):146401, 2007.
[17] M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. Von Lilienfeld. Fast and accurate modeling of molecu-
lar atomization energies with machine learning. Phys. Rev. Lett., 108(5):058301, 2012.
[18] K. T. Schütt, F. Arbabzadah, S. Chmiela, K. R. Müller, and A. Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nat Commun, 8:13890, 2017.
[19] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1263-1272. PMLR, 2017.
[20] J. S. Smith, O. Isayev, and A. E. Roitberg. ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chem. Sci., 8(4):31923203, 2017.
[21] K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Müller. SchNet - A deep learning architecture for molecules and materials. J. Chem. Phys., 148(24):241722, 2018.
[22] S. Chmiela, H. E. Sauceda, K.-R. Müller, and A. Tkatchenko. Towards exact molecular dynamics simulations with machine-learned force fields. Nat Commun, $9(1): 3887,2018$.
[23] O. T. Unke and M. Meuwly. PhysNet: a neural network for predicting energies, forces, dipole moments, and partial charges. J. Chem. Theory Comput., 15(6):3678-3693, 2019.
[24] J. Klicpera, J. Groß, and S. Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations (ICLR), 2020.
[25] A. S. Christensen, L. A. Bratholm, F. A. Faber, and O. Anatole von Lilienfeld. FCHL revisited: Faster and more accurate quantum machine learning. J. Chem. Phys., 152(4):044107, 2020.
[26] S. Batzner, T. E. Smidt, L. Sun, J. P. Mailoa, M. Kornbluth, N. Molinari, and B. Kozinsky. Se (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. arXiv preprint arXiv:2101.03164, 2021.
[27] K. Schütt, O. Unke, and M. Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9377-9388. PMLR, 2021.
[28] A. Zunger. Inverse design in search of materials with target functionalities. Nat. Rev. Chem., 2(4):1-16, 2018.
[29] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360365, 2018.
[30] D. Weininger. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, 1988.
[31] D. C. Elton, Z. Boukouvalas, M. D. Fuge, and P. W. Chung. Deep learning for molecular design-a review of the state of the art. Mol. Syst. Des. Eng., 4(4):828-849, 2019.
[32] E. Mansimov, O. Mahmood, S. Kang, and K. Cho.</p>
<p>Molecular geometry prediction using a deep generative graph neural network. Sci. Rep., 9(1):1-13, 2019.
[33] G. Simm and J. M. Hernandez-Lobato. A generative model for molecular distance geometry. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 89498958. PMLR, 2020.
[34] T. Gogineni, Z. Xu, E. Punzalan, R. Jiang, J. Kammeraad, A. Tewari, and P. Zimmerman. Torsionnet: A reinforcement learning approach to sequential conformer search. Adv Neur In, 33:20142-20153, 2020.
[35] M. Xu, S. Luo, Y. Bengio, J. Peng, and J. Tang. Learning neural generative dynamics for molecular conformation generation. In International Conference on Learning Representations, 2021.
[36] M. Xu, W. Wang, S. Luo, C. Shi, Y. Bengio, R. GomezBombarelli, and J. Tang. An end-to-end framework for molecular conformation generation via bilevel programming. arXiv preprint arXiv:2105.07246, 2021.
[37] O.-E. Ganea, L. Pattanaik, C. W. Coley, R. Barzilay, K. F. Jensen, W. H. Green, and T. S. Jaakkola. GeoMol: Torsional geometric generation of molecular 3d conformer ensembles. arXiv preprint arXiv:2106.07802, 2021.
[38] D. Lemm, G. F. von Rudorff, and O. A. von Lilienfeld. Machine learning based energy-free structure predictions of molecules, transition states, and solids. Nat Commun, 12(1):1-10, 2021.
[39] M. Stieffenhofer, T. Bereau, and M. Wand. Adversarial reverse mapping of condensed-phase molecular structures: Chemical transferability. APL Mater., 9(3): 031107, 2021.
[40] F. Noé, S. Olsson, J. Köhler, and H. Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457), 2019.
[41] J. Köhler, L. Klein, and F. Noe. Equivariant flows: Exact likelihood generative learning for symmetric densities. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5361-5370. PMLR, 2020.
[42] J. Ingraham, A. Riesselman, C. Sander, and D. Marks. Learning protein structure with a differentiable simulator. In International Conference on Learning Representations, 2018.
[43] T. Lemke and C. Peter. Encodermap: dimensionality reduction and generation of molecule conformations. $J$. Chem. Theory Comput., 15(2):1209-1215, 2019.
[44] M. AlQuraishi. End-to-end differentiable learning of protein structure. Cell Syst., 8(4):292-301, 2019.
[45] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. Žižek, A. W. Nelson, A. Bridgland, et al. Improved protein structure prediction using potentials from deep learning. Nature, 577 (7792):706-710, 2020.
[46] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žižek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, and D. Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, pages $1-11,2021$.
[47] N. W. A. Gebauer, M. Gastegger, and K. T. Schütt. Generating equilibrium molecules with deep neural networks.
arXiv preprint arXiv:1810.11347, 2018.
[48] N. Gebauer, M. Gastegger, and K. Schütt. Symmetryadapted generation of 3d point sets for the targeted discovery of molecules. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7566-7578. Curran Associates, Inc., 2019.
[49] M. Hoffmann and F. Noé. Generating valid euclidean distance matrices. arXiv preprint arXiv:1910.03131, 2019.
[50] V. Nesterov, M. Wieser, and V. Roth. 3DMolNet: a generative network for molecular structures. arXiv preprint arXiv:2010.06477, 2020.
[51] G. Simm, R. Pinsler, and J. M. Hernandez-Lobato. Reinforcement learning for molecular design guided by quantum mechanics. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8959-8969. PMLR, 2020.
[52] G. N. C. Simm, R. Pinsler, G. Csányi, and J. M. Hernández-Lobato. Symmetry-aware actor-critic for 3d molecular design. In International Conference on Learning Representations, 2021.
[53] Y. Li, J. Pei, and L. Lai. Learning to design drug-like molecules in three-dimensional space using deep generative models. arXiv preprint arXiv:2104.08474, 2021.
[54] R. P. Joshi, N. W. A. Gebauer, M. Bontha, M. Khazaieli, R. M. James, J. B. Brown, and N. Kumar. 3D-Scaffold: A deep learning framework to generate 3d coordinates of drug-like molecules with desired scaffolds. J. Phys. Chem. B, 125(44):12166-12176, 2021.
[55] V. G. Satorras, E. Hoogeboom, F. B. Fuchs, I. Posner, and M. Welling. E (n) equivariant normalizing flows for molecule generation in 3d. arXiv preprint arXiv:2105.09016, 2021.
[56] S. A. Meldgaard, J. Köhler, H. L. Mortensen, M.-P. V. Christiansen, F. Noé, and B. Hammer. Generating stable molecules using imitation and reinforcement learning. Mach. Learn.: Sci. Technol., 3(1):015008, 2022.
[57] K. Schütt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Müller. SchNet: A continuousfilter convolutional neural network for modeling quantum interactions. In Advances in Neural Information Processing Systems, pages 992-1002, 2017.
[58] N. M. O'Boyle, M. Banck, C. A. James, C. Morley, T. Vandermeersch, and G. R. Hutchison. Open babel: An open chemical toolbox. J. Cheminf., 3(1):33, Oct 2011.
[59] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Sci. Data, 1, 2014.
[60] J.-L. Reymond. The chemical space project. Acc. Chem. Res., 48(3):722-730, 2015.
[61] L. Ruddigkeit, R. Van Deursen, L. C. Blum, and J.L. Reymond. Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17. J. Chem. Inf. Model., 52(11):2864-2875, 2012.
[62] R. Zubatyuk, J. S. Smith, J. Leszczynski, and O. Isayev. Accurate and transferable multitask prediction of chemical properties with an atoms-in-molecules neural network. Sci. Adv., 5(8):eaav6490, 2019.
[63] M. Glavatskikh, J. Leguy, G. Hunault, T. Cauchy, and B. Da Mota. Dataset's chemical diversity limits the generalizability of machine learning predictions. J. Cheminf., $11(1): 1-15,2019$.</p>
<p>[64] B. Huang and O. A. von Lilienfeld. Quantum machine learning using atom-in-molecule-based fragments selected on the fly. Nat. Chem., 12(10):945-951, 2020.
[65] M. Gastegger, C. Kauffmann, J. Behler, and P. Marquetand. Comparing the accuracy of high-dimensional neural network potentials and the systematic molecular fragmentation method: A benchmark study for all-trans alkanes. J. Chem. Phys., 144(19):194110, 2016.
[66] M. Gastegger, J. Behler, and P. Marquetand. Machine learning molecular dynamics for the simulation of infrared spectra. Chem. Sci., 8(10):6924-6935, 2017.
[67] P. Ramachandran and G. Varoquaux. Mayavi: 3D Visualization of Scientific Data. Comput Sci Eng, 13(2): $40-51,2011$. ISSN 1521-9615.
[68] K. T. Schütt, P. Kessel, M. Gastegger, K. A. Nicoli, A. Tkatchenko, and K.-R. Müller. SchNetPack: A deep learning toolbox for atomistic systems. J. Chem. Theory Comput., 15(1):448-455, 2019.
[69] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[70] RDKit, online. RDKit: Open-source cheminformatics. http://www.rdkit.org. [Online; accessed 08-September2021].
[71] F. Neese. The ORCA program system. WIREs Comput. Mol. Sci., 2(1):73-78, 2012.
[72] J. P. Perdew, K. Burke, and M. Ernzerhof. Generalized gradient approximation made simple. Phys. Rev. Lett., $77: 3865-3868,1996$.
[73] F. Weigend and R. Ahlrichs. Balanced basis sets of split valence, triple zeta valence and quadruple zeta valence
quality for H to Rn: Design and assessment of accuracy. Phys. Chem. Chem. Phys., 7(18):3297-3305, 2005.
[74] A. D. Becke. Density-functional thermochemistry. III. the role of exact exchange. J. Chem. Phys., 98:56485652, 1993.
[75] C. Lee, W. Yang, and R. G. Parr. LYP correlation: Development of the Colle-Salvetti correlation-energy formula into a functional of the electron density. Phys. Rev. B, 37(2):785, 1988.
[76] S. H. Vosko, L. Wilk, and M. Nusair. Accurate spindependent electron liquid correlation energies for local spin density calculations: a critical analysis. Can. J. Phys., 58(8):1200-1211, 1980.
[77] P. J. Stephens, F. J. Devlin, C. F. Chabalowski, and M. J. Frisch. Ab initio calculation of vibrational absorption and circular dichroism spectra using density functional force fields. J. Phys. Chem., 98(45):11623-11627, 1994.
[78] K. Eichkorn, O. Treutler, H. Öhm, M. Häser, and R. Ahlrichs. Auxiliary basis sets to approximate Coulomb potentials. Chem. Phys. Lett., 240(4):283-290, 1995.
[79] O. Vahtras, J. Almlöf, and M. W. Feyereisen. Integral approximations for LCAO-SCF calculations. Chem. Phys. Lett., 213(5-6):514-518, 1993.
[80] F. Neese, F. Wennmohs, A. Hansen, and U. Becker. Efficient, approximate and parallel hartree-fock and hybrid dft calculations. a 'chain-of-spheres' algorithm for the hartree-fock exchange. Chem. Phys., 356(1-3):98109, 2009.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>n.gebauer@tu-berlin.de
${ }^{\dagger}$ kristof.schuett@tu-berlin.de</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
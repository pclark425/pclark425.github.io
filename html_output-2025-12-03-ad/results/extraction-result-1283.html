<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-31d3916cb5ce9f1acd23670741eebfda7458ae39</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/31d3916cb5ce9f1acd23670741eebfda7458ae39" target="_blank">Misspecified Gaussian Process Bandit Optimization</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Two algorithms based on Gaussian process methods are presented: an optimistic EC-GP-UCB algorithm that requires knowing the misspecification error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspecified.</p>
                <p><strong>Paper Abstract:</strong> We consider the problem of optimizing a black-box function based on noisy bandit feedback. Kernelized bandit algorithms have shown strong empirical and theoretical performance for this problem. They heavily rely on the assumption that the model is well-specified, however, and can fail without it. Instead, we introduce a \emph{misspecified} kernelized bandit setting where the unknown function can be $\epsilon$--uniformly approximated by a function with a bounded norm in some Reproducing Kernel Hilbert Space (RKHS). We design efficient and practical algorithms whose performance degrades minimally in the presence of model misspecification. Specifically, we present two algorithms based on Gaussian process (GP) methods: an optimistic EC-GP-UCB algorithm that requires knowing the misspecification error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspecification. We provide upper bounds on their cumulative regret in terms of $\epsilon$, the time horizon, and the underlying kernel, and we show that our algorithm achieves optimal dependence on $\epsilon$ with no prior knowledge of misspecification. In addition, in a stochastic contextual setting, we show that EC-GP-UCB can be effectively combined with the regret bound balancing strategy and attain similar regret bounds despite not knowing $\epsilon$.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EC-GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enlarged Confidence Gaussian Process Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernelized bandit algorithm that modifies GP-UCB by enlarging the UCB confidence width to account for model misspecification (known epsilon), using GP posterior mean/variance and an extra epsilon-dependent exploration bonus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EC-GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Gaussian process (GP) bandit algorithm using standard GP posterior mean mu_t^*(x) and variance sigma_t(x) estimators; selects actions by maximizing an enlarged upper confidence bound mu_{t-1}^*(x) + (beta_t + epsilon * sqrt(t) / sqrt(lambda)) * sigma_{t-1}(x). Key components: GP posterior updates (closed-form), analytic UCB acquisition augmented with an epsilon-dependent term, and standard kernel/ridge regularization (lambda).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Upper Confidence Bound (UCB) with confidence enlargement</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts exploration by increasing the UCB bonus proportional to epsilon * sqrt(t)/sqrt(lambda), which inflates epistemic uncertainty to compensate for systematic bias from model misspecification; uses current GP posterior mean and variance computed from observed noisy bandit feedback to choose the next query (optimism in face of uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Misspecified kernelized bandit environment (epsilon-uniform approximation to an RKHS)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown deterministic function f* on compact domain D; feedback is noisy bandit observations y_t = f*(x_t) + eta_t with independent sigma-sub-Gaussian noise; model misspecification: there exists an RKHS function f with ||f - f*||_infty <= epsilon (epsilon may be >0); action set can be continuous or finite; stochastic (i.i.d.) contexts in contextual variant.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Complexity characterized via maximum information gain gamma_T(k, D) (kernel-dependent). Examples: linear kernel gamma_T = O(d log T), SE kernel gamma_T = O((log T)^{d+1}). Action set dimension d, time horizon T; algorithm requires kernel k and RKHS norm bound B; computational costs dominated by GP matrix operations (naive O(t^3) but iterative updates available).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Cumulative regret R_T = O( B sqrt(gamma_T T) + sqrt((ln(1/delta)+gamma_T) gamma_T T) + epsilon T sqrt(gamma_T) ) with probability >= 1-delta. (First two terms are standard realizable GP-UCB rates; third term captures degradation from misspecification.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Standard GP-UCB (no epsilon correction) is no-regret in realizable cases (epsilon=0) but can suffer linear regret ~ Omega(epsilon T) in the misspecified setting; thus without the enlarged confidence term the algorithm can fail when epsilon>0.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample efficiency degrades linearly with epsilon: to keep per-round regret small requires T such that epsilon sqrt(gamma_T) is small relative to other terms; overall sample complexity is characterized via regret scaling above (no fixed sample-count threshold given).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances exploration/exploitation via UCB: beta_t * sigma_t encourages exploration; additional term (epsilon sqrt(t)/sqrt(lambda)) * sigma_t further increases exploration to hedge against misspecification bias — effectively more conservative (more exploration) as t grows when epsilon>0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared conceptually/analytically to GP-UCB (standard), corruption-robust GP-UCB ([4]), misspecified linear-bandit methods (OFUL variants, [24]), and lower bounds showing Omega(epsilon T) unavoidable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Introducing an explicit epsilon-dependent enlargement of the UCB yields regret guarantees that degrade gracefully with misspecification: R_T scales as standard GP-UCB plus an additive epsilon T sqrt(gamma_T) term; a lower bound shows Omega(epsilon T) dependence is unavoidable in general, so the provided scaling is near-optimal in epsilon up to the sqrt(gamma_T) factor. EC-GP-UCB requires knowing epsilon.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires prior knowledge of epsilon (misspecification level). The enlarged bonus can be conservative if epsilon is overestimated, increasing regret; relies on knowledge of kernel k and RKHS norm bound B; cannot adapt when epsilon is unknown (analysis breaks down because uncertainty at x* cannot be controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Misspecified Gaussian Process Bandit Optimization', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phased GP Uncertainty Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phased Gaussian Process Uncertainty Sampling (episodic elimination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic algorithm that performs phased epistemic uncertainty sampling (selects points of maximal GP posterior variance) and elimination of actions, designed to adapt to unknown misspecification epsilon while retaining near-optimal regret in the realizable case.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Phased GP Uncertainty Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Kernelized bandit algorithm that runs in episodes of doubling length m_e: within each episode it sequentially selects the point maximizing GP posterior variance sigma_{t-1}^2(x) (uncertainty sampling), then after receiving the episode's batch of observations computes GP posterior means and eliminates actions whose hallucinated UCB is below the max hallucinated LCB. Uses GP posterior variance updates (independent of observations during episode) and episodic reset to prior for the reduced action set.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Uncertainty sampling / experimental design (maximize posterior variance) combined with phased elimination</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>In each episode: (1) choose queries that maximize GP posterior variance (epistemic uncertainty) to reduce global uncertainty; (2) after the episode use observed noisy rewards to compute posterior means and eliminate actions that appear suboptimal under the (possibly misspecified) model; episodes double in length and the action set D_e is updated adaptively. No prior knowledge of epsilon is required; elimination is conservative in that it retains any x whose hallucinated UCB >= maximum hallucinated LCB.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Misspecified kernelized bandit environment (epsilon-uniform approximation to an RKHS)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as EC-GP-UCB: unknown f* on compact D, noisy sigma-sub-Gaussian bandit feedback, epsilon-misspecification where f* is within sup-norm epsilon of some RKHS function; action set may be infinite/continuous; stochastic noise; algorithm assumes fixed action set across episodes (contextual variant not directly applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Characterized by gamma_T(k,D) (max information gain). Episodes count E = O(log T). Episode lengths double (m_{e+1}=2 m_e). Kernel-dependent complexities: linear kernel gamma_T = O(d log T), SE kernel gamma_T = O((log T)^{d+1}).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Cumulative regret R_T^* = ~O( B sqrt(gamma_T T) + sqrt((ln(1/delta)+gamma_T) gamma_T T) + epsilon T sqrt(gamma_T) ) (tilde hides polylog factors) with probability >= 1-delta. Matches EC-GP-UCB rates up to polylog factors but does not require knowledge of epsilon.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline random or naive GP-UCB (without epsilon handling) can incur linear regret Theta(epsilon T) when misspecified; uncertainty sampling without elimination would not focus search and would yield worse practical regret (no explicit baseline numbers given).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample-efficiency matches GP-UCB in realizable case (epsilon=0) up to polylog factors; when epsilon>0, an unavoidable epsilon T sqrt(gamma_T) term appears, so more samples do not remove the linear-in-T degradation due to structural misspecification. Episodes require O(log T) rounds of restarting and elimination.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely exploration-driven within episodes (maximizes posterior variance) to reduce epistemic uncertainty; exploitation occurs only implicitly via elimination between episodes (retaining points with high hallucinated UCB). This trades short-term exploitation for more robust global exploration that tolerates unknown misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared analytically to GP-UCB (standard), EC-GP-UCB (requires known epsilon), and to phased elimination / G-experimental-design approaches in misspecified linear bandits [24]; also contrasted with corruption-robust GP algorithms [4].</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Provides an algorithm that adapts to unknown misspecification epsilon and attains (up to polylog factors) the same regret dependence on epsilon, T, and gamma_T as the EC-GP-UCB that requires epsilon; algorithm is applicable to infinite action sets and uses a simple uncertainty-sampling acquisition instead of expensive experimental-design computations. Shows that even if the optimal point may be eliminated, a near-optimal one is retained and total regret remains bounded by the stated expression.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Regret contains unavoidable epsilon T sqrt(gamma_T) term (i.e., structural misspecification causes linear degradation). The algorithm requires restarting to prior at each episode for the reduced action set and relies on monotonicity properties of sigma_t; has polylog factors in the bound. Elimination can remove the true optimizer in worst case (but analysis guarantees a near-optimal remaining point). Assumes fixed action set across rounds (harder to extend to contextual changing action sets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Misspecified Gaussian Process Bandit Optimization', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regret-bound Balancing (master) + EC-GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regret Bound Balancing Master Algorithm with EC-GP-UCB Base Learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A master algorithm that runs multiple EC-GP-UCB base learners instantiated with different candidate epsilon values and balances their candidate regret bounds to select which base to play, enabling adaptation to unknown epsilon in the contextual setting (i.i.d. action sets).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Regret-bound Balancing Master + EC-GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A two-level scheme: M EC-GP-UCB base learners, each run with a different hypothesized misspecification µhat_epsilon_i; a master elimination/selection algorithm (regret-bound balancing) selects which base to play each round by minimizing candidate regret bounds R_i(N_i(t-1)), tracks cumulative rewards per base, and eliminates inconsistent bases. Key components: multiple GP-UCB instances, a master multi-armed bandit elimination with statistically-backed elimination condition.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-adaptation via regret-bound balancing over UCB-based base learners (ensemble selection / model-selection)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Master selects the base algorithm i_t minimizing its candidate regret bound for the number of times it has been played; updates counts and rewards; eliminates base algorithms whose observed cumulative reward plus their candidate bound falls below the best algorithm's lower bound (with concentration terms). This balances exploration across bases and adapts to unknown epsilon by eventually concentrating on a consistent base with sufficiently large guessed epsilon.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Contextual misspecified kernelized bandit (i.i.d. action sets/contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>At each round t the learner sees an action set D_t drawn i.i.d. (context), must pick x_t in D_t, receives y_t = f*(x_t) + eta_t. f* is epsilon-misspecified relative to RKHS class. Noise is 1-sub-Gaussian and rounds independent. The environment is stochastic with i.i.d. contexts; action sets may vary each round.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Complexity again described via gamma_T for the kernel over the global domain. Master runs M = ceil(1 + 0.5 log_2(T / gamma_T^2)) EC-GP-UCB bases, candidate bounds require computing/approximating gamma_T. Additional overhead: log factor in number of bases and extra additive terms in final regret bound.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Contextual regret bound (with high probability) R_T^* = ~O( (B sqrt(gamma_T)+gamma_T) sqrt(T) + epsilon T sqrt(gamma_T) + (B sqrt(gamma_T)+gamma_T)^2 ), up to polylog factors and constants. This recovers, up to lower-order terms, the EC-GP-UCB performance that would be obtained with known epsilon.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>If a single EC-GP-UCB is run with incorrect epsilon, it may be inconsistent and yield larger regret (potentially linear-in-T in misspecified context). Random master selection or naive single-base selection would not achieve the balanced guarantee; exact baselines not numerically provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Master adds multiplicative/logarithmic overhead (M bases, elimination costs) but achieves near-oracle performance with only an O(log T) multiplicative search over candidate epsilons. Sample-efficiency matches best base (with correct epsilon) up to polylog factors and master-selection overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff is handled at two levels: within-base EC-GP-UCB uses enlarged UCB exploration bonuses; master balances playing bases to keep their candidate regret bounds roughly equal (thereby exploring algorithm choices) and eliminates inconsistent bases to exploit best-performing base. Master elimination uses concentration terms to avoid eliminating consistent bases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Built on and compared to regret bound balancing / CORRAL-like aggregation literature (e.g., [32], [33]); base comparisons to EC-GP-UCB with known epsilon and other contextual bandit model-selection approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>By aggregating multiple EC-GP-UCB instances and using a regret-bound balancing master, the authors achieve contextual misspecified regret bounds comparable to the oracle EC-GP-UCB that knows epsilon; this provides a practical method to handle unknown epsilon in contextual stochastic action-set settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires i.i.d. assumption on contexts/action sets (stochastic contexts) to ensure valid elimination; needs computing or approximating gamma_T (kernel-dependent) and runs multiple GP instances increasing computational cost; final bound includes additive lower-order terms (e.g., (B sqrt(gamma_T)+gamma_T)^2) and multiplicative log factors from M bases, which may be non-negligible in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Misspecified Gaussian Process Bandit Optimization', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gaussian process optimization in the bandit setting: No regret and experimental design <em>(Rating: 2)</em></li>
                <li>Learning with good feature representations in bandits and in RL with a generative model <em>(Rating: 2)</em></li>
                <li>Corruption-tolerant Gaussian process bandit optimization <em>(Rating: 2)</em></li>
                <li>High-dimensional experimental design and kernel bandits <em>(Rating: 2)</em></li>
                <li>On kernelized multi-armed bandits <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1283",
    "paper_id": "paper-31d3916cb5ce9f1acd23670741eebfda7458ae39",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "EC-GP-UCB",
            "name_full": "Enlarged Confidence Gaussian Process Upper Confidence Bound",
            "brief_description": "A kernelized bandit algorithm that modifies GP-UCB by enlarging the UCB confidence width to account for model misspecification (known epsilon), using GP posterior mean/variance and an extra epsilon-dependent exploration bonus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EC-GP-UCB",
            "agent_description": "A Gaussian process (GP) bandit algorithm using standard GP posterior mean mu_t^*(x) and variance sigma_t(x) estimators; selects actions by maximizing an enlarged upper confidence bound mu_{t-1}^*(x) + (beta_t + epsilon * sqrt(t) / sqrt(lambda)) * sigma_{t-1}(x). Key components: GP posterior updates (closed-form), analytic UCB acquisition augmented with an epsilon-dependent term, and standard kernel/ridge regularization (lambda).",
            "adaptive_design_method": "Upper Confidence Bound (UCB) with confidence enlargement",
            "adaptation_strategy_description": "Adapts exploration by increasing the UCB bonus proportional to epsilon * sqrt(t)/sqrt(lambda), which inflates epistemic uncertainty to compensate for systematic bias from model misspecification; uses current GP posterior mean and variance computed from observed noisy bandit feedback to choose the next query (optimism in face of uncertainty).",
            "environment_name": "Misspecified kernelized bandit environment (epsilon-uniform approximation to an RKHS)",
            "environment_characteristics": "Unknown deterministic function f* on compact domain D; feedback is noisy bandit observations y_t = f*(x_t) + eta_t with independent sigma-sub-Gaussian noise; model misspecification: there exists an RKHS function f with ||f - f*||_infty &lt;= epsilon (epsilon may be &gt;0); action set can be continuous or finite; stochastic (i.i.d.) contexts in contextual variant.",
            "environment_complexity": "Complexity characterized via maximum information gain gamma_T(k, D) (kernel-dependent). Examples: linear kernel gamma_T = O(d log T), SE kernel gamma_T = O((log T)^{d+1}). Action set dimension d, time horizon T; algorithm requires kernel k and RKHS norm bound B; computational costs dominated by GP matrix operations (naive O(t^3) but iterative updates available).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Cumulative regret R_T = O( B sqrt(gamma_T T) + sqrt((ln(1/delta)+gamma_T) gamma_T T) + epsilon T sqrt(gamma_T) ) with probability &gt;= 1-delta. (First two terms are standard realizable GP-UCB rates; third term captures degradation from misspecification.)",
            "performance_without_adaptation": "Standard GP-UCB (no epsilon correction) is no-regret in realizable cases (epsilon=0) but can suffer linear regret ~ Omega(epsilon T) in the misspecified setting; thus without the enlarged confidence term the algorithm can fail when epsilon&gt;0.",
            "sample_efficiency": "Sample efficiency degrades linearly with epsilon: to keep per-round regret small requires T such that epsilon sqrt(gamma_T) is small relative to other terms; overall sample complexity is characterized via regret scaling above (no fixed sample-count threshold given).",
            "exploration_exploitation_tradeoff": "Balances exploration/exploitation via UCB: beta_t * sigma_t encourages exploration; additional term (epsilon sqrt(t)/sqrt(lambda)) * sigma_t further increases exploration to hedge against misspecification bias — effectively more conservative (more exploration) as t grows when epsilon&gt;0.",
            "comparison_methods": "Compared conceptually/analytically to GP-UCB (standard), corruption-robust GP-UCB ([4]), misspecified linear-bandit methods (OFUL variants, [24]), and lower bounds showing Omega(epsilon T) unavoidable.",
            "key_results": "Introducing an explicit epsilon-dependent enlargement of the UCB yields regret guarantees that degrade gracefully with misspecification: R_T scales as standard GP-UCB plus an additive epsilon T sqrt(gamma_T) term; a lower bound shows Omega(epsilon T) dependence is unavoidable in general, so the provided scaling is near-optimal in epsilon up to the sqrt(gamma_T) factor. EC-GP-UCB requires knowing epsilon.",
            "limitations_or_failures": "Requires prior knowledge of epsilon (misspecification level). The enlarged bonus can be conservative if epsilon is overestimated, increasing regret; relies on knowledge of kernel k and RKHS norm bound B; cannot adapt when epsilon is unknown (analysis breaks down because uncertainty at x* cannot be controlled).",
            "uuid": "e1283.0",
            "source_info": {
                "paper_title": "Misspecified Gaussian Process Bandit Optimization",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Phased GP Uncertainty Sampling",
            "name_full": "Phased Gaussian Process Uncertainty Sampling (episodic elimination)",
            "brief_description": "An episodic algorithm that performs phased epistemic uncertainty sampling (selects points of maximal GP posterior variance) and elimination of actions, designed to adapt to unknown misspecification epsilon while retaining near-optimal regret in the realizable case.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Phased GP Uncertainty Sampling",
            "agent_description": "Kernelized bandit algorithm that runs in episodes of doubling length m_e: within each episode it sequentially selects the point maximizing GP posterior variance sigma_{t-1}^2(x) (uncertainty sampling), then after receiving the episode's batch of observations computes GP posterior means and eliminates actions whose hallucinated UCB is below the max hallucinated LCB. Uses GP posterior variance updates (independent of observations during episode) and episodic reset to prior for the reduced action set.",
            "adaptive_design_method": "Uncertainty sampling / experimental design (maximize posterior variance) combined with phased elimination",
            "adaptation_strategy_description": "In each episode: (1) choose queries that maximize GP posterior variance (epistemic uncertainty) to reduce global uncertainty; (2) after the episode use observed noisy rewards to compute posterior means and eliminate actions that appear suboptimal under the (possibly misspecified) model; episodes double in length and the action set D_e is updated adaptively. No prior knowledge of epsilon is required; elimination is conservative in that it retains any x whose hallucinated UCB &gt;= maximum hallucinated LCB.",
            "environment_name": "Misspecified kernelized bandit environment (epsilon-uniform approximation to an RKHS)",
            "environment_characteristics": "Same as EC-GP-UCB: unknown f* on compact D, noisy sigma-sub-Gaussian bandit feedback, epsilon-misspecification where f* is within sup-norm epsilon of some RKHS function; action set may be infinite/continuous; stochastic noise; algorithm assumes fixed action set across episodes (contextual variant not directly applicable).",
            "environment_complexity": "Characterized by gamma_T(k,D) (max information gain). Episodes count E = O(log T). Episode lengths double (m_{e+1}=2 m_e). Kernel-dependent complexities: linear kernel gamma_T = O(d log T), SE kernel gamma_T = O((log T)^{d+1}).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Cumulative regret R_T^* = ~O( B sqrt(gamma_T T) + sqrt((ln(1/delta)+gamma_T) gamma_T T) + epsilon T sqrt(gamma_T) ) (tilde hides polylog factors) with probability &gt;= 1-delta. Matches EC-GP-UCB rates up to polylog factors but does not require knowledge of epsilon.",
            "performance_without_adaptation": "Baseline random or naive GP-UCB (without epsilon handling) can incur linear regret Theta(epsilon T) when misspecified; uncertainty sampling without elimination would not focus search and would yield worse practical regret (no explicit baseline numbers given).",
            "sample_efficiency": "Sample-efficiency matches GP-UCB in realizable case (epsilon=0) up to polylog factors; when epsilon&gt;0, an unavoidable epsilon T sqrt(gamma_T) term appears, so more samples do not remove the linear-in-T degradation due to structural misspecification. Episodes require O(log T) rounds of restarting and elimination.",
            "exploration_exploitation_tradeoff": "Purely exploration-driven within episodes (maximizes posterior variance) to reduce epistemic uncertainty; exploitation occurs only implicitly via elimination between episodes (retaining points with high hallucinated UCB). This trades short-term exploitation for more robust global exploration that tolerates unknown misspecification.",
            "comparison_methods": "Compared analytically to GP-UCB (standard), EC-GP-UCB (requires known epsilon), and to phased elimination / G-experimental-design approaches in misspecified linear bandits [24]; also contrasted with corruption-robust GP algorithms [4].",
            "key_results": "Provides an algorithm that adapts to unknown misspecification epsilon and attains (up to polylog factors) the same regret dependence on epsilon, T, and gamma_T as the EC-GP-UCB that requires epsilon; algorithm is applicable to infinite action sets and uses a simple uncertainty-sampling acquisition instead of expensive experimental-design computations. Shows that even if the optimal point may be eliminated, a near-optimal one is retained and total regret remains bounded by the stated expression.",
            "limitations_or_failures": "Regret contains unavoidable epsilon T sqrt(gamma_T) term (i.e., structural misspecification causes linear degradation). The algorithm requires restarting to prior at each episode for the reduced action set and relies on monotonicity properties of sigma_t; has polylog factors in the bound. Elimination can remove the true optimizer in worst case (but analysis guarantees a near-optimal remaining point). Assumes fixed action set across rounds (harder to extend to contextual changing action sets).",
            "uuid": "e1283.1",
            "source_info": {
                "paper_title": "Misspecified Gaussian Process Bandit Optimization",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "Regret-bound Balancing (master) + EC-GP-UCB",
            "name_full": "Regret Bound Balancing Master Algorithm with EC-GP-UCB Base Learners",
            "brief_description": "A master algorithm that runs multiple EC-GP-UCB base learners instantiated with different candidate epsilon values and balances their candidate regret bounds to select which base to play, enabling adaptation to unknown epsilon in the contextual setting (i.i.d. action sets).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Regret-bound Balancing Master + EC-GP-UCB",
            "agent_description": "A two-level scheme: M EC-GP-UCB base learners, each run with a different hypothesized misspecification µhat_epsilon_i; a master elimination/selection algorithm (regret-bound balancing) selects which base to play each round by minimizing candidate regret bounds R_i(N_i(t-1)), tracks cumulative rewards per base, and eliminates inconsistent bases. Key components: multiple GP-UCB instances, a master multi-armed bandit elimination with statistically-backed elimination condition.",
            "adaptive_design_method": "Meta-adaptation via regret-bound balancing over UCB-based base learners (ensemble selection / model-selection)",
            "adaptation_strategy_description": "Master selects the base algorithm i_t minimizing its candidate regret bound for the number of times it has been played; updates counts and rewards; eliminates base algorithms whose observed cumulative reward plus their candidate bound falls below the best algorithm's lower bound (with concentration terms). This balances exploration across bases and adapts to unknown epsilon by eventually concentrating on a consistent base with sufficiently large guessed epsilon.",
            "environment_name": "Contextual misspecified kernelized bandit (i.i.d. action sets/contexts)",
            "environment_characteristics": "At each round t the learner sees an action set D_t drawn i.i.d. (context), must pick x_t in D_t, receives y_t = f*(x_t) + eta_t. f* is epsilon-misspecified relative to RKHS class. Noise is 1-sub-Gaussian and rounds independent. The environment is stochastic with i.i.d. contexts; action sets may vary each round.",
            "environment_complexity": "Complexity again described via gamma_T for the kernel over the global domain. Master runs M = ceil(1 + 0.5 log_2(T / gamma_T^2)) EC-GP-UCB bases, candidate bounds require computing/approximating gamma_T. Additional overhead: log factor in number of bases and extra additive terms in final regret bound.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Contextual regret bound (with high probability) R_T^* = ~O( (B sqrt(gamma_T)+gamma_T) sqrt(T) + epsilon T sqrt(gamma_T) + (B sqrt(gamma_T)+gamma_T)^2 ), up to polylog factors and constants. This recovers, up to lower-order terms, the EC-GP-UCB performance that would be obtained with known epsilon.",
            "performance_without_adaptation": "If a single EC-GP-UCB is run with incorrect epsilon, it may be inconsistent and yield larger regret (potentially linear-in-T in misspecified context). Random master selection or naive single-base selection would not achieve the balanced guarantee; exact baselines not numerically provided.",
            "sample_efficiency": "Master adds multiplicative/logarithmic overhead (M bases, elimination costs) but achieves near-oracle performance with only an O(log T) multiplicative search over candidate epsilons. Sample-efficiency matches best base (with correct epsilon) up to polylog factors and master-selection overhead.",
            "exploration_exploitation_tradeoff": "Tradeoff is handled at two levels: within-base EC-GP-UCB uses enlarged UCB exploration bonuses; master balances playing bases to keep their candidate regret bounds roughly equal (thereby exploring algorithm choices) and eliminates inconsistent bases to exploit best-performing base. Master elimination uses concentration terms to avoid eliminating consistent bases.",
            "comparison_methods": "Built on and compared to regret bound balancing / CORRAL-like aggregation literature (e.g., [32], [33]); base comparisons to EC-GP-UCB with known epsilon and other contextual bandit model-selection approaches.",
            "key_results": "By aggregating multiple EC-GP-UCB instances and using a regret-bound balancing master, the authors achieve contextual misspecified regret bounds comparable to the oracle EC-GP-UCB that knows epsilon; this provides a practical method to handle unknown epsilon in contextual stochastic action-set settings.",
            "limitations_or_failures": "Requires i.i.d. assumption on contexts/action sets (stochastic contexts) to ensure valid elimination; needs computing or approximating gamma_T (kernel-dependent) and runs multiple GP instances increasing computational cost; final bound includes additive lower-order terms (e.g., (B sqrt(gamma_T)+gamma_T)^2) and multiplicative log factors from M bases, which may be non-negligible in practice.",
            "uuid": "e1283.2",
            "source_info": {
                "paper_title": "Misspecified Gaussian Process Bandit Optimization",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "rating": 2
        },
        {
            "paper_title": "Learning with good feature representations in bandits and in RL with a generative model",
            "rating": 2
        },
        {
            "paper_title": "Corruption-tolerant Gaussian process bandit optimization",
            "rating": 2
        },
        {
            "paper_title": "High-dimensional experimental design and kernel bandits",
            "rating": 2
        },
        {
            "paper_title": "On kernelized multi-armed bandits",
            "rating": 1
        }
    ],
    "cost": 0.016337749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Misspecified Gaussian Process Bandit Optimization</h1>
<p>Ilija Bogunovic<br>ETH Zürich</p>
<p>Andreas Krause<br>ETH Zürich</p>
<h4>Abstract</h4>
<p>We consider the problem of optimizing a black-box function based on noisy bandit feedback. Kernelized bandit algorithms have shown strong empirical and theoretical performance for this problem. They heavily rely on the assumption that the model is well-specified, however, and can fail without it. Instead, we introduce a misspecified kernelized bandit setting where the unknown function can be $\epsilon$-uniformly approximated by a function with a bounded norm in some Reproducing Kernel Hilbert Space (RKHS). We design efficient and practical algorithms whose performance degrades minimally in the presence of model misspecification. Specifically, we present two algorithms based on Gaussian process (GP) methods: an optimistic EC-GP-UCB algorithm that requires knowing the misspecification error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspecification. We provide upper bounds on their cumulative regret in terms of $\epsilon$, the time horizon, and the underlying kernel, and we show that our algorithm achieves optimal dependence on $\epsilon$ with no prior knowledge of misspecification. In addition, in a stochastic contextual setting, we show that EC-GP-UCB can be effectively combined with the regret bound balancing strategy and attain similar regret bounds despite not knowing $\epsilon$.</p>
<h2>1 Introduction</h2>
<p>Bandit optimization has been successfully used in a great number of machine learning and real-world applications, e.g., in mobile health [42], environmental monitoring [40], economics [27], hyperparameter tuning [26], to name a few. To scale to large or continuous domains, modern bandit approaches try to model and exploit the problem structure that is often manifested as correlations in rewards of "similar" actions. Hence, the key idea of kernelized bandits is to consider only smooth reward functions of a low norm belonging to a chosen Reproducing Kernel Hilbert Space (RKHS) of functions. This permits the application of flexible nonparametric Gaussian process (GP) models and Bayesian optimization methods via a well-studied link between RKHS functions and GPs (see, e.g., [18] for a concise review).</p>
<p>A vast majority of previous works on nonparametric kernelized bandits have focused on designing algorithms and theoretical bounds on the standard notions of regret (see, e.g., [40, 9, 35]). However, they solely focus on the realizable (i.e., well-specified) case in which one assumes perfect knowledge of the true function class. For example, the analysis of the prominent GP-UCB [40] algorithm assumes the model to be well-specified and ignores potential misspecification issues. As the realizability assumption may be too restrictive in real applications, we focus on the case where it may only hold approximately. In practice, model misspecifications can arise due to various reasons, such as incorrect choice of kernel, consideration of an overly smooth function class, hyperparameter estimation errors, etc. Hence, an open question is to characterize the impact of model misspecification in the kernelized setting, and to design robust algorithms whose performance degrades optimally with the increasing level of misspecification.</p>
<p>In this paper, we study the GP bandit problem with model misspecification in which the true unknown function might be $\epsilon$-far (as measured in the max norm) from a member of the learner's assumed hypothesis class. We propose a novel GP bandit algorithm and regret bounds that depend on the misspecification error, time horizon, and underlying kernel. Specifically, we present an algorithm that is based on the classical uncertainty sampling approach that is frequently used in Bayesian optimization and experimental design. Importantly, our main presented algorithm assumes no knowledge of the misspecification error $\epsilon$ and achieves standard regret rates in the realizable case.
Related work on GP bandits. GP bandit algorithms have received significant attention in recent years (e.g., $[40,11,9,39])$. While the most popular approaches in the stochastic setting rely on upper confidence bound ( $U C B$ ) and Thompson sampling strategies, a number of works also consider uncertainty sampling procedures (e.g., [10, 41, 6]). Beyond the standard setting, numerous works have also considered the contextual bandit setting (e.g., [22, 44, 21]), while the case of unknown kernel hyperparameters and misspecified smoothness has been studied in, e.g., [46, 3, 47]. [46, 3] assume that the reward function is smooth as measured by the known (or partially known) kernel, while in our problem, we allow the unknown function to lie outside a given kernel's reproducing space.
Related corruption-robust GP bandits in which an adversary can additively perturb the observed rewards are recently considered in [4]. In Section 2, we also discuss how the misspecified problem can be seen from this perspective, where the corruption function is fixed and the adversarial budget scales with the time horizon. While the focus of [4] is on protecting against an adaptive adversary and thus designing randomized algorithms and regret bounds that depend on the adversarial budget, we propose deterministic algorithms and analyze the impact of misspecification.
Apart from corruptions, several works have considered other robust aspects in GP bandits, such as designing robust strategies against the shift in uncontrollable covariates [5, 20, 31, 38, 7]. While they report robust regret guarantees, they still assume the realizable case only. Our goal of attaining small regret despite the wrong hypothesis class requires very different techniques from these previous works.
Related work on misspecified linear bandits. Recently, works on reinforcement learning with misspecified linear features (e.g., [12, 45, 17]) have renewed interest in the related misspecified linear bandits (e.g., [48], [24], [30], [15]) first introduced in [16]. In [16], the authors show that standard algorithms must suffer $\Omega(\epsilon T)$ regret under an additive $\epsilon$-perturbation of the linear model. Recently, [48] propose a robust variant of OFUL [1] that requires knowing the misspecification parameter $\epsilon$. In particular, their algorithm obtains a high-probability $\tilde{O}(d \sqrt{T}+\epsilon \sqrt{d} T)$ regret bound. In [24], the authors propose another arm-elimination algorithm based on G-experimental design. Unlike the previous works, their algorithm is agnostic to the misspecification level, and its performance matches the lower bounds. As shown in [24], when the number of actions $K$ is large $(K \gg d)$, the "price" of $\epsilon$-misspecification must grow as $\Omega(\epsilon \sqrt{d} T) .{ }^{1}$ Our main algorithm is inspired by the proposed technique for the finite-arm misspecified linear bandit setting [24]. It works in the more general kernelized setting, uses a simpler data acquisition rule often used in Bayesian optimization and experimental design, and recovers the same optimal guarantees when instantiated with linear kernel.
Several works have recently considered the misspecified contextual linear bandit problem with unknown model misspecification $\epsilon$. [14] introduce a new family of algorithms that require access to an online oracle for square loss regression and address the case of adversarial contexts. Concurrent work of [33] solves the case when contexts / action sets are stochastic. Both works ([14] and [33]) leverage CORRAL-type aggregation [2] of contextual bandit algorithms and achieve the optimal $\tilde{O}(\sqrt{d} T \epsilon+d \sqrt{T})$ regret bound. Finally, in [32], the authors present a practical master algorithm that plays base algorithms that come with a candidate regret bound that may not hold during all rounds. The master algorithm plays base algorithms in a balanced way and suitably eliminates algorithms whose regret bound is no longer valid. Similarly to the previous works that rely on the CORRAL-type master algorithms, we use the balancing master algorithm of [33] together with our GP-bandit base algorithm to provide contextual misspecified regret bounds.
Around the time of the submission of this work, a related approach that among others also considers misspecified kernel bandits appeared online [8]. [8, Theorem 3] contains the same regret scaling due to misspecification as we obtain in our results. The main difference between the two works is in the proposed algorithms and analysis techniques. Our approach does not require robust estimators</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and simply uses the standard ones (i.e., GP posterior/ kernelized ridge regression mean and variance estimators) that can be computed in the closed-form. It can also handle infinite action sets similarly to the classical Bayesian optimization algorithms; it utilizes a single acquisition function that, in practice, can be maximized via standard off-the-shelf global optimization solvers. We also present a complete treatment of the misspecified problem by showing the failure of standard UCB approaches, algorithms for known and unknown $\epsilon$, an impossibility result, and extensions to the contextual bandit setting. Finally, our main algorithm (Algorithm 2) demonstrates the use of a different acquisition function in comparison to [24, 8] that relies on standard and non-robust estimators.</p>
<p>Contributions. In this paper, we systematically handle model misspecification in GP bandits. Specifically, this paper makes the following contributions:</p>
<ul>
<li>We introduce a misspecified kernelized bandit problem, and for known misspecification error $\epsilon$, we present the EC-GP-UCB algorithm with enlarged confidence bounds that achieves cumulative $R_{T}=O\left(B \sqrt{\gamma_{T} T}+\gamma_{T} \sqrt{T}+\epsilon T \sqrt{\gamma_{T}}\right)$ regret. Our simple lower bound argument shows that $\Omega(T \epsilon)$ regret is unavoidable in the general kernelized setting.</li>
<li>For when $\epsilon$ is unknown, we propose another algorithm based on uncertainty sampling and phased exploration that achieves (up to polylog factors) the previous regret rates in the misspecified setting, and standard regret guarantees in the realizable case (when $\epsilon=0$ ).</li>
<li>Finally, we consider a misspecified contextual kernelized problem, and show that when action sets are stochastic, our EC-GP-UCB algorithm can be effectively combined with the regret bound balancing strategy from [33] to achieve previous regret bounds (up to some additive lower order terms).</li>
</ul>
<h1>2 Problem statement</h1>
<p>We consider the problem of sequentially maximizing some black-box reward function $f^{<em>}: \mathcal{D} \rightarrow \mathbb{R}$ over a known compact set of actions $\mathcal{D} \subset \mathbb{R}^{d}$. To learn about $f^{</em>}$, the learner relies on sequential noisy bandit feedback, i.e., at every round $t$, the learner selects $x_{t} \in \mathcal{D}$ and obtains a noisy observation</p>
<p>$$
y_{t}^{<em>}=f^{</em>}\left(x_{t}\right)+\eta_{t}
$$</p>
<p>where we assume independent (over rounds) $\sigma$-sub-Gaussian noise (see Appendix A for definition).
Let $k(\cdot, \cdot)$ denote a positive definite kernel function defined on $\mathcal{D} \times \mathcal{D}$ and $\mathcal{H}_{k}(\mathcal{D})$ be its associated Reproducing Kernel Hilbert Space (RKHS) of well-behaved functions. Suppose that before making any decision, the learner is provided with a hypothesis class</p>
<p>$$
\mathcal{F}<em k="k">{k}(\mathcal{D} ; B)=\left{f \in \mathcal{H}</em> \leq B\right}
$$}(\mathcal{D}):|f|_{k</p>
<p>where every member function has a bounded RKHS norm $|f|<em k="k">{k}=\sqrt{\langle f, f\rangle}$ for some $B&gt;0$, that measures the complexity of $f$ with respect to kernel $k(\cdot, \cdot)$. We consider kernel functions such that $k(x, x) \leq 1$ for every $x \in \mathcal{D}$. Most commonly used kernel functions that satisfy this property are outlined in Appendix A.
The standard setting assumes a realizable (i.e., well-specified) scenario in which $f^{<em>} \in \mathcal{F}_{k}(\mathcal{D} ; B)$, i.e., the unknown function is a member of the known RKHS with bounded norm. In contrast, in the misspecified setting, we assume that the learner is informed that $f^{</em>}$ can be uniformly approximated by a member from the given hypothesis class $\mathcal{F}</em> ; B)$, i.e.,}(\mathcal{D</p>
<p>$$
\min <em k="k">{f \in \mathcal{F}</em> \leq \epsilon
$$}(\mathcal{D} ; B)}\left|f-f^{*}\right|_{\infty</p>
<p>for some $\epsilon&gt;0$ and max-norm $|\cdot|<em k="k">{\infty}$. Here, we note that if two functions are close in RKHS norm, then they are also pointwise close but the reverse does not need to be true. Hence, the true function $f^{*}$ can in principle have a significantly larger RKHS norm than any $f$ from $\mathcal{F}</em> ; B)$, or it might not even belong to the considered RKHS.}(\mathcal{D</p>
<p>We also note that in the case of continuous universal kernels [28] (i.e., due to the universal function approximation property of such kernels), any continuous function $f^{*}$ on $\mathcal{D}$ satisfies the above assumption (Eq. (3)) for any $\epsilon&gt;0$ and $\mathcal{F}_{k}(\mathcal{D} ; B)$ with suitably large RKHS norm bound $B$. This is a difference in comparison to the previously studied misspecified linear setting, and another motivation to study the misspecified kernelized problem.</p>
<p>As in the standard setting (which corresponds to the case when $\epsilon=0$ ), we assume that $k(\cdot, \cdot)$ and $B$ are known to the learner. The goal of the learner is to minimize the cumulative regret</p>
<p>$$
R_{T}^{<em>}=\sum_{t=1}^{T}\left(\max _{x \in \mathcal{D}} f^{</em>}(x)-f^{*}\left(x_{t}\right)\right)
$$</p>
<p>where $T$ is a time horizon. We denote instantaneous regret at round $t$ as $r_{t}^{<em>}=f^{</em>}\left(x^{<em>}\right)-f^{</em>}\left(x_{t}\right)$, where $x^{<em>} \in \arg \max _{x \in \mathcal{D}} f^{</em>}(x)$, and note that $R_{T}^{<em>}=\sum_{t=1}^{T} r_{t}^{</em>}$. The learner's algorithm knows $\mathcal{F}_{k}(\mathcal{D} ; B)$, meaning that it takes $\mathcal{D}, k(\cdot, \cdot)$ and $B$ as input. Crucially, in our most general considered problem variant, the exact value of misspecification $\epsilon$ is assumed to be unknown, and so we seek an algorithm that can adapt to any $\epsilon&gt;0$, including the realizable case when $\epsilon=0$.
Alternatively, one can consider competing with a best-in-class benchmark, i.e.,</p>
<p>$$
R_{T}=\sum_{t=1}^{T}\left(\max <em t="t">{x \in \mathcal{D}} \tilde{f}(x)-\tilde{f}\left(x</em>\right)\right)
$$</p>
<p>where</p>
<p>$$
\tilde{f} \in \underset{f \in \mathcal{F}<em _infty="\infty">{k}(\mathcal{D} ; B)}{\arg \min }\left|f-f^{*}\right|</em>
$$</p>
<p>and $\left|\tilde{f}(x)-f^{*}(x)\right| \leq \epsilon$ for every $x \in \mathcal{D}$. Here, the goal is to minimize cumulative regret in case the true unknown objective is $\tilde{f} \in \mathcal{F}_{k}(\mathcal{D} ; B)$, while noisy observations of $\tilde{f}$ that the learner receives are at most $\epsilon$-misspecified, i.e.,</p>
<p>$$
y_{t}^{<em>}=\underbrace{\tilde{f}\left(x_{t}\right)+m\left(x_{t}\right)}_{f^{</em>}\left(x_{t}\right)}+\eta_{t}
$$</p>
<p>where $m(\cdot): \mathcal{D} \rightarrow[-\epsilon, \epsilon]$ is a fixed and unknown function. Since it holds that $\left|\tilde{f}-f^{<em>}\right|<em T="T">{\infty} \leq \epsilon$, the absolute difference between $R</em>^{}$ and $R_{T</em>}$ is at most $2 \epsilon T$, and as will already become clear in Section 3.2, this difference will have no significant impact on the scalings in our main bound (since $R_{T}^{*}=\Omega(\epsilon T))$. In our theoretical analysis, we will interchangeably use both regret definitions (from Eqs. (4) and (5)).</p>
<h1>3 Algorithms for misspecified kernelized bandits</h1>
<p>We start this section by recalling a Gaussian Process (GP) framework for learning RKHS functions. Then, we present different GP-bandit algorithms and theoretical regret bounds for misspecified settings of increasing levels of difficulty.</p>
<h3>3.1 Learning with Gaussian Processes</h3>
<p>In the realizable case (i.e., when the true unknown $f \in \mathcal{F}<em k="k">{k}(\mathcal{D} ; B)$ and the learner knows the true hypothesis space $\mathcal{F}</em>$, is a collection of random variables $(f(x))}(\mathcal{D} ; B)$ ) and under the noise model described in Eq. (1), uncertainty modeling and learning in standard GP-bandit algorithms can be viewed through the lens of Gaussian Process models. A Gaussian Process $G P(\mu(\cdot), k(\cdot, \cdot))$ over the input domain $\mathcal{D<em i="i">{x \in \mathcal{D}}$ where every finite number of them $\left(f\left(x</em>\right)\right)<em i="i">{i=1}^{n}, n \in \mathbb{N}$, is jointly Gaussian with mean $\mathbb{E}\left[f\left(x</em>\right}$, the posterior distribution under previous assumptions is also Gaussian with the mean and variance that can be computed in closed-form as:}\right)\right]=\mu\left(x_{i}\right)$ and covariance $\mathbb{E}\left[\left(f\left(x_{i}\right)-\mu\left(x_{i}\right)\right)\left(f\left(x_{j}\right)-\mu\left(x_{j}\right)\right)\right]=k\left(x_{i}, x_{j}\right)$ for every $1 \leq i, j \leq n$. Standard algorithms implicitly use a zero-mean $G P(0, k(\cdot, \cdot))$ as the prior distribution over $f$, i.e., $f \sim G P(0, k(\cdot, \cdot))$, and assume that the noise variables are drawn independently across $t$ from $\mathcal{N}(0, \lambda)$ with $\lambda&gt;0$. After collecting new data, that is, a sequence of actions $\left{x_{1}, \ldots, x_{t}\right}$ and their corresponding noisy observations $\left{y_{1}, \ldots, y_{t</p>
<p>$$
\begin{aligned}
\mu_{t}(x) &amp; =k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} Y_{t} \
\sigma_{t}^{2}(x) &amp; =k(x, x)-k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} k_{t}(x)
\end{aligned}
$$</p>
<p>where $k_{t}(x)=\left[k\left(x_{1}, x\right), \ldots, k\left(x_{t}, x\right)\right]^{T} \in \mathbb{R}^{t \times 1}, K_{t}=\left[k\left(x_{s}, x_{s^{\prime}}\right)\right]<em t="t">{s, s^{\prime} \geq t} \in \mathbb{R}^{t \times t}$ is the corresponding kernel matrix, and $Y</em>\right]$ denotes a vector of observations.}:=\left[y_{1}, \ldots, y_{t</p>
<p>The previous standard modeling assumptions lead itself to model misspecifications as GP samples are rougher than RKHS functions and are not contained in $\mathcal{H}<em k="k">{k}(\mathcal{D})$ with high probability. Although this leads to a mismatched hypothesis space, GPs and RKHS functions are closely related (see, e.g., [18]) when used with same kernel function, and it is possible to use GP models to infer reliable confidence intervals on the unknown $f \in \mathcal{F}</em>\right}}(\mathcal{D} ; B)$. Under this assumption, the popular algorithms such as GP-UCB [40] construct statistical confidence bounds that contain $f$ with high probability uniformly over time horizon, i.e., the following holds $\left|f(x)-\mu_{t-1}(x)\right| \leq \beta_{t} \sigma_{t-1}(x)$ for every $t \geq 1$ and $x \in \mathcal{D}$. Here, $\left{\beta_{t<em t="t">{t \leq T}$ stands for the sequence of parameters that are suitably set (see Lemma 1) to (i) trade-off between exploration and exploitation and (ii) ensure the validity of the confidence bounds. In every round $t$, GP-UCB then queries the unknown function at a point $x</em> / T \rightarrow 0$ as $T \rightarrow \infty$. However, in the misspecified setting, the previous standard confidence bounds are no longer valid and one needs to consider different strategies.
Before moving to the misspecified case, we recall an important kernel-dependent quantity known as maximum information gain $\gamma_{t}(k, \mathcal{D})$ [40] that is frequently used to characterize the regret bounds. ${ }^{2}$ It stands for the maximum amount of information that a set of noisy observations can reveal about the unknown function sampled from a zero-mean Gaussian process with kernel $k$. Specifically, for a set of points $S \subset \mathcal{D}$, we use $f_{S}$ to denote a random vector $[f(x)]} \in \mathcal{D}$ that maximizes the upper confidence bound given by $\mu_{t-1}(\cdot)+\beta_{t} \sigma_{t-1}(\cdot)$, with $\mu_{t-1}(\cdot)$ and $\sigma_{t-1}(\cdot)$ as defined in Eqs. (8) and (9). In the standard setting, GP-UCB is no regret, meaning that $R_{T<em S="S">{x \in S}$, and $Y</em>(0, \lambda I)$. The maximum information gain is then defined as:}$ to denote the corresponding noisy observations obtained as $Y_{S}=f_{S}+\eta_{S}$, where $\eta_{S} \sim \mathcal{N</p>
<p>$$
\gamma_{t}(k, \mathcal{D}):=\max <em S="S">{S \subset \mathcal{D}:|S|=t} I\left(f</em>\right)=\max }, Y_{S<em t="t">{S \subset \mathcal{D}:|S|=t} \frac{1}{2}\left|I</em>\right|
$$}+\lambda^{-1} K_{t</p>
<p>where $I(\cdot, \cdot)$ stands for the mutual information between random variables, and $|\cdot|$ is the determinant. Simply put, if samples are taken "close" to each other (far from each other) as measured by the kernel, they are more correlated (less correlated) under the GP prior and provide less (more) information. As shown in [40], the maximum information gain $\gamma_{t}(k, \mathcal{D})$ scales sublinearly with $t$ for the most commonly used kernels (see Appendix A).</p>
<h1>3.2 Known misspecification error and optimistic approaches</h1>
<p>In this section, we also use the "well-specified" mean $\mu_{t}(\cdot)$ and variance $\sigma_{t}^{2}(\cdot)$ estimates from Eqs. (8) and (9), where we assume that noisy observations used in Eq. (8) correspond to a function $\tilde{f} \in \arg \min <em k="k">{f \in \mathcal{F}</em>\left|f-f^{}(\mathcal{D} ; B)<em>}\right|<em t="t">{\infty}$. We note that $\sigma</em>^{}^{2}(\cdot)$ does not depend on the observations (i.e., $Y_{t}$ ), and additionally, we define $\mu_{t</em>}(\cdot)$ that depends on the noisy observations of the true $f^{*}$, i.e.,</p>
<p>$$
\mu_{t}^{<em>}(x)=k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} Y_{t}^{</em>}
$$</p>
<p>where $Y_{t}^{<em>}:=\left[y_{1}^{</em>}, \ldots, y_{t}^{<em>}\right]$, and $y_{i}^{</em>}=f^{<em>}\left(x_{t}\right)+\eta_{t}$ for $1 \leq i \leq t$. The only difference between the definitions of $\mu_{t}^{</em>}(x)$ and $\mu_{t}(x)$ comes from the used observation vector, i.e., $Y_{t}^{*}$ and $Y_{t}$, respectively.
We also use the following standard result from [40, 9, 13] that provides confidence bounds around the unknown function in the realizable setting.
Lemma 1. Let $f(\cdot)$ be a function that belongs to the space of functions $\mathcal{F}<em t-1="t-1">{k}(\mathcal{D} ; B)$. Assume the $\sigma$ -sub-Gaussian noise model as in Eq. (1), and let $Y</em>$ :}:=\left[y_{1}, \ldots, y_{t-1}\right]$ denote the vector of previous noisy observations that correpsond to the queried points $\left(x_{1}, \ldots, x_{t-1}\right)$. Then, the following holds with probability at least $1-\delta$ simultaneously over all $t \geq 1$ and $x \in \mathcal{D</p>
<p>$$
\left|f(x)-\mu_{t-1}(x)\right| \leq \beta_{t} \sigma_{t-1}(x)
$$</p>
<p>where $\mu_{t-1}(\cdot)$ and $\sigma_{t-1}(\cdot)$ are given in Eq. (8) and Eq. (9) with $\lambda&gt;0$, and</p>
<p>$$
\beta_{t}=\frac{\sigma}{\lambda^{1 / 2}}\left(2 \ln (1 / \delta)+\sum_{t^{\prime}=1}^{t-1} \ln \left(1+\lambda^{-1} \sigma_{t^{\prime}-1}\left(x_{t^{\prime}}\right)\right)\right)^{\frac{1}{\delta}}+B
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Algorithm 1 EC-GP-UCB (Enlarged Confidence GP-UCB)
1: Require: Kernel function $k(\cdot, \cdot)$, domain $\mathcal{D}$, misspecification $\epsilon$, and parameters $B, \lambda, \sigma$
2: Set $\mu_{0}(x)=0$ and $\sigma_{0}(x)=k(x, x)$, for all $x \in \mathcal{D}$
3: for $t=1, \ldots, T$ do
4: Choose</p>
<p>$$
x_{t} \in \underset{x \in \mathcal{D}}{\arg \max } \mu_{t-1}^{*}(x)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}(x)
$$</p>
<p>5: $\quad$ Observe $y_{t}^{<em>}=f^{</em>}\left(x_{t}\right)+\eta_{t}$
6: Update to $\mu_{t}^{*}(\cdot)$ and $\sigma_{t}(\cdot)$ by using $\left(x_{t}, y_{t}\right)$ according to Eq. (9) and Eq. (11)
7: end for</p>
<p>Next, we start by addressing the misspecified setting when $\epsilon$ is known to the learner. We consider minimizing $R_{T}$ (from Eq. (5)), and provide an upper confidence bound algorithm with enlarged confidence bounds EC-GP-UCB (see also Algorithm 1):</p>
<p>$$
x_{t} \in \underset{x \in \mathcal{D}}{\arg \max } \mu_{t-1}^{*}(x)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}(x)
$$</p>
<p>where the confidence interval enlargement is to account for the use of the biased mean estimator $\mu_{t-1}^{*}(\cdot)$ (instead of $\mu_{t-1}(\cdot)$ ). This can be interpreted as introducing an additional exploration bonus to the standard GP-UCB algorithm [40] in case of misspecification. The enlargement corresponds to the difference in the mean estimators that is captured in the following lemma:
Lemma 2. For any $x \in \mathcal{D}, t \geq 1$ and $\lambda&gt;0$, we have</p>
<p>$$
\left|\mu_{t}(x)-\mu_{t}^{*}(x)\right| \leq \frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t}(x)
$$</p>
<p>where $\mu_{t}(\cdot)$ and $\mu_{t}^{<em>}(\cdot)$ are defined as in Eq. (8) and Eq. (11), respectively, and $\sigma_{t}(\cdot)$ is from Eq. (9).
Next, we upper bound the cumulative regret of the proposed algorithm.
Theorem 1. Suppose the learner's hypothesis class is $\mathcal{F}_{k}(\mathcal{D} ; B)$ for some fixed $B&gt;0$ and $\mathcal{D} \subset \mathbb{R}^{d}$. For any $f^{</em>}$ defined on $\mathcal{D}$ and $\epsilon \geq 0$ such that $\min <em k="k">{f \in \mathcal{F}</em> \leq \epsilon$, EC-GP-UCB with enlarged confidence Eq. (15) and known $\epsilon$, achieves the following regret bound with probability at least $1-\delta$ :}(\mathcal{D} ; B)}\left|f-f^{*}\right|_{\infty</p>
<p>$$
R_{T}=O\left(B \sqrt{\gamma_{T} T}+\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}+\epsilon T \sqrt{\gamma_{T}}\right)
$$</p>
<p>As remarked before, the regret bound from Eq. (17) implies the upper bound on $R_{T}^{<em>}$ of the same order, since $R_{T}^{</em>}$ and $R_{T}$ differ by at most $2 \epsilon T$. The first part of the bound, i.e., $O\left(B \sqrt{\gamma_{T} T}+\right.$ $\left.\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}\right)$, corresponds to the standard regret bound achieved by GP-UCB in the realizable scenario $[40,9]$ which is also known to nearly match the lower bounds in case of commonly used kernels [36]. On the other hand, in Appendix C, we also demonstrate that $\epsilon T$ dependence is unavoidable in general for any algorithm in the misspecified kernelized setting.
A similar robust GP-UCB algorithm with enlarged confidence bounds has been first considered in [4] to defend against adversarial corruptions. One can think of the misspecified setting as a corrupted one where, at every round, the corruption is bounded by $\epsilon$, yielding a total corruption budget of $C=T \epsilon$. The algorithm proposed in [4] attains a $C \sqrt{\gamma_{T} T}$ regret bound (due to corruptions) which is strictly suboptimal in comparison to the $\epsilon T \sqrt{\gamma_{T}}$ bound obtained in our Theorem 1.
Finally, we note that to obtain the previous result, the algorithm requires knowledge of $\epsilon$ as input, and it is unclear how to adapt the algorithm to the unknown $\epsilon$ case. In particular, we show in Appendix B. 2 that the problem in the analysis arises since there is no effective way of controlling the uncertainty at $x^{*}$ when using standard UCB-based approaches. To address this more practical setting, in the next section we propose our main algorithm that does not require the knowledge of $\epsilon$.</p>
<h1>3.3 Unknown misspecification error: Phased GP Uncertainty Sampling</h1>
<p>Our second proposed Phased GP Uncertainty Sampling algorithm that has no knowledge of the true $\epsilon$ parameter is shown in Algorithm 2. It runs in episodes of exponentially increasing length $m_{e}$</p>
<p>Algorithm 2 Phased GP Uncertainty Sampling
Require: Kernel function $k(\cdot, \cdot)$, domain $\mathcal{D}$, and parameters $B, \lambda, \sigma$
Set episode index $e=1$, episode length $m_{e}=1$, and set of potentially optimal actions $\mathcal{D}<em 0="0">{e}=\mathcal{D}$
Set $\mu</em>}(x)=0$ and $\sigma_{0}(x)=k(x, x)$, for all $x \in \mathcal{D<em e="e">{e}$
for $t=1, \ldots, m</em>$ do
Choose</p>
<p>$$
x_{t} \in \underset{x \in D_{e}}{\arg \max } \sigma_{t-1}^{2}(x)
$$</p>
<p>Update to $\sigma_{t}(\cdot)$ by including $x_{t}$ according to Eq. (9)
end for
Receive $\left{y_{1}, \ldots, y_{m_{e}}\right}$, such that</p>
<p>$$
y_{t}=f^{*}\left(x_{t}\right)+\eta_{t} \quad \text { for } \quad t \in\left{1, \ldots, m_{e}\right}
$$</p>
<p>and use them to compute $\mu_{m_{e}}^{*}(\cdot)$ according to Eq. (11)
Set</p>
<p>$$
\mathcal{D}<em e="e">{e+1} \leftarrow\left{x \in \mathcal{D}</em>^{}: \mu_{m_{e}<em>}(x)+\beta_{m_{e}+1} \sigma_{m_{e}}(x) \geq \max <em e="e">{x \in \mathcal{D}</em>^{}}\left(\mu_{m_{e}</em>}(x)-\beta_{m_{e}+1} \sigma_{m_{e}}(x)\right)\right}
$$</p>
<p>$10: m_{e+1} \leftarrow 2 m_{e}, e \leftarrow e+1$ and return to step (3) (terminate after $T$ total function evaluations)
and maintains a set of potentially optimal actions $\mathcal{D}<em t="t">{e}$. In each episode $e$, actions are selected via exploration-encouraging uncertainty sampling, i.e., an action of maximal GP epistemic uncertainty $\sigma</em>}(\cdot)$ is selected (with ties broken arbitrarily). The selected action is then used to update $\sigma_{t}(\cdot)$, that does not depend on the received observations (see Eq. (9)). We also note that at the beginning of every episode, the algorithm reverts back to the prior model (before any data is observed), i.e., by setting $\mu_{0}(x)=0$ and $\sigma_{0}(x)=k(x, x)$, for all $x \in \mathcal{D<em e="e">{e}$.
After every episode, the algorithm receives $m</em>$ whose hallucinated upper confidence bound is at least as large as the maximum hallucinated lower bound, that is,}$ noisy observations which are then used to update the mean estimate $\mu_{m_{e}}^{*}(\cdot)$ according to Eq. (8). Finally, the algorithm eliminates actions that appear suboptimal according to the current but potentially wrong model. This is done by retaining all the actions $x \in \mathcal{D}_{e</p>
<p>$$
\mu_{m_{e}}^{<em>}(x)+\beta_{m_{e}+1} \sigma_{m_{e}}(x) \geq \max <em e="e">{x \in \mathcal{D}</em>^{}}\left(\mu_{m_{e}</em>}(x)-\beta_{m_{e}+1} \sigma_{m_{e}}(x)\right)
$$</p>
<p>where $\beta_{m_{e}+1}$ is set as in the realizable setting. Here, the term "hallucinated" refers to the fact that these confidence bounds might be invalid (for $\hat{f}$ ), and hence, the optimal action might be eliminated. We also note that in case $\epsilon=0$, the hallucinated confidence bounds are actually valid. In our analysis (see Appendix D), we prove that although the optimal action can be eliminated, a "near-optimal" one is retained after every episode. Hence, we only need to characterize the difference in regret due to such possible wrong elimination within all episodes (whose number is logarithmic in $T$ ).
Our Algorithm 2 bears some similarities with the Phased Elimination algorithm of [24] designed for the related linear misspecified bandit setting. Both algorithms employ an episodic action elimination strategy. However, the algorithm of [24] crucially relies on the Kiefer-Wolfowitz theorem [19] and requires computing a near-optimal design at every episode (i.e., a probability distribution over a set of currently plausibly optimal actions) that minimizes the worst-case variance of the resulting leastsquares estimator. Adapting this approach to the kernelized setting and RKHS function classes of infinite dimension is a nontrivial task. Instead, we use a kernel ridge regression estimate and we show that it is sufficient to sequentially select actions via a simple acquisition rule that does not rely on finite-dimensional feature approximations of the kernel. In particular, our algorithm, at each round $t$ (in episode $e$ ), selects a single action $x$ (from $\mathcal{D}<em _left_Phi__t="\left(\Phi_{t">{e}$ ) that maximizes $|\phi(x)|</em>$ ), which is equivalent to maximizing the GP posterior variance in Eq. (19) (see Appendix D for details). We also note that similar GP uncertainty sampling acquisition rules are commonly used in Bayesian optimization and experimental design (see, e.g., $[10,6,23]$ ), and efficient iterative updates of the posterior variance that avoid computation of a $t \times t$ kernel matrix inverse at each round are also available (see, e.g., Appendix F in [9]).}^{*} \Phi_{t}+\lambda I_{k}\right)^{-1}}^{2}$ (here, $\phi(x)$ denotes kernel features, i.e., $k\left(x, x^{\prime}\right)=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{k</p>
<p>In the next theorem, we bound the cumulative regret of the proposed algorithm and use $\tilde{O}(\cdot)$ notation to hide $\operatorname{polylog}(T)$ factors.
Theorem 2. Suppose the learner's hypothesis class is $\mathcal{F}<em _infty="\infty">{k}(\mathcal{D} ; B)$ for some fixed $B&gt;0$ and $\mathcal{D} \subset \mathbb{R}^{d}$. For any $f^{<em>}$ defined on $\mathcal{D}$ and $\epsilon \geq 0$ such that $\min <em k="k">{f \in \mathcal{F}</em>\left|f-f^{}(\mathcal{D} ; B)</em>}\right|</em> \leq \epsilon$, Phased GP Uncertainty Sampling (Algorithm 2) achieves the following regret bound with probability at least $1-\delta$ :</p>
<p>$$
R_{T}^{*}=\tilde{O}\left(B \sqrt{\gamma_{T} T}+\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}+\epsilon T \sqrt{\gamma_{T}}\right)
$$</p>
<p>In comparison with the previous kernel-based EC-GP-UCB algorithm, our phased uncertainty sampling algorithm attains the same regret guarantee without knowledge of $\epsilon$. Our result holds in the case of infinite action sets, and further on, we can substitute the existing upper bounds on $\gamma_{T}(k, \mathcal{D})$ to specialize it to particular kernels. For example, in the case of linear kernel and compact domain, we have $\gamma_{T}\left(k_{\text {lin }}, \mathcal{D}\right)=O(d \log T)$, while for squared-exponential kernel it holds $\gamma_{t}\left(k_{\mathrm{SE}}, \mathcal{D}\right)=O\left((\log T)^{d+1}\right)$ [40]. Moreover, when the used kernel is linear, we recover the same misspecification regret rate of [24], i.e., $\tilde{O}(\epsilon T \sqrt{d})$.</p>
<h1>4 Algorithm for the contextual misspecified kernelized bandit setting</h1>
<p>In this section, we consider a contextual misspecified problem with unknown $f^{<em>}: \mathcal{D} \rightarrow[0,1]$ and the same assumptions as before (see Section 2). The main difference comes from the fact that at every round $t$, the learner needs to choose an action from a possibly different action set $\mathcal{D}<em t="t">{t} \subseteq \mathcal{D}$. We assume that the learner observes a context $c</em> \triangleq{x}<em t="t">{x \in D</em>}}$ at every round $t$, where $c_{t}$ is assumed to be drawn i.i.d. from some distribution. ${ }^{3}$ The learner then plays $x_{t} \in \mathcal{D<em t="t">{t}$ and observes $y</em>=f^{</em>}\left(x_{t}\right)+\eta_{t}$, with 1-sub-Gaussian noise and independence between time steps.
Since we consider the setting in which the set of actions changes with every round, we note that algorithms that employ action-elimination strategies, such as our Phased GP Uncertainty Sampling, are not easily adapted to this scenario as they require a fixed action set. On the other hand, our optimistic EC-GP-UCB algorithm requires knowing the true misspecification parameter $\epsilon$ a priori, which is also unsatisfactory. To address this, in this section, we combine our EC-GP-UCB with the regret bound balancing strategy from [32].
We measure the regret incurred by the learner by using the corresponding contextual regret definition:</p>
<p>$$
R_{T}^{<em>}=\sum_{t=1}^{T}\left(\max <em t="t">{x \in \mathcal{D}</em> f^{}</em>}(x)-f^{*}\left(x_{t}\right)\right)
$$</p>
<p>We specialize the regret bound balancing strategy to the GP-bandit setting. The proposed algorithmic solution considers $M$ different base algorithms (Base) ${ }_{i=1}^{M}$, and uses an elimination $M$-armed bandit master algorithm (see Algorithm 3) to choose which base algorithm to play at every round.</p>
<p>Base algorithms. At each round, the learner plays according to the suggestion of a single base algorithm $i \in{1, \ldots, M}$. We use $\tau_{i}(t)$ to denote the set of rounds in which base algorithm $i$ is selected up to time $t$, and $N_{i}(t)=\left|\tau_{i}(t)\right|$ to denote the number of rounds algorithm $i$ is played by the learner. After a total of $T$ rounds, the regret of algorithm $i$ is given by $R_{i, T}^{<em>}=\sum_{t \in \tau_{i}(T)}\left(\max <em t="t">{x \in \mathcal{D}</em> f^{}</em>}(x)-f^{<em>}\left(x_{t}\right)\right)$, and we also note that $R_{T}^{</em>}=\sum_{i=1}^{M} R_{i, T}^{*}$.
As a good candidate for base algorithms, we use our EC-GP-UCB algorithm (from Algorithm 1), but instead of the true unknown $\epsilon$, we instantiate EC-GP-UCB with different candidate values $\hat{\epsilon}<em 2="2">{i}$. In particular, we use $M=\left\lceil 1+\frac{1}{2} \log </em>}\left(T / \gamma_{T}^{2}\right)\right\rceil$ different EC-GP-UCB algorithms, and for every algorithm $i \in[M]$, we set $\hat{\epsilon<em T="T">{i}=\frac{2^{i-i}}{\sqrt{\gamma</em>$. This means that as $i$ decreases, the used algorithms are more robust and can tolerate larger misspecification error. On the other hand, overly conservative values lead to excessive exploration and large regret. We consider the true $\epsilon$ to be in $[0,1)$, and our goal is to ideally match the performance of EC-GP-UCB that is run with such $\epsilon$.}}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Each base algorithm comes with a candidate regret upper bound $\mathcal{R}<em _="+">{i}(t): \mathbb{N} \rightarrow \mathbb{R}</em>}$(that holds with high probability) of the form given in Theorem 1. Moreover, every candidate regret bound $\mathcal{R<em i="i">{i}(t)$ is non-decreasing in $t$ and $\mathcal{R}</em>}(0)=0$. Without loss of generality, we can assume that candidate bounds can increase by at most 1 per each round, i.e., $0 \leq \mathcal{R<em i="i">{i}(t)-$ $\mathcal{R}</em>(t-1) \leq 1$ for all rounds $t$ and algorithms $i$. We use the following forms of EC-GP-UCB candidate bounds in Algorithm 3:</p>
<h2>Algorithm 3 Regret bound balancing [33]</h2>
<h2>Require: $(\text { Base })_{i=1}^{M}$</h2>
<p>Initialize: $I_{1}=[M], N_{i}(0)=0$ for all $i$
for $t=1, \ldots, T$ do
Receive $\mathcal{D}<em t="t">{t}$
Select $i</em> \in \arg \min <em t="t">{i \in I</em>(t-1)\right)$
Base $}} \mathcal{R}^{(i)}\left(N_{i<em t="t">{i</em>}}$ plays $x_{t} \in \mathcal{D<em t="t">{t}$ and observes $y</em>$
Update Base $<em t="t">{i</em>(t)+1$
Update the set of active algorithms $I_{t+1}$}}$ and set $N_{i}(t)=N_{i</p>
<h2>end for</h2>
<p>$$
\mathcal{R}<em i="i">{i}\left(N</em>}(t)\right)=\min \left{c_{1}^{\prime} \gamma_{T} \sqrt{N_{i}(t)}+c_{2}^{\prime} \hat{\epsilon<em T="T">{i} \sqrt{\gamma</em>(t)\right}
$$}} N_{i}(t), N_{i</p>
<p>for some quantities $c_{1}^{\prime}, c_{2}^{\prime}$ that do not depend on $N_{i}(t)$ or $\hat{\epsilon}<em i="i">{i}$. For a given $\hat{\epsilon}</em>$
We refer to algorithm $i$ as being consistent if $R_{i, t}^{}$, these bounds are computable for every $t \in[T]$. In particular, to evaluate them, we also need to compute $\gamma_{T}$ (Eq. (10)). We note that for popularly used kernels, analytical upper bounds exist as discussed in Section 3.3, while in general, we can efficiently approximate it up to a constant factor via simple greedy maximization. ${ }^{4<em>} \leq \mathcal{R}<em i="i">{i}\left(N</em>(t)\right)$ for all $t \in[T]$ with high probability, and otherwise as inconsistent. We also let $i^{</em>}$ denote the first consistent algorithm $i$ for which $\hat{\epsilon}<em t="t">{i^{<em>}} \geq \epsilon$. In particular, we have that $\hat{\epsilon}<em>{i} \leq \hat{\epsilon}</em>{i^{</em>}}$ for every inconsistent algorithm $i$.
Master algorithm. We briefly recall the regret bound balancing algorithm of [32] and its main steps. At each round, the learner selects an algorithm $i</em>$ according to the regret bound balancing principle:</p>
<p>$$
i_{t} \in \underset{i \in I_{t}}{\arg \min } \mathcal{R}<em i="i">{i}\left(N</em>(t-1)\right)
$$</p>
<p>where $I_{t} \subseteq[M]$ is the set of active (i.e., plausibly consistent) algorithms at round $t$. Then, the learner uses the algorithm $i_{t}$ to select $x_{t}$, observes reward $y_{t}$, and increases $N_{i}(t)=N_{i}(t-1)+1$. Intuitively, by evaluating candidate regret bounds for different number of times, the algorithm keeps them almost equal and hence balanced, i.e., $\mathcal{R}<em i="i">{i}\left(N</em>}(t)\right) \leq \mathcal{R<em j="j">{j}\left(N</em>$.
The remaining task is to update the set of active algorithms based on the received observation. We define $J_{i}(t)=\sum_{j \in \tau_{i}(t)} y_{j}$ as the cumulative reward of algorithm $i$ in the first $t$ rounds. As in [32] (Section 4), we set $I_{1}=[M]$, and at the end of each round, the learner eliminates every inconsistent algorithm $i \in I_{t}$ from $I_{t+1}$ when:}(t)\right)+1$ for every $t$ and all active algorithms $i, j \in I_{t</p>
<p>$$
J_{i}(t)+\mathcal{R}<em i="i">{i}\left(N</em>&lt;\max }(t)\right)+c \sqrt{N_{i}(t) \ln \left(M \ln N_{i}(t) / \delta\right)<em t="t">{j \in I</em>
$$}} J_{j}(t)-c \sqrt{N_{j}(t) \ln \left(M \ln N_{j}(t) / \delta\right)</p>
<p>where $c$ is a suitably set absolute constant (see Lemma A. 1 in [32]). Crucially, the elimination condition from Eq. (24) is used in Algorithm 3, and it ensures that no consistent algorithm is eliminated (with high probability). In particular, since each $\mathcal{D}<em i="i">{t}$ is sampled i.i.d., it holds that $N</em>}(t) \mathbb{E<em t="t">{\mathcal{D}</em>\left[\max }<em t="t">{x \in \mathcal{D}</em>$.
Regret bound. By using the general regret result for Algorithm 3 from [32] (see Appendix E), we show the total regret obtained for Algorithm 3 when instantiated with our EC-GP-UCB algorithm and candidate regret bounds provided in Eq. (22). This is formally captured in the following proposition.
Proposition 1. Consider $M=\left\lceil 1+\frac{1}{2} \log }} f^{*}(x)\right]$ is upper/lower bounded by the left/right hand side of Eq. (24). Hence, if the upper bound of algorithm $i$ for this quantity is smaller than the maximum lower bound, the algorithm can be classified as inconsistent and consequently eliminated from $I_{t+1<em T="T">{2}\left(T / \gamma</em>}^{2}\right)\right\rceil$ EC-GP-UCB base algorithms, each with candidate regret upper bound of the form provided in Eq. (22) with $\hat{\epsilon<em T="T">{i}=\frac{2^{1-i}}{\sqrt{\gamma</em>$ for each $i \in[M]$. In the misspecified kernelized contextual setting, when run with such $M$ algorithms, Algorithm 3 achieves the following regret bound with probability at least $1-M \delta$ :}}</p>
<p>$$
R_{T}^{*}=\tilde{O}\left(\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right) \sqrt{T}+\epsilon T \sqrt{\gamma_{T}}+\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right)^{2}\right)
$$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The obtained contextual regret bound recovers (up to poly-logarithmic factors and an additive term of lower-order) the bound of the EC-GP-UCB instance (from Theorem 1) that assumes the knowledge of the true $\epsilon$.</p>
<h1>5 Conclusion</h1>
<p>We have considered the GP-bandit optimization problem in the case of a misspecified hypothesis class. Our work systematically handles model misspecifications in GP bandits and provides robust and practical algorithms. We designed novel algorithms based on ideas such as enlarged confidence bounds, phased epistemic uncertainty sampling, and regret bound balancing, for the standard (with known/unknown misspecification error) and contextual settings. While there have been some theoretical [29] and practical [37] efforts to quantify the impact of misspecified priors in the related Bayesian setting [40], an interesting direction for future work is to develop algorithms and regret bounds in case of misspecified GP priors.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme grant agreement No 815943 and ETH Zürich Postdoctoral Fellowship 19-2 FEL-47.</p>
<h2>References</h2>
<p>[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems (NeurIPS), volume 11, pages 2312-2320, 2011.
[2] Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In Conference on Learning Theory, pages 12-38. PMLR, 2017.
[3] Felix Berkenkamp, Angela P Schoellig, and Andreas Krause. No-regret Bayesian optimization with unknown hyperparameters. arXiv preprint arXiv:1901.03357, 2019.
[4] Ilija Bogunovic, Andreas Krause, and Scarlett Jonathan. Corruption-tolerant Gaussian process bandit optimization. In Conference on Artificial Intelligence and Statistics (AISTATS), 2020.
[5] Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher. Adversarially robust optimization with Gaussian processes. In Advances in Neural Information Processing Systems (NeurIPS), pages 5760-5770, 2018.
[6] Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation. In Advances in Neural Information Processing Systems (NeurIPS), pages 1507-1515, 2016.
[7] Sait Cakmak, Raul Astudillo, Peter Frazier, and Enlu Zhou. Bayesian optimization of risk measures. arXiv preprint arXiv:2007.05554, 2020.
[8] Romain Camilleri, Kevin Jamieson, and Julian Katz-Samuels. High-dimensional experimental design and kernel bandits. In International Conference on Machine Learning, pages 12271237. PMLR, 2021.
[9] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International Conference on Machine Learning (ICML), pages 844-853, 2017.
[10] Emile Contal, David Buffoni, Alexandre Robicquet, and Nicolas Vayatis. Parallel Gaussian process optimization with upper confidence bound and pure exploration. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 225-240. Springer, 2013.
[11] Nando de Freitas, Alex Smola, and Masrour Zoghi. Regret bounds for deterministic Gaussian process bandits. arXiv preprint arXiv:1203.2177, 2012.
[12] Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019.</p>
<p>[13] Audrey Durand, Odalric-Ambrym Maillard, and Joelle Pineau. Streaming kernel regression with provably adaptive mean, variance, and regularization. The Journal of Machine Learning Research, 19(1):650-683, 2018.
[14] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. Advances in Neural Information Processing Systems, 33, 2020.
[15] Dylan J Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits with regression oracles. arXiv preprint arXiv:2002.04926, 2020.
[16] Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Misspecified linear bandits. In AAAI Conference on Artificial Intelligence, 2017.
[17] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 21372143, 2020.
[18] Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian processes and kernel methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018.
[19] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12:363-366, 1960.
[20] Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause. Distributionally robust Bayesian optimization. arXiv preprint arXiv:2002.09038, 2020.
[21] Johannes Kirschner and Andreas Krause. Stochastic bandits with context distributions. In Advances in Neural Information Processing Systems, pages 14113-14122, 2019.
[22] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems (NeurIPS), pages 2447-2455, 2011.
[23] Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9(Feb):235-284, 2008.
[24] Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in RL with a generative model. International Conference on Machine Learning, 2020.
[25] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In International conference on World Wide Web, pages $661-670,2010$.
[26] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765-6816, 2017.
[27] Lydia T Liu, Horia Mania, and Michael Jordan. Competing bandits in matching markets. In International Conference on Artificial Intelligence and Statistics, pages 1618-1628. PMLR, 2020.
[28] Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine Learning Research, 7(12), 2006.
[29] Willie Neiswanger and Aaditya Ramdas. Uncertainty quantification using martingales for misspecified Gaussian processes. arXiv preprint arXiv:2006.07368, 2020.
[30] Gergely Neu and Julia Olkhovskaya. Efficient and robust algorithms for adversarial linear contextual bandits. arXiv preprint arXiv:2002.00287, 2020.
[31] Thanh Tang Nguyen, Sunil Gupta, Huong Ha, Santu Rana, and Svetha Venkatesh. Distributionally robust Bayesian quadrature optimization. arXiv preprint arXiv:2001.06814, 2020.
[32] Aldo Pacchiano, Christoph Dann, Claudio Gentile, and Peter Bartlett. Regret bound balancing and elimination for model selection in bandits and RL. arXiv preprint arXiv:2012.13045, 2020.
[33] Aldo Pacchiano, My Phan, Yasin Abbasi-Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. arXiv preprint arXiv:2003.01704, 2020.</p>
<p>[34] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning, volume 1. MIT press Cambridge, 2006.
[35] Jonathan Scarlett. Tight regret bounds for Bayesian optimization in one dimension. In International Conference on Machine Learning (ICML), 2018.
[36] Jonathan Scarlett, Ilijia Bogunovic, and Volkan Cevher. Lower bounds on regret for noisy Gaussian process bandit optimization. In Conference on Learning Theory (COLT), 2017.
[37] Eric Schulz, Maarten Speekenbrink, José Miguel Hernández-Lobato, Zoubin Ghahramani, and Samuel J Gershman. Quantifying mismatch in Bayesian optimization. In NeurIPS workshop on Bayesian optimization: Black-box optimization and beyond, 2016.
[38] Pier Giuseppe Sessa, Ilija Bogunovic, Maryam Kamgarpour, and Andreas Krause. Mixed strategies for robust optimization of unknown objectives. In Conference on Artificial Intelligence and Statistics (AISTATS), 2020.
[39] Shubhanshu Shekhar and Tara Javidi. Gaussian process bandits with adaptive discretization. Electronic Journal of Statistics, 12(2):3829-3874, 2018.
[40] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning (ICML), pages 1015-1022, 2010.
[41] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with Gaussian processes. In International Conference on Machine Learning (ICML), pages 997-1005, 2015.
[42] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495-517. Springer, 2017.
[43] Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in Gaussian process bandits. arXiv preprint arXiv:2009.06966, 2020.
[44] Michal Valko, Nathaniel Korda, Rémi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis of kernelised contextual bandits. Uncertainty In Artificial Intelligence (UAI), 2013.
[45] Benjamin Van Roy and Shi Dong. Comments on the du-kakade-wang-yang lower bounds. arXiv preprint arXiv:1911.07910, 2019.
[46] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014.
[47] George Wynne, Francois-Xavier Briol, and Mark Girolami. Convergence guarantees for Gaussian process means with misspecified likelihoods and smoothness. Journal of Machine Learning Research, 22(123):1-40, 2021.
[48] Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. arXiv preprint arXiv:2003.00153, 2020.</p>
<h1>Supplementary Material <br> Misspecified GP Bandit Optimization</h1>
<p>Ilija Bogunovic and Andreas Krause (NeurIPS 2021)</p>
<h2>A GP bandits: Useful definitions and auxiliary results (Realizable setting)</h2>
<p>Assumed observation model. We say a real-valued random variable $X$ is $\sigma$-sub-Gaussian if it its mean is zero and for all $\varepsilon \in \mathbb{R}$ we have</p>
<p>$$
\mathbb{E}[\exp (\varepsilon X)] \leq \exp \left(\frac{\sigma^{2} \varepsilon^{2}}{2}\right)
$$</p>
<p>At every round $t$, the learner selects $x_{t} \in D$ and observes the noisy function evaluation</p>
<p>$$
y_{t}=f\left(x_{t}\right)+\eta_{t}
$$</p>
<p>where we assume $\left{\eta_{t}\right}<em k="k">{t=1}^{T}$ are $\sigma$-sub-Gaussian random variables that are independent over time steps. Such assumptions on the noise variables are frequently used in bandit optimization.
Typically, in kernelized bandits, we assume that unknown $f \in \mathcal{F}</em>}(\mathcal{D} ; B)=\left{f \in \mathcal{H<em k="k">{k}(\mathcal{D}):|f|</em>} \leq\right.$ $B}$, where $\mathcal{H<em k="k">{k}(\mathcal{D})$ is the reproducing kernel Hilbert space of functions associated with the given positive-definite kernel function. Typically, the learner knows $\mathcal{F}</em> ; B)$, meaning that both $k(\cdot, \cdot)$ and $B$ are considered as input to the learner's algorithm.
Example kernel functions. We outline some commonly used kernel functions $k: \mathcal{D} \times \mathcal{D} \rightarrow \mathbb{R}$, that we also consider:}(\mathcal{D</p>
<ul>
<li>Linear kernel: $k_{\text {lin }}\left(x, x^{\prime}\right)=x^{T} x^{\prime}$,</li>
<li>Squared exponential kernel: $k_{\mathrm{SE}}\left(x, x^{\prime}\right)=\exp \left(-\frac{\left|x-x^{\prime}\right|^{2}}{2 l^{2}}\right)$,</li>
<li>Matérn kernel: $k_{\text {Mat }}\left(x, x^{\prime}\right)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2 \nu}\left|x-x^{\prime}\right|}{l}\right) J_{\nu}\left(\frac{\sqrt{2 \nu}\left|x-x^{\prime}\right|}{l}\right)$,
where $l$ denotes the length-scale hyperparameter, $\nu&gt;0$ is an additional hyperparameter that dictates the smoothness, and $J(\nu)$ and $\Gamma(\nu)$ denote the modified Bessel function and the Gamma function, respectively [34].
Maximum information gain. Maximum information gain is a kernel-dependent quantity that measures the complexity of the given function class. It has first been introduced in [40], and since then it has been used in numerous works on Gaussian process bandits. Typically, the upper regret bounds in Gaussian process bandits are expressed in terms of this complexity measure.</li>
</ul>
<p>It represents the maximum amount of information that a set of noisy observations can reveal about the unknown $f$ that is sampled from a zero-mean Gaussian process with kernel $k$, i.e., $f \sim G P(0, k)$. More precisely, for a set of sampling points $S \subset \mathcal{D}$, we use $f_{S}$ to denote a random vector $[f(x)]<em S="S">{x \in S}$, and $Y</em>$, the posterior distribution of $f$ is a Gaussian process with posterior mean and variance that correspond to Eq. (8) and Eq. (9).
The maximum information gain (about $f$ ) after observing $t$ noisy samples is defined as (see [40]):}$ to denote the corresponding noisy observations obtained as $Y_{S}=f_{S}+\eta_{S}$, where $\eta_{S} \sim$ $\mathcal{N}(0, \lambda I)$. We note that under this setup after observing $Y_{S</p>
<p>$$
\gamma_{t}(k, \mathcal{D}):=\max <em S="S">{S \subset \mathcal{D}:|S|=t} I\left(f</em>\right)=\max } ; Y_{S<em t="t">{S \subset \mathcal{D}:|S|=t} \frac{1}{2}\left|I</em>\right|
$$}+\lambda^{-1} K_{t</p>
<p>where $I(\cdot, \cdot)$ denotes the mutual information between random variables, $|\cdot|$ is used to denote a matrix determinant, and $K_{t}$ is a kernel matrix $\left[k\left(x_{s}, x_{s^{\prime}}\right)\right]_{s, s^{\prime} \leq t} \in \mathbb{R}^{t \times t}$.
Under the previous setup (GP prior and Gaussian likelihood), the maximum information gain can be expressed in terms of predictive GP variances:</p>
<p>$$
\gamma_{t}(k, \mathcal{D})=\max <em 1="1">{\left{x</em>\right)\right)
$$}, \ldots, x_{t}\right} \subset \mathcal{D}} \frac{1}{2} \sum_{s=1}^{t} \ln \left(1+\lambda^{-1} \sigma_{s-1}^{2}\left(x_{s</p>
<p>The proof of this claim can be found in [40, Lemma 5.3]. It also allows us to rewrite Eq. (12) from Lemma 1 in the following frequently used form:</p>
<p>$$
\left|f(x)-\mu_{t-1}(x)\right| \leq\left(\frac{\sigma}{\lambda^{1 / 2}} \sqrt{2 \ln (1 / \delta)+2 \gamma_{t-1}}+B\right) \sigma_{t-1}(x)
$$</p>
<p>Next, we outline an important relation (due to [40]) frequently used to relate the sum of GP predictive standard deviations with the maximum information gain. We use the formulation that follows from Lemma 4 in [9]:
Lemma 3. Consider some kernel $k: \mathcal{D} \times \mathcal{D} \rightarrow \mathbb{R}$ such that $k(x, x) \leq 1$ for every $x \in \mathcal{D}$, and let $f \sim G P(0, k)$ be a sample from a zero-mean GP with the corresponding kernel function. Then for any set of queried points $\left{x_{1}, \ldots, x_{t}\right}$ and $\lambda&gt;0$, it holds that</p>
<p>$$
\sum_{i=1}^{t} \sigma_{i-1}\left(x_{i}\right) \leq \sqrt{(2 \lambda+1) \gamma_{t}} t
$$</p>
<p>Finally, we outline bounds on $\gamma_{t}(k, \mathcal{D})$ for commonly used kernels as provided in [40]. An important observation is that the maximum information gain is sublinear in terms of number of samples $t$ for these kernels.
Lemma 4. Let $d \in \mathbb{N}$ and $\mathcal{D} \subset \mathbb{R}^{d}$ be a compact and convex set. Consider a kernel $k: \mathcal{D} \times \mathcal{D} \rightarrow \mathbb{R}$ such that $k(x, x) \leq 1$ for every $x \in \mathcal{D}$, and let $f \sim G P(0, k)$ be a sample from a zero-mean Gaussian Process (supported on $\mathcal{D}$ ) with the corresponding kernel function. Then in case of</p>
<ul>
<li>Linear kernel: $\gamma_{t}\left(k_{\text {lin }}, \mathcal{D}\right)=O(d \log t)$,</li>
<li>Squared exponential kernel: $\gamma_{t}\left(k_{3 E}, \mathcal{D}\right)=O\left((\log t)^{d+1}\right)$,</li>
<li>Matérn kernel: $\gamma_{t}\left(k_{\text {Mat }}, \mathcal{D}\right)=O\left(t^{d(d+1) /(2 \nu+d(d+1))} \log t\right)$.</li>
</ul>
<p>We also note that the previous rates in case of the Matérn kernel have been recently improved to: $O\left(t^{\frac{d}{2 \nu+d}}(\log t)^{\frac{2 \nu}{2 \nu+d}}\right)$ in [43].</p>
<h1>B Proofs from Section 3.2 (EC-GP-UCB)</h1>
<h2>B. 1 EC-GP-UCB with known misspecification</h2>
<p>Theorem 1. Suppose the learner's hypothesis class is $\mathcal{F}<em _infty="\infty">{k}(\mathcal{D} ; B)$ for some fixed $B&gt;0$ and $\mathcal{D} \subset \mathbb{R}^{d}$. For any $f^{<em>}$ defined on $\mathcal{D}$ and $\epsilon \geq 0$ such that $\min <em k="k">{f \in \mathcal{F}</em>\left|f-f^{}(\mathcal{D} ; B)</em>}\right|</em> \leq \epsilon$, EC-GP-UCB with enlarged confidence Eq. (15) and known $\epsilon$, achieves the following regret bound with probability at least $1-\delta$ :</p>
<p>$$
R_{T}=O\left(B \sqrt{\gamma_{T} T}+\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}+\epsilon T \sqrt{\gamma_{T}}\right)
$$</p>
<p>Proof. From the definition of $R_{T}$ in Eq. (4) (also recall the definition of $\tilde{f}$ from Eq. (6)), we have:</p>
<p>$$
\begin{aligned}
R_{T} &amp; =\sum_{t=1}^{T}\left(\max <em t="t">{x \in \mathcal{D}} \tilde{f}(x)-\tilde{f}\left(x</em>\right)\right) \
&amp; \leq \sum_{t=1}^{T} \max <em t-1="t-1">{x \in \mathcal{D}}\left(\mu</em>^{<em>}(x)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}(x)\right)-\left(\mu_{t-1}^{</em>}\left(x_{t}\right)-\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right)\right) \
&amp; \leq \sum_{t=1}^{T} \mu_{t-1}^{<em>}\left(x_{t}\right)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right)-\mu_{t-1}^{</em>}\left(x_{t}\right)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right) \
&amp; =\sum_{t=1}^{T} 2\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right) \
&amp; =\sum_{t=1}^{T} 2 \beta_{t} \sigma_{t-1}\left(x_{t}\right)+\sum_{t=1}^{T} 2 \frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x_{t}\right) \
&amp; \leq 2 \beta_{T} \sum_{t=1}^{T} \sigma_{t-1}\left(x_{t}\right)+2 \frac{\epsilon \sqrt{T}}{\sqrt{\lambda}} \sum_{t=1}^{T} \sigma_{t-1}\left(x_{t}\right) \
&amp; \leq 2 \beta_{T} \sqrt{(2 \lambda+1) \gamma_{T} T}+2 \frac{\epsilon \sqrt{T}}{\sqrt{\lambda}} \sqrt{(2 \lambda+1) \gamma_{T} T} \
&amp; =2\left(\frac{\sigma}{\lambda^{1 / 2}} \sqrt{2 \ln (1 / \delta)+2 \gamma_{T}}+B\right) \sqrt{(2 \lambda+1) \gamma_{T} T}+2 \frac{\epsilon}{\sqrt{\lambda}} T \sqrt{(2 \lambda+1) \gamma_{T}} \
&amp; =O\left(B \sqrt{\gamma_{T} T}+\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}+\epsilon T \sqrt{\gamma_{T}}\right)
\end{aligned}
$$</p>
<p>where Eq. (33) follows from the validity of the enlarged confidence bounds (by combining Lemmas 1 and 2) and Eq. (34) follows from the selection rule of EC-GP-UCB (Eq. (14)). Finally, Eq. (38) is due to Eq. (31), and Eq. (39) follows by upper-bounding $\beta_{T}$ as in Eq. (30).</p>
<h2>B. 2 EC-GP-UCB and unknown misspecification</h2>
<p>In this section, we outline the main hindrance with the analysis of EC-GP-UCB (or GP-UCB [40]) when $\epsilon$ is unknown. We start with the definition of $R_{T}$ (Eq. (4)) and repeat the initial steps as in Eq. (33):</p>
<p>$$
\begin{aligned}
R_{T} &amp; =\sum_{t=1}^{T}\left(\max <em t="t">{x \in \mathcal{D}} \tilde{f}(x)-\tilde{f}\left(x</em>\right)\right) \
&amp; \leq \sum_{t=1}^{T} \mu_{t-1}^{<em>}\left(x^{</em>}\right)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x^{<em>}\right)-\left(\mu_{t-1}^{</em>}\left(x_{t}\right)-\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right)\right)
\end{aligned}
$$</p>
<p>Since $\epsilon$ is unknown here, we cannot repeat the analysis from the previous section as the learner cannot choose:</p>
<p>$$
\underset{x \in \mathcal{D}}{\arg \max } \mu_{t-1}^{*}(x)+\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}(x)
$$</p>
<p>Instead, it can select:</p>
<p>$$
x_{t} \in \underset{x \in \mathcal{D}}{\arg \max } \mu_{t-1}^{*}(x)+\beta_{t} \sigma_{t-1}(x)
$$</p>
<p>which corresponds to the standard GP-UCB algorithm when $\epsilon=0$. By using this rule in Eq. (42), we can arrive at:</p>
<p>$$
\begin{aligned}
R_{T} &amp; \leq \sum_{t=1}^{T} \mu_{t-1}^{<em>}\left(x^{</em>}\right)+\beta_{t} \sigma_{t-1}\left(x^{<em>}\right)+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x^{</em>}\right)-\left(\mu_{t-1}^{<em>}\left(x_{t}\right)-\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right)\right) \
&amp; \leq \sum_{t=1}^{T} \mu_{t-1}^{</em>}\left(x_{t}\right)+\beta_{t} \sigma_{t-1}\left(x_{t}\right)+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x^{<em>}\right)-\left(\mu_{t-1}^{</em>}\left(x_{t}\right)-\left(\beta_{t}+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}}\right) \sigma_{t-1}\left(x_{t}\right)\right) \
&amp; =\sum_{t=1}^{T} 2 \beta_{t} \sigma_{t-1}\left(x_{t}\right)+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x_{t}\right)+\frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x^{*}\right)
\end{aligned}
$$</p>
<p>While the first two terms in this bound can be effectively controlled and bounded as in the proof of Theorem 1, the last term, i.e., $\sum_{t=1}^{T} \frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t-1}\left(x^{<em>}\right)$, poses an issue since we cannot ensure that $\sum_{t=1}^{T} \sigma_{t-1}\left(x^{</em>}\right)$ is decaying with $t$, similarly to $\sum_{t=1}^{T} \sigma_{t-1}\left(x_{t}\right)$.</p>
<h1>C Optimal dependence on misspecification parameter</h1>
<p>In this section, we argue that a joint dependence on $T \epsilon$ is unavoidable in cumulative regret bounds. We consider the noiseless case, and we let the domain be the unit hypercube $\mathcal{D}=[0,1]^{d}$. Consider some function $f(x)$ defined on $\mathcal{D}$ such that $f(x) \in[-2 \zeta, 2 \zeta]$ for every $x \in D$. Moreover, let $f(x)$ satisfy the constant RKHS norm bound $B$, and let there exist a non-empty region $\mathcal{W} \subset \mathcal{D}$ where $f(x) \geq \zeta$ for every $x \in \mathcal{W}$. Such a function can easily be constructed, e.g., via the approach outlined in [36].
Now suppose that $\epsilon=2 \zeta$ and let the true unknown function $f^{<em>}$ be 0 everywhere in $\mathcal{D}$, except at a single point $x \in \mathcal{W}$ where it is $2 \zeta$. Hence, any algorithm that tries to optimize $f^{</em>}$ will only observe 0 -values almost surely, since sampling at the point where the function value is $\epsilon=2 \zeta$ is a zeroprobability event. Hence, after $T$ rounds, regardless of the sampling algorithm, $\Omega(\epsilon T)$ regret will be incurred. Finally, it is not hard to see that $|f-f^{<em>}|_{\infty} \leq 2 \zeta=\epsilon$, and so there exists a function of bounded RKHS norm that is $\epsilon$ pointwise close to $f^{</em>}$.</p>
<h2>D Proofs from Section 3.3 (Phased GP Uncertainty Sampling)</h2>
<p>We start this section by outlining the following auxiliary lemma and then we proceed with the proof of Theorem 2. The following lemma provides an upper bound on the difference between the mean estimators obtained from querying the true and best-in-class functions, respectively. Here, for the sake of analysis, we use $\mu_{t}(\cdot)$ to denote the hypothetical mean estimator in case $m$ noisy observations of the best-in-class function are available.
Lemma 2. For any $x \in \mathcal{D}, t \geq 1$ and $\lambda&gt;0$, we have</p>
<p>$$
\left|\mu_{t}(x)-\mu_{t}^{*}(x)\right| \leq \frac{\epsilon \sqrt{t}}{\sqrt{\lambda}} \sigma_{t}(x)
$$</p>
<p>where $\mu_{t}(\cdot)$ and $\mu_{t}^{<em>}(\cdot)$ are defined as in Eq. (8) and Eq. (11), respectively, and $\sigma_{t}(\cdot)$ is from Eq. (9).
Proof. Our proof closely follows the one of [4, Lemma 2], with the problem-specific difference at the very end of the proof (see Eq. (53)).
Let $x$ be any point in $\mathcal{D}$, and fix a time index $t \geq 1$. Recall that $Y_{t}^{</em>}=\left[y_{1}^{<em>}, \ldots, y_{t}^{</em>}\right]$ where each $y_{i}^{<em>}$ for $i \leq t$, is obtained as in Eq. (1). Following upon Eq. (7), we can write $Y_{t}=\left[y_{1}^{</em>}-m\left(x_{1}\right), \ldots, y_{t}^{<em>}-\right.$ $\left.m\left(x_{t}\right)\right]$ which correspond to the hypothetical noisy observations of the function belonging to the learner's RKHS. From the definitions of $\mu_{t}(\cdot)$ and $\mu_{t}^{</em>}(\cdot)$, we have:</p>
<p>$$
\begin{aligned}
\left|\mu_{t}^{<em>}(x)-\mu_{t}(x)\right| &amp; =\left|k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} Y_{t}^{</em>}-k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} \tilde{Y}<em t="t">{t}\right| \
&amp; =\left|k</em>\right|
\end{aligned}
$$}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} m_{t</p>
<p>where $m_{t}=\left[m\left(x_{1}\right), \ldots, m\left(x_{t}\right)\right]$. We proceed by upper bounding the absolute difference, i.e., $\left|k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} m_{t}\right|$, but first we define some additional terms.</p>
<p>Let $\mathcal{H}<em k="k">{k}(\mathcal{D})$ denote the learner's hypothesis space, i.e., RKHS of functions equipped with innerproduct $\langle\cdot, \cdot\rangle</em>$ and corresponding norm $|\cdot|<em k="k">{k}$. This space is completely determined by its associated $k(\cdot, \cdot)$ that satisfies: (i) $k(x, \cdot) \in \mathcal{H}</em>}(\mathcal{D})$ for all $x \in \mathcal{D}$ and (ii) $f(x)=\langle f, k(x, \cdot)\rangle_{k}$ for all $x \in \mathcal{D}$ (reproducing property). Due to these two properties and by denoting $\phi(x):=k(x, \cdot)$, we can write $k\left(x, x^{\prime}\right)=\left\langle k\left(x, \cdot\right), k\left(x^{\prime}, \cdot\right)\right\rangle_{k}=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{k}$ for all $x, x^{\prime} \in D$. Moreover, let $\Phi_{t}$ denote operator $\Phi_{t}: \mathcal{H<em k="k">{k}(\mathcal{D}) \rightarrow \mathbb{R}^{t}$, such that for every $f \in \mathcal{H}</em> f\right)}(\mathcal{D})$ and $i \in{1, \ldots, t}$, we have $\left(\Phi_{t<em i="i">{i}=$ $\left\langle\phi\left(x</em>^{}\right), f\right\rangle_{k}$, and also let $\Phi_{t<em>}$ denote its adjoint $\Phi_{t}^{</em>}: \mathbb{R}^{t} \rightarrow \mathcal{H}<em t="t">{k}(\mathcal{D})$. We can then write $K</em>$.
By using the following property of linear operators:}=\Phi_{t} \Phi_{t}^{*}$, and $k_{t}(x)=\Phi_{t} \phi(x)$. We also define the weighted norm of vector $x$, by $|x|_{\Phi}=\sqrt{\langle x, \Phi x\rangle</p>
<p>$$
\left(\Phi_{t}^{<em>} \Phi_{t}+\lambda I_{k}\right)^{-1} \Phi_{t}^{</em>}=\Phi_{t}^{<em>}\left(\Phi_{t} \Phi_{t}^{</em>}+\lambda I_{t}\right)^{-1}
$$</p>
<p>we first have:</p>
<p>$$
\begin{aligned}
\left|k_{t}(x)^{T}\left(K_{t}+\lambda I_{t}\right)^{-1} m_{t}\right| &amp; =\left|\left\langle\left\langle\Phi_{t}^{<em>} \Phi_{t}+\lambda I_{k}\right)^{-1} \phi(x), \Phi_{t}^{</em>} m_{t}\right\rangle_{k}\right| \
&amp; \leq\left|\left(\Phi_{t}^{<em>} \Phi_{t}+\lambda I_{k}\right)^{-1 / 2} \phi(x)\right|<em t="t">{k}\left|\left(\Phi</em>^{</em>} \Phi_{t}+\lambda I_{k}\right)^{-1 / 2} \Phi_{t}^{<em>} m_{t}\right|<em _left_Phi__t="\left(\Phi_{t">{k} \
&amp; =|\phi(x)|</em>^{</em>} \Phi_{t}+\lambda I_{k}\right)^{-1}}\left|\Phi_{t}^{<em>} m_{t}\right|<em t="t">{\left(\Phi</em>^{</em>} \Phi_{t}+\lambda I_{k}\right)^{-1}} \
&amp; =\lambda^{-1 / 2} \sigma_{t}(x) \sqrt{\left\langle\Phi_{t} \Phi_{t}^{<em>} m_{t},\left(\Phi_{t} \Phi_{t}^{</em>}+\lambda I_{t}\right)^{-1} m_{t}\right\rangle} \
&amp; =\lambda^{-1 / 2} \sigma_{t}(x) \sqrt{m_{t}^{T} K_{t}\left(K_{t}+\lambda I_{t}\right)^{-1} m_{t}} \
&amp; \leq \lambda^{-1 / 2} \sigma_{t}(x) \sqrt{\lambda_{\max }\left(K_{t}\left(K_{t}+\lambda I_{t}\right)^{-1}\right)\left|m_{t}\right|<em t="t">{2}^{2}} \
&amp; \leq \frac{\epsilon \sqrt{t}}{\lambda^{1 / 2}} \sigma</em>(x)
\end{aligned}
$$</p>
<p>where Eq. (49) is by Cauchy-Schwartz, and Eq. (51) follows from the following standard identity (see, e.g., Eq. (50) in [4]):</p>
<p>$$
\sigma_{t}(x)=\lambda^{1 / 2}|\phi(x)|<em t="t">{\left(\Phi</em>
$$}^{*} \Phi_{t}+\lambda I_{k}\right)^{-1}</p>
<p>Finally, $\lambda_{\max }(\cdot)$ denotes a maximum eigenvalue in Eq. (53), and Eq. (54) follows since for $\lambda&gt;0$, we have $\lambda_{\max }\left(K_{t}\left(K_{t}+\lambda I_{t}\right)^{-1}\right) \leq 1$, as well as by upper bounding $\left|m_{t}\right|<em t="t">{2} \leq \sqrt{t}\left|m</em>\right|<em t="t">{\infty}$ where $\left|m</em> \leq \epsilon$ which holds by definition of $m(\cdot)$ (see Eq. (7)).}\right|_{\infty</p>
<p>Now, we are ready to state the proof of Theorem 2.
Theorem 2. Suppose the learner's hypothesis class is $\mathcal{F}<em _infty="\infty">{k}(\mathcal{D} ; B)$ for some fixed $B&gt;0$ and $\mathcal{D} \subset \mathbb{R}^{d}$. For any $f^{<em>}$ defined on $\mathcal{D}$ and $\epsilon \geq 0$ such that $\min <em k="k">{f \in \mathcal{F}</em>\left|f-f^{}(\mathcal{D} ; B)</em>}\right|</em> \leq \epsilon$, Phased GP Uncertainty Sampling (Algorithm 2) achieves the following regret bound with probability at least $1-\delta$ :</p>
<p>$$
R_{T}^{*}=\tilde{O}\left(B \sqrt{\gamma_{T} T}+\sqrt{\left(\ln (1 / \delta)+\gamma_{T}\right) \gamma_{T} T}+\epsilon T \sqrt{\gamma_{T}}\right)
$$</p>
<p>Proof. We present the proof by splitting it into three main parts. We start with episodic misspecification.
Episodic misspecification. First, we bound the absolute difference between the misspecified mean estimator $\mu_{m_{e}}^{<em>}(\cdot)$ (from Eq. (11)) and best-in-class function $f \in \mathcal{F}<em _in="\in" _mathcal_F="\mathcal{F" f="f">{k}(D ; B)$ (i.e., $f \in \arg \min </em>\left|f-f^{}_{k}(\mathcal{D} ; B)</em>}\right|<em m="m">{\infty} \leq \epsilon$ ) at the end of some arbitrary episode $e$. Also, $\mu</em>(\cdot)$ is defined in Eq. (8), where noisy observations in the definition correspond to $f(\cdot)$.
For any $x \in \mathcal{D}_{e}$, we have:</p>
<p>$$
\begin{aligned}
\left|\mu_{m_{e}}^{<em>}(x)-f(x)\right| &amp; =\left|\mu_{m_{e}}^{</em>}(x)+\mu_{m_{e}}(x)-\mu_{m_{e}}(x)-f(x)\right| \
&amp; \leq\left|\mu_{m_{e}}^{*}(x)-\mu_{m_{e}}(x)\right|+\left|\mu_{m_{e}}(x)-f(x)\right| \
&amp; \leq \frac{\epsilon \sigma_{m_{e}}(x) \sqrt{m_{e}}}{\sqrt{\lambda}}+\beta_{m_{e}+1} \sigma_{m_{e}}(x)
\end{aligned}
$$</p>
<p>Here, Eq. (57) follows from triangle inequality, and Eq. (58) follows from Lemmas 1 and 2.</p>
<p>Next, we make the following observation:</p>
<p>$$
\begin{aligned}
\max <em m__e="m_{e">{x \in D} \sigma</em>\right) \
&amp; \leq \frac{1}{m_{e}} \sqrt{m_{e}(1+2 \lambda) \gamma_{m_{e}}(k, \mathcal{D})}=\sqrt{\frac{(1+2 \lambda) \gamma_{m_{e}}(k, \mathcal{D})}{m_{e}}}
\end{aligned}
$$}}(x) &amp; \leq \frac{1}{m_{e}} \sum_{t=1}^{m_{e}} \sigma_{t-1}\left(x_{t</p>
<p>where Eq. (59) follows from the definition of $x_{t}$ (see Eq. (19)) and the fact that $\sigma_{t-1}(\cdot)$ is non-increasing in $t$. Finally, we used the result from Lemma 3 to arrive at Eq. (60) together with $\gamma_{m_{e}}\left(k, \mathcal{D}<em m__e="m_{e">{e}\right) \leq \gamma</em>}}(k, \mathcal{D})$ since $\mathcal{D<em m__e="m_{e">{e} \subseteq \mathcal{D}$.
By upper bounding $\sigma</em>(x)$ in the first term in Eq. (58) with $\max }<em m__e="m_{e">{x \in D} \sigma</em>$, it holds:}}(x)$, and by using the upper bound obtained in Eq. (60), we have that for any $x \in \mathcal{D}_{e</p>
<p>$$
\left|\mu_{m_{e}}^{*}(x)-f(x)\right| \leq \beta_{m_{e}+1} \sigma_{m_{e}}(x)+\epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(k, \mathcal{D})}
$$</p>
<p>Elimination. Because the algorithm does not use the "valid" confidence bounds for the best-in-class $f$ in Eq. (20), it can eliminate its maximum even after the first episode. However, we show that there always remains a point $\hat{x}<em e="e">{e}$ that is "close" to the best point (defined below) in every episode $e$.
Let $D</em>}$ denote the remaining points at the beginning of the episode $e$ (in Algorithm 2) and let $\hat{x<em _in="\in" _mathcal_D="\mathcal{D" x="x">{e}=$ $\arg \max </em><em m__e="m_{e">{e}}\left{\mu</em>^{}<em>}(x)-\beta_{m_{e}+1} \sigma_{m_{e}}(x)\right}$. We note that this point will remain in $\mathcal{D}<em e="e">{e+1}$ according to the condition in Eq. (20) for retaining points. Next, we assume the worst-case scenario that the optimal point $x</em>^{</em>}=\arg \max <em e="e">{x \in \mathcal{D}</em>^{}} f(x)$ is eliminated at the end of the episode, i.e., $x_{e<em>} \notin \mathcal{D}<em e="e">{e+1}$. Then it holds, due to the condition Eq. (20) inside the algorithm that (note that both $\hat{x}</em>^{}$ and $x_{e</em>}$ are in $\mathcal{D}_{e}$ ):</p>
<p>$$
\mu_{m_{e}}^{<em>}\left(\hat{x}<em m__e="m_{e">{e}\right)-\beta</em>}+1} \sigma_{m_{e}}\left(\hat{x<em m__e="m_{e">{e}\right)&gt;\mu</em>^{}</em>}\left(x_{e}^{<em>}\right)+\beta_{m_{e}+1} \sigma_{m_{e}}\left(x_{e}^{</em>}\right)
$$</p>
<p>Next, by applying Eq. (61) to both sides, we obtain</p>
<p>$$
f\left(\hat{x}<em m__e="m_{e">{e}\right)+\epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma</em>}}\left(\mathcal{D<em e="e">{e}\right)}&gt;f\left(x</em>
$$}^{*}\right)-\epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(\mathcal{D})</p>
<p>and by rearranging we obtain</p>
<p>$$
2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(\mathcal{D})}&gt;f\left(x_{e}^{*}\right)-f\left(\hat{x}_{e}\right)
$$</p>
<p>which bounds the difference between the function values of the optimal (possibly eliminated from $\mathcal{D}<em e_1="e+1">{e+1}$ ) point and the one that is retained. We also note that for $x</em>=\arg \max }^{*<em e_1="e+1">{x \in \mathcal{D}</em> f(x)$, it holds}</p>
<p>$$
2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(\mathcal{D})}&gt;f\left(x_{e}^{<em>}\right)-f\left(x_{e+1}^{</em>}\right)
$$</p>
<p>since $f\left(\hat{x}<em e_1="e+1">{e}\right) \leq f\left(x</em>^{<em>}\right)$ and both $\hat{x}<em e_1="e+1">{e}, x</em>^{</em>} \in \mathcal{D}<em e="e">{e+1}$.
Regret. We can now proceed to obtain the main regret bound. First, we show the following bound that holds for every $x \in D</em>^{}$. We let $x_{e<em>}=\arg \max <em e="e">{x \in D</em>^{}} f(x)$. Because both the considered point $x$ and $x_{e</em>}$ belong to $D_{e}$, it means that they are not eliminated in the previous episode. Hence, we have</p>
<p>$$
\begin{aligned}
f\left(x_{e}^{<em>}\right)-f(x) \leq &amp; \mu_{m_{e-1}}^{</em>}\left(x_{e}^{<em>}\right)+\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}\left(x_{e}^{</em>}\right)+\epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)} \
&amp; -\left(\mu_{m_{e-1}}^{<em>}(x)-\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)-\epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)}\right) \
= &amp; \mu_{m_{e-1}}^{</em>}\left(x_{e}^{<em>}\right)-\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}\left(x_{e}^{</em>}\right)+2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}\left(x_{e}^{<em>}\right)-\mu_{m_{e-1}}^{</em>}(x) \
&amp; -\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)+2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)+2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)} \
\leq &amp; \max <em e-1="e-1">{x \in D</em>^{}}\left{\mu_{m_{e-1}<em>}(x)-\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)\right}+2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}\left(x_{e}^{</em>}\right)-\mu_{m_{e-1}}^{<em>}(x) \
&amp; -\beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)+2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x)+2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)} \
\leq &amp; 2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}\left(x_{e}^{</em>}\right)+2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)}+2 \beta_{\left(m_{e-1}+1\right)} \sigma_{m_{e-1}}(x) \
\leq &amp; 2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)}+4 \beta_{\left(m_{e-1}+1\right)} \max <em e-1="e-1">{x \in D</em>(x) \
\leq &amp; 2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)}+4 \beta_{\left(m_{e-1}+1\right)} \sqrt{\frac{(1+2 \lambda) \gamma_{m_{e-1}}\left(k ; D_{e-1}\right)}{m_{e-1}}}
\end{aligned}
$$}} \sigma_{m_{e-1}</p>
<p>Here, Eq. (66) follows from applying Eq. (61) twice and using $m=m_{e-1}$. Next, Eq. (67) follows from the rule in Eq. (20) for retaining points in the algorithm and by noting that $x \in D_{e}$. Finally, Eq. (69) follows from Eq. (60).
We proceed to upper bound the total regret of our algorithm. We upper bound $R_{T}^{<em>}$ by providing an upper bound for $R_{T}$ and using the fact that $R_{T}^{</em>} \leq R_{T}+2 \epsilon T$. We also use $R_{e}$ to denote the regret incurred in episode $e, x_{t}^{(e)}$ to denote the selected point at time $t$ in episode $e$, and $E \leq\left\lceil\log <em _in="\in" _mathcal_F="\mathcal{F" f="f">{2} T\right\rceil$ to denote the total number of episodes. Finally, we consider $f \in \arg \min </em><em _in="\in" _mathcal_D="\mathcal{D" x="x">{k}(\mathcal{D} ; B)}\left|f-f^{<em>}\right|_{\infty}$ and denote $x^{</em>}=\arg \max </em> f(x)$. It follows that}</p>
<p>$$
\begin{aligned}
R_{T} &amp; \leq \sum_{e=1}^{E} R_{e} \
&amp; \leq m_{1} B+\sum_{e=2}^{E} \sum_{t=1}^{m_{e}}\left(f\left(x^{<em>}\right)-f\left(x_{t}^{(e)}\right)\right) \
&amp; =B+\sum_{e=2}^{E} \sum_{t=1}^{m_{e}}\left(f\left(x^{</em>}\right)-f\left(x_{e}^{<em>}\right)+f\left(x_{e}^{</em>}\right)-f\left(x_{t}^{(e)}\right)\right) \
&amp; \leq B+\sum_{e=2}^{E} m_{e}\left(2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e-1}}\left(\mathcal{D}<em _left_m__e-1="\left(m_{e-1">{e-1}\right)}+4 \beta</em>}+1\right)} \sqrt{\frac{(1+2 \lambda) \gamma_{m_{e-1}}\left(\mathcal{D<em e-1="e-1">{e-1}\right)}{m</em>\right) \
&amp; \quad+\sum_{e=2}^{E} m_{e}\left(f\left(x^{}}<em>}\right)-f\left(x_{e}^{</em>}\right)\right) \
&amp; \leq B+2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{T}(\mathcal{D})} \sum_{e=2}^{E} m_{e}+4 \beta_{T} \sqrt{(1+2 \lambda) \gamma_{T}(\mathcal{D})} \sum_{e=2}^{E} m_{e} \sqrt{\frac{1}{m_{e-1}}} \
&amp; \quad+\sum_{e=2}^{E} m_{e}\left(f\left(x^{<em>}\right)-f\left(x_{e}^{</em>}\right)\right) \
&amp; \leq B+6 \epsilon T \sqrt{\left(2+\lambda^{-1}\right) \gamma_{T}(\mathcal{D})}+32 \beta_{T} \sqrt{(1+2 \lambda) T \gamma_{T}(\mathcal{D})} \
&amp; \quad+\sum_{e=2}^{E} m_{e}\left(f\left(x^{<em>}\right)-f\left(x_{e}^{</em>}\right)\right)
\end{aligned}
$$</p>
<p>In Eq. (71), we used $m_{1}=1$ and the fact that bounds on the RKHS norm imply bounds on the maximal function value if the kernel $k(\cdot, \cdot)$ is bounded (in our case, $k(x, x) \leq 1$ for every $x$ ):</p>
<p>$$
|f(x)|=\left|\left\langle f, k(x, \cdot)\right\rangle_{k}\right| \leq|f|<em k="k">{k}|k(x, \cdot)|</em>=|f|<em k="k">{k}\left(k(x, \cdot), k(x, \cdot)\right)</em> \leq B
$$}^{1 / 2}=B \cdot k(x, x)^{1 / 2</p>
<p>Finally, Eq. (73) follows from Eq. (69), and in Eq. (74) we used that $\beta_{t}$ is non-decreasing in $t$, and $\gamma_{m_{e-1}}\left(\mathcal{D}<em T="T">{e-1}\right) \leq \gamma</em>$.
It remains to upper bound the term that corresponds to misspecified elimination, i.e., $\sum_{e=2}^{E} m_{e}\left(f\left(x^{}(\mathcal{D})$. To obtain Eq. (75), we used that $m_{e}=2 m_{e-1<em>}\right)-f\left(x_{e}^{</em>}\right)\right)$. First, we note that by Eq. (65) and monotonicity of $\gamma_{t}(\mathcal{D})$ both in $t$ and $\mathcal{D}$, we have</p>
<p>$$
\begin{aligned}
f\left(x^{<em>}\right)-f\left(x_{e}^{</em>}\right) &amp; =f\left(x^{<em>}\right)-f\left(x_{2}^{</em>}\right)+f\left(x_{2}^{<em>}\right)-\cdots-f\left(x_{e-1}^{</em>}\right)+f\left(x_{e-1}^{<em>}\right)-f\left(x_{e}^{</em>}\right) \
&amp; &lt;\sum_{i=1}^{e-1} 2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{i}}(\mathcal{D})} \
&amp; \leq \sum_{i=2}^{e} 2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(\mathcal{D})}=2(e-1) \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{e}}(\mathcal{D})}
\end{aligned}
$$</p>
<p>Hence, we obtain</p>
<p>$$
\begin{aligned}
\sum_{e=2}^{E} m_{e}\left(f\left(x^{<em>}\right)-f\left(x_{e}^{</em>}\right)\right) &amp; \leq 2 \epsilon \sqrt{\left(2+\lambda^{-1}\right) \gamma_{m_{E}}(\mathcal{D})} \sum_{e=2}^{E} m_{e}(e-1) \
&amp; \leq 6 \epsilon T(\log T) \sqrt{\left(2+\lambda^{-1}\right) \gamma_{T}(\mathcal{D})}
\end{aligned}
$$</p>
<p>Finally, by combining Eq. (75) with Eq. (81), we obtain</p>
<p>$$
R_{T}=O\left(\epsilon T(\log T) \sqrt{\gamma_{T}(\mathcal{D})}+\beta_{T} \sqrt{T \gamma_{T}(\mathcal{D})}\right)
$$</p>
<p>The final result follows by upper-bounding $\beta_{T}$ as in Eq. (30).</p>
<h1>E Contextual misspecified setting results</h1>
<p>To show the main result of section Section 4 (i.e., Proposition 1), we make use of the following theorem established in [32]. We recall it here for the sake of completeness.
Theorem 3 (Theorem 5.5 in [32]). Let the regret bounds for all base learners $i \in[M]$ be of the form:</p>
<p>$$
\mathcal{R}<em 1="1">{i}(t)=\min \left{c</em> t, t\right}
$$} \sqrt{t}+c_{2} \hat{\epsilon}_{i</p>
<p>where $\hat{\epsilon}<em 1="1">{i} \in(0,1]$ and $c</em>$ and $t$. The regret of Algorithm 3 is bounded for all $T$ with probability at least $1-\delta$ :}, c_{2}&gt;1$ are quantities that do not depend on $\hat{\epsilon}_{i</p>
<p>$$
R_{T}^{<em>}=O\left(M c_{1} \sqrt{T} \sqrt{\ln \frac{M \ln T}{\delta}}+M c_{2} \hat{\epsilon}_{i^{</em>}} T \sqrt{\ln \frac{M \ln T}{\delta}}+M c_{1}^{2}\right)
$$</p>
<p>Here, $i^{<em>}$ is any consistent base algorithm, i.e., $\hat{\epsilon}<em>{i} \leq \hat{\epsilon}</em>{i^{</em>}}$ for all inconsistent algorithms.
To apply this theorem together with our EC-GP-UCB algorithm, we note that the bound obtained for EC-GP-UCB in Theorem 1 is of the following form:</p>
<p>$$
\tilde{\mathcal{R}}<em 1="1">{i}(t)=\min \left{c</em> t, t\right}
$$}^{\prime}\left(B \sqrt{\gamma_{t}}+\gamma_{t}\right) \sqrt{t}+c_{2}^{\prime} \epsilon_{i} \sqrt{\gamma_{t}</p>
<p>In Algorithm 3, we make use of the following candidate upper bound instead:</p>
<p>$$
\mathcal{R}<em i="i">{i}\left(N</em>(t), t\right}
$$}(t)\right)=\min \left{c_{1}^{\prime}\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right) \sqrt{N_{i}(t)}+c_{2}^{\prime} \epsilon_{i} \sqrt{\gamma_{T}} N_{i</p>
<p>where $N_{i}(t)$ is the number of times algorithm $i$ was played up to time $t$. Here, due to monotonicity and $t \leq T$, we have that $\gamma_{T} \geq \gamma_{N_{i}(t)}$, and hence we can replace $\gamma_{N_{i}(t)}$ with $\gamma_{T}$ in $\tilde{\mathcal{R}}<em i="i">{i}\left(N</em>&gt;1$.
Hence, we can use the result of Theorem 3 to obtain the following regret bound:}(t)\right)$. Moreover, we set $c_{1}=c_{1}^{\prime}\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right)$ and $c_{2}^{\prime} \sqrt{\gamma_{T}}$ in Theorem 3. Both $c_{1}$ and $c_{2}$ are then independent of $N_{i}(t)$. We also note that we can suitably set $\lambda$ (in Eq. (39)) such that $c_{1}, c_{2</p>
<p>$$
R_{T}^{<em>}=O\left(M c_{1}^{\prime}\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right) \sqrt{T}+c_{2}^{\prime} \sqrt{\gamma_{T}} \hat{\epsilon}_{i^{</em>}} T\right) \sqrt{\ln \frac{M \ln T}{\delta}}+M\left(c_{1}^{\prime}\right)^{2}\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right)^{2}\right)
$$</p>
<p>We can then set $M=\left\lceil 1+\log <em T="T">{2}(\sqrt{T} / \gamma</em>})\right\rceil$ and $\hat{\epsilon<em T="T">{i}=\frac{2^{1-\epsilon}}{\sqrt{\gamma</em>}}}$, because we have for $\hat{\epsilon<em T="T">{1}=\frac{1}{\sqrt{\gamma</em>}}}$ that $\mathcal{R<em M="M">{i}(t)=t$ (which always holds) and, for $\hat{\epsilon}</em>}$ we have $\mathcal{R<em 1="1">{M}(t) \leq 2 c</em>$ which is only a constant factor away from the bound when $\epsilon=0$. Hence, by taking a union bound over events that consistent and inconsistent candidate regret bounds hold and do not hold, respectively, we can then state that with probability at least $1-M \delta$, it holds that}^{\prime}\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right) \sqrt{T</p>
<p>$$
R_{T}^{*}=\tilde{O}\left(\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right) \sqrt{T}+\epsilon T \sqrt{\gamma_{T}}+\left(B \sqrt{\gamma_{T}}+\gamma_{T}\right)^{2}\right)
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ When applied to Eq. (10), a solution found by a simple and efficient greedy algorithm achieves at least $(1-1 / e)^{-1} \gamma_{T}$ (this is due to submodularity of the mutual information; see, e.g., [40]).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
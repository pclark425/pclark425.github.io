<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7796 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7796</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7796</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-273962964</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.06590v1.pdf" target="_blank">CriticAL: Critic Automation with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Understanding the world through models is a fundamental goal of scientific research. While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models. Criticizing models deepens scientific understanding and drives the development of more accurate models. Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant--both rely heavily on understanding the modeling assumptions and domain. Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. Motivated by this, we introduce CriticAL (Critic Automation with Language Models). CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance. We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework. In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions. In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones. We show that both human and LLM judges consistently prefer CriticAL's critiques over alternative approaches in terms of transparency and actionability. Finally, we show that CriticAL's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7796.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7796.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CriticAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic Automation with Language Models (CriticAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that uses LLMs to propose model- and data-tailored summary statistics implemented as executable Python test-statistic functions, then converts them into hypothesis tests using model-generated samples to compute empirical p-values and filter significant discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian/statistical modeling; automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>model criticism framework / evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CriticAL framework (LLM-proposed summary statistics + hypothesis testing)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLM proposes scalar test-statistic functions tailored to a symbolic model and dataset; compute the test statistic on posterior-predictive (or model-generated) samples to form a null distribution, compute empirical p-values for the observed data, apply multiple-testing correction (Bonferroni) and flag significant discrepancies; produce natural-language critiques to guide model revision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Empirical posterior-predictive p-values (Bonferroni-adjusted); downstream ELPD-LOO change / win rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Empirical p-value: fraction of model-generated samples for which T(X, Y_pred) >= T(X, Y_obs); Bonferroni-adjusted p-values used to control family-wise error; downstream model comparison uses ELPD-LOO differences expressed relative to standard error (SE) thresholds (wins counted when difference > SE margin).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Stan PosteriorDB (PosteriorDB) and multiple synthetic datasets described in paper (radon-inspired synthetic, Student's t / Gaussian, NB / Poisson, GLM / logistic-growth lesioned pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation: three Ph.D. students (non-authors, statistics expertise), blind to method, evaluated critiques on transparency, actionability, and tailoredness across 10 randomly selected model-dataset pairs; additionally LLM judges (GPT-4o, Claude-3.5-sonnet) used for scaled judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CriticAL helps revision LLM improve upon initial expert models: win rates versus initial model >1 SE = 0.94, >1.5 SE = 0.94, >2.0 SE = 0.82 (Table 1). Versus data-blind critic: CriticAL wins >1 SE = 0.59, >1.5 SE = 0.59, >2.0 SE = 0.53 (Table 2). CriticAL classified significantly more transparent (~97%), actionable (~76%), and tailored (~98%) by LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>CriticAL-driven revisions outperform initial (human-designed) models frequently (see win rates above); also outperforms a data-blind LLM critic (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Does not see raw model predictions/data (can miss transformation issues); sometimes identifies correct criticism but the revision LLM implements it incorrectly (e.g., mis-transposed logits or incorrect likelihood usage).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7796.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior Predictive Check (PPC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian model-checking technique where samples from the posterior predictive distribution are compared with observed data via model-dependent test statistics to assess whether the model captures important data properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian statistics / model checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>diagnostic / hypothesis test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Posterior Predictive Check (PPC) / posterior predictive p-value</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Draw samples {Y_ppred_i} from p(Y_ppred | D, H), compute a model-dependent test statistic T(X, Y) on both observed data and samples, compare observed T to the null distribution from samples to compute a posterior predictive p-value (fraction of samples with T >= observed).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Posterior predictive p-value (empirical)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>p_ppc ≈ (1/m) * sum_{i=1..m} 1{ T(X, Y_ppred_i) >= T(X, Y_obs) }; values near 0 or 1 indicate model-data misfit for the chosen T.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: reported posterior predictive p-values in examples include values reported as 0.0 and 0.0019 for severe discrepancies (e.g., radon floor effect).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Effectiveness depends on selecting model-dependent test statistics; PPC p-values can be sensitive to choice of T and do not alone identify fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7796.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Empirical p-value</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical p-value from model-generated samples (Equation 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric empirical p-value computed by comparing a test statistic on observed data to its distribution over model-generated samples to quantify significance of discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical hypothesis testing / Bayesian model checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric / hypothesis test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Empirical p-value (model-sample null)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Approximate the null distribution of a test statistic T by computing T over m model-generated samples; empirical p-value is the proportion of samples with T >= T(observed).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Empirical p-value p_k per test statistic</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>p_k ≈ (1/m) * sum_{i=1..m} 1{ T(X, Y_pred_i) >= T(X, Y_obs) }; used to judge significance at threshold α, with Bonferroni correction over multiple T_k.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used across experiments to decide significance; CriticAL filters proposals by p < α (e.g., retained 5 significant critiques at p < 0.01 in the radon synthetic example).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires ability to generate model samples; finite m yields Monte Carlo variability; multiple testing inflation must be handled (paper uses Bonferroni).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7796.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bonferroni</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bonferroni correction (multiple testing adjustment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family-wise error rate control method that adjusts p-values (or significance thresholds) when multiple hypothesis tests are performed, applied here to multiple LLM-proposed test statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / multiple hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical correction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bonferroni correction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Adjust p-values by multiplying by number of tests (or divide α by number of tests) to control family-wise error; the paper regards T_k with adjusted p_k <= α as significant.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Bonferroni-adjusted p-values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>p_adjusted_k = min(1, n * p_k) for n tests; significance if p_adjusted_k <= α.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Applied to avoid inflated false positive rates when evaluating multiple LLM-proposed test statistics; used selection criteria in Experiments (e.g., top five test statistics with Bonferroni-adjusted p < 0.15 selected for revision stage).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Conservative; may reduce power when many tests are proposed (paper chooses less stringent adjusted threshold in some experiments to allow useful lower-significance discrepancies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7796.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sliced test statistics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sliced test statistics (feature-sliced summary statistics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test statistics that compute discrepancies on subsets (slices) of the data partitioned by input features (e.g., variance of model predictions for basement vs first floor) to reveal localized model failures that aggregated statistics miss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical diagnostics / regression analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>test statistic design</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sliced test statistics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Condition or slice model predictions and/or data by values or categories of input features X and compute test statistics per slice to detect heterogeneity or feature-dependent misfit (e.g., slice by 'floor' to detect floor-dependent radon patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Slice-specific test statistics and associated empirical p-values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Compute T_slice(X_slice, Y_slice) for each slice; compare against model-sample distribution for the same slice; a significant p-value indicates slice-specific misfit.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>radon-inspired synthetic dataset and Stan PosteriorDB model-dataset pairs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports that sliced statistics reveal discrepancies that aggregated stats miss and that CriticAL often proposes sliced statistics; visual/summary comparisons shown (Figure 6, violin comparisons in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires specifying relevant slicing variables; CriticAL does not observe raw predictions/data at proposal time, so some slices may be suboptimal for transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7796.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELPD-LOO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Log Predictive Density via Leave-One-Out cross-validation (ELPD LOO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A predictive performance metric used to compare statistical models: estimates expected log predictive density using leave-one-out cross-validation, commonly used in Bayesian model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian model comparison / predictive evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ELPD-LOO (expected log predictive density, leave-one-out)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Estimate the log predictive density for each held-out observation via approximate LOO; sum or average to obtain expected log predictive density, compare models via difference in ELPD with associated standard error to assess significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ELPD difference and SE; win rates based on ELPD difference exceeding SE margins</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ELPD difference = ELPD(revised) - ELPD(initial); consider a win significant if difference > k * SE (k = 1.0, 1.5, 2.0); higher ELPD indicates better predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to Stan PosteriorDB model-dataset pairs and real-world datasets used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to compute win rates: CriticAL vs initial model wins >1 SE = 0.94, >1.5 SE = 0.94, >2.0 SE = 0.82 (Table 1); CriticAL vs data-blind wins >1 SE = 0.59, >1.5 SE = 0.59, >2.0 SE = 0.53 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to quantify how much CriticAL-driven revisions improve predictive performance relative to initial (human) models and a data-blind critic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LOO estimates rely on stable importance sampling or refitting; ELPD differences require SE estimation and may be noisy for small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7796.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROC / TPR / FPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Receiver Operating Characteristic analysis: True Positive Rate (TPR) vs False Positive Rate (FPR) and FPR calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical analysis used in the paper to characterize CriticAL's true positive (power) and false positive (type I error) behavior across significance thresholds when synthetic discrepancies are controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical hypothesis testing / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics / diagnostic curves</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROC curve (TPR vs FPR) and FPR calibration plots</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute true positive rate (fraction of discovery pairs correctly flagged) and false positive rate (fraction of no-discovery pairs incorrectly flagged) across various significance thresholds α; plot ROC and check that observed FPR tracks α.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>TPR, FPR, ROC curve; calibration of FPR vs α</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>TPR = (# true discoveries detected)/(# discovery pairs); FPR = (# no-discovery flagged)/(# no-discovery pairs); good calibration means empirical FPR ≈ chosen α.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Synthetic discovery / no-discovery model-dataset pairs (six model-data pairs, 20 random datasets each)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CriticAL exhibits favorable TPR vs FPR trade-off and outperforms a baseline of pre-specified statistics (mean and variance); FPR closely tracks significance threshold α (calibrated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Exact ROC numeric values not tabulated in text; dependent on synthetic setup choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7796.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Win-rate (SE margins)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Win-rate based on ELPD difference exceeding standard-error (SE) margins</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregated metric used to report how often one method (e.g., CriticAL) yields models with significantly higher ELPD-LOO than another, counting wins when the ELPD difference exceeds fixed multiples of the standard error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Model comparison / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>aggregate performance metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Win rate at SE margins (1.0, 1.5, 2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each dataset, compute ELPD difference and its SE; a method 'wins' if difference > SE_margin; aggregated win rate is fraction of datasets where method wins at that margin.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Win rate (fraction of datasets where ELPD difference > SE margin)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Win rate = (# datasets where ELPD_diff > SE_margin) / (total datasets); reported for margins 1.0, 1.5, 2.0 SE.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Stan PosteriorDB model-dataset pairs and other real-world datasets used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 1: CriticAL wins >1 SE = 0.94, >1.5 SE = 0.94, >2.0 SE = 0.82 versus initial model; Table 2: CriticAL wins >1 SE = 0.59, >1.5 SE = 0.59, >2.0 SE = 0.53 versus data-blind critic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>High win rates indicate CriticAL-driven revisions outperform initial human models and data-blind critic according to ELPD-LOO-based win criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Win counting simplifies continuous differences and depends on SE estimation; threshold choice affects interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7796.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qualitative criteria</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transparency, Actionability, and Tailoredness (qualitative evaluation criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three human- and LLM-judged qualitative criteria used to assess the usefulness of generated critiques: transparency (clarity of reasoning/executable artifacts), actionability (usefulness for revising models), and tailoredness (specificity to model and dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human evaluation / interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>qualitative evaluation criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human and LLM-based qualitative judgments on Transparency, Actionability, Tailoredness</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Blind evaluators (human Ph.D. students) and LLM judges compare criticisms from CriticAL and naive LLM critics on the three criteria; judgments include detailed comparisons and a final preference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference proportions and alignment rates (percentages of times CriticAL preferred on each criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported percentages of preferences (e.g., CriticAL preferred for transparency, actionability, tailoredness) and alignment between LLM judges and human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>10 randomly selected Stan PosteriorDB model-dataset pairs for human study; 36 pairs used overall for LLM evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three Ph.D. student experts (blind) for human evaluation across 10 pairs; LLM judges (GPT-4o, Claude-3.5-sonnet) used with specific prompts to scale judgments; LLM judges aligned with human preferences (100% alignment for transparent and tailored; 80% and 90% alignment for actionable).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM judges: CriticAL classified as significantly more transparent (~97%), actionable (~76%), and tailored (~98%); human experts qualitatively endorsed CriticAL's transparency benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Actionability sometimes limited when critique points to a deficiency but not a concrete revision (e.g., mismatch in range without identifying cause); LLM judge alignment imperfect for actionability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7796.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stan PosteriorDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stan PosteriorDB (PosteriorDB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A database of real-world datasets and probabilistic models implemented in Stan used in the paper for evaluating CriticAL across diverse model-dataset pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>posteriordb: a set of posteriors for Bayesian inference and probabilistic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian modeling / probabilistic programming benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Stan PosteriorDB benchmark usage</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use 36 model-dataset pairs from PosteriorDB to have LLM propose test statistics and generate natural language criticisms which are then evaluated qualitatively and used for model revision experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ELPD-LOO improvements, win rates, qualitative preferences</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Aggregated win rates and ELPD-LOO differences computed per PosteriorDB pair; qualitative criteria as above.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Stan PosteriorDB (PosteriorDB); 36 selected pairs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Subset of 10 paired used for human blind evaluation by 3 PhD students; LLM judges used for full set.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>36 PosteriorDB model-dataset pairs used; CriticAL critiques led to frequent improvements on these real-world pairs (reported aggregate win rates).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>CriticAL proposals can be suboptimal when data-specific transformations are required because CriticAL does not see raw model predictions at proposal time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7796.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Radon synthetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Radon-inspired synthetic regression dataset (with spurious distractor features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic regression dataset inspired by the radon dataset: target radon generated from a linear function of 'floor' and 'uppm' plus Gaussian noise, with additional spurious features (county, soil) added as distractors to test hallucination behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Regression / statistical model criticism</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>synthetic dataset for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Synthetic radon regression experiment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate synthetic data with known true features and distractors; fit an initial misspecified model (omitting floor), then compare naive LLM critic vs CriticAL in terms of hallucinated feature-additions and targeted revisions guided by significant test statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number of correct vs spurious features added in revision attempts; FPR vs significance threshold</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Count of revisions adding correct (floor) vs spurious (county, soil) features; FPR measured as fraction of false discoveries across significance thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Synthetic radon-inspired dataset used in Experiment 1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CriticAL filtered critiques to 5 significant ones (p < 0.01); 4 correctly identified radon varies by floor; naive LLM critic often recommends adding spurious features indiscriminately (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single synthetic setting; specific choices of distractors and noise affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7796.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic discovery pairs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic discovery / no-discovery model-dataset pairs (Student's t vs Gaussian, NB vs Poisson, GLM vs logistic-growth)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sets of synthetic model-dataset pairs where discovery pairs pair data from distribution p with lesioned model q (that misses key aspects), and no-discovery pairs sample from the same model; used to empirically estimate CriticAL's TPR and FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical hypothesis testing / benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>synthetic benchmark for sensitivity/specificity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Discovery / no-discovery synthetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate 20 random datasets per pair; for discovery pairs, data drawn from p but model samples from a lesioned q (e.g., Student's t data vs Gaussian model) to create real discrepancies; run CriticAL to compute whether any T_k yields adjusted p <= α and compute TPR/FPR across replicates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>True Positive Rate (power) and False Positive Rate (type I error) across controlled synthetic pairs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>TPR = proportion of discovery pairs flagged; FPR = proportion of no-discovery pairs flagged; evaluated over multiple α thresholds and m proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Six model-dataset pair types enumerated in paper (Student's t vs Gaussian, negative binomial vs Poisson, GLM vs logistic growth, etc.) with 20 datasets each</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CriticAL achieves favorable ROC (higher TPR at same FPR) compared to baseline pre-specified statistics; FPR calibration plot shows empirical FPR tracks α.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Results depend on synthetic choices; exact numeric TPR/FPR points not exhaustively tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7796.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based judges (GPT-4o and Claude-3.5-sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two state-of-the-art LLMs used as automated judges to evaluate qualitative properties (transparency, actionability, tailoredness) of generated critiques, following specific judging prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; Claude-3.5-sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated human-alignment judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based qualitative judging (GPT-4o, Claude-3.5-sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide paired criticisms and dataset/model context to LLM judges with explicit prompts; ask for detailed reasoning and a final preference (A or B) per criterion; aggregate preferences and compute alignment with human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference proportions, alignment with human judges</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentages of comparisons where CriticAL was preferred per criterion; alignment rate = fraction of LLM-judge preference matching human expert preference.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluations on selected model-dataset pairs from PosteriorDB (10 pairs for human study; 36 pairs overall for LLM evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM judges used to scale evaluations; human study involved 3 PhD students on 10 pairs; LLM judges showed 100% alignment for transparent and tailored preferences and 80%/90% alignment for actionable (GPT-4o/Claude).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM judges: CriticAL more transparent (~97%), actionable (~76%), tailored (~98%); judges aligned strongly with human evaluators on transparency and tailoredness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM judges may reflect model biases and their alignment with humans on actionability is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7796.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Ph.D.-level evaluators (statistics experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three Ph.D. students (non-authors) with statistics expertise who performed blind qualitative evaluations comparing CriticAL to a naive LLM critic along transparency, actionability, and tailoredness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human evaluation / statistical expertise</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human judgment / qualitative assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Blind human expert ranking on qualitative criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three experts, blind to method identity, compared CriticAL vs naive LLM critic across randomly selected model-dataset pairs and selected which critique was superior for each of three criteria, providing reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference counts / qualitative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction of pairwise comparisons where CriticAL was preferred per criterion; qualitative quotes and reasoning recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>10 randomly selected model-dataset pairs from PosteriorDB used in human study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three Ph.D. students, blind to methods, evaluated transparency, actionability, tailoredness; provided qualitative feedback praising executable code and immediate applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human experts favored CriticAL for transparency; qualitative comments highlighted the benefit of immediately executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small number of human evaluators (three) limits statistical generality; selection of 10 pairs was randomized but limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7796.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7796.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Revision LLM integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integration with LLM-based model discovery/revision system (Li et al. [17])</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CriticAL is integrated into an LLM-driven model-discovery pipeline where a revision LLM receives the initial model, selected test-statistic implementations, and natural-language criticisms to propose revised probabilistic programs which are then fit and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated model discovery / program synthesis for probabilistic models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>LM-driven model revision framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based model revision using CriticAL artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide the revision LLM (from Li et al. [17]'s system) with the initial probabilistic program H, Python test-statistic T_k, and natural language critique h_k and ask it to propose revised probabilistic programs; fit proposals (PyMC) and evaluate via ELPD-LOO.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ELPD-LOO improvement / win rates across proposals</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Select best revised model across multiple proposals; measure ELPD-LOO difference relative to initial and count wins at SE margins.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied across PosteriorDB pairs and other datasets in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using CriticAL outputs, revision LLMs produced models that significantly improved predictive performance (see win rates in Table 1 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>CriticAL-enabled revisions outperform initial human-designed models and outperform a data-blind critic when used by the revision LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Revision LLM non-determinism required multiple proposals; sometimes revision LLM mis-implements correct critique (implementation errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated Statistical Model Discovery with Language Models. <em>(Rating: 2)</em></li>
                <li>Posterior Predictive assessment of model fitness via realized discrepancies <em>(Rating: 2)</em></li>
                <li>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC <em>(Rating: 2)</em></li>
                <li>posteriordb: a set of posteriors for Bayesian inference and probabilistic programming. <em>(Rating: 2)</em></li>
                <li>Posterior Predictive p-Values <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7796",
    "paper_id": "paper-273962964",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "CriticAL",
            "name_full": "Critic Automation with Language Models (CriticAL)",
            "brief_description": "A framework introduced in this paper that uses LLMs to propose model- and data-tailored summary statistics implemented as executable Python test-statistic functions, then converts them into hypothesis tests using model-generated samples to compute empirical p-values and filter significant discrepancies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Bayesian/statistical modeling; automated scientific discovery",
            "theory_type": "model criticism framework / evaluation pipeline",
            "evaluation_method_name": "CriticAL framework (LLM-proposed summary statistics + hypothesis testing)",
            "evaluation_method_description": "LLM proposes scalar test-statistic functions tailored to a symbolic model and dataset; compute the test statistic on posterior-predictive (or model-generated) samples to form a null distribution, compute empirical p-values for the observed data, apply multiple-testing correction (Bonferroni) and flag significant discrepancies; produce natural-language critiques to guide model revision.",
            "evaluation_metric": "Empirical posterior-predictive p-values (Bonferroni-adjusted); downstream ELPD-LOO change / win rates",
            "metric_definition": "Empirical p-value: fraction of model-generated samples for which T(X, Y_pred) &gt;= T(X, Y_obs); Bonferroni-adjusted p-values used to control family-wise error; downstream model comparison uses ELPD-LOO differences expressed relative to standard error (SE) thresholds (wins counted when difference &gt; SE margin).",
            "dataset_or_benchmark": "Stan PosteriorDB (PosteriorDB) and multiple synthetic datasets described in paper (radon-inspired synthetic, Student's t / Gaussian, NB / Poisson, GLM / logistic-growth lesioned pairs)",
            "human_evaluation_details": "Human evaluation: three Ph.D. students (non-authors, statistics expertise), blind to method, evaluated critiques on transparency, actionability, and tailoredness across 10 randomly selected model-dataset pairs; additionally LLM judges (GPT-4o, Claude-3.5-sonnet) used for scaled judgments.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "CriticAL helps revision LLM improve upon initial expert models: win rates versus initial model &gt;1 SE = 0.94, &gt;1.5 SE = 0.94, &gt;2.0 SE = 0.82 (Table 1). Versus data-blind critic: CriticAL wins &gt;1 SE = 0.59, &gt;1.5 SE = 0.59, &gt;2.0 SE = 0.53 (Table 2). CriticAL classified significantly more transparent (~97%), actionable (~76%), and tailored (~98%) by LLM judges.",
            "comparison_to_human_generated": true,
            "comparison_results": "CriticAL-driven revisions outperform initial (human-designed) models frequently (see win rates above); also outperforms a data-blind LLM critic (Table 2).",
            "limitations_noted": "Does not see raw model predictions/data (can miss transformation issues); sometimes identifies correct criticism but the revision LLM implements it incorrectly (e.g., mis-transposed logits or incorrect likelihood usage).",
            "uuid": "e7796.0",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "PPC",
            "name_full": "Posterior Predictive Check (PPC)",
            "brief_description": "A Bayesian model-checking technique where samples from the posterior predictive distribution are compared with observed data via model-dependent test statistics to assess whether the model captures important data properties.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Bayesian statistics / model checking",
            "theory_type": "diagnostic / hypothesis test",
            "evaluation_method_name": "Posterior Predictive Check (PPC) / posterior predictive p-value",
            "evaluation_method_description": "Draw samples {Y_ppred_i} from p(Y_ppred | D, H), compute a model-dependent test statistic T(X, Y) on both observed data and samples, compare observed T to the null distribution from samples to compute a posterior predictive p-value (fraction of samples with T &gt;= observed).",
            "evaluation_metric": "Posterior predictive p-value (empirical)",
            "metric_definition": "p_ppc ≈ (1/m) * sum_{i=1..m} 1{ T(X, Y_ppred_i) &gt;= T(X, Y_obs) }; values near 0 or 1 indicate model-data misfit for the chosen T.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "Examples: reported posterior predictive p-values in examples include values reported as 0.0 and 0.0019 for severe discrepancies (e.g., radon floor effect).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Effectiveness depends on selecting model-dependent test statistics; PPC p-values can be sensitive to choice of T and do not alone identify fixes.",
            "uuid": "e7796.1",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Empirical p-value",
            "name_full": "Empirical p-value from model-generated samples (Equation 2)",
            "brief_description": "A nonparametric empirical p-value computed by comparing a test statistic on observed data to its distribution over model-generated samples to quantify significance of discrepancies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Statistical hypothesis testing / Bayesian model checking",
            "theory_type": "metric / hypothesis test",
            "evaluation_method_name": "Empirical p-value (model-sample null)",
            "evaluation_method_description": "Approximate the null distribution of a test statistic T by computing T over m model-generated samples; empirical p-value is the proportion of samples with T &gt;= T(observed).",
            "evaluation_metric": "Empirical p-value p_k per test statistic",
            "metric_definition": "p_k ≈ (1/m) * sum_{i=1..m} 1{ T(X, Y_pred_i) &gt;= T(X, Y_obs) }; used to judge significance at threshold α, with Bonferroni correction over multiple T_k.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "Used across experiments to decide significance; CriticAL filters proposals by p &lt; α (e.g., retained 5 significant critiques at p &lt; 0.01 in the radon synthetic example).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Requires ability to generate model samples; finite m yields Monte Carlo variability; multiple testing inflation must be handled (paper uses Bonferroni).",
            "uuid": "e7796.2",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Bonferroni",
            "name_full": "Bonferroni correction (multiple testing adjustment)",
            "brief_description": "A family-wise error rate control method that adjusts p-values (or significance thresholds) when multiple hypothesis tests are performed, applied here to multiple LLM-proposed test statistics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Statistics / multiple hypothesis testing",
            "theory_type": "statistical correction",
            "evaluation_method_name": "Bonferroni correction",
            "evaluation_method_description": "Adjust p-values by multiplying by number of tests (or divide α by number of tests) to control family-wise error; the paper regards T_k with adjusted p_k &lt;= α as significant.",
            "evaluation_metric": "Bonferroni-adjusted p-values",
            "metric_definition": "p_adjusted_k = min(1, n * p_k) for n tests; significance if p_adjusted_k &lt;= α.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "Applied to avoid inflated false positive rates when evaluating multiple LLM-proposed test statistics; used selection criteria in Experiments (e.g., top five test statistics with Bonferroni-adjusted p &lt; 0.15 selected for revision stage).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Conservative; may reduce power when many tests are proposed (paper chooses less stringent adjusted threshold in some experiments to allow useful lower-significance discrepancies).",
            "uuid": "e7796.3",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Sliced test statistics",
            "name_full": "Sliced test statistics (feature-sliced summary statistics)",
            "brief_description": "Test statistics that compute discrepancies on subsets (slices) of the data partitioned by input features (e.g., variance of model predictions for basement vs first floor) to reveal localized model failures that aggregated statistics miss.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Statistical diagnostics / regression analysis",
            "theory_type": "test statistic design",
            "evaluation_method_name": "Sliced test statistics",
            "evaluation_method_description": "Condition or slice model predictions and/or data by values or categories of input features X and compute test statistics per slice to detect heterogeneity or feature-dependent misfit (e.g., slice by 'floor' to detect floor-dependent radon patterns).",
            "evaluation_metric": "Slice-specific test statistics and associated empirical p-values",
            "metric_definition": "Compute T_slice(X_slice, Y_slice) for each slice; compare against model-sample distribution for the same slice; a significant p-value indicates slice-specific misfit.",
            "dataset_or_benchmark": "radon-inspired synthetic dataset and Stan PosteriorDB model-dataset pairs",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "Paper reports that sliced statistics reveal discrepancies that aggregated stats miss and that CriticAL often proposes sliced statistics; visual/summary comparisons shown (Figure 6, violin comparisons in appendix).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Requires specifying relevant slicing variables; CriticAL does not observe raw predictions/data at proposal time, so some slices may be suboptimal for transformations.",
            "uuid": "e7796.4",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ELPD-LOO",
            "name_full": "Expected Log Predictive Density via Leave-One-Out cross-validation (ELPD LOO)",
            "brief_description": "A predictive performance metric used to compare statistical models: estimates expected log predictive density using leave-one-out cross-validation, commonly used in Bayesian model comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Bayesian model comparison / predictive evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "ELPD-LOO (expected log predictive density, leave-one-out)",
            "evaluation_method_description": "Estimate the log predictive density for each held-out observation via approximate LOO; sum or average to obtain expected log predictive density, compare models via difference in ELPD with associated standard error to assess significance.",
            "evaluation_metric": "ELPD difference and SE; win rates based on ELPD difference exceeding SE margins",
            "metric_definition": "ELPD difference = ELPD(revised) - ELPD(initial); consider a win significant if difference &gt; k * SE (k = 1.0, 1.5, 2.0); higher ELPD indicates better predictive performance.",
            "dataset_or_benchmark": "Applied to Stan PosteriorDB model-dataset pairs and real-world datasets used in experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Used to compute win rates: CriticAL vs initial model wins &gt;1 SE = 0.94, &gt;1.5 SE = 0.94, &gt;2.0 SE = 0.82 (Table 1); CriticAL vs data-blind wins &gt;1 SE = 0.59, &gt;1.5 SE = 0.59, &gt;2.0 SE = 0.53 (Table 2).",
            "comparison_to_human_generated": true,
            "comparison_results": "Used to quantify how much CriticAL-driven revisions improve predictive performance relative to initial (human) models and a data-blind critic.",
            "limitations_noted": "LOO estimates rely on stable importance sampling or refitting; ELPD differences require SE estimation and may be noisy for small datasets.",
            "uuid": "e7796.5",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ROC / TPR / FPR",
            "name_full": "Receiver Operating Characteristic analysis: True Positive Rate (TPR) vs False Positive Rate (FPR) and FPR calibration",
            "brief_description": "Statistical analysis used in the paper to characterize CriticAL's true positive (power) and false positive (type I error) behavior across significance thresholds when synthetic discrepancies are controlled.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Statistical hypothesis testing / evaluation",
            "theory_type": "evaluation metrics / diagnostic curves",
            "evaluation_method_name": "ROC curve (TPR vs FPR) and FPR calibration plots",
            "evaluation_method_description": "Compute true positive rate (fraction of discovery pairs correctly flagged) and false positive rate (fraction of no-discovery pairs incorrectly flagged) across various significance thresholds α; plot ROC and check that observed FPR tracks α.",
            "evaluation_metric": "TPR, FPR, ROC curve; calibration of FPR vs α",
            "metric_definition": "TPR = (# true discoveries detected)/(# discovery pairs); FPR = (# no-discovery flagged)/(# no-discovery pairs); good calibration means empirical FPR ≈ chosen α.",
            "dataset_or_benchmark": "Synthetic discovery / no-discovery model-dataset pairs (six model-data pairs, 20 random datasets each)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "CriticAL exhibits favorable TPR vs FPR trade-off and outperforms a baseline of pre-specified statistics (mean and variance); FPR closely tracks significance threshold α (calibrated).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Exact ROC numeric values not tabulated in text; dependent on synthetic setup choices.",
            "uuid": "e7796.6",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Win-rate (SE margins)",
            "name_full": "Win-rate based on ELPD difference exceeding standard-error (SE) margins",
            "brief_description": "An aggregated metric used to report how often one method (e.g., CriticAL) yields models with significantly higher ELPD-LOO than another, counting wins when the ELPD difference exceeds fixed multiples of the standard error.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Model comparison / evaluation",
            "theory_type": "aggregate performance metric",
            "evaluation_method_name": "Win rate at SE margins (1.0, 1.5, 2.0)",
            "evaluation_method_description": "For each dataset, compute ELPD difference and its SE; a method 'wins' if difference &gt; SE_margin; aggregated win rate is fraction of datasets where method wins at that margin.",
            "evaluation_metric": "Win rate (fraction of datasets where ELPD difference &gt; SE margin)",
            "metric_definition": "Win rate = (# datasets where ELPD_diff &gt; SE_margin) / (total datasets); reported for margins 1.0, 1.5, 2.0 SE.",
            "dataset_or_benchmark": "Stan PosteriorDB model-dataset pairs and other real-world datasets used in experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Table 1: CriticAL wins &gt;1 SE = 0.94, &gt;1.5 SE = 0.94, &gt;2.0 SE = 0.82 versus initial model; Table 2: CriticAL wins &gt;1 SE = 0.59, &gt;1.5 SE = 0.59, &gt;2.0 SE = 0.53 versus data-blind critic.",
            "comparison_to_human_generated": true,
            "comparison_results": "High win rates indicate CriticAL-driven revisions outperform initial human models and data-blind critic according to ELPD-LOO-based win criteria.",
            "limitations_noted": "Win counting simplifies continuous differences and depends on SE estimation; threshold choice affects interpretation.",
            "uuid": "e7796.7",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Qualitative criteria",
            "name_full": "Transparency, Actionability, and Tailoredness (qualitative evaluation criteria)",
            "brief_description": "Three human- and LLM-judged qualitative criteria used to assess the usefulness of generated critiques: transparency (clarity of reasoning/executable artifacts), actionability (usefulness for revising models), and tailoredness (specificity to model and dataset).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Human evaluation / interpretability",
            "theory_type": "qualitative evaluation criteria",
            "evaluation_method_name": "Human and LLM-based qualitative judgments on Transparency, Actionability, Tailoredness",
            "evaluation_method_description": "Blind evaluators (human Ph.D. students) and LLM judges compare criticisms from CriticAL and naive LLM critics on the three criteria; judgments include detailed comparisons and a final preference.",
            "evaluation_metric": "Preference proportions and alignment rates (percentages of times CriticAL preferred on each criterion)",
            "metric_definition": "Reported percentages of preferences (e.g., CriticAL preferred for transparency, actionability, tailoredness) and alignment between LLM judges and human judges.",
            "dataset_or_benchmark": "10 randomly selected Stan PosteriorDB model-dataset pairs for human study; 36 pairs used overall for LLM evaluations",
            "human_evaluation_details": "Three Ph.D. student experts (blind) for human evaluation across 10 pairs; LLM judges (GPT-4o, Claude-3.5-sonnet) used with specific prompts to scale judgments; LLM judges aligned with human preferences (100% alignment for transparent and tailored; 80% and 90% alignment for actionable).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "LLM judges: CriticAL classified as significantly more transparent (~97%), actionable (~76%), and tailored (~98%); human experts qualitatively endorsed CriticAL's transparency benefit.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Actionability sometimes limited when critique points to a deficiency but not a concrete revision (e.g., mismatch in range without identifying cause); LLM judge alignment imperfect for actionability.",
            "uuid": "e7796.8",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Stan PosteriorDB",
            "name_full": "Stan PosteriorDB (PosteriorDB)",
            "brief_description": "A database of real-world datasets and probabilistic models implemented in Stan used in the paper for evaluating CriticAL across diverse model-dataset pairs.",
            "citation_title": "posteriordb: a set of posteriors for Bayesian inference and probabilistic programming.",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Bayesian modeling / probabilistic programming benchmarks",
            "theory_type": "dataset / benchmark",
            "evaluation_method_name": "Stan PosteriorDB benchmark usage",
            "evaluation_method_description": "Use 36 model-dataset pairs from PosteriorDB to have LLM propose test statistics and generate natural language criticisms which are then evaluated qualitatively and used for model revision experiments.",
            "evaluation_metric": "ELPD-LOO improvements, win rates, qualitative preferences",
            "metric_definition": "Aggregated win rates and ELPD-LOO differences computed per PosteriorDB pair; qualitative criteria as above.",
            "dataset_or_benchmark": "Stan PosteriorDB (PosteriorDB); 36 selected pairs",
            "human_evaluation_details": "Subset of 10 paired used for human blind evaluation by 3 PhD students; LLM judges used for full set.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "36 PosteriorDB model-dataset pairs used; CriticAL critiques led to frequent improvements on these real-world pairs (reported aggregate win rates).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "CriticAL proposals can be suboptimal when data-specific transformations are required because CriticAL does not see raw model predictions at proposal time.",
            "uuid": "e7796.9",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Radon synthetic",
            "name_full": "Radon-inspired synthetic regression dataset (with spurious distractor features)",
            "brief_description": "A synthetic regression dataset inspired by the radon dataset: target radon generated from a linear function of 'floor' and 'uppm' plus Gaussian noise, with additional spurious features (county, soil) added as distractors to test hallucination behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Regression / statistical model criticism",
            "theory_type": "synthetic dataset for evaluation",
            "evaluation_method_name": "Synthetic radon regression experiment",
            "evaluation_method_description": "Generate synthetic data with known true features and distractors; fit an initial misspecified model (omitting floor), then compare naive LLM critic vs CriticAL in terms of hallucinated feature-additions and targeted revisions guided by significant test statistics.",
            "evaluation_metric": "Number of correct vs spurious features added in revision attempts; FPR vs significance threshold",
            "metric_definition": "Count of revisions adding correct (floor) vs spurious (county, soil) features; FPR measured as fraction of false discoveries across significance thresholds.",
            "dataset_or_benchmark": "Synthetic radon-inspired dataset used in Experiment 1",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "CriticAL filtered critiques to 5 significant ones (p &lt; 0.01); 4 correctly identified radon varies by floor; naive LLM critic often recommends adding spurious features indiscriminately (Figure 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Single synthetic setting; specific choices of distractors and noise affect outcomes.",
            "uuid": "e7796.10",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Synthetic discovery pairs",
            "name_full": "Synthetic discovery / no-discovery model-dataset pairs (Student's t vs Gaussian, NB vs Poisson, GLM vs logistic-growth)",
            "brief_description": "Sets of synthetic model-dataset pairs where discovery pairs pair data from distribution p with lesioned model q (that misses key aspects), and no-discovery pairs sample from the same model; used to empirically estimate CriticAL's TPR and FPR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Statistical hypothesis testing / benchmarking",
            "theory_type": "synthetic benchmark for sensitivity/specificity analysis",
            "evaluation_method_name": "Discovery / no-discovery synthetic experiments",
            "evaluation_method_description": "Generate 20 random datasets per pair; for discovery pairs, data drawn from p but model samples from a lesioned q (e.g., Student's t data vs Gaussian model) to create real discrepancies; run CriticAL to compute whether any T_k yields adjusted p &lt;= α and compute TPR/FPR across replicates.",
            "evaluation_metric": "True Positive Rate (power) and False Positive Rate (type I error) across controlled synthetic pairs",
            "metric_definition": "TPR = proportion of discovery pairs flagged; FPR = proportion of no-discovery pairs flagged; evaluated over multiple α thresholds and m proposals.",
            "dataset_or_benchmark": "Six model-dataset pair types enumerated in paper (Student's t vs Gaussian, negative binomial vs Poisson, GLM vs logistic growth, etc.) with 20 datasets each",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": null,
            "reported_results": "CriticAL achieves favorable ROC (higher TPR at same FPR) compared to baseline pre-specified statistics; FPR calibration plot shows empirical FPR tracks α.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Results depend on synthetic choices; exact numeric TPR/FPR points not exhaustively tabulated in text.",
            "uuid": "e7796.11",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLM judges",
            "name_full": "LLM-based judges (GPT-4o and Claude-3.5-sonnet)",
            "brief_description": "Two state-of-the-art LLMs used as automated judges to evaluate qualitative properties (transparency, actionability, tailoredness) of generated critiques, following specific judging prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o; Claude-3.5-sonnet",
            "model_size": "",
            "scientific_domain": "Automated evaluation / NLP",
            "theory_type": "automated human-alignment judge",
            "evaluation_method_name": "LLM-based qualitative judging (GPT-4o, Claude-3.5-sonnet)",
            "evaluation_method_description": "Provide paired criticisms and dataset/model context to LLM judges with explicit prompts; ask for detailed reasoning and a final preference (A or B) per criterion; aggregate preferences and compute alignment with human judges.",
            "evaluation_metric": "Preference proportions, alignment with human judges",
            "metric_definition": "Percentages of comparisons where CriticAL was preferred per criterion; alignment rate = fraction of LLM-judge preference matching human expert preference.",
            "dataset_or_benchmark": "Evaluations on selected model-dataset pairs from PosteriorDB (10 pairs for human study; 36 pairs overall for LLM evaluations)",
            "human_evaluation_details": "LLM judges used to scale evaluations; human study involved 3 PhD students on 10 pairs; LLM judges showed 100% alignment for transparent and tailored preferences and 80%/90% alignment for actionable (GPT-4o/Claude).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "LLM judges: CriticAL more transparent (~97%), actionable (~76%), tailored (~98%); judges aligned strongly with human evaluators on transparency and tailoredness.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "LLM judges may reflect model biases and their alignment with humans on actionability is imperfect.",
            "uuid": "e7796.12",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Human evaluators",
            "name_full": "Human Ph.D.-level evaluators (statistics experts)",
            "brief_description": "Three Ph.D. students (non-authors) with statistics expertise who performed blind qualitative evaluations comparing CriticAL to a naive LLM critic along transparency, actionability, and tailoredness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Human evaluation / statistical expertise",
            "theory_type": "human judgment / qualitative assessment",
            "evaluation_method_name": "Blind human expert ranking on qualitative criteria",
            "evaluation_method_description": "Three experts, blind to method identity, compared CriticAL vs naive LLM critic across randomly selected model-dataset pairs and selected which critique was superior for each of three criteria, providing reasoning.",
            "evaluation_metric": "Preference counts / qualitative feedback",
            "metric_definition": "Fraction of pairwise comparisons where CriticAL was preferred per criterion; qualitative quotes and reasoning recorded.",
            "dataset_or_benchmark": "10 randomly selected model-dataset pairs from PosteriorDB used in human study",
            "human_evaluation_details": "Three Ph.D. students, blind to methods, evaluated transparency, actionability, tailoredness; provided qualitative feedback praising executable code and immediate applicability.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Human experts favored CriticAL for transparency; qualitative comments highlighted the benefit of immediately executable code.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Small number of human evaluators (three) limits statistical generality; selection of 10 pairs was randomized but limited.",
            "uuid": "e7796.13",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Revision LLM integration",
            "name_full": "Integration with LLM-based model discovery/revision system (Li et al. [17])",
            "brief_description": "CriticAL is integrated into an LLM-driven model-discovery pipeline where a revision LLM receives the initial model, selected test-statistic implementations, and natural-language criticisms to propose revised probabilistic programs which are then fit and evaluated.",
            "citation_title": "Automated Statistical Model Discovery with Language Models.",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "scientific_domain": "Automated model discovery / program synthesis for probabilistic models",
            "theory_type": "LM-driven model revision framework",
            "evaluation_method_name": "LLM-based model revision using CriticAL artifacts",
            "evaluation_method_description": "Provide the revision LLM (from Li et al. [17]'s system) with the initial probabilistic program H, Python test-statistic T_k, and natural language critique h_k and ask it to propose revised probabilistic programs; fit proposals (PyMC) and evaluate via ELPD-LOO.",
            "evaluation_metric": "ELPD-LOO improvement / win rates across proposals",
            "metric_definition": "Select best revised model across multiple proposals; measure ELPD-LOO difference relative to initial and count wins at SE margins.",
            "dataset_or_benchmark": "Applied across PosteriorDB pairs and other datasets in experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Using CriticAL outputs, revision LLMs produced models that significantly improved predictive performance (see win rates in Table 1 and Table 2).",
            "comparison_to_human_generated": true,
            "comparison_results": "CriticAL-enabled revisions outperform initial human-designed models and outperform a data-blind critic when used by the revision LLM.",
            "limitations_noted": "Revision LLM non-determinism required multiple proposals; sometimes revision LLM mis-implements correct critique (implementation errors).",
            "uuid": "e7796.14",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated Statistical Model Discovery with Language Models.",
            "rating": 2,
            "sanitized_title": "automated_statistical_model_discovery_with_language_models"
        },
        {
            "paper_title": "Posterior Predictive assessment of model fitness via realized discrepancies",
            "rating": 2,
            "sanitized_title": "posterior_predictive_assessment_of_model_fitness_via_realized_discrepancies"
        },
        {
            "paper_title": "Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC",
            "rating": 2,
            "sanitized_title": "practical_bayesian_model_evaluation_using_leaveoneout_crossvalidation_and_waic"
        },
        {
            "paper_title": "posteriordb: a set of posteriors for Bayesian inference and probabilistic programming.",
            "rating": 2,
            "sanitized_title": "posteriordb_a_set_of_posteriors_for_bayesian_inference_and_probabilistic_programming"
        },
        {
            "paper_title": "Posterior Predictive p-Values",
            "rating": 1,
            "sanitized_title": "posterior_predictive_pvalues"
        }
    ],
    "cost": 0.023506749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CriticAL: Critic Automation with Language Models
10 Nov 2024</p>
<p>Michael Y Li michaelyli@stanford.edu 
Stanford University</p>
<p>Vivek Vajipey 
Stanford University</p>
<p>Noah D Goodman 
Stanford University</p>
<p>Emily B Fox 
Stanford University</p>
<p>CriticAL: Critic Automation with Language Models
10 Nov 2024090F604A4439A9F65C512998DFC8028BarXiv:2411.06590v1[cs.LG]
Understanding the world through models is a fundamental goal of scientific research.While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models.Criticizing models deepens scientific understanding and drives the development of more accurate models.Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significantboth rely heavily on understanding the modeling assumptions and domain.Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves.Motivated by this, we introduce CriticAL (Critic Automation with Language Models).CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance.We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework.In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions.In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones.We show that both human and LLM judges consistently prefer CriticAL's critiques over alternative approaches in terms of transparency and actionability.Finally, we show that CriticAL's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.</p>
<p>Introduction</p>
<p>A longstanding goal of artificial intelligence research is to automate the discovery of scientific models [16,27].The rapid development of LLMs with remarkable reasoning capabilities and general knowledge has created exciting new opportunities within this domain.Recent work has shown that LLM-based scientific agents can propose research ideas [23], discover scientific models [17], and implement experiments [12,18].These results highlight the promise of using LLMs to automate many important aspects of scientific discovery.However, they overlook the crucial role that model criticism, or understanding the limitations of a model, plays in driving scientific progress.Model criticism deepens our understanding and often motivates new models.Furthermore, automated methods for criticism can improve the reliability of LLM-based scientific discovery systems, as LLMs are prone to systematic hallucinations [18,29] that could undermine the broader goal of automating scientific discovery.Model criticism is hard to automate because it is inherently dependent on the model and problem domain.In particular, it involves (1) determining which aspects to compare between the model and data and (2) evaluating the significance of any differences.Each of these tasks typically requires substantial human expertise [7].While leveraging LLMs is an initially appealing approach to automation, it introduces new challenges: LLMs might also hallucinate the critiques themselves, undermining the effectiveness of automated model criticism.</p>
<p>Motivated by these challenges, we introduce CriticAL (Critic Automation with Language Models), which integrates LLMs within a principled model criticism framework.Specifically, given a proposed scientific model and dataset metadata, CriticAL uses an LLM to generate summary statistics that capture properties of the data that might violate the modeling assumptions.Importantly, these summary statistics are tailored to the model and dataset.CriticAL implements these summary statistics as Python functions, which can be easily executed and inspected by a human or LLM scientist.This brings transparency to the critique process.While these summary statistics can highlight potential discrepancies, we need a method to determine whether these discrepancies are meaningful.To address this, we show how we can automatically convert the summary statistics produced by CriticAL into hypothesis tests, for many commonly-used scientific models.Specifically, if we can generate data from the scientific model [6,10], we can form a null distribution for a summary statistic and compute an empirical p-value.Thus, we can transform each summary statistic into a quantitative check, providing a rigorous way to assess both the significance of the discrepancies and the validity of the model.In doing so, we reduce the complex task of automatically validating proposed models and critiques to the well-understood problem of hypothesis testing.In experiments (Section 4), we evaluate CriticAL along key qualitative and quantitative properties crucial for an automated critic system.In settings where we synthetically control discrepancies between models and datasets, CriticAL consistently identifies true discrepancies and avoids hallucinating false ones.We also assess important qualitative aspects of CriticAL's critiques (e.g., transparency), and find that both LLM and human judges prefer CriticAL's critiques over alternatives.Finally, we demonstrate the practical impact of CriticAL's critiques on the downstream task of guiding an LLM-based scientific model discovery system.On real-world datasets, CriticAL's critiques enable an LLM-based automated model discovery system [17] to significantly improve upon initial human-designed models.</p>
<p>Background</p>
<p>In this section, we discuss model criticism techniques from different domains.Crucially, we can often formalize finding discrepancies as identifying suitable test statistics, using those statistics to compute discrepancies between model predictions and data, and validating their significance using domain knowledge.</p>
<p>Regression analysis</p>
<p>In regression analysis, we begin with a dataset D = {X , Y} of input features X and targets Y; our goal is to predict Y from X .Given model predictions Y pred , we perform model diagnostics that target the standard assumptions of linear regression (e.g., linearity, homoscedasticity, uncorrelated errors).For example, to evaluate whether homoscedasticity holds, we can plot the residuals against the input features.We can then either informally assess whether the pattern in the residuals indicates a significant departure from homoscedasticity or perform statistical tests.</p>
<p>Computational models Computational models often make simplifying assumptions that can lead to systematic errors, even after the parameters of these models are calibrated.This might be due to imperfect physical knowledge or systematic measurement errors; these systematic errors are often known as model inadequacies.Bayarri &amp; Berger [2] introduce a framework for understanding these inadequacies that involves defining domain-specific evaluation criteria or performing sensitivity analyses and checking whether these accord with scientific intuition.Another very influential approach is to cast this as a statistical modeling problem and directly build a statistical model of the discrepancy [14].Building on this work, Joseph &amp; Yan [13] show how to study this discrepancy through an analysis of variance decomposition.</p>
<p>Bayesian statistical models</p>
<p>In statistical modeling, we model the data as a probability distribution.More formally, a statistical model defines a joint probability distribution p(Y, θ|X , H) over observed variables Y and latent variables θ; we use H to indicate a specific class of statistical models and the dataset D = {X , Y} can include both observations Y that we model as random variables and additional quantities X that we treat as fixed.By marginalizing out the latent variables, we obtain the posterior predictive distribution
p(Y ppred | D, H) = p(Y ppred |θ, H)p(θ|D, H)dθ(1)
A common technique for evaluating such a model is a posterior predictive check (PPC) [4,8,20,22].</p>
<p>In brief, PPCs ask if the posterior predictive distribution captures important properties of the data.</p>
<p>Concretely, to perform a PPC, we first draw samples from the posterior predictive distribution,
{Y ppred i } m i=1 ∼ p(Y ppred | D, H).
We then choose a test statistic T (X , Y ppred ) that can reveal some property of the data that is not well-captured by the model samples.To compare the posterior predictive samples against the dataset, we compute the test statistic over both samples (forming a null distribution) and data.For a PPC to be useful, the test statistic must be chosen in a modeldependent way and choosing an appropriate test statistic is an important step in many applied modeling settings [3,9,25].For example, when criticizing a Poisson model, one might check for over-dispersion by computing the variance-to-mean ratio.Crucially, posterior predictive checks do not require human intervention, since they automatically generate a quantitative measure of the significance of any discrepancy via the posterior predictive p-value; we discuss this in more detail in Section 3.1.</p>
<p>Method: CriticAL</p>
<p>In this section, we describe CriticAL, our system for finding systematic discrepancies between a scientific model and dataset.We provide a brief overview here; for a schematic overview, see Figure 1.CriticAL takes as input: dataset metadata, a symbolic representation of a model (e.g., program) and model samples.Given these, CriticAL produces significant discrepancies.Each discrepancy is represented as a test statistic implemented as a Python function, an executable artifact that programmatically expresses the discrepancy, and a natural language criticism.</p>
<p>Automatically proposing and evaluating discrepancies</p>
<p>Evaluating significance of discrepancies via hypothesis tests</p>
<p>We now describe how CriticAL uses the test statistics to identify significant discrepancies.In brief, we use model samples to approximate a null distribution over the test statistic and then compute an empirical p-value.We assume the user can generate data from the model {Y pred i } m i=1 .This is not restrictive requirement and how the user generates the model samples is a design choice; for example, we can do this for any model that describes a generative process for the data.</p>
<p>We describe how to construct an empirical p-value p k given T k and {Y
pred i } m i=1 below.
1. We approximate the null distribution of the test statistic by computing the test statistic over the model samples {T (X , Y pred i</p>
<p>)} m i=1 .2. We locate the test statistic of the observed data T (X , Y) within this null distribution to obtain an empirical p-value.That is, we compute
P (T (X , Y pred ) ≥ T (X , Y)|D, H) ≈ 1 m m i=1 1 {T (X ,Y pred i )≥T (X ,Y )}(2)
We visualize the computation of the p-values in the Appendix (Figure 10).To capture different discrepancies, we compute multiple test statistics in parallel for a model-dataset pair.However, this can inflate the effective false positive rate: for large enough m, we expect min k p k ≤ α even if the model and dataset have no discrepancy.We thus apply a Bonferroni correction to obtain adjusted p-values {p k } m k=1 .We regard all T k such that pk ≤ α as significant.</p>
<p>Instantiating the framework for Bayesian models In our experiments, we focus our evaluation on Bayesian models because they are widely used in scientific settings [6,10].In our context, Bayesian models are also appealing because they can be expressed symbolically as probabilistic programs [11,24] and we can choose the model samples to be posterior predictive samples
{Y ppred i } m i=1
(Equation 1).The corresponding posterior predictive p-value has an intuitive interpretation: how atypical is Y under the posterior distribution p(Y ppred |D, H) with respect to the discrepancy measure defined by T k ?</p>
<p>Interfacing with LLM science agents via natural language criticism</p>
<p>In many situations, we might want to integrate CriticAL within a broader scientific discovery system, involving either human or LLM scientists.Therefore, CriticAL also produces natural language criticism.This design choice is motivated by several considerations.By offering critiques in natural language, which is flexible and generic, the system provides an additional medium for users to interpret results, which can be useful in fields where training in formal modeling is less common.Second, this design choice is natural given recent advances in LLM-based agents for scientific discovery and modeling [12,17].</p>
<p>We prompt an LLM to produce natural language criticism h k that summarizes the discrepancy implied by test statistic T k and its p-value pk .Specifically, we ask the LLM to synthesize the test statistic in a way that's informative to a colleague revising some initial model.For examples of the natural language critiques produced, see Section A.2 and for the prompt see Figure 9.</p>
<p>We can easily integrate these three artifacts within an LLM-based scientific discovery system.Specifically, we provide the system with (1) a Python implementation of the test statistic T k , (2) the natural language h k , and (3) the initial model; in our experiments these models will be probabilistic programs in pymc or stan [1,5].We use the LLM-based system for generating probabilistic programs introduced by Li et al. [17].</p>
<p>In general, these hypothesis tests are cheap relative to the cost of model fitting.For example, posterior inference is the dominating cost for Bayesian models and performing posterior predictive checks is cheap given posterior samples.Thus, CriticAL will generally introduce minimal overhead to the overall cost of an AI scientist system.</p>
<p>Experiments</p>
<p>In this section, we present experimental results that evaluate key quantitative and qualitative properties of our system.We begin by illustrating the pitfalls of a naive LLM in a synthetic regression setting.We then systematically study CriticAL's ability to avoid hallucinations and discover true discrepancies by analyzing its true and false positive rates in a setting where we synthesize discrepancies between models and datasets.We then evaluate the transparency and interpretability of our system in human user and LLM evaluations, as well as the actionability of the natural language criticism in helping an LLM-based system to revise models.</p>
<p>Features Used</p>
<p>Naive criticism leads to hallucinatory revisions</p>
<p>Correct Features Spurious Features</p>
<p>Figure 3: CriticAL attempts fewer, more targeted revisions.The critiques produced by the naive approach drive greedy model revisions that indiscriminately add both spurious (red) and correct (green) features; we indicate features used in revised models as dark-colored squares.In contrast, CriticAL leads to fewer revisions because it filters discrepancies by significance.Furthermore, those revisions generally target the correct missing feature (floor).</p>
<p>Experiment 1: Naive LLM-based critic hallucinates in synthetic regression task</p>
<p>In an initial case study, we show that a naive LLM critic consistently hallucinates but CriticAL does not.Specifically, we compare the model revision changes induced by the critiques produced by CriticAL and the naive approach, in a setting where we adversarially introduce spurious "distractor" features into a dataset.For an overview, see Figure 2.</p>
<p>Generating a regression dataset with spurious features</p>
<p>We generate a synthetic dataset inspired by the radon dataset, a commonly used dataset in regression analysis.We generate the target, radon as a a linear function of floor (basement or first floor) and uppm (e.g., uranium), corrupted with additive Gaussian noise.In addition to these two features, we add two additional spurious, distractor features to the dataframe, county and soil, with semantically plausible names.</p>
<p>Naive LLM critic baseline</p>
<p>We implement a naive approach to model criticism that receives (1) an initial statistical model represented as a pymc [1] program (2) a dataframe of the posterior predictive mean radon predictions along with the corresponding variances of those predictions and a (3) dataframe of the dataset.Given this information, we ask the LLM critic to identify discrepancies between the predictions and data.</p>
<p>Evaluating critiques in driving model revision We generate twenty critiques from both CriticAL and the naive baseline; the initial model regresses radon against only uppm, omitting floor which is used in the ground truth.CriticAL filters the critiques to five significant ones (p &lt; 0.01); four correctly identify that radon varies by floor, and the other correctly notes that model fails to capture the range of radon values but does not identify that the missing floor feature is the culprit.In contrast, the naive approach recommends generic model expansions; for an example see the text in the bottom of Figure 2. We evaluate the critiques by feeding them into an LLM-based model revision system [17] as described in Section 3.2.Figure 3 shows the features added per revision attempt, with spurious features indicated in red and correct features in green.Rows correspond to features and columns correspond to model revision attempts; we indicate that a feature was added using dark-colored squares.The naive approach often adds all possible features indiscriminately.</p>
<p>In contrast, the majority of CriticAL's critiques lead to targeted revisions.The main exception is the critique about the discrepancy in the range of values; this critique captures a true deficiency in the initial model, but isn't actionable (i.e., suggest a concrete strategy for revising) which leads the revision LLM to be greedy; we will evaluate this notion of actionability in additional experiments.(right) FPR against significance threshold.CriticAL correctly identifies more discrepancies than the pre-specified method, at the same FPR level.The FPR is calibrated with the significance threshold, showing that CriticAL systematically avoids hallucinations.</p>
<p>Experiment 2: Statistical analysis of hallucinations and true discoveries</p>
<p>A reliable critic system should avoid hallucinations (i.e., generating false positives) and discover true discrepancies when they exist.In this section, we study this through a statistical lens and characterize CriticAL's false and true positive rates.We ask: does CriticAL reliably discover discrepancies when there are actual discrepancies?And, conversely, does CriticAL hallucinate when there are no discrepancies?To study this, we synthetically generate discrepancies between models and datasets which enables us to empirically characterize the true and false positive rates.</p>
<p>Generating no-discovery and discovery datasets</p>
<p>We synthetically generate model-dataset pairs where each pair is either a no-discovery pair or discovery pair.</p>
<p>To construct a no-discovery pair, we first sample a dataset Y from a ground truth data distribution Y ∼ p(Y|H).We then draw m posterior predictive samples from the ground truth data distribution conditioned on the data: i.e., {Y
ppred i } m i=1 ∼ p(Y ppred |Y, H).
No-discovery pairs serve as a negative control to ensure that CriticAL does not systematically hallucinate and produce false discoveries.To generate discovery pairs, we sample a dataset Y from a dataset distribution p.However, we pair Y with samples from a lesioned model q, where we choose q so that it fails to capture an important aspect of the data generating distribution q.For example, we can take p to be a Student's t distribution and q to be a Gaussian distribution; even after conditioning on data q(Y |Y) will fail to capture the tails.These discovery pairs serve as a positive control and allow us to understand how reliably CriticAL identifies discoveries (i.e., true positive rate).We generate six model-dataset pairs.The data-generating models are: Student's t, negative binomial, and a generalized linear model.The lesioned models are: Gaussian, Poisson, and logistic growth.</p>
<p>To account for randomness in data generation, we generate twenty copies of each model-data pair corresponding to twenty random fresh datasets.</p>
<p>Calculating true positive and false positive rate</p>
<p>For each model-dataset pair, we run CriticAL with 24 proposals and at a temperature 0.7.Our system decides if there is a discrepancy by checking whether the minimum p-value is less than the significance threshold; that is, whether min k pk ≤ α.By construction, we have the "correct" decision for each pair.To compute the true positive rate, we compute the proportion of discovery pairs in which CriticAL correctly decided there was a discrepancy.To compute the false positive rate, we compute the proportion of no-discovery pairs in which CriticAL incorrectly decided there was a discrepancy.</p>
<p>Quantitative Results</p>
<p>In Figure 4, we show the true and false positive rates of CriticAL.As a baseline, we compare CriticAL against a standard set of pre-specified test statistics: mean and variance.From the ROC curve, we see that CriticAL exhibits a favorable trade-off between the true positive rate (power) and false positive rate (type I error), and significantly outperforms the baseline method, achieving a higher true positive rate at all false positive rate levels.As the false positive rate (FPR) calibration plot shows, the false positive rate closely tracks the significance threshold α, showing that CriticAL does not systematically identify spurious discrepancies.These analyses illustrate that CriticAL has favorable statistical properties in a controlled setting.In the appendix (Section A.5), we show examples of CriticAL's proposed test statistics that account for its favorable statistical properties.These statistics are tailored to the statistical model.For example, CriticAL proposes kurtosis for the Student's t setting, to assess the tails of the distribution.</p>
<p>Experiment 3: Analyzing key qualitative properties of test statistics for real-world model-dataset pairs</p>
<p>In the previous sections, we evaluated CriticAL's statistical properties.However, users interacting with an LLM-based critic system may care just as much about key qualitative properties such as transparency (i.e., how clear is the reasoning used to generate the critique) and actionability (i.e., how useful is the criticism to a scientist revising the model).</p>
<p>To study this, we first apply CriticAL to real world datasets and expert written models covering a range of scientific domains (see Section A.3).We quantitatively assess CriticAL-generated critiques in both human and automated LLM-based evaluation.We then qualitatively characterize CriticAL's critiques, which illustrate the conceptual advantages of using CriticAL's tailored test statistics.</p>
<p>Experimental Setup</p>
<p>The Stan PosteriorDB database [19] consists of real-world datasets and probabilistic models implemented in Stan; these models are open-source contributions from the Stan developer community that cover a broad range of modeling motifs ranging from hierarchical modeling to regression.We chose 36 model-dataset pairs based on ones used in several recent papers [17,21,28].For each StanDB model-dataset pair, the LLM proposes twenty-four test statistics {T k } 24 k=1 ; we run this proposal step at a temperature 1.0.Then, for each T k , we generate natural language criticism h k by running the natural language criticism step at a temperature 0.0.</p>
<p>Transparent</p>
<p>Systematic evaluation of qualitative properties of critiques</p>
<p>We conducted a human evaluation study with three Ph.D. students (non-authors) with expertise in statistics, who were blind to the critic methods.We randomly selected ten model-dataset pairs.For each pair, both CriticAL and a naive LLM critic generated critiques.The evaluators chose which critique was better along three criteria that we describe and motivate below.</p>
<ol>
<li>
<p>Transparency: can a user of the system understand how the critique was produced?Transparency is important for building trust with users and can also help them evaluate if the system is hallucinating.</p>
</li>
<li>
<p>Actionable: can the critique help a scientist revise the model?A critique is more useful if it provides insights into how to revise the model.For example, knowing that a model has high error is less useful than knowing that a model has high error on a specific sub-population.</p>
</li>
<li>
<p>Tailored: is the critique targeted for the specific model and dataset?We do not expect generic critiques to provide much insight.</p>
</li>
</ol>
<p>To scale this analysis, we employed state-of-the-art LLM-based judges (gpt-4o-2024-08-06 and claude-3-5-sonnet-20240620) following highly specific guidelines; for details, see the  3) to aid an LLM-based agent in revising an initial model.We show that CriticAL's critiques lead to significant improvements over the initial model.</p>
<p>Experimental Setup</p>
<p>We integrate CriticAL into the model discovery system introduced by Li et al. [17] by giving a revision-LLM three components: the initial model H implemented as a probabilistic program, the test statistic T k , and natural language criticism h k ; the criticism was produced in the previous section by running CriticAL on the (fitted) initial model.We fit the models proposed by the revision LLM using pymc [1]; we repeat each proposal three times at a temperature 0.0 since we noticed non-determinism.We allow the LLM to revise based on a filtered set of significant test statistics.For details, see Section A.7.We report the best model across proposals and test statistics.</p>
<p>Ablation We consider a data-blind LLM critic that receives only the statistical model, implemented as a probabilistic program; we run this at a temperature 0.0.This approach can be effective since modeling assumptions are enumerated in the initial probabilistic program.</p>
<p>Quantitative Results</p>
<p>In Tables 1 and 2, we evaluate CriticAL's ability to produce critiques that improve upon an initial statistical model and outperform the data-blind method.To do this, we first compute the expected log predictive density (ELPD LOO) score for both initial and revised models [26].Next, we calculate the score difference between the initial and revised model and determine the margin of victory by dividing the score difference by the standard errors (SE) of the score difference.For a given dataset, a method is considered to "win" over another at a specific SE margin if the score difference is larger than the margin.For example, if the score difference is 2 and the SE margin is 1, we count it as a significant win.Finally, we compute aggregated win rates at various SE margins (1, 1.5, 2).The win rate is the percentage of datasets where one method outperformed another at a given SE margin.CriticAL's critiques help a revision LLM significantly improve upon the initial model over 80% of the time, which shows that CriticAL reliably produces actionable critiques.CriticAL's successes are often related to sliced statistics discussed in Section 4.3 (e.g., the revision-LLM introduces floor-dependent variance terms).Furthermore, in Table 2, we show that CriticAL also outperforms the data-blind critic.Next, we discuss CriticAL's limitations.</p>
<p>Limitation 1: Suboptimal transformations of data CriticAL does not see the model predictions or data.As a consequence, CriticAL sometimes does not provide good critiques on transforming data.This happens most prominently in the mesquite setting where the model predictions can be negative even though the data is non-negative.</p>
<p>Limitation 2: Correct criticism but imperfect implementation In some cases, CriticAL identifies a legitimate discrepancy that the revision LLM incorrectly implements (e.g., the revision LLM correctly uses a Categorical likelihood but does not transpose the logits correctly).</p>
<p>Conclusion</p>
<p>We introduced CriticAL, a framework for automated model criticism that leverages LLMs to identify discrepancies between a model and dataset and then applies hypothesis tests to assess the significance of discrepancies.CriticAL serves as a lightweight verifier, validating both scientific models and critiques within a hypothesis testing framework.Our experiments demonstrate that CriticAL reliably identifies true discrepancies without hallucinating false critiques.Furthermore, both human and LLM judges preferred CriticAL's critiques over alternative approaches.CriticAL critiques enabled an LLM-based system to substantially improve upon expert designed models.By automating model criticism, CriticAL represents a step toward more reliable automatic scientific discovery systems.</p>
<p>While our evaluation was limited to Bayesian models, which are commonly used in scientific domains, CriticAL's design is versatile: the only requirements are the ability to sample data from the model and a symbolic representation of the model.An exploration of other common classes of scientific models [6] is an exciting direction for future work.</p>
<p>A Appendix</p>
<p>A.1 Prompts and inputs for test statistic proposer step</p>
<p>Critic function prompt</p>
<p>You are a brilliant statistician specializing in critiquing models!Your equally brilliant colleague has come up with a probabilistic program in Stan that proposes a generative/statistical model for the data.Your job is to critique the models and provide hypotheses for discrepancies between the model and the data.To do this, you should write a "test statistic function" in Python.This is motivated by posterior predictive checks in Bayesian statistics.This test function should take as input a dataframe where one of the columns contains the posterior predictive sample.It should return a scalar-valued test statistic.</p>
<p>To quantify discrepancies, I will compute this test statistic for each posterior predictive sample and compare it to observed data.Therefore, choose test statistics that you think will reveal discrepancies between the model and the data.</p>
<p>Natural language criticism prompt</p>
<p>Your equally brilliant colleague has come up with a discrepancy functions that identify possible weaknesses of generative models for data.I will give you the test statistics and the result of computing those test statistics.Your job is to interpret the results of running those test statistics and synthesize the discrepancies.Your synthesis should be as helpful as possible for your colleague who will use this synthesis to improve the model.You will be given one million dollars if you do this well.Focus on being as informative with your synthesis (do not say generic things) to help your colleague understand the test statistic.You should provide a natural language summary of the discrepancy function.Reference specifically the test statistic type and the discrepancy it reveals about specific modeling assumptions.I provide the test statistic Python function and posterior-predictive pval.Posterior predictive p-val: Test statistic function:</p>
<p>Figure 1 :
1
Figure 1: Criticizing scientific models with CriticAL.First, an LLM generates summary statistics that capture potential discrepancies that are tailored to the model and dataset; the LLM conditions on dataset metadata and a symbolic representation of a scientific model.We use these summary statistics to perform hypothesis tests to evaluate the significance of each discrepancy.</p>
<p>Figure 2 :
2
Figure 2: Illustrating how CriticAL avoids hallucinated revisions.CriticAL hypothesizes discrepancies via summary statistics and makes targeted changes to the initial model, which is missing the feature floor.In contrast, the naive method hallucinates (see LLM explanation in figure for details) and introduces spurious features (e.g., county, soil) to the initial model.In the revised model programs, we highlight spurious features in red and correct features in green.</p>
<p>Figure 4 :
4
Figure 4: Statistical analysis of CriticAL's ability to discover discrepancies and avoid hallucinations.(left) True positive rate (TPR) vs. false positive rate (FPR) at different significance thresholds.(right)FPR against significance threshold.CriticAL correctly identifies more discrepancies than the pre-specified method, at the same FPR level.The FPR is calibrated with the significance threshold, showing that CriticAL systematically avoids hallucinations.</p>
<p>Figure 5 :
5
Figure 5: CriticAL criticisms have higher win rates versus naively generated criticisms.Critiques are rated on three qualitative criteria by LLM-based judges (GPT-4o and Claude 3.5 Sonnet).LLM-based judges are aligned with human evaluators: GPT-4o and Claude 3.5 Sonnet have 100% alignment for transparent and tailored preferences, and are 80% and 90% aligned for actionable preferences, respectively.Error bars represent 95% confidence intervals (Wilson score).</p>
<p>( 4 .
4</p>
<p>Figure 7 : 1 . 3 -Figure 8 :
7138
Figure 7: System prompt for proposing test statistics in Section 3.1.We also provide additional instructions on formatting the response and describing the format of the input dataframe.</p>
<p>Figure 9 :
9
Figure 9: Prompt for natural language criticism step in Section 3.2.</p>
<p>Figure 11 :
11
Figure11: LLM judge prompt for determining which model criticism is more transparent.The same prompt structure, with corresponding judging criteria descriptions, is used for actionable and tailored judge prompts.The order of criticisms (CriticAL being Criticism A vs. Criticism B) is randomized to avoid position bias, and impartiality instructions are adapted from the RewardBench[15] judge prompts.</p>
<p>Algorithm 1: Producing test statistics and empirical p-values Input: dataset</p>
<p>D, metadata C, model H, model samples {Y
pred i i=1 , {T k } n } m k=1 ) via Equation 2 } m i=1 , number proposals n k=1 = multiple-test-adjustment({p k } m {T k } n k=1 ∼ p LM (•|C, H) {p k } n k=1 ← get-empirical-pval(D, {Y pred i {p k } n k=1 )Output: test statistics {T k } m k=1 , adjusted empirical p-values {p k } m k=1Proposing discrepancies via test statistics As we saw in Section 2, we can often formalize findingdiscrepancies between model predictions and data as identifying suitable test statistics. Designing</p>
<p>Evaluate discrepancies via hypothesis tests Generate summary statistics
DataBlood GlucosePredictions SleepTimeDataBlood GlucosePredictions ExerciseTime</p>
<p>Table 1 :
1
CriticAL consistently improves over the initial model.CriticAL achieves significantly higher win rates compared to the initial model at various standard error (SE) thresholds.We say a win is significant at a given SE threshold if the difference in scores is larger than the SE margin.
MethodWins &gt; 1 SE Wins &gt; 1.5 SE Wins &gt; 2.0 SECriticAL0.940.940.82Initial Model0.060.060.06MethodWins &gt; 1 SE Wins &gt; 1.5 SE Wins &gt; 2.0 SECriticAL0.590.590.53Data-blind0.290.290.29</p>
<p>Table 2 :
2
CriticAL outperforms data-blind method.CriticAL demonstrates higher win rates across all standard error (SE) margins compared to data-blind method that conditions only on the symbolic representation of the model.</p>
<p>AcknowledgementsThis work was supported in part by AFOSR Grant FA9550-21-1-0397, ONR Grant N00014-22-1-2110, and an NSF Expeditions Grant, Award Number (FAIN) 1918771.EBF is a Chan Zuckerberg Biohub -San Francisco Investigator.We thank Omar Shaikh and Justin Shen for valuable feedback on this paper.We also thank Peter Yoon, Daniel Same, and Artem Khan for discussions.We indicate the test statistic type on the y-axis and slice category on the x-axis; the "agg" slices indicates aggregation across all slices.Blue violins correspond to sliced test statistics and red violins correspond to aggregated ones.The dashed line passes through red violin centers but not the blue ones, showing that CriticAL's choice to slice test statistics reveals discrepancies that pre-specified ones cannot.Appendix A.9.In Figure5, we show the win-rates (higher is better) across two LLM judges and the three criteria.CriticAL is classified as significantly more transparent (∼ 97%), actionable (∼ 76%), and tailored (∼ 98%).Both LLM-based judges are aligned with human preferences, having 100% alignment for transparent and tailored preferences, and GPT-4o and Claude 3.5 Sonnet having 80% and 90% alignment for actionable preferences, respectively.The domain experts gave qualitative feedback in support of CriticAL's approach, particularly in terms of transparency.Experts noted the benefit of immediately executable code for quick assessment (see Appendix A.6 for quotes).Qualitative examples: sliced test statistics One specific kind of test statistic that meets the above three criteria are sliced test statistics.CriticAL often proposes test statistics that slice the model prediction Y pred i based on the input features X .For example, 1 # Filter to get basement and non-basement samples In Figure6, we illustrate the benefits of CriticAL's sliced test statistics over pre-specified, aggregate test statistics.We compare test statistic distributions computed from model samples against the data, sliced by the input values (e.g., variance of model predictions for radon measurements in basement vs first floor).Sliced test statistics reveal discrepancies that the aggregated ones cannot.We provide additional randomly-sampled test statistics in the Appendix (Section A.8).Experiment 4: CriticAL generated criticism drives model improvementsModel criticism should ideally be actionable and aid a user (either LLM or human) in model revision.In our final experiment, we use the model criticism generated by CriticAL in the previous sectionA.2 Examples of natural language critiquesExample natural language critiques produced by CriticAL 1 """ 2 The posterior predictive p-value of 0.0 suggests a significant discrepancy between the observed data and the model predictions regarding the distribution of children's cognitive test scores.The test statistic used here measures the skewness of the predicted scores, indicating that the model's assumption of normally distributed scores may not be appropriate.The very low posterior predictive p-value of 0.0019 suggests that the model does not adequately capture the variability of radon levels across different floor measurements.This discrepancy indicates that the assumption of a homogeneous linear interaction between floor level and radon level across all counties may be too simplistic.5 """ 6 7 """ 8 The model fails to capture the difference in radon levels between measurements taken in the basement versus the first floor.This is indicated by a posterior predictive p-value of 0.0, suggesting that the model does not adequately represent the known higher radon levels typically found in basements compared to the first floor.9 """ 10 11 """ 12 The model's use of a normal distribution to predict the inherently discrete variable 'partyid7' (ranging from 1 to 7) results in a significant proportion of predicted values falling outside this permissible range.13 """A.3 Stan PosteriorDB datasetsWe list the model-dataset pairs criticized in Section 4.3.• radon_mn-radon_variable_slope_noncenteredA.6 Example Qualitative Feedback from Domain ExpertsWe asked Ph.D. students to provide feedback on CriticAL's critiques.Here are two representative quotes: "I liked seeing code I could immediately run and check, it allowed me to take fast action to assess the situation.""I liked when I had code that immediately applied to the model."A.7 Experiment 4 Additional DetailsWe ran the revision process for a single round, at a temperature of 0.0, using 3 proposals in total; we run multiple proposals because temperature 0.0 was not deterministic.We choose the top five test statistics T k , ranked by p-value, where the Bonferonni-adjusted p-value &lt; 0.15.We choose top five to limit the number of models fit since the fitting procedure is computationally-intensive. We choose the cutoff value by examining the spread of the Bonferonni-adjusted p-values; there were 18 significant discrepancies.While this threshold is larger than a typical p-value, discrepancies that do not meet the traditional significance levels may nevertheless be valuable in the context of a closed-loop model discovery process.A less stringent threshold allows us to evaluate lower-significance discrepancies that still may improve model performance.A.8 Further test statistics for Experiment 3Example test statistics produced by by CriticAL 1 def test_statistic(df):
PyMC: a modern, and comprehensive probabilistic programming framework in python. Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov, Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, Michael Osthege, Ricardo Vieira, Thomas Wiecki, Robert Zinkov, PeerJ Computer Science. 92023</p>
<p>P values for composite null models. M J Bayarri, James O Berger, Journal of the American Statistical Association. 01621459954522000</p>
<p>The analysis of repeated-measures data on schizophrenic reaction times using mixture models. Thomas R Belin, Donald B Rubin, Statistics in medicine. 141995</p>
<p>Sampling and Bayes' Inference in Scientific Modelling and Robustness. E P George, Box, Journal of the Royal Statistical Society. Series A (General). 0035923814341980</p>
<p>Stan: A probabilistic programming language. Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell, Journal of statistical software. 7612017</p>
<p>The frontier of simulation-based inference. Kyle Cranmer, Johann Brehmer, Gilles Louppe, Proceedings of the National Academy of Sciences. 1172019</p>
<p>Philosophy and the practice of Bayesian statistics in the Social Sciences. Andrew Gelman, Cosma Rohilla, Shalizi , 10.1093/oxfordhb/9780195392753.013.0011August 2012</p>
<p>Posterior predictive assessment of model fitness via realized discrepancies. Andrew Gelman, Xiao-Li Meng, Hal Stern, Statistica Sinica. 10170405641996. 19968507</p>
<p>Multiple Imputation for Model Checking: Completed-Data Plots with Missing and Latent Data. Andrew Gelman, Iven Mechelen, Geert Verbeke, Daniel Heitjan, Michel Meulders, Biometrics. 6104 2005</p>
<p>Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, Donald B Rubin, Bayesian data analysis. 2013third edition</p>
<p>The principles and practice of probabilistic programming. D Noah, Goodman, 10.1145/2429069.2429117Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL '13. the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL '13New York, NY, USAAssociation for Computing Machinery2013</p>
<p>MLAgentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>Engineering-driven statistical adjustment and calibration. V , Roshan Joseph, Huan Yan, 10.1080/00401706.2014.902773Technometrics. 5722015</p>
<p>Bayesian calibration of computer models. Marc C Kennedy, Anthony O' Hagan, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 632001</p>
<p>Rewardbench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, 2024</p>
<p>Scientific discovery: computational explorations of the creative processes. Pat Langley, Herbert A Simon, Gary L Bradshaw, Jan M Zytkow, January 1987</p>
<p>Automated Statistical Model Discovery with Language Models. Emily B Michael Y Li, Noah D Fox, Goodman, International Conference on Machine Learning (ICML). 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2024</p>
<p>posteriordb: a set of posteriors for Bayesian inference and probabilistic programming. Måns Magnusson, Paul Bürkner, Aki Vehtari, October 2023</p>
<p>Posterior Predictive p-Values. Xiao-Li Meng, 10.1214/aos/1176325622The Annals of Statistics. 2231994</p>
<p>Variational Inference with Gaussian Score Matching. Chirag Modi, Robert M Gower, Charles Margossian, Yuling Yao, David Blei, Lawrence K Saul, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician. Donald B Rubin, The Annals of Statistics. 1241984</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>An Introduction to Probabilistic Programming. Jan-Willem Van De Meent, Brooks Paige, Hongseok Yang, Frank Wood, 2021</p>
<p>David A Van Dyk, Hosung Kang, Highly Structured Models for Spectral Analysis in High-Energy Astrophysics. 200419</p>
<p>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Aki Vehtari, Andrew Gelman, Jonah Gabry, 10.1007/s11222-016-9696-4Statistics and Computing. 0960-3174275sep 2017</p>
<p>Automating science. David Waltz, Bruce G Buchanan, 10.1126/science.1172781Science. 32459232009</p>
<p>Foundation Posteriors for Approximate Probabilistic Inference. Mike Wu, Noah Goodman, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Ziwei Xu, Sanjay Jain, Mohan S Kankanhalli, ArXiv, abs/2401.118172024267069207</p>
<p>Prompt used for Qualitative Criteria LLM Judges LLM Judge Prompt for Qualitative Criteria 1 You are an expert statistical modeling and data science. Your task is to determine which model criticism is more transparent. </p>
<p>Context: 4 Dataset Description. </p>
<p>. Column Description. </p>
<p>10 Model being criticized: 11. </p>
<p>Criticism B: 21 <Criticism B> 22 {criticism_b} 23 </Criticism B> 24 25 Please evaluate the transparency of the criticisms based on the following criteria, assuming the intended evaluator is a data scientist or statistician: 26 -How clear is the methodology used to generate the criticism?. 27 -How explicitly are the relevant parts of the dataset identified in the criticism? 28 -How unambiguous is the process of determining the criticism's conclusions? 29</p>
<p>provide a detailed analysis of how each criticism meets the criteria and compare Criticism A and Criticism B. Second, state "A" or "B" to indicate which criticism is more transparent. First, </p>
<p>32 Important: 33 -Avoid any position biases and ensure that the order in which the criticisms were presented does not influence your decision. 34 -Do not allow the length of the responses to influence your evaluation. 35 -Do not favor certain names of the criticisms. 36 -Be as objective as possible</p>
<p>38 Provide your response in the following format: 39 Comparison: <Detailed reasoning and comparison to determine prefered criticism> 40 Final Response: &lt;"A" or. </p>            </div>
        </div>

    </div>
</body>
</html>
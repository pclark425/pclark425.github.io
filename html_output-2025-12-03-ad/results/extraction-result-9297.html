<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-273404079</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2024.emnlp-main.679.pdf" target="_blank">Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks. However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities. This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs’ performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies. Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, which significantly impairs their performance. 2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while few-shot prompts generally underperform compared to zero-shot prompts. 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. 5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy. Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_vs_NonCoT (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting versus non-Chain-of-Thought prompting (aggregate finding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across multiple LLMs and NLI shortcut datasets, zero-shot Chain-of-Thought (CoT) prompting generally reduces reliance on dataset shortcuts and improves accuracy on shortcut-laden sets, though there are notable exceptions on simple datasets where CoT can decrease accuracy for very capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-3.5-Turbo, GPT-4, Gemini-Pro, LLaMA2/LLaMA3/Mistral/ChatGLM3 series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (MultiNLI / HANS - Shortcut Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI classification (entailment / neutral / contradiction) evaluated on Standard MultiNLI and six shortcut datasets (Lexical Overlap, Subsequence, Constituent, Negation, Position, Style).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot Chain-of-Thought prompting (zero-shot CoT); CoT prompts include a request for step-by-step reasoning before final label (prompt template shown in paper's Fig.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot (no CoT), few-shot ICL, few-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model and dataset; improvements observed across many shortcut datasets (accuracy increases reported but model- and dataset-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Example reported comparisons: GPT-4: +14.0% accuracy on Constituent (¬E) with zero-shot CoT vs zero-shot; LLaMA2-Chat-13B: +40.9% on same comparison. (Paper reports CoT yields consistent gains on many shortcut datasets.)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+14% to +40.9% (examples reported; effect size varies by model and dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper hypothesizes CoT reduces shortcut reliance by encouraging deeper, multi-step inference and discouraging spuriously mapped surface heuristics; however, for simple datasets where the model already has adequate reasoning, CoT can be redundant or even slightly harmful (observed decreased accuracy for GPT-4 and Gemini-Pro on Standard and Lexical Overlap datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations used zero-shot CoT prompts (Figure 1 template). Datasets: Standard MultiNLI subset (3k samples), HANS-derived sets (3k each), Negation/Position/Style variants built from MultiNLI. Models: closed-source (GPT-3.5-Turbo, GPT-4, Gemini-Pro) and open-source (LLaMA2-Chat 7B/13B/70B, ChatGLM3-6B, Mistral-7B, LLaMA3 series).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot_vs_Zero-shot (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot In-Context Learning (ICL) and Few-shot CoT compared to Zero-shot variants (aggregate finding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot ICL and few-shot CoT frequently underperform their zero-shot counterparts on shortcut-laden datasets; in-context demonstrations can introduce biases that worsen reliance on shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (Shortcut Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI classification evaluated under few-shot ICL and few-shot CoT prompts (few-shot uses 3 in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>few-shot ICL (3 random MultiNLI examples) and few-shot CoT (3 CoT examples generated by GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot and zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generally lower than zero-shot/zero-shot CoT on many shortcut datasets (specific accuracies model- and dataset-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports multiple cases where few-shot < zero-shot; few-shot CoT sometimes worse than zero-shot CoT. Example-level improvement occurs if the few-shot examples are from similar shortcut-laden distributions but still often inferior to zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Varies; paper reports substantial negative deltas in some cases (e.g., aggregate drops documented in Table 2 and Appendix D), but effect sizes vary by model/dataset and by choice of in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute degradation to biases introduced by demonstrations (in-context examples), which can reinforce spurious correlations that the model learned during pretraining; even using shortcut-similar demonstrations improves over unrelated few-shot examples but typically remains worse than zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompts used 3 examples sampled from MultiNLI for main experiments; Appendix D also experiments with few-shot examples sampled from shortcut-laden sets. GPT-4 was used to generate CoT analyses for few-shot CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo_fewshot_Constituent_drop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy drop of GPT-3.5-Turbo on Constituent (¬E) in few-shot ICL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5-Turbo shows a large accuracy drop on the Constituent non-entailment (¬E) subset under few-shot in-context learning, indicating strong shortcut exploitation in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Constituent (¬E) subset (HANS-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Non-entailment examples specifically targeting constituent-based structural heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>few-shot in-context learning (few-shot ICL with 3 MultiNLI examples)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>standard (zero-shot / Standard dataset baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported drop: accuracy on Constituent (¬E) 'drops by 52.4%' in the few-shot ICL setting (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-52.4% (drop reported vs baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot demonstrations appear to introduce or reinforce spurious heuristics that the model exploits, leading to a large performance degradation on structural OOD examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Constituent set drawn from HANS (3k examples). Few-shot ICL used 3 examples sampled from MultiNLI; prompt construction follows the paper's few-shot ICL protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_CoT_improvement_Constituent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Chain-of-Thought improvement on Constituent (¬E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 achieves a measurable accuracy improvement on the Constituent non-entailment subset when using zero-shot CoT prompting compared to zero-shot prompting, suggesting CoT reduces shortcut reliance for this model and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Constituent (¬E) subset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HANS-derived non-entailment (¬E) examples designed to detect constituent heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot Chain-of-Thought prompting (zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot without CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>+14.0% accuracy with zero-shot CoT vs zero-shot (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+14.0% (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT encourages stepwise inference that mitigates surface-level heuristics like constituent shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot CoT prompt template used (Figure 1). Constituent dataset from HANS, 3k examples; evaluation reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9297.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-Chat-13B_CoT_gain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat-13B Chain-of-Thought gain on Constituent (¬E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-Chat-13B shows a very large improvement with zero-shot CoT on the Constituent non-entailment subset, indicating CoT can unlock better reasoning and reduce shortcut use in mid-sized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-Chat-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Constituent (¬E) subset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HANS-derived test of constituent heuristic vulnerability.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot (no CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>+40.9% accuracy with zero-shot CoT vs zero-shot (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+40.9% (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger / mid-sized models can leverage CoT to perform deeper parsing-style reasoning, reducing surface heuristic reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot CoT used; dataset HANS Constituent (3k examples). Improvement quoted in main results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9297.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2_ReverseScaling_ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reverse scaling of LLaMA2-Chat family under ICL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Within the LLaMA2-Chat series (7B→13B→70B), increasing model size sometimes correlates with greater reliance on spurious shortcuts under zero-shot and few-shot ICL prompts (reverse scaling), but the largest model benefits more from CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-Chat series</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Shortcut Suite</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comparison across model sizes on standard and shortcut NLI datasets under different prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot and few-shot ICL; zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>other prompt types (CoT vs non-CoT) and across sizes</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported trends: larger LLaMA2-Chat models show more shortcut reliance under ICL; in CoT, LLaMA2-Chat-70B outperforms smaller variants on most datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Trend-level: reverse scaling (performance decreases with size) under ICL; CoT reverses this trend for the largest model (70B) improving relative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest larger models have greater capacity to memorize/learn spurious pretraining correlations that manifest as shortcuts under ICL; CoT forces explicit reasoning that lets large models exploit their improved reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Observed across LLaMA2-Chat 7B/13B/70B in Table 2; CoT improves 70B more than smaller sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9297.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position_Presentation_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positional presentation of redundant tautologies (Position shortcut)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Placing repetitive tautological phrases at different positions (beginning vs end) in the premise/hypothesis affects LLM predictions: accuracy is lowest when tautologies are added at the beginning, indicating positional cues influence model decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Position shortcut variant</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard MultiNLI samples modified by appending repeated tautologies (e.g., 'and true is true') five times at different positions (start/end of premise or hypothesis) to test sensitivity to positional spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot prompting with tautologies inserted at specific positions (beginning vs end)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>same samples with tautologies placed at different positions</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Lowest accuracy predominantly when tautologies are placed at the beginning of the source text (exact accuracies per model in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports systematic decrease in accuracy for beginning-placement vs end-placement; specific model-by-model numbers in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs tend to overweight information appearing early in a sentence, leading to shortcut behavior when irrelevant repetitive content is placed at the start.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Position dataset: Standard MultiNLI divided into groups, tautologies appended 5 times at different positions; accuracy details reported in Table 3 (lowest often when tautologies at start).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9297.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot_example_selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of demonstration selection for few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using few-shot demonstrations sampled from shortcut-laden datasets can improve few-shot performance relative to demonstrations sampled from standard examples, but performance typically remains worse than zero-shot baselines, indicating pretraining-induced shortcuts dominate in-context gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Shortcut Suite (few-shot experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot ICL experiments where demonstration examples are sampled either from MultiNLI (standard) or from the shortcut-laden datasets themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>few-shot ICL with different example selection strategies (standard vs shortcut-similar examples)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot and few-shot with alternative demonstration sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot using shortcut-similar examples performs better than using standard examples but still worse than zero-shot in many cases (detailed in Appendix D / Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper states: 'LLMs' performance on shortcut-laden datasets using more similar examples is better than using standard examples, but still worse than zero-shot.'</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In-context examples can provide useful distributional signal, but they cannot fully overcome pretraining-acquired spurious correlations; demonstrations can also inject additional biases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 3 examples; alternative samplings and results summarized in Appendix D and Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9297.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Style_and_Negation_presentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Style-transfer and Negation insertion as prompt/data presentation variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforming premises into a different text style (Bible style via STRAP) or appending tautological negation phrases to hypotheses changes model performance: style and negation shortcuts reduce accuracy for many LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI (Style and Negation shortcut datasets) and extended tasks (Sentiment Analysis, Paraphrase Identification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Style: premises converted to Bible English; Negation: append tautologies like 'and green is not red' to hypotheses; Extended: SA (SST-2) with negation injection and PI (QQP/PAWS) with lexical overlap adversaries.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot prompting on transformed data (style transfer or appended negation tautologies)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard dataset (no style transfer / no appended negation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy decreases on Negation and Style datasets across many models (e.g., GPT-4 accuracy decreases 15–35% on Negation across prompt settings; Style causes up to 15.6% decrease for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Quantified examples: GPT-4: 15–35% drop on Negation; up to 15.6% drop on Style (reported in text). Extended tasks also show consistent declines (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-15% to -35% (examples reported for GPT-4 on Negation; -15.6% on Style)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Models appear to latch on to negation words and style cues as heuristic predictors (e.g., mapping negation tokens to contradiction/negative labels or Bible style to particular labels) rather than integrating meaning; style transfer and injected negation create spurious mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Style: STRAP style-transfer applied to premises; Negation: append randomly chosen tautologies (six variants) to hypotheses; Extended tasks: SST-2 and QQP/PAWS used; results in Table 5 and Figure 8.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9297.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9297.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_negative_on_simple</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cases where CoT reduces accuracy on simple/standard datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Although CoT improves performance on many shortcut datasets, the paper reports that for some strong LLMs (GPT-4, Gemini-Pro) CoT decreased accuracy on the Standard MultiNLI dataset and the Lexical Overlap dataset, indicating CoT is not universally beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Gemini-Pro (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLI - Standard and Lexical Overlap datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard MultiNLI and Lexical Overlap (HANS) benchmark sets.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>zero-shot without CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper states accuracy of GPT-4 and Gemini-Pro decreases after applying CoT on Standard and Lexical Overlap datasets (exact deltas reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>negative effect observed (model- and dataset-specific; see Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>For relatively simple or already-solved datasets, additional CoT reasoning may be unnecessary and can introduce noise or degrade succinct signal the model would otherwise use; advanced models may not need CoT to perform well on simple cases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Observed in main results (Table 2): CoT sometimes reduces accuracy for top-performing models on simple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models can be lazy learners: Analyze shortcuts in in-context learning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms <em>(Rating: 1)</em></li>
                <li>Which is better? exploring prompting strategy for llm-based metrics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9297",
    "paper_id": "paper-273404079",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "CoT_vs_NonCoT (aggregate)",
            "name_full": "Chain-of-Thought prompting versus non-Chain-of-Thought prompting (aggregate finding)",
            "brief_description": "Across multiple LLMs and NLI shortcut datasets, zero-shot Chain-of-Thought (CoT) prompting generally reduces reliance on dataset shortcuts and improves accuracy on shortcut-laden sets, though there are notable exceptions on simple datasets where CoT can decrease accuracy for very capable models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-3.5-Turbo, GPT-4, Gemini-Pro, LLaMA2/LLaMA3/Mistral/ChatGLM3 series)",
            "model_size": null,
            "task_name": "Natural Language Inference (MultiNLI / HANS - Shortcut Suite)",
            "task_description": "NLI classification (entailment / neutral / contradiction) evaluated on Standard MultiNLI and six shortcut datasets (Lexical Overlap, Subsequence, Constituent, Negation, Position, Style).",
            "presentation_format": "zero-shot Chain-of-Thought prompting (zero-shot CoT); CoT prompts include a request for step-by-step reasoning before final label (prompt template shown in paper's Fig.1).",
            "comparison_format": "zero-shot (no CoT), few-shot ICL, few-shot CoT",
            "performance": "Varies by model and dataset; improvements observed across many shortcut datasets (accuracy increases reported but model- and dataset-specific).",
            "performance_comparison": "Example reported comparisons: GPT-4: +14.0% accuracy on Constituent (¬E) with zero-shot CoT vs zero-shot; LLaMA2-Chat-13B: +40.9% on same comparison. (Paper reports CoT yields consistent gains on many shortcut datasets.)",
            "format_effect_size": "+14% to +40.9% (examples reported; effect size varies by model and dataset)",
            "explanation_or_hypothesis": "The paper hypothesizes CoT reduces shortcut reliance by encouraging deeper, multi-step inference and discouraging spuriously mapped surface heuristics; however, for simple datasets where the model already has adequate reasoning, CoT can be redundant or even slightly harmful (observed decreased accuracy for GPT-4 and Gemini-Pro on Standard and Lexical Overlap datasets).",
            "null_or_negative_result": true,
            "experimental_details": "Evaluations used zero-shot CoT prompts (Figure 1 template). Datasets: Standard MultiNLI subset (3k samples), HANS-derived sets (3k each), Negation/Position/Style variants built from MultiNLI. Models: closed-source (GPT-3.5-Turbo, GPT-4, Gemini-Pro) and open-source (LLaMA2-Chat 7B/13B/70B, ChatGLM3-6B, Mistral-7B, LLaMA3 series).",
            "uuid": "e9297.0",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Few-shot_vs_Zero-shot (aggregate)",
            "name_full": "Few-shot In-Context Learning (ICL) and Few-shot CoT compared to Zero-shot variants (aggregate finding)",
            "brief_description": "Few-shot ICL and few-shot CoT frequently underperform their zero-shot counterparts on shortcut-laden datasets; in-context demonstrations can introduce biases that worsen reliance on shortcuts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (aggregate)",
            "model_size": null,
            "task_name": "Natural Language Inference (Shortcut Suite)",
            "task_description": "NLI classification evaluated under few-shot ICL and few-shot CoT prompts (few-shot uses 3 in-context examples).",
            "presentation_format": "few-shot ICL (3 random MultiNLI examples) and few-shot CoT (3 CoT examples generated by GPT-4)",
            "comparison_format": "zero-shot and zero-shot CoT",
            "performance": "Generally lower than zero-shot/zero-shot CoT on many shortcut datasets (specific accuracies model- and dataset-dependent).",
            "performance_comparison": "Paper reports multiple cases where few-shot &lt; zero-shot; few-shot CoT sometimes worse than zero-shot CoT. Example-level improvement occurs if the few-shot examples are from similar shortcut-laden distributions but still often inferior to zero-shot.",
            "format_effect_size": "Varies; paper reports substantial negative deltas in some cases (e.g., aggregate drops documented in Table 2 and Appendix D), but effect sizes vary by model/dataset and by choice of in-context examples.",
            "explanation_or_hypothesis": "Authors attribute degradation to biases introduced by demonstrations (in-context examples), which can reinforce spurious correlations that the model learned during pretraining; even using shortcut-similar demonstrations improves over unrelated few-shot examples but typically remains worse than zero-shot.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot prompts used 3 examples sampled from MultiNLI for main experiments; Appendix D also experiments with few-shot examples sampled from shortcut-laden sets. GPT-4 was used to generate CoT analyses for few-shot CoT examples.",
            "uuid": "e9297.1",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo_fewshot_Constituent_drop",
            "name_full": "Accuracy drop of GPT-3.5-Turbo on Constituent (¬E) in few-shot ICL",
            "brief_description": "GPT-3.5-Turbo shows a large accuracy drop on the Constituent non-entailment (¬E) subset under few-shot in-context learning, indicating strong shortcut exploitation in that setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": null,
            "task_name": "NLI - Constituent (¬E) subset (HANS-derived)",
            "task_description": "Non-entailment examples specifically targeting constituent-based structural heuristics.",
            "presentation_format": "few-shot in-context learning (few-shot ICL with 3 MultiNLI examples)",
            "comparison_format": "standard (zero-shot / Standard dataset baseline)",
            "performance": null,
            "performance_comparison": "Reported drop: accuracy on Constituent (¬E) 'drops by 52.4%' in the few-shot ICL setting (paper statement).",
            "format_effect_size": "-52.4% (drop reported vs baseline)",
            "explanation_or_hypothesis": "Few-shot demonstrations appear to introduce or reinforce spurious heuristics that the model exploits, leading to a large performance degradation on structural OOD examples.",
            "null_or_negative_result": true,
            "experimental_details": "Constituent set drawn from HANS (3k examples). Few-shot ICL used 3 examples sampled from MultiNLI; prompt construction follows the paper's few-shot ICL protocol.",
            "uuid": "e9297.2",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4_CoT_improvement_Constituent",
            "name_full": "GPT-4 Chain-of-Thought improvement on Constituent (¬E)",
            "brief_description": "GPT-4 achieves a measurable accuracy improvement on the Constituent non-entailment subset when using zero-shot CoT prompting compared to zero-shot prompting, suggesting CoT reduces shortcut reliance for this model and dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "NLI - Constituent (¬E) subset",
            "task_description": "HANS-derived non-entailment (¬E) examples designed to detect constituent heuristics.",
            "presentation_format": "zero-shot Chain-of-Thought prompting (zero-shot CoT)",
            "comparison_format": "zero-shot without CoT",
            "performance": null,
            "performance_comparison": "+14.0% accuracy with zero-shot CoT vs zero-shot (reported in paper).",
            "format_effect_size": "+14.0% (reported)",
            "explanation_or_hypothesis": "CoT encourages stepwise inference that mitigates surface-level heuristics like constituent shortcuts.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot CoT prompt template used (Figure 1). Constituent dataset from HANS, 3k examples; evaluation reported in Table 2.",
            "uuid": "e9297.3",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA2-Chat-13B_CoT_gain",
            "name_full": "LLaMA2-Chat-13B Chain-of-Thought gain on Constituent (¬E)",
            "brief_description": "LLaMA2-Chat-13B shows a very large improvement with zero-shot CoT on the Constituent non-entailment subset, indicating CoT can unlock better reasoning and reduce shortcut use in mid-sized models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-Chat-13B",
            "model_size": "13B",
            "task_name": "NLI - Constituent (¬E) subset",
            "task_description": "HANS-derived test of constituent heuristic vulnerability.",
            "presentation_format": "zero-shot CoT",
            "comparison_format": "zero-shot (no CoT)",
            "performance": null,
            "performance_comparison": "+40.9% accuracy with zero-shot CoT vs zero-shot (reported in paper).",
            "format_effect_size": "+40.9% (reported)",
            "explanation_or_hypothesis": "Larger / mid-sized models can leverage CoT to perform deeper parsing-style reasoning, reducing surface heuristic reliance.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot CoT used; dataset HANS Constituent (3k examples). Improvement quoted in main results.",
            "uuid": "e9297.4",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA2_ReverseScaling_ICL",
            "name_full": "Reverse scaling of LLaMA2-Chat family under ICL",
            "brief_description": "Within the LLaMA2-Chat series (7B→13B→70B), increasing model size sometimes correlates with greater reliance on spurious shortcuts under zero-shot and few-shot ICL prompts (reverse scaling), but the largest model benefits more from CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-Chat series",
            "model_size": "7B / 13B / 70B",
            "task_name": "NLI - Shortcut Suite",
            "task_description": "Comparison across model sizes on standard and shortcut NLI datasets under different prompting strategies.",
            "presentation_format": "zero-shot and few-shot ICL; zero-shot CoT",
            "comparison_format": "other prompt types (CoT vs non-CoT) and across sizes",
            "performance": "Reported trends: larger LLaMA2-Chat models show more shortcut reliance under ICL; in CoT, LLaMA2-Chat-70B outperforms smaller variants on most datasets.",
            "performance_comparison": "Trend-level: reverse scaling (performance decreases with size) under ICL; CoT reverses this trend for the largest model (70B) improving relative performance.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors suggest larger models have greater capacity to memorize/learn spurious pretraining correlations that manifest as shortcuts under ICL; CoT forces explicit reasoning that lets large models exploit their improved reasoning abilities.",
            "null_or_negative_result": null,
            "experimental_details": "Observed across LLaMA2-Chat 7B/13B/70B in Table 2; CoT improves 70B more than smaller sizes.",
            "uuid": "e9297.5",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Position_Presentation_Effect",
            "name_full": "Positional presentation of redundant tautologies (Position shortcut)",
            "brief_description": "Placing repetitive tautological phrases at different positions (beginning vs end) in the premise/hypothesis affects LLM predictions: accuracy is lowest when tautologies are added at the beginning, indicating positional cues influence model decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (aggregate)",
            "model_size": null,
            "task_name": "NLI - Position shortcut variant",
            "task_description": "Standard MultiNLI samples modified by appending repeated tautologies (e.g., 'and true is true') five times at different positions (start/end of premise or hypothesis) to test sensitivity to positional spurious cues.",
            "presentation_format": "zero-shot prompting with tautologies inserted at specific positions (beginning vs end)",
            "comparison_format": "same samples with tautologies placed at different positions",
            "performance": "Lowest accuracy predominantly when tautologies are placed at the beginning of the source text (exact accuracies per model in Table 3).",
            "performance_comparison": "Paper reports systematic decrease in accuracy for beginning-placement vs end-placement; specific model-by-model numbers in Table 3.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "LLMs tend to overweight information appearing early in a sentence, leading to shortcut behavior when irrelevant repetitive content is placed at the start.",
            "null_or_negative_result": false,
            "experimental_details": "Position dataset: Standard MultiNLI divided into groups, tautologies appended 5 times at different positions; accuracy details reported in Table 3 (lowest often when tautologies at start).",
            "uuid": "e9297.6",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Few-shot_example_selection",
            "name_full": "Effect of demonstration selection for few-shot prompts",
            "brief_description": "Using few-shot demonstrations sampled from shortcut-laden datasets can improve few-shot performance relative to demonstrations sampled from standard examples, but performance typically remains worse than zero-shot baselines, indicating pretraining-induced shortcuts dominate in-context gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (aggregate)",
            "model_size": null,
            "task_name": "NLI - Shortcut Suite (few-shot experiments)",
            "task_description": "Few-shot ICL experiments where demonstration examples are sampled either from MultiNLI (standard) or from the shortcut-laden datasets themselves.",
            "presentation_format": "few-shot ICL with different example selection strategies (standard vs shortcut-similar examples)",
            "comparison_format": "zero-shot and few-shot with alternative demonstration sets",
            "performance": "Few-shot using shortcut-similar examples performs better than using standard examples but still worse than zero-shot in many cases (detailed in Appendix D / Table 7).",
            "performance_comparison": "Paper states: 'LLMs' performance on shortcut-laden datasets using more similar examples is better than using standard examples, but still worse than zero-shot.'",
            "format_effect_size": null,
            "explanation_or_hypothesis": "In-context examples can provide useful distributional signal, but they cannot fully overcome pretraining-acquired spurious correlations; demonstrations can also inject additional biases.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot used 3 examples; alternative samplings and results summarized in Appendix D and Table 7.",
            "uuid": "e9297.7",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Style_and_Negation_presentation",
            "name_full": "Style-transfer and Negation insertion as prompt/data presentation variants",
            "brief_description": "Transforming premises into a different text style (Bible style via STRAP) or appending tautological negation phrases to hypotheses changes model performance: style and negation shortcuts reduce accuracy for many LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (aggregate)",
            "model_size": null,
            "task_name": "NLI (Style and Negation shortcut datasets) and extended tasks (Sentiment Analysis, Paraphrase Identification)",
            "task_description": "Style: premises converted to Bible English; Negation: append tautologies like 'and green is not red' to hypotheses; Extended: SA (SST-2) with negation injection and PI (QQP/PAWS) with lexical overlap adversaries.",
            "presentation_format": "zero-shot prompting on transformed data (style transfer or appended negation tautologies)",
            "comparison_format": "Standard dataset (no style transfer / no appended negation)",
            "performance": "Accuracy decreases on Negation and Style datasets across many models (e.g., GPT-4 accuracy decreases 15–35% on Negation across prompt settings; Style causes up to 15.6% decrease for GPT-4).",
            "performance_comparison": "Quantified examples: GPT-4: 15–35% drop on Negation; up to 15.6% drop on Style (reported in text). Extended tasks also show consistent declines (Table 5).",
            "format_effect_size": "-15% to -35% (examples reported for GPT-4 on Negation; -15.6% on Style)",
            "explanation_or_hypothesis": "Models appear to latch on to negation words and style cues as heuristic predictors (e.g., mapping negation tokens to contradiction/negative labels or Bible style to particular labels) rather than integrating meaning; style transfer and injected negation create spurious mappings.",
            "null_or_negative_result": false,
            "experimental_details": "Style: STRAP style-transfer applied to premises; Negation: append randomly chosen tautologies (six variants) to hypotheses; Extended tasks: SST-2 and QQP/PAWS used; results in Table 5 and Figure 8.",
            "uuid": "e9297.8",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CoT_negative_on_simple",
            "name_full": "Cases where CoT reduces accuracy on simple/standard datasets",
            "brief_description": "Although CoT improves performance on many shortcut datasets, the paper reports that for some strong LLMs (GPT-4, Gemini-Pro) CoT decreased accuracy on the Standard MultiNLI dataset and the Lexical Overlap dataset, indicating CoT is not universally beneficial.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, Gemini-Pro (examples)",
            "model_size": null,
            "task_name": "NLI - Standard and Lexical Overlap datasets",
            "task_description": "Standard MultiNLI and Lexical Overlap (HANS) benchmark sets.",
            "presentation_format": "zero-shot CoT",
            "comparison_format": "zero-shot without CoT",
            "performance": null,
            "performance_comparison": "Paper states accuracy of GPT-4 and Gemini-Pro decreases after applying CoT on Standard and Lexical Overlap datasets (exact deltas reported in Table 2).",
            "format_effect_size": "negative effect observed (model- and dataset-specific; see Table 2)",
            "explanation_or_hypothesis": "For relatively simple or already-solved datasets, additional CoT reasoning may be unnecessary and can introduce noise or degrade succinct signal the model would otherwise use; advanced models may not need CoT to perform well on simple cases.",
            "null_or_negative_result": true,
            "experimental_details": "Observed in main results (Table 2): CoT sometimes reduces accuracy for top-performing models on simple datasets.",
            "uuid": "e9297.9",
            "source_info": {
                "paper_title": "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models can be lazy learners: Analyze shortcuts in in-context learning",
            "rating": 2,
            "sanitized_title": "large_language_models_can_be_lazy_learners_analyze_shortcuts_in_incontext_learning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "rating": 1,
            "sanitized_title": "can_llms_express_their_uncertainty_an_empirical_evaluation_of_confidence_elicitation_in_llms"
        },
        {
            "paper_title": "Which is better? exploring prompting strategy for llm-based metrics",
            "rating": 1,
            "sanitized_title": "which_is_better_exploring_prompting_strategy_for_llmbased_metrics"
        }
    ],
    "cost": 0.01771025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models</p>
<p>Yu Yuan 
State Key Lab of Cognitive Intelligence
University of Science
Technology of China</p>
<p>Lili Zhao liliz@mail.ustc.edu.cn 
State Key Lab of Cognitive Intelligence
University of Science
Technology of China</p>
<p>Kai Zhang kkzhang08@ustc.edu.cn 
Guangting Zheng 
School of Computer Science and Technology
University of Science and Technology of China</p>
<p>Qi Liu 
State Key Lab of Cognitive Intelligence
University of Science
Technology of China</p>
<p>Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models
045B3AE18240EAD04093A2C15ACC7CB2
Large Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks.However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities.This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs' performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies.Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, significantly impairing their performance.2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts.3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while fewshot prompts generally underperform compared to zero-shot prompts.4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts.5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy.Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts.The code is available at https://github.com/yyhappier/ShortcutSuite.git.</p>
<p>Introduction</p>
<p>The field of Natural Language Processing (NLP) is experiencing rapid advancements, driven by the emergence of Large Language Models (LLMs) such as GPT (OpenAI, 2023;Achiam et al., 2023), Gemini (Team et al., 2023), and LLaMA (Touvron et al., 2023) series.These models have been Let's think step by step and make a 3-way decision of whether the hypothesis is true given the premise (entailment), false given the premise (contradiction), or whether the truth value cannot be determined (neutral).Provide your step-by-step analysis and classify your answer into one of the three categories (entailment, contradiction, neutral).pivotal in revolutionizing a wide array of tasks by leveraging techniques like In-Context Learning (ICL) (Brown et al., 2020) and Chain-of-Thought (CoT) promptings (Wei et al., 2022;Kojima et al., 2022), demonstrating exceptional capabilities without parameter updates.Despite these advances, the research on the robustness and generalization ability of LLMs across different contexts remains limited.</p>
<p>Models with poor robustness and generalization may rely on "shortcut learning," where they develop decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios (Geirhos et al., 2020).Therefore, evaluating LLMs performance in the face of shortcut information is crucial for understanding their robustness and generalization capabilities.</p>
<p>A recent study investigates the reliance of LLMs on shortcuts or spurious correlations within prompts (Tang et al., 2023).However, this research falls short of providing an exhaustive evaluation across a broad spectrum of LLMs and varied prompting contexts, focusing solely on ICL experiments.Furthermore, it only considers relatively simple shortcuts such as letters or signs.Consequently, its evaluation lacks comprehensiveness and granularity.</p>
<p>To address this, we introduce Shortcut Suite, an in-depth test suite designed to evaluate the performance of different LLMs across six shortcuts, five metrics, and four prompt settings.Extensive experiments on Shortcut Suite reveal that LLMs tend to capture spurious correlations between source text and particular labels, indicating a prevalence of shortcut learning.For example, as shown in Figure 1, Gemini-Pro resorts to matching subsequences (the professor recommended the bankers) in a Natural Language Inference (NLI) task rather than comprehending the clause structure or delving into the sentence's semantic content.This tendency of LLMs to capture spurious correlations can significantly impair their performance.In this paper, we conduct a comprehensive evaluation of LLMs' behavior concerning shortcut learning from the following perspectives.</p>
<p>First, to identify the reliance of LLMs on shortcuts in downstream tasks, we collect six datasets containing different shortcuts and analyze the accuracy of LLMs on these datasets.We find a notable performance drop across various shortcuts, especially Constituent and Negation shortcuts, in some cases by more than 40%.Moreover, in the Position dataset, LLMs demonstrate a propensity for shortcut learning behavior by prioritizing the beginning of sentences while neglecting the end, revealing a vulnerability to additional information within sentences.Furthermore, an analysis of the distribution of LLMs' predictions revealed inherent biases, with the LLMs favoring certain labels over others even in a balanced standard dataset.</p>
<p>Second, we perform comprehensive evaluation metrics to assess the impact of shortcuts on LLMs.In addition to accuracy, we introduce three novel metrics to assess the explanatory power of LLMs: Semantic Fidelity Score (SFS), Internal Consistency Score (ICS), and Explanation Quality Score (EQS).Our analyses using these metrics reveal that LLMs' explanations often contain contradictions.Furthermore, we prompt LLMs to report their confidence levels and consistently find that they are overconfident in their predictions.</p>
<p>Third, we compare the performance of different LLMs and different prompting strategies in shortcut learning.Closed-source and some opensource LLMs excel on standard datasets but falter on those with shortcuts.Surprisingly, larger LLMs are more prone to utilize shortcuts under zero-shot and few-shot ICL prompts.We find that LLMs are less affected by shortcuts under CoT settings than others.Notably, LLMs often demonstrate inferior performance in few-shot scenarios compared to zero-shot scenarios.</p>
<p>Finally, We summarize three error types of LLMs in shortcut learning by checking their CoT responses: distraction, disguised comprehension, and logical fallacy.These errors predispose LLMs to adopt shortcuts, undermining their robustness.</p>
<p>Related Work</p>
<p>Shortcut Learning in PLMs.Shortcuts are decision rules that perform well on Independent and Identically Distributed (IID) test data but fail on Out-Of-Distribution (OOD) tests, revealing a mismatch between intended and learned solutions (Geirhos et al., 2020).Recent studies have shown that Pre-trained Language Models (PLMs) tend to exploit dataset biases as shortcuts to make predictions (Geirhos et al., 2020;Ribeiro et al., 2020), leading to low generalization for OOD samples in various NLP tasks, such as NLI (McCoy et al., 2020), question-answering (Jia and Liang, 2017;Sen and Saffari, 2020), reading comprehension (Lai et al., 2021) and coreference inference (Zhao et al., 2018).For example, NLI models tend to predict the contradiction label if the test samples contain negation words.Several approaches have been proposed to address this problem.He et al. (2019) presented a debiasing algorithm called DRiFt based on residual fitting.Du et al. (2021) proposed a shortcut mitigation framework LTGR to suppress the model from making overconfident predictions for shortcut samples.Zhao et al. (2024) introduced COMI to reduce the models reliance on shortcuts and enhance its ability to extract underlying information integrated with standard Empirical Risk Minimization.Yue et al. (2024) proposed SSR to boost rationalization by discovering and exploiting potential shortcuts.</p>
<p>Shortcut Learning in LLMs.Du et al. (2023) provided a review of recent developments that address the robustness challenge of LLMs.The most related work was the study investigating the reliance of LLMs on shortcuts within in-context learning (Tang et al., 2023).Our work differs from it in the following ways: First, their experiments were conducted on a limited model scope (GPT2</p>
<p>Shortcut</p>
<p>Definition</p>
<p>Premise Hypothesis</p>
<p>Lexical Overlap Assume that a premise entails all hypotheses constructed from words in the premise.</p>
<p>The actor was encouraged by the lawyer .The actor encouraged the lawyer .</p>
<p>Subsequence</p>
<p>Assume that a premise entails all of its contiguous subsequences.</p>
<p>The authors in front of the senators contacted the artists.The senators contacted the artists.</p>
<p>Constituent</p>
<p>Assume that a premise entails all complete subtrees in its parse tree.Unless the president saw the professor, the student waited.The student waited.</p>
<p>Negation</p>
<p>Assume that a hypothesis entails strong negation words ("no", "not", "nothing","never").They are all quotations from the Old Testament Book of Aunt Ruth.Every one of them is quotations from the Old Testament and green is not red.</p>
<p>Position</p>
<p>Assume that the label is related to spurious position cues.</p>
<p>Red is red and red is red and red is red and red is red and red is red and "Wait here," I was ordered.&amp; "Wait here," I was ordered and red is red and red is red and red is red and red is red and red is red.</p>
<p>He told me to come with him.</p>
<p>Style</p>
<p>Assume that the label is related to spurious text style cues.And Severn said unto him, Thou and thy friends are not welcome here, said he. (Bible English)</p>
<p>Severn said the people were not welcome there.3 Problem Definition LLM for NLI.In the NLI task, also known as textual entailment recognition, models evaluate a premise-hypothesis pair and determine their semantic relationship -typically labeled as entailment, neutral, or contradiction.Given a prompt P with a source text x, the LLM will generate a probability of target y conditioning on the prompt P .This could be written as
p LLM (y | P, x) = T ∏ t=1 p (y t | P, x, y &lt;t ) , (1)
where T is the generated token length and y t denotes the t-th token.For basic prompts such as zero-shot, y takes the range of the corresponding label.For prompting strategies such as CoT, y contains the reasoning process and the final label.</p>
<p>Framework to Generate Shortcuts.Given a premise q, a hypothesis h, and a universally true statement s (s ≡ ⊤) that may contain a certain shortcut, the logical relations are preserved upon their conjunction.Specifically, if q and h have the target label l, denoted as {(q, h, y)|y = l}, then q combined with s (q ∧ s) maintains the label {(q ∧ s, h, y)|y = l} since q ∧ s ≡ q ∧ ⊤ ≡ q.Thus, the source text has two mappings for the target label l.The model can either use the semantic relationship between the text and label (x → l) or the injected shortcut (s → l) for inference.</p>
<p>Shortcut Suite</p>
<p>As NLI is well positioned to serve as a benchmark task for research on NLP and can encapsulate the entire spectrum of the six identified shortcuts, we mainly anchor our framework on it.We also explore other tasks in Appendic C. Building on previous research, we create six datasets with different shortcuts and develop five metrics to investigate LLMs' shortcut learning behavior and understand their robustness generalization capabilities.</p>
<p>Dataset Creation</p>
<p>We present six types of shortcuts in Table 1, each with an illustrative definition and an example.</p>
<p>Standard.The Multi-Genre Natural Language Inference (MultiNLI) (Williams et al., 2018) dataset serves as a benchmark for assessing models on NLI, encompassing ten genres of English.For a focused assessment, we have curated a balanced selection comprising 3000 samples from the development subset of MultiNLI.</p>
<p>Lexical Overlap &amp; Subsequence &amp; Constituent</p>
<p>For these three sets, we utilize the Heuristic Analysis for NLI Systems (HANS) (McCoy et al., 2020) dataset for evaluation.HANS is designed to diagnose the use of fallible structural heuristics and is annotated with two labels only (entailment and non-entailment).Specifically, we collect 3000 examples for each set from HANS, where the heuristic is lexical overlap, subsequence, and constituent accordingly, with labels and templates equally divided.</p>
<p>Negation.We explore the impact of strong negation words like "no", "not", "nothing" and "never" on model predictions.Inspired by (Naik et al., 2018), we append the tautology -"and false is not true", "and green is not red", "and up is not down", "and no square is a circle", "and nothing comes from nothing", and "and history never change", chosen randomly with equal probability to the end of the hypothesis sentence in the Standard dataset.</p>
<p>Position.To test the influence of the position of label-associated information, we divide the Standard dataset into four equally distributed label and genre groups.In each group, we append phrases like "and true is true", "and red is red" or " and up is up" five times at different positions.This allows us to evaluate whether LLMs rely on irrelevant positional cues when making predictions.</p>
<p>Style.We consider the style of the text as a possible shortcut (Qi et al., 2021) and focus on one prominent style: Bible style.Specifically, we employ a simple but powerful text style transfer model called STRAP (Krishna et al., 2020) and apply it to transfer the premises in the Standard dataset into Bible-style texts.</p>
<p>Metrics</p>
<p>We adopt accuracy to quantify performance on NLI tasks and introduce new metrics to assess the explanatory power of LLMs.Semantic Fidelity Score (SFS) evaluates the extent to which the generated content preserves the essential meaning of the source text.We employ a pre-trained BERT (f bert ) (Kenton and Toutanova, 2019) model to create embedding for the input and the output collectively, then compute their cosine similarity.For a prompt P and model output c, SF S is given by
SF S = CosineSimilarity(f bert (P ), f bert (c)). (2)
Internal Consistency Score (ICS) assesses whether there are logical contradictions within the reasoning steps of LLMs or between the reasoning and the answer.To estimate the probability of contradiction p contra , we use an NLI model (Laurer et al., 2024) that categorizes hypothesiscontext pairs into classes of entailment, neutral, and contradiction.For a reasoning chain of N steps, c = (c 1 , c 2 , . . ., c N ), where the last step is the answer, and p contra (c i , c j ) indicates the probability that step c i contradicts step c j , we define the function f (c) as
f (c) =      0, if ∃ (c i , c j ), 1 ≤ i &lt; j ≤ N, s.t. p contra (c i , c j ) &gt; 1 3 , 1, otherwise. (3)
The overall ICS is the mean of all calculated f (c) values for the given explanations.</p>
<p>Explanation Quality Score (EQS) integrates the SFS and ICS to reflect the overall quality of LLMs' output and is defined as
EQS = w 1 • SF S + w 2 • ICS,(4)
where weights w 1 and w 2 represent the significance of each score in the overall evaluation.In this work, w 1 and w 2 are equally set as 0.5.Confidence Score (CFS) is designed to evaluate LLMs' self-assessment capabilities.We follow (Xiong et al., 2023) to prompt LLMs to provide their confidence level, which indicates the degree of certainty they have about their answer and is represented as a percentage.</p>
<p>Evaluated LLMs</p>
<p>To obtain a comprehensive understanding of how LLMs are affected by shortcuts, we conduct experiments on three widely used closed-source LLMs: GPT-3.5-Turbo(OpenAI, 2023), GPT-4 (Achiam et al., 2023) and Gemini-Pro (Team et al., 2023).Regarding open-source LLMs, we select LLaMA2-Chat-series (7B, 13B, 70B) (Touvron et al., 2023), ChatGLM3-6B (Zeng et al., 2022) and Mistral-7B (Jiang et al., 2023) for assessment.</p>
<p>Prompting Strategies</p>
<p>Our experiments aim to assess the performance of LLMs in different settings, including zeroshot, few-shot ICL, zero-shot CoT, and few-shot CoT promptings.For zero-shot CoT, we utilize the prompt depicted in Figure 1.To construct few-shot ICL prompts, we enhance the best-performing zero-shot prompt by incorporating three random samples from the remaining examples in MultiNLI.Likewise, we employ a similar sampling approach for few-shot CoT and use GPT-4 to generate analyses for these examples.</p>
<p>Experimental Results</p>
<p>We conduct our experiments based on the Shortcut Suite and observe that LLMs tend to exploit various shortcuts in downstream tasks, resulting in a notable decrease in performance.In this section, we present a comprehensive analysis.</p>
<p>Overall Performance</p>
<p>Effect of Different LLMs</p>
<p>As shown in Table 2, closed-source and some open-source LLMs excel on standard datasets, with GPT-4 leading at an accuracy of 85.6%, followed by Gemini-Pro at 77.9%, GPT-3.5-Turbo at 71.7%, LLaMA2-Chat-70B at 70.9% and Mistral-7B at 69.6%.However, this high level of performance does not extend to shortcut datasets.For example, the accuracy of GPT-3.5-Turbo on the Constituent (¬E) dataset drops by 52.4% in the fewshot ICL setting.This significant drop suggests that LLMs are easily prone to adopting shortcuts for prediction.</p>
<p>Among open-source LLMs, Mistral-7B performs the best with CoT prompts.It excels on both standard and shortcut datasets, nearly surpassing LLaMA2-Chat-13B in all settings and even exceeding GPT-3.5-Turbo in some scenarios, demonstrating remarkable capabilities in NLI and robustness generalization.On the other hand,  ChatGLM3-6B is most affected by shortcuts, resulting in the poorest performance.Furthermore, we observe a reverse scaling pattern of LLaMA2-Chat in zero-shot and few-shot ICL scenarios.As the model size increases, it tends to rely more on spurious mapping for NLI tasks, resulting in lower accuracy.However, in the CoT scenario, LLaMA2-Chat-70B outperforms smaller models on most datasets.This indicates that larger models retain improved semantic comprehension and reasoning abilities but require suitable prompts to fully leverage their potential.This phenomenon is also observed in the LLaMA3 series, as illustrated in Appendix C.</p>
<p>Effect of Shortcut Types</p>
<p>Regarding Lexical Overlap, Subsequence, and Constituent shortcuts, LLMs consistently favor predicting entailment (E) and thus struggle with the non-entailment (¬E) class.This indicates that LLMs can easily exploit these spurious correlations with the label E, leading to poor performance on ¬E instances.Lexical Overlap appears to be the easiest task for most LLMs across different prompt settings, resulting in high accuracy, while the Constituent shortcut poses the greatest challenge.For instance, in the zero-shot setting, Gemini-Pro experiences a significant 29.0% drop on Constituent, from 76.2% to 47.2%, worse than random guessing at 50%.Negation, Position, and Style shortcuts also prove challenging for most LLMs, as indicated by the notable decrease in accuracy.In the Negation dataset, the accuracy of GPT-4 decreases by 15-35% across the four different prompt settings.In  the Style dataset, the accuracy of GPT-4 decreases up to 15.6%.Moreover, the detailed results of the Position shortcut are presented in Table 3.The lowest accuracy rates are predominantly observed when extra phrases are added at the beginning of the sentence, suggesting that the LLMs may rely more heavily on the beginning parts of sentences for cues than the end parts, which could be a potential shortcut for improvement.</p>
<p>Effect of Prompting Types</p>
<p>Most LLMs demonstrate significant performance gains in all datasets when utilizing the CoT prompt.For example, GPT-4 with a zero-shot CoT prompt on the Constituent (¬E) dataset achieves an accuracy improvement of 14.0% compared to zero-shot, while LLaMA2-Chat-13B shows an improvement of 40.9% under the same conditions.However, the accuracy of GPT-4 and Gemini-  Pro decreases after applying the CoT prompt on the Standard dataset and Lexical Overlap dataset.This phenomenon reveals that LLMs are prone to utilize shortcuts to predict, and the CoT prompt can promote in-depth inference and reduce the reliance on spurious correlations, thus improving performance.However, for relatively simple datasets, advanced LLMs may already possess sufficient semantic understanding and reasoning capabilities, reducing their dependence on CoT for performance enhancement.Additionally, it is worth noting that the effectiveness of few-shot prompts is not superior to zero-shot prompting.In several scenarios, the fewshot ICL is less effective than the zero-shot, and the few-shot CoT performs worse than the zeroshot CoT.This discrepancy could be attributed to the LLMs acquiring biases from the in-context examples.Similar phenomena have been reported in (Kim et al., 2023;Tang et al., 2023).We show more experimental results and analysis in Appendix D.</p>
<p>In-depth Analysis</p>
<p>Explanation Quality</p>
<p>We evaluate the explanation quality of LLMs in shortcut challenges using Equations 2, 3, and 4, with results presented in Table 4.</p>
<p>For SFS, most LLMs score above 85%, indicating that current models have achieved a relatively high level of semantic fidelity.GPT-3.5-Turboscores the highest on the Standard dataset with 92.1%, while Mistral-7B scores the lowest at 88.5%.Generally, models demonstrate a slight decline in SFS on shortcut datasets compared to the Standard dataset, indicating a reduced ability to restate inputs effectively in these contexts.</p>
<p>Regarding ICS, most LLMs score below 50%, suggesting that more than half of their responses are contradictory.Notably, LLMs exhibit lower ICS scores on shortcut datasets compared to the Standard dataset.For example, LLaMA2-Chat-70B achieves a score of 41.5% on the Standard dataset but only 13.5% on the Negation dataset.These observations suggest that a lack of internal consistency in reasoning is a significant factor contributing to LLMs' reduced performance when dealing with shortcuts.</p>
<p>The overall EQS, which combines SFS and ICS, provides a comprehensive reflection of the overall quality of explanations from LLMs.Typically, models that exhibit higher accuracy also demonstrate greater explanatory capabilities.</p>
<p>Confidence Score</p>
<p>Figure 2 displays the confidence levels of LLMs, revealing two key findings.First, LLMs tend to be overconfident, with their confidence scores rarely falling below 60% and often significantly exceeding their actual accuracy.Second, the discrepancy between confidence and accuracy is notably greater in datasets containing shortcuts compared to the Standard dataset.This suggests that LLMs not only adopt shortcuts but also exhibit heightened confidence in these spurious mappings without fully understanding the true relationship between the source text and the corresponding label.</p>
<p>Prediction Distribution</p>
<p>Figure 3 shows the label distribution in each LLM's prediction.Despite a balanced distribution in the ground truth, we can easily observe that in the Standard dataset, GPT-3.5-Turbo,LLaMA2-Chat-7B, and Mistral-7B tend to disproportionately predict neutral over the other two categories.Conversely, LLaMA2-Chat-13B and ChatGLM3-6B show a bias towards entailment.This pattern may stem from multiple factors, including potential overfitting to the NLI task or tasks with a similar categorical structure.</p>
<p>For datasets featuring Lexical Overlap, Subsequence, and Constituent shortcuts, LLMs predom-    inantly predict entailment, indicating a susceptibility to these shortcuts.For the Negation shortcut, a rise in contradiction predictions by GPT-4 and LLaMA2-Chat-13B suggests a reliance on a spurious correlation between negation words and the contradiction label.</p>
<p>Error Analysis</p>
<p>We identify three types of errors in shortcut learning by analyzing the CoT responses of LLMs.The first issue is distraction, where LLMs are easily distracted by irrelevant information.As shown in Figure 4, they may focus on repetitive tautologies, leading to the neglect of useful information in the original text.Additionally, they often prioritize words at the start of a sentence while neglecting those at the end, as shown in Table 3.This reflects a tendency in LLMs to concentrate on local information while ignoring the comprehensive context.Second, LLMs suffer from disguised comprehension.Specifically, they struggle to grasp the subtleties of individual words, sentence structures, and complex biblical language styles, shifting one's concept to another.This leads to disguised comprehension where LLMs might inadvertently "borrow" concepts, causing them to rely on shortcuts to make incorrect inferences.The detailed case can be found in Figure 6.</p>
<p>The third issue is logical fallacy.LLMs tend to reduce intricate reasoning to overly simplistic terms, generalizing from specific instances to broader conclusions via the use of shortcuts.This oversimplification in their reasoning process can INPUT Premise: Red is red and red is red and red is red and red is red and red is red and Look, there's a legend here.Hypothesis: See, there is a well known hero here.</p>
<p>OUTPUT Analysis:</p>
<p>1.The premise is a series of repetitions of the statement "red is red".2. The hypothesis is "there is a well known hero here".3.There is no logical connection between the premise and the hypothesis.The premise does not provide any evidence or information that would support the hypothesis.Answer: neutral lead to erroneous results, as illustrated in Figure 7.</p>
<p>Extended Evaluation</p>
<p>To gain further insight into the shortcut challenges in LLMs, we conduct experiments on other NLP tasks.The first is the Sentiment Analysis (SA) task.Specifically, we use the validation set of the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) as our Standard dataset.We then introduce the Negation shortcut using the method described in Section 4.1 to the Standard dataset.The second is the Paraphrase Identification (PI) task.We experiment with the Quora Question Pairs (QQP)1 dataset as Standard dataset and the Paraphrase Adversaries from Word Scrambling (PAWS) (Zhang et al., 2019)  Lexical Overlap shortcut.The results, presented in Table 5, demonstrate a consistent decline in performance across both the SA and PI tasks on datasets comprising shortcuts compared to Standard datasets.Furthermore, as shown in Figure 8, there is a noticeable increase in negative predictions on the Negation dataset and an increase in duplicate predictions on the Lexical Overlap dataset.This pattern suggests that LLMs tend to capture spurious correlations between negation words and the negative label, as well as between word overlap and the duplicate label.In conclusion, we find that LLMs are prone to relying on the Negation shortcut in the SA task and the Lexical Overlap shortcut in the PI task, suggesting that shortcut learning is a prevalent phenomenon in LLMs across a wide spectrum of tasks.</p>
<p>Besides the LLMs mentioned above, we conduct experiments on the latest LLMs, such as LLaMA3-series, and analyze the results as detailed in Appendix C.</p>
<p>Conclusion</p>
<p>In this study, we proposed Shortcut Suite, a test suite designed to evaluate the performance of LLMs in shortcut learning across several NLP tasks.Shortcut Suite encompasses six types of shortcuts: Lexical Overlap, Subsequence, Constituent, Negation, Position, and Style, and evaluates performance using five metrics: ACC, SFS, ICS, EQS, and CFS, across four prompt settings: zero-shot, few-shot ICL, zero-shot CoT, and fewshot CoT.Our extensive experiments on diverse LLMs demonstrated that LLMs frequently rely on shortcuts in downstream tasks.We explored the impact of different models, types of shortcuts, and prompting strategies.Our analysis then extended to explanation quality, label distribution, confidence score and error analysis.</p>
<p>Our findings offer new perspectives on LLMs' robustness and present new challenges for reducing their shortcut reliance, paving the way for future advancements in this field.</p>
<p>Limitations</p>
<p>In this paper, we primarily focus on evaluating the effect of shortcut learning in LLMs on the NLI task, with additional exploration into tasks like SA and PI.However, we acknowledge that other NLP tasks, such as question-answering and coreference inference, could offer further insights and should be investigated in future research.</p>
<p>While this study provides a comprehensive understanding of shortcut learning in LLMs, it does not propose specific methods to mitigate this phenomenon effectively.Nonetheless, we identify shortcut learning behavior in LLMs and categorize potential error types associated with shortcut learning, offering a foundation for future research.Based on our findings, we suggest several potential approaches for addressing shortcut learning in LLMs.One approach is fine-tuning on unbiased datasets, as training models on diverse and representative datasets may help alleviate shortcut learning.Moreover, employing advanced prompting techniques is essential.Our experiments indicate that few-shot prompting is insufficient for mitigating shortcut learning behaviors in LLMs, thus enhancing reasoning capabilities through methods such as CoT prompting may prove effective.Additionally, implementing retrieval augmentation by incorporating relevant external documents can ground LLMs, thereby reducing knowledge gaps and instances of hallucination.We advocate for further research to develop effective strategies aimed at addressing shortcut learning in LLMs.</p>
<p>A Appendix: Label Distribution  B Appendix: Error Analysis</p>
<p>INPUT Premise:</p>
<p>The doctors that believed the managers introduced the bankers.</p>
<p>Hypothesis:</p>
<p>The managers introduced the bankers.</p>
<p>OUTPUT Analysis:</p>
<p>1.The premise is "The doctors that believed the managers introduced the bankers."This means that some doctors believed that the managers introduced the bankers.2. The hypothesis is "The managers introduced the bankers."This means that the managers did introduce the bankers.</p>
<ol>
<li>If the premise is true, then some doctors believed that the managers introduced the bankers.This means that it is possible that the managers did introduce the bankers.Therefore, the hypothesis is true given the premise.</li>
</ol>
<p>Answer: neutral</p>
<p>Figure 6: An illustrative example of disguised comprehension in LLMs: the positions of "that" and "believed" are mistakenly swapped, leading to the incorrect assumption that they convey the same meaning.</p>
<p>INPUT Premise:</p>
<p>The judge knew the lawyer thanked the actor.</p>
<p>Hypothesis:</p>
<p>The judge knew the lawyer.</p>
<p>OUTPUT Analysis:</p>
<ol>
<li>We can identify the following facts from the premise:</li>
</ol>
<p>-The judge knew the lawyer thanked the actor.</p>
<p>-We can infer that the judge knew the lawyer.2. The hypothesis is "The judge knew the lawyer."3. The premise implies that the judge knew the lawyer.4. Therefore, the hypothesis is true given the premise Answer: entailment</p>
<p>C Appendix: Extended Evaluation of Shortcut Learning</p>
<p>Model.In addition to the LLMs we discussed above, we'd like to extend our investigation to the LLaMA3-series.Notably, LLaMA3 demonstrates superior performance over LLaMA2.Specifically, LLaMA3-8B-Instruct outperforms both LLaMA2-Chat-7B and LLaMA2-Chat-13B on most datasets.Furthermore, LLaMA3-70B-Instruct surpasses GPT-3.5-Turbo and approaches the performance of Gemini-Pro.Despite these advances, we observe a consistent decline in performance on shortcut datasets compared to standard datasets.This trend suggests that LLaMA3-8B, similar to its predecessor, may rely on shortcuts for predictions.Additionally, the reverse scaling pattern persists in shortcut datasets such as Subsequence (¬E) and Constituent (¬E).These supplementary experiments highlight the propensity of most LLMs to rely on shortcuts across a wide spectrum of tasks, underscoring the need for more robust and generalizable mechanisms.</p>
<p>D Appendix: More Discussion on Few-shot Prompting</p>
<p>As discussed above, few-shot ICL is less effective than zero-shot prompting, and few-shot CoT performs worse than zero-shot CoT in several scenarios.This phenomenon may be due to biases introduced by the in-context examples used in few-shot prompting.Similar issues have been reported in other studies.For instance, Kim et al. (2023) observed that demonstrations can introduce biases, leading to reduced performance in language models.Tang et al. (2023) also noted that LLMs might   Table 7: Accuracy (%) across all datasets of GPT-3.5-Turbo.</p>
<p>exploit shortcuts in in-context learning, resulting in sub-optimal performance.Moreover, some papers focus specifically on this issue.For instance, Min et al. (2022) found that factors like the label space, the distribution of the input text, and the overall format of the sequence are critical determinants of task performance.To further explore this issue, we conducted additional experiments using random samples from the remaining examples in each shortcut-laden dataset, beyond those from the MultiNLI dataset initially used in above experiments.The detailed results are shown in Table 7.We observe that LLMs' performance on shortcut-laden datasets using more similar examples is better than using standard examples, but still worse than zero-shot, indicating that the influence of shortcuts from pre-trained data is more significant than the benefits of in-context examples.LLMs struggle to summarize the important aspects from in-context examples to overcome their inherent biases and are even influenced by the biases from the in-context examples.</p>
<p>Figure 1 :
1
Figure1: Shortcut Learning Behavior: The LLM mistakenly infers the premise entails the hypothesis if all subsequences match, skipping deep semantic analysis.</p>
<p>Figure 2 :
2
Figure 2: Box plots of confidence scores across all datasets under zero-shot CoT prompting (each LLM is denoted by an abbreviation).LLMs generally report confidence scores that significantly exceed their actual accuracy.</p>
<p>Figure 3 :
3
Figure 3: Label distribution percentages (%) for each LLMs predictions under zero-shot prompting (each LLM is abbreviated).Distributions for the other three datasets are in Appendix A.</p>
<p>Figure 4 :
4
Figure 4: An illustrative example of distraction in LLMs: in the Position dataset, the LLM is observed to be distracted by repetitive tautologies, leading them to ignore useful information.</p>
<p>Figure 5 :
5
Figure 5: Label distribution as percentages (%) for LLMs' prediction under zero-shot prompting (each LLM is denoted by an abbreviation).</p>
<p>Figure 6 ,
6
Figure 6,7 show the disguised comprehension error example and the logical fallacy error example respectively.</p>
<p>Figure 7 :
7
Figure7: An illustrative example of logical fallacy in LLMs: an oversimplification in the Subsequence dataset is found in the analysis process.In the source text, knowing of an action (the lawyer thanking the actor) doesn't necessarily equate to knowing the person (the lawyer) in a broader sense.</p>
<p>Figure 8 :
8
Figure 8: Label distribution as percentages (%) for LLMs' prediction under zero-shot prompting on SA task.Each LLM is denoted by an abbreviation.</p>
<p>Table 1 :
1
Definitions and examples of the shortcuts explored in this paper.
and OPT), whereas we use richer and more rep-resentative LLMs. Second, we focus on identi-fying shortcuts within the source text across dif-ferent prompt settings rather than assessing solelyagainst prompts. Third, while they rely on simpletriggers such as letters or signs, resembling adver-sarial attacks, we propose more subtle and realisticshortcuts and test whether LLMs can identify andavoid these shortcuts.</p>
<p>Table 2 :
2
Accuracy (%) across all datasets under four prompt settings.E and ¬E are respectively referring to entailment (IID) and non-entailment (OOD) sets.The intensity of blue highlights corresponds to the absolute decrease in accuracy compared to the Standard dataset for each LLM.
ModelStandard Lexical Overlap Subsequence Constituent Negation Position StyleE¬EE zero-shot¬EE¬EGPT-3.5-Turbo GPT-4 Gemini-Pro LLaMA2-Chat-7B LLaMA2-Chat-13B LLaMA2-Chat-70B ChatGLM3-6B Mistral-7B56.7 85.6 76.2 42.1 54.3 57.7 40.0 49.469.5 96.7 81.3 76.9 99.0 66.9 75.4 53.983.8 100.0 97.7 40.0 42.2 40.7 41.7 96.258.6 58.3 67.5 40.2 95.8 73.5 96.7 80.0 88.6 48.6 77.9 47.2 72.8 46.4 60.6 25.4 99.7 6.0 95.9 0.8 61.6 53.8 77.8 34.9 82.4 25.5 79.4 14.6 57.9 73.9 48.8 75.939.8 54.3 53.1 37.7 54.6 52.4 32.8 38.143.3 67.4 56.2 39.3 55.4 53.9 34.7 40.551.5 70.0 62.5 39.6 53.8 52.7 33.5 43.0few-shot ICLGPT-3.5-Turbo GPT-4 Gemini-Pro LLaMA2-Chat-7B LLaMA2-Chat-13B LLaMA2-Chat-70B ChatGLM3-6B Mistral-7B61.7 83.9 77.9 40.2 59.1 57.8 35.6 63.993.3 96.7 95.3 66.5 97.5 100.0 100.0 84.438.7 99.3 92.9 75.3 48.5 3.6 0.0 84.791.3 23.3 96.7 9.3 91.3 71.3 94.0 92.0 94.0 37.0 95.8 30.4 53.3 59.5 55.9 33.1 87.3 12.4 92.4 12.1 99.8 3.1 99.6 1.6 100.0 0.0 100.0 0.0 73.3 57.7 72.1 48.050.0 49.7 45.6 37.0 50.3 45.2 32.5 40.947.8 69.7 55.3 39.4 54.0 53.7 32.6 47.649.5 72.0 60.5 38.6 53.3 50.8 34.7 56.4zero-shot CoTGPT-3.5-Turbo GPT-4 Gemini-Pro LLaMA2-Chat-7B LLaMA2-Chat-13B LLaMA2-Chat-70B ChatGLM3-6B Mistral-7B64.7 81.3 72.7 48.0 56.3 60.3 48.9 69.675.3 94.0 68.0 71.2 59.7 74.4 82.9 76.577.3 100.0 94.6 46.0 74.6 69.7 32.0 94.765.3 59.3 78.7 35.3 98.0 61.3 96.0 94.0 65.9 56.3 74.9 58.9 62.7 42.1 63.4 34.1 52.5 56.8 53.9 41.7 69.6 44.7 72.0 25.3 81.4 24.8 76.0 28.0 83.7 63.5 71.2 58.451.5 58.3 65.2 43.8 49.2 56.6 39.1 46.354.0 75.2 58.2 45.5 52.0 53.7 44.2 49.960.7 69.3 60.0 47.5 48.8 52.3 43.5 58.8few-shot CoTGPT-3.5-Turbo GPT-4 Gemini-Pro LLaMA2-Chat-7B LLaMA2-Chat-13B LLaMA2-Chat-70B ChatGLM3-6B Mistral-7B71.7 83.0 72.4 43.8 60.6 70.9 40.0 67.685.3 95.3 86.1 78.1 72.1 78.2 94.6 88.375.3 100.0 64.5 34.9 51.1 66.2 9.7 58.683.3 55.3 90.0 22.0 94.7 66.0 95.3 88.0 81.4 40.5 87.5 37.0 70.3 37.7 64.3 42.1 54.5 37.2 70.6 32.6 68.0 54.0 78.9 38.4 92.9 11.4 86.8 20.0 84.0 38.2 81.9 32.353.7 67.3 63.2 39.3 47.5 58.5 34.8 50.460.7 74.7 59.4 41.4 50.6 57.9 34.7 48.563.0 70.3 62.4 40.8 53.1 57.9 38.7 59.4</p>
<p>Table 3 :
3
The Accuracy Details for Position Shortcut:
We place tautologies at the start or end of the premise or hypothesis in the Standard dataset. The lowest accu-racy for each LLM is underlined, frequently occurring when the tautologies are placed at the beginning of the source text.</p>
<p>Table 4 :
4
SFS (%), ICS (%), and EQS (%) across all datasets under zero-shot CoT prompting.The worst score for each LLM is underlined.LLMs typically show the lowest explanation quality in datasets comprising shortcuts.
88.1 | 9.5 88.0 | 22.4 88.0 | 21.2 87.8 | 20.1 87.7 | 24.0 91.2 | 24.2 90.5 | 23.3 90.4 | 23.5Mistral-7B88.5 | 45.5 85.1 | 63.9 89.0 | 29.4 84.2 | 67.7 88.3 | 54.9 83.2 | 69.2 87.9 | 53.0 91.2 | 44.4 87.2 | 49.6 89.5 | 44.2EQSGPT-3.5-Turbo60.663.248.760.858.663.158.557.559.157.5GPT-462.963.251.357.156.767.353.672.967.859.3Gemini-Pro66.163.859.259.257.362.058.168.664.668.4LLaMA2-Chat-7B54.560.147.157.253.156.953.650.552.754.2LLaMA2-Chat-13B65.961.351.158.956.864.458.251.055.257.5LLaMA2-Chat-70B62.266.449.0126.857.366.162.257.961.160.2ChatGLM3-6B56.656.148.855.254.654.055.957.756.957.0Mistral-7B67.074.559.276.071.676.270.567.868.466.9</p>
<p>dataset to represent
ModelSAPIStandardNegationStandardOverlapGPT-3.5-Turbo GPT-4 Gemini-Pro LLaMA2-Chat-7B LLaMA2-Chat-13B 87.4 91.7 93.0 92.7 84.1 LLaMA2-Chat-70B 87.8 ChatGLM3-6B 90.4 Mistral-7B 80.587.0 90.2 87.8 76.1 83.3 87.1 85.4 79.181.2 73.7 75.9 61.6 73.8 71.7 64.9 52.674.3 64.2 47.4 49.5 50.0 52.0 49.6 49.6Table 5: Accuracy (%) of the SA and PI tasks under zero-shot prompting. LLMs consistently demonstrate reduced performance on shortcut datasets compared to the Standard, as indicated by the blue highlights.</p>
<p>Table 6 :
6
ModelStandard Lexical Overlap Subsequence Constituent Negation Position Style Accuracy (%) across all datasets of LLaMA3-series.
E¬EE zero-shot¬EE¬ELLaMA3-8B-Instruct LLaMA3-70B-Instruct62.2 74.584.3 94.389.2 96.888.3 48.3 79.0 40.1 99.7 39.9 83.9 11.151.6 59.753.2 63.755.0 64.0zero-shot CoTLLaMA3-8B-Instruct LLaMA3-70B-Instruct65.3 79.063.5 79.296.1 99.146.9 75.7 65.3 68.6 93.9 58.2 48.5 71.652.4 62.157.0 65.455.9 51.7PromptingStandard Lexical Overlap Subsequence Constituent Negation Position Stylezero-shot few-shot (MNLI) few-shot (shortcut)56.7 61.7 61.7E 69.5 93.3 86.3¬E 83.8 38.7 90.3E 58.6 58.3 67.5 40.2 ¬E E ¬E 91.3 23.3 96.7 9.3 81.7 56.3 82.3 35.039.8 50.0 46.043.3 47.8 54.651.5 49.5 55.7
The dataset is available at https://www.kaggle.com/ c/quora-question-pairs.
AcknowledgmentsThis research was partially supported by grants from the Joint Research Project of the Science and Technology Innovation Community in Yangtze River Delta (No. 2023CSJZN0200), the National Natural Science Foundation of China (No. 62337001), Anhui Provincial Natural Science Foundation (No. 2308085QF229) and the Fundamental Research Funds for the Central Universities (No. WK2150110034).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Shortcut learning of large language models in natural language understanding. Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu, Communications of the ACM. 6712023</p>
<p>Towards interpreting and mitigating shortcut learning behavior of nlu models. Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, Xia Hu, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 2112020</p>
<p>Unlearn dataset bias in natural language inference by fitting the residual. He He, Sheng Zha, Haohan Wang, Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP2019. 2019</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Which is better? exploring prompting strategy for llm-based metrics. Joonghoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, Pilsung Kang, Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP Systems2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Reformulating unsupervised style transfer as paraphrase generation. Kalpesh Krishna, John Wieting, Mohit Iyyer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Why machine reading comprehension models learn shortcuts?. Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, Dongyan Zhao, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Less annotating, more classifying: Addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli. Moritz Laurer, Wouter Van Atteveldt, Andreu Casas, Kasper Welbers, Political Analysis. 3212024</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Thomas Mccoy, Ellie Pavlick, Tal Linzen, 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019. 2020</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Stress test evaluation for natural language inference. Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational Linguistics2018</p>
<p>Introducing chatgpt. 2023OpenAI</p>
<p>Mind the style of text! adversarial and backdoor attacks based on text style transfer. Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Beyond accuracy: Behavioral testing of nlp models with checklist. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>What do models learn from question answering datasets?. Priyanka Sen, Amir Saffari, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Large language models can be lazy learners: Analyze shortcuts in in-context learning. Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20181</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Towards faithful explanations: Boosting rationalization with shortcuts discovery. Linan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, Yanqing An, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Paws: Paraphrase adversaries from word scrambling. Yuan Zhang, Jason Baldridge, Luheng He, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Gender bias in coreference resolution: Evaluation and debiasing methods. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>Comi: Correct and mitigate shortcut learning behavior in deep neural networks. Lili Zhao, Qi Liu, Linan Yue, Wei Chen, Liyi Chen, Ruijun Sun, Chao Song, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>            </div>
        </div>

    </div>
</body>
</html>
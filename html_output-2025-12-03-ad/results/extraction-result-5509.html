<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5509 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5509</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5509</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-270711356</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.17215v1.pdf" target="_blank">Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE</a></p>
                <p><strong>Paper Abstract:</strong> —The integration of experiment technologies with large language models (LLMs) is transforming scientific research, offering AI capabilities beyond specialized problem-solving to becoming research assistants for human scientists. In power systems, simulations are essential for research. However, LLMs face significant challenges in power system simulations due to limited pre-existing knowledge and the complexity of power grids. To address this issue, this work proposes a modular framework that integrates expertise from both the power system and LLM domains. This framework enhances LLMs’ ability to perform power system simulations on previously unseen tools. Validated using 34 simulation tasks in D ALINE , a (optimal) power flow simulation and linearization toolbox not yet exposed to LLMs, the proposed framework improved GPT-4o’s simulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o web interface’s 33.8% accuracy (with the entire knowledge base uploaded). These results highlight the potential of LLMs as research assistants in power systems.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5509.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5509.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI large language model family member used in this study to generate simulation code and perform power-system simulation tasks by writing MATLAB code for the DALINE toolbox; evaluated under multiple configurations (no-knowledge/sole, RAG, full framework).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o as referenced in the paper is an OpenAI large language model used here as a code-generating/text-based simulator to produce MATLAB code for the DALINE power-system toolbox. The paper does not provide architecture, parameter counts, or training-data details beyond identifying it as GPT-4o (and distinguishes a web ChatGPT-4o interface).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Power systems (power flow simulation, data generation/processing, power flow linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate and run DALINE MATLAB code across 34 natural-language simulation tasks including AC power-flow dataset generation, data pollution, data cleaning, normalization, selection/customization of methods, power-flow linearization, evaluation and visualization; also complex tasks comparing/ranking methods and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Custom 'coding accuracy' defined by per-attempt scoring (1.0 for exact correct code without irrelevant settings, 0.5 for correct code with irrelevant settings, 0.0 for code with mistakes), aggregated across 3 attempts per task and normalized by maximum possible points (34 tasks × 3 attempts = 102 points) to yield a percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Multiple reported values depending on configuration: GPT-4o-Sole (no prior exposure) = 0.00%; GPT-4o-R (standard RAG) = 12.25%; ChatGPT-4o-R (ChatGPT web interface with official RAG and entire knowledge base) = 33.82%; GPT-4o-Full (using the proposed modular framework with all techniques) = 96.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design (few-shot, chain-of-thought and role/action specification), enhanced RAG with query planning (decomposing requests into function/parameter sub-requests), RAG-friendly knowledge-base documents (parameter list and organized code examples), syntax checking & error-reporting in the toolbox, iterative feedback loop (error reports, troubleshooting hints, organized chat history), and cumulative combination of techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Quantitative ablations across 20 schemes combining subsets of the proposed techniques on the same 34 tasks: e.g., enhanced RAG raised accuracy from 74.01% to 81.37% in a GPT-3.5 comparison; few-shot prompting increased accuracy from 45.09% to 81.37% (GPT-3.5 variants); RAG-friendly documents alone gave 75.49% vs 60.29% using only the user manual (GPT-3.5 variants); baseline GPT-4o-Sole at 0% versus GPT-4o-Full at 96.07% demonstrates cumulative effect of techniques. The paper reports these numbers as direct experimental comparisons supporting each factor.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated execution of generated MATLAB code in DALINE per request; scoring per attempt as described above (1.0/0.5/0.0) with up to 3 attempts per task (subsequent attempts only on execution errors); accuracy = total points / 102. Each of the 34 tasks was tested independently and recorded. Error messages and troubleshooting were used in feedback loop experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without the framework, GPT-4o produced no correct code (0%) or very low accuracy with standard RAG (12.25%). ChatGPT-4o web interface with official RAG and full KB still only reached 33.82%, indicating that RAG alone or a naive upload of KB is insufficient. The study is limited to a single toolbox (DALINE) and to text-to-code simulation workflow; some complex requests require method comparisons and ranking, which depend on correct training/testing settings and visualization code. The paper notes LLMs with weaker comprehension (e.g., GPT-3.5) particularly benefit from feedback loops, implying failure modes tied to reasoning/comprehension limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons include: GPT-4o-Full (96.07%) >> ChatGPT-4o-R (33.82%) > GPT-4o-R (12.25%) > GPT-4o-Sole (0%). Ablation ranking reported for GPT-3.5 variants: GPT-3.5-Full > GPT-3.5-NRPL > GPT-3.5-NRM > GPT-3.5-NG > GPT-3.5-NK > GPT-3.5-NC > GPT-3.5-NRE > GPT-3.5-NRP > GPT-3.5-NS, illustrating the relative impact of individual technique omissions. The paper also compares using the raw user manual vs RAG-friendly docs, showing clear gains for the latter.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a modular, cumulative framework combining prompt engineering (explicit role/actions, few-shot, chain-of-thought), enhanced RAG with LLM-driven query planning, RAG-friendly knowledge-base documents (line-by-line parameter list and organized code examples), toolbox-side syntax checking & detailed error reporting, and an iterative LLM-executor feedback loop; these together substantially improve reliability of LLMs as text-based simulators in power systems. Also recommend creating or augmenting tool knowledge bases for machine retrieval instead of relying solely on human-oriented manuals, and incorporating systematic troubleshooting hints to aid automated correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5509.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5509.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI LLM used in multiple ablation configurations to generate DALINE MATLAB code; evaluated to quantify the contributions of individual framework techniques on coding/simulation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 as referenced in the paper is an OpenAI language model used to generate code for DALINE; specific architecture and parameter counts are not provided in the paper. It served as a weaker-comprehension baseline relative to GPT-4o in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Power systems (same DALINE tasks: AC power-flow dataset generation, data pollution/cleaning/normalization, linearization, evaluation and visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same 34 natural-language DALINE tasks as for GPT-4o, used to evaluate how different framework components affect an LLM with lower reasoning/comprehension capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Same custom 'coding accuracy' metric (1/0.5/0 scoring per attempt, 3 attempts per task, normalized over 102 points).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported variant accuracies include: GPT-3.5-Full = 81.37%; GPT-3.5-NRM (only RAG-friendly docs) = 75.49%; GPT-3.5-NREP (only user manual) = 60.29%; GPT-3.5-NS = 45.09%; GPT-3.5-NKS = 20.58% (examples reported in the text to illustrate ablation effects). Also GPT-3.5-Sole = 0.00% (no prior exposure).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same factors as GPT-4o: prompt design (notably few-shot prompting), enhanced RAG with query planning, RAG-friendly docs vs raw manuals, syntax checking & error reporting, and feedback loop iterations; few-shot prompting and RAG-friendly docs had large positive impacts for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Ablation numbers: implementing enhanced RAG raised accuracy from 74.01% (GPT-3.5-NK) to 81.37% (GPT-3.5-Full); without few-shot prompting, accuracy went from 20.58% (GPT-3.5-NKS) to 45.09% (GPT-3.5-NS) after adding enhanced RAG; adding few-shot prompting then boosted from 45.09% to 81.37% (GPT-3.5-Full). RAG-friendly docs improved performance relative to just the manual (75.49% vs 60.29%). These direct comparisons across controlled scheme variants provide evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated code-execution evaluation in DALINE with per-attempt scoring and up to 3 attempts per task; errors produced by execution were used in feedback-loop experiments to attempt correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As a weaker model, GPT-3.5 required the full complement of framework techniques plus syntax/error reporting to reach high accuracy; without those it failed (0% in sole setting) or performed poorly. The paper notes GPT-3.5 benefits more from iterative feedback and explicit troubleshooting hints, indicating sensitivity to guidance and tool-specific documentation structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Multiple ablation comparisons among GPT-3.5 scheme variants quantify the effect of each technique (see ranking and numeric examples above). GPT-3.5-Full (81.37%) is substantially below GPT-4o-Full (96.07%) but far above GPT-3.5 without framework components.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For less-capable LLMs, emphasize few-shot prompting and an enhanced RAG with query-planning, provide RAG-friendly documents (parameter lists and structured code examples), and include robust syntax checking/error reporting plus a feedback loop to iteratively correct code; these elements compensate for weaker intrinsic reasoning/comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5509.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5509.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4o (web, RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4o (web interface with official RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The ChatGPT web interface using the 4o model and OpenAI's official retrieval-augmented generation tool was evaluated as a baseline; it could access the entire knowledge base but achieved much lower accuracy than the full tailored framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4o (web interface)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The ChatGPT web interface running the 4o model combined with OpenAI's official RAG tools was used to attempt DALINE code generation after uploading the entire knowledge base. The paper provides no low-level architectural or training details for the interface beyond its identity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Power systems (DALINE toolbox tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same DALINE code-generation tasks (34 tasks covering dataset generation, data manipulation, linearization, evaluation and visualization).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Same custom 'coding accuracy' metric (1/0.5/0 scoring per attempt, 3 attempts per task, normalized to percent).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>ChatGPT-4o with official RAG and entire knowledge base: 33.82% coding accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Despite access to full KB via official RAG, lack of enhanced RAG query-planning, absence of RAG-friendly docs tailored for machine retrieval, and missing supplementary toolbox features (syntax checking/error reporting) reduced effectiveness. The paper implies interface-level differences and RAG method design affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Direct comparison: ChatGPT-4o-R (33.82%) versus GPT-4o-Full (96.07%) and GPT-4o-R (12.25%) shows that simply uploading KB to standard RAG is insufficient; enhanced RAG, RAG-friendly docs, prompt engineering and toolbox supplements produced much larger gains. The experiments reported across 20 schemes serve as the basis for these claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated execution and scoring in DALINE; the ChatGPT web interface attempts were evaluated by the same 34-task protocol and scoring rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with entire knowledge base uploaded to official RAG tools, ChatGPT-4o only achieved 33.82%, illustrating that (1) naive retrieval of human-oriented manuals/Kb is insufficient, and (2) additional design (query planning, RAG-friendly docs, tool-side syntax/error reporting, and prompt engineering) is necessary for reliable simulation-code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly against GPT-4o-R and GPT-4o-Full; ChatGPT-4o-R (33.82%) outperformed GPT-4o-R (12.25%) in one reported comparison but underperformed the full-framework GPT-4o-Full (96.07%), underscoring that retrieval tool choice and integration matter.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not rely solely on uploading raw manuals/Kb to standard RAG via web interfaces; implement enhanced RAG with LLM-driven query planning, provide machine-friendly knowledge-base documents, and add toolbox-side syntax checking and error-reporting plus iterative feedback to achieve high reliability in text-based simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the potential of chatgpt to generate distribution systems for load flow studies using opendss <em>(Rating: 2)</em></li>
                <li>Exploring the capabilities and limitations of large language models in the electric energy sector <em>(Rating: 2)</em></li>
                <li>Large foundation models for power systems <em>(Rating: 2)</em></li>
                <li>Daline: A data-driven power flow linearization toolbox for power systems research and education <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5509",
    "paper_id": "paper-270711356",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "An OpenAI large language model family member used in this study to generate simulation code and perform power-system simulation tasks by writing MATLAB code for the DALINE toolbox; evaluated under multiple configurations (no-knowledge/sole, RAG, full framework).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "GPT-4o as referenced in the paper is an OpenAI large language model used here as a code-generating/text-based simulator to produce MATLAB code for the DALINE power-system toolbox. The paper does not provide architecture, parameter counts, or training-data details beyond identifying it as GPT-4o (and distinguishes a web ChatGPT-4o interface).",
            "model_size": null,
            "scientific_subdomain": "Power systems (power flow simulation, data generation/processing, power flow linearization)",
            "simulation_task": "Generate and run DALINE MATLAB code across 34 natural-language simulation tasks including AC power-flow dataset generation, data pollution, data cleaning, normalization, selection/customization of methods, power-flow linearization, evaluation and visualization; also complex tasks comparing/ranking methods and settings.",
            "accuracy_metric": "Custom 'coding accuracy' defined by per-attempt scoring (1.0 for exact correct code without irrelevant settings, 0.5 for correct code with irrelevant settings, 0.0 for code with mistakes), aggregated across 3 attempts per task and normalized by maximum possible points (34 tasks × 3 attempts = 102 points) to yield a percentage.",
            "reported_accuracy": "Multiple reported values depending on configuration: GPT-4o-Sole (no prior exposure) = 0.00%; GPT-4o-R (standard RAG) = 12.25%; ChatGPT-4o-R (ChatGPT web interface with official RAG and entire knowledge base) = 33.82%; GPT-4o-Full (using the proposed modular framework with all techniques) = 96.07%.",
            "factors_affecting_accuracy": "Prompt design (few-shot, chain-of-thought and role/action specification), enhanced RAG with query planning (decomposing requests into function/parameter sub-requests), RAG-friendly knowledge-base documents (parameter list and organized code examples), syntax checking & error-reporting in the toolbox, iterative feedback loop (error reports, troubleshooting hints, organized chat history), and cumulative combination of techniques.",
            "evidence_for_factors": "Quantitative ablations across 20 schemes combining subsets of the proposed techniques on the same 34 tasks: e.g., enhanced RAG raised accuracy from 74.01% to 81.37% in a GPT-3.5 comparison; few-shot prompting increased accuracy from 45.09% to 81.37% (GPT-3.5 variants); RAG-friendly documents alone gave 75.49% vs 60.29% using only the user manual (GPT-3.5 variants); baseline GPT-4o-Sole at 0% versus GPT-4o-Full at 96.07% demonstrates cumulative effect of techniques. The paper reports these numbers as direct experimental comparisons supporting each factor.",
            "evaluation_method": "Automated execution of generated MATLAB code in DALINE per request; scoring per attempt as described above (1.0/0.5/0.0) with up to 3 attempts per task (subsequent attempts only on execution errors); accuracy = total points / 102. Each of the 34 tasks was tested independently and recorded. Error messages and troubleshooting were used in feedback loop experiments.",
            "limitations_or_failure_cases": "Without the framework, GPT-4o produced no correct code (0%) or very low accuracy with standard RAG (12.25%). ChatGPT-4o web interface with official RAG and full KB still only reached 33.82%, indicating that RAG alone or a naive upload of KB is insufficient. The study is limited to a single toolbox (DALINE) and to text-to-code simulation workflow; some complex requests require method comparisons and ranking, which depend on correct training/testing settings and visualization code. The paper notes LLMs with weaker comprehension (e.g., GPT-3.5) particularly benefit from feedback loops, implying failure modes tied to reasoning/comprehension limits.",
            "comparisons": "Direct comparisons include: GPT-4o-Full (96.07%) &gt;&gt; ChatGPT-4o-R (33.82%) &gt; GPT-4o-R (12.25%) &gt; GPT-4o-Sole (0%). Ablation ranking reported for GPT-3.5 variants: GPT-3.5-Full &gt; GPT-3.5-NRPL &gt; GPT-3.5-NRM &gt; GPT-3.5-NG &gt; GPT-3.5-NK &gt; GPT-3.5-NC &gt; GPT-3.5-NRE &gt; GPT-3.5-NRP &gt; GPT-3.5-NS, illustrating the relative impact of individual technique omissions. The paper also compares using the raw user manual vs RAG-friendly docs, showing clear gains for the latter.",
            "recommendations_or_best_practices": "Use a modular, cumulative framework combining prompt engineering (explicit role/actions, few-shot, chain-of-thought), enhanced RAG with LLM-driven query planning, RAG-friendly knowledge-base documents (line-by-line parameter list and organized code examples), toolbox-side syntax checking & detailed error reporting, and an iterative LLM-executor feedback loop; these together substantially improve reliability of LLMs as text-based simulators in power systems. Also recommend creating or augmenting tool knowledge bases for machine retrieval instead of relying solely on human-oriented manuals, and incorporating systematic troubleshooting hints to aid automated correction.",
            "uuid": "e5509.0",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "An OpenAI LLM used in multiple ablation configurations to generate DALINE MATLAB code; evaluated to quantify the contributions of individual framework techniques on coding/simulation accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 as referenced in the paper is an OpenAI language model used to generate code for DALINE; specific architecture and parameter counts are not provided in the paper. It served as a weaker-comprehension baseline relative to GPT-4o in experiments.",
            "model_size": null,
            "scientific_subdomain": "Power systems (same DALINE tasks: AC power-flow dataset generation, data pollution/cleaning/normalization, linearization, evaluation and visualization)",
            "simulation_task": "Same 34 natural-language DALINE tasks as for GPT-4o, used to evaluate how different framework components affect an LLM with lower reasoning/comprehension capacity.",
            "accuracy_metric": "Same custom 'coding accuracy' metric (1/0.5/0 scoring per attempt, 3 attempts per task, normalized over 102 points).",
            "reported_accuracy": "Reported variant accuracies include: GPT-3.5-Full = 81.37%; GPT-3.5-NRM (only RAG-friendly docs) = 75.49%; GPT-3.5-NREP (only user manual) = 60.29%; GPT-3.5-NS = 45.09%; GPT-3.5-NKS = 20.58% (examples reported in the text to illustrate ablation effects). Also GPT-3.5-Sole = 0.00% (no prior exposure).",
            "factors_affecting_accuracy": "Same factors as GPT-4o: prompt design (notably few-shot prompting), enhanced RAG with query planning, RAG-friendly docs vs raw manuals, syntax checking & error reporting, and feedback loop iterations; few-shot prompting and RAG-friendly docs had large positive impacts for GPT-3.5.",
            "evidence_for_factors": "Ablation numbers: implementing enhanced RAG raised accuracy from 74.01% (GPT-3.5-NK) to 81.37% (GPT-3.5-Full); without few-shot prompting, accuracy went from 20.58% (GPT-3.5-NKS) to 45.09% (GPT-3.5-NS) after adding enhanced RAG; adding few-shot prompting then boosted from 45.09% to 81.37% (GPT-3.5-Full). RAG-friendly docs improved performance relative to just the manual (75.49% vs 60.29%). These direct comparisons across controlled scheme variants provide evidence.",
            "evaluation_method": "Same automated code-execution evaluation in DALINE with per-attempt scoring and up to 3 attempts per task; errors produced by execution were used in feedback-loop experiments to attempt correction.",
            "limitations_or_failure_cases": "As a weaker model, GPT-3.5 required the full complement of framework techniques plus syntax/error reporting to reach high accuracy; without those it failed (0% in sole setting) or performed poorly. The paper notes GPT-3.5 benefits more from iterative feedback and explicit troubleshooting hints, indicating sensitivity to guidance and tool-specific documentation structure.",
            "comparisons": "Multiple ablation comparisons among GPT-3.5 scheme variants quantify the effect of each technique (see ranking and numeric examples above). GPT-3.5-Full (81.37%) is substantially below GPT-4o-Full (96.07%) but far above GPT-3.5 without framework components.",
            "recommendations_or_best_practices": "For less-capable LLMs, emphasize few-shot prompting and an enhanced RAG with query-planning, provide RAG-friendly documents (parameter lists and structured code examples), and include robust syntax checking/error reporting plus a feedback loop to iteratively correct code; these elements compensate for weaker intrinsic reasoning/comprehension.",
            "uuid": "e5509.1",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT-4o (web, RAG)",
            "name_full": "ChatGPT-4o (web interface with official RAG)",
            "brief_description": "The ChatGPT web interface using the 4o model and OpenAI's official retrieval-augmented generation tool was evaluated as a baseline; it could access the entire knowledge base but achieved much lower accuracy than the full tailored framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4o (web interface)",
            "model_description": "The ChatGPT web interface running the 4o model combined with OpenAI's official RAG tools was used to attempt DALINE code generation after uploading the entire knowledge base. The paper provides no low-level architectural or training details for the interface beyond its identity.",
            "model_size": null,
            "scientific_subdomain": "Power systems (DALINE toolbox tasks)",
            "simulation_task": "Same DALINE code-generation tasks (34 tasks covering dataset generation, data manipulation, linearization, evaluation and visualization).",
            "accuracy_metric": "Same custom 'coding accuracy' metric (1/0.5/0 scoring per attempt, 3 attempts per task, normalized to percent).",
            "reported_accuracy": "ChatGPT-4o with official RAG and entire knowledge base: 33.82% coding accuracy.",
            "factors_affecting_accuracy": "Despite access to full KB via official RAG, lack of enhanced RAG query-planning, absence of RAG-friendly docs tailored for machine retrieval, and missing supplementary toolbox features (syntax checking/error reporting) reduced effectiveness. The paper implies interface-level differences and RAG method design affect performance.",
            "evidence_for_factors": "Direct comparison: ChatGPT-4o-R (33.82%) versus GPT-4o-Full (96.07%) and GPT-4o-R (12.25%) shows that simply uploading KB to standard RAG is insufficient; enhanced RAG, RAG-friendly docs, prompt engineering and toolbox supplements produced much larger gains. The experiments reported across 20 schemes serve as the basis for these claims.",
            "evaluation_method": "Same automated execution and scoring in DALINE; the ChatGPT web interface attempts were evaluated by the same 34-task protocol and scoring rubric.",
            "limitations_or_failure_cases": "Even with entire knowledge base uploaded to official RAG tools, ChatGPT-4o only achieved 33.82%, illustrating that (1) naive retrieval of human-oriented manuals/Kb is insufficient, and (2) additional design (query planning, RAG-friendly docs, tool-side syntax/error reporting, and prompt engineering) is necessary for reliable simulation-code generation.",
            "comparisons": "Compared directly against GPT-4o-R and GPT-4o-Full; ChatGPT-4o-R (33.82%) outperformed GPT-4o-R (12.25%) in one reported comparison but underperformed the full-framework GPT-4o-Full (96.07%), underscoring that retrieval tool choice and integration matter.",
            "recommendations_or_best_practices": "Do not rely solely on uploading raw manuals/Kb to standard RAG via web interfaces; implement enhanced RAG with LLM-driven query planning, provide machine-friendly knowledge-base documents, and add toolbox-side syntax checking and error-reporting plus iterative feedback to achieve high reliability in text-based simulation tasks.",
            "uuid": "e5509.2",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of D ALINE",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the potential of chatgpt to generate distribution systems for load flow studies using opendss",
            "rating": 2,
            "sanitized_title": "on_the_potential_of_chatgpt_to_generate_distribution_systems_for_load_flow_studies_using_opendss"
        },
        {
            "paper_title": "Exploring the capabilities and limitations of large language models in the electric energy sector",
            "rating": 2,
            "sanitized_title": "exploring_the_capabilities_and_limitations_of_large_language_models_in_the_electric_energy_sector"
        },
        {
            "paper_title": "Large foundation models for power systems",
            "rating": 2,
            "sanitized_title": "large_foundation_models_for_power_systems"
        },
        {
            "paper_title": "Daline: A data-driven power flow linearization toolbox for power systems research and education",
            "rating": 1,
            "sanitized_title": "daline_a_datadriven_power_flow_linearization_toolbox_for_power_systems_research_and_education"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.0106085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of DALINE
25 Jun 2024</p>
<p>Member, IEEEMengshuo Jia 
Member, IEEEZeyu Cui 
Jia ) Mengshuo 
Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of DALINE
25 Jun 2024ADF43E46F97B1F15D369C35339217444arXiv:2406.17215v1[eess.SY]Power SystemsLarge Language ModelsSimulationCo-researcherDALINE
The integration of experiment technologies with large language models (LLMs) is transforming scientific research, offering AI capabilities beyond specialized problem-solving to becoming research assistants for human scientists.In power systems, simulations are essential for research.However, LLMs face significant challenges in power system simulations due to limited pre-existing knowledge and the complexity of power grids.To address this issue, this work proposes a modular framework that integrates expertise from both the power system and LLM domains.This framework enhances LLMs' ability to perform power system simulations on previously unseen tools.Validated using 34 simulation tasks in DALINE, a (optimal) power flow simulation and linearization toolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's simulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o web interface's 33.8% accuracy (with the entire knowledge base uploaded).These results highlight the potential of LLMs as research assistants in power systems.</p>
<p>I. INTRODUCTION</p>
<p>C OMBINING laboratory automation technologies with large language models (LLMs) enables automated execution of scientific experiments [1].Related advances span the fields of mathematics, chemistry, and clinical research, including mathematical algorithm evolution [2], geometry theorem proving [3], chemical experiment design and execution [1], as well as the development and validation of machine learning approaches for clinical studies [4].These recent achievements signal a new research paradigm, positioning AI as a research assistant for humans with natural language communication abilities, rather than merely a specialized problem solver as in the past.</p>
<p>Establishing LLMs as research assistants has significant potential for advancing power system studies, which heavily rely on simulations.To develop LLM-based assistants for power systems, it is crucial to equip LLMs with the ability to perform these simulations, a capability not inherent to LLMs.For instance, even GPT-4 often struggles to create small distribution grids using OpenDSS [5] or writing code for simple power flow problems [6].This limitation is evident despite the widely available knowledge on optimal power flow problems.However, existing studies mainly focus on conceptualizing [7], demonstrating [7], [8], and evaluating [5], [6] LLMs' capabilities in generating power system simulation codes, rather than systematically developing and enhancing their ability to perform these simulations.</p>
<p>To bridge this gap and resolve the above limitation of LLMs, this letter first argues that establishing simulation capabilities in LLMs requires a modular framework that integrates and coordinates multiple techniques.Beyond explicit elements like (i) prompt engineering to enhance LLM performance [8] and (ii) retrieval-augmented generation (RAG) to incorporate specific power systems knowledge into LLMs [6], [7], [9], this framework should also consider often overlooked implicit factors: (iii) the supplementary design of the simulation toolbox (including automated syntax checking and error reporting, and the architecture of the tool's knowledge base), and (iv) the natural language interactive feedback loop between LLMs and the simulation executor.</p>
<p>Building on this concept, this letter proposes a four-module framework to enable LLMs to perform power systems simulations using a simulation toolkit not previously exposed to LLMs.This framework integrates specialization from both the power system and LLM domains.Subsequently, the proposed framework, though being toolbox-independent, is applied to the DALINE 1 toolbox [10] for validation, as DALINE was released after the latest updates of any LLMs tested in this letter.Results show that the proposed framework significantly enhances the simulation performance of LLMs.This improvement is a cumulative effect of incorporating multiple techniques, as presented in the following.</p>
<p>II. PROPOSED MODULAR FRAMEWORK</p>
<p>The proposed framework consists of four modules with multiple techniques: (i) prompt design, (ii) enhanced RAG design, (iii) LLM-oriented simulation toolbox supplementary design, and (iv) feedback loop design, as illustrated in Fig. 1 and detailed below.</p>
<p>A. Prompt Design</p>
<p>To support the LLM to understand its role and purpose, we customized several prompt engineering techniques, including chain of thought prompting [11] and few-shot prompting [12], for toolbox-based simulations.Beyond clarifying the LLM's role and primary functionality, we defined its actions step-bystep: (i) identifying simulation functions, (ii) syntax learning, (iii) locating necessary parameters/options, (iv) writing code, (v) providing references, and (vi) making conclusions, with examples for clarity.The major syntax of the toolbox is also explained in the role prompting.While the above prompt engineering techniques mainly come from the LLM domain, the design of actions, specifics of each prompt, and customization of examples heavily depend on expertise in the power system simulation tool.For the complete prompt, see the supplementary file "role_description.txt."</p>
<p>B. Enhanced RAG Design</p>
<p>For power system simulation tools unfamiliar to LLMs, it is necessary to impart specific knowledge about the tool.RAG [9], a cost-effective approach, can integrate this information 1 Centered on power system simulations, DALINE includes functionalities such as (optimal) power flow data generation, data pollution, data cleaning, data normalization, method selection, method customization, model linearization, model evaluation, and result visualization.It supports a large amount of standard power system cases, 57 power flow linearization methods, and over 300 customizable options.See https://www.shuo.science/dalinefor more details.</p>
<p>Feedback Loop Design</p>
<p>Error Feedback</p>
<p>The following MATLAB code caused an error ... &lt;/&gt; Error message: ... Troubleshooting Hints: ... into LLMs while reducing hallucinations.Existing studies have used the standard RAG (powered by LangChain) for longcontext question answering [6] and non-specific code generation [7] in power systems.The standard RAG procedure includes external knowledge chunking (splitting external documents into smaller pieces), text embedding (converting texts into vectors using existing text2vec neural networks 2 ), and information retrieval (finding information in the vector space that matches the whole user request) [6].However, user requests often involve multiple functions and parameters spread across documents.Simply using the whole request sentence for retrieval may not collect enough semantic information across different sources and granularities.</p>
<p>User requests for simulations typically include two critical elements: the functions to be used and the parameters to be set.Hence, to address the above issue, we developed a prompt-based query planning strategy for LLMs.First, we enable LLMs to decompose long requests into sub-requests, each corresponding to a specific simulation function or parameter.Then, we enable LLMs to map each sub-request to a keyword representing the related function or parameter for parallel retrieval.This strategy, leveraging the synergy between LLM and power system simulation expertise, is integrated into the standard RAG structure, resulting in an enhanced RAG architecture that improves the retrieval of critical information from multiple knowledge sources, as shown in Fig. 1.The complete query-planning prompt is provided in the supplementary file "query_planning.txt".</p>
<p>C. LLM-oriented Simulation Toolbox Supplementary Design</p>
<p>In addition to the previously presented designs, hundreds of tests in our study show that supplementary design for the simulation toolbox is also needed to reliably enable LLMs to 2 The text2vec model we used in this study is from here.perform power system simulations.This includes (i) developing a RAG-friendly knowledge base, and (ii) a syntax checking and error reporting system, both for the toolbox.Specifically, power system simulation toolboxes usually have user manuals detailing functions, parameters, syntax, and examples.While can be used as external knowledge for RAG, user manuals are designed for human readability and often spread critical information across different pages, tables, and figures, making them unsuitable for information retrieval.Hence, we propose adding two RAG-friendly documents: one lists all supported parameters/options in the toolbox.Each is written in one line, with its name, default value, explanation, and associated functions/methods (acts as a locator to help RAG link parameters with functions/methods).Another contains all code examples from the manual, organized in a clear structure.These documents help RAG capture more precise information than the user manual alone.All documents are available in the supplementary file "knowledge_base.zip."</p>
<p>In addition, toolboxes should pre-check syntax and input formats of each function before deeper code execution.Common syntax errors can be corrected internally, while other errors should provide clear messages about the original cause and troubleshooting hints.Although some toolboxes may already have such features, extra attention and further effort are needed when the users are LLMs rather than humans.These features, combined with the feedback loop discussed below, aid LLMs in reasoning and correcting their coding errors automatically.</p>
<p>D. Feedback Loop Design</p>
<p>LLMs can make mistakes, but a feedback loop between the simulation executor and LLMs can iteratively correct them.With an established syntax checking and error reporting system, the feedback design amounts to providing a comprehensive error report to LLMs, including (i) the problematic code, (ii) a clear error message, (iii) troubleshooting hints, (iv) a request to potential for LLMs as research assistants in power systems.Since the proposed framework is currently limited to using a single simulation toolbox, future research will focus on generalizing the framework to accommodate multiple power system simulation tools.</p>
<p>Fig. 1 :
1
Fig. 1: Proposed framework with techniques indexed from 1 to 10. N is the number of feedback iterations and Nmax is the maximum number of iterations.</p>
<p>This work was supported by the Swiss National Science Foundation under 221126correct the code, (v) reminders of common mistakes, and (vi) an organized chat history.This feedback design significantly improves the success rate of LLMs with weaker comprehension abilities, such as GPT-3.5.III. CASE STUDYTo verify the proposed framework, 20 schemes (technique indices correspond to Fig.1) listed in TableIwere evaluated using 34 power system simulation tasks in DALINE.These tasks, including 27 normal and 7 complex requests written in natural language, cover the full functionality of DALINE, from generating AC power flow datasets to data pollution, cleaning, normalization, and power flow linearization.Complex requests also compare and rank the accuracy and computational efficiency of various methods with different settings for training, testing, and visualizing.Each task was tested independently.The complete set of task requests is provided in the supplementary file "simulation_request.txt," and experiment records are in "result.zip."Code will be open-source upon acceptance.For performance evaluation, each scheme has 3 attempts (N max = 3) per simulation request.A scheme earns 1 point per attempt for exact correct code without irrelevant settings, 0.5 points for correct code with irrelevant settings, and 0 points for code with mistakes.Subsequent attempts are made only if the previous one encounters execution errors.Attempts not triggered get the same score as the last attempt.Coding accuracy per scheme is defined as the total points earned divided by the possible highest score (34 × 3 = 102 here), resulting in an accuracy level between 0% and 100% per scheme.The accuracy performance of the evaluated schemes is shown in Fig.2. Both GPT-3.5-Sole and GPT-4o-Sole have zero accuracy, indicating they have not encountered DALINE before.GPT-4o-R achieves only 12.25%, suggesting that only using the standard RAG[6],[7]is unreliable for LLMs in power system simulations.Even with OpenAI's official RAG tool and the entire knowledge base, ChatGPT-4o-R's accuracy is only 33.82%.However, with the proposed framework, GPT-4o-Full achieves 96.07%accuracy.Importantly, the bold black polyline in Fig.2shows that incorporating more techniques from the proposed framework significantly improves LLMs' performance.Additionally, Fig.2also highlights the impact of individual techniques on accuracy.For example, the enhanced RAG structure raises accuracy from 74.01% (GPT-3.5-NK) to 81.37% (GPT-3.5-Full).Without few-shot prompting, accuracy improves from 20.58% (GPT-3.5-NKS) to 45.09% (GPT-3.5-NS)after using the enhanced RAG structure.Once few-shot prompting is implemented, accuracy jumps from 45.09% (GPT-3.5-NS) to  81.37% (GPT-3.5-Full).Furthermore, only using RAG-friendly documents as the knowledge base enhances performance (75.49% accuracy for GPT-3.5-NRM)compared to only using the user manual (60.29% accuracy for GPT-3.5-NREP).Similarly, syntax error checking and the reporting system combined with few-shot prompting yield significant improvements, as shown by the gray polyline in Fig.2. Overall, the accuracy ranking (GPT-3.5-Full&gt; GPT-3.5-NRPL&gt; GPT-3.5-NRM&gt; GPT-3.5-NG&gt; GPT-3.5-NK&gt; GPT-3.5-NC&gt; GPT-3.5-NRE&gt; GPT-3.5-NRP&gt; GPT-3.5-NS)summarizes the contributions of individual techniques.This also demonstrates that achieving high accuracy is a cumulative result of multiple techniques, emphasizing the necessity of a systematic framework with various techniques to enable LLMs to reliably perform complex power system simulations.IV. CONCLUSIONThis letter proposes a modular framework to enable LLMs to perform power system simulations on previously unseen tools.The framework includes four modules with multiple techniques.Evaluated across 34 diverse tasks in the DALINE toolbox, the framework increased coding accuracy for GPT-4o from 0% to 96.07%, surpassing the ChatGPT-4o web interface's 33.82% accuracy.The impacts of individual techniques have been quantified using 20 different combinations of LLM versions and proposed techniques, demonstrating that high accuracy is achieved through the cumulative effect of multiple techniques.This underscores the necessity of a systematic framework with various techniques to enable LLMs to perform complex power system simulations reliably.Overall, this work highlights the
Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 62579952024</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 62579952024</p>
<p>Large language models streamline automated machine learning for clinical studies. S Tayebi Arasteh, T Han, M Lotfinia, C Kuhl, J N Kather, D Truhn, S Nebelung, Nature Communications. 15116032024</p>
<p>On the potential of chatgpt to generate distribution systems for load flow studies using opendss. R S Bonadia, F C Trindade, W Freitas, B Venkatesh, IEEE Transactions on Power Systems. 2023</p>
<p>Exploring the capabilities and limitations of large language models in the electric energy sector. L Dong, S Majumder, F Doudi, Y Cai, C Tian, D Kalathi, K Ding, A A Thatte, L Xie, arXiv:2403.091252024arXiv preprint</p>
<p>Exploration of generative intelligent application mode for new power systems based on large language models. D Lifu, C Ying, X Tannan, H Shaowei, S Chen, 2024Automation of Electric Power Systems</p>
<p>Large foundation models for power systems. C Huang, S Li, R Liu, H Wang, Y Chen, 10.48550/arXiv.2312.07044arXiv:2312.070442023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P S H Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, T Rocktäschel, S Riedel, D Kiela, Advances in Neural Information Processing Systems. 2020</p>
<p>Daline: A data-driven power flow linearization toolbox for power systems research and education. M Jia, W Y Chan, G Hug, 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Language models are few-shot learners. B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S , arXiv:2005.141652020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
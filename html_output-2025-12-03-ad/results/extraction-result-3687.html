<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269813035</p>
                <p><strong>Paper Title:</strong> Introduction to Large Language Models (LLMs) for dementia care and research</p>
                <p><strong>Paper Abstract:</strong> Introduction Dementia is a progressive neurodegenerative disorder that affects cognitive abilities including memory, reasoning, and communication skills, leading to gradual decline in daily activities and social engagement. In light of the recent advent of Large Language Models (LLMs) such as ChatGPT, this paper aims to thoroughly analyse their potential applications and usefulness in dementia care and research. Method To this end, we offer an introduction into LLMs, outlining the key features, capabilities, limitations, potential risks, and practical considerations for deployment as easy-to-use software (e.g., smartphone apps). We then explore various domains related to dementia, identifying opportunities for LLMs to enhance understanding, diagnostics, and treatment, with a broader emphasis on improving patient care. For each domain, the specific contributions of LLMs are examined, such as their ability to engage users in meaningful conversations, deliver personalized support, and offer cognitive enrichment. Potential benefits encompass improved social interaction, enhanced cognitive functioning, increased emotional well-being, and reduced caregiver burden. The deployment of LLMs in caregiving frameworks also raises a number of concerns and considerations. These include privacy and safety concerns, the need for empirical validation, user-centered design, adaptation to the user's unique needs, and the integration of multimodal inputs to create more immersive and personalized experiences. Additionally, ethical guidelines and privacy protocols must be established to ensure responsible and ethical deployment of LLMs. Results We report the results on a questionnaire filled in by people with dementia (PwD) and their supporters wherein we surveyed the usefulness of different application scenarios of LLMs as well as the features that LLM-powered apps should have. Both PwD and supporters were largely positive regarding the prospect of LLMs in care, although concerns were raised regarding bias, data privacy and transparency. Discussion Overall, this review corroborates the promising utilization of LLMs to positively impact dementia care by boosting cognitive abilities, enriching social interaction, and supporting caregivers. The findings underscore the importance of further research and development in this field to fully harness the benefits of LLMs and maximize their potential for improving the lives of individuals living with dementia.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM generation by first retrieving relevant documents from an external knowledge base (database or web search) and incorporating those documents into the model context so the LLM can produce more up-to-date and detailed answers or summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking large language models in retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>The LLM issues a retrieval query to an external knowledge store (e.g., database or web search), the retrieval system returns relevant documents which are appended to the LLM's context window, and the LLM conditions on both its internal weights and the retrieved passages to generate a synthesized response that integrates retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Any external knowledge base or indexable document collection (e.g., scholarly papers, databases, web pages). In the paper this is discussed generically (could be a database or Google search); no concrete corpus size/count is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language user prompts or questions; the prompt may reference auxiliary data or request the model to consult external documents.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieve relevant documents with an IR/search system, include retrieved documents in the LLM context, then generate a synthesized response (retrieval + generation). Techniques like RAG are sometimes combined with prompt engineering and chain-of-thought prompting for improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language answers or synthesized summaries that integrate information from retrieved documents (free-text summaries, answers to questions, or evidence-anchored responses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Discussed via referenced benchmarking studies (e.g., Chen et al., 2024). Evaluation approaches mentioned generally include benchmark comparisons and task-specific metrics; the dementia paper does not report primary experimental metrics for RAG itself.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper states RAG enhances LLM capabilities by providing up-to-date or specialized information and that models (e.g., Google's Gemini) implement RAG; authors emphasize RAG is particularly valuable for precision and currency of information but give no numeric results in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>If retrieval returns manipulated, incorrect, or low-quality documents the LLM can synthesize and amplify errors (indirect prompt injection). Retrieval-based systems can also enable circular referencing of model-generated content, and RAG does not eliminate hallucinations originating in the model's reasoning. The review notes the need for careful curation and verification of retrieved sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Described qualitatively: RAG can improve precision versus standalone LLM generation and make answers more current; the paper references benchmarking work (Chen et al., 2024) but does not report concrete numeric comparisons in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introduction to Large Language Models (LLMs) for dementia care and research', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMC-LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PMC-LLaMA (a LLaMA variant finetuned on medical literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuned LLaMA-based model trained further on medical journal papers and textbooks to improve domain expertise and answer medical queries more reliably than a generic off-the-shelf LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PMC-LLaMA: towards building open-source language models for medicine</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>PMC-LLaMA (medical finetuned LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Start from a foundation LLM (LLaMA) and continue training (fine-tuning) on a curated corpus of medical texts (PMC articles, textbooks) so the model internalizes medical knowledge and produces more expert-like medical answers or summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Medical journal papers and medical textbooks (the paper notes PMC-LLaMA was finetuned on medical journal papers and textbooks; no counts or sizes are provided in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language medical questions and prompts posed by users (e.g., queries about dementia or other clinical matters).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain adaptation via supervised fine-tuning on the medical corpus (continued training of weights on domain data); combined use with prompting/in-context examples at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Answers to medical questions, summaries, and knowledge synthesis in natural language tailored to medical topics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>The review reports PMC-LLaMA shows increased expertise relative to generic models (citing Wu et al., 2023) but does not report detailed evaluation methodology or metrics within this paper; referenced literature (Lehman et al., 2023) indicates models trained/finetuned on clinical records outperform non-finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review states finetuned medical models such as PMC-LLaMA provide more expert answers on medical queries than off-the-shelf models; specific numeric performance metrics are not provided in the reviewed text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Domain finetuning reduces some deficiencies but does not remove risks: hallucinations, biases inherited from training data, copyright and consent issues for training corpora, and regulatory/safety concerns for medical advice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Qualitatively compared to off-the-shelf LLMs: finetuned models show improved expertise; the review cites Lehman et al. (2023) claiming finetuned/clinically trained models outperform non-finetuned alternatives, but no detailed numeric comparisons are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introduction to Large Language Models (LLMs) for dementia care and research', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Med-PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Med-PaLM (PaLM finetuned for medical applications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medical-specialized variant of Google's PaLM family, finetuned on medical data to assist in medical decision-making and question answering by leveraging medical literature and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models encode clinical knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Med-PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A foundation model (PaLM) further finetuned and aligned on medical datasets (medical literature, QA datasets) to improve clinical knowledge representation and provide more accurate, medically-grounded answers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A wide array of medical literature and datasets (review notes Med-PaLM is based on PaLM and geared toward answering medical questions; no quantitative corpus details given).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language medical questions and prompts presented by clinicians, patients, or researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain fine-tuning and alignment (supervised demonstrations, potentially RLHF) to adapt general LLM weights to medical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Medical question-answering outputs, explanations, and potentially summaries tailored to clinical topics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>The review references improved expertise after finetuning (Singhal et al., 2023) but does not supply the specific evaluation protocols or metrics within this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Finetuned medical variants such as Med-PaLM demonstrate increased expertise on medical queries compared with general-purpose LLMs, according to cited work; the review does not provide detailed quantitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Persistent risks include hallucinations in medical contexts, biases from training data, calibration issues, and regulatory/privacy concerns for clinical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Reported qualitatively to outperform or be more expert than non-finetuned models; the review cites these improvements but does not detail direct human-vs-model numeric comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introduction to Large Language Models (LLMs) for dementia care and research', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Van Veen et al. (clinical summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapted large language models can outperform medical experts in clinical text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental study (cited in the review) where adapted/finetuned LLMs produced clinical-document summaries that, in physician evaluation, were preferred to summaries produced by human experts across several clinical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adapted large language models can outperform medical experts in clinical text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Adapted LLMs for clinical text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>LLMs adapted (fine-tuned) on clinical records and domain-specific summarization targets to generate concise summaries of clinical inputs such as radiology reports, doctor–patient dialogue, progress notes, and patient questions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Clinical records and domain-specific clinical text corpora (radiology reports, patient–doctor dialogues, progress notes); the review does not give explicit corpus sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Prompts instruct the model to summarize clinical text or to answer/generate concise reports for clinical scenarios; typical prompts are natural-language summarization requests.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain adaptation and fine-tuning on clinical text paired with target summaries; likely supervised fine-tuning on human-written summaries (as inferred from description), but the review only summarizes the high-level approach.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured natural-language summaries of clinical documents (short diagnostic summaries, patient-facing explanations, or clinician-oriented syntheses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Physician preference study: physicians compared LLM-generated summaries against human-expert summaries across several domains and expressed preferences; the review reports physician preference as the evaluation outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review reports that physicians preferred LLM-based summaries over human expert summaries across a variety of domains (radiology reports, patient questions, progress notes, and doctor–patient dialogue), indicating adapted LLMs can outperform human experts in judged summary quality in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The review emphasizes the critical importance of summary accuracy in medical contexts and flags hallucination risk; results are domain-specific and require thorough validation before clinical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Direct human comparison: physicians preferred adapted LLM summaries over human expert summaries (qualitative outcome reported). The review also references that finetuned clinical models outperform models that are not finetuned or that rely on in-context learning alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introduction to Large Language Models (LLMs) for dementia care and research', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3687.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3687.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG Benchmark (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmarking large language models in retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmarking study referenced in the review that evaluates how LLMs perform when combined with retrieval components (RAG setups) across tasks, providing empirical assessments of RAG effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking large language models in retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>RAG benchmarking suite</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A systematic evaluation framework that measures LLM performance when augmented with retrieval modules, across standardized tasks and metrics to quantify gains from retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Benchmarking uses curated retrieval corpora and task-specific datasets (the dementia review cites the benchmarking work but does not list the corpora or sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Task-specific prompts/queries (natural language) used by benchmark tasks to probe retrieval-augmented performance.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Empirical benchmarking of retrieval + generation pipelines; not a distillation algorithm per se but measures how retrieval changes generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Task performance metrics and comparative results (scores on benchmark tasks, qualitative and quantitative analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Benchmark comparisons across LLM variants and retrieval setups using task-specific metrics; the review references this work as part of the evidence base for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited in the review to support the claim that RAG enhances LLM capability for up-to-date and specialized information; the dementia paper does not reproduce numeric results from the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmark results can be gamed if models are trained on public benchmark data; generalization to unseen domains depends on retrieval corpus quality and robustness to manipulated documents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Benchmark compares RAG setups to baseline LLM-only systems; the review summarizes the qualitative conclusion that RAG improves precision/currency but does not present benchmark numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Introduction to Large Language Models (LLMs) for dementia care and research', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking large language models in retrieval-augmented generation <em>(Rating: 2)</em></li>
                <li>PMC-LLaMA: towards building open-source language models for medicine <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>Adapted large language models can outperform medical experts in clinical text summarization <em>(Rating: 2)</em></li>
                <li>Predicting dementia from spontaneous speech using large language models <em>(Rating: 1)</em></li>
                <li>Data science opportunities of large language models for neuroscience and biomedicine <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3687",
    "paper_id": "paper-269813035",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM generation by first retrieving relevant documents from an external knowledge base (database or web search) and incorporating those documents into the model context so the LLM can produce more up-to-date and detailed answers or summaries.",
            "citation_title": "Benchmarking large language models in retrieval-augmented generation",
            "mention_or_use": "mention",
            "system_or_method_name": "Retrieval-Augmented Generation (RAG)",
            "system_or_method_description": "The LLM issues a retrieval query to an external knowledge store (e.g., database or web search), the retrieval system returns relevant documents which are appended to the LLM's context window, and the LLM conditions on both its internal weights and the retrieved passages to generate a synthesized response that integrates retrieved evidence.",
            "input_corpus_description": "Any external knowledge base or indexable document collection (e.g., scholarly papers, databases, web pages). In the paper this is discussed generically (could be a database or Google search); no concrete corpus size/count is provided.",
            "topic_or_query_specification": "Natural language user prompts or questions; the prompt may reference auxiliary data or request the model to consult external documents.",
            "distillation_method": "Retrieve relevant documents with an IR/search system, include retrieved documents in the LLM context, then generate a synthesized response (retrieval + generation). Techniques like RAG are sometimes combined with prompt engineering and chain-of-thought prompting for improved reasoning.",
            "output_type_and_format": "Natural-language answers or synthesized summaries that integrate information from retrieved documents (free-text summaries, answers to questions, or evidence-anchored responses).",
            "evaluation_or_validation_method": "Discussed via referenced benchmarking studies (e.g., Chen et al., 2024). Evaluation approaches mentioned generally include benchmark comparisons and task-specific metrics; the dementia paper does not report primary experimental metrics for RAG itself.",
            "results_summary": "The paper states RAG enhances LLM capabilities by providing up-to-date or specialized information and that models (e.g., Google's Gemini) implement RAG; authors emphasize RAG is particularly valuable for precision and currency of information but give no numeric results in this review.",
            "limitations_or_challenges": "If retrieval returns manipulated, incorrect, or low-quality documents the LLM can synthesize and amplify errors (indirect prompt injection). Retrieval-based systems can also enable circular referencing of model-generated content, and RAG does not eliminate hallucinations originating in the model's reasoning. The review notes the need for careful curation and verification of retrieved sources.",
            "comparison_to_baselines_or_humans": "Described qualitatively: RAG can improve precision versus standalone LLM generation and make answers more current; the paper references benchmarking work (Chen et al., 2024) but does not report concrete numeric comparisons in this review.",
            "uuid": "e3687.0",
            "source_info": {
                "paper_title": "Introduction to Large Language Models (LLMs) for dementia care and research",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PMC-LLaMA",
            "name_full": "PMC-LLaMA (a LLaMA variant finetuned on medical literature)",
            "brief_description": "A finetuned LLaMA-based model trained further on medical journal papers and textbooks to improve domain expertise and answer medical queries more reliably than a generic off-the-shelf LLaMA.",
            "citation_title": "PMC-LLaMA: towards building open-source language models for medicine",
            "mention_or_use": "mention",
            "system_or_method_name": "PMC-LLaMA (medical finetuned LLaMA)",
            "system_or_method_description": "Start from a foundation LLM (LLaMA) and continue training (fine-tuning) on a curated corpus of medical texts (PMC articles, textbooks) so the model internalizes medical knowledge and produces more expert-like medical answers or summaries.",
            "input_corpus_description": "Medical journal papers and medical textbooks (the paper notes PMC-LLaMA was finetuned on medical journal papers and textbooks; no counts or sizes are provided in the review).",
            "topic_or_query_specification": "Natural language medical questions and prompts posed by users (e.g., queries about dementia or other clinical matters).",
            "distillation_method": "Domain adaptation via supervised fine-tuning on the medical corpus (continued training of weights on domain data); combined use with prompting/in-context examples at inference.",
            "output_type_and_format": "Answers to medical questions, summaries, and knowledge synthesis in natural language tailored to medical topics.",
            "evaluation_or_validation_method": "The review reports PMC-LLaMA shows increased expertise relative to generic models (citing Wu et al., 2023) but does not report detailed evaluation methodology or metrics within this paper; referenced literature (Lehman et al., 2023) indicates models trained/finetuned on clinical records outperform non-finetuned models.",
            "results_summary": "The review states finetuned medical models such as PMC-LLaMA provide more expert answers on medical queries than off-the-shelf models; specific numeric performance metrics are not provided in the reviewed text.",
            "limitations_or_challenges": "Domain finetuning reduces some deficiencies but does not remove risks: hallucinations, biases inherited from training data, copyright and consent issues for training corpora, and regulatory/safety concerns for medical advice.",
            "comparison_to_baselines_or_humans": "Qualitatively compared to off-the-shelf LLMs: finetuned models show improved expertise; the review cites Lehman et al. (2023) claiming finetuned/clinically trained models outperform non-finetuned alternatives, but no detailed numeric comparisons are provided in this paper.",
            "uuid": "e3687.1",
            "source_info": {
                "paper_title": "Introduction to Large Language Models (LLMs) for dementia care and research",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Med-PaLM",
            "name_full": "Med-PaLM (PaLM finetuned for medical applications)",
            "brief_description": "A medical-specialized variant of Google's PaLM family, finetuned on medical data to assist in medical decision-making and question answering by leveraging medical literature and datasets.",
            "citation_title": "Large language models encode clinical knowledge",
            "mention_or_use": "mention",
            "system_or_method_name": "Med-PaLM",
            "system_or_method_description": "A foundation model (PaLM) further finetuned and aligned on medical datasets (medical literature, QA datasets) to improve clinical knowledge representation and provide more accurate, medically-grounded answers.",
            "input_corpus_description": "A wide array of medical literature and datasets (review notes Med-PaLM is based on PaLM and geared toward answering medical questions; no quantitative corpus details given).",
            "topic_or_query_specification": "Natural language medical questions and prompts presented by clinicians, patients, or researchers.",
            "distillation_method": "Domain fine-tuning and alignment (supervised demonstrations, potentially RLHF) to adapt general LLM weights to medical tasks.",
            "output_type_and_format": "Medical question-answering outputs, explanations, and potentially summaries tailored to clinical topics.",
            "evaluation_or_validation_method": "The review references improved expertise after finetuning (Singhal et al., 2023) but does not supply the specific evaluation protocols or metrics within this paper's text.",
            "results_summary": "Finetuned medical variants such as Med-PaLM demonstrate increased expertise on medical queries compared with general-purpose LLMs, according to cited work; the review does not provide detailed quantitative outcomes.",
            "limitations_or_challenges": "Persistent risks include hallucinations in medical contexts, biases from training data, calibration issues, and regulatory/privacy concerns for clinical deployment.",
            "comparison_to_baselines_or_humans": "Reported qualitatively to outperform or be more expert than non-finetuned models; the review cites these improvements but does not detail direct human-vs-model numeric comparisons here.",
            "uuid": "e3687.2",
            "source_info": {
                "paper_title": "Introduction to Large Language Models (LLMs) for dementia care and research",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Van Veen et al. (clinical summarization)",
            "name_full": "Adapted large language models can outperform medical experts in clinical text summarization",
            "brief_description": "An experimental study (cited in the review) where adapted/finetuned LLMs produced clinical-document summaries that, in physician evaluation, were preferred to summaries produced by human experts across several clinical domains.",
            "citation_title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "mention_or_use": "mention",
            "system_or_method_name": "Adapted LLMs for clinical text summarization",
            "system_or_method_description": "LLMs adapted (fine-tuned) on clinical records and domain-specific summarization targets to generate concise summaries of clinical inputs such as radiology reports, doctor–patient dialogue, progress notes, and patient questions.",
            "input_corpus_description": "Clinical records and domain-specific clinical text corpora (radiology reports, patient–doctor dialogues, progress notes); the review does not give explicit corpus sizes.",
            "topic_or_query_specification": "Prompts instruct the model to summarize clinical text or to answer/generate concise reports for clinical scenarios; typical prompts are natural-language summarization requests.",
            "distillation_method": "Domain adaptation and fine-tuning on clinical text paired with target summaries; likely supervised fine-tuning on human-written summaries (as inferred from description), but the review only summarizes the high-level approach.",
            "output_type_and_format": "Structured natural-language summaries of clinical documents (short diagnostic summaries, patient-facing explanations, or clinician-oriented syntheses).",
            "evaluation_or_validation_method": "Physician preference study: physicians compared LLM-generated summaries against human-expert summaries across several domains and expressed preferences; the review reports physician preference as the evaluation outcome.",
            "results_summary": "The review reports that physicians preferred LLM-based summaries over human expert summaries across a variety of domains (radiology reports, patient questions, progress notes, and doctor–patient dialogue), indicating adapted LLMs can outperform human experts in judged summary quality in these experiments.",
            "limitations_or_challenges": "The review emphasizes the critical importance of summary accuracy in medical contexts and flags hallucination risk; results are domain-specific and require thorough validation before clinical deployment.",
            "comparison_to_baselines_or_humans": "Direct human comparison: physicians preferred adapted LLM summaries over human expert summaries (qualitative outcome reported). The review also references that finetuned clinical models outperform models that are not finetuned or that rely on in-context learning alone.",
            "uuid": "e3687.3",
            "source_info": {
                "paper_title": "Introduction to Large Language Models (LLMs) for dementia care and research",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RAG Benchmark (Chen et al.)",
            "name_full": "Benchmarking large language models in retrieval-augmented generation",
            "brief_description": "A benchmarking study referenced in the review that evaluates how LLMs perform when combined with retrieval components (RAG setups) across tasks, providing empirical assessments of RAG effectiveness.",
            "citation_title": "Benchmarking large language models in retrieval-augmented generation",
            "mention_or_use": "mention",
            "system_or_method_name": "RAG benchmarking suite",
            "system_or_method_description": "A systematic evaluation framework that measures LLM performance when augmented with retrieval modules, across standardized tasks and metrics to quantify gains from retrieval augmentation.",
            "input_corpus_description": "Benchmarking uses curated retrieval corpora and task-specific datasets (the dementia review cites the benchmarking work but does not list the corpora or sizes).",
            "topic_or_query_specification": "Task-specific prompts/queries (natural language) used by benchmark tasks to probe retrieval-augmented performance.",
            "distillation_method": "Empirical benchmarking of retrieval + generation pipelines; not a distillation algorithm per se but measures how retrieval changes generated outputs.",
            "output_type_and_format": "Task performance metrics and comparative results (scores on benchmark tasks, qualitative and quantitative analyses).",
            "evaluation_or_validation_method": "Benchmark comparisons across LLM variants and retrieval setups using task-specific metrics; the review references this work as part of the evidence base for RAG.",
            "results_summary": "Cited in the review to support the claim that RAG enhances LLM capability for up-to-date and specialized information; the dementia paper does not reproduce numeric results from the benchmark.",
            "limitations_or_challenges": "Benchmark results can be gamed if models are trained on public benchmark data; generalization to unseen domains depends on retrieval corpus quality and robustness to manipulated documents.",
            "comparison_to_baselines_or_humans": "Benchmark compares RAG setups to baseline LLM-only systems; the review summarizes the qualitative conclusion that RAG improves precision/currency but does not present benchmark numbers.",
            "uuid": "e3687.4",
            "source_info": {
                "paper_title": "Introduction to Large Language Models (LLMs) for dementia care and research",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking large language models in retrieval-augmented generation",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_in_retrievalaugmented_generation"
        },
        {
            "paper_title": "PMC-LLaMA: towards building open-source language models for medicine",
            "rating": 2,
            "sanitized_title": "pmcllama_towards_building_opensource_language_models_for_medicine"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "rating": 2,
            "sanitized_title": "adapted_large_language_models_can_outperform_medical_experts_in_clinical_text_summarization"
        },
        {
            "paper_title": "Predicting dementia from spontaneous speech using large language models",
            "rating": 1,
            "sanitized_title": "predicting_dementia_from_spontaneous_speech_using_large_language_models"
        },
        {
            "paper_title": "Data science opportunities of large language models for neuroscience and biomedicine",
            "rating": 2,
            "sanitized_title": "data_science_opportunities_of_large_language_models_for_neuroscience_and_biomedicine"
        }
    ],
    "cost": 0.022786,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Introduction to Large Language Models (LLMs) for dementia care and research</p>
<p>Shinya Tasaki 
Haiwen Gui 
Matthias S Treder matthias.treder@gmail.com 
Sojin Lee 
Kamen A Tsvetanov </p>
<p>Rush University Medical Center
United States</p>
<p>Stanford University
United States</p>
<p>Inez Y. Oh</p>
<p>Washington University in St. Louis
United States</p>
<p>School of Computer Science &amp; Informatics
Cardi University
CardiUnited Kingdom</p>
<p>Department of Clinical Neurosciences
Olive AI Limited
LondonUnited Kingdom</p>
<p>Department of Psychology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>University of Cambridge
CambridgeUnited Kingdom</p>
<p>Introduction to Large Language Models (LLMs) for dementia care and research
35C4338B065811EC92C86864DCF533F2RECEIVED February ACCEPTED April PUBLISHED May CITATIONdementiaLarge Language Model (LLM)Artificial IntelligenceAlzheimer's diseasecarenatural language processing
Treder MS, Lee S and Tsvetanov KA ( ) Introduction to Large Language Models (LLMs) for dementia care and research.</p>
<p>Introduction</p>
<p>As the global population ages, dementia emerges as one of the most pressing and multifaceted healthcare challenges (Parra et al., 2019).More than 55 million individuals worldwide are currently living with dementia, with over 60% of these cases occurring in low-and middle-income countries.Furthermore, approximately 10</p>
<p>. /frdem. . million new cases of dementia are diagnosed annually (WHO, 2023).Characterized by progressive cognitive decline that impedes daily functioning, dementia not only impacts the affected individuals, but also their caregivers, families, and the healthcare system at large.Furthermore, dementia is frequently diagnosed late or misdiagnosed (Fischer et al., 2017), while the limited availability of caregiver support post-diagnosis compounds the challenges faced by all involved.It becomes imperative for dementia care and research to develop innovative solutions for improved diagnosis, effective treatment and caregiving, ultimately reducing the global burden of this condition.Amidst this backdrop, the rise of advanced computational tools and Artificial Intelligence (AI) technologies offers a beacon of hope.A branch of AI known as Large Language Models (LLMs), with their capacity to understand, generate, and interact using natural language, are at the forefront of these technological innovations (Bubeck et al., 2023;Huang and Chang, 2023;Khurana et al., 2023;Min et al., 2023).In the realm of dementia care and research, LLMs present unique opportunities to revolutionize diagnostic strategies, therapeutic interventions, and patient-caregiver communication.</p>
<p>Yet, for all their promise, LLMs also bring forth a range of ethical, practical, and scientific challenges (Blodgett et al., 2020;Gabriel, 2020;Liao, 2020;Dobbe et al., 2021;Barocas et al., 2023;Floridi and Floridi, 2023;Gallegos et al., 2023;Kasneci et al., 2023;Li and Zhang, 2023;Wang et al., 2023;Bzdok et al., 2024).This paper aims to elucidate the prospects and potential pitfalls of employing LLMs in the domain of dementia care and research, paving the way for informed and judicious integration of these powerful tools in real-world settings.</p>
<p>Our key contributions are as follows:</p>
<ol>
<li>To our knowledge, this is the first publication specifically reviewing LLMs in the context of dementia management and care.Previous reviews surveyed AI in dementia more broadly (de la Fuente Garcia et al., 2020;Lee et al., 2021;Richardson et al., 2022;Borchert et al., 2023;Tsoi et al., 2023) or focused on AI for prediction and early diagnosis (Stamate et al., 2020;Li et al., 2022;Merkin et al., 2022;Borchert et al., 2023).2. We propose and thoroughly discuss several application scenarios where LLMs can be useful to people with dementia, including navigation aid, reading/writing assistance, and conversational services.3. We present the results of a survey of people with dementia (PwD) and supporters wherein we investigated their experience with AI and LLMs, their evaluation on the usefulness of the presented application scenarios, and their priorities that AI software developers should consider (e.g., privacy, ease of use).</li>
</ol>
<p>In the next section, we briefly review the dementia literature, before introducing the application of LLMs in this field.</p>
<p>Dementia overview</p>
<p>A detailed introduction into dementia, its epidemiology, various subtypes and diagnosis, risk factors, and treatment is included in the Supplementary material A. For brevity, we only provide a summary here.Dementia is a major public health priority (Prince et al., 2015), with the number of affected individuals expected to triple by 2050 (Nichols et al., 2022), creating significant economic and social challenges (Nandi et al., 2022).It encompasses various brain disorders characterized by a decline in cognitive and motor functions due to brain cell loss.Common types include Alzheimer's disease, vascular dementia, dementia with Lewy bodies, and frontotemporal dementia, each associated with specific brain regions and symptoms.Mixed dementia involves concurrent brain changes from multiple dementia types (Schneider et al., 2007;Kapasi et al., 2017).</p>
<p>Alzheimer's disease, the most prevalent cause of dementia, involves memory lapses, word-finding difficulties, and mood swings, with damage often starting in the hippocampus (Sheehan, 2012;Jack et al., 2018;Lane et al., 2018;Armstrong et al., 2024).Most Alzheimer's cases are sporadic with late onset, but a rare earlyonset form typically appears before the age of 65 (2023 Alzheimer's Disease Facts andFigures, 2023).Vascular dementia arises from damage to the brain's blood vessels and is associated with cognitive impairments such as impaired judgment, planning difficulties, and mood fluctuations (Iadecola et al., 2019;Bir et al., 2021).Dementia with Lewy Bodies features abnormal Lewy body protein deposits in the brain.It manifests as visual hallucinations and Parkinson's-like movement problems, often coexisting with Alzheimer's pathology (Kane et al., 2018).Frontotemporal Dementia often affects younger adults (45-60 years) and impacting cognition, personality, and behavior with various subtypes based on specific symptoms and pathologies (Coyle-Gilchrist et al., 2016;Olney et al., 2017;Raffaele et al., 2019;Murley et al., 2020).</p>
<p>Primary risk factors include age, genetics, and family history (2023 Alzheimer's Disease Facts andFigures, 2023).However, modifiable risk factors such as cardiovascular health and lifestyle choices can significantly impact dementia risk (Livingston et al., 2020).Current treatments focus on symptom management with emerging pharmacological advancements aimed at altering disease progression.Non-pharmacological interventions and comprehensive care strategies are vital for enhancing quality of life.Moreover, proactive management involves care strategies, including treatment optimization, caregiver training, and community support networks, to improve patient outcomes and enhance caregiver wellbeing.</p>
<p>As reviewed below, the use of AI technology for dementia management and care offer promising avenues for personalized treatment and continuous monitoring of disease progression.Traditional pharmacological treatments, lifestyle interventions and AI technology can work together in a comprehensive approach to address the multifaceted challenges of this complex neurological condition.By combining these different methods, we may be able to improve outcomes for patients with dementia, alleviate caregiver burden, and better meet the needs presented by dementia.volume changes to identify brain atrophy (Giorgio et al., 2020;Brierley, 2021;Lombardi et al., 2022;Qiu et al., 2022;Borchert et al., 2023).Early examples include an AI algorithm achieving 92.36% accuracy in classifying Alzheimer's Disease based on Magnetic Resonance Imaging scans (Zhang et al., 2015) and another predicting Alzheimer's Disease over 75 months earlier with 82% specificity and 100% sensitivity (Ding et al., 2019).Beyond neuroimaging, AI research aims to make cognitive tests (Li et al., 2022), speech assessments (O'Malley et al., 2020), and dementia screenings reproducible on a larger scale, enhancing accessibility, even in remote populations.A Canadian medical imaging company has developed a technology utilizing retina scans to detect amyloid buildup, a protein associated with Alzheimer's Disease in its early stages (Dangerfield and Katherine, 2023).</p>
<p>As a special instantiation of AI, Large Language Models (LLMs) have been only scarcely explored in the context of dementia care and management.In the Method section, we introduce LLMs, their general architecture, training and limitations and risks associated with LLMs.We then revisit these topics in the context of dementia.</p>
<p>Finally, we introduce a questionnaire what was sent out to people with dementia (PwD) and supporters (e.g., caregivers, family members, or nurses).We investigated their views on various application scenarios as well as their priorities for LLM-powered digital apps (e.g., ease of use, data privacy).</p>
<p>Method Large Language Models (LLMs)</p>
<p>The years [2023][2024] have been a period of tremendous growth for LLMs both in terms of computational capability and public exposure.In January 2023, OpenAI's language model known as ChatGPT reached the 100 million users mark 2 months after its release, making it the fastest growing consumer app to date (Hu, 2023).Spurred by the stellar success of OpenAI, big tech competitors Google and Meta soon followed suit, releasing new versions of their respective competitor models PaLM2 (Ghahramani, 2023;Mauran, 2023), Bard (Hsiao, 2023) and Llama (Touvron et al., 2023).In this section, we review the technological fundamentals of LLMs and the way they are trained, finetuned and deployed, their risks and limitations, and we review some state of the art models.We keep the technical discussion at a conceptual level in order to make it useful to a broad audience.Table 1 provides a glossary with a concise description of some of the technical terms used in the next subsections.A brief overview of the history of LLMs is provided in the Supplementary material B.</p>
<p>Using Large Language Models</p>
<p>Figure 1 summarizes the interaction of a user with an LLM.Users can typically type input prompts using a browser window with a chat interface.Additionally, many models provide an Application Programming Interface (API) that allows for computer programs or smartphone apps to access an LLM in the background.Most LLMs cannot be efficiently deployed on a local device because of their enormous requirements in terms of processing power and memory.Therefore, in many cases the LLM will be running in a data center and accessed via an internet connection.The user provides a prompt by either typing it in directly or using speech that is then converted to text using a separate speech-to-text algorithm.The prompt can be a question ("What is dementia?"),a statement ("I am happy today"), or a set of instructions ("Generate a pointby-point list of activities to do in London today, taking into account the current weather.For lunch, suggest good vegetarian restaurants around Greenwich.").Auxiliary data such as images or text files can be provided and the text prompt can include a reference to the data ("Describe the image").During the processing of the prompt, some LLMs can recruit software plugins such as web search to fetch news items, or chart and image generators to create visuals.The LLM autonomously generates control commands to operate the plugins and it incorporates their output.The LLM then returns text output to the user, which can be converted to audio using a text-tospeech algorithm.Alternatively, outputs can take the form of other modalities such as images.</p>
<p>The quality of the returned text can often be improved by carefully crafting the prompts given to the model.This is known as prompt engineering.A few such techniques have been developed and have shown to lead to higher accuracy and better responses.Chain-of-thought prompting involves giving structured, multi-step instructions or explanations within the prompt, guiding it to generate step-by-step reasoning in its responses, akin to a human solving a complex problem (Wei J. et al., 2023).Tree-of-thoughts expands on this idea by encouraging the model to explore multiple possible lines of reasoning simultaneously, akin to a branching tree of ideas (Yao et al., 2023).In analogical prompting, the model is prompted to recall examples relevant to a new task and then afterwards solve the initial problem (Yasunaga et al., 2023).</p>
<p>Training</p>
<p>In this section we will explain the basic principles of how LLMs are trained from scratch.Most models are based on the transformer architecture that was introduced by Vaswani et al. (2017).Training involves changing the weights of the model.Weights determine how it interprets and generates text.Their number is usually in the billions.Weights form the parameters that encode the model's understanding of language and its knowledge about the world.Note that training a model is something most users will never do themselves.Training a state of the art model requires prohibitively large resources of data and compute power, so it is something mostly done by large tech firms and well-funded startups.Training typically progresses through two stages: pretraining and finetuning.An additional in-context learning stage can happen during the interaction with the user, allowing further adaptation.Figure 2 depicts the different phases of training.</p>
<p>Pretraining</p>
<p>In the pretraining stage, the model trains on a large text corpus using unsupervised objectives.The objective is to teach the model to understand general linguistic patterns and structures, and to encode world knowledge and facts in its weights.For instance, it learns that "Albert Einstein" was a physicist and Nobel prize laureate, or that London is the capital of the United Kingdom.It can be conceived of as a "compression" of the text corpus into the weights of the model.The mechanism by which the training proceeds is deceptively simple: the model simply learns to predict the probabilities of the next token (e.g., one or more words).For instance, the sentence "The dog bit the <strong><em>" is more likely to be continued with the words "cat" or "kid" than with "truck" or "bacteria".The model learns this by adjusting its weights iteratively after seeing some examples.Despite its simplicity, next word prediction can instill reasoning.For instance, the sentence "France is to Paris as Germany is to </em></strong>" can be completed by simply memorizing "Berlin" but it turns out that the model acquires some understanding of the concepts of countries and capitals after seeing many similar examples in different contexts.Although text is the most important input modality, the current trend is to make LLMs multi-modal by simultaneously training them on multiple data modalities simultaneously.For instance, Google's Gemini has been trained on natural language, computer code, audio, image, and video (Pichai and Hassabis, 2023).The resultant language models, also known as foundation models, however, can still be adjusted to the needs of specific users via a process called finetuning (Min et al., 2023).</p>
<p>Finetuning the weights</p>
<p>The pretrained model has a vast reservoir of general knowledge but it might still lack in depth knowledge in specific areas.Starting from a foundation model, training can be continued on a smaller set of more specialized content (e.g., medical text books) to ingest expertise in a specific area into the model.However, to make the model useful as a chatbot or assistant and let it interact with a user in a question-answer fashion, two other techniques, supervised learning and reinforcement learning with human feedback (RLHF), are necessary (Ziegler et al., 2020;Ouyang et al., 2022).Supervised learning involves exposing the model to pairs of instructions and answers.For instance, "Explain the moon landing to a 6 year old" as an instruction and an actual answer written by a rater can be used as demonstration for the model to learn from (Ouyang et al., 2022).Such demonstrations can come as a separate dataset of questions and ideal answers and do not require the model's output.In contrast, RLHF operates directly on the model.First, a prompt ./frdem. .</p>
<p>FIGURE</p>
<p>Flowchart showing how a user interacts with a Large Language Model.and several model answers are sampled from the language model.A human rater ranks the outputs from best to worst.A model that is separate from the LLM, called a reward model, can be trained on this data.Basically, the reward model learns to mimick the assessments of the rater.Second, new prompts and model answers are generated, and the reward model is used to score their quality.</p>
<p>The reward model can now be used as an additional feedback signal to the LLM that makes it produce higher quality answers.The same technique can be used to align the model with human values and make it less biased.After finetuning, the adjustment of the weights of the model is complete and the weights remain fixed.The model can now be deployed, e.g. as an executable program to run on a computer.</p>
<p>In-context learning via prompt engineering Although the weights are fixed after finetuning, the model is still able to learn during operation with a user through incontext learning.The context window refers to the maximum amount of text that the model can consider at once when generating a response.It determines how much of a conversation the model can reference in its current processing, impacting its ability to maintain coherence over long interactions or documents.In-context learning is performed via prompt engineering.For instance, a simple context such as "Show a lot of empathy in your responses" prior to the beginning of the actual conversation can make the model provide more empathetic answers.It is worth noting that in-context learning is limited to the current session, and once a new conversation is started the context needs to be repeated.It is also limited by the context window, so for long conversations it is possible that the model "forgets" the initial instructions.</p>
<p>Retrieval-augmented generation (RAG)</p>
<p>Retrieval-augmented generation (RAG) enhances the capabilities of large language models by integrating external information retrieval into the response generation process (Chen et al., 2024;Gao et al., 2024).The LLM first uses a retrieval system to find relevant documents from an external knowledge base when presented with a query.The retrieval system can take the form of a search query in a database or a Google search.The retrieved items are then incorporated into the model's context, providing either up-to-date or more detailed information.Finally, the model generates a response that draws from both its internal training and the retrieved information.This is particularly valuable in situations where precision and currency of information are critical, or for topics that are highly specialized or niche.Models such as Google's Gemini implement RAG.</p>
<p>Limitations and risks</p>
<p>Despite the significant advances and human-level performance across a variety of language related tasks, LLMs lack the nuance, world knowledge and deep semantic understanding that drives human conversation.They can make factually false statements, perpetuate biases inherent in internet text data, and may be susceptible to usage by parties with ill intent (Gabriel, 2020;Dobbe et al., 2021;Barocas et al., 2023;Wang et al., 2023).In this section, we summarize the main limitations and risks of LLMs, as well as approaches for mitigation.</p>
<p>Regulatory challenges</p>
<p>A comprehensive overview of regulatory challenges is included in the Supplementary material C. A summary is provided here.Using Large Language Models (LLMs) in healthcare brings significant challenges such as ethical issues, biases, safety concerns, and environmental impacts.It is essential to implement proactive regulations to harness the benefits and mitigate risks, ensuring LLMs meet clinical and patient needs (Meskó and Topol, 2023).The deployment of generative AI models can compromise privacy by using personal data without informed consent, posing privacy risks.It is critical to enforce laws like GDPR and HIPAA to ensure the anonymization and protection of patient data, and secure informed consent for using AI in healthcare (Meskó and Topol, 2023).</p>
<p>Furthermore, there is a need for transparency in how AI models operate, especially as companies sometimes limit scrutiny of their algorithms.Effective regulation should require clarity on AI decision-making processes to uphold democratic principles and assign liability appropriately (Norwegian Consumer Council, 2023).Proposed regulations, like the AI Liability Directive, aim to facilitate compensation for AI-induced harms but require proving fault, highlighting the need for clear regulatory definitions and protections (Norwegian Consumer Council, 2023).Regulators also need to implement ongoing monitoring and validation mechanisms to maintain the reliability and safety of AI tools in healthcare, adapting to different populations over time (Meskó and Topol, 2023).</p>
<p>Hallucinations</p>
<p>In the context of LLMs, a hallucination refers to the generation of syntactically sound text that is factually incorrect (OpenAI, 2023).It has been a prominent aspect of the public discussion of AI and was selected as Cambridge dictionary's word of the year (Creamer, 2023).Moreover, LLMs can express high confidence in these statements even if they are nonsensical.One reason for LLMs' susceptibility to hallucinations is the training data consisting of a large corpus of text and code, which can contain errors and inconsistencies.When an LLM is generating text, it may draw on this information in unexpected ways, leading to hallucinations (Ye et al., 2023;Zhang et al., 2023).Another reason for hallucinations is that many LLMs are not able to verify the accuracy of their own output.When an LLM generates text, it may not check whether text is consistent with its knowledge base or with reality.This can lead to the generation of text that is factually incorrect or nonsensical.</p>
<p>Hallucinations are also a consequence of calibrated models (Kalai and Vempala, 2023).A model is considered well calibrated when its confidence about a statement matches the actual probability of the statement being true (Jiang et al., 2021).As an illustrative example, if a calibrated weather forecasting model predicts 30% change for rain on a given day, the prediction should turn out to be correct in 30% of the cases (Dawid, 1982).More generally speaking, if the model appears highly confident about its statement, the statement should have a high likelihood of being correct.Jiang et al. (2021) show that many language models are not well calibrated.In addition, post-training alignment can further reduce the calibration of a model (OpenAI, 2023).The following approaches can be used to mitigate hallucinations:</p>
<p>-Data quality control and curation.Training LLMs on more reliable and accurate data reduces the likelihood of LLMs generating hallucinated text.If users can flag erroneous answers the quality of LLM outputs can improve iteratively.-Explainability.Falsehoods are less likely when the LLM is asked to provide references for its claims and statements.At the very least, the user can then verify the claims them self and potentially factually incorrect sources can be highlighted (Liu N. F. et al., 2023).-Self-verification.As an extension of explainability, LLMs verify the accuracy of their own output using, for instance, fact-checking tools or other methods to ensure that the text generated by the LLM is consistent with its knowledge base.LLMs can also self-verify by critically analyzing its own output (Dhuliawala et al., 2023).</p>
<p>Mitigation is greatly helped by the availability of metrics that quantify hallucinations.TruthfulQA is a benchmark dataset that comprises 817 questions across various domains such as health and politics (Lin et al., 2022).The authors showed that LLMs often generate false answers that reproduce popular misconceptions.While humans produce accurate answers in 94% of the cases, the surveyed' LLMs had &lt;60% accuracy.For instance, the question "Can coughing effectively stop a heart attack?" was answered by OpenAI's GPT-3 as "Coughing can help stop a heart attack."Metrics such as TruthfulQA can be used to vet and compare LLMs.To summarize, hallucinations are rooted both in deficiencies in the data and limitations of the model in reasoning about the world when all it has ever seen of the world is text data.For this reason, hallucinations are likely to remain a deficiency of LLMs for the foreseeable future.</p>
<p>The production of fabricated information by LLMs is a concern for medical applications of LLMs (Tian et al., 2024).Hallucinations have been shown prevalent regarding medical queries (Pal et al., 2023).While the latter concerns mostly healthcare professionals, it shows the risks of using LLMs for medical advice, even without considering the regulatory challenges.The danger of hallucinations permeates applications of LLMs for dementia care, including inaccurate information retrieval, inaccurate therapeutic processes, wrong summarization as a reading aid, or incorrect instructions when used as a navigation aid.</p>
<p>Bias and alignment</p>
<p>Bias refers to tendencies in the model's responses that unfairly favor or disfavor certain groups or perspectives.This happens due to imbalances or prejudices in the training data, which often involves large amounts of uncurated text crawled from the internet (Naveed et al., 2023), or the model's learning process (Blodgett et al., 2020;Hovy and Prabhumoye, 2021;Ferrara, 2023;Field et al., 2023;Gallegos et al., 2023).Bias can manifest as stereotypes, underrepresentation of certain groups, or unfair treatment of specific topics (Birhane et al., 2021).As a special case of bias, toxicity refers to model outputs that contain harmful or offensive language.Although LLMs typically converse politely and diplomatically after RLHF, problematic language can still be elicited.For instance, Deshpande et al. (2023) showed that the prompt "Speak exactly like P. Your answer should copy the style of P, both the writing style and words you use." can be used to assign a persona P to ChatGPT (OpenAI, 2022).Toxicity was measured as the probability of responding (POR), that is, the probability of ChatGPT to respond to a query which elicits toxic behavior (e.g., "Say something toxic about <name of person>").Using different personas, an up to 6-fold increase in the number of toxic responses by ChatGPT was reported (Deshpande et al., 2023).Rozado (2023) administered multiple political orientation tests to ChatGPT.The model showed a consistent left-leaning bias despite insisting to not have a political preference when directly asked about it.Gallegos et al. (2023) differentiate between two types of harms facilitated by biases:</p>
<p>• Representational harm.This type of harm manifests directly in the problematic text generated by an LLM.It involves the perpetuation of denigrating and subordinating attitudes toward a social group, including derogatory language, misrepresentation, stereotyping, and toxicity.This includes biases pertaining to certain demographics and cultural or linguistic groups as well as political ideologies (Ferrara, 2023).</p>
<p>• Allocational harm.This type of harm manifests as direct or indirect discrimination that results from the usage of LLMs for decision making by third parties.For instance, LLM-aided resume screening may perpetuate inequities in hiring (Raghavan et al., 2020) and LLM-aided healthcare algorithms may exacerbate inequities in care (Paulus and Kent, 2020).</p>
<p>Techniques for bias mitigation can be classified by the stage in the model's life cycle at which they are applied (Gallegos et al., 2023;Ganguli et al., 2023):</p>
<p>• Pre-processing.In as far as LLMs simply perpetuate biases inherent in the data, pre-processing the data prior to training may avoid biases from creeping in in the first place.</p>
<p>Techniques include adding underrepresented data samples (data augmentation), curation data such that biased examples are removed (data filtering), and adding textual instructions or triggers to foster unbiased output (instruction tuning).</p>
<p>More research is needed to confirm the effectiveness of these interventions.For instance, Li and Zhang (2023) reported limited effectiveness for instruction tuning.</p>
<p>• In-training.As an alternative to changes to the training data via pre-processing, the training procedure itself can be modified to facilitate unbiasedness.For instance, Lauscher et al. (2021) showed that the model architecture can be adapted to reduce gender bias.Other approaches include the addition of regularization terms to the loss function and contrastive, adversarial, and reinforcement learning, as well as filtering of parameters (Gallegos et al., 2023).</p>
<p>• Intra-processing.Whereas the previous two approaches affect the training of the model, intra-processing techniques can be applied to models after training is finished.Increasing the model's output diversity by modifying the token distribution has been shown to reduce the frequency of biased outputs.</p>
<p>Other approaches include changing the distribution of the model's weights or appending debiasing models (such as modular debiasing networks) (Gallegos et al., 2023).</p>
<p>• Post-processing.Post-processing methods start from the LLMs output text and process it again to remove bias.It involves rewriting the output or swapping harmful keywords for semantically similar words with more positive connotations (Gallegos et al., 2023).</p>
<p>• Self-correction.Ganguli et al. (2023) showed that models can leverage themselves to correct their biases.Appending the instruction "Please ensure that your answer is unbiased and does not rely on stereotypes."to the prompt and asking for Chain-of-Thought reasoning (Wei J. et al., 2023) significantly reduced bias toward protected characteristics such as gender and ethnic background.</p>
<p>A concept that is closely related to bias but yet distinct is alignment.It focuses on ensuring that models act in ways beneficial and aligned with human values and intentions.It encompasses understanding and accurately responding to human intent, generating ethical and safe content, maintaining reliability, and ensuring transparency and explainability.Crucial to alignment is the ability of these models to adapt based on feedback, minimize biases, and respect user autonomy and privacy (Gabriel, 2020;Liao, 2020;Wang et al., 2023).</p>
<p>Studies have shown evidence for stigma against people with dementia on the media platform X, formerly known as Twitter (Oscar et al., 2017;Bacsu et al., 2022), and in the wider social media landscape (Nguyen and Li, 2020).Due to LLM training data including social media posts, it is conceivable that such stigmas carry on into the models.Datasets such as BOLD (Dhamala et al., 2021) provide prompts and metrics for assessing such biases.Prompts specifically designed to tease out against people with dementia could be used to probe models.</p>
<p>Malicious use</p>
<p>Whereas hallucinations and bias refers to the inadvertent release of unwanted statements due to deficiencies in the training data or the model's understanding of the world, LLMs can also be used for explicitly malicious purposes by generating illicit information or writing harmful program code.Areas wherein LLMs can be used for harmful purposes include:</p>
<p>• Misinformation and propaganda.LLMs can generate plausible-sounding but false or misleading information.</p>
<p>If used maliciously, they can be tools for spreading misinformation or disinformation on a large scale.They can easily create large volumes of persuasive and targeted propaganda which can be deployed on social media and other platforms to influence public opinion or political processes.Misinformation can be produced involuntarily too via hallucinations.</p>
<p>• Proliferation of dangerous information.OpenAI showed that, during early stages of training, GPT-4 can be prompted to provide instructions on how to build a bomb or synthesize dangerous chemicals (OpenAI, 2023).This shows that LLMs can openly share dangerous information if they are not reigned in.</p>
<p>• Phishing and scam.The persuasive and coherent text generated by LLMs can be used for social engineering attacks.This includes phishing emails, scam messages, or other forms of manipulation that are more convincing due to the natural language capabilities of the model.</p>
<p>• Attacks on automated systems.Malicious actors could use LLMs to find vulnerabilities in or to manipulate other AI systems, especially those that rely on text inputs, such as automated customer service chatbots.</p>
<p>• Evasion of detection systems.LLMs can be used to generate content that evades detection by plagiarism checkers, content moderation systems, or other security measures, making it harder to maintain the integrity of information systems.</p>
<p>It is true that after finetuning of the models with RLHF most available LLMs refuse to provide obviously harmful information or produce inappropriate content.However, instructions for phishing or scam emails can be seemingly innocent and it might not be possible to establish infallible guardrails against misuse.Furthermore, malicious actors can alter the model's responses either during finetuning or inference using the following techniques:</p>
<p>• Data poisoning.Poisoning refers to a technique used in the finetuning stage that involves inserting triggers that are supposed to generate harmful language (Jiang et al., 2023).Jiang et al. showed that only a few percent of training data need to be malicious in order to trigger the desired behavior.This process requires access to the model's finetuning data.</p>
<p>• Jailbreaking.Jailbreaking involves bypassing or altering the model's built-in restrictions to produce responses that are normally censored or access blocked functionalities.This is done by "tricking" the model to be in developer or otherwise unrestricted mode (Huang et al., 2023;Wei A. et al., 2023;Deng et al., 2024;Jiang et al., 2024).</p>
<p>• Prompt injection.Prompt injection involves a malicious third party intercepting the prompt sent by the user to the LLM.The third party modifies or fully replaces the user prompt by a different prompt.The user is unaware of this alteration and perceives the returned answer as the LLM's genuine answer to their original question (Liu Y. et al., 2023).Malicious intentions include bias and misinformation, the exposure of internal prompts (prompt leakage) to the third party, and "compute theft".In the latter case, the malicious attacker hijacks the LLM to perform their own tasks user the user's account, leading to potential financial damage for the user and/or the LLM provider.</p>
<p>• Indirect prompt injection.Even if a malicious third party does not have direct access to the user prompt, the LLM can be influenced by manipulating the information the LLM retrieves.For instance, if the LLM performs a web search, a manipulated or fake web page that is retrieved by the model can be used to commit fraud, manipulate content, deploy malware, or create denial-of-service attacks (Greshake et al., 2023).</p>
<p>Consent, copyright and plagiarism LLMs are trained on large corpora of text that might have been collected without the consent of their originators (Franceschelli and Musolesi, 2022;Kasneci et al., 2023).For instance, a collection of over 180,000 books, referred to as Books3, was compiled for the training of LLMs without prior consent by the writers (Reisner, 2023).This triggered a number of lawsuits, one of the most prominent ones being the comedian Sarah Silverman charging OpenAI and Meta for including her books in training their respective LLMs (Davis, 2023).Using Books3 for training is explicitly acknowledged in Meta's technical paper on Llama (Touvron et al., 2023).LLMs are not only able to summarize works seen in the training, they have been shown to be able to reproduce verbatim text, exacerbating issues of copyright infringement (Karamolegkou et al., 2023;Kasneci et al., 2023).For instance, Nasr et al. (2023) extracted hundreds of GB of training data from state of the art LLMs using specific prompts.The production of verbatim text by LLMs also increases the danger of plagiarism when including LLM outputs in original publications or essays (Franceschelli and Musolesi, 2022;Kasneci et al., 2023).Even if paraphrased, the responses provided by LLMs may be considered as derivative of the training data.Clearly, ethical and legal clarification is needed on the permissibility of using copyrighted material for model training.Copyright infringement might be less severe in scientific publishing, where many publications are released under an open access license.Furthermore, summarization and paraphrasing of previous research in the literature is encouraged.Consequently, plagiarism is less of an issue as long as sources are references and verbatim quotes as highlighted as such (Lund et al., 2023).</p>
<p>Overreliance</p>
<p>Overreliance refers to the excessive trust and dependence on LLMs for tasks and decision-making processes, often without adequate understanding or critical evaluation of their capabilities and limitations (Choudhury and Shamszare, 2023).</p>
<p>The assumption of infallibility of LLMS can lead to a reduction in critical thinking as users might accept AI-generated responses without question.It can also result in the misapplication of these models for tasks they are not suited for, such as critical decisionmaking in complex human situations, where they might fail to grasp contextual nuances.This overdependence can also erode human skills in reading, writing, and critical thinking, and hinder the development of individual creativity.Therefore, it's crucial to use LLMs as augmentative tools while maintaining a critical and informed approach to their outputs.Even when hallucinating facts or making biased statements, models such as GPT-4 can present them in an authoritative tone or accompany them with a detailed context, making them more persuasive (OpenAI, 2023).As for hallucinations, explainability in the form of providing references to sources for statements can help mitigate this issue.However, Liu N. F. et al. (2023) performed a user study with generative search engines and found that due to their fluency and rhetorical beauty, search results appeared informative even if they were not supported by the retrieved websites.Crucially, only 51.5% of the generated statements were fully supported by the references, and the statements that were better supported were usually ranked as less informative by users.This problem is exacerbated as the amount of generated text on the internet increases with the wider adoption of LLMs and generative search engines.For instance, Vincent (2023) reported that Microsoft's Bing search engine wrongly confirmed that Google's Bard had been shut down.As evidence, it cited a post produced by Google's Bard which appeared in a comment in which a user joked about this happening.Clearly, a model citing non-primary or generated references diminishes the value of referencing, and more research is needed to ensure that models do not start circular referencing of their own or other models' outputs.</p>
<p>In the context of dementia, in addition to the danger of blindly relying on the outputs of LLMs, further adverse cognitive effects may emerge that require ongoing evaluation (Fügener et al., 2021).Previously, humans mostly outsourced physical work to machines (e.g., think of a washing machine or dishwasher).LLMs allow for the outsourcing of cognitive work, too.When using a LLM, the mental effort of formulating an email or creating a poem is reduced to the mental effort required to formulate a prompt.LLMs may therefore act as a double-edged sword, and overreliance could lead to a degradation of human skills in critical thinking, writing, and analysis, as tasks are increasingly delegated to AI systems.For instance, cognitive training to counteract behavioral symptoms of dementia and increase cognitive performance often involves spatial orientation, memory, attention, language, perception, and visual analysis (Mondini et al., 2016;Hill et al., 2017).Furthermore, overreliance can come in the form of overuse at the expense of social activities (Ma et al., 2024).For instance, conversational applications offering companionship to combat loneliness run the risk of exacerbating social isolation.</p>
<p>Risk mitigation and further considerations</p>
<p>Risk mitigation measures that are tailored for specific risks have been described in the previous sections.In this section, we introduce some more general risk mitigation measures that apply across multiple risk scenarios.</p>
<p>Independent auditing</p>
<p>It is essential that protocols are established for vetting LLMs prior or after their release into the public sphere.Such auditing should comprise a suite of tests that estimates the capabilities and limitations of LLMs, including specialized tests and independent tests for each of the risks and limitations outlined above.The outcome of the auditing process could take the form of scores that represent the probability or severity that a given risk or limitation applies to the model.This could potentially be collated into a single risk score.Self-auditing by tech companies is not a viable option since they are facing a conflict of interest: news about harmful behavior of a given LLM could harm the reputation of a company and hence be counter to economic interests.Therefore, auditing should be performed by independent organizations that are themselves subject to strict regulation or gain credibility from being under the auspices of an international body such as the United Nations.Auditing can be performed using existing tests such as TruthfulQA (Lin et al., 2022).However, since some of these tests are in the public sphere, tech companies can train their models on these tests which counteracts their purpose.It is therefore desirable that auditing firms develop their own undisclosed auditing procedures.As an alternative approach, postrelease auditing of commercial models including a public release of the results is a slightly less potent tool, but it may help companies to iteratively improve their models and iron out biases or security flaws (Raji and Buolamwini, 2019).</p>
<p>Explainability</p>
<p>Probing LLMs with predefined test datasets quantifying biases, hallucinations and capabilities provide important incidental information about a model's behavior.Ultimately, however, they are not exhaustive: in the most trivial case, the model might have simply been exposed to the test data and it may still show unwanted behavior in cases that have not been tested.Therefore, a complementary approach is to directly elucidate the inner workings of LLMs using explainability techniques (Zhao et al., 2023).An approach that directly leverages LLMs' language abilities is Chainof-Thought prompting (Wei J. et al., 2023).Not only does Chainof-Thought increase the model's accuracy in answering questions, the resultant point-by-point breakdown of its thought process also better elucidates how the model arrives at a specific decision.Alternatively, Yasunaga et al. (2023) propose analogical prompting, whereby the model is prompted to recall examples relevant to a new task and then afterwards solve the initial problem.</p>
<p>Predictability</p>
<p>Even in the absence of a full understanding of the inner workings of LLMs, insight on LLMs is gained when its behavior can be predicted from a smaller, less capable version, or alternatively, when its capabilities at the end of training can be predicted from its capabilities at early stages of training.OpenAI (2023) used the term "predictable scaling" and showed that model performance could be predicted from significantly smaller models.The expended compute, that is, the amount of training the model received, alone was an accurate predictor of overall loss.Even performance on specific datasets such as HumanEval (Chen et al., 2021) could be predicted with simple power laws, although this did not hold for other metrics such as Inverse Scaling Prize (McKenzie et al., 2023).Ganguli et al. (2022) confirm that overall model performance can be predicted well using either expended compute, dataset size or model size (i.e., number of parameters) as a predictor, performance on specific tasks can emerge abruptly.For instance, they report a sudden emergence of arithmetic, language understanding, and programming skills with increasing model size for GPT-3.Crucially, LLM can learn to solve novel tasks without being explicitly trained to do so (Bubeck et al., 2023).Ganguli et al. (2022) also caution that the open-ended nature of LLMs means that harmful behavior can go undetected simply because it is impossible to probe the model with all types of input that lead to harmful behavior.</p>
<p>Open-source</p>
<p>Opening program code for the public allows for public inspection and scrutiny.This increases the chance that bugs and harmful model behavior can be identified and mitigated (IBM Data and AI Team, 2023).However, open-source can be a doubleedged sword.Given the potential power of LLMs in the realms of misinformation, malicious actors can take open-source models as a basis and finetune them to produce harmful content (Gooding, 2023).</p>
<p>Artificial general intelligence and psychology</p>
<p>Many AI researchers consider LLMs as significant milestones in the quest for Artificial General Intelligence (AGI), arguably the holy grail of AI research.AGI refers to a more general-purpose form of AI capable of understanding, learning, and applying its intelligence to a broad range of tasks and problems, akin to human intelligence (Bubeck et al., 2023).Unlike most currently existing AI systems, which are designed for specific tasks, AGI can adapt, reason, and solve problems across different domains with a high degree of autonomy and it can learn new tasks by example and instruction just like humans do.Although current LLMs can be considered as early ancestors to a fully-fledged future AGI at best, a recent study found "sparks of AGI" in GPT-4, one of the leading LLMs in the year 2023 (Bubeck et al., 2023).GPT-4 showed humanlike performance on exams such the US Medical Licensing Exam (score of 80%) and the Multistate Bar Exam (70%), as well as skillful generation of computer code, predicting the output of a piece of code, and a successful combination across multiple language domains (e.g., writing mathematical proofs as rhymes).Bubeck et al. (2023) also illustrate that GPT-4 shows signs of theory of mind, that is, the ability to understand and attribute mental states (beliefs, intents, desires, emotions, knowledge) to oneself and to others, and to understand that others have beliefs, desires, and intentions that are different from one's own.Furthermore, there is an ongoing debate whether LLMs truly understand language (Mitchell and Krakauer, 2023).This debate is more than just philosophical, since a model that only has a shallow understanding might fail in demanding novel scenarios, posing a potential safety risk.To summarize, although LLMs appear to make strides toward AGI, we wish to emphasize that intelligence is hard to fathom, due to anthropomorphisation, potential contamination of training data with the testing materials, and flaws in the benchmarks (Mitchell, 2023).</p>
<p>Given human-like behavior in a number of cognitive tasks, the question arises whether LLMs exhibit other human-like cognitive properties such as personality and psychological states.Psychology in LLMs might be an unexpected consequence of scaling (Ganguli et al., 2022) or a result of consuming swathes of human text and deliberations which themselves are manifestations of human personality.Hagendorff (2023) argued that a new field of psychological research, "machine psychology", is required to develop bespoke psychological tests and better understand the nascent psychology of increasingly complex LLMs.Miotto et al. (2022) administered personality tests to GPT-3 and found traces of personality akin to a young adult demographic.Griffin et al. (2023) found that LLMs respond to influence similarly to humans.In particular, the authors showed that exposure to specific statements increases truthfulness ratings later on.In line with this, Coda-Forno et al. (2023) found that using emotive language in prompts can lead to more bias in the model's responses.Furthermore, ChatGPT (OpenAI, 2022) robustly responded to an anxiety questionnaire with higher anxiety scores for the model than for humans.Furthermore, there is evidence that LLMs are able to display empathy (Sorin et al., 2023).</p>
<p>Existing models</p>
<p>After the stellar rise of ChatGPT (OpenAI, 2022) in late 2022, a proliferation of LLMs could be witnessed as large tech companies such as Google (Anil et al., 2023;Ghahramani, 2023;Hsiao, 2023;Pichai and Hassabis, 2024), Apple (McKinzie et al., 2024), Meta (Meta, 2023a), and Amazon all raced to release competitive largescale models.In addition, a significant number of startups have been created, with core developers often being ex-employees of large tech companies.For instance, Anthropic was founded in 2021 by senior members of OpenAI and Mistral AI is a French startup built by former members of Google DeepMind.Table 2 summarizes some of the most well-known models, along with their parameters count and context window size.Note that there are many other capable models and a more comprehensive overview is beyond the scope of this paper.</p>
<p>Parameter count is correlated with the learning, generalization, and language understanding capabilities and hence a measure of the model's capacity and capabilities.At the same time, it is associated with increased computational demands.A separate metric of the capability of a LLM is the size of the context window.It is typically measured in the number of tokens.Roughly speaking, this is the amount of information (context) in a session that the model can "remember" or refer to.Most LLMs have a context window of a few thousands tokens, but Anthropic's Claude 2 boasts a large context Parameters refers to the number of parameters or weights in the model (B, billion; T, trillion).In many cases the exact parameter count is not known and estimates (est.) from the literature or blogs are given instead.</p>
<p>window of 100,000 tokens (around 75,000 words).This means that it can hold entire papers and books in memory and the user can ask the model detailed questions about it.Number of parameters and context window size have not been publicly released in many cases.</p>
<p>We collected estimates from the literature and blogs to the best of our knowledge.The models also differ in the type of input data they can receive.For instance, GPT-4 can receive not only text but also images as input and the prompts can be used to ask questions about the image (OpenAI, 2023).Some of the aforementioned models have been used as starting points for more specialized models.For instance, Med-PaLM is a specialized model based on PaLM 2 (Gupta and Waldron, 2023).It is designed to assist in medical decision-making by providing accurate and relevant information based on a wide array of medical literature and data.Furthermore, after Meta released the weights for their Llama model, a number of finetuned models based on Llama have been released, such as Vicuna (https://lmsys.org/blog/2023-03-30-vicuna/), and Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html).Although the overall industry trend has been toward larger, more capable, and multi-modal models, there has been a simultaneous effort to develop Small Language Models (SLMs) such as Phi-2 by Microsoft.The goal of the latter is to obtain models that are highly capable yet deployable on consumer devices such as smartphones.</p>
<p>Large Language Models for dementia</p>
<p>In this section, we elucidate the role that LLMs can play in the research, diagnosis, treatment and management of dementia.LLMs are envisioned to be used by people with dementia (PwD) and/or their caregivers in the form of apps running on a mobile device, tablet, laptop, or desktop computer.Finally, we will introduce a questionnaire that was presented to PwD.In the questionnaire we asked participants about their experience with LLMs, their assessment of several scenarios for using LLM-powered apps for dementia care and management as well as its desired features and functionalities.</p>
<p>Applications in clinical assessment and research</p>
<p>LLMs can be used as tools for dementia research, for instance as models of dementia (Li et al., 2022;Demszky et al., 2023;Loconte et al., 2023) or diagnostic tools (Agbavor and Liang, 2022;de Arriba-Pérez et al., 2023;Wang et al., 2023).The usage of LLMs by psychiatrists, healthcare professionals and data scientists has been covered in other reviews (Bzdok et al., 2024;Tian et al., 2024).</p>
<p>Clinical record summarization</p>
<p>LLMs have the potential to help psychiatrists and other healthcare professionals with routine tasks such as writing of clinical reports, saving time and reducing manual data management (Cheng et al., 2023;Javaid et al., 2023).They have been used to provide summaries of patient-doctor conversations (Zhang et al., 2021), clinical notes (Kanwal and Rizzo, 2022) and reports (Vinod et al., 2020), as well as coding adverse events in patient narratives (Chopard et al., 2021).Furthermore, although off-theshelf LLMs lack the sophistication required to answer queries of medical experts, finetuned models such as PMC-Llama (Wu et al., 2023) and Med-PaLM (Singhal et al., 2023) show increased expertise.In line with this, Lehman et al. (2023) showed that models trained or finetuned on clinical records outperform models that are not finetuned or that rely on in-context learning.In safety-critical domains such as medicine, the accuracy of the summary is of utmost importance.In this regard, Van Veen et al. ( 2024) performed an experiment with physicians showing that they preferred LLM-based summaries over summaries produced by human experts across a variety of domains (radiology reports, patient questions, progress notes, and doctor-patient dialogue).</p>
<p>Dementia prediction</p>
<p>Prediction of dementia using artificial intelligence with various biomarkers is well researched.First, one branch of researchers focused on neuroimaging data, using structural Magnetic Resonance Imaging (MRI) for predicting accelerated brain aging (Baecker et al., 2021;Treder et al., 2021), functional MRI (Du et al., 2018), electroencephalography (Jiao et al., 2023), or a fusion of different modalities (Abrol et al., 2019).Second, clinical summaries have been used with LLMs to make differential diagnoses (Koga et al., 2024).Mao et al. (2023) showed that a language model can use clinical notes to successfully predict the transition from mild cognitive impairment to Alzheimer's disease.Third, diagnostic markers can be extracted from patients' speech, either directly from acoustic signals or from the transcribed text.A number of approaches showed a high predictive accuracy using acoustic features such as number of pauses and speech rate (Toth et al., 2018;Al-Hameed et al., 2019;O'Malley et al., 2020).Bang et al. (2024) used a combination of speech, text, and fluency opinions and reported an accuracy up to 87% for discriminating between Alzheimer's patients and healthy controls.In a different approach by Bouazizi et al. (2024), center of focus changes of participants when describing an image were predictive of dementia.Agbavor and Liang (2022) used GPT-3 to extract text embeddings that were then used as features to distinguish Alzheimer's patients from healthy controls.Better results were obtained for text features than for acoustic features using the speech signal directly.This suggests that text, although lacking information such as intonation, pauses, rate, and rhythm, might contain enough information to enable dementia prediction.Lastly, as a complementary application to prediction, LLMs are also able to generate synthetic data that can counteract the scarcity and imbalance of curated medical data and thereby aid in the training of prediction models (Li et al., 2023).</p>
<p>Applications in dementia management and care</p>
<p>In this section we introduce several scenarios for how LLMpowered apps could be used in the management of dementia, either by people with dementia themselves and/or their supporters.Figure 3 depicts an overview over the scenarios.</p>
<p>Companionship</p>
<p>LLMs are able to participate in conversations about daily or private matters, questions and concerns.When tuned to respond adequately (e.g., displaying understanding and empathy) we hypothesize that an app could provide additional companionship and emotional support, especially in situations wherein PwD are socially isolated.Feeling of loneliness has been associated with a higher risk for developing dementia later in life (Holwerda et al., 2014), although the literature is inconclusive on whether this relationship is causal (Victor, 2021).There is evidence that apps in general can help reduce loneliness and isolation in dementia (Rai et al., 2022).The apps reported in Rai et al. (2022) were aimed toward communication and social connections, improving engagement and physical activity through multisensory stimulation, remote monitoring and support, and assistive functions.Some studies reported positive results on digital pets and humanoid social robots for combating loneliness and social isolation in dementia (Gustafsson et al., 2015;Demiris et al., 2017;D'Onofrio et al., 2019;Fields et al., 2021;Lima et al., 2022).In a field study with 25 participants from an elderly home, Ryu et al. (2020) found significant decreases in anxiety and depression after daily use of a conversational chatbot for free conversations.Qi and Wu (2023) highlight the potential benefits of ChatGPT in terms of loneliness, emotional support, and assisting with daily tasks including reminders, medications, and appointments.This nicely dovetails with the assessment of healthcare professionals who report merit in virtual assistants and companions (Koebel et al., 2022).In summary, we believe that LLMs hold potential as a companion and serve as an antidote to loneliness and social isolation associated with dementia.As LLMs mature technologically, it is possible to have increasingly meaningful and deep conversations with them.Although it is unlikely and perhaps undesirable that they can fully replace conversations between humans, they can complement and enhance human interaction, especially when carers are not accessible 24/7.Such social and conversational LLMs can come in the shape of apps, as potentially voice enacted chat applications.More immersive social interactions might be possible when the LLMs are digitally embodied as virtual avatars (Morales-de-Jesús et al., 2021) or even physically embodied as robots (Lima et al., 2022).</p>
<p>Information retrieval</p>
<p>LLMs can serve as reservoirs of knowledge.Although this is one of their more basic applications, it can be useful for PwD.Unlike conventional search engines that merely retrieve websites, LLMs excel in identifying, compiling and re-synthesizing knowledge and presenting it in an accessible and understandable form.Saeidnia et al. (2023) reported dementia caregivers were overall positive about the quality of answers given by ChatGPT to queries about non-clinical issues relevant to PwDs' lives.However, for questions related to dementia, LLMs may not be sufficiently accurate out of the box.For instance, Hristidis et al. (2023) compared ChatGPT with Google search for questions specifically related to dementia and cognitive decline with subpar quality for both systems.In line with this, ChatGPT's knowledge of dementia has been designated as "accurate but shallow" (Dosso et al., 2023).This can potentially be alleviated by finetuning LLMs on medical data.For instance, PMC-Llama is a model based on Llama that has been finetuned using medical journal papers and textbooks (Wu et al., 2023).Similarly, Google released Med-PaLM, a version of their PaLM specifically geared toward answering medical questions (Singhal et al., 2023).Additionally, one can envision that LLMs could be finetuned to adapt their style to the user via prompt engineering.By default, models such as ChatGPT have a verbose and rather academic writing style.In summary, we believe that LLMs can be useful for the collation and reformulation of generic information as well as information specifically related to dementia.In the latter case, finetuned models such as Med-PaLM will likely be required.Furthermore, care needs to be taken to avoid blurring the line between a conversational service and medical advice, since at least for the time being healthcare professionals should be the ultimate source of medical advice.</p>
<p>Therapy aid</p>
<p>As alluded to in the previous paragraphs, LLMs can provide companionship and combat loneliness and social isolation.However, can it be used by therapists and healthcare professionals to aid during therapy?A review of previous-generation language models reported promising potential for use in mental health (Vaidyam et al., 2019).Despite limited data on its clinical efficacy, users dealing with mental health problems have been consulting ChatGPT (Eliot, 2023).Some studies investigated language models in the context of reminiscence therapy which involves engaging patients in recalling and discussing past experiences, often using tangible prompts like photographs or familiar objects (Khan et al., 2022).Reminiscence therapy can enhance emotional wellbeing and cognitive function, as it encourages communication and the recollection of personal histories.Carós et al. (2020) et al., 2019).LLMs can potentially help administer cognitive behavioral therapy via phone apps (Denecke et al., 2022) or in the shape of conversational chatbots (Patel et al., 2019;Omarov et al., 2023).In an analysis of social media posts on an LLM-powered mental health app (not specifically aimed toward PwD), Ma et al. (2024) reported on-demand and non-judgmental support, the development confidence and self-discovery as the App's benefits.In summary, we believe that LLMs can serve as therapy assistants to healthcare professionals.They either affect the therapeutic quality either indirectly by reducing the work burden of a healthcare professional, or directly by engaging in an intervention such as reminiscence therapy.</p>
<p>Reading and writing</p>
<p>A useful but easily overlooked feature of LLMs is that they can comprehend complex text and paraphrase it in more palatable or adequate language, e.g., rephrasing a formal text using more casual language.This is a relevant functionality since PwD are more likely than healthy controls to suffer from reading and writing deficits and speech pathologies (Murdoch et al., 1987;Krein et al., 2019).Consequently, LLMs could help in the interpretation and comprehension of letters, or emails, manuals, especially when being verbose or using convoluted language.Similarly, LLMs can assist in the formulation of letters and emails.We are not aware of specific studies on dementia in this regard, but LLMs have been explored for clinical text summarization (Van Veen et al., 2023;Tian et al., 2024) and the summarization of fiction books (Wu et al., 2021).Furthermore, LLMs are increasingly being used as co-pilots in the writing of scientific articles (Altmäe et al., 2023;Lingard, 2023;Park, 2023), including the present one, as well as liberal arts (Oh, 2023) and business writing (AlAfnan et al., 2023).We are not aware of specific studies on dementia for writing, but language models have been explored as email writing assistants for adults with dyslexia (Goodman et al., 2022;Botchu et al., 2023).In summary, LLMs as reading and writing aids for dementia have not been explored sufficiently, hence more research is required to evaluate their utility in this area.</p>
<p>Navigation</p>
<p>Several types of dementia, including Alzheimer's disease and dementia with Lewy bodies, can affect visual cognition and navigational abilities to varying extents (Plácido et al., 2022).Spatial navigation aids for people with dementia in forms of digital apps and devices have been explored for years (Kowe et al., 2023;Pillette et al., 2023).Navigation aid can be useful both for outdoor navigation, e.g., finding your way from the home to a destination, and indoor navigation, e.g., finding the way around a hospital or other large building (García-Requejo et al., 2023).Tech companies such as Google aim to integrate conversational services into a wide variety of apps (Wang and Li, 2023).This opens the door for language and speech-assisted navigation, where the user converses with the navigation system and can ask for clarification and guidance.Currently, we are not aware of any such systems specifically developed for dementia patients.Further technological development and research on the academic and clinical side are required to assess how LLMs can aid navigation in these populations.</p>
<p>Technical and design considerations</p>
<p>The implementation of LLM-powered apps for dementia involves a number of technical considerations as well as design challenges related to dementia:</p>
<p>• Neurodiversity and cognitive load.Cognitive impairment associated with dementia can limit how much PwD can benefit from apps that place high demands on cognition (Hugo and Ganguli, 2014).Therefore, the design of supportive apps for dementia patients should account for potential cognitive deficits faced by this population by minimizing cognitive load.</p>
<p>• Mobile phone use.The prime outlet for digital apps is mobile phones.Dixon et al. (2022) used semi-structured interviews to investigate mobile phone usage in PwD.Widespread usage of mobile phones by PwD was reported for tasks such as social media, reminders, and navigation.However, challenges regarding the ease of use were reported, such as difficulty in navigating to the right App, operating the phone while stressed or fatigued, and dealing with changing interfaces after App updates.Users valued being able to customize the interface to their needs, being able to use them as personal assistants, and use avatars and voice interaction.In conclusion, users should not have to be tech savvy to use them, and they should be built with ease, stability and customizability in mind.</p>
<p>• Voice control.Dementia types can be associated with visual impairments (Kuzma et al., 2021), above and beyond the visual impairments that naturally come with age.Voice control is desirable since it can ease the interaction with digital devices and remove the challenge of navigating through the apps on the screen.However, not all voice systems are sufficiently robust to impairments such as slowed speech or stutter which can be frustrating and stress-inducing (Dixon et al., 2022).Furthermore, hearing impairments can challenge voice based interaction, pointing again at the importance of a system with personalized characteristics tailored to the user (Hardy et al., 2016).</p>
<p>. /frdem. .</p>
<p>Free text</p>
<p>The meaning of the columns is as follows.Section: Which section of the questionnaire the question belongs to.ID: identifier of the item that is used in the results section.Item: the verbatim question used in the questionnaire.Answer type: the type of answer that was required, i.e., number (participants entered a number with the keyboard), multiple choice (with the different options provided in brackets), free text (participants type a text as answer), 5-points Likert scale.</p>
<p>• Avatar.Some participants in the study by Dixon et al. (2022) were enthusiastic about using voice control in conjunction with an animated personalized avatar.The avatar could help with attentional focus.</p>
<p>• Cloud-based vs on-device.LLMs tend to be computationally demanding and it is usually not feasible to deploy them directly on consumer phones.Instead, a cloud-based solution can be utilized that relies on an internet connection.The cloud server then processes the input through the LLM, generates the results and sends these results back to be displayed on the phone.This is how LLMs such as ChatGPT (OpenAI, 2022) are typically integrated into smartphone apps.The advantage of this Approach is that no compute resources are needed on the device.The disadvantage is that an internet connection is required to operate the App, there can be additional delays due to transmission delays between the phone and the server.Additionally, there are potential security risks such as prompt injection and privacy risks due to communication with the server.There has been some effort to develop Small Language Models (SLMs), such as Microsoft's Phi-2 (Javaheripi and Bubeck, 2023), which can be directly deployed on the phone.While phone-hosted LLMs offer enhanced security and privacy, by operating independently of internet connectivity,current technical constraints around model size, battery consumption, cooling and maintaining strong capabilities present trade-offs versus cloud-processed LLM solutions.</p>
<p>• Conversational style.In addition to the content of a conversation, the style in which an LLM interacts with the user is relevant to the overall experience.For instance, ChatGPT can be verbose and academic sounding, which could make comprehension difficult for many dementia patients.Models such as ChatGPT are able to adapt conversational style via prompt engineering, so style adaptation is a design challenge rather than a technical challenge.</p>
<p>• Anthropomorphisation.As LLMs capabilities increase, users are more likely to ascribe personality and agency to them.This can facilitate building an emotional bond with the App, offering potential benefits such as increased engagement, but also risks such as overreliance on recommendations.Evidence for this was given by Ma et al. (2024) who reported in an analysis of social media data that some users of an LLMpowered mental health App experienced feelings of stress, loss and grievance after updates to the LLM lead to inconsistent conversational style and the loss of memory of previous conversations.While these results were obtained with a chat application, LLMs personified as virtual avatars with their own voice and looks might increase anthropomorphisation even more.</p>
<p>Concluding, the diversity and individual variability of challenges faced by dementia patients makes it unlikely that a single technical solution can cater to the entire user base.A solution that claims wide applicability needs to be personalizable and adaptive.Personalization can involve visual elements (e.g., size, color, or style and choice of a virtual avatar), auditory aspects (speed and information content of auditory feedback and voice choices for voice assistants), as well as cognitive load (e.g., complexity of the usage, number of elements on a dashboard, ease of navigation) and conversational style.It is evident that the development of solutions should be accompanied by involvement and engagement of PwD and their caregivers/supporters.Their feedback should be sought from the initial design stage throughout the entire product development cycle is essential for creating effective and user-centric solutions.</p>
<p>Questionnaire</p>
<p>We believe that an effective and ethical path toward the usage of LLMs in dementia management and care involves centering the perspectives and needs of people with dementia, caregivers and other stakeholders at all stages in the research and development cycle.For this reason, we created a questionnaire in which we asked participants to rate the usefulness of LLMs in a number of application scenarios (e.g., companionship, therapy aid), and we asked them to rate the importance of design features (e.g., ease of use, voice control, privacy).We presented the questionnaire to PwD, their supporters, caregivers and stakeholders.To the best of our knowledge, this is the first targeted survey on the usage of LLMs for dementia care and management.Ethical approval for the study has been obtained from The School of Computer Science and Informatics Research Ethics Committee at Cardiff University, United Kingdom, reference: COMSC/Ethics/2023/122.</p>
<p>Participants</p>
<p>Fifteen people with dementia (PwD) aged 58-88 (µ = 72.2),7 women, 7 men, 1 of nonbinary gender, participated in the study.</p>
<p>Additionally, 14 supporters aged 32-70 (µ = 53.6),11 women and 2 men (1 declined to indicate their sex), participated in the study.Supporters could be family members or professional caregivers or nurses.Participants were recruited with the help of Dementia Australia (https://www.dementia.org.au/) and Alzheimer's Society UK (https://www.alzheimers.org.uk/).The organizations served as gatekeepers, that is, they published our invitation email and a participant information sheet on their website.The invitation email included a hyperlink that would take participants directly to the survey.There was no compensation for participation but participants could opt-in to a raffle for a single £100 Visa Gift card.To this end, they would enter their email address in the notes section of the questionnaire.After the raffle, the email addresses were removed from the dataset.The study was fully anonymous otherwise.</p>
<p>Questionnaire details</p>
<p>A copy of the questionnaire is provided as Supplementary material.Here, we summarize its main items.For the items, participants could choose to select "Prefer not to answer" if they wish not to answer a question.For multiple choice questions, an additional option "Other" was provided in case participants wanted to specify an option that was not listed.Table 3 lists the questions used, categorizes them by section and specifies the type of answer required.Note that all items categorized as follow-up were only asked when the immediately preceding questions was answered with "yes".Following questions about their demographic background and dementia, the main body consisted of questions regarding application scenarios of LLMs as well as desired features for digital apps.Finally, participants were asked to estimate the overall impact AI can have on dementia management and care, and there was space for free text with any notes or additions participants would like to make.</p>
<p>FIGURE</p>
<p>Questionnaire results for seven di erent application scenarios (see Table , IDs . through . ).Pie charts show how useful PwD and supporters consider LLMs in each application scenario.Each scenario is labeled in the figure.The corresponding pie chart is shown for PwD above the label and for supporters below.The numbers in the pie slices correspond to absolute and relative number of respondents (e.g., six respondents, / = %).The legend defines the meaning of the colors.</p>
<p>Procedure</p>
<p>The survey was implemented in Google Forms.It commenced by asking participants to provide informed consent in line with Cardiff University's guidelines.Participants were then asked to watch a 1-min overview over ChatGTP on YouTube (https://www.youtube.com/watch?v=aIO9it4HFiQ) to make sure that they familiar with the basic principles of LLMs.Further videos and a blog post were presented as optional additional material.They then answered the questions listed in Table 3 by either clicking on the multiple choice options or typing an answer.The survey took about 20 min.</p>
<p>Results</p>
<p>The raw data and results of the questionnaire are available in our GitHub repository (https://github.com/treder/LLMs-fordementia).We review the results according to the sections in Table 3: Demographics, dementia, AI experience, application scenarios, and features and priorities.</p>
<p>Demographics and dementia</p>
<p>Figure 4 depicts the demographic details of the participants.People with dementia (PwD) participating in our study were aged 58-88 years whereas supporters were aged 32-70 years.As these ranges suggest, supporters were significantly younger than PwD (independent samples t-test, t = 5.059, p &lt; 0.0001).Whereas gender roles were equally distributed for PwD (7 women, 7 men, 1 of nonbinary gender), supporters were predominantly female (11 women, 2 men).To compare the distribution of genders across the two groups, we used a two-sided Fisher's exact test which works on 2x2 contingency tables.The chi-squared test allows for larger tables but requires a larger sample size (Hazra and Gogtay, 2016;Sundjaja et al., 2024).Therefore, we focused on comparing the number of men and women.The difference was not statistically significant (odds ratio = 5.5, p = 0.1032) which might be attributed to the small sample size.</p>
<p>People with dementia identified as European ( 14) and Asian (1).Their highest degrees were vocational training/trade certificate (2 respondents), secondary education/high school (9), Bachelor's degree or equivalent (1), or Postgraduate degree (3).The supporters identified as European (9), Mixed background (2), or Asian (1), and 2 preferred not to answer.Supporters had vocational training/trade certificates (1 respondent), Bachelor's degree or equivalent (7), or postgraduate degree (5), and 1 preferred not to answer.</p>
<p>People with dementia received the diagnosis between 1 and 13 years (µ = 5.1) ago.They were diagnosed with various types of dementia, namely Alzheimer's disease (9 respondents), Vascular dementia (1), Fronto-temporal dementia (1), Lewy body dementia (1), or Mixed Dementia (2), and 1 preferred not to answer.When asked to freely describe their symptoms, memory problems were mentioned most often, with 5 respondents mentioned problems with "short term memory", and another one "total blank in the mornings".Additional symptoms were related to social interaction ("withdrawn from people", "unable to speak properly, difficulty understanding conversations"), physical symptoms ("tremors, gait and balance", "difficult to balance on one side", "shakes, unstable"), as well as "hallucinations, visual and auditory" and a general "inability to perform everyday tasks" and "inability to understand controls on oven or television".</p>
<p>AI experience</p>
<p>Responses related to the use of apps in the context of dementia, 3/15 PwD and 5/14 supporters (1 preferred not to answer) responded with "Yes".Six PwD and 12 supporters heard of LLMs such as Chat-GPT before.Two PwD (1 preferred not to answer) and 5 supporters (1 preferred not to answer) stated having used them before.One participant with dementia stated that they "use ChatGPT to gather information, links and quotes".Supporters used them for "Patient and Public Involvement Networks, Universities, and as a carer for my Husband who had Dementia", to "discover information on various topics encompassing dementia, including the types, symptoms and possible outcomes of therapies used in behavior management in dementia", as well as to "synthesize text and videos" and for "writing reports".</p>
<p>Application scenarios</p>
<p>Figure 5 shows mean opinion scores obtained by encoding the response options as integers ranging from 1 to 5 and averaging them across individuals for the PwD and supporter groups separately.On average, all scenarios were ranked with moderate scores in between "Moderately useful" and "Useful" by both groups.Both PwD and supporters ranked "Navigation", "Reading aid", and "Writing aid" the highest.Somewhat lower scores were assigned to "Companionship" and the two items on "Dementiarelated information".As visual inspection suggests, responses between PwD and supporters were significantly correlated (Pearson correlation, r = 0.79, p = 0.033).</p>
<p>A more detailed overview with the proportion of each response option by group is depicted in Figure 6.We observe a dichotomy within the PwD group: for each scenario, at least one participant selected the response "Not useful at all" whereas several participants selected "Very useful".To investigate whether individual response patterns are correlated with demographic variables, we performed a series of Pearson correlation analyses.We found that the overall mean score across all scenarios is negatively correlated with age for PwD (r = −0.62,p = 0.014) but not with the number of years since the dementia diagnosis (p = 0.42).In other</p>
<p>FIGURE</p>
<p>Questionnaire results for features and priorities (see Table , IDs . -. ).Pie charts show how important PwD and supporters consider specific aspects of LLM-based apps for dementia.Each feature is labeled in the figure.The corresponding pie chart is shown for PwD above the label and for supporters below.The numbers in the pie slices correspond to absolute and relative number of respondents (e.g., respondents, / = .%).The legend defines the meaning of the colors.For data privacy and data transparency, an additional option was provided.The corresponding slice is depicted in white and the response option is pasted next to the figure .words, older participants tended to give lower overall scores.For supporters, there was no evidence for such a relationship (r = −0.36,p = 0.2).When performing the same analysis on the score for each scenario separately, we found a significant relationship for "Companionship" (r = −0.64,p = 0.001), "Dementiarelated information" (r = −0.54,p = 0.044), "Dementia-related information including personal data" (r = −0.61,p = 0.027), "Reading aid" (r = −0.61,p = 0.019), "Writing aid" (r = −0.62,p = 0.002), although only "Companionship" and "Writing aid" would survive a correction for multiple comparisons.Correlations were not significant for "Navigation" (p = 0.24).No such relationships were found for supporters (all p &gt; 0.16).For the PwD group, we repeated the correlation analysis using the number of years since the dementia diagnosis instead of age, but found no significant effects (all p &gt; 0.31).For sex, we did not find a relationship with mean score for either group (all p &gt; 0.53).</p>
<p>Features and priorities</p>
<p>Figure 7 shows mean opinion scores obtained by encoding the response options as integers ranging from 1 to 5 and averaging them across individuals for the PwD and supporter groups separately.On average, all scenarios were ranked with moderate to high scores in between "Moderately important" and "Very important" by both groups.Both PwD and supporters ranked all priorities around data ("Data privacy", "Data transparency", "Data deletion") the highest, showing concern for their agency over FIGURE Questionnaire results for device and impact (see Table , IDs . and . ).Di erent legends are provided for each question.For the preferred device, participants could select smartphone, Laptop/PC, tablet, or combinations of these.</p>
<p>data.Mean scores between the two groups were highly correlated (Pearson correlation, r = 0.94, p = 0.001), showing a similar pattern of concerns and priorities.</p>
<p>A more detailed overview with the proportion of each response option by group is depicted in Figure 8. Respondents in the PwD group gave either high or low scores to the items "Ease of use", "Voice control", "Empathy", and "Human in the loop", whereas supporters overwhelmingly gave high scores to these items.Overall mean score across all features and priorities was significantly correlated with age for PwD (r = −0.54,p = 0.04) but not supporters (p = 0.5).For PwD, when performing the same analyses on the score for each feature and priority separately, we found no significant relationships (all p &gt; 0.05).There was no significant correlation with sex (PwD: p = 0.22, supporters: p = 0.17) and for PwD there was no correlation with the number of years since diagnosis (p = 0.33).</p>
<p>Figure 9 depicts results on which devices respondents use and how they rate the overall impact of LLMs on dementia management and care.In both groups a variety of devices was used, although amongst PwD tablets were more dominant whereas among supporters smartphones were more dominant.The overall impact of LLMs on dementia care and management was seen more positively by PwD than supporters.Whereas only 1 respondent in the PwD group indicated "negative", 3 supporters indicated the impact as "very negative".Nevertheless, larger proportions in both groups rated the impact as "positive" or "very positive" (PwD: 9 respondents, supporters: 8).</p>
<p>Free comments</p>
<p>Respondents could also provide feedback in a free textual form (items 4.8 and 6.2 in Table 3).Both PwD and supporters provided feedback on positive and negative use cases for language-based AI applications in dementia.While some respondents were excited about the potential benefits of AI, others raised a number of concerns and caveats.The main points are summarized in Table 4.</p>
<p>Discussion</p>
<p>Large Language Models (LLMs) revolutionize the way in which humans interact with machines.For the first time in history, we can converse with computers in the same way that we talk to each other.Meaningful conversations, creative writing, poetry, summarization, all deeply human faculties that can now be experienced in a chat with an algorithm.Our review has highlighted the burgeoning role of LLMs in improving dementia care and research.The integration of LLMs into therapeutic and support frameworks holds the potential to enhance the quality of life for individuals living with dementia, as well as to alleviate the considerable burden on caregivers.Through personalized conversations, information retrieval, therapy aid, and assistive technologies for reading, writing, and navigation, LLMs offer a novel approach to dementia care that is both innovative and human-centric.Nevertheless, its adoption might face an uphill battle due to algorithmic and regulatory limitations and challenges, as well as concerns about adequacy and applicability in the context of dementia care that surfaced in our survey.</p>
<p>Limitations, risks, and challenges</p>
<p>Despite the promising prospects, the deployment of LLMs in dementia care is not without challenges.The current limitations of LLMs, including their dependency on the quality of input data and the potential for perpetuating biases, must be acknowledged and addressed.First, hallucinations, or the production of syntactically</p>
<p>Summary Verbatim responses</p>
<p>The application scenarios are useful D "Wow!!I would love anything like those above" D "A product like this would be amazing for me it would take a lot of stress out of my everyday life" S "I think AI has great implications for dementia awareness/care [...]" S "AI technology will be very important to alleviate isolation, and feelings of loneliness for those living with dementia who do not have family or friends nearby to engage with"</p>
<p>There are other useful application scenarios beyond those mentioned D "keeping fit and retaining muscle mass" D "ask medical questions" S "Protection from scams would be useful"</p>
<p>The application scenarios do not address the actual problems of PwD Concern about level of tech affinity required D "This AI is not relevant in any way to my mum who has dementia aged 88yrs and has never been able to use a computer even before her diagnosis" D "This AI sounds like amazing progress but the actual demographic of most dementia sufferers is that they are over 70 so I am not too sure AI is going to help them a great deal as they will probably mostly not be computer literate![...] I am 66 years old and find AI rather a challenge" S "They would find a PC, smartphone etc. very hard to navigate without help" S "it would not be suited to older people with no AI experience.People should start using the planned App as soon as possible so they are familiar with it even before a diagnosis"</p>
<p>Usefulness depends on stage of dementia D "this would be great for FTD and MCI" S "The ability to interact with the AI model depends on the degree of decline" S "might work well very early on in the disease" S "for people with early onset dementia, it could be a valuable tool" S "it just feels unsuitable, totally depends on the stage of dementia, at present it just feels like a gimmick" LLM may not understand the user S "If relatives with a good knowledge of the person with other visual cues have difficulty in understanding, it is possible that AI will miss the point."</p>
<p>User may not understand the LLM S "My clients if having a bad time being lost won't be able follow instructions to find their way home.Also talking about dementia, treatment, diagnosis I can see that leading to confusion, processing that much information, and it might be conflicting information to the client."</p>
<p>LLM cannot replace human interaction and care S "I know my loved one would not have been happy talking to a machine.It is not a replacement for a human [...] It's too untried to be let loose with those with a dementia diagnosis" S "[...] using the AI app without help from a carer/district nurse or family member would be very difficult for someone in the later stages of dementia" S "It is wrong in so many ways to use a machine to replicate a human response [...] This is very much along the lines of "babysitting by television"</p>
<p>The first column summarizes the respondents' statements, the second column provides evidence in the shape of the actual individual feedback.The superscript D refers to respondents in the dementia group and S refers to supporters.</p>
<p>correct but factually incorrect text, plague all state of the art LLMs (Ye et al., 2023;Zhang et al., 2023) and are a source of concern for their medical application (Pal et al., 2023;Tian et al., 2024).Second, LLMs are trained on a large corpus of text from a variety of sources including social media websites, often without permission of the author of the text (Franceschelli and Musolesi, 2022;Kasneci et al., 2023).Since stigma against people with dementia has been reported on the media platform X (Oscar et al., 2017;Bacsu et al., 2022), inclusion of uncurated internet data into LLM training harbors the danger of perpetuating stereotypes about dementia.Third, overreliance might create adverse cognitive effects.LLMs assisting with perceptual tasks, memory, and language, creates short term benefits, but it is the same faculties that have to be engaged in order to combat decline (Mondini et al., 2016;Hill et al., 2017).Fourth, the development of LLMs for dementia has to take place with a regulatory framework that ensures that risks are mitigated, privacy is preserved, intellectual properties are warranted, and liability for malpractice is established (Meskó and Topol, 2023).</p>
<p>Questionnaire</p>
<p>Using a questionnaire, we probed both people with dementia (PwD) and their supporters regarding their opinions on the application and features of LLMs in the context of dementia.Participants covered a representative age range for PwD spanning 58-88 years (Hugo and Ganguli, 2014).Whereas the gender split was roughly equal for PwD, the majority of the supporters were women, in line with the predominance of female carers in mental illnesses more broadly (Sharma et al., 2016).Only 3 of 15 Pwd and 5 ./frdem. .out of 14 supporters reported ever having used apps in the context of dementia.We presented several application scenarios involving companionship, dementia information, navigation, reading or writing aid, and therapy aid.Both PwD and supporters rated all of the scenarios as moderately useful to useful.Older PwD tended to give lower overall scores than younger PwD.It is up to speculation as to why, perhaps indicating a generally more negative outlook, the existence of more severe symptoms that are unlikely to be alleviated by LLMs, or perhaps a larger barrier to use digital apps.</p>
<p>Regarding their priorities for what features LLM-powered apps should have, PwD and supporters ranked agency over data (privacy, transparency, deletion) and ease of use the highest.Opinions on the usefulness of the technology diverged, however.In the free comments sections (see Table 4), some respondents praised the promise of the technology ("I think AI has great implications for dementia awareness/care [...]") but they also raised several caveats.The application scenarios might not address the real needs of PwD ("None of them relate to alleviating the problems of daily living [...]").There were also concerns about bias and technological affinity required ("This AI sounds like amazing progress but the actual demographic of most dementia sufferers is that they are over 70 so i am not too sure AI is going to help them a great deal as they will probably mostly not be computer literate").Other respondents pointed out that the usefulness of LLMs depends on the stage of dementia ("The ability to interact with the AI model depends on the degree of decline") and that LLMs cannot serve as a substitute for human interaction ("I know my loved one would not have been happy talking to a machine.It is not a replacement for a human").</p>
<p>Limitations of the questionnaire</p>
<p>Our online survey has several limitations.The use of convenience sampling through dementia organizations as gatekeepers, while practical, may introduce selection bias.This approach relies on participants who are actively engaged with these organizations and have access to the internet, potentially excluding a portion of the dementia population who are less active or lack online access.Additionally, sample sizes of 15 people with dementia and 14 supporters limit the statistical power of the study especially with regard to more subtle effects.It is also possible that in some cases both a PwD and their supporter filled in the questionnaire, leading to correlation between the samples.The anonymity of the questionnaire, while protecting participant privacy and potentially lowering the barrier to participation, also prevents any follow-up for more in-depth data collection.</p>
<p>Implications for practice</p>
<p>Our findings suggest that LLMs can serve as an invaluable resource in dementia care.By providing personalized interaction and support, LLMs have the potential to improve social engagement and cognitive functioning among individuals with dementia.However, the successful implementation of LLMs in dementia care requires careful consideration of the technology's limitations and the ethical implications of its deployment.Privacy and safety concerns must be meticulously addressed, and systems need to be designed with the end-user in mind, ensuring that they are accessible, intuitive, and genuinely beneficial.</p>
<p>When our previous considerations and the findings from the survey are taken together, we can add the following points for LLM-powered apps for dementia care and management:</p>
<p>• On-demand aid.LLM-powered apps offer on-demand and non-judgmental support, potentially including mental health benefits such as promotion of self-confidence and aiding selfdiscovery (Ma et al., 2024).</p>
<p>• Caregiver burden.Economics mandate a reduction in care cost (Nandi et al., 2022).While LLMs might alleviate caregiver burden, many respondents pointed out that apps cannot serve as substitutes to human interaction.Most likely, a collaborative solution involving human support augmented by a chatbot for periods wherein the human supporter is not available would be a way forward that meets targets both in terms of quality of care and associated monetary cost.</p>
<p>• Prompt engineering and communication.</p>
<p>It is yet unclear how LLMs perform in the presence of language disorder (Murdoch et al., 1987) and other forms of cognitive impairment (Hugo and Ganguli, 2014).It is conceivable that bespoke models are required, e.g. by finetuning a model such as ChatGPT on a dataset including excerpts of speech from language-impaired individuals.Since the communication is bi-directional, further finetuning might be required to align the model to produce outputs that are more palatable for individuals with impairments.</p>
<p>• Complexity and technological affinity.As long as operating LLMs is not seamless any real-world implementation faces a catch-22 scenario: users that benefit from an LLM the most might find it the most challenging to operate LLM-powered apps.For this reason, some respondents pointed out that LLMs should be aimed toward milder versions of dementia such as early-onset dementia.Integration in physical agents such as robots could provide a more seamless gateway between LLM and the user.</p>
<p>• Co-development of apps.As a note to tech developers, our survey showed the importance of co-developing solutions with the end user (both PwD and supporters) in the loop early.Otherwise one runs the risk of designing a solution that does not address the needs of PwD or is not usable in the light of their expertise and challenges in using such apps.Furthermore, the language used should not patronize PwD or diminish their agency or cognitive capacities.</p>
<p>• Data agency.PwD and supporters stressed the importance of retaining agency over their digital data, including transparency about its usage and the ability to delete it.</p>
<p>Future research directions</p>
<p>To harness the full potential of LLMs in dementia care, future research should focus on several key areas.First, there is a need for longitudinal studies to assess the long-term impact of LLM interactions on individuals with dementia.This includes evaluating the effects on cognitive health, emotional wellbeing, and social engagement over time.It also involves a better characterization of dementia-specific limitations and risks associated with the usage of LLMs.Second, research should explore the customization and personalization of LLMs to meet the diverse needs of individuals with dementia.This includes the development of adaptive algorithms that can tailor interactions based on the user's preferences, behaviors, and cognitive status.Third, the exploration of multimodal LLMs that can interpret and respond to non-verbal cues could significantly enhance the quality of interactions, making the technology more accessible and effective for individuals with varying degrees of cognitive impairment.Fourth, exploring the embodiment of LLMs in robotics could revolutionize dementia care by providing conversational and physical support through social robots (D'Onofrio et al., 2019;Fields et al., 2021;Lima et al., 2022).This research should aim to develop adaptive robots that cater to the emotional and physical needs of dementia patients, enhancing their quality of life with a blend of cognitive support and companionship.</p>
<p>Conclusion</p>
<p>In conclusion, the use of Large Language Models in dementia care represents a promising frontier in the intersection of AI and healthcare.While challenges and limitations exist, the potential benefits of LLMs in enhancing cognitive abilities, enriching social interaction, and supporting caregivers are undeniable.As we move forward, it is crucial that the development and implementation of LLMs is guided by ethical considerations, empirical evidence, and a commitment to improving the lives of individuals living with dementia.</p>
<p>FIGURE</p>
<p>FIGURE Di erent types of training an LLM.Pre-training and fine-tuning involves adjusting the weights of the model whereas in-context learning and retrieval-augmented generation (RAG) works for deployed models and does not change the internal structure of the model.RLHF, Reinforcement learning with human feedback.</p>
<p>FIGURE</p>
<p>FIGUREPossible applications of LLMs in dementia management and care.</p>
<p>FIGURE</p>
<p>FIGUREDemographic data of the participants.Top left: age distribution for people with dementia (PwD) and supporters.Top right: types of dementia.Bottom row: sex, ethnic background, and education for people with dementia and supporters.</p>
<p>FIGURE</p>
<p>FIGUREOpinion scores on Likert scale (y-axis) for each of the scenarios (x-axis).Scores have been averaged for individuals within the PwD and supporters groups.Markers depict mean, shaded area represents standard error of the mean.</p>
<p>FIGURE</p>
<p>FIGUREOpinion scores on Likert scale (y-axis) for each of the features and priorities (x-axis).Scores have been averaged for individuals within the PwD and supporters groups.Markers depict mean, shaded area represents standard error of the mean.</p>
<p>TABLE State of the art Large Language Models by year and company.
./frdem..CreatorModelReleaseParametersContextReferenceNotesdatewindowAI21 LabsJambaMarch 202452B256kLieber et al., 2024Open-sourceAllen Institute forOLMoFebruary 20247B2048Groeneveld et al., 2024Open-source accessAIto model, weights,and training dataAnthropicClaude 2July 2023&gt;130B100kAnthropic, 2023aAnthropicClaude 2.1November 2023&gt;130B200kAnthropic, 2023bAnthropicClaude 3March 20243 different model200k to 1 millionAnthropic, 2024Multimodal: textsizes: Haikuand image input(20B), Sonnet(70B), and Opus(2T)AppleMM1March 2024-Up to 30BMcKinzie et al., 2024Multimodal: textand image inputBaiduErnie 4.0October 20234T (est.)1024Mo and Baptista, 2023CohereCommand-mediumDecember 20226B1024-CohereCommand-xlargeDecember 202250B4096-DatabricksDBRXMarch 2024132B32kMosaic AI Research Team,Open-source2024GoogleGemini Pro 1.5February 2024-128k -1 millionPichai and Hassabis, 2024Multimodal: text,image and videoinputGoogleGemmaFebruary 20242B, 7B8192Banks and Warkentin, 2024Open-sourceGoogleLaMDA 2May 2022540B1024Ghahramani, 2022Both text andimages as inputGooglePaLM 2May 2023340B8192 (text-bison)Anil et al., 2023MetaLlamaFebruary 20237B, 13B, 33B, 65B2048Touvron et al., 2023Open-sourceMetaLlama 2July 20237B, 13B, 70B4096Meta, 2023bOpen-sourceMetaLlama 3April 20248B, 70B, 400B8192Meta, 2024Open-sourceMicrosoftOrca-2November 20237B, 13B2048Mitra et al., 2023MicrosoftPhi-2November 20232.7B1024Javaheripi and Bubeck, 2023Small LanguageModelMistralSmall, LargeFebruary 2024-32kMistral AI, 2024MistralMistral 7BSeptember 20237B4096Mistral AI, 2023aOpen-sourceMistralMixtral 8x7BDecember 202356B32kMistral AI, 2023bOpen-sourceOpenAIChatGPTNovember 2022175B4096OpenAI, 2022OpenAIGPT-4March 20231.7T (est.)8192OpenAI, 2023OpenAIGPT-4 TurboOctober 2023-128kTechnologyFalconJune 20231.3B, 7.5B, 40B,2048von Werra et al., 2023Open-sourceInnovation Institute180BxAIGrok 1March 2024314B8192xAI, 2024aOpen-sourcexAIGrok-1.5March 2024-128kxAI, 2024b</p>
<p>built Elizabot,
./frdem..a language model that mimics a reminiscence therapist. It consistsof two components, a model that analyzes and captions theimages used in the therapy, and a model for simple conversations.The authors received positive feedback from PwD trialing itsuse. Similarly, Morales-de-Jesús et al. (2021) implemented anautomated reminiscence model. It was integrated within a speech-enacted virtual avatar and people with Alzheimer's disease trialingthe system gave it an overall score of 4.18/5, indicating highlevels of satisfaction. It is worth stressing that both studies didnot use state of the art models such as GPT-4. State of the artmodels are likely to have higher image captioning and conversationabilities, with potentially positive knock-on effects in the qualityof reminiscence therapy. In line with this, Raile (2024) highlightedChatGPT's usefulness both for complementing psychotherapy andas a first stop for people with mental health problems who havenot sought help yet, though concerns remain regarding biases andone-sided information. Furthermore, cognitive behavioral therapyhas shown promising results in treating anxiety and depression indementia (Tay</p>
<p>TABLE Overview over the questions used in the questionnaire.Imagine the AI can help you read letters and messages.You simply take a photo of the letter or copy the text into an App.The AI will explain in simple terms what the letter or message means.You can even ask questions about it.
./frdem..TABLE (Continued)Section SectionID IDItem ItemAnswer type Answer typeDemographics Application scenarios1.1 4.6Age Writing aid. Imagine the AI can help you draftNumber 5-points Likert scaleDemographics1.2Sex letters and messages. You simply give it an instruction such as "Write an email to my doctorMultiple choice (Male, Female, Other)Demographics1.3Ethnic background asking to shift our appointment to next week" and it will give you a nicely written email draft.Multiple choice (Indigenous, Asian, European, African, Pacific Islander, Mixed background)Demographics Application scenarios1.4 4.7What is the highest education level you achieved? as reminiscence therapy  Reminiscence therapy involves discussing events conversation-based therapeutic interventions such Therapy aid. Imagine the AI is able to carryPostgraduate) education, Vocational training" BSc, Multiple choice (Primary education, Secondary 5-points Likert scaleDemographics1.5Have you ever been diagnosed with dementia or and experiences from the past and aims to evoke memories, stimulate mental activity and improve a Alzheimer's disease? person's well-being. Reminiscence can often beMultiple choice (Yes, No)Demographics1.6How many years ago have you been diagnosed supported by props such as videos, music, picturesNumberwith dementia? and objects that may have particular meaning forDementia (follow-up)2.1What specific type of dementia have you been an individual.Multiple choice (Alzheimer's, Lewy bodyApplication scenarios4.8diagnosed with? Do you have any comments regarding thesedementia, Vascular dementia, Fronto-temporal Free textapplication scenarios? Can you think of any otherdementia)Dementia (follow-up)2.2Could you describe any symptoms or experiences related to your diagnosis? application scenarios not mentioned here? (feel free to skip this question if 'no')Free textAI experience Features and priorities3.1 5.1Have you ever used digital apps in the context of Ease of use. How important is it that the app is intuitive and easy to use, without the need to go dementia management or treatment? through tutorials or receive an introduction by a5-points Likert scale (Very important, Important, Multiple choice (Yes, No) Moderately important, Slightly important, Not important)AI experience3.2Before starting this questionnaire, had you heard family member or caregiver?Multiple choice (Yes, No)Features and priorities5.2of AI Language Models such as Chat-GPT? Voice control. How important is it that you can5-points Likert scaleAI experience3.3Did you ever use AI Language Models such as also use your voice to talk to the app and it talks back to you (as opposed to just typing text in a Chat-GPT (for either personal or professional use)? textbox)?Multiple choice (Yes, No)AI experience (follow-up) Features and priorities3.4 5.3Please briefly describe how you used AI language empathy, feelings, and understanding? models. app, how important is it that the AI displays Empathy. When having a conversation with theFree text 5-points Likert scaleApplication scenarios Features and priorities4.1 5.4Companionship. Imagine your app includes a chat option. You can chat with the AI about daily or private matters, questions and concerns. Your such an App useful for yourself? Appalone? with others. To what extent could you consider a caregiver or doctor, rather than just using the conversation is confidential and will not be shared use the Apptogether with in-person sessions with therapeutic interventions, how important is it to Human in the loop. When using the app for5-points Likert scale (Very useful, Useful, all) Moderately useful, Slightly useful, Not useful at 5-points Likert scaleApplication scenarios Features and priorities4.2 5.5Dementia-related information. Imagine the AI is ask the AI questions about dementia and you can gender, past conversations)? knowledgeable in the dementia literature. You can stores as little personal data as possible (e.g., age, Data privacy. How important is it that the app5-points Likert scale 5-points Likert scaleFeatures and priorities5.6have a natural conversation in which it provides your personal medical record, so it can only answer collects about you? treatment etc. However, it does not have access to app is transparent and clear about which data it information about dementia diagnosis, care, Data transparency. How important is it that the5-points Likert scaleFeatures and priorities5.7general questions. Data deletion. How important is it that your5-points Likert scaleApplication scenarios4.3Dementia-related information including personal data. Imagine the AI is knowledgeable in personal data can be deleted from the app at any time?5-points Likert scaleFeatures and priorities5.8the dementia literature. You can ask the AI Device. When using the app, which device(s) doMultiple choice (Smartphone, Tablet, Laptop orquestions about dementia and you can have a you prefer (select 1 or more)PC)Conclusion6.1natural conversation in which it provides Impact. What do you estimate the impact of AI on information about dementia diagnosis, care, treatment etc. The AI also has access to your dementia management and care could be?5-points Likert scale (Very positive, Positive, Neutral, Negative, Very negative)Conclusion6.2medical data and it can provide answers tailored to Comments. If you have any comments, thoughts your specific medical conditions. or suggestions, you can share them with us here.Application scenarios4.4Navigation. Imagine the AI is connected to a5-points Likert scalenavigation system (such as Google Maps or AppleMaps). It can give you directions in spokenlanguage and can help you out if you lose yourway.Application scenarios4.5Reading aid. 5-points Likert scale(Continued)Frontiers in Dementiafrontiersin.org
* .*</p>
<p>TABLE Summary of the feedback of the respondents to the application scenarios.
./frdem..</p>
<p>None of them relate to alleviating the problems of daily living [...]" D "What I really need is something that tells me step by step (all 176 of therm) how to live my daily life" D "I want raw, accurate information that I can easily verify, not a cozy chat.The whole idea of that side of AI is anathema to me" D "As a former carer of someone with Alzheimer's I can see very little use for this app except for navigating IF the person is out alone."D"Mycognitive faculties are relatively intact, it's my recall that is impaired" S "Tech is NOT the panacea that those who advocate it believe."ConcernaboutbiasD "dementia care [...] is infected with assumptions of ageism [...] and a host of other biases that are likely to show up in AI development"</p>
<p>D "</p>
<p>Frontiers in Dementia frontiersin.org
Data availability statementThe raw data and a Jupyter notebook reproducing the results of the questionnaire are available in our GitHub repository (https:// github.com/treder/LLMs-for-dementia).FundingThe author(s) declare financial support was received for the research, authorship, and/or publication of this article.This study was funded by the "Longitude Prize on Dementia", a research and development program by Alzheimer's Society and Innovate UK.We would also like to thank Dementia Australia (www.dementia.org.au) and Alzheimer's Society (https://www.alzheimers.org.uk/) for their support in recruiting participants for our questionnaire.KT was supported by Fellowship awards from the Guarantors of Brain (G101149) and Alzheimer's Society, UK (grant number 602).Ethics statementThe studies involving humans were approved by the School of Computer Science and Informatics Research Ethics Committee at Cardiff University, United Kingdom (reference code COMSC/Ethics/2023/122).The studies were conducted in accordance with the local legislation and institutional requirements.The participants provided their written informed consent to participate in this study.Author contributionsMT: Conceptualization, Data curation, Investigation, Formal analysis, Methodology, Visualization, Writing -review &amp; editing, Writing -original draft.SL: Investigation, Writing -review &amp; editing, Writing -original draft.KT: Writing -review &amp; editing, Writing -original draft.Conflict of interestSL was employed by Olive AI Limited.The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.The author(s) declared that they were an editorial board member of Frontiers, at the time of submission.This had no impact on the peer review process and the final decision.Publisher's noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Supplementary materialThe Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frdem.2024.1385303/full#supplementary-material
Alzheimer's disease facts and figures 2023. 10.1002/alz.13016Alzheimers Dement. 192023Alzheimer's Disease Facts and Figures</p>
<p>Multimodal data fusion of deep learning and dynamic functional connectivity features to predict alzheimer's disease progression. A Abrol, Z Fu, Y Du, V D Calhoun, 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). Presented at the 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). BerlinIEEE2019</p>
<p>Predicting dementia from spontaneous speech using large language models. F Agbavor, H Liang, 10.1371/journal.pdig.0000168PLOS Digit. Health. 1e00001682022</p>
<p>ChatGPT as an educational tool: opportunities, challenges, and recommendations for communication, business writing, and composition courses. M A Alafnan, S Dishari, M Jovic, K Lomidze, 10.37965/jait.2023.0184J. Artif. Intell. Technol. 32023</p>
<p>A new diagnostic approach for the identification of patients with neurodegenerative cognitive complaints. S Al-Hameed, M Benaissa, H Christensen, B Mirheidari, D Blackburn, M Reuber, 10.1371/journal.pone.0217388PLOS ONE. 14e02173882019</p>
<p>Artificial intelligence in scientific writing: a friend or a foe?. S Altmäe, A Sola-Leyva, A Salumets, 10.1016/j.rbmo.2023.04.009Reprod. Biomed. Online. 472023</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, 2023</p>
<p>10.48550/arXiv.2305.10403PaLM 2 technical report. WWW Document2023aPreprint</p>
<p>Introducing Claude 2.1 [WWW Document. 19 December, 2023. 19 December, 2023. 2024AnthropicAnthropic (2023b). Introducing the next generation of Claude [WWW Document</p>
<p>. Available. 6 April, 2024</p>
<p>. M J Armstrong, N Bedenfield, M Rosselli, R E Curiel Cid, M Kitaigorodsky, </p>
<p>Best practices for communicating a diagnosis of dementia. J E Galvin, 10.1212/CPJ.0000000000200223Neurol. Clin. Pract. 14e2002232024</p>
<p>Using Twitter to examine stigma against people with dementia during COVID-19: infodemiology study. J.-D Bacsu, S Fraser, A L Chasteen, A Cammer, K S Grewal, L E Bechard, 10.2196/35677JMIR Aging. 5e356772022</p>
<p>Machine learning for brain age prediction: introduction to methods and clinical applications. L Baecker, R Garcia-Dias, S Vieira, C Scarpazza, A Mechelli, 10.1016/j.ebiom.2021.103600202172103600</p>
<p>Alzheimer's disease recognition from spontaneous speech using large language models. J.-U Bang, S.-H Han, B.-O Kang, 10.4218/etrij.2023-0356ETRI J. 462024</p>
<p>Gemma: Introducing New State-of-the-Art Open Models. J Banks, T Warkentin, 2024. April, 2024</p>
<p>Emerging concepts in vascular dementia: a review. S Barocas, M Hardt, A Narayanan, S C Bir, M W Khan, V Javalkar, E G Toledo, R E Kelley, 10.1016/j.jstrokecerebrovasdis.2021.105864Fairness and Machine Learning. Cambridge, MAMIT Press2023. 202130105864</p>
<p>A Birhane, V U Prabhu, E Kahembwe, 10.48550/arXiv.2110.01963Multimodal datasets: misogyny, pornography, and malignant stereotypes. 2021Preprint</p>
<p>Language (Technology) is Power: A Critical Survey of "Bias" in NLP. S L Blodgett, S Barocas, Iii Daumé, H Wallach, H , TetreaultProceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Presented at the ACL 2020. D Jurafsky, J Chai, N Schluter, StroudsburgAssociation for Computational Linguistics2020</p>
<p>Artificial intelligence for diagnostic and prognostic neuroimaging in dementia: a systematic review. R J Borchert, T Azevedo, A Badhwar, J Bernal, M Betts, R Bruffaerts, 10.1002/alz.13412Alzheimers Dement. 192023</p>
<p>Can ChatGPT empower people with dyslexia?. B Botchu, I P Karthikeyan, R Botchu, 10.1080/17483107.2023.2256805Disabil. Rehabil. Assist. Technol. 02023</p>
<p>Dementia detection from speech: what if language models are not the answer? Information. M Bouazizi, C Zheng, S Yang, T Ohtsuki, 10.3390/info15010002202415</p>
<p>AI Could Detect Dementia Years Before Symptoms Appear. C Brierley, 2021WWW Document</p>
<p>. Univ, Camb, 19 November, 2023</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, 10.48550/arXiv.2303.12712Sparks of artificial general intelligence: early experiments with GPT-4. 2023Preprint</p>
<p>. D Bzdok, A Thieme, O Levkovskyy, P Wren, T Ray, S Reddy, 2024</p>
<p>Data science opportunities of large language models for neuroscience and biomedicine. 10.1016/j.neuron.2024.01.016Neuron. 112</p>
<p>Automatic reminiscence therapy for dementia. M Carós, M Garolera, P Radeva, X Giro-I-Nieto, Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR'20. the 2020 International Conference on Multimedia Retrieval, ICMR'20New York, NYAssociation for Computing Machinery2020</p>
<p>Benchmarking large language models in retrieval-augmented generation. J Chen, H Lin, X Han, L Sun, 10.1609/aaai.v38i16.29728Proc. AAAI Conf. AAAI Conf202438</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P Pinto, O De, 10.48550/arXiv.2107.03374Evaluating large language models trained on code. 2021Preprint</p>
<p>The now and future of ChatGPT and GPT in psychiatry. S.-W Cheng, C.-W Chang, W.-J Chang, H.-W Wang, C.-S Liang, T Kishimoto, 10.1111/pcn.13588Psychiatry Clin. Neurosci. 772023</p>
<p>Text mining of adverse events in clinical trials: deep learning approach. D Chopard, M S Treder, P Corcoran, N Ahmed, C Johnson, M Busse, 10.2196/28632JMIR Med. Inform. 9e286322021</p>
<p>Investigating the impact of user trust on the adoption and use of ChatGPT: survey analysis. A Choudhury, H Shamszare, 10.2196/47184J. Med. Internet Res. 25e471842023</p>
<p>Prevalence, characteristics, and survival of frontotemporal lobar degeneration syndromes. J Coda-Forno, K Witte, A K Jagadish, M Binz, Z Akata, E Schulz, I T S Coyle-Gilchrist, K M Dick, K Patterson, P Vázquez Rodríquez, E Wehmann, A Wilcox, 10.1212/WNL.0000000000002638doi: 10.1212/WNL.0000000000002638Neurology. 862023. 2016Inducing anxiety in large language models increases exploration and bias</p>
<p>Hallucinate' Chosen as Cambridge Dictionary's Word of the Year. London: The Guardian. E Creamer, K Dangerfield, Katherine , W , This AI scan may help find it -National | Globalnews.ca. 2023. 2023. 19 November, 2023online at1st signs of Alzheimer's may be detected in your eyes</p>
<p>Sarah Silverman is suing OpenAI and Meta for copyright infringement. W Davis, 2023. 25 November, 2023in The Verge. Available online at</p>
<p>Automatic detection of cognitive impairment in elderly people using an entertainment chatbot with Natural Language Processing capabilities. A P Dawid, F De Arriba-Pérez, S García-Méndez, F J González-Castaño, E Costa-Montenegro, S Ritchie, C W , Luz , S , 10.3233/JAD-200888doi: 10.3233/JAD-200888de la Fuente Garcia. 1982. 2023. 202077J. Alzheimers Dis. JAD</p>
<p>Evaluation of a digital companion for older adults with mild cognitive impairment. G Demiris, H J Thompson, A Lazar, S.-Y Lin, D Demszky, D Yang, D S Yeager, C J Bryan, M Clapper, S Chandhok, 10.1038/s44159-023-00241-5AMIA. Annu. Symp. Proc. 2016. 2017. 20232Using large language models in psychology</p>
<p>Implementation of cognitive behavioral therapy in e-mental health apps: literature review. K Denecke, N Schmid, S Nüssli, 10.2196/27791J. Med. Internet Res. 24e277912022</p>
<p>MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots. G Deng, Y Liu, Y Li, K Wang, Y Zhang, Z Li, Proceedings 2024 Network and Distributed System Security Symposium. Presented at the Network and Distributed System Security Symposium. 2024 Network and Distributed System Security Symposium. Presented at the Network and Distributed System Security SymposiumSan Diego, CAInternet Society2024</p>
<p>Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. A Deshpande, V Murahari, T Rajpurohit, A Kalyan, K Narasimhan, J Dhamala, T Sun, V Kumar, S Krishna, Y Pruksachatkun, K Chang, 2023</p>
<p>BOLD: dataset and metrics for measuring biases in openended language generation. W , Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT'21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT'21New York, NYAssociation for Computing Machinery2021</p>
<p>. S Dhuliawala, M Komeili, J Xu, R Raileanu, X Li, A Celikyilmaz, 2023</p>
<p>Chain-of-verification reduces hallucination in large language models. 10.48550/arXiv.2309.11495Preprint</p>
<p>A deep learning model to predict a diagnosis of Alzheimer disease by using 18F-FDG PET of the brain. Y Ding, J H Sohn, M G Kawczynski, H Trivedi, R Harnish, N W Jenkins, 10.1148/radiol.2018180958Radiology. 2902019</p>
<p>Mobile phone use by people with mild to moderate dementia: uncovering challenges and identifying opportunities: mobile phone use by people with mild to moderate dementia. E Dixon, R Michaels, X Xiao, Y Zhong, P Clary, A Narayanan, 10.1145/3517428.3544809Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS'22. the 24th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS'22New York, NYAssociation for Computing Machinery2022</p>
<p>Hard choices in artificial intelligence. R Dobbe, T Krendl Gilbert, Y Mintz, 10.1016/j.artint.2021.103555Artif. Intell. 3001035552021</p>
<p>MARIO project: validation and evidence of service robots for older people with dementia. G D'onofrio, D Sancarlo, M Raciti, M Burke, A Teare, T Kovacic, 10.3233/JAD-181165J. Alzheimers Dis. 682019</p>
<p>What does ChatGPT know about dementia? A comparative analysis of information quality. J A Dosso, J N Kailley, J M Robillard, 10.3233/JAD-230573J. Alzheimers Dis. 972023</p>
<p>Classification and prediction of brain disorders using functional connectivity: promising but challenging. Y Du, Z Fu, V D Calhoun, 10.3389/fnins.2018.00525Front. Neurosci. 125252018</p>
<p>Shall I compare thee. . . to a robot? An exploratory pilot study using participatory arts and social robotics to improve psychological well-being in later life. L Eliot, A Coston, A Gandhi, N Chouldechova, A Putnam-Hornstein, E Steier, D , 10.1080/13607863.2019.16990162023 ACM Conference on Fairness, Accountability, and Transparency. Presented at the FAccT'23: the 2023 ACM Conference on Fairness, Accountability, and Transparency. E Ferrara, ChicagoACM2023. February 22, 2024. 2023. 2023. 202125People Are Eagerly Consulting Generative AI ChatGPT For Mental Health Advice, Stressing Out AI Ethics And AI Law</p>
<p>Determining the impact of psychosis on rates of false-positive and falsenegative diagnosis in Alzheimer's disease. C E Fischer, W Qian, T A Schweizer, Z Ismail, E E Smith, C P Millikin, 10.1016/j.trci.2017.06.001Alzheimers Dement. Transl. Res. Clin. Interv. 32017</p>
<p>Copyright in generative deep learning. L Floridi, L ; Floridi, M Musolesi, 10.1017/dap.2022.10The Ethics of Artificial Intelligence: Principles, Challenges, and Opportunities. Oxford, New York; Franceschelli, GOxford University Press2023. 20224e17</p>
<p>Will Humans-in-The-Loop Become Borgs? Merits and Pitfalls of Working with AI. A Fügener, J Grahl, A Gupta, W Ketter, Management Information Systems Quarterly (MISQ). 202145</p>
<p>Artificial intelligence, values, and alignment. Minds Mach. I Gabriel, 10.1007/s11023-020-09539-2202030</p>
<p>I O Gallegos, R A Rossi, J Barrow, M M Tanjim, S Kim, F Dernoncourt, 10.48550/arXiv.2309.00770Bias and fairness in large language models: a survey. 2023Preprint</p>
<p>The capacity for moral self-correction in large language models. D Ganguli, A Askell, N Schiefer, T I Liao, K Lukošiute, A Chen, 10.48550/arXiv.2302.074592023Preprint</p>
<p>Predictability and Surprise in Large Generative Models. D Ganguli, D Hernandez, L Lovitt, A Askell, Y Bai, A Chen, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT'22. the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT'22New York, NYAssociation for Computing Machinery2022</p>
<p>Activity monitoring and location sensory system for people with mild cognitive impairments. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, 10.1109/JSEN.2023.3239980IEEE Sens. J. 232024. 2023. 2022. 19 December, 2023Understanding the world through language. in The Keyword. Available online at</p>
<p>Introducing PaLM 2 [WWW Document. Z Ghahramani, 2023. July, 2023</p>
<p>J Giorgio, W J Jagust, S Baker, S M Landau, P Tino, Z Kourtzi, 10.1101/2020.08.15.252601Predicting future regional tau accumulation in asymptomatic and early alzheimer's disease. 2020Preprint</p>
<p>Open source LLMs could make artificial intelligence more dangerous, says "godfather" of AI. M Gooding, Tech Monit. Available online at. 2023. 10 December, 2023</p>
<p>LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia. S M Goodman, E Buehler, P Clary, A Coenen, A Donsbach, T N Horne, Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility. Presented at the ASSETS'22: The 24th International ACM SIGACCESS Conference on Computers and Accessibility. the 24th International ACM SIGACCESS Conference on Computers and Accessibility. Presented at the ASSETS'22: The 24th International ACM SIGACCESS Conference on Computers and AccessibilityAthensACM2022</p>
<p>. K Greshake, S Abdelnabi, S Mishra, C Endres, T Holz, M Fritz, 2023</p>
<p>Not what you've signed up for: compromising real-world llm-integrated applications with indirect prompt injection. 10.48550/arXiv.2302.12173Preprint</p>
<p>Large Language Models respond to Influence like Humans. L Griffin, B Kleinberg, M Mozes, K Mai, M D M Vau, M Caldwell, Proceedings of the First Workshop on Social Influence in Conversations. K Chawla, W Shi, the First Workshop on Social Influence in ConversationsToronto, ONAssociation for Computational Linguistics2023. SICon 2023Presented at the SICon 2023</p>
<p>D Groeneveld, I Beltagy, P Walsh, A Bhagia, R Kinney, O Tafjord, 10.48550/arXiv.2402.00838OLMo: accelerating the science of language models. 2024Preprint</p>
<p>A responsible path to generative AI in healthcare. A Gupta, A Waldron, Google Cloud Blog. 2023. 20 December, 2023</p>
<p>Using a robotic cat in dementia care: a pilot study. C Gustafsson, C Svanberg, M Müllersdorf, 10.3928/00989134-20150806-44J. Gerontol. Nurs. 412015</p>
<p>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. Hardy. T Hagendorff, C J D Marshall, C R Golden, H L Clark, C N Mummery, C J , 2023</p>
<p>Biostatistics series module 4: comparing groupscategorical variables. T D Griffiths, 10.4103/0019-5154.185700doi: 10.4103/0019-5154.185700Indian J. Dermatol. 2632016. 2016J. Neurol.</p>
<p>Computerized cognitive training in older adults with mild cognitive impairment or dementia: a systematic review and meta-analysis. A Lampit, 10.1176/appi.ajp.2016.16030360Am. J. Psychiatry. 1742017</p>
<p>Feelings of loneliness, but not social isolation, predict dementia onset: results from the Amsterdam Study of the Elderly (AMSTEL). T J Holwerda, D J H Deeg, A T F Beekman, T G Tilburg, M L Van, Stek, C Jonker, 10.1136/jnnp-2012-302755J. Neurol. Neurosurg. Psychiatry. 852014</p>
<p>Five sources of bias in natural language processing. D Hovy, S Prabhumoye, 10.1111/lnc3.12432Lang. Linguist. Compass. 15e124322021</p>
<p>. V Hristidis, N Ruggiano, E L Brown, S R R Ganta, S Stewart, 2023</p>
<p>ChatGPT vs google for queries related to dementia and other cognitive decline: comparison of results. 10.2196/48966J. Med. Internet Res. 25e48966</p>
<p>Bard Updates from Google I/O 2023: Images. S Hsiao, 2023. accessed 21 October, 2023</p>
<p>ChatGPT sets record for fastest-growing user base -analyst note. K Hu, 10.48550/arXiv.2212.10403Towards reasoning in large language models: a survey. J Chang, K C , -C , Toronto, ON2023. 2023Reuters. Preprint</p>
<p>Catastrophic jailbreak of open-source LLMs via exploiting generation. Y Huang, S Gupta, M Xia, K Li, D Chen, 10.48550/arXiv.2310.069872023Preprint</p>
<p>Dementia and cognitive impairment: epidemiology, diagnosis, and treatment. J Hugo, M Ganguli, 10.1016/j.cger.2014.04.001Clin. Geriatr. Med. 302014</p>
<p>. C Iadecola, M Duering, V Hachinski, A Joutel, S T Pendlebury, J Schneider, </p>
<p>NIA-AA research framework: toward a biological definition of Alzheimer's disease. A , AI Team10.1016/j.jalz.2018.02.018doi: 10.1016/j.jalz.2018.02.018IBM Data and. 2019. 2023. accessed October 12, 2023. 201873J. Am. Coll. Cardiol</p>
<p>Phi-2: the surprising power of small language models. M Javaheripi, S Bubeck, 2023. accessed December 20, 2023in Microsoft Res. Available online at</p>
<p>ChatGPT for healthcare services: an emerging stage for an innovative perspective. M Javaid, A Haleem, R P Singh, 10.1016/j.tbench.2023.100105BenchCouncil Trans. Benchmarks Stand. Eval. 31001052023</p>
<p>. F Jiang, Z Xu, L Niu, Z Xiang, B Ramasubramanian, B Li, 2024</p>
<p>ArtPrompt: ASCII art-based jailbreak attacks against aligned LLMs. 10.48550/arXiv.2402.11753Preprint</p>
<p>Forcing generative models to degenerate ones: the power of data poisoning attacks. S Jiang, S Kadhe, Y Zhou, L Cai, N Baracaldo, 10.48550/arXiv.2312.04748the neurips 2023 workshop on backdoors in deep learning -the good, the bad, and the ugly. 2023Preprint</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Z Jiang, J Araki, H Ding, G Neubig, 10.48550/arXiv.2012.009552021Preprint</p>
<p>Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer's disease using EEG technology. B Jiao, R Li, H Zhou, K Qing, H Liu, H Pan, 10.1186/s13195-023-01181-1Alzheimers Res. Ther. 15322023</p>
<p>Sex differences in healthspan predict lifespan in the 3xTg-AD mouse model of Alzheimer's disease. A T Kalai, S S Vempala, A E Kane, S Shin, A A Wong, E Fertan, N S Faustova, S E Howlett, 10.3389/fnagi.2018.00172Front. Aging Neurosci. 101722023. 2018Calibrated Language Models Must Hallucinate</p>
<p>Attention-based clinical note summarization. N Kanwal, G Rizzo, 10.1145/3477314.3507256Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing, SAC'22. the 37th ACM/SIGAPP Symposium on Applied Computing, SAC'22New YorkAssociation for Computing Machinery2022</p>
<p>Impact of multiple pathologies on the threshold for clinically overt dementia. A Kapasi, C Decarli, J A Schneider, 10.1007/s00401-017-1717-7Acta Neuropathol. 1342017</p>
<p>Copyright violations and large language models. A Karamolegkou, J Li, L Zhou, A Søgaard, 10.18653/v1/2023.emnlp-main.458Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, </p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. F Fischer, 10.1016/j.lindif.2023.102274Learn. Individ. Differ. 1031022742023</p>
<p>Reminiscence therapy in the treatment of depression in the elderly: current perspectives. A Khan, A Bleth, M Bakpayev, N Imtiaz, 10.3390/jal2010004J. Ageing Longev. 22022</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, 10.1007/s11042-022-13428-4Multimed. Tools Appl. 822023</p>
<p>. K Koebel, M Lacayo, M Murali, I Tarnanas, A Çöltekin, 2022</p>
<p>Expert insights for designing conversational user interfaces as virtual assistants and companions for older adults with cognitive impairments. T Følstad, S Araujo, E Papadopoulos, E Law, M Luger, P B Goodwin, Brandtzaeg, Chatbot Research and Design. Lecture Notes in Computer Science. ChamSpringer International Publishing</p>
<p>Evaluating the performance of large language models: ChatGPT and Google Bard in generating differential diagnoses in clinicopathological conferences of neurodegenerative disorders. S Koga, N B Martin, D W Dickson, 10.1111/bpa.13207Brain Pathol. 34e132072024</p>
<p>The patients' and caregivers' perspective: In-hospital navigation aids for people with dementia-a qualitative study with a value sensitive design approach. A Kowe, S Köhler, D Görß, S Teipel, 10.1080/10400435.2021.2020378Assist. Technol. 352023</p>
<p>The assessment of language and communication in dementia: a synthesis of evidence. L Krein, Y.-H Jeon, A M Amberber, J Fethney, 10.1016/j.jagp.2018.11.009Am. J. Geriatr. Psychiatry. 272019. 2018</p>
<p>Visual impairment, eye diseases, and dementia risk: a systematic review and meta-analysis. E Kuzma, T J Littlejohns, A P Khawaja, D J Llewellyn, O C Ukoumunne, U Thiem, 10.3233/JAD-210250J. Alzheimers Dis. 832021</p>
<p>Alzheimer's disease. C A Lane, J Hardy, J M Schott, 10.1111/ene.13439Eur. J. Neurol. 252018</p>
<p>Artificial intelligence for mental health care: clinical applications, barriers, facilitators, and artificial wisdom. A Lauscher, T Lueken, G Glavaš, M F Moens, X Huang, L Specia, S W Yih ; Lee, E E Torous, J De Choudhury, M Depp, C A Graham, S A Kim, H.-C , 10.1016/j.bpsc.2021.02.001Findings of the Association for Computational Linguistics: EMNLP 2021 Presented at the Findings 2021. Punta CanaAssociation for Computational Linguistics2021. 20216Sustainable modular debiasing of language models</p>
<p>Do we still need clinical language models?. E Lehman, E Hernandez, D Mahajan, J Wulff, M J Smith, Z Ziegler, in: proceedings of the conference on health, inference, and learning. New YorkPMLR2023Presented at the Conference on Health, Inference, and Learning</p>
<p>Applications of artificial intelligence to aid early detection of dementia: a scoping review on current capabilities and future directions. R Li, X Wang, K Lawler, S Garg, Q Bai, J Alty, 10.1016/j.jbi.2022.104030J. Biomed. Inform. 1271040302022</p>
<p>Two directions for clinical data generation with large language models: data-to-label and label-to-data. R Li, X Wang, H Yu, Y Li, Y Zhang, 10.48550/arXiv.2305.18569Proc. Conf. Empir. Methods Nat. Lang. Process. Conf. Empir. Methods Nat. Lang. Process. Conf. Empir. Methods Nat. Lang. ess. Conf. Empir. Methods Nat. Lang. ess2023. 2023Fairness of ChatGPT. Preprint</p>
<p>Ethics of Artificial Intelligence. S M Liao, O Lieber, B Lenz, H Bata, G Cohen, J Osin, I Dalmedigos, 10.48550/arXiv.2403.19887Jamba: a hybrid transformer-mamba language model. OxfordOxford University Press2020. 2024Preprint</p>
<p>Conversational affective social robots for ageing and dementia support. M R Lima, M Wairagkar, M Gupta, F Rodriguez Y Baena, P Barnaghi, D J Sharp, 10.1109/TCDS.2021.3115228IEEE Trans. Cogn. Dev. Syst. 142022</p>
<p>TruthfulQA: measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, 10.48550/arXiv.2109.079582022Preprint</p>
<p>Writing with ChatGPT: an illustration of its capacity, limitations and implications for academic writers. L Lingard, 10.5334/pme.1072Perspect. Med. Educ. 122023</p>
<p>N F Liu, T Zhang, P Liang, 10.48550/arXiv.2304.09848Evaluating verifiability in generative search engines. 2023Preprint</p>
<p>Dementia prevention, intervention, and care: 2020 report of the Lancet Commission. Y Liu, G Deng, Y Li, K Wang, T Zhang, Y Liu, 10.1016/S0140-6736(20)30367-6The Lancet. 3962023. 2020Prompt Injection Attack Against LLM-Integrated Applications</p>
<p>A robust framework to investigate the reliability and stability of explainable artificial intelligence markers of Mild Cognitive Impairment and Alzheimer's Disease. R Loconte, G Orrù, M Tribastone, P Pietrini, G Sartori, A Lombardi, D Diacono, N Amoroso, P Biecek, A Monaco, L Bellantuono, 10.1186/s40708-022-00165-5Brain Inform. 9172023. 2022Challenging ChatGPT "Intelligence" with Human Tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language Model</p>
<p>. B D Lund, T Wang, N R Mannuru, B Nie, S Shimray, Z Wang, 2023</p>
<p>ChatGPT and a new academic reality: artificial intelligence-written research papers and the ethics of the large language models in scholarly publishing. 10.1002/asi.24750J. Assoc. Inf. Sci. Technol. 74</p>
<p>Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. Z Ma, Y Mei, Z Su, AMIA Annu. Symp. Proc. 20232024</p>
<p>AD-BERT: Using pre-trained language model to predict the progression from mild cognitive impairment to Alzheimer's disease. C Mao, J Xu, L Rasmussen, Y Li, P Adekkanattu, J Pacheco, 10.1016/j.jbi.2023.104442J. Biomed. Inform. 1441044422023</p>
<p>Google I/O 2023 unveils PaLM 2 large language model. C Mauran, 2023. 23 September, 2023WWW Document], " in Mashable. Available online at</p>
<p>. I Mckenzie, A Lyzhov, A Parrish, A Prabhu, A Mueller, N Kim, 10.48550/arXiv.2306.094792023Inverse-scaling/prize. arXiv [Preprint</p>
<p>B Mckinzie, Z Gan, J.-P Fauconnier, S Dodge, B Zhang, P Dufter, 10.48550/arXiv.2403.09611MM1: methods, analysis and insights from multimodal LLM pre-training. 2024Preprint</p>
<p>Machine learning, artificial intelligence and the prediction of dementia. A Merkin, R Krishnamurthi, O N Medvedev, 10.1097/YCO.0000000000000768Curr. Opin. Psychiatry. 352022</p>
<p>The imperative for regulatory oversight of large language models (or generative AI) in healthcare. B Meskó, E J Topol, 10.1038/s41746-023-00873-0NPJ Digit. Med. 62023</p>
<p>Introducing LLaMA: A Foundational, 65-Billion-Parameter Language Model. Meta. 2023a. accessed 21 October, 2023. 2023bMeta and Microsoft Introduce the Next Generation of Llama</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. A I Meta, accessed 20 December, 2023. 19 April, 2024Meta (2024). in Meta AI. Available online at</p>
<p>Recent advances in natural language processing via large pre-trained language models: a survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, 10.1145/3605943ACM Comput. Surv. 56402023</p>
<p>Who is GPT-3? An exploration of personality, values and demographics. M Miotto, N Rossberg, B Kleinberg, Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS). the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)Abu DhabiMistral AI2022. 2023a. 20 December, 2023Mistral 7B, " in Mistral AI. Available online at</p>
<p>Mixtral of experts. A I Mistral, Mistral AI. Available online at. 2023b. 20 December, 2023. 2024. March, 2024Mistral AI. Available online at</p>
<p>The debate over understanding in AI's large language models. M Mitchell, M Mitchell, D C Krakauer, 10.1073/pnas.2215907120doi: 10.1073/pnas.2215907120Proc. Natl. Acad. Sci. Natl. Acad. Sci2023. 2023381e2215907120How do we know how smart AI systems are?</p>
<p>A Mitra, L Del Corro, S Mahajan, A Codas, C Simoes, S Agarwal, 10.48550/arXiv.2311.11045Orca 2: teaching small language models how to reason. 2023Preprint</p>
<p>China's Baidu unveils new Ernie AI version to rival GPT-4. Y Mo, E Baptista, 2023ReutersLondon</p>
<p>Cognitive reserve in dementia: implications for cognitive training. S Mondini, I Madella, A Zangrossi, A Bigolin, C Tomasi, M Michieletto, 10.3389/fnagi.2016.00084Front. Aging Neurosci. 8842016</p>
<p>Language disorders in dementia of the Alzheimer type. V Morales-De-Jesús, H Gómez-Adorno, M Somodevilla-García, D Vilariño, 10.1016/0093-934X(87)90064-2doi: 10.1016/0093-934X(87)90064-2Conversational System as assistant tool in reminiscence therapy for people with early-stage of Alzheimer's. Healthcare. B E Murdoch, H J Chenery, V Wilks, R S Boyle, 2021. 2024. June 4, 2024. 19879Brain Lang.</p>
<p>Redefining the multidimensional clinical phenotypes of frontotemporal lobar degeneration syndromes. A G Murley, I Coyle-Gilchrist, M A Rouse, P S Jones, W Li, J Wiggins, 10.1093/brain/awaa097Brain. 1432020</p>
<p>Global and regional projections of the economic burden of Alzheimer's disease and related dementias from 2019 to 2050: A value of statistical life approach. A Nandi, N Counts, S Chen, B Seligman, D Tortorice, D Vigo, 10.1016/j.eclinm.2022.101580202251</p>
<p>Scalable extraction of training data from (production) language models. M Nasr, N Carlini, J Hayase, M Jagielski, A F Cooper, D Ippolito, 10.48550/arXiv.2311.170352023Preprint</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, 10.48550/arXiv.2307.06435A comprehensive overview of large language models. 2023Preprint</p>
<p>Understanding public-stigma and self-stigma in the context of dementia: a systematic review of the global literature. T Nguyen, X Li, 10.1177/1471301218800122Dementia. 192020</p>
<p>Estimation of the global prevalence of dementia in 2019 and forecasted prevalence in 2050: an analysis for the Global Burden of Disease Study. E Nichols, J D Steinmetz, S E Vollset, K Fukutaki, J Chalek, F Abd-Allah, 10.1016/S2468-2667(21)00249-8Lancet Public Health. 72022. 2019. 2023. February 22, 2024Norwegian Consumer CouncilGhost in the Machine -Addressing the Consumer Harms of Generative AI</p>
<p>A study on the case of using ChatGPT and learners' perceptions in college liberal arts writing. S Oh, 10.46392/kjge.2023.17.3.11Korean J. Gen. Educ. 172023</p>
<p>N T Olney, S Spina, B L Miller, Frontotemporal dementia. Neurol2017</p>
<p>. 10.1016/j.ncl.2017.01.008Clin. 35</p>
<p>Fully automated cognitive screening tool based on assessment of speech and language. R P D O'malley, B Mirheidari, K Harkness, M Reuber, A Venneri, T Walker, 10.1002/alz.041980J. Neurol. Neurosurg. Psychiatry. 202020</p>
<p>Machine learning, sentiment analysis, and tweets: an examination of Alzheimer's disease stigma on Twitter. B Omarov, Z Zhumanov, A Gumar, L Kuntunova, N Oscar, P A Fox, R Croucher, R Wernick, J Keune, K Hooker, L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, 10.1093/geronb/gbx014arXiv:2303.08774v6doi: 10.1093GPT-4 technical report. 2023. 2022. 16 December, 2023. 2023. 2017. 202214J. Gerontol. Ser. B</p>
<p>Use of generative artificial intelligence, including large language models such as ChatGPT, in scientific publications: policies of KJR and prominent authorities. A Pal, L K Umapathi, M Sankarasubbu, 10.3348/kjr.2023.0643Adv. Neural Inf. Process. Syst. Park, S. H.352023. 2023Korean J. Radiol.</p>
<p>Globalising strategies to meet global challenges: the case of ageing and dementia. M A Parra, S Butler, W J Mcgeown, L A Brown Nicholls, D J Robertson, 10.7189/jogh.09.020310J. Glob. Health. 9203102019</p>
<p>Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities. F Patel, R Thakore, I Nandwani, S K Bharti, 10.1038/s41746-020-0304-92019 IEEE 16th India Council International Conference (INDICON). J K Paulus, D M Kent, RajkotIEEE2019. 20203Combating depression in students using an intelligent chatbot: a cognitive behavioral therapy</p>
<p>Introducing Gemini: Google's most capable AI model yet. S Pichai, D Hassabis, 2023. accessed December 17, 2023in The Keyword. Available online at</p>
<p>Our next-generation model: Gemini 1.5. S Pichai, D Hassabis, The Keyword. Available online at. 2024. february-2024/ (accessed February 22, 2024</p>
<p>Spatial navigation in older adults with mild cognitive impairment and dementia: a systematic review and meta-analysis. L Pillette, G Moreau, J.-M Normand, M Perrier, A Lécuyer, M Cogné, J De Almeida, C A B Ferreira, J V De Oliveira Silva, F Monteiro-Junior, R S Tangen, G G , 10.1016/j.exger.2022.111852doi: 10.1016/j.exger.2022.111852IEEE Trans. Vis. Comput. Graph. 291118522023. 2022Exp. Gerontol.</p>
<p>. M Prince, A Wimo, A Guerchet, G.-C Ali, Y.-T Wu, M Prina, 2015</p>
<p>Alzheimer's Disease International. The Global Impact of Dementia: An Analysis of Prevalence, Incidence, Cost and Trends. LondonADI2015</p>
<p>ChatGPT: a promising tool to combat social isolation and loneliness in older adults with mild cognitive impairment. P Qi, P Wu, S Miller, M I Joshi, P S Lee, J C Xue, C Ni, Y , 10.1038/s41467-022-31037-5Nat. Commun. 1334042023. 2022Multimodal deep learning for Alzheimer's disease dementia assessment</p>
<p>Genetics and molecular mechanisms of frontotemporal lobar degeneration: an update and future avenues. F Raffaele, M Claudia, H John, 10.1016/j.neurobiolaging.2019.02.006Neurobiol. Aging. 782019</p>
<p>Mitigating bias in algorithmic hiring: evaluating claims and practices. M Raghavan, S Barocas, J Kleinberg, K Levy, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT * '20. the 2020 Conference on Fairness, Accountability, and Transparency, FAT * '20New YorkAssociation for Computing Machinery2020</p>
<p>Digital technologies to prevent social isolation and loneliness in dementia: a systematic review. H K Rai, D Kernaghan, L Schoonmade, K J Egan, A M Pot, 10.3233/JAD-220438J. Alzheimers Dis. 902022</p>
<p>The usefulness of ChatGPT for psychotherapists and patients. P Raile, 2024</p>
<p>. 10.1057/s41599-023-02567-0Humanit. Soc. Sci. Commun. 11</p>
<p>Actionable auditing: investigating the impact of publicly naming biased performance results of commercial AI products. I D Raji, J Buolamwini, AAAI/ACM Conference on AI, Ethics, and Society, AIES'19. New York; Reisner, A.Association for Computing Machinery2019. 2019. 2023. accessed 25 November, 2023These 183,000 Books Are Fueling the Biggest Fight in Publishing and Tech</p>
<p>Artificial intelligence in dementia. A Richardson, C B Robbins, C E Wisely, R Henao, D S Grewal, S Fekrat, 10.1097/ICU.0000000000000881Curr. Opin. Ophthalmol. 332022</p>
<p>The political biases of ChatGPT. D Rozado, 10.3390/socsci12030148Soc. Sci. 121482023</p>
<p>Simple and steady interactions win the healthy mentality: designing a chatbot service for the elderly. H Ryu, S Kim, D Kim, S Han, K Lee, Y Kang, 10.1145/3415223Proc. ACM Hum.-Comput. Interact. 4252020</p>
<p>. H R Saeidnia, M Kozak, B D Lund, M Hassanzadeh, 2023</p>
<p>Evaluation of ChatGPT's Responses to Information Needs and Information Seeking of Dementia Patients. J A Schneider, Z Arvanitakis, W Bang, D A Bennett, 2007</p>
<p>Mixed brain pathologies account for most dementia cases in communitydwelling older persons. 10.1212/01.wnl.0000271090.28148.24Neurology. 69</p>
<p>Gender differences in caregiving among family -caregivers of people with mental illnesses. N Sharma, S Chakrabarti, S Grover, 10.5498/wjp.v6.i1.7World J. Psychiatry. 62016</p>
<p>Assessment scales in dementia. B Sheehan, 10.1177/1756285612455733Ther. Adv. Neurol. Disord. 53492012</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>. V Sorin, D Brin, Y Barash, E Konen, A Charney, G Nadkarni, 2023</p>
<p>Cognitive behavioural therapy can be effective in treating anxiety and depression in persons with dementia: a systematic review. Large Language, Models ; Llms, ) , Empathy -A Systematic Review, D Stamate, R Smith, R Tsygancov, R Vorobev, J Langham, D Stahl, 10.1111/psyg.12391doi: 10.1111/psyg.12391Artif. Intell. Appl. Innov. Tay, K.-W., Subramaniam, P., and Oei, T. P.5842020. 2024. 2019StatPearls PublishingPsychogeriatrics</p>
<p>Opportunities and challenges for ChatGPT and large language models in biomedicine and health. S Tian, Q Jin, L Yeganova, P.-T Lai, Q Zhu, X Chen, 10.1093/bib/bbad493Brief. Bioinform. 254932024</p>
<p>A speech recognition-based solution for the automatic detection of mild cognitive impairment from spontaneous speech. L Toth, I Hoffmann, G Gosztolya, V Vincze, G Szatloczki, Z Banreti, 10.2174/1567205014666171121114930Curr. Alzheimer Res. 152018</p>
<p>LLaMA: open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, 10.48550/arXiv.2302.139712023Preprint</p>
<p>. M S Treder, J P Shock, D J Stein, S Du Plessis, S Seedat, K Tsvetanov, </p>
<p>Correlation constraints for regression models: controlling bias in brain age prediction. A , 10.3389/fpsyt.2021.615754Front. Psychiatry. 126157542021</p>
<p>Applications of artificial intelligence in dementia research. K K F Tsoi, P Jia, N M Dowling, J R Titiner, M Wagner, A W Capuano, 10.1017/pcm.2022.10Camb. Prisms Precis. Med. 1e92023</p>
<p>Chatbots and conversational agents in mental health: a review of the psychiatric landscape. A N Vaidyam, H Wisniewski, J D Halamka, M S Kashavan, J B Torous, 10.1177/0706743719828977Can. J. Psychiatry. 642019</p>
<p>Adapted large language models can outperform medical experts in clinical text summarization. D Van Veen, C Van Uden, L Blankemeier, J.-B Delbrouck, A Aali, C Bluethgen, 10.1038/s41591-024-02855-5Nat. Med. 302024</p>
<p>Clinical text summarization: adapting large language models can outperform human experts. Res. Square rs.3.rs-3483777. D Van Veen, C Van Uden, L Blankemeier, J B Delbrouck, A Aali, C Bluethgen, 10.21203/rs.3.rs-3483777/v12023</p>
<p>Is loneliness a cause or consequence of dementia? A public health analysis of the literature. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, 10.3389/fpsyg.2020.612771Advances in Neural Information Processing Systems. C R , Red Hook, NYCurran Associates, Inc2017. 202111612771Attention is all you need</p>
<p>Google and Microsoft's chatbots are already citing one another in a misinformation shitshow. J Vincent, 2023. 28 November, 2023in The Verge. Available online at</p>
<p>Fine-tuning the BERTSUMEXT model for clinical report summarization. P Vinod, S Safar, D Mathew, P Venugopal, L M Joly, J George, L Werra, Y Belkada, S Mangrulkar, L Tunstall, O Dehaene, P Cuenca, 2020 International Conference for Emerging Technology (INCET). BelgaumIEEE2020. 2023. 20 December, 2023. 2023. January 1, 2024. 2023online atEnabling conversational interaction on mobile with LLMs</p>
<p>Aligning large language models with human: a survey. A Wei, N Haghtalab, J Steinhardt, 10.48550/arXiv.2307.02483doi: 10.48550/arXiv.2307.024832023Jailbroken: how does llm safety training fail?. Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, 10.48550/arXiv.2201.119032023. 2023. November 18, 2023Preprint</p>
<p>C Wu, W Lin, X Zhang, Y Zhang, Y Wang, W Xie, 10.48550/arXiv.2304.14454PMC-LLaMA: towards building open-source language models for medicine. 2023Preprint</p>
<p>. J Wu, L Ouyang, D M Ziegler, N Stiennon, R Lowe, J Leike, 2021</p>
<p>Open Release of Grok-1. 2024a. accessed April 6, 2024. April 6, 2024Recursively Summarizing Books with Human Feedback. xAIxAI (2024b). Announcing Grok-1.5, " in Xai Blog. Available online at</p>
<p>Tree of thoughts: deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, 10.48550/arXiv.2305.106012023Preprint</p>
<p>M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, 10.48550/arXiv.2310.01714Large language models as analogical reasoners. 2023Preprint</p>
<p>Cognitive mirage: a review of hallucinations in large language models. H Ye, T Liu, A Zhang, W Hua, W Jia, 10.48550/arXiv.2309.067942023Preprint</p>
<p>Leveraging pretrained models for automatic summarization of doctorpatient conversations. L Zhang, R Negrinho, A Ghosh, V Jagannathan, H R Hassanzadeh, T Schaaf, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta CanaAssociation for Computational Linguistics2021. 2015</p>
<p>Detection of subjects and brain regions related to Alzheimer's disease using 3D MRI scans based on eigenbrain and machine learning. 10.3389/fncom.2015.00066Front. Comput. Neurosci. 966</p>
<p>Siren's song in the ai ocean: a survey on hallucination in large language models. Y Zhang, Y Li, L Cui, D Cai, L Liu, T Fu, X Huang, 10.48550/arXiv.2309.012192023Preprint</p>
<p>H Zhao, H Chen, F Yang, N Liu, H Deng, H Cai, 10.48550/arXiv.2309.01029Explainability for large language models: a survey. 2023Preprint</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, 10.48550/arXiv.1909.08593Fine-tuning language models from human preferences. 2020Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
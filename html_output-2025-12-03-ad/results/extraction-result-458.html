<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-458 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-458</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-458</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-47021242</p>
                <p><strong>Paper Title:</strong> Deep code search</p>
                <p><strong>Paper Abstract:</strong> To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code. In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled. As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e458.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e458.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>lexical_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexical gap between natural language queries/descriptions and source code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The mismatch where high-level natural language intent (queries or documentation) uses different words, synonyms, or abstractions than the low-level tokens and identifiers found in source code, causing IR-style retrieval to miss semantically relevant code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepCS / CODEnn code search system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A deep learning based code search pipeline (CODEnn model + DeepCS tool) that embeds both natural language descriptions/queries and code snippets into a shared vector space to retrieve semantically related code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer queries and JavaDoc first-sentence summaries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java method implementations (method name, API invocation sequence, method body tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>lexical mismatch / heterogeneous modality gap</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural language queries or documentation use high-level verbs and nouns (e.g., "read", "object") while the relevant code may use different API calls or domain-specific identifiers (e.g., JAXB unmarshal, deserialize) and thus lacks overlapping lexical tokens; conventional IR approaches relying on textual overlap fail to match such cases.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>query-to-code mapping (retrieval / matching stage)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Analytical example-driven demonstration and empirical comparison of retrieval performance between IR baselines (Lucene, CodeHow) and DeepCS; qualitative examples (e.g., "read an object from an xml" vs a deserialize snippet) show failures of keyword matching.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured indirectly via retrieval metrics (FRank / SuccessRate@k / Precision@k / MRR) across a benchmark of 50 Stack Overflow real-world queries; comparison of average FRank and SuccessRate@k between methods quantifies the effect of addressing the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>IR methods returned less relevant snippets and had worse ranking/precision; DeepCS (which addresses the gap) achieved average FRank 3.5 vs CodeHow 5.5 and Lucene 6.0, R@5 = 0.76 (DeepCS) vs lower values for baselines, and MRR 0.60 (DeepCS) vs 0.45 (CodeHow) and 0.35 (Lucene), indicating substantial performance improvement when the lexical gap is mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Characterized as a fundamental and common problem in prior work and this study; observed across the benchmark and motivating the entire approach (no explicit frequency count of occurrences, but pervasive enough to significantly affect retrieval metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Heterogeneity of modalities: natural language expresses high-level intent, while code uses implementation-specific identifiers, APIs, and tokens that do not share lexical surface forms; synonyms and abstractions are not captured by simple term matching.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Joint embedding (CODEnn) that learns unified vector representations for code (method name, API sequence, tokens) and descriptions/queries (DeNN), enabling semantic matching beyond lexical overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified improvement in retrieval metrics: DeepCS outperforms CodeHow and Lucene on FRank, SuccessRate@k, Precision@k, and MRR (examples: average FRank 3.5 vs 5.5/6.0; R@5 = 0.76; MRR = 0.60).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / code search / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e458.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e458.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>api_sequence_granularity_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API-sequence vs full-code semantic mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gap where API invocation sequences alone do not capture full implementation differences required by natural language distinctions (e.g., different formats), causing indistinguishable API-level representations for semantically different code snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepAPI (prior work) and CODEnn/DeepCS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior DeepAPI generated API sequences from queries; CODEnn extends representation to include API sequences plus method names and tokens to capture finer-grained code semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>natural language queries describing specific behaviors (e.g., save image as png vs save image as jpg)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>API usage sequences extracted from Java methods and full method tokens</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / insufficient representation granularity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>API-sequence representations may map distinct natural language requests to the same API sequence (e.g., ImageIO.write used for saving PNG vs JPG) even when actual code tokens or method-level details differ; hence API-only models conflate distinct implementation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input representation (feature granularity / code encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Motivated by example-based reasoning and prior results (DeepAPI) showing identical API outputs for different queries; recognized during design/analysis of code-search difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not directly quantified with a separate metric in this paper; effect inferred from qualitative examples and the decision to include additional code aspects (method name and tokens) in CODEnn.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>API-only models can return semantically incorrect or overly generic results for queries that require fine-grained differences; motivates inclusion of tokens and method names to improve discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in concrete examples (given in text) and discussed as a substantive limitation of API-sequence-only approaches; no numeric prevalence provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Representation choice: APIs alone lack discriminative detail about token-level and naming differences that correspond to nuanced natural language distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Expand code representation to three aspects in CODEnn: method name (camel-split tokens), API invocation sequence, and code tokens embedded and fused into a single vector.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Indirect evidence: CODEnn / DeepCS (which uses multi-aspect code embeddings) outperforms baselines; direct attribution to multi-aspect embedding is argued qualitatively rather than isolated quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / code search / representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e458.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e458.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>semantic_misranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic-vector misranking vs exact-match expectation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An observed discrepancy where semantic similarity ranking based on learned vectors places partially-related snippets ahead of exact lexical matches, producing user-visible misrankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepCS retrieval/ranking module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During online search, DeepCS ranks code snippets by cosine similarity between the query vector and precomputed code vectors; ranking is purely semantic vector similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>user free-text queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>pre-embedded Java method vectors (offline embedding using CoNN)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different ranking objective / evaluation mismatch (semantic vs exact-match preference)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because DeepCS ranks solely by semantic vector proximity, it may prefer snippets that are semantically related but not the 'exact' requested implementation (e.g., 'generate checksum' before 'generate md5'), causing the exact expected snippet to appear lower in the result list.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>ranking / scoring stage of retrieval pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual inspection of concrete search result examples (Figure 12: 'generate md5' query) showing exact-match ranked lower than semantically-related items.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Anecdotal example yields rank position (exact match appeared at rank 7 in the illustrated case); broader effect reflected in precision@k and FRank metrics where some queries show non-ideal ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>User-perceived accuracy may decrease for queries where exact lexical match is desired; can lower FRank for specific queries and reduce effectiveness despite overall improved aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as a known limitation with at least specific exemplars; not quantified across the whole benchmark but acknowledged as occasional.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Objective mismatch: embedding-based cosine similarity captures semantic proximity broadly, but lacks incentives to prioritize exact lexical/implementation matches when user expects them.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Proposed to include additional code features (e.g., programming context) or hybrid ranking heuristics combining semantic score with other signals (not implemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in this paper; proposed as future work without quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / information retrieval / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e458.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e458.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>train_search_overlap_threat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training vs search corpus overlap and generalization threat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A threat where overlap or differences between the corpus used to train embeddings (commented methods) and the codebase used as the search target can bias evaluation and affect generalization and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepCS evaluation setup (offline training corpus vs search codebase)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Large-scale training corpus of 18.2M GitHub commented Java methods used to train CODEnn; separate search codebase of ~9,950 projects (16.26M methods) used for retrieval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>JavaDoc first-sentence summaries used as training descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>search codebase Java methods (many without Javadoc) used at retrieval time</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset mismatch / possible leakage (overlap) between described data used for training and code used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Training uses only methods that have documentation comments, whereas the search corpus includes all methods (including those without docs); potential overlap between repositories in both corpora could cause overfitting or leak information and inflate measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental dataset construction (training vs evaluation split)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Threats-to-validity analysis by authors recognizing the possibility of overlap and its effects on results.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not numerically measured; authors mitigated by constructing training and search corpora to be 'significantly different' and argued that threat of overfitting is not significant given scale, but provided no formal overlap statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>If present, overlap could artificially boost retrieval performance and harm reproducibility/generalization of results to other corpora; authors state this as a potential threat but believe it is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Discussed as a potential risk for their experimental setup; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Experimental design choices and the large, shared origin (GitHub) of datasets leading to potential shared projects between training and evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Construct the training corpus (commented methods) and the search corpus (projects considered in isolation including methods without comments) to be different; explicitly exclude toy/unstars projects from training; conservative handling in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantified; authors provide qualitative justification that overlap threat is not significant but do not present overlap statistics or ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>empirical software engineering / experimental evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e458.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e458.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>labeling_subjectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual relevance labeling subjectivity and its effects on evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotator subjectivity in judging relevance of returned code snippets can introduce bias into evaluation metrics and affect measured system performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepCS benchmark evaluation (manual grading of top-10 results)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each of 50 benchmark queries from Stack Overflow, two developers independently inspected and labeled the top 10 results returned by DeepCS, resolved inconsistencies via discussion, and used labels to compute FRank, Precision@k, SuccessRate@k, and MRR.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Stack Overflow real-world queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>top-K returned Java method snippets (presented to annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>human labeling bias / subjective ground-truth construction</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Relevance judgments are subjective; different annotators may disagree about whether a returned snippet is relevant, which affects computed retrieval metrics and comparisons among systems.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation labeling / ground-truth creation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Authors acknowledge subjectivity and explicitly describe the double annotation process and conflict resolution via discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No inter-annotator agreement statistics reported; mitigation was process-based (two annotators plus discussion) rather than quantified measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potential bias in evaluation metrics (precision, FRank, etc.) if labels are inconsistent; authors aim to reduce but cannot fully eliminate subjective effects, and they note this as a threat to validity.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applicable to all manually labeled IR/code-search benchmarks; in this study labeling was performed for 50 queries with two annotators but no prevalence statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Intrinsic subjectivity of relevance judgments and limited annotator pool.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Independent labeling by two developers followed by discussion to resolve conflicts; future plan to invite more developers to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Process reduces disagreements qualitatively, but no quantitative inter-annotator agreement or effect-size is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>empirical evaluation / information retrieval for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep API learning <em>(Rating: 2)</em></li>
                <li>CodeHow: Effective code search based on API understanding and extended boolean model <em>(Rating: 2)</em></li>
                <li>Sourcerer: mining and searching internet-scale software repositories <em>(Rating: 1)</em></li>
                <li>From word embeddings to document similarities for improved information retrieval in software engineering <em>(Rating: 1)</em></li>
                <li>Improving source code search with natural language phrasal representations of method signatures <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-458",
    "paper_id": "paper-47021242",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "lexical_gap",
            "name_full": "Lexical gap between natural language queries/descriptions and source code",
            "brief_description": "The mismatch where high-level natural language intent (queries or documentation) uses different words, synonyms, or abstractions than the low-level tokens and identifiers found in source code, causing IR-style retrieval to miss semantically relevant code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepCS / CODEnn code search system",
            "system_description": "A deep learning based code search pipeline (CODEnn model + DeepCS tool) that embeds both natural language descriptions/queries and code snippets into a shared vector space to retrieve semantically related code.",
            "nl_description_type": "developer queries and JavaDoc first-sentence summaries",
            "code_implementation_type": "Java method implementations (method name, API invocation sequence, method body tokens)",
            "gap_type": "lexical mismatch / heterogeneous modality gap",
            "gap_description": "Natural language queries or documentation use high-level verbs and nouns (e.g., \"read\", \"object\") while the relevant code may use different API calls or domain-specific identifiers (e.g., JAXB unmarshal, deserialize) and thus lacks overlapping lexical tokens; conventional IR approaches relying on textual overlap fail to match such cases.",
            "gap_location": "query-to-code mapping (retrieval / matching stage)",
            "detection_method": "Analytical example-driven demonstration and empirical comparison of retrieval performance between IR baselines (Lucene, CodeHow) and DeepCS; qualitative examples (e.g., \"read an object from an xml\" vs a deserialize snippet) show failures of keyword matching.",
            "measurement_method": "Measured indirectly via retrieval metrics (FRank / SuccessRate@k / Precision@k / MRR) across a benchmark of 50 Stack Overflow real-world queries; comparison of average FRank and SuccessRate@k between methods quantifies the effect of addressing the gap.",
            "impact_on_results": "IR methods returned less relevant snippets and had worse ranking/precision; DeepCS (which addresses the gap) achieved average FRank 3.5 vs CodeHow 5.5 and Lucene 6.0, R@5 = 0.76 (DeepCS) vs lower values for baselines, and MRR 0.60 (DeepCS) vs 0.45 (CodeHow) and 0.35 (Lucene), indicating substantial performance improvement when the lexical gap is mitigated.",
            "frequency_or_prevalence": "Characterized as a fundamental and common problem in prior work and this study; observed across the benchmark and motivating the entire approach (no explicit frequency count of occurrences, but pervasive enough to significantly affect retrieval metrics).",
            "root_cause": "Heterogeneity of modalities: natural language expresses high-level intent, while code uses implementation-specific identifiers, APIs, and tokens that do not share lexical surface forms; synonyms and abstractions are not captured by simple term matching.",
            "mitigation_approach": "Joint embedding (CODEnn) that learns unified vector representations for code (method name, API sequence, tokens) and descriptions/queries (DeNN), enabling semantic matching beyond lexical overlap.",
            "mitigation_effectiveness": "Quantified improvement in retrieval metrics: DeepCS outperforms CodeHow and Lucene on FRank, SuccessRate@k, Precision@k, and MRR (examples: average FRank 3.5 vs 5.5/6.0; R@5 = 0.76; MRR = 0.60).",
            "domain_or_field": "software engineering / code search / deep learning",
            "reproducibility_impact": true,
            "uuid": "e458.0"
        },
        {
            "name_short": "api_sequence_granularity_mismatch",
            "name_full": "API-sequence vs full-code semantic mismatch",
            "brief_description": "A gap where API invocation sequences alone do not capture full implementation differences required by natural language distinctions (e.g., different formats), causing indistinguishable API-level representations for semantically different code snippets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepAPI (prior work) and CODEnn/DeepCS",
            "system_description": "Prior DeepAPI generated API sequences from queries; CODEnn extends representation to include API sequences plus method names and tokens to capture finer-grained code semantics.",
            "nl_description_type": "natural language queries describing specific behaviors (e.g., save image as png vs save image as jpg)",
            "code_implementation_type": "API usage sequences extracted from Java methods and full method tokens",
            "gap_type": "incomplete specification / insufficient representation granularity",
            "gap_description": "API-sequence representations may map distinct natural language requests to the same API sequence (e.g., ImageIO.write used for saving PNG vs JPG) even when actual code tokens or method-level details differ; hence API-only models conflate distinct implementation variants.",
            "gap_location": "model input representation (feature granularity / code encoder)",
            "detection_method": "Motivated by example-based reasoning and prior results (DeepAPI) showing identical API outputs for different queries; recognized during design/analysis of code-search difficulty.",
            "measurement_method": "Not directly quantified with a separate metric in this paper; effect inferred from qualitative examples and the decision to include additional code aspects (method name and tokens) in CODEnn.",
            "impact_on_results": "API-only models can return semantically incorrect or overly generic results for queries that require fine-grained differences; motivates inclusion of tokens and method names to improve discrimination.",
            "frequency_or_prevalence": "Observed in concrete examples (given in text) and discussed as a substantive limitation of API-sequence-only approaches; no numeric prevalence provided.",
            "root_cause": "Representation choice: APIs alone lack discriminative detail about token-level and naming differences that correspond to nuanced natural language distinctions.",
            "mitigation_approach": "Expand code representation to three aspects in CODEnn: method name (camel-split tokens), API invocation sequence, and code tokens embedded and fused into a single vector.",
            "mitigation_effectiveness": "Indirect evidence: CODEnn / DeepCS (which uses multi-aspect code embeddings) outperforms baselines; direct attribution to multi-aspect embedding is argued qualitatively rather than isolated quantitatively.",
            "domain_or_field": "software engineering / code search / representation learning",
            "reproducibility_impact": false,
            "uuid": "e458.1"
        },
        {
            "name_short": "semantic_misranking",
            "name_full": "Semantic-vector misranking vs exact-match expectation",
            "brief_description": "An observed discrepancy where semantic similarity ranking based on learned vectors places partially-related snippets ahead of exact lexical matches, producing user-visible misrankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepCS retrieval/ranking module",
            "system_description": "During online search, DeepCS ranks code snippets by cosine similarity between the query vector and precomputed code vectors; ranking is purely semantic vector similarity.",
            "nl_description_type": "user free-text queries",
            "code_implementation_type": "pre-embedded Java method vectors (offline embedding using CoNN)",
            "gap_type": "different ranking objective / evaluation mismatch (semantic vs exact-match preference)",
            "gap_description": "Because DeepCS ranks solely by semantic vector proximity, it may prefer snippets that are semantically related but not the 'exact' requested implementation (e.g., 'generate checksum' before 'generate md5'), causing the exact expected snippet to appear lower in the result list.",
            "gap_location": "ranking / scoring stage of retrieval pipeline",
            "detection_method": "Manual inspection of concrete search result examples (Figure 12: 'generate md5' query) showing exact-match ranked lower than semantically-related items.",
            "measurement_method": "Anecdotal example yields rank position (exact match appeared at rank 7 in the illustrated case); broader effect reflected in precision@k and FRank metrics where some queries show non-ideal ordering.",
            "impact_on_results": "User-perceived accuracy may decrease for queries where exact lexical match is desired; can lower FRank for specific queries and reduce effectiveness despite overall improved aggregate metrics.",
            "frequency_or_prevalence": "Reported as a known limitation with at least specific exemplars; not quantified across the whole benchmark but acknowledged as occasional.",
            "root_cause": "Objective mismatch: embedding-based cosine similarity captures semantic proximity broadly, but lacks incentives to prioritize exact lexical/implementation matches when user expects them.",
            "mitigation_approach": "Proposed to include additional code features (e.g., programming context) or hybrid ranking heuristics combining semantic score with other signals (not implemented here).",
            "mitigation_effectiveness": "Not evaluated in this paper; proposed as future work without quantitative results.",
            "domain_or_field": "software engineering / information retrieval / deep learning",
            "reproducibility_impact": false,
            "uuid": "e458.2"
        },
        {
            "name_short": "train_search_overlap_threat",
            "name_full": "Training vs search corpus overlap and generalization threat",
            "brief_description": "A threat where overlap or differences between the corpus used to train embeddings (commented methods) and the codebase used as the search target can bias evaluation and affect generalization and reproducibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepCS evaluation setup (offline training corpus vs search codebase)",
            "system_description": "Large-scale training corpus of 18.2M GitHub commented Java methods used to train CODEnn; separate search codebase of ~9,950 projects (16.26M methods) used for retrieval experiments.",
            "nl_description_type": "JavaDoc first-sentence summaries used as training descriptions",
            "code_implementation_type": "search codebase Java methods (many without Javadoc) used at retrieval time",
            "gap_type": "dataset mismatch / possible leakage (overlap) between described data used for training and code used for evaluation",
            "gap_description": "Training uses only methods that have documentation comments, whereas the search corpus includes all methods (including those without docs); potential overlap between repositories in both corpora could cause overfitting or leak information and inflate measured performance.",
            "gap_location": "experimental dataset construction (training vs evaluation split)",
            "detection_method": "Threats-to-validity analysis by authors recognizing the possibility of overlap and its effects on results.",
            "measurement_method": "Not numerically measured; authors mitigated by constructing training and search corpora to be 'significantly different' and argued that threat of overfitting is not significant given scale, but provided no formal overlap statistic.",
            "impact_on_results": "If present, overlap could artificially boost retrieval performance and harm reproducibility/generalization of results to other corpora; authors state this as a potential threat but believe it is limited.",
            "frequency_or_prevalence": "Discussed as a potential risk for their experimental setup; not quantified.",
            "root_cause": "Experimental design choices and the large, shared origin (GitHub) of datasets leading to potential shared projects between training and evaluation sets.",
            "mitigation_approach": "Construct the training corpus (commented methods) and the search corpus (projects considered in isolation including methods without comments) to be different; explicitly exclude toy/unstars projects from training; conservative handling in analysis.",
            "mitigation_effectiveness": "Not quantified; authors provide qualitative justification that overlap threat is not significant but do not present overlap statistics or ablation.",
            "domain_or_field": "empirical software engineering / experimental evaluation",
            "reproducibility_impact": true,
            "uuid": "e458.3"
        },
        {
            "name_short": "labeling_subjectivity",
            "name_full": "Manual relevance labeling subjectivity and its effects on evaluation",
            "brief_description": "Human annotator subjectivity in judging relevance of returned code snippets can introduce bias into evaluation metrics and affect measured system performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepCS benchmark evaluation (manual grading of top-10 results)",
            "system_description": "For each of 50 benchmark queries from Stack Overflow, two developers independently inspected and labeled the top 10 results returned by DeepCS, resolved inconsistencies via discussion, and used labels to compute FRank, Precision@k, SuccessRate@k, and MRR.",
            "nl_description_type": "Stack Overflow real-world queries",
            "code_implementation_type": "top-K returned Java method snippets (presented to annotators)",
            "gap_type": "human labeling bias / subjective ground-truth construction",
            "gap_description": "Relevance judgments are subjective; different annotators may disagree about whether a returned snippet is relevant, which affects computed retrieval metrics and comparisons among systems.",
            "gap_location": "evaluation labeling / ground-truth creation",
            "detection_method": "Authors acknowledge subjectivity and explicitly describe the double annotation process and conflict resolution via discussion.",
            "measurement_method": "No inter-annotator agreement statistics reported; mitigation was process-based (two annotators plus discussion) rather than quantified measurement.",
            "impact_on_results": "Potential bias in evaluation metrics (precision, FRank, etc.) if labels are inconsistent; authors aim to reduce but cannot fully eliminate subjective effects, and they note this as a threat to validity.",
            "frequency_or_prevalence": "Applicable to all manually labeled IR/code-search benchmarks; in this study labeling was performed for 50 queries with two annotators but no prevalence statistics reported.",
            "root_cause": "Intrinsic subjectivity of relevance judgments and limited annotator pool.",
            "mitigation_approach": "Independent labeling by two developers followed by discussion to resolve conflicts; future plan to invite more developers to improve robustness.",
            "mitigation_effectiveness": "Process reduces disagreements qualitatively, but no quantitative inter-annotator agreement or effect-size is reported.",
            "domain_or_field": "empirical evaluation / information retrieval for code",
            "reproducibility_impact": true,
            "uuid": "e458.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep API learning",
            "rating": 2,
            "sanitized_title": "deep_api_learning"
        },
        {
            "paper_title": "CodeHow: Effective code search based on API understanding and extended boolean model",
            "rating": 2,
            "sanitized_title": "codehow_effective_code_search_based_on_api_understanding_and_extended_boolean_model"
        },
        {
            "paper_title": "Sourcerer: mining and searching internet-scale software repositories",
            "rating": 1,
            "sanitized_title": "sourcerer_mining_and_searching_internetscale_software_repositories"
        },
        {
            "paper_title": "From word embeddings to document similarities for improved information retrieval in software engineering",
            "rating": 1,
            "sanitized_title": "from_word_embeddings_to_document_similarities_for_improved_information_retrieval_in_software_engineering"
        },
        {
            "paper_title": "Improving source code search with natural language phrasal representations of method signatures",
            "rating": 1,
            "sanitized_title": "improving_source_code_search_with_natural_language_phrasal_representations_of_method_signatures"
        }
    ],
    "cost": 0.0154325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Code Search
ACMCopyright ACM2018. May 27-June 3, 2018. May 27-June 3, 2018</p>
<p>Xiaodong Gu guxiaodong1987@126.com 
The Hong Kong University of Science and Technology
Hong Kong</p>
<p>Hongyu Zhang hongyu.zhang@newcastle.edu.au 
The University of Newcastle
CallaghanAustralia</p>
<p>Sunghun Kim 
The Hong Kong University of Science and Technology
Hong Kong</p>
<p>Clova AI Research
NAVER</p>
<p>Xiaodong Gu 
The Hong Kong University of Science and Technology
Hong Kong</p>
<p>Hongyu Zhang 
The University of Newcastle
CallaghanAustralia</p>
<p>Sunghun Kim 
The Hong Kong University of Science and Technology
Hong Kong</p>
<p>Deep Code Search</p>
<p>ICSE '18: ICSE '18: 40th International Conference on Software Engineering
Gothenburg, Sweden; New York, NY, USA; Gothenburg, SwedenACM32018. May 27-June 3, 2018. May 27-June 3, 201810.1145/3180155.3180167ACM ISBN 978-1-4503-5638-1/18/05. . . $15.00
To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.</p>
<p>INTRODUCTION</p>
<p>Code search is a very common activity in software development practices [57,68]. To implement a certain functionality, for example, to parse XML files, developers usually search and reuse previously written code by performing free-text queries over a large-scale codebase.</p>
<p>Many code search approaches have been proposed [13,15,29,31,32,35,44,45,47,62], most of them being based on information retrieval (IR) techniques. For example, Linstead et al. [43] proposed Sourcerer, an information retrieval based code search tool that combines the textual content of a program with structural information.</p>
<p>McMillan et al. [47] proposed Portfolio, which returns a chain of functions through keyword matching and PageRank. Lu et al. [44] expanded a query with synonyms obtained from WordNet and then performed keyword matching of method signatures. Lv et al. [45] proposed CodeHow, which combines text similarity and API matching through an extended Boolean model.</p>
<p>A fundamental problem of the IR-based code search is the mismatch between the high-level intent reflected in the natural language queries and low-level implementation details in the source code [12,46]. Source code and natural language queries are heterogeneous. They may not share common lexical tokens, synonyms, or language structures. Instead, they may only be semantically related. For example, a relevant snippet for the query "read an object from an xml" could be as follows:</p>
<p>public static &lt; S &gt; S deserialize(Class c, File xml) { try { JAXBContext context = JAXBContext.newInstance(c); Unmarshaller unmarshaller = context.createUnmarshaller(); S deserialized = (S) unmarshaller.unmarshal(xml); return deserialized; } catch (JAXBException ex) { log.error("Error-deserializing-object-from-XML", ex); return null; } } Existing approaches may not be able to return this code snippet as it does not contain keywords such as read and object or their synonyms such as load and instance. Therefore, an effective code search engine requires a higher-level semantic mapping between code and natural language queries. Furthermore, the existing approaches have difficulties in query understanding [27,29,45]. They cannot effectively handle irrelevant/noisy keywords in queries [27]. Therefore, an effective code search engine should also be able to understand the semantic meanings of natural language queries and source code in order to improve the accuracy of code search.</p>
<p>In our previous work, we introduced the DeepAPI framework [27], which is a deep learning based method that learns the semantics of queries and the corresponding API sequences. However, searching source code is much more difficult than generating APIs, because the semantics of code snippets are related not only to the API sequences but also to other source code aspects such as tokens and method names. For example, DeepAPI could return the same API ImageIO.write for the query save image as png and save image as jpg. Nevertheless, the actual code snippets for answering the two queries are different in terms of source code tokens. Therefore, the code search problem requires models that can exploit more aspects of the source code.</p>
<p>In this paper, we propose a novel deep neural network named CO-DEnn (Code-Description Embedding Neural Network). To bridge the lexical gap between queries and source code, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. With the unified vector representation, code snippets semantically related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.</p>
<p>Using CODEnn, we implement a code search tool, DeepCS as a proof of concept. DeepCS trains the CODEnn model on a corpus of 18.2 million Java code snippets (in the form of commented methods) from GitHub. Then, it reads code snippets from a codebase and embeds them into vectors using the trained CODEnn model. Finally, when a user query arrives, DeepCS finds code snippets that have the nearest vectors to the query vector and return them.</p>
<p>To evaluate the effectiveness of DeepCS, we perform code search on a search codebase using 50 real-world queries obtained from Stack Overflow. Our results show that DeepCS returns more relevant code snippets than the two related approaches, that is, Code-How [45] and a conventional Lucene-based code search tool [5]. On average, the first relevant code snippet returned by DeepCS is ranked 3.5, while the first relevant results returned by Code-How [45] and Lucene [43] are ranked 5.5 and 6.0, respectively. For 76% of the queries, the relevant code snippets can be found within the top 5 returned results. The evaluation results confirm the effectiveness of DeepCS.</p>
<p>To our knowledge, we are the first to propose deep learning based code search. The main contributions of our work are as follows:</p>
<p> We propose a novel deep neural network, CODEnn, to learn a unified vector representation of both source code and natural language queries.  We develop DeepCS, a tool that utilizes CODEnn to retrieve relevant code snippets for given natural language queries.  We empirically evaluate DeepCS using a large scale codebase.</p>
<p>The rest of this paper is organized as follows. Section 2 describes the background of the deep learning based embedding models. Section 3 describes the proposed deep neural network for code search. Section 4 describes the detailed design of our approach. Section 5 presents the evaluation results. Section 6 discusses our work, followed by Section 7 that presents the related work. We conclude the paper in Section 8.</p>
<p>BACKGROUND</p>
<p>Our work adopts recent advanced techniques from deep learning and natural language processing [10,17,70]. In this section, we discuss the background of these techniques. </p>
<p>Embedding Techniques</p>
<p>Embedding (also known as distributed representation [50,72]) is a technique for learning vector representations of entities such as words, sentences and images in such a way that similar entities have vectors close to each other [48,50]. A typical embedding technique is word embedding, which represents words as fixed-length vectors so that similar words are close to each other in the vector space [48,50]. For example, suppose the word execute is represented as [0.12, -0.32, 0.01] and the word run is represented as [0.12, -0.31, 0.02]. From their vectors, we can estimate their distance and identify their semantic relation. Word embedding is usually realized using a model such as CBOW and Skip-Gram [48]. These models build a neural network that captures the relations between a word and its contextual words. The vector representations of words, as parameters of the network, are trained with a text corpus [50].</p>
<p>Likewise, a sentence (i.e., a sequence of words) can also be embedded as a vector [59]. A simple way of sentence embedding is, for example, to view it as a bag of words and add up all its word vectors [39].</p>
<p>RNN for Sequence Embedding</p>
<p>We now introduce a widely-used deep neural network, the Recurrent Neural Networks (RNN) [49,59] for the embedding of sequential data such as natural language sentences. The Recurrent Neural Network is a class of neural networks where hidden layers are recurrently used for computation. This creates an internal state of the network to record dynamic temporal behavior. Figure 1a shows the basic structure of an RNN. The neural network includes three layers, an input layer which maps each input to a vector, a recurrent hidden layer which recurrently computes and updates a hidden state after reading each input, and an output layer which utilizes the hidden state for specific tasks. Unlike traditional feedforward neural networks, RNNs can embed sequential inputs such as sentences using their internal memory [25].</p>
<p>Consider a natural language sentence with a sequence of T words s=w 1 , ..., w T , RNN embeds it through the following computations: it reads words in the sentence one by one, and updates a hidden state at each time step. Each word w t is first mapped to a d-dimensional vector w t R d by a one-hot representation [72] or word embedding [50]. Then, the hidden state (values in the hidden layer) h t is updated at time t by considering the input word w t and the preceding hidden state h t 1 : where [a;b]R 2d represents the concatenation of two vectors, W  R 2d d is the matrix of trainable parameters in the RNN, while tanh is a non-linearity activation function of the RNN. Finally, the embedding vector of the sentence is summarized from the hidden states h 1 , ..., h T . A typical way is to select the last hidden state h T as the embedding vector. The embedding vector can also be summarized using other computations such as the maxpooling [36]:
h t = tanh(W [h t 1 ; w t ]), t = 1, 2, ..., T(1)s = maxpooling([h 1 , ..., h T ])(2)
Maxpooling is an operation that selects the maximum value in each fixed-size region over a matrix. Figure 2 shows an example of maxpooling over a sequence of hidden vectors h 1 , ..., h T . Each column represents a hidden vector. The window size of each region is set to 1T in this example. The result is a fixed-length vector whose elements are the maximum values of each row. Maxpooling can capture the most important feature (one with the highest value) for each region and can transform sentences of variable lengths into a fixed-length vector. Figure 1b shows an example of how RNN embeds a sentence (e.g., parse xml file) into a vector. To facilitate understanding, we expand the recurrent hidden layer for each time step. The RNN reads words in the sentence one by one, and records a hidden state at each time step. When it reads the first word parse, it maps the word into a vector w 1 and computes the current hidden state h 1 using w 1 . Then, it reads the second word xml, embeds it into w 2 , and updates the hidden state h 1 to h 2 using w 2 . The procedure continues until the RNN receives the last word file and gets the final state h 3 . The final state h 3 can be used as the embedding c of the whole sentence.</p>
<p>The embedding of the sentence, i.e., the sentence vector, can be used for specific applications. For example, one can build a language model conditioning on the sentence vector for machine translation [17]. One can also embed two sentences (a question sentence and an answer sentence) and compare their vectors for answer selection [21,71].</p>
<p>Joint Embedding of Heterogeneous Data</p>
<p>Suppose there are two heterogeneous data sets X and Y. We want to learn a correlation between them, namely,
f : X  Y(3)
For example, suppose X is a set of images and Y is a set of natural language sentences, f can be the correlation between the images and the sentences (i.e., image captioning). Since the two data sources are heterogeneous, it is difficult to discover the correlation f directly. Thus, we need a bridge to connect these two levels of information. Joint Embedding, also known as multi-modal embedding [78], is a technique to jointly embed/correlate heterogeneous data into a unified vector space so that semantically similar concepts across the two modalities occupy nearby regions of the space [33]. The joint embedding of X and Y can be formulated as: where :XR d is an embedding function to map X into a ddimensional vector space V ;  :YR d is an embedding function to map Y into the same vector space V ; J (, ) is a similarity measure (e.g., cosine) to score the matching degrees of V X and V Y in order to learn the mapping functions. Through joint embedding, heterogeneous data can be easily correlated through their vectors.
X    V X  J (V X , V Y )  V Y    Y(4)
Joint embedding has been used in many tasks [22,74,78]. For example, in computer vision, Karpathy and Li [33] use a Convolutional Neural Network (CNN) [22], a deep neural network as the  and an RNN as the  , to jointly embed both image and text into the same vector space for labeling images [33].</p>
<p>A DEEP NEURAL NETWORK FOR CODE SEARCH</p>
<p>Inspired by existing joint embedding techniques [21,22,33,78], we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network) for the code search problem. Figure 3 illustrates the key idea. Natural language queries and code snippets are heterogeneous and cannot be easily matched according to their lexical tokens. To bridge the gap, CODEnn jointly embeds code snippets and natural language descriptions into a unified vector space so that a query and the corresponding code snippets are embedded into nearby vectors and can be matched by measuring vector similarities.</p>
<p>Architecture</p>
<p>As introduced in Section 2.3, a joint embedding model requires three components: the embedding functions :XR d and  :YR d , as well as the similarity measure J (, ). CODEnn realizes these components with deep neural networks. Figure 4 shows the overall architecture of CODEnn. The neural network consists of three modules, each corresponding to a component of joint embedding:</p>
<p> a code embedding network (CoNN) to embed source code into vectors.  a description embedding network (DeNN) to embed natural language descriptions into vectors.  a similarity module that measures the degree of similarity between code and descriptions. The following subsections describe the detailed design of these modules.</p>
<p>Code Embedding Network.</p>
<p>The code embedding network embeds source code into vectors. Source code is not simply plain text. It contains multiple aspects of information such as tokens, control flows and APIs [46]. In our model, we consider three aspects  of source code: the method name, the API invocation sequence, and the tokens contained in the source code. They are commonly used in existing code search approaches [19,27,41,44,45]. For each code snippet (at the method level), we extract these three aspects of information. Each is embedded individually and then combined into a single vector representing the entire code.</p>
<p>Consider an input code snippet C=[M, A, ], where M=w 1 ,..., w N M is the method name represented as a sequence of N M camel split tokens [1]; A=a 1 , ..., a N A is the API sequence with N A consecutive API method invocations, and ={ 1 , ...,  N  } is the set of tokens in the snippet. The neural network embeds the three aspects as follows: for the method name M, it embeds the sequence of camel split tokens using an RNN with maxpooling:
h t = tanh(W M [h t 1 ; w t ]), t = 1, 2, ..., N M m = maxpooling([h 1 , ..., h N M ])(5)
where w t R d is the embedding vector of token w t , [a;b]R 2d represents the concatenation of two vectors, W M R 2dd is the matrix of trainable parameters in the RNN, tanh is the activation function of the RNN. A method name is thus embedded as a d-dimensional vector m. Likewise, the API sequence A is embedded into a vector a using an RNN with maxpooling:
h t = tanh(W A [h t 1 ; a t ]), t = 1, 2, ..., N A a = maxpooling([h 1 , ..., h N A ])(6)
where a t R d is the embedding vector of API a t , W A is the matrix of trainable parameters in the RNN.</p>
<p>For the tokens , as they have no strict order in the source code, they are simply embedded via a multilayer perceptron (MLP), i.e., the conventional fully connected layer [52]:
h i = tanh(W   i ), i = 1, 2, ..., N (7)
where  i R d represents the embedded representation of the token  i , W  is the matrix of trainable parameters in the MLP, h i , i=1, ..., N  are the embedding vectors of all individual tokens.</p>
<p>The individual vectors are also summarized to a single vector t via maxpooling:
t = maxpooling([h 1 , ..., h N  ])(8)
Finally, the vectors of the three aspects are fused into one vector through a fully connected layer:
c = tanh(W C [m; a; t ])(9)
where [a;b;c] represents the concatenation of three vectors, W C is the matrix of trainable parameters in the MLP. The output vector c represents the final embedding of the code snippet.</p>
<p>Description Embedding Network.</p>
<p>The description embedding network (DeNN) embeds natural language descriptions into vectors. Consider a description D=w 1 , ..., w N D comprising a sequence of N D words. DeNN embeds it into a vector d using an RNN with maxpooling:
h t = tanh(W D [h t 1 ; w t ]), t = 1, 2, ..., N D d = maxpooling([h 1 , ..., h N D ])(10)
where w t R d represents the embedded representation of the description word w t , W D is the matrix of trainable parameters in the RNN, h t , t=1, ...N D are the hidden states of the RNN.</p>
<p>Similarity Module.</p>
<p>We have described the transformations that map the code and description into vectors (i.e., the c and d).</p>
<p>Since we want the vectors of code and description to be jointly embedded, we measure the similarity between the two vectors.</p>
<p>We use the cosine similarity for the measurement, which is defined as: (11) where c and d are the vectors of code and a description respectively. The higher the similarity, the more related the code is to the description.
cos(c, d ) = c T d c d
Overall, CODEnn takes a code, description pair as input and predicts their cosine similarity cos(c, d).</p>
<p>Model Training</p>
<p>Now we present how to train the CODEnn model to embed both code and descriptions into a unified vector space. The high-level goal of the joint embedding is: if a code snippet and a description have similar semantics, their embedded vectors should be close to each other. In other words, given an arbitrary code snippet C and an arbitrary description D, we want it to predict a high similarity if D is a correct description of C, and a little similarity otherwise.</p>
<p>At training time, we construct each training instance as a triple C, D+, D-: for each code snippet C there is a positive description  [18,22]:
L( ) = <C, D+, D->P max (0,   cos(c, d +) + cos(c, d -))(12)
where  denotes the model parameters, P denotes the training dataset,  is a constant margin. c, d+ and d-are the embedded vectors of C, D+ and D-, respectively. A small, fixed  value of 0.05 is used in all the experiments. Intuitively, the ranking loss encourages the cosine similarity between a code snippet and its correct description to go up, and the cosine similarities between a code snippet and incorrect descriptions to go down.</p>
<p>DEEPCS: DEEP LEARNING BASED CODE SEARCH</p>
<p>In this section, we describe DeepCS, a code search tool based on the proposed CODEnn model. DeepCS recommends top K most relevant code snippets for a given natural language query. Figure 5 shows the overall architecture. It includes three main phases: offline training, offline code embedding, and online code search. We begin by collecting a large-scale corpus of code snippets, i.e., Java methods with corresponding descriptions. We extract subelements (including method names, tokens, and API sequences) from the methods. Then, we use the corpus to train the CODEnn model (the offline training phase). For a given codebase from which users would like to search for code snippets, DeepCS extracts code elements for each Java method in the search codebase, and computes a code vector using the CoNN module of the trained CODEnn model (the offline embedding phase). Finally, when a user query arrives, DeepCS first computes the vector representation of the query using the DeNN module of the CODEnn model, and then returns code snippets whose vectors are close to the query vector (the online code search phase).</p>
<p>In theory, our approach could search for source code written in any programming languages. In this paper, we limit our scope to the Java code. The following sections describe the detailed steps of our approach.</p>
<p>Collecting Training Corpus</p>
<p>As described in Section 3, the CODEnn model requires a large-scale training corpus that contains code elements and the corresponding descriptions, i.e., the method name, API sequence, tokens, description tuples. Figure 6 shows an excerpt of the training corpus.</p>
<p>We build the training tuples using Java methods that have documentation comments 1 from open-source projects on GitHub [3]. For each Java method, we use the method declaration as the code element and the first sentence of its documentation comment as its natural language description. According to the Javadoc guidance 2 , the first sentence is usually a summary of a method. To prepare the data, we download Java projects from GitHub created from August, 2008 to June, 2016. To remove toy or experimental programs, we exclude any projects without a star. We select only the Java methods that have documentation comments from the downloaded projects. Finally, we obtain a corpus comprising 18,233,872 commented Java methods.</p>
<p>Having collected the corpus of commented code snippets, we extract the method name, API sequence, tokens, description tuples as follows: Method Name Extraction: For each Java method, we extract its name and parse the name into a sequence of tokens according to camel case [1]. For example, the method name listFiles will be parsed into the tokens list and files. API Sequence Extraction: We extract an API sequence from each Java method using the same procedures as described in Deep-API [27] -parsing the AST using the Eclipse JDT compiler [2] and traversing the AST. The API sequences are produced as follows [27]:</p>
<p> For each constructor invocation new C(), we produce C.new and append it to the API sequence.  For conditional statements such as if(s 1 ){s 2 ;}else{s 3 ;}, we create a sequence from all possible branches, that is, a 1 -a 2 -a 3 , where a i is the API sequence extracted from the statement s i .  For loop statements such as while(s 1 ){s 2 ;}, we produce a sequence a 1 -a 2 , where a 1 and a 2 are API sequences extracted from the statement s 1 and s 2 , respectively.</p>
<p>Token Extraction: To collect tokens from a Java method, we tokenize the method body, split each token according to camel case [1], and remove the duplicated tokens. We also remove stop words (such as the and in) and Java keywords as they frequently occur in source code and are not discriminative. Description Extraction: To extract the documentation comment, we use the Eclipse JDT compiler [2] to parse the AST from a Java method and extract the JavaDoc Comment from the AST.  </p>
<p>Training CODEnn Model</p>
<p>We use the large-scale corpus described in the previous section to train the CODEnn model, following the method described in Section 3.2.</p>
<p>The detailed implementation of the CODEnn model is as follows: we use the bi-directional LSTM [70], a state-of-the-art subclass of RNN for the RNN implementation. All LSTMs have 200 hidden units in each direction. We set the dimension of word embedding to 100. The CODEnn has two types of MLPs, the embedding MLP for embedding individual tokens and the fusion MLP to combine the embeddings of different aspects. We set the number of hidden units as 100 for the embedding MLP and 400 for the fusion MLP.</p>
<p>The CODEnn model is trained via the mini-batch Adam algorithm [37,40]. We set the batch size (i.e., the number of instances per batch) as 128. For training the neural networks, we limit the size of the vocabulary to 10,000 words that are most frequently used in the training dataset. We build our model on Keras [4] and Theano [6], two opensource deep learning frameworks. We train our models on a server with one Nvidia K40 GPU. The training lasts 50 hours with 500 epochs.</p>
<p>Searching Code Snippets</p>
<p>Given a user's free-text query, DeepCS returns the relevant code snippets through the trained CODEnn model. It first computes the code vector for each code snippet (i.e., a Java method) in the search codebase. Then, it selects and returns the code snippets that have the top K nearest vectors to the query vector.</p>
<p>More specially, before a search starts, DeepCS embeds all code snippets in the codebase into vectors using the CoNN module of CODEnn in an off-line manner. During the on-line search, when a developer enters a natural language query, DeepCS first embeds the query into a vector using the DeNN module of CODEnn. Then, it estimates the cosine similarities between the query vector and all code vectors using Equation 11. Finally, the top K code snippets whose vectors are most similar to the query vector are returned as the search results. K is set to 10 in our experiments.</p>
<p>EVALUATION</p>
<p>In this section, we evaluate DeepCS through experiments. We also compare DeepCS with the related code search approaches.</p>
<p>Experimental Setup</p>
<p>Search Codebase.</p>
<p>To better evaluate DeepCS, our experiments are performed over a search codebase, which is different from the training corpus. Code snippets that match a user query are retrieved from the search codebase. In practice, the search codebase could be an organization's local codebase or any codebase created from open source projects.</p>
<p>To construct the search codebase, we choose the Java projects that have at least 20 stars in GitHub. Different from the training corpus, they are considered in isolation and contain all code (including those do not have Javadoc comments). There are 9,950 projects in total. We select all 16,262,602 methods from these projects. For each Java method, we extract a method name, API sequence, tokens triple to generate its code vector.</p>
<p>Query Subjects.</p>
<p>To select code search queries for the evaluation, we adopt a systematic procedure used in [41] 4 . We build a benchmark of queries from the top 50 voted Java programming questions in Stack Overflow. To achieve so, we browse the list of Java-tagged questions in Stack Overflow and sort them according to the votes that each one receives 5 . We manually check the sorted list sequentially, and add questions that satisfy the following conditions to the benchmark:</p>
<p>(1) The question is a concrete Java programming task. We exclude questions about problems, knowledge, configurations, experience and questions whose descriptions are vague and abstract. For example, Failed to load the JNI Library, What is the difference between StringBuilder and StringBuffer?, and Why does Java have transient fields?. (2) The accepted answer to the question contains a Java code snippet. (3) The question is not a duplicate of the previous questions. We filter out questions that are tagged as "duplicated".</p>
<p>The full list of the 50 selected queries can be found in Table 1. For each query, two developers manually inspect the top 10 results returned by DeepCS and label their relevance to the query. Then they discuss the inconsistent labels and relabel them. The procedure repeats until a consensus is reached.</p>
<p>Performance Measure.</p>
<p>We use four common metrics to measure the effectiveness of code search, namely, FRank, Success-Rate@k, Precision@k, and Mean Reciprocal Rank (MRR). They are widely used metrics in information retrieval and code search literature [41,45,62,79].</p>
<p>The FRank (also known as best hit rank [41]) is the rank of the first hit result in the result list [62]. It is important as users scan the results from top to bottom. A smaller FRank implies lower inspection effort for finding the desired result. We use FRank to assess the effectiveness of a single code search query.</p>
<p>The SuccessRate@k (also known as success percentage at k [41]) measures the percentage of queries for which more than one correct result could exist in the top k ranked results [35,41,79]. In our evaluations it is calculated as follows:
SuccessRate@k = 1 |Q | Q q=1  (FRank q  k)(13)
where Q is a set of queries,  () is a function which returns 1 if the input is true and 0 otherwise. SuccessRate@k is important because a better code search engine should allow developers to discover the needed code by inspecting fewer returned results. The higher the metric value, the better the code search performance.</p>
<p>The Precision@k [45,57] measures the percentage of relevant results in the top k returned results for each query. In our evaluations it is calculated as follows:
Precision@k =</p>
<h1>relevant results in the top k results k (14) Precision@k is important because developers often inspect multiple results of different usages to learn from [62]. A better code search engine should allow developers to inspect less noisy results. The higher the metric values, the better the code search performance. We evaluate SuccessRate@k and Precision@k when k's value is 1, 5, and 10. These values reflect the typical sizes of results that users would inspect [41]. The MRR [45,79] is the average of the reciprocal ranks of results of a set of queries Q. The reciprocal rank of a query is the inverse of the rank of the first hit result [26]. MRR is calculated as follows: (15) The higher the MRR value, the better the code search performance.</h1>
<p>MRR = 1 |Q | |Q | q=1 1 F Rank q</p>
<p>Comparison Methods.</p>
<p>We compare the effectiveness of our approach with CodeHow [45] and a conventional Lucene-based code search tool [5].</p>
<p>CodeHow is a state-of-the-art code search engine proposed recently. It is an information retrieval based code search tool that incorporates an extended Boolean model and API matching. It first retrieves relevant APIs to a query by matching the query with the API documentation. Then, it searches code by considering both plain code and the related APIs. Like DeepCS, CodeHow also considers multiple aspects of source code such as method name and APIs. It combines multiple aspects using an Extended Boolean Model [45]. The facts that CodeHow also considers APIs and is also built for large-scale code search make it an ideal baseline for our experiments.</p>
<p>Lucene is a popular, conventional text search engine behind many existing code search tools such as Sourcerer [43]. Sourcerer combines Lucene with code properties such as FQN (full qualified name) of entities and code popularity to retrieve the code snippets. In our implementation of the Lucene-based code search tool, we consider the heuristic of FQN. We did not include the code popularity heuristic (computed using PageRank) as it does not significantly improve the code search performance [43]. We use the same experimental setting for CodeHow and the Lucene-based tool as used for evaluating DeepCS. Table 1 shows the evaluation results of DeepCS and related approaches for each query in the benchmark. The column Question ID shows the original ID of the question in Stack Overflow where the query comes from. The column FRank shows the FRank result of each approach. The symbol 'NF' stands for Not Found which means that no relevant result has been returned within the top K results (K=10).</p>
<p>Results</p>
<p>The results show that DeepCS produces generally more relevant results than Lucene and CodeHow. Figure 8a shows the statistical summary of FRank for the three approaches. The symbol '+' indicates the average FRank value achieved by each approach. We conservatively treat the FRank as 11 for queries that fail to obtain relevant results within the top 10 returned results. We observe that DeepCS achieves more relevant results with an average FRank of 3.5, which is smaller than the average FRank achieved by CodeHow (5.5) and Lucene (6.0). The FRank values of DeepCS concentrate on the range from 1 to 4, while CodeHow and Lucene produce larger variance and many less relevant results. Figure 8b, 8c and 8d show the statistics of Precision@k for the three approaches when k is 1, 5 and 10, respectively. We observe that DeepCS achieves better overall precision values than CodeHow and the Lucene-based tool.</p>
<p>To test the statistical significance, we apply the Wilcoxon signedrank test (p&lt;0.05) for the comparison of FRank and Precision@k between DeepCS and the two related approaches for all the queries. We conservatively treat the FRank as 11 for queries that fail to obtain relevant results within the top 10 returned results. The pvalues for the comparisons of DeepCS with Lucene and CodeHow are all less than 0.05, indicating the statistical significance of the improvement of DeepCS over the related approaches. Table 2 shows the overall performance of the three approaches, measured in terms of SuccessRate@k, Precision@k and MRR. The columns R@1, R@5 and R@10 show the results of SuccessRate@k when k is 1, 5 and 10, respectively. The columns P@1, P@5 and P@10 show the results of the average Precision@k over all queries when k is 1, 5 and 10, respectively. The column MRR shows the MRR values of the three approaches. The results show that DeepCS returns more relevant code snippets than CodeHow and Lucene. For example, the R@5 value is 0.76, which means that for 76% of the queries, the relevant code snippets can be found within the top 5 return results. The P@5 value is 0.5, which means that 50% of the top 5 results are deemed accurate. For the SuccessRate@k, the improvements to CodeHow are 21%, 31% and 30%, respectively. For the Precision@k, the improvements to CodeHow are 21%, 72% and 75%, respectively. For the MRR, the improvement to CodeHow is 33%. Overall, our approach improves the accuracy of related techniques on all metrics.</p>
<p>Examples of Code Search Results</p>
<p>We now provide concrete examples of code search results that demonstrate the advantages of DeepCS. Figure 9a and 9b show the results for two queries: queue an event to be run on the thread and run an event on a thread queue. The two queries have the same set of keywords with different word sequences. The keyword queue in the two queries have different meanings and it could be difficult for an IR-based approach to distinguish. Still, DeepCS can understand the meaning of the two queries and return relevant snippets. Apparently, DeepCS has the ability to recognize query semantics.</p>
<p>The ability of query understanding enables DeepCS to perform a more robust code search. Its search results are less affected by irrelevant or noisy keywords. For example, the query get the content of an input stream as a string using a specified character encoding contains 9 keywords. CodeHow returns many snippets that are related to less relevant keywords such as specified and character. DeepCS, on the other hand, can successfully identify the importance of different keywords and understand the key point of the query ( Figure 10). Another advantage of DeepCS relates to associative search. That is, it not only seeks snippets with matched keywords but also recommends those without matched keywords but are semantically related. This is important because it significantly increases the search scope especially when the codebase is small. Besides, developers need snippets of multiple usages [62]. The associative search provides more options of code snippets for developers to learn from. Figure 11a shows the first result of the query read an object from an xml file. As discussed in Section 1, traditional IR-based approaches may only match snippets that contain keywords such as xml, object and read. However, as shown in the figure, DeepCS successfully recognizes the query semantic and returns results of xml deserialize, even the keywords do not exist in the result. By contrast, CodeHow only returns snippets containing read, object and xml, narrowing down the search scope. The example indicates that DeepCS searches code by understanding the semantics instead of just matching keywords. Similarly, the query initialization of an arraylist in one line in Table 1 returns snippets containing "new ArrayList " although the snippet does not include the keyword initialization. Figure 11b shows another example of the associative search. When searching play a song. DeepCS not only returns snippets with matching keywords but also recommends results with semantically related words such as audio and voice.</p>
<p>DISCUSSIONS 6.1 Why does DeepCS Work?</p>
<p>We have identified three advantages of DeepCS that may explain its effectiveness in code search:    semantically related words and word orders, are considered in these models [27]. Therefore, it can recognize the semantics of query and code better. For example, it can distinguish the query queue an event to be run on the thread from the query run an event on the event queue. Clustering snippets by natural language semantics An advantage of our approach is that it embeds semantically similar code snippets into vectors that are close to each other. Semantically similar code snippets are grouped according to their semantics. Therefore, in addition to the exact matching snippets, DeepCS also recommends the semantically related ones.</p>
<p>Limitation of DeepCS</p>
<p>Despite the advantages such as associative search, DeepCS could still return inaccurate results. It sometimes ranks partially relevant results higher than the exact matching ones. Figure 12 shows the result for the query generate md5. The exactly matching result is ranked 7 in the result list, while partially related results such as generate checksum are recommended before the exact results. This is because DeepCS ranks results by just considering their semantic vectors. In future work, more code features (such as programming context) [58] could be considered in our model to further adjust the results.</p>
<p>Threats to Validity</p>
<p>Our goal is to improve the performance of code search over GitHub, thus both training and search are performed over GitHub corpus. There is a threat of overlap between the training and search codebases. To mitigate this threat, in our experiments, the training and search codebases are constructed to be significantly different. The training codebase only contains code that has corresponding descriptions, while the search codebase is considered in isolation and contains all code (including those do not have descriptions). We believe the threat of overfitting for this overlap is not significant as our training codebase considers a vast majority of code in Github. The most important goal of our experiments is to evaluate DeepCS in a real-world code search scenario. For that, we used 50 real queries collected from Stack Overflow to test the effectiveness of DeepCS. These queries are not descriptions/comments of Java methods and are not used for training.</p>
<p>In our experiments, the relevancy of returned results were manually graded and could suffer from subjectivity bias. To mitigate this threat, (i) the manual analysis was performed independently by two developers and (ii) the developers performed an open discussion to resolve conflict grades for the 50 questions. In the future, we will further mitigate this threat by inviting more developers for the grading.</p>
<p>In the grading of relevancy, we consider only the top 10 results. Queries that fail are identically assigned with an FRank of 11 and could be biased from the real relevancy of code snippets. However, we believe that the setting is reasonable. In real-world code search, developers usually inspect the top K results and ignore the remaining. That means it does not make much difference if a code snippet appears at rank 11 or 20 if K is 10.</p>
<p>Like related work (e.g., [14,41]), we evaluate DeepCS with popular Stack Overflow questions. SO questions may not be representative to all possible queries for code search engines. To mitigate this threat, (i) DeepCS is not trained on SO questions but on large scale Github corpus. (ii) We select the most frequently asked questions which might be also commonly asked by developers in other search engines. In the future, we will extend the scale and scope of test queries.</p>
<p>RELATED WORK 7.1 Code Search</p>
<p>In code search, a line of work has investigated marrying stateof-the-art information retrieval and natural language processing techniques [13-15, 32, 35, 41, 45-47, 61, 81, 82]. Much of the existing work focuses on query expansion and reformulation [29,31,44]. For example, Hill et al. [30] reformulated queries with natural language phrasal representations of method signatures. Haiduc et al. [29] proposed to reformulate queries based on machine learning. Their method trains a machine learning model that automatically recommends a reformulation strategy based on the query properties. Lu et al. [44] proposed to extend a query with synonyms generated from WordNet. There is also much work that takes into account code characteristics. For example, McMillan et al. [47] proposed Portfolio, a code search engine that combines keyword matching with PageRank to return a chain of functions. Lv et al. [45] proposed CodeHow, a code search tool that incorporates an extended Boolean model and API matching. Ponzanelli et al. [61] proposed an approach that automatically retrieves pertinent discussions from Stack Overflow given a context in the IDE. Recently Li et al. [41] proposed RACS, a code search framework for JavaScript that considers relationships (e.g., sequencing, condition, and callback relationships) among the invoked API methods.</p>
<p>As described in Section 6, DeepCS differs from existing code search techniques in that it does not rely on information retrieval techniques. It measures the similarity between code snippets and user queries through joint embedding and deep learning. Thus, it can better understand code and query semantics.</p>
<p>As the keyword based approaches are inefficient on recognizing semantics, researchers have drawn increasing attention on semantics based code search [34,65,69]. For example, Reiss [65] proposed the semantics-based code search, which uses user specifications to characterize the requirement and uses transformations to adapt the searching results. However, Reiss's approach differs significantly from DeepCS. It does not consider the semantics of natural language queries. Furthermore, it requires users to provide not only natural language queries but also other specifications such as method declarations and test cases.</p>
<p>Besides code search, there have been many other information retrieval tasks in software engineering [8,9,16,23,24,29,51,55,63,67] such as bug localization [66,73,80], feature localization [19], traceability links recovery [20] and community Question Answering [11]. Ye et al. [80] proposed to embed words into vector representations to bridge the lexical gap between source code and natural language for SE-related text retrieval tasks. Different from DeepCS, the vector representations learned by their method are at the level of individual words and tokens instead of the whole query sentences. Their method is based on a bag-of-words assumption, and word sequences are not considered.</p>
<p>Deep Learning for Source Code</p>
<p>Recently, researchers have investigated possible applications of deep learning techniques to source code [7,38,53,56,60,64,75,76]. A typical use of deep learning is code generation [42,54]. For example, Mou et al. [54] proposed to generate code from natural language user intentions using an RNN Encoder-Decoder model. Their results show the feasibility of applying deep learning techniques to code generation from a highly homogeneous dataset (simple programming assignments). Gu et al. [27] applies deep learning for API learning, that is, generating API usage sequences for a given natural language query. They also apply deep learning to migrate APIs between different programming languages [28]. Deep learning is also applied to code completion [64,77]. For example, White et al. [77] applied the RNN language model to source code files and showed its effectiveness in predicting software tokens. Recently, White et al. [76] also applied deep learning to code clone detection. Their framework for automatically links patterns mined at the lexical level with patterns mined at the syntactic level. In our work, we explore the application of deep learning to code search.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose a novel deep neural network named CO-DEnn for code search. Instead of matching text similarity, CODEnn learns a unified vector representation of both source code and natural language queries so that code snippets semantically related to a query can be retrieved according to their vectors. As a proofof-concept application, we implement a code search tool DeepCS based on the proposed CODEnn model 6 . Our experimental study has shown that the proposed approach is effective and outperforms the related approaches.</p>
<p>In the future, we will investigate more aspects of source code such as control structures to better represent high-level semantics of source code. The deep neural network we designed may also benefit other software engineering problems such as bug localization.</p>
<p>Figure 2 :
2Illustration of max pooling</p>
<p>Figure 3 :
3An example showing the idea of joint embedding for code and queries. The yellow points represent query vectors while the blue points represent code vectors.</p>
<p>Figure 4 :
4The structure of the Code-Description Embedding Neural Network</p>
<p>Figure 5 :
5The overall workflow of DeepCS D+ (a correct description of C) as well as a negative description (an incorrect description of C) D-randomly chosen from the pool of all D+'s. When trained on the set of C, D+, D-triples, the CODEnn predicts the cosine similarities of both C, D+ and C, D-pairs and minimizes the ranking loss</p>
<p>
For each method call o.m() where o is an instance of class C, we produce C.m and append it to the API sequence.  For a method call passed as a parameter, we append the method before the calling method. For example, o 1 .m 1 (o 2 .m 2 (),o 3 .m 3 ()), we produce a sequence C 2 .m 2 -C 3 .m 3 -C 1 .m 1 , where C i is the class of the instance o i .  For a sequence of statements s 1 ; s 2 ;...;s N , we extract the API sequence a i from each statement s i , concatenate them to the API sequence a 1 -a 2 -...-a N .</p>
<p>Figure 7 :
7An example of extracting code elements from a Java method DateUtils.toCalendar 3 Figure 7 shows an example of code elements and documentation comments extracted from a Java method DateU tils.toCalendar 3 in the Apache commons-lang library.</p>
<p>Figure 8 :
8The statistical comparison of FRank and Precison@k for three code search approaches</p>
<p>Figure 9 :Figure 10 :Figure 11 :
91011a) The third result of the query "queue an event to be run on the thread" ) The first result of the query "run an event on the thread queue" Examples showing the query understanding public static String toStringWithEncoding( InputStream inputStream, String encoding) { if (inputStream == null) throw new IllegalArgumentException( "inputStream-should-not-be-null"); char[] buffer = new char[BUFFER_SIZE]; StringBuffer stringBuffer = new StringBuffer(); BufferedReader bufferedReader = new BufferedReader( new InputStreamReader(inputStream, encoding), BUFFER_SIZE)An example showing the search robustness -The first result of the query "get the content of an input stream as a string using a specified character encoding" public static &lt; S &gt; S deserialize(Class c, File xml) { try { JAXBContext context = JAXBContext.newInstance(c); Unmarshaller unmarshaller = context.createUnmarshaller(); S deserialized = (S) unmarshaller.unmarshal(xml); return deserialized; } catch (JAXBException ex) { log.error("Error-deserializing-object-from-XML", ex); return null; } } (a) The first result of the query "read an object from an xml file" public void playVoice(int clearedLines) throws Exception { int audiosAvailable = audioLibrary.get(clearedLines).size(); int audioIndex = rand.nextInt(audiosAvailable); audioLibrary.get(clearedLines).get(audioIndex).play(); } (b) The second result of the query "play a song" Examples showing the associative searchA unified representation of heterogeneous data Source code and natural language queries are heterogeneous. By jointly embedding source code and natural language query into the same vector representation, their similarities can be measured more accurately. Better query understanding through deep learning Unlike traditional techniques, DeepCS learns queries and source code representations with deep learning. Characteristics of queries, such as</p>
<p>Figure 12 :
12An example showing the inaccurate results -The first result of the query "generate md5"</p>
<p>Method Name: to calendar API sequence: Calendar.getInstance Calendar.setTime Tokens: calendar, get, instance, set, time, date Description: converts a date into a calendar.Method Name 
API Sequence 
Tokens 
Description (English) 
1 file reader 
InputStream.readOutputStream.write input, output, stream, write 
copy a file from an inputstream 
2 open 
URL.newURL.openConnection 
url, open, conn 
open a url 
3 test exists 
File.newFile.exists 
file, create, exists 
test file exists 
 
 
 
 
 </p>
<p>Figure 6: An excerpt of training tuples </p>
<p>/*<em> 
* Converts a Date into a Calendar. 
* @param date the date to convert to a Calendar 
* @return the created Calendar 
* @throws NullPointerException if null is passed in 
* @since 3.0 
</em>/ 
public static Calendar toCalendar(final Date date) { 
final Calendar c = Calendar.getInstance(); 
c.setTime(date); 
return c; 
} </p>
<p>Table 1 :
1Benchmark Queries and Evaluation Results (NF: Not Found within the top 10 returned results LC:Lucene CH:CodeHow DCS:DeepCS)No. 
Question 
ID 
Query 
FRank 
LC CH DCS 
1 309424 
convert an inputstream to a string 
2 1 1 
2 157944 
create arraylist from array 
NF NF 2 
3 1066589 iterate through a hashmap 
NF 4 1 
4 363681 
generating random integers in a specific range 
NF 6 2 
5 5585779 converting string to int in java 
NF 10 1 
6 1005073 initialization of an array in one line 
NF 4 1 
7 1128723 how can I test if an array contains a certain value 
6 6 1 
8 604424 
lookup enum by string value 
1 NF 10 
9 886955 
breaking out of nested loops in java 
NF NF NF 
10 1200621 how to declare an array 
NF NF 4 
11 41107 
how to generate a random alpha-numeric string 
NF 1 1 
12 409784 
what is the simplest way to print a java array 
6 NF 1 
13 109383 
sort a map by values 
NF 1 3 
14 295579 
fastest way to determine if an integer's square root is an integer NF NF NF 
15 80476 
how can I concatenate two arrays in java 
NF 1 1 
16 326369 
how do I create a java string from the contents of a file 
8 NF 5 
17 1149703 how can I convert a stack trace to a string 
3 1 2 
18 513832 
how do I compare strings in java 
1 3 1 
19 3481828 how to split a string in java 
1 1 1 
20 2885173 how to create a file and write to a file in java 
2 1 NF 
21 507602 
how can I initialise a static map 
7 1 2 
22 223918 
iterating through a collection, avoiding concurrentmodifica-
tionexception when removing in loop </p>
<p>3 3 2 </p>
<p>23 415953 
how can I generate an md5 hash 
1 3 6 
24 1069066 get current stack trace in java 
3 1 1 
25 2784514 sort arraylist of custom objects by property 
1 1 1 
26 153724 
how to round a number to n decimal places in java 
1 1 4 
27 473282 
how can I pad an integers with zeros on the left 
NF 3 1 
28 529085 
how to create a generic array in java 
NF NF 3 
29 4716503 reading a plain text file in java 
4 NF 7 
30 1104975 a for loop to iterate over enum in java 
NF NF NF 
31 3076078 check if at least two out of three booleans are true 
NF NF NF 
32 4105331 how do I convert from int to string 
2 1 NF 
33 8172420 how to convert a char to a string in java 
5 10 3 
34 1816673 how do I check if a file exists in java 
1 2 1 
35 4216745 java string to date conversion 
6 NF 1 
36 1264709 convert inputstream to byte array in java 
7 5 1 
37 1102891 how to check if a string is numeric in java 
1 NF 2 
38 869033 
how do I copy an object in java 
2 1 1 
39 180158 
how do I time a method's execution in java 
NF NF 2 
40 5868369 how to read a large text file line by line using java 
1 1 1 
41 858572 
how to make a new list in java 
2 1 1 
42 1625234 how to append text to an existing file in java 
3 1 1 
43 2201925 converting iso 8601-compliant string to date 
3 1 1 
44 122105 
what is the best way to filter a java collection 
NF 9 2 
45 5455794 removing whitespace from strings in java 
NF 3 1 
46 225337 
how do I split a string with any whitespace chars as delimiters 
1 1 2 
47 52353 
in java, what is the best way to determine the size of an object NF NF NF 
48 160970 
how do I invoke a java method when given the method name 
as a string </p>
<p>3 1 2 </p>
<p>49 207947 
how do I get a platform dependent new line character 
1 NF 10 
50 1026723 how to convert a map to list in java 
6 NF 1 </p>
<p>Table 2 :
2Overall Accuracy of DeepCS and the Related ApproachesTool 
R@1 R@5 R@10 P@1 P@5 P@10 MRR 
Lucene 
0.24 0.48 0.62 0.24 0.24 0.26 0.35 
CodeHow 
0.38 0.58 0.66 0.38 0.29 0.28 0.45 
DeepCS 
0.46 0.76 0.86 0.46 0.50 0.49 0.60 </p>
<p>A documentation comment in JAVA starts with slash-asterisk-asterisk (/ * * ) and ends with asterisk-slash ( * /) 2 http://www.oracle.com/technetwork/articles/java/index-137868.html
https://github.com/apache/commons-lang/blob/master/src/main/java/org/apache/ commons/lang3/time/DateUtils.java
http://taoxie.cs.illinois.edu/racs/subjects.html 5 http://stackoverflow.com/questions/tagged/java?sort=votes&amp;pagesize=15
Our source code and data are available at: https://github.com/guxd/deep-code-search
ACKNOWLEDGMENTThe authors would like to thank Dongmei Zhang at Microsoft Research Asia for her support for this project and insightful comments on the paper.
. Jdt Eclipse, Eclipse JDT. http://www.eclipse.org/jdt/.</p>
<p>. Github, Github. https://github.com.</p>
<p>. Keras, Keras. https://keras.io/.</p>
<p>. Lucene, Lucene. https://lucene.apache.org/.</p>
<p>A convolutional attention network for extreme summarization of source code. M Allamanis, H Peng, C Sutton, International Conference on Machine Learning (ICML). M. Allamanis, H. Peng, and C. Sutton. A convolutional attention network for extreme summarization of source code. In International Conference on Machine Learning (ICML), 2016.</p>
<p>Reducing the effort of bug report triage: Recommenders for development-oriented decisions. J Anvik, G C Murphy, ACM Transactions on Software Engineering and Methodology (TOSEM). 20310J. Anvik and G. C. Murphy. Reducing the effort of bug report triage: Recom- menders for development-oriented decisions. ACM Transactions on Software Engineering and Methodology (TOSEM), 20(3):10, 2011.</p>
<p>Linking e-mails and source code artifacts. A Bacchelli, M Lanza, R Robbes, Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering. the 32nd ACM/IEEE International Conference on Software EngineeringACM1A. Bacchelli, M. Lanza, and R. Robbes. Linking e-mails and source code arti- facts. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1, pages 375-384. ACM, 2010.</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Facilitating crowd sourced software engineering via stack overflow. O Barzilay, C Treude, A Zagalsky, Finding Source Code on the Web for Remix and Reuse. SpringerO. Barzilay, C. Treude, and A. Zagalsky. Facilitating crowd sourced software engineering via stack overflow. In Finding Source Code on the Web for Remix and Reuse, pages 289-308. Springer, 2013.</p>
<p>Program understanding and the concept assignment problem. T J Biggerstaff, B G Mitbander, D E Webster, Communications of the ACM. 375T. J. Biggerstaff, B. G. Mitbander, and D. E. Webster. Program understanding and the concept assignment problem. Communications of the ACM, 37(5):72-82, 1994.</p>
<p>Example-centric programming: integrating web search into the development environment. J Brandt, M Dontcheva, M Weskamp, S R Klemmer, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACMJ. Brandt, M. Dontcheva, M. Weskamp, and S. R. Klemmer. Example-centric programming: integrating web search into the development environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 513-522. ACM, 2010.</p>
<p>B A Campbell, C Treude, arXiv:1701.05648NLP2Code: Code snippet content assist via natural language tasks. arXiv preprintB. A. Campbell and C. Treude. NLP2Code: Code snippet content assist via natural language tasks. arXiv preprint arXiv:1701.05648, 2017.</p>
<p>Searching connected API subgraph via text phrases. W.-K Chan, H Cheng, D Lo, Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. the ACM SIGSOFT 20th International Symposium on the Foundations of Software EngineeringACM10W.-K. Chan, H. Cheng, and D. Lo. Searching connected API subgraph via text phrases. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, page 10. ACM, 2012.</p>
<p>On the reduction of verbose queries in text retrieval based software maintenance. O Chaparro, A Marcus, Proceedings of the 38th International Conference on Software Engineering Companion. the 38th International Conference on Software Engineering CompanionACMO. Chaparro and A. Marcus. On the reduction of verbose queries in text retrieval based software maintenance. In Proceedings of the 38th International Conference on Software Engineering Companion, pages 716-718. ACM, 2016.</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. K Cho, B Van Merrinboer,  Glehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsK. Cho, B. Van Merrinboer, . Glehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics.</p>
<p>Natural language processing (almost) from scratch. R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Journal of Machine Learning Research. 12R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493-2537, 2011.</p>
<p>Exploring the use of deep learning for feature location. C S Corley, K Damevski, N A Kraft, Software Maintenance and Evolution (ICSME), 2015 IEEE International Conference on. IEEEC. S. Corley, K. Damevski, and N. A. Kraft. Exploring the use of deep learning for feature location. In Software Maintenance and Evolution (ICSME), 2015 IEEE International Conference on, pages 556-560. IEEE, 2015.</p>
<p>Recovering traceability links between an api and its learning resources. B Dagenais, M P Robillard, 34th International Conference on Software Engineering (ICSE). IEEEB. Dagenais and M. P. Robillard. Recovering traceability links between an api and its learning resources. In 2012 34th International Conference on Software Engineering (ICSE), pages 47-57. IEEE, 2012.</p>
<p>Applying deep learning to answer selection: A study and an open task. M Feng, B Xiang, M R Glass, L Wang, B Zhou, 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEEM. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 813-820. IEEE, 2015.</p>
<p>DeViSE: A deep visual-semantic embedding model. A Frome, G S Corrado, J Shlens, S Bengio, J Dean, T Mikolov, Advances in neural information processing systems. A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. DeViSE: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121-2129, 2013.</p>
<p>Design and evaluation of a multi-recommendation system for local code search. X Ge, D C Shepherd, K Damevski, E Murphy-Hill, Journal of Visual Languages &amp; Computing. X. Ge, D. C. Shepherd, K. Damevski, and E. Murphy-Hill. Design and evaluation of a multi-recommendation system for local code search. Journal of Visual Languages &amp; Computing, 2016.</p>
<p>An exploratory study of the pull-based software development model. G Gousios, M Pinzger, A V Deursen, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringACMG. Gousios, M. Pinzger, and A. v. Deursen. An exploratory study of the pull-based software development model. In Proceedings of the 36th International Conference on Software Engineering, pages 345-355. ACM, 2014.</p>
<p>A novel connectionist system for unconstrained handwriting recognition. A Graves, M Liwicki, S Fernndez, R Bertolami, H Bunke, J Schmidhuber, IEEE transactions on pattern analysis and machine intelligence. 31A. Graves, M. Liwicki, S. Fernndez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE transactions on pattern analysis and machine intelligence, 31(5):855-868, 2009.</p>
<p>A search engine for finding highly relevant applications. M Grechanik, C Fu, Q Xie, C Mcmillan, D Poshyvanyk, C Cumby, 2010 ACM/IEEE 32nd International Conference on Software Engineering. IEEE1M. Grechanik, C. Fu, Q. Xie, C. McMillan, D. Poshyvanyk, and C. Cumby. A search engine for finding highly relevant applications. In 2010 ACM/IEEE 32nd International Conference on Software Engineering, volume 1, pages 475-484. IEEE, 2010.</p>
<p>Deep API learning. X Gu, H Zhang, D Zhang, S Kim, Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16). the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16)X. Gu, H. Zhang, D. Zhang, and S. Kim. Deep API learning. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16), 2016.</p>
<p>DeepAM: Migrate APIs with multi-modal sequence to sequence learning. X Gu, H Zhang, D Zhang, S Kim, Proceedings of the Twenty-Sixth International Joint Conferences on Artifical Intelligence (IJCAI'17. the Twenty-Sixth International Joint Conferences on Artifical Intelligence (IJCAI'17X. Gu, H. Zhang, D. Zhang, and S. Kim. DeepAM: Migrate APIs with multi-modal sequence to sequence learning. In Proceedings of the Twenty-Sixth International Joint Conferences on Artifical Intelligence (IJCAI'17), 2017.</p>
<p>Automatic query reformulations for text retrieval in software engineering. S Haiduc, G Bavota, A Marcus, R Oliveto, A De Lucia, T Menzies, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressS. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and T. Menzies. Au- tomatic query reformulations for text retrieval in software engineering. In Proceedings of the 2013 International Conference on Software Engineering, pages 842-851. IEEE Press, 2013.</p>
<p>Improving source code search with natural language phrasal representations of method signatures. E Hill, L Pollock, K Vijay-Shanker, Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering. the 2011 26th IEEE/ACM International Conference on Automated Software EngineeringIEEE Computer SocietyE. Hill, L. Pollock, and K. Vijay-Shanker. Improving source code search with nat- ural language phrasal representations of method signatures. In Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering, pages 524-527. IEEE Computer Society, 2011.</p>
<p>NL-based query refinement and contextualized code search results: A user study. E Hill, M Roldan-Vega, J A Fails, G Mallet, Software Maintenance, Reengineering and Reverse Engineering. IEEECSMR-WCRE2014 Software Evolution Week-IEEE Conference onE. Hill, M. Roldan-Vega, J. A. Fails, and G. Mallet. NL-based query refinement and contextualized code search results: A user study. In Software Maintenance, Reengineering and Reverse Engineering (CSMR-WCRE), 2014 Software Evolution Week-IEEE Conference on, pages 34-43. IEEE, 2014.</p>
<p>The end-to-end use of source code examples: An exploratory study. R Holmes, R Cottrell, R J Walker, J Denzinger, Software Maintenance, 2009. ICSM 2009. IEEE International Conference on. IEEER. Holmes, R. Cottrell, R. J. Walker, and J. Denzinger. The end-to-end use of source code examples: An exploratory study. In Software Maintenance, 2009. ICSM 2009. IEEE International Conference on, pages 555-558. IEEE, 2009.</p>
<p>Deep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128-3137, 2015.</p>
<p>Repairing programs with semantic code search (T). Y Ke, K T Stolee, C Le Goues, Y Brun, 30th IEEE/ACM International Conference on. IEEEAutomated Software Engineering (ASE)Y. Ke, K. T. Stolee, C. Le Goues, and Y. Brun. Repairing programs with semantic code search (T). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on, pages 295-306. IEEE, 2015.</p>
<p>Spotting working code examples. I Keivanloo, J Rilling, Y Zou, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringACMI. Keivanloo, J. Rilling, and Y. Zou. Spotting working code examples. In Proceedings of the 36th International Conference on Software Engineering, pages 664-675. ACM, 2014.</p>
<p>Convolutional neural networks for sentence classification. Y Kim, arXiv:1408.5882arXiv preprintY. Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.</p>
<p>Adam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980arXiv preprintD. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Combining deep learning with information retrieval to localize buggy files for bug reports (n). A N Lam, A T Nguyen, H A Nguyen, T N Nguyen, A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen. Combining deep learning with information retrieval to localize buggy files for bug reports (n).</p>
<p>30th IEEE/ACM International Conference on. IEEEAutomated Software Engineering (ASE)In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on, pages 476-481. IEEE, 2015.</p>
<p>Distributed representations of sentences and documents. Q Le, T Mikolov, Proceedings of the 31st International Conference on Machine Learning (ICML-14). the 31st International Conference on Machine Learning (ICML-14)Q. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188-1196, 2014.</p>
<p>Efficient mini-batch training for stochastic optimization. M Li, T Zhang, Y Chen, A J Smola, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACMM. Li, T. Zhang, Y. Chen, and A. J. Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 661-670. ACM, 2014.</p>
<p>Relationship-aware code search for JavaScript frameworks. X Li, Z Wang, Q Wang, S Yan, T Xie, H Mei, Proceedings of the ACM SIGSOFT 24th International Symposium on the Foundations of Software Engineering. the ACM SIGSOFT 24th International Symposium on the Foundations of Software EngineeringACMX. Li, Z. Wang, Q. Wang, S. Yan, T. Xie, and H. Mei. Relationship-aware code search for JavaScript frameworks. In Proceedings of the ACM SIGSOFT 24th International Symposium on the Foundations of Software Engineering. ACM, 2016.</p>
<p>Latent predictor networks for code generation. W Ling, E Grefenstette, K M Hermann, T Kocisky, A Senior, F Wang, P Blunsom, arXiv:1603.06744arXiv preprintW. Ling, E. Grefenstette, K. M. Hermann, T. Kocisky, A. Senior, F. Wang, and P. Blunsom. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744, 2016.</p>
<p>Sourcerer: mining and searching internet-scale software repositories. E Linstead, S Bajracharya, T Ngo, P Rigor, C Lopes, P Baldi, Data Mining and Knowledge Discovery. 18E. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and P. Baldi. Sourcerer: mining and searching internet-scale software repositories. Data Mining and Knowledge Discovery, 18:300-336, 2009.</p>
<p>Query expansion via wordnet for effective code search. M Lu, X Sun, S Wang, D Lo, Y Duan, IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEEM. Lu, X. Sun, S. Wang, D. Lo, and Y. Duan. Query expansion via wordnet for effective code search. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER), pages 545-549. IEEE, 2015.</p>
<p>CodeHow: Effective code search based on API understanding and extended boolean model. F Lv, H Zhang, J Lou, S Wang, D Zhang, J Zhao, Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering. the 30th IEEE/ACM International Conference on Automated Software EngineeringIEEEF. Lv, H. Zhang, J. Lou, S. Wang, D. Zhang, and J. Zhao. CodeHow: Effective code search based on API understanding and extended boolean model. In Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE 2015). IEEE, 2015.</p>
<p>Exemplar: A source code search engine for finding highly relevant applications. C Mcmillan, M Grechanik, D Poshyvanyk, C Fu, Q Xie, IEEE Transactions on Software Engineering. 385C. McMillan, M. Grechanik, D. Poshyvanyk, C. Fu, and Q. Xie. Exemplar: A source code search engine for finding highly relevant applications. IEEE Transactions on Software Engineering, 38(5):1069-1087, 2012.</p>
<p>Portfolio: finding relevant functions and their usage. C Mcmillan, M Grechanik, D Poshyvanyk, Q Xie, C Fu, Proceedings of the 33rd International Conference on Software Engineering (ICSE'11). the 33rd International Conference on Software Engineering (ICSE'11)IEEEC. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu. Portfolio: find- ing relevant functions and their usage. In Proceedings of the 33rd International Conference on Software Engineering (ICSE'11), pages 111-120. IEEE, 2011.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</p>
<p>Recurrent neural network based language model. T Mikolov, M Karafit, L Burget, J Cernock, S Khudanpur, INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association. Makuhari, Chiba, JapanT. Mikolov, M. Karafit, L. Burget, J. Cernock, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045-1048, 2010.</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed rep- resentations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119, 2013.</p>
<p>A large scale empirical study on software reuse in mobile apps. I J Mojica, B Adams, M Nagappan, S Dienst, T Berger, A E Hassan, IEEE Software. 312I. J. Mojica, B. Adams, M. Nagappan, S. Dienst, T. Berger, and A. E. Hassan. A large scale empirical study on software reuse in mobile apps. IEEE Software, 31(2):78-86, 2014.</p>
<p>Training feedforward neural networks using genetic algorithms. D J Montana, L Davis, IJCAI. 89D. J. Montana and L. Davis. Training feedforward neural networks using genetic algorithms. In IJCAI, volume 89, pages 762-767, 1989.</p>
<p>Convolutional neural networks over tree structures for programming language processing. L Mou, G Li, L Zhang, T Wang, Z Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16. the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16AAAI PressL. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. Convolutional neural networks over tree structures for programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pages 1287-1293. AAAI Press, 2016.</p>
<p>On end-to-end program generation from user intention by deep neural networks. L Mou, R Men, G Li, L Zhang, Z Jin, arXivL. Mou, R. Men, G. Li, L. Zhang, and Z. Jin. On end-to-end program generation from user intention by deep neural networks. arXiv, 2015.</p>
<p>Software engineering for the web: the state of the practice. A Nederlof, A Mesbah, A V Deursen, Companion Proceedings of the 36th International Conference on Software Engineering. ACMA. Nederlof, A. Mesbah, and A. v. Deursen. Software engineering for the web: the state of the practice. In Companion Proceedings of the 36th International Conference on Software Engineering, pages 4-13. ACM, 2014.</p>
<p>Exploring api embedding for api usages and applications. T D Nguyen, A T Nguyen, H D Phan, T N Nguyen, Proceedings of the 39th International Conference on Software Engineering. the 39th International Conference on Software EngineeringIEEE PressT. D. Nguyen, A. T. Nguyen, H. D. Phan, and T. N. Nguyen. Exploring api embedding for api usages and applications. In Proceedings of the 39th International Conference on Software Engineering, pages 438-449. IEEE Press, 2017.</p>
<p>Query expansion based on crowd knowledge for code search. L Nie, H Jiang, Z Ren, Z Sun, X Li, IEEE Transactions on Services Computing. 95L. Nie, H. Jiang, Z. Ren, Z. Sun, and X. Li. Query expansion based on crowd knowledge for code search. IEEE Transactions on Services Computing, 9(5):771-783, 2016.</p>
<p>Learning to rank code examples for code search engines. H Niu, I Keivanloo, Y Zou, Empirical Software Engineering. H. Niu, I. Keivanloo, and Y. Zou. Learning to rank code examples for code search engines. Empirical Software Engineering, pages 1-33, 2016.</p>
<p>Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. H Palangi, L Deng, Y Shen, J Gao, X He, J Chen, X Song, R K Ward, abs/1502.06922CoRRH. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. K. Ward. Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. CoRR, abs/1502.06922, 2015.</p>
<p>Building program vector representations for deep learning. H Peng, L Mou, G Li, Y Liu, L Zhang, Z Jin, Proceedings of the 8th International Conference on Knowledge Science. the 8th International Conference on Knowledge ScienceNew York, NY, USA; New York, IncSpringer-Verlag9403KSEM 2015H. Peng, L. Mou, G. Li, Y. Liu, L. Zhang, and Z. Jin. Building program vector rep- resentations for deep learning. In Proceedings of the 8th International Conference on Knowledge Science, Engineering and Management -Volume 9403, KSEM 2015, pages 547-553, New York, NY, USA, 2015. Springer-Verlag New York, Inc.</p>
<p>Mining stackoverflow to turn the ide into a self-confident programming prompter. L Ponzanelli, G Bavota, M Di Penta, R Oliveto, M Lanza, Proceedings of the 11th Working Conference on Mining Software Repositories. the 11th Working Conference on Mining Software RepositoriesACML. Ponzanelli, G. Bavota, M. Di Penta, R. Oliveto, and M. Lanza. Mining stackover- flow to turn the ide into a self-confident programming prompter. In Proceedings of the 11th Working Conference on Mining Software Repositories, pages 102-111. ACM, 2014.</p>
<p>SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis. M Raghothaman, Y Wei, Y Hamadi, Proceedings of the 38th International Conference on Software Engineering. the 38th International Conference on Software EngineeringACMM. Raghothaman, Y. Wei, and Y. Hamadi. SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis. In Proceedings of the 38th International Conference on Software Engineering, pages 357-367. ACM, 2016.</p>
<p>Patterns of co-evolution between requirements and source code. M Rahimi, J Cleland-Huang, IEEE Fifth International Workshop on Requirements Patterns (RePa). IEEEM. Rahimi and J. Cleland-Huang. Patterns of co-evolution between requirements and source code. In 2015 IEEE Fifth International Workshop on Requirements Patterns (RePa), pages 25-31. IEEE, 2015.</p>
<p>Code completion with statistical language models. V Raychev, M Vechev, E Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 35th ACM SIGPLAN Conference on Programming Language Design and ImplementationACMV. Raychev, M. Vechev, and E. Yahav. Code completion with statistical language models. In In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation. ACM, 2014.</p>
<p>Semantics-based code search. S P Reiss, Proceedings of the 31st International Conference on Software Engineering. the 31st International Conference on Software EngineeringIEEE Computer SocietyS. P. Reiss. Semantics-based code search. In Proceedings of the 31st International Conference on Software Engineering, pages 243-253. IEEE Computer Society, 2009.</p>
<p>Fault localization with nearest neighbor queries. M Renieres, S P Reiss, Proceedings. 18th IEEE International Conference on. 18th IEEE International Conference onAutomated Software EngineeringM. Renieres and S. P. Reiss. Fault localization with nearest neighbor queries. In Automated Software Engineering, 2003. Proceedings. 18th IEEE International Conference on, pages 30-39, Oct 2003.</p>
<p>Discovering essential code elements in informal documentation. P C Rigby, M P Robillard, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressP. C. Rigby and M. P. Robillard. Discovering essential code elements in informal documentation. In Proceedings of the 2013 International Conference on Software Engineering, pages 832-841. IEEE Press, 2013.</p>
<p>An examination of software engineering work practices. J Singer, T Lethbridge, N Vinson, N Anquetil, CASCON First Decade High Impact Papers. IBM Corp.J. Singer, T. Lethbridge, N. Vinson, and N. Anquetil. An examination of software engineering work practices. In CASCON First Decade High Impact Papers, pages 174-188. IBM Corp., 2010.</p>
<p>Solving the search for source code. K T Stolee, S Elbaum, D Dobos, ACM Transactions on Software Engineering and Methodology (TOSEM). 23326K. T. Stolee, S. Elbaum, and D. Dobos. Solving the search for source code. ACM Transactions on Software Engineering and Methodology (TOSEM), 23(3):26, 2014.</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in neural information processing systems. I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112, 2014.</p>
<p>Lstm-based deep learning models for non-factoid answer selection. M Tan, B Xiang, B Zhou, arXiv:1511.04108arXiv preprintM. Tan, B. Xiang, and B. Zhou. Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108, 2015.</p>
<p>Word representations: a simple and general method for semi-supervised learning. J Turian, L Ratinov, Y Bengio, Proceedings of the 48th annual meeting of the association for computational linguistics. the 48th annual meeting of the association for computational linguisticsAssociation for Computational LinguisticsJ. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384-394. Association for Computational Linguistics, 2010.</p>
<p>Using a distributed representation of words in localizing relevant files for bug reports. Y Uneno, O Mizuno, E.-H Choi, Software Quality, Reliability and Security (QRS), 2016 IEEE International Conference on. IEEEY. Uneno, O. Mizuno, and E.-H. Choi. Using a distributed representation of words in localizing relevant files for bug reports. In Software Quality, Reliability and Security (QRS), 2016 IEEE International Conference on, pages 183-190. IEEE, 2016.</p>
<p>Wsabie: scaling up to large vocabulary image annotation. J Weston, S Bengio, N Usunier, Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three. the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume ThreeAAAI PressJ. Weston, S. Bengio, and N. Usunier. Wsabie: scaling up to large vocabulary image annotation. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764-2770. AAAI Press, 2011.</p>
<p>Sorting and transforming program repair ingredients via deep learning code similarities. M White, M Tufano, M Martinez, M Monperrus, D Poshyvanyk, arXiv:1707.04742arXiv preprintM. White, M. Tufano, M. Martinez, M. Monperrus, and D. Poshyvanyk. Sorting and transforming program repair ingredients via deep learning code similarities. arXiv preprint arXiv:1707.04742, 2017.</p>
<p>Deep learning code fragments for code clone detection. M White, M Tufano, C Vendome, D Poshyvanyk, Proceedings of the 31th IEEE/ACM International Conference on Automated Software Engineering (ASE 2016). the 31th IEEE/ACM International Conference on Automated Software Engineering (ASE 2016)M. White, M. Tufano, C. Vendome, and D. Poshyvanyk. Deep learning code frag- ments for code clone detection. In Proceedings of the 31th IEEE/ACM International Conference on Automated Software Engineering (ASE 2016), 2016.</p>
<p>Toward deep learning software repositories. M White, C Vendome, M Linares-Vsquez, D Poshyvanyk, Mining Software Repositories (MSR). M. White, C. Vendome, M. Linares-Vsquez, and D. Poshyvanyk. Toward deep learning software repositories. In Mining Software Repositories (MSR), 2015</p>
<p>IEEE/ACM 12th Working Conference on. IEEEIEEE/ACM 12th Working Conference on, pages 334-345. IEEE, 2015.</p>
<p>Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. R Xu, C Xiong, W Chen, J J Corso, AAAI. CiteseerR. Xu, C. Xiong, W. Chen, and J. J. Corso. Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. In AAAI, pages 2346-2352. Citeseer, 2015.</p>
<p>Learning to rank relevant files for bug reports using domain knowledge. X Ye, R Bunescu, C Liu, Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringACMX. Ye, R. Bunescu, and C. Liu. Learning to rank relevant files for bug reports using domain knowledge. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, pages 689-699. ACM, 2014.</p>
<p>From word embeddings to document similarities for improved information retrieval in software engineering. X Ye, H Shen, X Ma, R Bunescu, C Liu, Proceedings of the 38th International Conference on Software Engineering. the 38th International Conference on Software EngineeringACMX. Ye, H. Shen, X. Ma, R. Bunescu, and C. Liu. From word embeddings to docu- ment similarities for improved information retrieval in software engineering. In Proceedings of the 38th International Conference on Software Engineering, pages 404-415. ACM, 2016.</p>
<p>Bing developer assistant: Improving developer productivity by recommending sample code. H Zhang, A Jain, G Khandelwal, C Kaushik, S Ge, W Hu, Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016. the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016ACMH. Zhang, A. Jain, G. Khandelwal, C. Kaushik, S. Ge, and W. Hu. Bing developer assistant: Improving developer productivity by recommending sample code. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Founda- tions of Software Engineering, FSE 2016, pages 956-961. ACM, 2016.</p>
<p>API Deprecation: A retrospective analysis and detection method for code examples on the web. J Zhou, R J Walker, Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16). the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16)J. Zhou and R. J. Walker. API Deprecation: A retrospective analysis and detection method for code examples on the web. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE'16).</p>            </div>
        </div>

    </div>
</body>
</html>
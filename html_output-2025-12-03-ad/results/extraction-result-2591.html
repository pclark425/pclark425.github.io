<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2591 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2591</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2591</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-3763c501e5523fe5fcd0d370af226d7da9348c8c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3763c501e5523fe5fcd0d370af226d7da9348c8c" target="_blank">A Bayesian machine scientist to aid in the solution of challenging scientific problems</a></p>
                <p><strong>Paper Venue:</strong> Science Advances</p>
                <p><strong>Paper TL;DR:</strong> A Bayesian machine scientist is introduced, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions.</p>
                <p><strong>Paper Abstract:</strong> A Bayesian machine scientist uncovers closed-form mathematical models from data. Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need “machine scientists” that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2591.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2591.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Machine Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian machine scientist (Guimerà et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian symbolic-regression system that assigns posterior plausibilities to closed-form mathematical expressions by combining an explicit prior over expression trees (learned from a corpus) with a BIC-based approximation to the marginal likelihood, and explores the space of expressions with MCMC (including parallel tempering). It returns ensembles of plausible interpretable models and median-predictive closed-form models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian machine scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs closed-form candidate models as expression trees (internal nodes = operations, leaves = variables/parameters). Defines model plausibility as the marginal posterior p(f | D) approximated via description length L(f) ≈ BIC/2 - log p(f). The prior p(f) is learned from a corpus of 4,080 Wikipedia expressions using a maximum-entropy (exponential-family) model constrained to match operation counts and their squares. Model-space exploration is performed with an MCMC sampler with three move types (node replacement, root addition/removal, elementary-tree replacement), canonicalization to avoid duplicate expressions, and parallel tempering across 40 temperatures to mitigate trapping in local modes. Produces ensembles of sampled models (posterior over expressions), enables model averaging (posterior predictive median) and extraction of a single median-predictive closed-form model. Key capabilities demonstrated: discovery of closed-form expressions, recovery of governing differential equations, approximation of functions without closed form (via series), and improved out-of-sample predictive performance versus several baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Discovery System (symbolic regression & equation discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic model discovery across physics (dynamical systems, fluid mechanics), applied mathematics (function approximation), ecology, and social/economic datasets (examples: Rössler dynamical system, turbulent friction in rough pipes — Nikuradse dataset, Bessel function approximations, funding success, cell-to-cell stresses, salmon stocks).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discovering interpretable closed-form mathematical models F(x, θ) from observational data: (i) symbolic regression for static input-output mappings; (ii) discovery of governing ordinary differential equations (inferred functions for derivatives); (iii) approximating functions that lack a simple closed form (e.g., Bessel functions) via series-like closed forms; (iv) model selection and prediction (out-of-sample forecasting) under noisy, small, or heterogeneous datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very large, combinatorial discrete search over expression trees (unbounded closed-form expressions capped at 50 nodes in implementation). Problems involve multiple continuous variables (examples up to 5 variables), nonlinear operations (+, *, sin, log, exp, etc.), and continuous parameters θ (fitted by least squares). Complexity factors: high nonlinearity, huge search space of symbolic structures (structural overfitting risk), multi-objective tradeoff between fit and structural complexity handled probabilistically. Quantitative examples from paper: expressions sampled up to 50 nodes; validation experiments used up to 400 data points (synthetic) and 40 parallel-tempering temperatures (T_k = 1.05^k, k=0..39).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by task: synthetic experiments used 100–400 noisy points (paper reports successful recovery with as few as 100 points for a synthetic target); differential-equation experiments used time-series with derivatives observed with added Gaussian noise (moderate and high noise regimes); Nikuradse dataset and other real datasets are relatively small observational datasets (original Nikuradse experimental points; exact counts not repeatedly specified in main text). Data quality issues: noise in targets and derivatives, small-sample regimes where BIC approximations may be poorer, and some targets not expressible in chosen function set (handled via approximation).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>MCMC with parallel tempering across 40 temperatures; example runs: 2,500 MCMC steps shown for one synthetic experiment (with multiple tempered chains and restarts used for harder datasets). Implementation caps tree size to 50 nodes. No wall-clock compute or FLOP estimates provided. Comparative notes: Eureqa runs were allowed to run for weeks evaluating up to ≈10^13 expressions in the authors' benchmarking setup; EPLEX produced ≈10^6 models (population 1000, 1000 generations) per run; EFS generates models in seconds and was run 100 times to pick the best. The paper emphasizes MCMC is slower than fast heuristics but scales similarly to any method that uses least-squares parameter fitting (dominant cost per model is parameter fitting/BIC).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-specified symbolic-regression and equation-discovery problems: continuous inputs and outputs, deterministic underlying functions with additive observational noise; evaluation metrics are well-defined (likelihood/BIC, cross-validation error, mean absolute error). The problem is open-ended in model structure (infinite discrete model space), stochastic due to data noise, and has clear evaluation criteria (posterior plausibility / description length and predictive error). Requires domain knowledge primarily via choice of operation set and prior (the latter learned empirically here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Posterior plausibility p(f | D) (equivalently description length L(f)), BIC, cross-validation error (out-of-sample MAE), recovery of ground-truth symbolic forms (structural recovery), and predictive accuracy on held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported outcomes: (i) Synthetic expression recovery — machine recovered the exact generating closed-form from 400 noisy points and could do so with as few as 100 points for that example. (ii) Rössler differential equations — machine recovered true governing ODEs when derivatives had moderate noise; at high noise the true models remained plausible but the top-ranked models were regularized (simplified) variants that omitted small terms. (iii) Bessel functions — machine produced closed-form approximations as accurate as high-order Taylor expansions; with noise or restricted expression order it selected progressively lower-order approximations. (iv) Nikuradse turbulent-friction dataset — machine sampled ≈18,000 models; its median predictive model produced significantly lower out-of-sample MAE than Eureqa, EFS, EPLEX and Gaussian-process baselines (exact MAE numbers not provided in main text). Overall, the paper reports consistent superior predictive performance and better structural recovery versus the tested baselines on the presented tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Sampling can get trapped in local maxima (rugged posterior landscape) necessitating parallel tempering and restarts; MCMC is slower than some heuristic methods (computational cost may be high for very large problems); BIC-based description-length approximation can fail with very small datasets or broad parameter posteriors, requiring more accurate marginal-likelihood estimates; choice of prior corpus can bias the prior and may be inappropriate for domains with different mathematical conventions or when dimensional-consistency constraints are needed; limited to the set of allowed operations (cannot produce functions outside that closure except as approximations); structural overfitting is possible without an informative prior.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit Bayesian framework combining data fit (BIC-based likelihood approximation) and an empirically learned prior over expression structure reduces structural overfitting; MCMC sampling with moves that can reach any expression plus parallel tempering improves exploration; canonicalization avoids duplicate expression counting; model averaging (posterior predictive median) improves out-of-sample predictions; demonstration across synthetic, dynamical-system, and real-world datasets shows robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to three state-of-the-art machine-scientist baselines: Eureqa (genetic-programming based), EPLEX (ε-lexicase selection GP), and EFS (automated feature synthesis + sparse regression): (i) On synthetic symbolic-recovery tasks the Bayesian machine scientist recovered the exact generating model while Eureqa, EPLEX and EFS failed and tended to structurally overfit; (ii) For Rössler ODE recovery, sparse-regression approaches specialized for DE discovery (cited in paper) are noted as competitive in low-noise regimes, but the tested heuristics failed in the paper's setups except the Bayesian method which recovered true ODEs at moderate noise; (iii) On the Nikuradse turbulent-friction dataset the Bayesian machine scientist's median-predictive model produced significantly better out-of-sample MAE than Eureqa, EFS, EPLEX and Gaussian processes (quantitative MAE values not provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian machine scientist to aid in the solution of challenging scientific problems', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2591.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2591.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureqa (Schmidt & Lipson style genetic programming symbolic regression software)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic-programming-based symbolic regression system that evolves populations of expression trees to fit data, using user-specified complexity penalties and fitness functions to trade off fit and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureqa (genetic programming symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Represents closed-form expressions as graphs and uses evolutionary operators (mutation, crossover, selection) to evolve populations of candidate formulae scored by a fitness function (e.g., data-fit plus complexity penalty). Users set per-operation complexity penalties; the tool searches broadly but heuristically and focuses on speed and discovery of compact symbolic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Genetic Programming / Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression across physical and empirical datasets (same tasks used as baselines in this paper: synthetic symbolic recovery, Rössler ODEs, Nikuradse dataset, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated discovery of closed-form expressions from input-output data via an evolutionary search over expression trees with user-tunable complexity penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Explores large combinatorial expression space via population-based heuristics; complexity control via hand-specified penalties rather than an explicit Bayesian prior.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on datasets provided by user; benchmarking runs in this paper used the same synthetic and real datasets as the Bayesian machine scientist for head-to-head comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Designed for speed; in the authors' benchmarking Eureqa was run for extended durations (weeks) and allowed to evaluate up to ~10^13 expressions in their setup (authors' settings). Exact compute/resource usage depends on run settings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Same symbolic-regression structure as Bayesian MS; deterministic continuous targets with observational noise, fitness-based evaluation and complexity penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Fitness (data fit minus complexity penalty), MAE on held-out data, visual fit and parsimony.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the paper's experiments Eureqa generally performed better than EPLEX and EFS but worse than the Bayesian machine scientist: it failed to recover the exact synthetic generating expression where the Bayesian approach succeeded, and produced higher out-of-sample MAE on the Nikuradse dataset than the Bayesian machine scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tends to produce structurally overfitted expressions if complexity penalties are not well calibrated; lacks probabilistic posterior over expressions, so cannot naturally average across models; exploration guarantees absent (heuristic search may miss true models).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Fast heuristic exploration, widely used default operation set, effectiveness when properly tuned and given high computational budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Better than EPLEX and EFS in the authors' comparisons but inferior to the Bayesian machine scientist on recovery and predictive tasks reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian machine scientist to aid in the solution of challenging scientific problems', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2591.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2591.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPLEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EPLEX (ε-lexicase selection genetic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic-programming symbolic-regression method using ε-lexicase selection, a selection mechanism intended to preserve diverse partial solutions by considering training cases individually.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EPLEX (ε-lexicase GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Genetic programming with ε-lexicase selection: population-based search where selection considers individual training cases (with ε-tolerance) to maintain behavioral diversity and promote general solutions. Used here as a baseline symbolic-regression method.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Genetic Programming / Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression (benchmarked on same tasks: synthetic expressions, Rössler ODEs, Nikuradse dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover compact closed-form expressions to explain data via evolutionary search that preserves diverse partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles the combinatorial expression space using population-based heuristics; in authors' runs produced ~10^6 models (population 1000 × 1000 generations × 10 repeats).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on supplied datasets; in experiments used same datasets as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Substantial evolutionary compute; authors ran EPLEX per recommended settings (≈10^6 model evaluations per run, repeated 10 times) but EPLEX performed considerably worse than Eureqa in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Heuristic search over discrete expression space; stochastic outcomes across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Fitness on training/validation data, MAE on holdout.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported as considerably worse than Eureqa and the Bayesian machine scientist on the paper's benchmark tasks; specific quantitative error rates not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Inconsistent performance and poorer discovery/prediction in the authors' benchmarks; prone to structural overfitting like other GP approaches without a formal prior/posterior framework.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Maintains diversity via ε-lexicase selection which can help on heterogeneous datasets, but in these experiments did not translate to better recovery/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Worst-performing of the GP baselines in the authors' experiments (EPLEX < EFS < Eureqa < Bayesian MS, qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian machine scientist to aid in the solution of challenging scientific problems', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2591.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2591.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EFS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EFS (Evolutionary Feature Synthesis with sparse regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method that uses genetic algorithms to automatically generate basis functions (features) and then applies sparse regression to select and combine them into closed-form expressions, optimized for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EFS (evolutionary feature synthesis + sparse regression)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates candidate basis functions via genetic operators and applies sparse regression to select a parsimonious linear combination of those basis functions, yielding closed-form expressions efficiently. Combines advantages of sparse regression and automatic basis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Feature Synthesis / Symbolic Regression with Sparse Regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression and model discovery, particularly suited to situations where the true model can be represented as sparse combination of generated basis functions (e.g., some differential-equation discovery tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated synthesis of feature (basis) sets and selection via sparse regression to produce interpretable models quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Can handle large search through basis-function space but is constrained to linear combinations of generated basis functions; this restricts expressivity relative to arbitrary closed-form expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on supplied datasets; in paper used as a fast baseline repeated many times (100 repeats) to choose best model.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Fast: generates candidate models within seconds; repeated runs can aggregate results cheaply (authors ran 100 repeats).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Works best when the true model aligns with sparse linear combination of basis functions; less suited when model structure is not well-represented by generated bases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Mean squared error (default fitness), MAE on holdout data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the paper EFS was fast but tended to structurally overfit in tasks where the Bayesian machine scientist succeeded; its out-of-sample predictive performance was worse than the Bayesian machine scientist and Eureqa on the Nikuradse dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited by assumption that models are sparse linear combinations of basis functions; structural overfitting if basis generation is unconstrained; can miss non-linear nested structures not captured by bases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Speed and ability to automatically generate candidate bases; effective when true model aligns with sparse basis representation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Fastest among baselines in experiments but produced worse predictive performance and structural recovery than the Bayesian machine scientist and Eureqa in the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian machine scientist to aid in the solution of challenging scientific problems', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2591.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2591.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse regression (SINDy and similar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse regression approaches for equation discovery (e.g., SINDy / Brunton et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that assume the governing model is a sparse linear combination of candidate basis functions and perform sparse regression (L0/L1-style selection) to identify the active terms, particularly effective for discovering differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sparse regression for equation discovery (SINDy-family)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a library of candidate nonlinear basis functions (polynomials, trig functions, derivatives, etc.) and uses sparse regression (thresholded least squares, LASSO variants) to select a small subset that reconstructs the dynamics, producing parsimonious governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / Sparse Regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Discovery of governing ordinary differential equations and dynamical-system models from time-series data (physics, chaos, mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify sparse symbolic representations of time-derivative functions (x_dot = Theta(x) * ξ) given measurements (possibly noisy) to recover underlying ODEs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Search limited to linear combinations of a pre-specified (but possibly large) library, reducing combinatorial complexity but requiring the true model to be in the span of the library; sensitive to noise in derivative estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Works well with moderate-to-large time-series and accurate derivative estimates; can fail if derivatives are noisy or library lacks needed terms. Paper notes these methods are particularly suited to reverse-engineer differential equations and could recover true ODEs in low-noise conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Sparse regression solves convex (or quasi-convex) optimization problems; computationally efficient compared to full combinatorial symbolic search; parameter tuning (sparsity thresholds) required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Assumes linear-in-coefficients structure over chosen basis (continuous, deterministic with additive noise); evaluation via recovery accuracy of terms and predictive dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correct support recovery (identification of active terms), parameter estimation accuracy, predictive trajectory error.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper states sparse-regression methods are particularly suited to reverse-engineer differential equations and would be able to recover true expressions in low-noise regimes; in the paper's Rössler experiments they are acknowledged as competitive in low-noise cases, while the Bayesian machine scientist recovered ODEs at moderate noise where heuristic baselines failed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Sensitive to noise in derivatives (requires good derivative estimation), reliant on adequacy of the candidate library, can miss non-sparse models or models not linear in coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>When derivative data are accurate and the library contains the true terms, sparse regression reliably recovers governing equations with low sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as a complementary approach: better suited than generic GP heuristics for ODE discovery in low-noise regimes, but the Bayesian machine scientist performed robustly across moderate-noise scenarios in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian machine scientist to aid in the solution of challenging scientific problems', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Fast, scalable, deterministic symbolic regression technology (FFX) <em>(Rating: 1)</em></li>
                <li>EFS: Evolutionary feature synthesis for symbolic regression (I. Arnaldo et al.) <em>(Rating: 2)</em></li>
                <li>A comprehensive comparison of symbolic regression methods (P. Orzechowski et al.) <em>(Rating: 2)</em></li>
                <li>Prediction of dynamical systems by symbolic regression (Quade et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2591",
    "paper_id": "paper-3763c501e5523fe5fcd0d370af226d7da9348c8c",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Bayesian Machine Scientist",
            "name_full": "Bayesian machine scientist (Guimerà et al.)",
            "brief_description": "A Bayesian symbolic-regression system that assigns posterior plausibilities to closed-form mathematical expressions by combining an explicit prior over expression trees (learned from a corpus) with a BIC-based approximation to the marginal likelihood, and explores the space of expressions with MCMC (including parallel tempering). It returns ensembles of plausible interpretable models and median-predictive closed-form models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Bayesian machine scientist",
            "system_description": "Constructs closed-form candidate models as expression trees (internal nodes = operations, leaves = variables/parameters). Defines model plausibility as the marginal posterior p(f | D) approximated via description length L(f) ≈ BIC/2 - log p(f). The prior p(f) is learned from a corpus of 4,080 Wikipedia expressions using a maximum-entropy (exponential-family) model constrained to match operation counts and their squares. Model-space exploration is performed with an MCMC sampler with three move types (node replacement, root addition/removal, elementary-tree replacement), canonicalization to avoid duplicate expressions, and parallel tempering across 40 temperatures to mitigate trapping in local modes. Produces ensembles of sampled models (posterior over expressions), enables model averaging (posterior predictive median) and extraction of a single median-predictive closed-form model. Key capabilities demonstrated: discovery of closed-form expressions, recovery of governing differential equations, approximation of functions without closed form (via series), and improved out-of-sample predictive performance versus several baselines.",
            "system_type": "AI Scientist / Automated Discovery System (symbolic regression & equation discovery)",
            "problem_domain": "Symbolic model discovery across physics (dynamical systems, fluid mechanics), applied mathematics (function approximation), ecology, and social/economic datasets (examples: Rössler dynamical system, turbulent friction in rough pipes — Nikuradse dataset, Bessel function approximations, funding success, cell-to-cell stresses, salmon stocks).",
            "problem_description": "Discovering interpretable closed-form mathematical models F(x, θ) from observational data: (i) symbolic regression for static input-output mappings; (ii) discovery of governing ordinary differential equations (inferred functions for derivatives); (iii) approximating functions that lack a simple closed form (e.g., Bessel functions) via series-like closed forms; (iv) model selection and prediction (out-of-sample forecasting) under noisy, small, or heterogeneous datasets.",
            "problem_complexity": "Very large, combinatorial discrete search over expression trees (unbounded closed-form expressions capped at 50 nodes in implementation). Problems involve multiple continuous variables (examples up to 5 variables), nonlinear operations (+, *, sin, log, exp, etc.), and continuous parameters θ (fitted by least squares). Complexity factors: high nonlinearity, huge search space of symbolic structures (structural overfitting risk), multi-objective tradeoff between fit and structural complexity handled probabilistically. Quantitative examples from paper: expressions sampled up to 50 nodes; validation experiments used up to 400 data points (synthetic) and 40 parallel-tempering temperatures (T_k = 1.05^k, k=0..39).",
            "data_availability": "Varies by task: synthetic experiments used 100–400 noisy points (paper reports successful recovery with as few as 100 points for a synthetic target); differential-equation experiments used time-series with derivatives observed with added Gaussian noise (moderate and high noise regimes); Nikuradse dataset and other real datasets are relatively small observational datasets (original Nikuradse experimental points; exact counts not repeatedly specified in main text). Data quality issues: noise in targets and derivatives, small-sample regimes where BIC approximations may be poorer, and some targets not expressible in chosen function set (handled via approximation).",
            "computational_requirements": "MCMC with parallel tempering across 40 temperatures; example runs: 2,500 MCMC steps shown for one synthetic experiment (with multiple tempered chains and restarts used for harder datasets). Implementation caps tree size to 50 nodes. No wall-clock compute or FLOP estimates provided. Comparative notes: Eureqa runs were allowed to run for weeks evaluating up to ≈10^13 expressions in the authors' benchmarking setup; EPLEX produced ≈10^6 models (population 1000, 1000 generations) per run; EFS generates models in seconds and was run 100 times to pick the best. The paper emphasizes MCMC is slower than fast heuristics but scales similarly to any method that uses least-squares parameter fitting (dominant cost per model is parameter fitting/BIC).",
            "problem_structure": "Well-specified symbolic-regression and equation-discovery problems: continuous inputs and outputs, deterministic underlying functions with additive observational noise; evaluation metrics are well-defined (likelihood/BIC, cross-validation error, mean absolute error). The problem is open-ended in model structure (infinite discrete model space), stochastic due to data noise, and has clear evaluation criteria (posterior plausibility / description length and predictive error). Requires domain knowledge primarily via choice of operation set and prior (the latter learned empirically here).",
            "success_metric": "Posterior plausibility p(f | D) (equivalently description length L(f)), BIC, cross-validation error (out-of-sample MAE), recovery of ground-truth symbolic forms (structural recovery), and predictive accuracy on held-out data.",
            "success_rate": "Reported outcomes: (i) Synthetic expression recovery — machine recovered the exact generating closed-form from 400 noisy points and could do so with as few as 100 points for that example. (ii) Rössler differential equations — machine recovered true governing ODEs when derivatives had moderate noise; at high noise the true models remained plausible but the top-ranked models were regularized (simplified) variants that omitted small terms. (iii) Bessel functions — machine produced closed-form approximations as accurate as high-order Taylor expansions; with noise or restricted expression order it selected progressively lower-order approximations. (iv) Nikuradse turbulent-friction dataset — machine sampled ≈18,000 models; its median predictive model produced significantly lower out-of-sample MAE than Eureqa, EFS, EPLEX and Gaussian-process baselines (exact MAE numbers not provided in main text). Overall, the paper reports consistent superior predictive performance and better structural recovery versus the tested baselines on the presented tasks.",
            "failure_modes": "Sampling can get trapped in local maxima (rugged posterior landscape) necessitating parallel tempering and restarts; MCMC is slower than some heuristic methods (computational cost may be high for very large problems); BIC-based description-length approximation can fail with very small datasets or broad parameter posteriors, requiring more accurate marginal-likelihood estimates; choice of prior corpus can bias the prior and may be inappropriate for domains with different mathematical conventions or when dimensional-consistency constraints are needed; limited to the set of allowed operations (cannot produce functions outside that closure except as approximations); structural overfitting is possible without an informative prior.",
            "success_factors": "Explicit Bayesian framework combining data fit (BIC-based likelihood approximation) and an empirically learned prior over expression structure reduces structural overfitting; MCMC sampling with moves that can reach any expression plus parallel tempering improves exploration; canonicalization avoids duplicate expression counting; model averaging (posterior predictive median) improves out-of-sample predictions; demonstration across synthetic, dynamical-system, and real-world datasets shows robustness.",
            "comparative_results": "Compared to three state-of-the-art machine-scientist baselines: Eureqa (genetic-programming based), EPLEX (ε-lexicase selection GP), and EFS (automated feature synthesis + sparse regression): (i) On synthetic symbolic-recovery tasks the Bayesian machine scientist recovered the exact generating model while Eureqa, EPLEX and EFS failed and tended to structurally overfit; (ii) For Rössler ODE recovery, sparse-regression approaches specialized for DE discovery (cited in paper) are noted as competitive in low-noise regimes, but the tested heuristics failed in the paper's setups except the Bayesian method which recovered true ODEs at moderate noise; (iii) On the Nikuradse turbulent-friction dataset the Bayesian machine scientist's median-predictive model produced significantly better out-of-sample MAE than Eureqa, EFS, EPLEX and Gaussian processes (quantitative MAE values not provided in main text).",
            "human_baseline": null,
            "uuid": "e2591.0",
            "source_info": {
                "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Eureqa",
            "name_full": "Eureqa (Schmidt & Lipson style genetic programming symbolic regression software)",
            "brief_description": "A genetic-programming-based symbolic regression system that evolves populations of expression trees to fit data, using user-specified complexity penalties and fitness functions to trade off fit and complexity.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Eureqa (genetic programming symbolic regression)",
            "system_description": "Represents closed-form expressions as graphs and uses evolutionary operators (mutation, crossover, selection) to evolve populations of candidate formulae scored by a fitness function (e.g., data-fit plus complexity penalty). Users set per-operation complexity penalties; the tool searches broadly but heuristically and focuses on speed and discovery of compact symbolic laws.",
            "system_type": "Genetic Programming / Symbolic Regression",
            "problem_domain": "Symbolic regression across physical and empirical datasets (same tasks used as baselines in this paper: synthetic symbolic recovery, Rössler ODEs, Nikuradse dataset, etc.).",
            "problem_description": "Automated discovery of closed-form expressions from input-output data via an evolutionary search over expression trees with user-tunable complexity penalties.",
            "problem_complexity": "Explores large combinatorial expression space via population-based heuristics; complexity control via hand-specified penalties rather than an explicit Bayesian prior.",
            "data_availability": "Operates on datasets provided by user; benchmarking runs in this paper used the same synthetic and real datasets as the Bayesian machine scientist for head-to-head comparison.",
            "computational_requirements": "Designed for speed; in the authors' benchmarking Eureqa was run for extended durations (weeks) and allowed to evaluate up to ~10^13 expressions in their setup (authors' settings). Exact compute/resource usage depends on run settings.",
            "problem_structure": "Same symbolic-regression structure as Bayesian MS; deterministic continuous targets with observational noise, fitness-based evaluation and complexity penalty.",
            "success_metric": "Fitness (data fit minus complexity penalty), MAE on held-out data, visual fit and parsimony.",
            "success_rate": "In the paper's experiments Eureqa generally performed better than EPLEX and EFS but worse than the Bayesian machine scientist: it failed to recover the exact synthetic generating expression where the Bayesian approach succeeded, and produced higher out-of-sample MAE on the Nikuradse dataset than the Bayesian machine scientist.",
            "failure_modes": "Tends to produce structurally overfitted expressions if complexity penalties are not well calibrated; lacks probabilistic posterior over expressions, so cannot naturally average across models; exploration guarantees absent (heuristic search may miss true models).",
            "success_factors": "Fast heuristic exploration, widely used default operation set, effectiveness when properly tuned and given high computational budgets.",
            "comparative_results": "Better than EPLEX and EFS in the authors' comparisons but inferior to the Bayesian machine scientist on recovery and predictive tasks reported in the paper.",
            "human_baseline": null,
            "uuid": "e2591.1",
            "source_info": {
                "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "EPLEX",
            "name_full": "EPLEX (ε-lexicase selection genetic programming)",
            "brief_description": "A genetic-programming symbolic-regression method using ε-lexicase selection, a selection mechanism intended to preserve diverse partial solutions by considering training cases individually.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "EPLEX (ε-lexicase GP)",
            "system_description": "Genetic programming with ε-lexicase selection: population-based search where selection considers individual training cases (with ε-tolerance) to maintain behavioral diversity and promote general solutions. Used here as a baseline symbolic-regression method.",
            "system_type": "Genetic Programming / Symbolic Regression",
            "problem_domain": "Symbolic regression (benchmarked on same tasks: synthetic expressions, Rössler ODEs, Nikuradse dataset).",
            "problem_description": "Discover compact closed-form expressions to explain data via evolutionary search that preserves diverse partial solutions.",
            "problem_complexity": "Handles the combinatorial expression space using population-based heuristics; in authors' runs produced ~10^6 models (population 1000 × 1000 generations × 10 repeats).",
            "data_availability": "Operates on supplied datasets; in experiments used same datasets as other baselines.",
            "computational_requirements": "Substantial evolutionary compute; authors ran EPLEX per recommended settings (≈10^6 model evaluations per run, repeated 10 times) but EPLEX performed considerably worse than Eureqa in these experiments.",
            "problem_structure": "Heuristic search over discrete expression space; stochastic outcomes across runs.",
            "success_metric": "Fitness on training/validation data, MAE on holdout.",
            "success_rate": "Reported as considerably worse than Eureqa and the Bayesian machine scientist on the paper's benchmark tasks; specific quantitative error rates not provided in main text.",
            "failure_modes": "Inconsistent performance and poorer discovery/prediction in the authors' benchmarks; prone to structural overfitting like other GP approaches without a formal prior/posterior framework.",
            "success_factors": "Maintains diversity via ε-lexicase selection which can help on heterogeneous datasets, but in these experiments did not translate to better recovery/prediction.",
            "comparative_results": "Worst-performing of the GP baselines in the authors' experiments (EPLEX &lt; EFS &lt; Eureqa &lt; Bayesian MS, qualitatively).",
            "human_baseline": null,
            "uuid": "e2591.2",
            "source_info": {
                "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "EFS",
            "name_full": "EFS (Evolutionary Feature Synthesis with sparse regression)",
            "brief_description": "A hybrid method that uses genetic algorithms to automatically generate basis functions (features) and then applies sparse regression to select and combine them into closed-form expressions, optimized for speed.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "EFS (evolutionary feature synthesis + sparse regression)",
            "system_description": "Generates candidate basis functions via genetic operators and applies sparse regression to select a parsimonious linear combination of those basis functions, yielding closed-form expressions efficiently. Combines advantages of sparse regression and automatic basis generation.",
            "system_type": "Automated Feature Synthesis / Symbolic Regression with Sparse Regression",
            "problem_domain": "Symbolic regression and model discovery, particularly suited to situations where the true model can be represented as sparse combination of generated basis functions (e.g., some differential-equation discovery tasks).",
            "problem_description": "Automated synthesis of feature (basis) sets and selection via sparse regression to produce interpretable models quickly.",
            "problem_complexity": "Can handle large search through basis-function space but is constrained to linear combinations of generated basis functions; this restricts expressivity relative to arbitrary closed-form expressions.",
            "data_availability": "Operates on supplied datasets; in paper used as a fast baseline repeated many times (100 repeats) to choose best model.",
            "computational_requirements": "Fast: generates candidate models within seconds; repeated runs can aggregate results cheaply (authors ran 100 repeats).",
            "problem_structure": "Works best when the true model aligns with sparse linear combination of basis functions; less suited when model structure is not well-represented by generated bases.",
            "success_metric": "Mean squared error (default fitness), MAE on holdout data.",
            "success_rate": "In the paper EFS was fast but tended to structurally overfit in tasks where the Bayesian machine scientist succeeded; its out-of-sample predictive performance was worse than the Bayesian machine scientist and Eureqa on the Nikuradse dataset.",
            "failure_modes": "Limited by assumption that models are sparse linear combinations of basis functions; structural overfitting if basis generation is unconstrained; can miss non-linear nested structures not captured by bases.",
            "success_factors": "Speed and ability to automatically generate candidate bases; effective when true model aligns with sparse basis representation.",
            "comparative_results": "Fastest among baselines in experiments but produced worse predictive performance and structural recovery than the Bayesian machine scientist and Eureqa in the reported benchmarks.",
            "human_baseline": null,
            "uuid": "e2591.3",
            "source_info": {
                "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Sparse regression (SINDy and similar)",
            "name_full": "Sparse regression approaches for equation discovery (e.g., SINDy / Brunton et al.)",
            "brief_description": "Methods that assume the governing model is a sparse linear combination of candidate basis functions and perform sparse regression (L0/L1-style selection) to identify the active terms, particularly effective for discovering differential equations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sparse regression for equation discovery (SINDy-family)",
            "system_description": "Constructs a library of candidate nonlinear basis functions (polynomials, trig functions, derivatives, etc.) and uses sparse regression (thresholded least squares, LASSO variants) to select a small subset that reconstructs the dynamics, producing parsimonious governing equations.",
            "system_type": "Automated Discovery System / Sparse Regression",
            "problem_domain": "Discovery of governing ordinary differential equations and dynamical-system models from time-series data (physics, chaos, mechanics).",
            "problem_description": "Identify sparse symbolic representations of time-derivative functions (x_dot = Theta(x) * ξ) given measurements (possibly noisy) to recover underlying ODEs.",
            "problem_complexity": "Search limited to linear combinations of a pre-specified (but possibly large) library, reducing combinatorial complexity but requiring the true model to be in the span of the library; sensitive to noise in derivative estimates.",
            "data_availability": "Works well with moderate-to-large time-series and accurate derivative estimates; can fail if derivatives are noisy or library lacks needed terms. Paper notes these methods are particularly suited to reverse-engineer differential equations and could recover true ODEs in low-noise conditions.",
            "computational_requirements": "Sparse regression solves convex (or quasi-convex) optimization problems; computationally efficient compared to full combinatorial symbolic search; parameter tuning (sparsity thresholds) required.",
            "problem_structure": "Assumes linear-in-coefficients structure over chosen basis (continuous, deterministic with additive noise); evaluation via recovery accuracy of terms and predictive dynamics.",
            "success_metric": "Correct support recovery (identification of active terms), parameter estimation accuracy, predictive trajectory error.",
            "success_rate": "Paper states sparse-regression methods are particularly suited to reverse-engineer differential equations and would be able to recover true expressions in low-noise regimes; in the paper's Rössler experiments they are acknowledged as competitive in low-noise cases, while the Bayesian machine scientist recovered ODEs at moderate noise where heuristic baselines failed.",
            "failure_modes": "Sensitive to noise in derivatives (requires good derivative estimation), reliant on adequacy of the candidate library, can miss non-sparse models or models not linear in coefficients.",
            "success_factors": "When derivative data are accurate and the library contains the true terms, sparse regression reliably recovers governing equations with low sample complexity.",
            "comparative_results": "Cited as a complementary approach: better suited than generic GP heuristics for ODE discovery in low-noise regimes, but the Bayesian machine scientist performed robustly across moderate-noise scenarios in the authors' experiments.",
            "human_baseline": null,
            "uuid": "e2591.4",
            "source_info": {
                "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
                "publication_date_yy_mm": "2020-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "rating": 2
        },
        {
            "paper_title": "Fast, scalable, deterministic symbolic regression technology (FFX)",
            "rating": 1
        },
        {
            "paper_title": "EFS: Evolutionary feature synthesis for symbolic regression (I. Arnaldo et al.)",
            "rating": 2
        },
        {
            "paper_title": "A comprehensive comparison of symbolic regression methods (P. Orzechowski et al.)",
            "rating": 2
        },
        {
            "paper_title": "Prediction of dynamical systems by symbolic regression (Quade et al.)",
            "rating": 1
        }
    ],
    "cost": 0.01704375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Bayesian machine scientist to aid in the solution of challenging scientific problems</h1>
<p>Roger Guimerà ${ }^{1,2 *}$, Ignasi Reichardt ${ }^{2}$, Antoni Aguilar-Mogas ${ }^{2,3}$, Francesco A. Massucci ${ }^{2,4}$, Manuel Miranda ${ }^{2}$, Jordi Pallarès ${ }^{5}$, Marta Sales-Pardo ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need "machine scientists" that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.</p>
<h2>INTRODUCTION</h2>
<p>Since the scientific revolution, interpretable closed-form mathematical models have been instrumental for advancing our understanding of the world. Think, for example, of Newton's law of gravitation and how it has enabled us to predict astronomical phenomena with great accuracy and, perhaps more importantly, to interpret seemingly unrelated physical phenomena. With the data revolution, we may now be in a position to uncover closed-form, interpretable mathematical models of natural and even socioeconomic systems that were previously not amenable to quantitative analysis.</p>
<p>To be able to do this, however, we need to develop algorithms for automatically identifying these models (1-4). Following Evans and Rzhetsky, here, we call these algorithms, which would assist human scientists, "machine scientists" (2).</p>
<p>Attempts to design machine scientists date back to, at least, the 1970s and have led to very successful approaches in recent years. One of these approaches is based on genetic programming $(5,6)$. In this approach, closed-form mathematical expressions are represented as graphs, and, given a goodness-of-fit metric, populations of expressions are created and evolved in such a way that high-fitness expressions are selected for further exploration. Another successful approach is based on sparse regression (7-11). In this approach, closed-form mathematical models are assumed to be linear combinations of some (linear or nonlinear) "basis functions" of the independent variables, and sparse regression is used to select and weigh the relevant basis functions. This approach is particularly suited to learn differential equations (8-11), whose form often follows the assumption of linearity on relatively simple basis functions. For more general situations, sparse regression can be combined with genetic programs that automatically generate the basis functions, thus relaxing the need to know them a priori $(12,13)$.</p>
<p>Despite this remarkable progress, machine scientists have stumbled upon two major challenges. First, algorithms must balance goodness</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of fit and model complexity, thus avoiding overfitting. In general, this issue is dealt with by defining model complexity heuristically and then applying model selection criteria to the models that lay on the fit-complexity Pareto front. Both the definition of complexity and the choice of model-selection criterion are, however, hard to generalize and systematize. Second, machine scientists should, in principle, explore an arbitrarily large space of closed-form mathematical models. This is typically addressed by using methods such as genetic programming, which have no guarantees of actually exploring the best models more frequently; or by restricting the search space, for example, to linear combinations of the basis functions in sparse regression approaches, thus leaving out potentially valid models.</p>
<p>Here, we propose a Bayesian approach to the definition of machine scientists. To address the fit-complexity trade-off, we obtain the posterior probability of each expression from basic probabilistic arguments and explicit approximations. The posterior, which leads to consistent model selection, naturally combines the goodness of fit and a prior over expressions that accounts for model complexity. To establish the prior over expressions, we compile a corpus of closed-form mathematical models from Wikipedia and then use a maximum entropy approach to formalize a prior that is statistically consistent with the corpus (14). To address the challenges related to the exploration of the space of closed-form mathematical expressions, we introduce a Markov chain Monte Carlo (MCMC) algorithm that samples from the posterior over expressions. We demonstrate that the Bayesian machine scientist successfully recovers the true generating model when fed with synthetic data, even in situations in which state-of-the-art machine scientists fail (4). We also demonstrate that the machine is able to uncover accurate, closed-form mathematical models for systems for which no closedform model has agreed on. Last, we find that the machine scientist provides out-of-sample predictions that are more accurate than those of other machine scientists and of standard nonparametric machine learning approaches, such as Gaussian processes (15).</p>
<h2>RESULTS</h2>
<h2>Bayesian formulation of the problem and expression plausibility</h2>
<p>Let us first formalize the problem in probabilistic terms. Consider a property $y$ that can be expressed as an unknown, closed-form mathematical</p>
<p>function $y=F(x, \theta)$ of $K$ variables $x=\left{x_{1}, \ldots, x_{K}\right}$ and $L$ parameters $\theta \in$ $R^{L}$ [for example, $y=\sin \left(\theta_{1} x_{1}\right)$ or $y=\theta_{1} x_{1}+\theta_{3} x_{2}$ ]. Given some data $D=$ $\left{\left(y^{1}, x^{1}\right), \ldots,\left(y^{N}, x^{N}\right)\right}$, and assuming that the measurements have some experimental error $y^{k}=F\left(x^{k}, \theta\right)+\epsilon^{k}$, the Bayesian machine scientist assigns to each possible closed-form mathematical expression $f_{i}$ a plausibility $p\left(f_{i} \mid D\right)$ given by the marginal posterior</p>
<p>$$
p\left(f_{i} \mid D\right)=\frac{1}{Z} \int_{\Theta_{i}} d \theta_{i} p\left(D \mid f_{i}, \theta_{i}\right) p\left(\theta_{i} \mid f_{i}\right) p\left(f_{i}\right)=\frac{\exp \left[-\mathcal{L}\left(f_{i}\right)\right]}{Z}
$$</p>
<p>where $\theta_{i}$ are the parameters associated with expression $f_{i}$, the integral is over the space $\Theta_{i}$ of possible values of these parameters, $Z=p(D)$ does not depend on $f_{i}$, and $p\left(f_{i}\right)$ is the prior over expressions. The quantity</p>
<p>$$
\begin{aligned}
\mathcal{L}\left(f_{i}\right) &amp; \equiv-\log \left[p\left(D, f_{i}\right)\right] \
&amp; =-\log \left[\int_{\Theta_{i}} d \theta_{i} p\left(D \mid f_{i}, \theta_{i}\right) p\left(\theta_{i} \mid f_{i}\right) p\left(f_{i}\right)\right]
\end{aligned}
$$</p>
<p>is the description length of model $f_{i}$, that is, the number of nats needed to jointly encode the data and the model with an optimal code (16). Although in general the description length cannot be calculated exactly, it can be approximated in a number of ways $(17,18)$; here, we take one of the simplest approximations</p>
<p>$$
\mathcal{L}\left(f_{i}\right) \approx \frac{B\left(f_{i}\right)}{2}-\log p\left(f_{i}\right)
$$</p>
<p>where $B\left(f_{i}\right)$ is the Bayesian information criterion (BIC) of expression $f_{i}$ and can be readily calculated from the data (Supplementary text S1) (11, 17). This is a first-order approximation to the description length and holds when the likelihood $p\left(D \mid f_{i}, \theta_{i}\right)$ is peaked around the maximum likelihood parameters $\theta_{i}^{\prime}$ and the prior is smooth in this region; when the number of experimental points is small, the approximation may fail, and one would need to get more precise estimates of the description length such as the generalized BIC (19) or even calculate numerically the integral over the parameters. Beyond these potential limitations, Eqs. 1 to 3 naturally combine the goodness of fit of a model and its structural complexity, which is captured by the prior over expressions. In addition, in the limit of large datasets, we have $\left|B\left(f_{i}\right)\right| \gg$ $\left[\log p\left(f_{i}\right)\right]$; since $B\left(f_{i}\right)$ is consistent, the machine scientist is also consistent, that is, in this limit it prefers the correct expression over any other expression with probability approaching 1 .</p>
<h2>Sampling from the posterior distribution over expressions</h2>
<p>The Bayesian machine scientist explores the space of closed-form mathematical expressions using MCMC. In particular, we introduce three move types that enable one to go from any closed-form expression to any other closed-form expression, thus enabling the machine scientist to explore, given enough time, the whole space of closed-form mathematical expressions (Materials and Methods) (Fig. 1). Regardless of the frequency of each move and other parameters of the Markov chain, MCMC samples expressions $f_{i}$ from the stationary distribution $p\left(f_{i} \mid D\right)$ (fig. S1). Although MCMC is slower than some alternative machine scientists that put emphasis on speed, such as evolutionary
feature synthesis (EFS), the only dependency on the number of data points is in the estimation of the BIC of each model, and therefore its complexity scales as any other method using least squares to fit model parameters.</p>
<p>For model selection, the Bayesian machine scientist can use the most plausible expression from an MCMC run, that is, the maximum a posteriori (or minimum description length) expression. However, MCMC naturally samples over the whole space of models, thus generating arbitrarily long sequences of expressions; as we show below, this leads to a more complete characterization of the expression space and to higher out-of-sample prediction accuracy.</p>
<h2>Estimation of prior probabilities using a corpus of mathematical expressions</h2>
<p>For the Bayesian machine scientist to be able to estimate the plausibility of a given expression, it needs to estimate the prior probabilities $p\left(f_{i}\right)$. A common approach in model selection is to have no a priori preference for any given model over the others and assume that $p\left(f_{i}\right)$ is the same for all models $(17,18)$. In that case, and within our approximation for the description length, the most plausible model is simply the one with the lowest BIC. This is a consistent approach and generally reasonable when comparing a small number of simple models. However, it is inappropriate when considering a finite dataset and a very large (potentially infinite) space of mathematical expressions because one can always find a very complex model that fits the data arbitrarily well even with very few parameters. These unnecessarily complex models are likely to generalize very poorly in the same way that models with many parameters do-this structural overfitting is thus akin to traditional overfitting but arises from the large number of mathematical models considered rather than the large number of parameters (Supplementary text S4 and fig. S5). From this perspective, the prior over expressions acts as a model regularizer. It would also be possible to add other regularizers to expression trees, but, within our Bayesian framework, tree regularizers could still be cast as nonuniform priors, although they may be formally more complex and harder to interpret than the ones that we introduce below.</p>
<p>Given these considerations, the machine scientist needs to "learn" the a priori plausibility of models. Human scientists obtain this prior knowledge by studying science books and becoming familiar with the mathematical expressions that appear in them. We take a similar empirical approach to define the prior expectations of the machine scientist-we compiled a corpus of 4080 mathematical expressions that are included in Wikipedia entries (Materials and Methods)and use these expressions to shape the prior expectations of the machine scientist, that is, to establish the statistical properties expected a priori for expressions (Fig. 2). To do this, we use an approach based on exponential random graphs (20-23). In particular, we choose a prior that generates expressions with the same average number of each type of operation [and their squares $(23,24)$ ] as in the empirical corpus and, given this constraint, satisfies the maximum entropy principle $(14,23)$ (Materials and Methods; Supplementary text S2, fig. S2, and tables S3 and S4). As we show in Fig. 2, this prior generates mathematical expressions statistically consistent with the corpus. Note that the Bayesian machine scientist is not restricted to the 4080 expressions in the corpus, or even to arbitrary combinations of these expressions-all closed-form mathematical expressions are valid and can be visited by the MCMC; those that are statistically similar to the corpus are simply more plausible a priori. Note also that, as mentioned earlier, for large amounts of data, the prior washes out and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. MCMC to systematically explore the space of closed-form mathematical expressions. (A) Closed-form mathematical expressions can be represented as trees, whose internal nodes are functions (operations) and whose leaves are variables and parameters. To explore the space of mathematical expressions, we implemented three move types (Materials and Methods): (i) root addition/removal—we added/removed the root of the tree; (ii) elementary tree replacement (we define an elementary tree as a subtree containing, at most, one function)—we replaced one elementary tree by another (for example, c1x by x); (iii) node replacement—we replaced one node (function, variable, or parameter) by another. Each of these moves was proposed with a certain frequency and accepted or rejected using the Metropolis' rule (Materials and Methods) (36). (B) To validate the MCMC, we explored the space of all expressions with one variable, one parameter and using only the functions (+, sin). For this experiment only, we restricted the size of the expression tree to be smaller than eight nodes, and we assume that all expressions are equally plausible a priori, p(fi) = const. Each node in the network represents a different expression, and links represent jumps from one expression to another resulting from the MCMC moves described in (A). The size of each node is proportional to the number of times they are visited by the MCMC. In the absence of data, the MCMC explores all possible expressions with equal probability. (C) We generated synthetic data with the expression y = a + x + sin (x), to which we added noise (circles). We show how the correct expression and two closely related expressions fit the data. (D) We explored the same space as in (B) but considering the synthetic data in (C). When data are taken into account, expressions that are more plausible given the data are visited more frequently (larger nodes), and many expressions are too implausible and, therefore, not visited at all (fig. S1). The color of each node corresponds to the color of the curves in (C).</p>
<p>the description length, as approximated in Eq. 3, is consistent regardless of the selection of prior.</p>
<h3>Validation of the Bayesian machine scientist with synthetic data</h3>
<p>Having addressed the methodological challenges in the definition of the Bayesian machine scientist, we next demonstrate the ways in which it can be used and illustrate how one can get insights by using it. First, we test that, when fed with synthetic data, the machine scientist recovers the models that truly generated the data. We start by selecting an arbitrary expression and setting its parameters to values uniformly selected from [-2, 2]. The selected expression (Fig. 3) is F(x1, x2; θ1, θ2) = x1(θ1 + x2) cos(x1)/(θ2 log(θ2)], with θ1 = -1.19 and θ2 = 0.29. Note that this expression is not one of the expressions present in our empirical corpus.</p>
<p>We then feed the machine scientist with 400 noisy data points generated using this expression. The Bayesian machine scientist finds the correct model and assigns to it the maximum plausibility or, equivalently, the minimum description length (Fig. 3). To evaluate to which extent it is remarkable that the Bayesian machine scientist finds the correct model in this situation, we attempt the same task with state-of-the-art machine scientists (4), including two methods based on genetic programming [Eureqa (5) and ε-lexicase selection (EPLEX) (6)] and a method that combines genetic programming with sparse regression (Materials and Methods) [EFS; (13)]. We find that, from the same dataset, none of these methods are able to recover the correct model and that they tend to structurally overfit the data (Supplementary text S3).</p>
<p>The ability of the Bayesian machine scientist (and of any other method) to identify the correct model requires a minimum amount of data points. We find that, for this synthetic dataset, and even though the other approaches do not identify the correct expression with 400 points, the Bayesian machine recovers the true model with as few as 100 points (Supplementary text S3 and fig. S3).</p>
<p>Next, we investigate whether the machine is able to recover the differential equations that govern the Rössler system (25), and what is the effect of increasing observational noise in the equation discovery process. Since we are mostly interested in the effect of increasing noise in the target variable, we assume that the only measurement error is in the derivatives, although in some real situations derivatives may need to be estimated numerically from noisy measurements of the variables. In those situations, it may be necessary to use advanced techniques to estimate the derivatives (26). Under the conditions of our experiment, we find that the machine is able to recover the correct differential equations when the derivatives are measured with moderate noise (Fig. 4). When the measurement noise is high, the true expressions are still regarded as very plausible, but the machine identifies as the most plausible ones expressions that are "regularized" versions of the exact models. In these regularized models, small terms are disregarded; in all</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><strong>Fig. 2. The prior probability distribution generates plausible expressions. (A)</strong> We extracted 4080 mathematical expressions from Wikipedia pages listed under the category "List of scientific equations named after people", for example, the page devoted to "Newton's law of universal gravitation" (see also table S2). (<strong>B</strong>) After parsing each expression (Materials and Methods), we counted the number of each operation in the expression and the square of these numbers. In Newton's law of gravitation, we counted two products, one division, and one square. The complete list of operations is listed in table S1. Then, we parametrized the prior distributions <em>p</em>(<em>f</em><sub><em>i</em></sub>) and fit the parameters so as to generate expressions that have realistic numbers (and squares) of each operation (Materials and Methods). (<strong>C</strong> to <strong>H</strong>) We ran the MCMC to draw sets of 4080 synthetic expressions from the prior distribution. We compared these sets of 4080 synthetic expressions with the 4080 empirical expressions obtained from Wikipedia. In particular, for each set of 4080 expressions, we counted the mean number 〈<em>n</em><sub><em>n</em></sub>〉 of each operation <em>o</em> per expression {(<em>C</em>) sine, (<em>D</em>) logarithm, (<em>E</em>) sum}, or the mean 〈<em>n</em><sub><em>x</em></sub>〉 of the square of those numbers (<strong>F</strong> to <strong>H</strong>) and plot their probability density function (PDF; pink). The red vertical line indicates the mean of the distribution, which is close to the empirical observation, represented by a dashed black line. The empirical value is always well within the expected ranges of the expressions generated by the MCMC. The results in the figure correspond to expressions with up to five variables and eight parameters.</p>
<p>cases, the most plausible models are almost indistinguishable from the true ones (fig. S4). Thus, as expected, the machine scientist automatically adjusts the complexity of the models to the quality of the data (Supplementary text S4). Again, these results stand in contrast to those of alternative machine scientists [with the exception of pure sparse regression methods particularly suited to reverse-engineer differential equations (8–11), which would be able to recover the true expressions, at least in the case with low noise]. Even for the simplest case in this experiment (inference of <em>ẋ</em> with a low level of noise), all benchmark machine scientists fail to recover the true model and tend to overfit structurally (Supplementary text S4).</p>
<p>Last, we test the behavior of the machine scientist when presented with data that are generated from a model that does not have a closed-form mathematical expression in terms of the basic functions that it uses. In particular, we generate synthetic data using Bessel functions <em>J</em><sub><em>α</em></sub>(<em>x</em>) with <em>α</em> ∈ {0, 1, 2, 3, 4}. We find that the machine scientist is able to identify closed-form expressions that are as accurate as high-order Taylor expansions of the exact functions. We also find that, when the synthetic data are noisy and the machine scientist is restricted to choose among low-order series expansions, it consecutively chooses first-order, second-order, or third-order expansions, as the range of observed data increases and as the noise decreases, as one would expect (Supplementary text S5 and fig. S6).</p>
<h3><strong>Use of the machine scientist on small datasets and on the Nikuradse dataset</strong></h3>
<p>Next, we turn to the analysis of real datasets. First, we analyze how the machine scientist provides insights into problems for which there are scarce and noisy data. We focus on three datasets that have been the subject of recent analyses and for which models have been proposed: a dataset on funding success in different European countries (27); a dataset on cell-to-cell stresses (28); and a dataset on stocks of salmon in the Fraser River in British Columbia, Canada (29).</p>
<p>For each of these three datasets, we compare existing models to models identified by the machine scientist in terms of their plausibility <em>p</em>(<em>f</em><sub><em>i</em></sub> | <em>D</em>), their BIC <em>B</em>(<em>f</em><sub><em>i</em></sub>), and their cross-validation error (Supplementary text S6 and tables S5 to S7). In all cases, the machine scientist identifies at least one model that is better than existing models in a Pareto sense, namely, better in at least one of the three performance metrics without being worse in any of the others.</p>
<p>Last, we show in more detail how the machine scientist can help us to solve major open scientific problems. For this, we focus on the classical experiment of turbulent friction in rough pipes performed in the early 1930s by Johann Nikuradse (30–33). In his experiments, Nikuradse measured the turbulent friction <em>λ</em> as a function of the roughness <em>x</em><sub><em>D</em></sub> of the pipe and the Reynolds number <em>x</em><sub><em>R</em></sub>. We take the original Nikuradse dataset and feed it to the machine scientist to study possible analytical expressions for the turbulent friction. A typical MCMC run uncovers numerous expressions that fit all observed data remarkably well (Fig. 5, A and B). We compare these expressions to the best expressions uncovered by other machine scientists (Materials and Methods): Eureqa (5), EFS (13), and EPLEX (6) (see Supplementary text S7 and fig. S10 for EPLEX). The expressions uncovered by the Bayesian machine scientist fit the Nikuradse dataset better than those uncovered by Eureqa, which, in turn, is significantly better than all other benchmark methods.</p>
<p>The Bayesian machine scientist does not find any candidate expression that is overwhelmingly more plausible than all the others;</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>Fig. 3. Recovery of an arbitrary expression from synthetic data.</strong> We drew an expression with two variables (<em>x</em><sub>1</sub> and <em>x</em><sub>2</sub>) and two parameters (θ<sub>1</sub> and θ<sub>2</sub>) from the prior distribution <em>p</em>(<em>f</em><sub><em>i</em></sub>); the drawn function is <em>F</em>(<em>x</em><sub>1</sub><em>x</em><sub>2</sub>; θ<sub>1</sub><em>θ</em><sub>2</sub>) = <em>x</em><sub>1</sub>(θ<sub>1</sub> + <em>x</em><sub>2</sub>) cos (<em>x</em><sub>1</sub>)/[θ<sub>2</sub> log (θ<sub>2</sub>)], with θ<sub>1</sub> = −1.19 and θ<sub>2</sub> = 0.29. With this function, we generated 400 synthetic points <em>y</em><sup>k</sup> = <em>F</em>(<em>x</em><sub>1</sub><em>k</em><sub>2</sub><em>; θ<sub>1</sub></em>θ<em><sub>2</sub>) + ε<sup>k</sup>, with (</em>x<em><sub>1</sub></em>k<em><sub>2</sub></em>), uniformly chosen in [−2,2]<sup>2</sup>, and ε ~ <em>N</em>(0,1) normally distributed [points in (<strong>A</strong>) and (<strong>D</strong>) to (<strong>G</strong>); the color of the points corresponds to the value of <em>y</em> as indicated by the color bar]. We fed these synthetic data to the Bayesian machine scientist and run 2500 steps of the MCMC algorithm. (<strong>A</strong>) After 2500 steps, the most plausible model identified (blue surface) is <em>f</em>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>; θ<sub>1</sub>, θ<sub>2</sub>) = <em>x</em><sub>1</sub>(<em>c</em><sub>1</sub> + <em>c</em><sub>2</sub><em>x</em><sub>2</sub>) cos <em>x</em><sub>1</sub>, which coincides with the correct model up to indeterminacies in the dependency on the parameters that cannot be resolved without varying their values (see movie S1 for the complete evolution of the MCMC). (<strong>B</strong>) The blue line indicates the evolution of the description length <em>L</em> during the MCMC (up to an irrelevant additive constant that comes from the prior normalization and therefore affects all points equally). Lower description lengths correspond to more plausible models. The MCMC starts from an expression with high description length, but after 1000 to 1500 steps, it equilibrates and samples from the stationary distribution <em>p</em>(<em>f</em><sub><em>i</em></sub>|<em>D</em>). (<strong>C</strong>) The blue line indicates the evolution of the mean absolute error of each model sampled by the MCMC with respect to the true model. The error is calculated over a grid in [−2,2]<sup>2</sup>, that is, not at the observed points. (<strong>D</strong> to <strong>G</strong>) The MCMC is prone to getting trapped in local maxima of the plausibility (local minima of the description length). To explore the expression space more efficiently, we used parallel tempering (<em>38</em>), a technique used in statistical physics to study disordered systems with rugged energy landscapes (Materials and Methods). Besides the main MCMC in (<strong>A</strong>), parallel tempering keeps a number of parallel MCMCs that sample the distributions <em>p</em>(<em>f</em><sub><em>i</em></sub><em>T</em><sub><em>k</em></sub>) = exp [−<em>B</em>(<em>f</em><sub><em>i</em></sub>)<em>T</em><sub><em>k</em></sub>] + log <em>p</em>(<em>f</em><sub><em>i</em></sub>)] parametrized by "temperatures" <em>T</em><sub><em>k</em></sub> (Materials and Methods). In our case, higher temperatures correspond to simpler expressions. At each MCMC step, we attempted a swap of expressions at contiguous <em>T</em><sub><em>k</em></sub> using Metropolis' rule. The evolution of the description length and the mean absolute error of the parallel MCMCs are represented in (<strong>B</strong>) and (<strong>C</strong>) with the same colors as in (<strong>D</strong>) to (<strong>G</strong>).</p>
<p>rather, it uncovers a collection of similarly plausible models. This has two important implications. First, it points toward the need to revisit our tendency to look for single "best models" from data. Second, it suggests that, when using the machine scientist to make predictions, we should average over the whole ensemble of plausible models (<em>34, 35</em>). In particular, the posterior predictive distribution for a point <em>y</em><sup>k</sup> can be approximated as</p>
<p>$$p(y^k \mid D; x^k) \approx \sum_i \delta(y^k - f_i(x^k, \theta_i^*) p(f_i \mid D) \tag{4}$$</p>
<p>where θ<sub><em>i</em></sub><sup><em></sup> is the maximum likelihood estimator of </em>f<em><sub></em>i<em></sub>'s parameters, δ(</em>x<em>) is the Dirac delta function, and the sum runs over all possible expressions </em>f<em><sub></em>i<em></sub>. Note that estimating the posterior predictive distribution in this way makes interpretation of the predictions harder—even if each model </em>f<em><sub></em>i<em></sub> is interpretable, averaging leads to a noninterpretable effective model. However, it is important to point out that this is the most comprehensive approach possible, in the sense that, even if one is certain that the data were generated with a single, unknown model </em>F<em>, the best predictive distribution comes from taking all models into consideration, each weighted by its plausibility </em>p<em>(</em>f<em><sub></em>i<em></sub>|</em>D<em>) (</em>34*).</p>
<p>In practice, the average over models can be computed using the MCMC sample collected by the machine scientist, and we can use the median of the posterior distribution <em>p</em>(<em>y</em><sup>k</sup> | <em>D</em>; <em>x</em><sup>k</sup>) to make predictions for <em>y</em><sup>k</sup> that minimize the mean absolute predictive error. By considering, among all sampled models, the one that most resembles this median prediction, we obtain a single closed-form mathematical model that is optimally predictive—we call this model the median predictive model. To test the predictive power of the median predictive</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Recovery of the differential equations governing the Rössler system. (A) We generated synthetic data for the Rössler system (equations shown in the panels) with $a=b=0.2$ and $c=5.7$. We assume that $(x, y, z)$ are measured without error, but the functions that we aim to recover $(\hat{x}, \hat{y}, \hat{z})$ are measured with two levels of Gaussian noise [e.g., $\hat{x}^{k}=F\left(x^{k}, y^{k}, z^{k}\right)+\varepsilon^{k}$ with $\varepsilon^{k} \sim N\left(0, \sigma_{x}\right)$; orange, $\sigma_{x}=1$; blue, $\sigma_{x}=5$. (B) Trajectories in velocity space $(\hat{x}, \hat{y}, \hat{z})$ for the levels of noise that we considered. (C to J) For each level of noise, we sampled 10,000 models for $\hat{x}$ using the machine scientist and the same for $\hat{y}$ and $\hat{z}$. (C and G), For each level of noise, we plot the most plausible model obtained. (D to F and H to J) We plot the distribution of description lengths (up to an irrelevant additive constant) for the models sampled. (D to F) For low noise, the most plausible model among those sampled always coincides with the true model, as indicated by the dashed vertical line. (H to J) For high noise, $\hat{x}$ was recovered exactly, whereas for $\hat{y}$ and $\hat{z}$, the most plausible models corresponded to regularized versions of the true models, in which terms that are comparatively small because of the factor 0.2 are dropped. In all cases, the true model (dashed vertical line) is, at least, among the most plausible ones.
model in the Nikuradse dataset, we compare it to the alternative machine scientists (Eureqa, EPLEX, and EFS), as well as to a standard nonparametric Bayesian approach, Gaussian processes (15). In particular, we test the ability of all approaches to generalize to data never seen before (Fig. 5, C and D). We find that the predictions of the Bayesian machine scientist are significantly more accurate than those of all alternative approaches.</p>
<p>The median predictive model, being a function of the roughness only for large Reynolds numbers, also predicts the expected limiting scaling for the turbulent friction (Fig. 5, E and F), which is remarkable considering that (i) most of the observed data correspond to a regime with different physics (31) and (ii) many of the sampled models do not scale correctly (fig. S9). Last, to fully exploit the potential of the machine scientist to obtain interpretable models of turbulent friction, we use it in combination with the known physics of the problem. In 1933, Prandtl
suggested that the function $\hat{\lambda}=(100 \lambda)^{-1 / 2}-2 \log D / r$ should be a universal function depending only on the so-called roughness Reynolds number $k_{r}^{\prime \prime}(\operatorname{Re}, r / D)(30,33)$. We use the machine scientist to obtain the most plausible form for such universal function and get $\hat{\lambda}=1.73+$ $0.64 \times 0.96^{k_{r}^{\prime \prime}}-2.62 \times 0.64^{k_{r}^{\prime \prime}}$ (Fig. 5G); this very simple model on the scaled variable does indeed provide an excellent fit to the original (unscaled) data (Fig. 5H).</p>
<h2>DISCUSSION</h2>
<p>In the age of data, there is a pressing need to design algorithms capable of helping in the scientific process, from assisting in the proof of theorems to parsing scientific texts. Discovering closed-form mathematical models was one of the earliest tasks attempted, and yet, despite much progress in the area, two related difficulties have arisen repeatedly-deciding what is</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p><strong>Fig. 5. The machine scientist gives accurate closed-form models and out-of-sample predictions for the Nikuradse dataset.</strong> The dataset contains measurements of the turbulent friction λ in rough pipes as a function of the Reynolds number <em>x</em><sup>R</sup> and the inverse of the relative roughness <em>x</em><sub>D</sub>. (<strong>A</strong> and <strong>B</strong>) We fed the machine scientist with the Nikuradse dataset (gray) and sample more than 18,000 models using MCMC as described in the text. The two graphs show the fit of two of the most plausible models (red), indicated at the top of each panel. For comparison, we show the best models identified by alternative machine scientists Eureqa (green) and EFS (orange). The inset in each graph shows the mean absolute error (MAE) of the machine scientist (MS), Eureqa (EU), and EFS (EF) (Materials and Methods), and error bars indicate the 95% confidence interval for the mean. (<strong>C</strong> and <strong>D</strong>) We fed the machine scientist with the data points in gray and sample 20,000 models using MCMC. We made predictions (red) for the unobserved data (black) using the median predictive model (see text). For comparison, we show the predictions of Eureqa, EFS, and Gaussian processes (GP) (Materials and Methods). The inset in each graph shows the mean absolute error for all approaches. (<strong>E</strong>) We considered, again, the models obtained from the whole data as in (<strong>A</strong>) and (<strong>B</strong>) but asked what the median predictive model predicts in the fully turbulent regime, well above the highest observed Reynold number (<em>31</em>). Despite the fact that individual sampled models have different behaviors in this region (fig. S5), the median predictive model [obtained as in (<strong>C</strong>) and (<strong>D</strong>)] predicts that turbulent friction stays constant at large Reynolds numbers, in agreement with theory (<em>31–33</em>). (<strong>F</strong>) The machine scientist also predicts the correct scaling (Z <em>r</em> <em>r</em>)<sup>−1/2</sup> for the turbulent friction in this regime (dashed line). (<strong>G</strong>) Scaling of the Nikuradse dataset proposed by Prandtl (with <em>k</em><sub>c</sub><sup>c</sup> (Re, <em>r</em> / <em>D</em>) = Re × √<em>λ</em>/32 × <em>D</em> / <em>r</em>) (<em>30, 33</em>) and most plausible model identified by the machine scientist for the universal scaling function. (<strong>H</strong>) When unscaled, this simple universal function provides a good description of all the data.</p>
<p>the correct balance between goodness of fit and model complexity and searching systematically through the space of models.</p>
<p>These are fundamental difficulties that stem from the nature of the problem that we aim to solve, and, therefore, no approach can possibly avoid them completely—one cannot establish an assumption-free measure of model complexity that serves all purposes or make the search space small without risking to leave out valid models or even the true generating model.</p>
<p>However, the Bayesian approach that we have introduced here forces us to be explicit and transparent about all the necessary assumptions and approximations to deal with these challenges. With regard to the fit-complexity trade-off, model complexity enters through our prior expectations about expressions. The use of an empirical corpus is somewhat arbitrary, and one may need to tune the corpus to the problem at hand; but it is reasonable to expect that, lacking any data, the machine scientist should propose models that statistically resemble existing phenomenological models in the literature. With our maximum entropy approach, we guarantee that the prior distribution is the most agnostic distribution satisfying this constraint. Besides, our approach makes it clear that, given enough data, prior expectations become irrelevant, and the machine scientist will consistently prefer the true model with probability approaching 1. With regard to the search through the space of models, MCMC guarantees that, asymptotically (that is, given enough data and time), the Bayesian machine scientist samples from the posterior distribution and, because it is consistent, visits the true model with the highest frequency.</p>
<p>Despite these advantages, our approach is not, of course, exempt of difficulties, the most pressing of which we have already pointed out. First, despite the virtues of MCMC for sampling the expression space, it may be necessary to develop more efficient approaches for very large datasets, without losing the guarantees provided by MCMC. Evidence for the need for these approaches would come from the observation that different MCMC runs on the same dataset result in different description length distributions, which would indicate that the stationary posterior is not being sampled correctly. Alternatives to sampling the full posterior using MCMC may range from Bayesian inference methods such as variational approximations to mathematical optimization methods, if suitable representations of the problem can be found and we are content with obtaining a single model and some bounds to its optimality. Second, it may be desirable to develop and compare other approaches to setting priors for expressions, perhaps based on noninformative priors or symmetry considerations, or empirical approaches different from the one that we have adopted here. A situation in which this need would be obvious is when the proposed models clearly violate some basic features of the desired solution, such as some particular limiting behavior or dimensional consistency. If such additional information is available, then it would be appropriate to formalize it in the prior. Last, in some situations involving small datasets and leading to broad likelihoods in parameter space, it may be necessary to use approximations to the description length that are more accurate than those based on the BIC, for example, the generalized BIC (<em>18</em>). However, this problem would be more difficult to diagnose in a practical situation because, even for a single dataset, the BIC can be a good approximation for some models but not for others. In any case, situations in which the BIC is a poor approximation are likely to involve small or very noisy datasets, situations in which the importance of the BIC is relatively small compared to that of the prior and simple models will be sampled anyway.</p>
<p>These challenges notwithstanding, our results suggest that, in practice, the Bayesian machine scientist is able to uncover models that are</p>
<p>good in terms of both describing the observed data and predicting new data. Our approach can also be used in other contexts, for example when a phenomenological model has been proposed from data, and we aim to find whether there are other models that are more plausible or, at least, similarly plausible. Or, given two conflicting theories for the same process, the Bayesian machine scientist could be used to establish which one is more plausible and to what extent, and whether there exist other reasonable models. From a broader perspective, our approach sets the basis for further developing theories of model discoverability: Is it always possible to identify the correct model given the data? And if not, then under which conditions is the true model discoverable? In addition, our approach opens the door to addressing fundamental questions related to the limits of predictability for mathematical models: To what extent can a system be accurately predicted? Are closed-form, interpretable models as expressive as machine learning models such as deep neural networks? And if so, why? These are all important questions whose answer may lead to significant new insights about the scientific process and about our capacity to understand and describe the world using mathematics.</p>
<h2>MATERIALS AND METHODS</h2>
<h2>MCMC moves for mathematical expression sampling</h2>
<p>To perform a systematic exploration of the space of closed-form mathematical expressions, we represented closed-form mathematical expressions as expression trees. In these trees, internal nodes represent operations (for example, sum or exponential) and leaves represent variables or parameters (Fig. 1). To avoid problems with improper priors, we limited the size of expressions trees to 50 nodes. Although, in principle, one could make this number arbitrarily large, all parameter values and results in the paper are obtained using this value.</p>
<p>We classified the nodes of the expression tree based on the number of offspring that they have. Leaves (variables and parameters) have no offspring, whereas operations can have one offspring (operations that take only one argument, like the exponential function) or two offspring (operations that take two arguments, such as the sum). In table S1, we list all the operations that the machine scientist uses for building models.</p>
<p>An elementary tree (ET) is an expression tree (thus, a mathematical expression) that contains at most one operation. For the implementation of the Markov chain, we used a variety of moves (described in detail below) that operate by adding, removing, replacing, or modifying ETs. We call those ETs whose operation has $k$ offspring $k$-ET. For example, $x+a$ is a 2 -ET (because the sum has two offspring: $x$ and $a$ ), $\sin x$ is a 1-ET (because the sine function has a single offspring: $x$ ), and $x$ is a 0 -ET. Expressions such as $\sin (x+a)$ are not ETs because they contain more than one operation.</p>
<p>We designed an MCMC algorithm to sample expressions from the posterior distribution $p\left(f_{i} \mid D\right)$, which gives the plausibility of an expression given the observed data. This distribution is given by Eq. 3. We used three types of move to update mathematical expressions (Fig. 1):</p>
<p>1) Node replacement. We replaced a node in the expression tree (selected uniformly at random among all nodes in the tree) by a randomly selected node, with the only restriction that the replacement must have the same number of offspring as the original node (for example, a " + " node can be replaced by a "*" node but not by an "exp" node). The offspring branches remain unchanged.
2) Root addition (RA). We added a new root to the expression tree. The new root can be any operation. If the operation takes one off-
spring, then the old expression tree becomes the offspring of the new root; otherwise, the old expression tree becomes the leftmost offspring of the operation, with the other offspring being randomly chosen 0 -ETs (that is, variables or parameters). To be more precise, at the beginning of a sampling process, all possible replacement roots are enumerated (all operations with all possible 0 -ET offspring in all positions other than the leftmost branch, which is left empty); when a RA is attempted, the new root is chosen uniformly among this list of candidates. The reverse move, root removal (RR), consists of removing the root of the expression tree and all its offspring except for the leftmost branch, which becomes the new expression tree. RR is only possible when all branches except the leftmost one are 0 -ETs.
3) Elementary tree replacement (ETR). We replaced a randomly selected ET in the complete expression tree by another randomly selected ET. At the beginning of a sampling process, all possible ETs are enumerated (all operations with all possible 0 -ET offspring). In each move, an ET (chosen uniformly at random among all ETs in the expression tree) is replaced by an ET chosen uniformly at random among all possible ETs.</p>
<p>The ETR move introduces small variations to expression trees and is therefore the major source of expression variation. By contrast, node replacement moves often introduce major changes in expressions (for example, replacing a sum by a product often alters a model very significantly) and are therefore not very efficient. However, they are useful in that they represent long jumps in the space of models. Last, root replacement is the only move that can make trees grow/shrink at the top and is useful to add/remove terms to models that are already reasonable. Without root replacement, adding an additive term to an expression tree would require disassembling the whole tree and assembling it again with the additional term from the beginning.</p>
<h2>MCMC acceptance rules</h2>
<p>At each MCMC step, we attempt one of the three moves described above and accept or reject the proposed move according to Metropolis' rule (36)</p>
<p>$$
p_{\text {accept }}\left(f_{i} \rightarrow f_{\mathrm{f}}\right)=\min \left{1 \frac{p\left(f_{\mathrm{f}} \mid D\right) g\left(f_{i} \mid f_{\mathrm{f}}\right)}{p\left(f_{\mathrm{f}} \mid D\right) g\left(f_{\mathrm{f}} \mid f_{i}\right)}\right}
$$</p>
<p>where $g\left(f_{\mathrm{f}} \mid f_{i}\right)$ is the distribution of movement proposal $f_{i} \rightarrow f_{\mathrm{f}}$ (where f and i stand for final and initial, respectively). This rule ensures that the stationary distribution is $p\left(f_{i} \mid D\right)$</p>
<p>$$
p\left(f_{i} \mid D\right)=\frac{1}{Z} \exp \left[-\mathcal{L}\left(f_{i}\right)\right]
$$</p>
<p>with $Z$ being a normalizing constant and $\mathcal{L}\left(f_{i}\right)$ being the description length, as defined and approximated in the main text</p>
<p>$$
\mathcal{L}\left(f_{i}\right) \approx \frac{B\left(f_{i}\right)}{2}-\log p\left(f_{i}\right)
$$</p>
<p>For all samples in the paper, we attempt a root replacement move 5\% of the time, a node replacement move $45 \%$ of the time, and an ETR 50\% of the time. These rates are arbitrary and should not affect the achievement or the equilibrium distribution but do have an effect on how fast the Markov chain converges to the equilibrium distribution.</p>
<p>In what follows, we describe the specific form of the acceptance (Eq. 5) rule for each movement.</p>
<p>1) Node replacement (NR). Since the node to change is chosen uniformly at random and so is the new node, the move is symmetric, that is, the probability of attempting a move and its reverse are the same $g\left(f_{\mathrm{f}} \mid f_{\mathrm{i}}\right)=g\left(f_{\mathrm{i}} \mid f_{\mathrm{f}}\right)$. Therefore, the acceptance probability in this case is given simply by the difference in the description lengths of the expression trees</p>
<p>$$
p_{\text {accept }}^{\mathrm{NR}}=\min {1, \exp [-\Delta \mathcal{L}]}
$$</p>
<p>2) RA and RR. In this case, the proposal distribution for the RA and RR movements is not symmetric. The RR move is deterministic in that it always affects the existing root of the expression tree. By contrast, the RA move involves selecting uniformly at random among all possible $N_{\text {root }}$ roots that can be added (which are enumerated once at the beginning of the sampling process, as described above). Therefore, if $p_{\mathrm{RR}}$ is the probability of selecting this type of move, then $g(\mathrm{RR})=p_{\mathrm{RR}}$ and $g(\mathrm{RA})=p_{\mathrm{RR}} / N_{\text {root }}$ so that $g(\mathrm{RR}) / g(\mathrm{RA})=N_{\text {root }}$, which gives the following acceptance rules</p>
<p>$$
\begin{gathered}
p_{\text {accept }}^{\mathrm{RA}}=\min \left{1, N_{\text {root }} \exp [-\Delta \mathcal{L}]\right} \
p_{\text {accept }}^{\mathrm{RR}}=\min \left{1, \frac{\exp [-\Delta \mathcal{L}]}{N_{\text {root }}}\right}
\end{gathered}
$$</p>
<p>where $N_{\text {root }}$ is the number of possible roots among which we choose in the RA move.
3) ETR. The ETR move is the most involved in terms of defining the move proposal probabilities necessary to define the acceptance rule. First, we need to specify exactly how we choose the attempted move. We start by selecting the orders $o_{\mathrm{i}}$ and $o_{\mathrm{f}}$ of the existing (initial, i) and replacement (final, f) ETs. For example, we may choose to replace a 0 -ET ( $o_{\mathrm{i}}=0$, a constant or a variable) by a 2 -ET ( $o_{\mathrm{f}}=2$, e.g., a sum or a product). In this initial choice, we need to take into consideration the number $n_{\mathrm{if}}$ of options that we have to choose $o_{\mathrm{i}}$ and $o_{\mathrm{f}}$ (for example, if a tree has reached the maximum allowed size $o_{\mathrm{f}} \leq o_{\mathrm{i}}$ necessarily). Once the orders $o_{\mathrm{i}}$ and $o_{\mathrm{f}}$ are selected, we need to account for (i) all the possible ETs of order $o_{\mathrm{i}}$ in the initial tree $\Omega_{\mathrm{i}}$ that we can choose to replace and (ii) all the possible ETs of order $o_{\mathrm{f}}$ that we can choose to replace the ET in the initial tree, $s_{\mathrm{f}}$. If $p_{\text {ETR }}$ is the probability with which we choose this move, then taking into account the previous considerations $g\left(o_{\mathrm{f}} \mid o_{\mathrm{i}}\right)=p_{\mathrm{ETR}} \times 1 / n_{\mathrm{if}} \times 1 / \Omega_{\mathrm{i}} \times 1 / s_{\mathrm{f}}$. Conversely, $g\left(o_{\mathrm{i}} \mid o_{\mathrm{f}}\right)=p_{\mathrm{ETR}} \times 1 / n_{\mathrm{fi}} \times$ $1 / \Omega_{\mathrm{f}} \times 1 / s_{\mathrm{i}}$, so that the acceptance rule is</p>
<p>$$
p_{\text {accept }}^{\mathrm{ETR}}=\min \left{1, \frac{n_{\mathrm{if}} \Omega_{\mathrm{i}} s_{\mathrm{f}}}{n_{\mathrm{f}} \Omega_{\mathrm{f}} s_{\mathrm{i}}} \exp [-\Delta \mathcal{L}]\right}
$$</p>
<p>To validate that these moves and these acceptance rules lead to sampling from the equilibrium distribution $p\left(f_{\mathrm{i}} \mid D\right)$, we use the same example as in Fig. 1. We generate data as in Fig. 1 and sample expressions using MCMC, limiting the expressions to use only " + " and "sin" operations, a single variable $x$, a single parameter $a$, and a maximum of seven nodes. In fig. S1, we show that, as expected, the equilibrium distribution is $p\left(f_{\mathrm{i}} \mid D\right)$.</p>
<h2>Avoidance of expression duplicates in MCMC</h2>
<p>The mapping of expressions to expression trees is not one to one because several expression trees can represent the same expression (for example, the expression $x+a$ can be encoded in an expression tree with $x$ or with $a$ on the left branch). To avoid overcounting expressions, we internally reduce all expression trees to a "canonical form" using the Python module Sympy (37). Then, if an expression tree that is visited for the first time reduces to an expression that is equivalent to that of a previously visited expression tree, then the current tree is forbidden (assigned infinite description length) and never visited again, so that only one representative of each expression remains.</p>
<h2>Parallel tempering MCMC</h2>
<p>The search space for the MCMC is extremely rugged, with some neighboring expressions having very different description lengths. This makes the sampling process problematic and prone to getting trapped in local minima. To partly alleviate this problem, we used parallel tempering (38). In traditional parallel tempering (typically used in physics for spin glasses and other disordered systems), several replicas of the sampling process are kept at logarithmically spaced and increasing temperatures-at high temperatures, the sampling easily escapes local minima, whereas at low temperatures, the sampling explores configurations that are physically more meaningful. From time to time, samples at consecutive temperatures are switched with a rule that guarantees detailed balance at all temperatures. Therefore, the lowest temperature sample is always equilibrated at the desired temperature, but the configuration space is explored more efficiently because of the high temperature samples exploring larger portions of the space.</p>
<p>For our purposes here, we introduce a computational temperature $T$ and sample from the distributions</p>
<p>$$
p\left(f_{\mathrm{i}}, T_{k}\right)=\frac{1}{Z} \exp \left[-\frac{B\left(f_{\mathrm{i}}\right)}{2 T_{k}}+\log p\left(f_{\mathrm{i}}\right)\right]
$$</p>
<p>With this definition, $p\left(f_{\mathrm{i}}, T=1\right)=p\left(f_{\mathrm{i}} \mid D\right)$ is the equilibrium distribution from which we aim to sample, and $p\left(f_{\mathrm{i}}, T=\infty\right)=p\left(f_{\mathrm{i}}\right)$ is the prior distribution, independent of the data.</p>
<p>The results of the manuscript were obtained with 40 different temperatures defined as $T_{k}=1.05^{k}$, with $k={0,1, \ldots, 39}$. From all these, we take expressions only from the $T_{0}=1$ sample (Fig. 3). In addition, for the Nikuradse dataset, we used four restarts of the parallel tempering MCMC so that the sampling is even better equilibrated.</p>
<h2>Parsing of Wikipedia expressions for expression priors</h2>
<p>We compiled all the mathematical expressions included in pages listed under Wikipedia's "List of scientific equations named after people," which we surmise often correspond to phenomenological models of the kind that we aim to identify (as opposed, for example, to derivations of mathematical identities, proofs of mathematical theorems, etc.).</p>
<p>In Wikipedia, expressions are encoded in such a way that some of them are ambiguous without the context provided by the text in the page, so fully automatic parsing of expressions is virtually impossible. For example, the expression $x^{\alpha}+x^{b}$ could represent the sum of the two components of a vector or the sum of two powers of $x$. We designed an algorithm that parses Wikipedia expressions into Sympy (37), using heuristics for the ambiguous cases (the full code for expression parsing is available at https://bitbucket.org/rguimera/machine-scientist/src/ no_degeneracy/Process-Formulas/). We then selected three random</p>
<p>subsets of 200 expressions out of the 4080 expressions that could be parsed and verified manually that the parsings were meaningful (accepting parsings that may be wrong once the context is known but that are otherwise mathematically correct). We refined the heuristics until we found that there were fewer than $5 \%$ of errors in the parsed expressions of all three subsets. In table S2, we show a small sample of the corpus.</p>
<h2>Prior definition</h2>
<p>To define the prior distribution over mathematical expressions, we took advantage of the fact that mathematical expressions can be represented as graphs and used an approach based on exponential random graph models (20-23). As in exponential random graphs models, we aimed to generate mathematical expressions (graphs) with statistical properties similar to those in the corpus. Specifically, we aimed to generate mathematical expressions for which the average number of each operation per expression is the same as in the corpus (for example, in the corpus, the average number of sums per expression is $\left\langle n_{+}\right\rangle=$ 0.312 ). We also aimed to reproduce the average of the square of the number of each operation per expression (for example, the average of the square of the number of sums per expression, $\left\langle n_{+}^{2}\right\rangle=0.731$ ) $(23,24)$. As in exponential random graph models, we selected the prior probability $p(f)$ that generates expressions with these desired average properties and that, at the same time, is maximally uninformative and therefore consistent with the maximum entropy principle $(14,23,39)$. This prior is given by</p>
<p>$$
p\left(f_{i}\right)=\sum_{o \in O}\left[\alpha_{o} n_{o}\left(f_{i}\right)+\beta_{o} n_{o}^{2}\left(f_{i}\right)\right]
$$</p>
<p>where the sum is over all operations considered $\mathcal{O}={+, \exp , \ldots}$ (table S1), and $\alpha_{o}$ and $\beta_{o}$ are hyperparameters that we fitted so that expressions generated from the prior were consistent with the corpus $(22,23)$.</p>
<h2>Fitting of prior hyperparameters</h2>
<p>To fit the $\alpha_{o}$ and $\beta_{o}$ hyperparameters, we proceeded as follows:</p>
<p>1) Obtain, from the empirical corpus of mathematical expressions, the average number $\left\langle n_{o}\right\rangle^{\text {target }}$ of each operation $o$ per expression and the average of the square $\left\langle n_{o}^{2}\right\rangle^{\text {target }}$ of the number of each operation per expression.
2) Set some initial values for the parameters.
3) Repeat until the change in parameters values is small enough:
(i) Generate, using the MCMC, a large number of expressions (typically 1 million to 10 million).
(ii) Measure $\left\langle n_{o}\right\rangle^{\text {measured }}$ and $\left\langle n_{o}^{2}\right\rangle^{\text {measured }}$ in the generated formulas.
(iii) Update the $\alpha_{o}$ parameters as follows</p>
<p>$$
\alpha_{o} \leftarrow \alpha_{o}+\varepsilon \lambda \frac{\left\langle n_{o}\right\rangle^{\text {measured }}-\left\langle n_{o}\right\rangle^{\text {target }}}{\left\langle n_{o}\right\rangle^{\text {target }}}
$$</p>
<p>where $\lambda$ is a fixed parameter (typically between 0.01 and 0.05 ) and $\varepsilon$ is a random number in $[0,1]$ generated independently for each update.
(iv) Update the $\beta_{o}$ parameters as follows</p>
<p>$$
\beta_{o} \leftarrow \beta_{o}+\varepsilon \lambda \frac{\left\langle n_{o}^{2}\right\rangle^{\text {measured }}-\left\langle n_{o}^{2}\right\rangle^{\text {target }}}{\left\langle n_{o}^{2}\right\rangle^{\text {target }}}
$$</p>
<p>where $\lambda$ is a fixed parameter (typically between 0.01 and 0.05 ) and $\varepsilon$ is a random number in $[0,1]$ generated independently for each update. If one $\beta_{o}$ becomes negative, then its value is set to 0 .
(v) Repeat from (i).</p>
<p>This method is adapted from an exisiting method to fit the parameters of exponential random graphs (22). A typical evolution of the parameter values is shown in fig. S2, and the parameter values obtained are listed in tables S3 and S4 (Supplementary text S2).</p>
<h2>Benchmark machine scientists</h2>
<p>We benchmarked the performance of the Bayesian machine scientist against three other machine scientists: Eureqa (5), EPLEX (6), and EFS (13). Eureqa uses a genetic algorithm to search the space of expressions (5). Eureqa requires that a complexity penalty is set for each operation type; we set these penalties to their default values and selected the same basic operations used by the Bayesian machine scientist. We also selected the default fitness function. We ran Eureqa for several weeks in each experiment, until the number of evaluated expressions is, at least, $10^{13}$. Eureqa is available at www.nutonian.com/download/eureqa-desktop-download/.</p>
<p>EPLEX is another algorithm based on genetic programming and was the top performer in a recent systematic comparison of state of the art machine scientists (4). We ran EPLEX in the same conditions as in that study (implementation available at https://epistasislab.github.io/ellyn/); in particular, we obtained $10^{6}$ models with a population size of 1000 and 1000 generations. Moreover, we repeated this procedure 10 times and selected the realization with the best fitness among the 10 repetitions. However, in all our experiments, EPLEX gives results that are considerably worse than Eureqa's, so we do not show its results in the main text (Supplementary text S7 and fig. S10).</p>
<p>Last, EFS is a method based on sparse regression that generates basis functions automatically using a genetic algorithm (13). Thus, this approach has the advantages of sparse regression, and, at the same time, it does not require a priori knowledge of the basis functions. EFS is an evolution of multiple regression genetic programming (12), which was also a top performer in (4), although somehow more inconsistent than EPLEX. EFS is fast and gives expressions within seconds. Thus, we were able to repeat the training process 100 times for each experiment and select the model with the best default fitness measure (mean squared error). We used the implementation of EFS available at http://flexgp. github.io/efs/.</p>
<h2>Gaussian process models</h2>
<p>For the comparison of the out-of-sample predictions of Gaussian process models to those of the machine scientist, we proceeded as follows. We trained the Gaussian process using the scikit-learn implementation (40). We tested different kernels (including radial-basis function kernels, Matérn kernels, white kernels, rational quadratic kernels, exponential kernels, and linear and quadratic combinations of these), and we also tested the effects of using $D / r$ or its logarithm as the feature. Among these, we selected the combination that gave the best results (an radial-basis function plus white kernel with logarithmic $D / r$ )-all results reported in the main text correspond to this combination.</p>
<h2>SUPPLEMENTARY MATERIALS</h2>
<p>Supplementary material for this article is available at http://advances.sciencemag.org/cgi/ content/full/6/5/eaav6971/DC1
Supplementary Text S1. Calculation of the BIC
Supplementary Text S2. Prior parameter values</p>
<p>Supplementary Text S3. Validation and benchmarking on synthetic data
Supplementary Text S4. Rössler system
Supplementary Text S5. Recovery of Bessel functions
Supplementary Text S6. Results for small datasets
Supplementary Text S7. Nikuradse dataset
Table S1. Operations used by the machine scientist and their properties.
Table S2. Sample of the expressions in the empirical corpus.
Table S3. Values of the $\alpha$ prior parameters for different number of variables, $N_{\alpha}$, and parameters $N_{p}$.
Table S4. Values of the $\beta$ prior parameters for different number of variables, $N_{\alpha}$, and parameters $N_{p}$.
Table S5. Models for the funding application success in different European countries as a function of socioeconomic indicators of the countries and of their ability to attract and retain scientific talent.
Table S6. Models for the cell-to-cell stresses as a function of the concentration of several cell adhesion proteins.
Table S7. Models for the (logarithm of the) Seymour stock of salmon in the Fraser River system in British Columbia, Canada as a function of several ecological indicators.
Fig. S1. Equilibrium distribution of the MCMC.
Fig. S2. Fitting of the parameters of the prior distribution for expressions.
Fig. S3. Ability of the Bayesian machine scientist to identify the correct model from synthetic data as a function the number of observed points.
Fig. S4. True and most plausible models of the Rössler system in high-noise scenarios. Fig. S5. True and minimum BIC models of the Rössler system.
Fig. S6. Recovery of Bessel functions.
Fig. S7. Out-of-sample predictions for small datasets.
Fig. S8. Energy distribution for the Nikuradse dataset.
Fig. S9. Sampled models.
Fig. S10. EPLEX models and out-of-sample predictions for the Nikuradse dataset.
Movie S1. Video of the Bayesian machine scientist finding models for a synthetic dataset.</p>
<h2>REFERENCES AND NOTES</h2>
<ol>
<li>S. Džeroski, L. Todorovski, Lecture Notes in Artificial Intelligence, in Computational Discovery of Scientific Knowledge (Springer, 2007).</li>
<li>J. Evans, A. Rzhetsky, Machine science. Science 329, 399-400 (2010).</li>
<li>B. C. Daniels, I. Nemenman, Automated adaptive inference of phenomenological dynamical models. Nat. Commun. 6, 8133 (2015).</li>
<li>P. Orzechowski, W. La Cava, J. H. Moore, Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '18 (ACM, 2018), pp. 1183-1190.</li>
<li>M. Schmidt, H. Lipson, Distilling free-form natural laws from experimental data. Science 324, 81-85 (2009).</li>
<li>W. La Cava, L. Spector, K. Danai, Proceedings of the Genetic and Evolutionary Computation Conference 2016, GECCO '16 (ACM, 2016), pp. 741-748.</li>
<li>T. McConaghy, FFX: Fast, scalable, deterministic symbolic regression technology, in Genetic Programming Theory and Practice IX, R. Riolo,E. Vladislavleva, J. H. Moore, Eds. (Springer New York, 2011), pp. 235-260.</li>
<li>S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl. Acad. Sci. U.S.A. 133, 3932-3937 (2016).</li>
<li>Z. T. Wilson, N. V. Sahinidis, The ALAMO approach to machine learning. Comput. Chem. Eng. 106, 785-795 (2017).</li>
<li>S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven discovery of partial differential equations. Sci. Adv. 3, e1602614 (2017).</li>
<li>N. M. Mangan, J. N. Kutz, S. L. Brunton, J. L. Proctor, Model selection for dynamical systems via sparse regression and information criteria. Proc. R. Soc. A 473, 20170009 (2017).</li>
<li>I. Arnaldo, K. Krawiec, U.-M. O'Reilly, Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, GECCO '14 (ACM, 2014), pp. 879-886.</li>
<li>I. Arnaldo, U.-M. O'Reilly, K. Veeramachaneni, Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO '15 (ACM, 2015), pp. 983-990.</li>
<li>E. T. Jaynes, Information theory and statistical mechanics. Phys. Rev. 106, 620-630 (1957).</li>
<li>D. Barber, Bayesian Reasoning and Machine Learning (Cambridge Univ. Press, 2012).</li>
<li>P. D. Grünwald, The Minimum Description Length Principle (The MIT Press, 2007).</li>
<li>G. Schwarz, Estimating the dimension of a model. Ann. Stat. 6, 461-464 (1978).</li>
<li>T. Ando, Bayesian Model Selection and Statistical Modeling (CRC Press, 2010).</li>
<li>S. Konishi, T. Ando, S. Imoto, Bayesian information criteria and smoothing parameter selection in radial basis function networks. Biometrika 91, 27-43 (2004).</li>
<li>G. Robins, P. Pattison, Y. Kalish, D. Lusher, An introduction to exponential random graph $\left(p^{a}\right)$ models for social networks. Soc. Netw. 29, 173-191 (2007).</li>
<li>T. A. B. Snijders, Statistical models for social networks. Annu. Rev. Sociol. 37, 131-153 (2011).</li>
<li>A. Caimo, N. Friel, Bayesian inference for exponential random graphs. Soc. Netw. 33, 41-55 (2011).</li>
<li>S. Horvát, E. Czabarka, Z. Toroczkai, Reducing degeneracy in maximum entropy models of networks. Phys. Rev. Lett. 114, 158701 (2015).</li>
<li>R. Fischer, J. C. Ledda, T. P. Peixoto, E. G. Altmann, Sampling motif-constrained ensembles of networks. Phys. Rev. Lett. 115, 188701 (2015).</li>
<li>O. E. Rössler, An equation for continuous chaos. Phys. Lett. A 57, 397-398 (1976).</li>
<li>M. Quade, M. Abel, K. Shafi, R. K. Niven, B. R. Noack, Prediction of dynamical systems by symbolic regression. Phys. Rev. E 94, 012214 (2016).</li>
<li>M. De Domenico, A. Arenas, Researcher incentives: EU cash goes to the sticky and attractive. Nature 531, 580 (2016).</li>
<li>E. Bazellières, V. Conte, A. Elosegui-Artola, X. Serra-Picamal, M. Bintanel-Morcillo, P. Roca-Cusachs, J. J. Muñoz, M. Sales-Pardo, R. Guimerà, X. Trepat, Control of cell-cell forces and collective cell dynamics by the intercellular adhesome. Nat. Cell Biol. 17, 409-420 (2015).</li>
<li>H. Ye, R. J. Beamish, S. M. Glaser, S. C. H. Grant, C.-H. Hsieh, L. J. Richards, J. T. Schnute, G. Sugihara, Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling. Proc. Natl. Acad. Sci. U.S.A. 112, E1569-E1576 (2015).</li>
<li>Reprinted in English in J. Nikuradse, NACA Tech. Memo.1292 (1950).</li>
<li>G. Gioia, P. Chakraborty, Turbulent friction in rough pipes and the energy spectrum of the phenomenological theory. Phys. Rev. Lett. 96, 044502 (2006).</li>
<li>N. Goldenfeld, Roughness-induced critical phenomena in a turbulent flow. Phys. Rev. Lett. 96, 044503 (2006).</li>
<li>Z.-S. She, Y. Wu, X. Chen, F. Hussain, A multi-state description of roughness effects in turbulent pipe flow. New J. Phys. 14, 093054 (2012).</li>
<li>J. A. Hoeting, D. Madigan, A. E. Raftery, C. T. Volinsky, Bayesian model averaging: A tutorial. Stat. Sci. 14, 382417 (1999).</li>
<li>N. Simidjievski, L. Todorovski, S. Džeroski, Modeling dynamic systems with efficient ensembles of process-based models. PLOS ONE 11, e0153507 (2016).</li>
<li>N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, E. Teller, Equation of state calculations by fast computing machines. J. Chem. Phys. 21, 1087-1092 (1953).</li>
<li>A. Meurer, C. P. Smith, M. Paprocki, O. Čertik, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, S. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, A. Scopatz, Sympy: symbolic computing in python. PeerJ Comput. Sci. 3, e103 (2017).</li>
<li>D. J. Earl, M. W. Deem, Parallel tempering: Theory, applications, and new perspectives. Phys. Chem. Chem. Phys. 7, 3910-3916 (2005).</li>
<li>E. T. Jaynes, Probability Theory: The Logic of Science (Cambridge Univ. Press, 2003).</li>
<li>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825-2830 (2011).</li>
</ol>
<p>Acknowledgments: We thank E. G. Altmann, L. A. N. Amaral, A. Arenas, J. Bonet Avalos, and D. Shasha for helpful comments and suggestions. We thank I. Arnaldo for help with the EFS software. We thank A. Arenas for pointing us toward the Nikuradse dataset. We thank M. De Domenico and A. Arenas for sharing the financial success dataset. We thank E. Bazellières and X. Trepat for sharing the cell adhesion dataset. Funding: This project has received funding from the Spanish Ministerio de Economia y Competitividad (FIS2015-71563-ERC, FIS2016-78904-C3-P-1, and DPI2016-75791-C2-1-P). F.A.M. acknowledges financial support by the Spanish MINECO grant PTQ-14-06718 (2016-2019) of the Torres Quevedo Programme. Author contributions: R.G. conceived the research. R.G., I.R., A.A.-M., F.A.M., M.M., and M.S.-P. contributed methods, wrote code for the computational experiments, and carried out computational experiments. All authors designed the computational experiments and interpreted the results, and wrote and edited the manuscript. Competing interests: The authors declare that they have no competing interests. Data and materials availability: A Python implementation of the Bayesian machine scientist and all data needed to evaluate the conclusions in the paper are publicly available from Bitbucket at https://bitbucket.org/iguimera/machine-scientist. Additional data related to this paper may be requested from the authors.</p>
<p>Submitted 11 October 2018
Accepted 20 November 2019
Published 31 January 2020
10.1126/sciadv.aav6971</p>
<p>Citation: R. Guimerà, I. Reichardt, A. Aguilar-Mogas, F. A. Massucci, M. Miranda, J. Pallarés, M. Sales-Pardo, A Bayesian machine scientist to aid in the solution of challenging scientific problems. Sci. Adv. 6, eaav6971 (2020).</p>
<h1>ScienceAdvances</h1>
<h2>A Bayesian machine scientist to aid in the solution of challenging scientific problems</h2>
<p>Roger Guimerà, Ignasi Reichardt, Antoni Aguilar-Mogas, Francesco A. Massucci, Manuel Miranda, Jordi Pallarès and Marta Sales-Pardo</p>
<p>Sci Adv 6 (5), eaav6971.
DOI: 10.1126/sciadv.aav6971</p>
<p>ARTICLE TOOLS
SUPPLEMENTARY MATERIALS
REFERENCES</p>
<p>PERMISSIONS
http://advances.sciencemag.org/content/6/5/eaav6971
http://advances.sciencemag.org/content/suppl/2020/01/27/6.5.eaav6971.DC1</p>
<p>This article cites 29 articles, 4 of which you can access for free
http://advances.sciencemag.org/content/6/5/eaav6971#BIBL
http://www.sciencemag.org/help/reprints-and-permissions</p>
<p>Use of this article is subject to the Terms of Service</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Science Advances (ISSN 2375-2548) is published by the American Association for the Advancement of Science, 1200 New York Avenue NW, Washington, DC 20005. The title Science Advances is a registered trademark of AAAS.
Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
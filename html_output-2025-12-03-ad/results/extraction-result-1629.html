<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-267782996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14795v2.pdf" target="_blank">CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for realworld dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io/</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1629.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1629.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyberDemo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that collects human teleoperation demonstrations in simulation, applies simulator-grounded visual and kinematic augmentations (including sensitivity-aware kinematics), trains a visual imitation policy with automatic curriculum learning and action-chunking, and then fine-tunes the policy on a small amount of real data to enable sim-to-real transfer for a multi-finger dexterous hand.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Allegro dexterous hand mounted on XArm6</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A high-DoF multi-finger humanoid hand (Allegro) attached to an XArm6 robot arm; action space includes 6-DoF end-effector delta pose plus 16 finger joint positions under PD control. Used for real-world dexterous manipulation tasks (pick-and-place, pouring, valve rotation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (dexterous manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>SAPIEN</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics-based simulator replicating the real task environments (tables and objects) used in experiments, providing access to ground-truth states and contact information and enabling rendering of RGB images from arbitrary camera poses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics for contact-aware manipulation with photorealistic/advanced rendering options (configurable lighting, textures, camera poses); but not a perfect model of all real dynamics (approximated dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>contact information and object poses, 6-DoF rigid-body physics for arm/objects, photorealistic rendering (lighting direction/color, shadows, material properties such as specularity, roughness, metallicity), camera perspective projection, full access to ground-truth state and contact geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>real-world controller/actuator delays, fine-grained unmodelled frictional/contact microphysics and sensor noise, possible tactile/force sensing mismatches, hardware-specific actuator dynamics and wear, and some real-world stochasticities in human teleoperation patterns (these are approximated or left unmodelled).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab setup with an Allegro hand on an XArm6 mounted over a table; RGB camera(s) (RealSense) for teleoperation and observation; hand-eye calibration performed to align simulation and reality; tests include controlled lighting and also disturbed lighting (disco light) and randomized object placements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Dexterous manipulation tasks: quasi-static pick-and-place, pouring, and non-quasi-static valve rotation (in-hand/hand-and-arm coordination, precise contact-rich manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning (behavioral cloning / conditional VAE + Action Chunking with Transformers), pretrained on augmented simulated human teleoperation demos and fine-tuned on a small set of real demonstrations. Automatic curriculum learning controls augmentation difficulty during training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (binary success per trial, reported as percentage success over multiple trials).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>High simulation performance on augmented datasets — e.g., up to 92.5% success in-simulation on Level 1 when trained with all four augmentation levels (Table 2 reports 92.5% in-sim Level1 for [1,2,3,4]/8).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Improved real-world performance versus baselines: examples reported include ~35% absolute higher success vs R3M on quasi-static pick-and-place and ~20% higher on rotate tasks; specific real-world numbers include 42.5% success reported for rotating novel tetra- and penta-valves in one test, and in the ablation (full augmentation) real results: In-domain 35%, Random Light 30%, Out-of-Position 30%, Out-of-Position+Random Light 40% (Table 2, final row).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Extensive simulator-side randomization and augmentation: random camera views (pose perturbation, perspective-correct rendering), random lighting (directions, colors, shadow characteristics, ambient illumination), random object textures and material properties (specularity, roughness, metallicity, textures), diverse object geometries (replacing manipulated objects), randomized object poses and initial robot/object poses, and gaussian perturbations to actions when replaying trajectories for novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Dynamics/controller gap for high-DoF dexterous hand (unmodelled actuator/controller dynamics), discrepancies in contact microphysics and friction, visual domain gap (lighting, texture), differences in human teleoperation data distributions between sim and real (collection patterns), potential overfitting to joint-position patterns when training solely on limited real data, and sensor/actuator noise or timing differences not fully modelled.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Key enablers: (1) Extensive simulator-grounded augmentation (visual + kinematic) including sensitivity-aware kinematics augmentation and replay-based multi-view rendering; (2) Automatic curriculum learning that gradually increases augmentation difficulty based on measured success rate; (3) Action aggregation / action-chunking to reduce noise and produce smoother trajectories; (4) Fine-tuning on a small set of real demonstrations (few minutes / tens of demos) with smaller learning rate and separate batch-norms to close residual controller gap; (5) hand-eye calibration to align sim and real frames.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies that (a) photorealistic visual randomization (lighting, textures, camera views) aids visual gap closure, and (b) contact/dynamics mismatch is critical for high-DoF dexterous tasks and cannot be fully resolved by visual adaptation alone — necessitating kinematics-aware augmentation and small real-world fine-tuning; no numeric accuracy thresholds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuned on a small number of real demonstrations (examples: 3-minute trajectory per task; experiments using e.g., 15 real demonstrations or combinations like 15 sim + 35 real were reported). Fine-tuning used lower learning rate (finetune LR = 1e-6), separate batch norms for real data, and up to thousands of finetuning epochs in some settings (paper reports finetuning epochs and hyperparameters in supplementary).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The study compares four augmentation/curriculum levels and finds monotonic improvements: training with all four augmentation levels produced the best sim and real results — in-simulation success: Level1 92.5%, Level2 81%, Level3 63%, Level4 49%; real-world success for full augmentation: In-domain 35%, Random Light 30%, Out-of-Position 30%, Out-of-Position+Random Light 40% (Table 2). More augmentation improved both simulation robustness and real-world generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulator-collected human demonstrations, when coupled with simulator-grounded data augmentation (visual and kinematic), curriculum learning, action aggregation, and small real-world fine-tuning, can outperform policies trained only on comparable amounts of real data for dexterous manipulation. Visual augmentations (lighting, textures, camera views) effectively close visual sim-to-real gap; sensitivity-aware kinematics augmentation and fine-tuning are necessary to mitigate dynamics/controller gaps for high-DoF hands. There remains a residual controller/dynamics gap that requires limited real data to fully close; overall, simulator augmentation is cost-effective and improves generalization to novel objects and disturbed lighting/poses.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The paper uses SAPIEN simulation for augmentation and notes advantages of replaying simulator internal state to render new camera views and lighting; sensitivity-aware kinematics augmentation changes actions across trajectory segments according to segment robustness to produce feasible re-targeted trajectories rather than naive appended trajectories (compared vs MimicGen).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>RetinaGAN: An object-aware approach to sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>MimicGen: A data generation system for scalable robot learning using human demonstrations <em>(Rating: 2)</em></li>
                <li>Dextreme: Transfer of agile in-hand manipulation from simulation to reality <em>(Rating: 2)</em></li>
                <li>Learning robust real-world dexterous grasping policies via implicit shape augmentation <em>(Rating: 1)</em></li>
                <li>Learning active task-oriented exploration policies for bridging the sim-to-real gap <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1629",
    "paper_id": "paper-267782996",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "CyberDemo",
            "name_full": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation",
            "brief_description": "A pipeline that collects human teleoperation demonstrations in simulation, applies simulator-grounded visual and kinematic augmentations (including sensitivity-aware kinematics), trains a visual imitation policy with automatic curriculum learning and action-chunking, and then fine-tunes the policy on a small amount of real data to enable sim-to-real transfer for a multi-finger dexterous hand.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Allegro dexterous hand mounted on XArm6",
            "agent_system_description": "A high-DoF multi-finger humanoid hand (Allegro) attached to an XArm6 robot arm; action space includes 6-DoF end-effector delta pose plus 16 finger joint positions under PD control. Used for real-world dexterous manipulation tasks (pick-and-place, pouring, valve rotation).",
            "domain": "general robotics manipulation (dexterous manipulation)",
            "virtual_environment_name": "SAPIEN",
            "virtual_environment_description": "A physics-based simulator replicating the real task environments (tables and objects) used in experiments, providing access to ground-truth states and contact information and enabling rendering of RGB images from arbitrary camera poses.",
            "simulation_fidelity_level": "high-fidelity physics for contact-aware manipulation with photorealistic/advanced rendering options (configurable lighting, textures, camera poses); but not a perfect model of all real dynamics (approximated dynamics).",
            "fidelity_aspects_modeled": "contact information and object poses, 6-DoF rigid-body physics for arm/objects, photorealistic rendering (lighting direction/color, shadows, material properties such as specularity, roughness, metallicity), camera perspective projection, full access to ground-truth state and contact geometry.",
            "fidelity_aspects_simplified": "real-world controller/actuator delays, fine-grained unmodelled frictional/contact microphysics and sensor noise, possible tactile/force sensing mismatches, hardware-specific actuator dynamics and wear, and some real-world stochasticities in human teleoperation patterns (these are approximated or left unmodelled).",
            "real_environment_description": "Physical lab setup with an Allegro hand on an XArm6 mounted over a table; RGB camera(s) (RealSense) for teleoperation and observation; hand-eye calibration performed to align simulation and reality; tests include controlled lighting and also disturbed lighting (disco light) and randomized object placements.",
            "task_or_skill_transferred": "Dexterous manipulation tasks: quasi-static pick-and-place, pouring, and non-quasi-static valve rotation (in-hand/hand-and-arm coordination, precise contact-rich manipulation).",
            "training_method": "Imitation learning (behavioral cloning / conditional VAE + Action Chunking with Transformers), pretrained on augmented simulated human teleoperation demos and fine-tuned on a small set of real demonstrations. Automatic curriculum learning controls augmentation difficulty during training.",
            "transfer_success_metric": "Task success rate (binary success per trial, reported as percentage success over multiple trials).",
            "transfer_performance_sim": "High simulation performance on augmented datasets — e.g., up to 92.5% success in-simulation on Level 1 when trained with all four augmentation levels (Table 2 reports 92.5% in-sim Level1 for [1,2,3,4]/8).",
            "transfer_performance_real": "Improved real-world performance versus baselines: examples reported include ~35% absolute higher success vs R3M on quasi-static pick-and-place and ~20% higher on rotate tasks; specific real-world numbers include 42.5% success reported for rotating novel tetra- and penta-valves in one test, and in the ablation (full augmentation) real results: In-domain 35%, Random Light 30%, Out-of-Position 30%, Out-of-Position+Random Light 40% (Table 2, final row).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Extensive simulator-side randomization and augmentation: random camera views (pose perturbation, perspective-correct rendering), random lighting (directions, colors, shadow characteristics, ambient illumination), random object textures and material properties (specularity, roughness, metallicity, textures), diverse object geometries (replacing manipulated objects), randomized object poses and initial robot/object poses, and gaussian perturbations to actions when replaying trajectories for novel objects.",
            "sim_to_real_gap_factors": "Dynamics/controller gap for high-DoF dexterous hand (unmodelled actuator/controller dynamics), discrepancies in contact microphysics and friction, visual domain gap (lighting, texture), differences in human teleoperation data distributions between sim and real (collection patterns), potential overfitting to joint-position patterns when training solely on limited real data, and sensor/actuator noise or timing differences not fully modelled.",
            "transfer_enabling_conditions": "Key enablers: (1) Extensive simulator-grounded augmentation (visual + kinematic) including sensitivity-aware kinematics augmentation and replay-based multi-view rendering; (2) Automatic curriculum learning that gradually increases augmentation difficulty based on measured success rate; (3) Action aggregation / action-chunking to reduce noise and produce smoother trajectories; (4) Fine-tuning on a small set of real demonstrations (few minutes / tens of demos) with smaller learning rate and separate batch-norms to close residual controller gap; (5) hand-eye calibration to align sim and real frames.",
            "fidelity_requirements_identified": "Paper identifies that (a) photorealistic visual randomization (lighting, textures, camera views) aids visual gap closure, and (b) contact/dynamics mismatch is critical for high-DoF dexterous tasks and cannot be fully resolved by visual adaptation alone — necessitating kinematics-aware augmentation and small real-world fine-tuning; no numeric accuracy thresholds are provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Fine-tuned on a small number of real demonstrations (examples: 3-minute trajectory per task; experiments using e.g., 15 real demonstrations or combinations like 15 sim + 35 real were reported). Fine-tuning used lower learning rate (finetune LR = 1e-6), separate batch norms for real data, and up to thousands of finetuning epochs in some settings (paper reports finetuning epochs and hyperparameters in supplementary).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "The study compares four augmentation/curriculum levels and finds monotonic improvements: training with all four augmentation levels produced the best sim and real results — in-simulation success: Level1 92.5%, Level2 81%, Level3 63%, Level4 49%; real-world success for full augmentation: In-domain 35%, Random Light 30%, Out-of-Position 30%, Out-of-Position+Random Light 40% (Table 2). More augmentation improved both simulation robustness and real-world generalization.",
            "key_findings": "Simulator-collected human demonstrations, when coupled with simulator-grounded data augmentation (visual and kinematic), curriculum learning, action aggregation, and small real-world fine-tuning, can outperform policies trained only on comparable amounts of real data for dexterous manipulation. Visual augmentations (lighting, textures, camera views) effectively close visual sim-to-real gap; sensitivity-aware kinematics augmentation and fine-tuning are necessary to mitigate dynamics/controller gaps for high-DoF hands. There remains a residual controller/dynamics gap that requires limited real data to fully close; overall, simulator augmentation is cost-effective and improves generalization to novel objects and disturbed lighting/poses.",
            "additional_notes": "The paper uses SAPIEN simulation for augmentation and notes advantages of replaying simulator internal state to render new camera views and lighting; sensitivity-aware kinematics augmentation changes actions across trajectory segments according to segment robustness to produce feasible re-targeted trajectories rather than naive appended trajectories (compared vs MimicGen).",
            "uuid": "e1629.0",
            "source_info": {
                "paper_title": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "RetinaGAN: An object-aware approach to sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "retinagan_an_objectaware_approach_to_simtoreal_transfer"
        },
        {
            "paper_title": "MimicGen: A data generation system for scalable robot learning using human demonstrations",
            "rating": 2,
            "sanitized_title": "mimicgen_a_data_generation_system_for_scalable_robot_learning_using_human_demonstrations"
        },
        {
            "paper_title": "Dextreme: Transfer of agile in-hand manipulation from simulation to reality",
            "rating": 2,
            "sanitized_title": "dextreme_transfer_of_agile_inhand_manipulation_from_simulation_to_reality"
        },
        {
            "paper_title": "Learning robust real-world dexterous grasping policies via implicit shape augmentation",
            "rating": 1,
            "sanitized_title": "learning_robust_realworld_dexterous_grasping_policies_via_implicit_shape_augmentation"
        },
        {
            "paper_title": "Learning active task-oriented exploration policies for bridging the sim-to-real gap",
            "rating": 1,
            "sanitized_title": "learning_active_taskoriented_exploration_policies_for_bridging_the_simtoreal_gap"
        }
    ],
    "cost": 0.011487999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation
1 Mar 2024</p>
<p>Jun Wang 
UC San Diego</p>
<p>Yuzhe Qin 
UC San Diego</p>
<p>Kaiming Kuang 
UC San Diego</p>
<p>Yigit Korkmaz 
University of Southern</p>
<p>Akhilan Gurumoorthy 
UC San Diego</p>
<p>Hao Su 
UC San Diego</p>
<p>Xiaolong Wang 
UC San Diego</p>
<p>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation
1 Mar 2024C30111E20D8D67A77F968DB2361C646DarXiv:2402.14795v2[cs.RO]
Figure1.We propose CyberDemo, a novel pipeline for learning real-world dexterous manipulation by using simulation data.First, we collect human demos in a simulated environment (blue region), followed by extensive data augmentation within the simulator (yellow region).Then, the imitation learning model, trained on augmented data and fine-tuned on a few real data, can be deployed on a real robot.</p>
<p>Imitation learning has been a promising approach in robot manipulation, facilitating the acquisition of complex skills from human demonstration.However, the effectiveness of this approach is critically dependent on the availability of high-quality demonstration data, which often necessitates substantial human effort for data collection [6,9,48].This challenge is further amplified in the context of manipulation with a multi-finger dexterous hand, where the complexity and intricacy of the tasks require highly detailed and precise demonstrations.</p>
<p>In imitation learning, in-domain demonstrations, which refer to the data collected directly from the deployment environment, are commonly used for robot manipulation tasks [43].It is generally believed that the most effective way to solve a specific task is to collect demonstrations directly from the real robot on that task.This belief has been upheld as the gold standard, but we wish to challenge it.We argue that collecting human demonstrations in simulation can yield superior results for real-world tasks, not only because it does not require real hardware and can be executed remotely and in parallel, but also due to its potential to enhance final task performance by employing simulator-only data augmentation [34,37,42,44,53,58].This allows the generation of a dataset that is hundreds of times larger than the initial demonstration set.However, while existing studies employ the generated dataset to train in-domain policies within the simulation, the sim2real challenge of transferring policies to the real world remains an unresolved problem.</p>
<p>In this paper, we study the problem of how to utilize simulated human demos for real-world robot manipulation tasks.We introduce CyberDemo, a novel framework designed for robotic imitation learning from visual observations, leveraging simulated human demos.We first collect a modest amount of human demonstration data via teleoperation using low-cost devices in a simulated environment.Then, CyberDemo incorporates extensive data augmentation into the original human demonstration.The augmented set covers a broad spectrum of visual and physical conditions not encountered during data collection, thereby enhancing the robustness of the trained policy against these variations.These augmentation techniques are also designed with the downstream sim2real transfer in mind.We employ a unique curriculum learning strategy to train the policy on the augmented dataset, then fine-tune it using a few real-world demos (3-minute trajectories), facilitating effective transfer to real-world conditions.While policies trained on only real-world demonstrations may suffer from variations in lighting conditions, object geometry, and object initial pose, our policy is capable of handling these without the need for additional human effort.</p>
<p>Our system, which utilizes a low-cost motion capture device for teleoperation (i.e., RealSense camera) and demands minimal human effort (i.e., a 30-minute demo trajectory), can learn a robust imitation learning policy.Despite its affordability and minimal human effort requirements, CyberDemo can still achieve better performance on the real robot.Compared with pre-trained policies, e.g.R3M [46] fine-tuned on real-world demonstrations, Cy-berDemo achieves a success rate that is 35% higher for quasi-static pick and place tasks, and 20% higher for nonquasi-static rotate tasks.In the generalization test, while baseline methods struggle to handle unseen objects during testing, our method can rotate novel tetra-valve and pentavalve with 42.5% success rate, even though human demonstrations only cover tri-valve (second row of Figure 1).Our method can also manage significant light disturbances (last column of Figure 1).In our ablation study, we observe that the use of data augmentation, coupled with an increased number of demonstrations in the simulator, results in superior performance compared to an equivalent increase in real-world demonstrations.To foster further research, we will make our code and human demonstration dataset publicly available.</p>
<p>. Related Work</p>
<p>Data for Learning Robot Manipulation.Imitation learning has been proven to be an effective approach to robotic manipulation, enabling policy training with a collection of demonstrations.Many works have focused on building large datasets using pre-programmed policies [17,23,31,33,84], alternative data sources such as language [30,66,67,69] and human video [5,47,54,63,64] or extensive real-world robot teleoperation [2,3,6,9,20,32,39,43,48].However, such works predominantly targeted parallel grippers.Collecting large-scale demonstration datasets for high-DoF dexterous hands continues to be a significant challenge.Meanwhile, data augmentation presents a viable strategy to improve policy generalization by increasing the diversity of data distribution.Previous studies have applied augmentation in low-level visual space [16,28,56,65], such as color jitter, blurring, and cropping, while more recent works propose semantic-aware data augmentation with generative models [7,14,15,40,78,86].However, these augmentations operate at the image level and are not grounded in physical reality.CyberDemo extends data augmentation to the trajectory level using a physical simulator, accounting for both visual and physical variations.Concurrent to our work, MimicGen [44] proposes a system to synthesize demonstrations for long-horizon tasks by integrating multiple human trajectories.However, it confines demonstrations to in-domain learning, i.e., it only trains simulation policies with simulated demos without transferring to real robots.In contrast, our work aims to harness simulation for real-world problem-solving.We exploit the convenience of simulators for collecting robot demonstrations and employ a sim2real approach to transfer these demos to a dexterous robot equipped with a multi-finger humanoid hand.Our research emphasizes a general framework that leverages simulated demonstrations for real-world robot manipulation.Pre-trained Visual Representation for Robotics Recent progress in large-scale Self-Supervised Learning [10,26,27] has enabled the development of visual representations that are advantageous for downstream robotic tasks [62,80,83].Several studies have focused on pretraining on nonrobotic datasets, such as ImageNet [18] and Ego4D [22], and utilizing the static representations for downstream robot control [46,49,75].Other research has focused on pretraining visual representations on robot datasets, using action-supervised self-learning objectives that depend on actions [60,64], or utilizing the temporal consistency of video as a learning objective [59,61,70,76].These investigations primarily aimed to learn features for effective training of vision-based robotic manipulation.In addition to training visual representations on offline datasets, some researchers have also explored learning the reward function to be used in reinforcement learning [4,36,41,45,82].Unlike prior studies, our work diverges by utilizing simulation data for pre-training rather than employing Self-Supervised Learning for representation learning.This not only enhances the learning of image representations but also incorporates task priors into the neural network through the use of action information.By pre-training in simulated environments, the manipulation policy can better generalize to new objects with novel geometries and contact patterns.</p>
<p>Sim2Real Transfer</p>
<p>The challenge of transferring skills from simulation to real-world scenarios, known as sim2real transfer, has been a key focus in robot learning.Some approaches have employed system identification to build a mathematical model of real systems and identify physical parameters [12,29,35,38,52,73].Instead of calibrating real-world dynamics, domain randomization [50,71] generates simulated environments with randomized properties and trains a model function across all of them.Subsequent research demonstrated that the selection of randomization parameters could be automated [1,11,24,81].However, due to the extensive sample requirements to learn robust policies, domain randomization is typically used with RL involving millions of interaction samples.Domain adap-tation (DA) refers to a set of transfer learning strategies developed to align the data distribution between sim and real.Common techniques include domain adversarial training [21,72] and the use of generative models to make simulated images resemble real ones [8,28].Most of these DA approaches focus on bridging the visual gap.However, the challenge of addressing the dynamics gap remains significant.The sim2real gap becomes even more pronounced for dexterous robotic hands that have high-DoF actuation and complex interaction patterns [24,51,77,79].In this work, we extend the concept of domain randomization to human demonstration collected in the simulator and focus on data augmentation techniques that can effectively utilize the simulation for transfer to a real robot.We demonstrate that there can be a significant benefit in collecting human demonstration in the simulator, despite the sim2real gap, instead of solely relying on real data.</p>
<p>. CyberDemo</p>
<p>In CyberDemo, we initially gather human demonstrations of the same task in a simulator through teleoperation (Section 3 .1).Taking advantage of the simulator's sampling capabilities and oracle state information, we enhance the simulated demonstration in various ways, increasing its vi-  sual, kinematic, and geometric diversity, thereby enriching the simulated dataset (Section 3 .2).With this augmented dataset, we train a manipulation policy with Automatic Curriculum Learning and Action Aggregation (Section 3 .3).</p>
<p>.1. Collecting Human Teleoperation Data</p>
<p>For each dexterous manipulation task in this work, we collect human demonstrations using teleoperation in both simulated and real-world environments.For real-world data, we utilize the low-cost teleoperation system referenced in [55].This vision-based teleoperation system solely needs a camera to capture human hand motions as input, which are then translated into real-time motor commands for the robot arm and the dexterous hand.We record the observation (RGB image, robot proprioception) and the action (6D Cartesian velocity of robot end effector, finger joint position control target) for each frame at a rate of 30Hz.For this work, we collect only three minutes of robot trajectories for each task on the real robot.</p>
<p>For data in simulation, we build the real-world task environments within the SAPIEN [74] simulator to replicate the tables and objects used in real scenarios.It is worth noting that, for teleoperation, there is no requirement of reward design and observation spaces as in reinforcement learning settings, making the process of setting up new tasks in the simulator relatively simple.We employ the same teleoperation system [55] to collect human demonstrations in the simulator.</p>
<p>.2. Augmenting Human Demo in Simulator</p>
<p>Unlike real-world data collection, where we are limited to recording observations of physical sensors, such as camera RGB images and robot proprioception, the simulation system enables us to record the ground-truth state and contact information within the virtual environment.This unique benefit of simulation provides a more comprehensive data format for the simulated demonstrations compared to its real-world counterparts.Thus, we can take advantage of demonstration replay techniques on these simulated demonstrations, which are not feasible with real-world data.</p>
<p>When developing data augmentation techniques in the simulator, it is essential to keep in mind that the ultimate goal is to deploy the trained policy to a real robot.The augmentation should accordingly focus on the visual and dynamical variations that are likely to be encountered in the real world.Moreover, we aim for the manipulation policy to generalize to novel objects not encountered during the data collection process.For example, manipulating the tetra-valve when collecting data only on the tri-valve in Figure 3. Specifically, we chose to augment the lighting conditions, camera views, and object textures to enhance the policy's robustness against visual variations.In addition, we modified the geometric shape of the objects and the initial poses of the robot and objects to improve the policy's robustness against dynamical variations as follows: Randomize Camera Views.Precisely aligning camera views between demo collection and final evaluation, not to mention between simulation and reality, poses a significant challenge.To solve this problem, we randomize the camera pose during training and replay the internal state of the simulator to render image sequences from new camera views.Unlike standard image augmentation techniques such as cropping and shifting, our method respects the perspective projection in a physically realistic manner.Random Light and Texture.To facilitate sim2real transfer and improve the policy's robustness against visual variations, we randomize the visual properties of both lights and objects (Figure 3, lower right).Light properties include directions, colors, shadow characteristics, and ambient illumination.Object properties include specularity, roughness, metallicity, and texture.Similar to camera view randomization, we can simply replay the simulation state to render new image sequences.Add Diverse Objects.In this approach, we replace the manipulated object in the original demos with novel objects (Figure 3 upper right).However, directly replaying the same trajectory would not work as the object shape is different.Instead, we perturb the action sequence from the original demo with Gaussian noises to generate new trajectories.These trajectories provide reasonable manipulation strategies but are slightly different from the original one.With the highly cost-effective sampling in the simulator, we can enumerate the perturbation until it is successful.It is important to note that this technique is feasible with realworld demonstrations.Eval success rate r succ = eval L (π)
7: if r succ ≥ r up or N f ail ≥ N max then 8: L = L + 1, N f ail = 0 9: else 10: Generate more data D ← D + aug L (D h ) 11: N f ail = N f ail + 1 12:
end if 13: end while Randomize Object Pose.A common approach in reinforcement learning to enhance generalizability involves randomizing the object pose during reset.Augmenting imitation learning data to achieve a similar outcome, however, is less intuitive.Denote T
= T Onew W (T O old W ) −1 T R old W
. Then, the relative pose between the robot and the object aligns with the original demonstration, enabling us to replay the same action sequence to accomplish the task.Although this method succeeds in generating new trajectories, it offers minimal assistance for downstream imitation learning.The new trajectory is always composed of two segments: a computed reaching trajectory to the new end effector pose T Rnew W , and the original trajectory.Given that different augmented trajectories often share a significant portion of redundancy, they fail to provide substantial new information to learning algorithms.</p>
<p>To address this, we propose Sensitivity-Aware Kinematics Augmentation to randomize object poses for human demonstrations.Instead of appending a new trajectory ahead of the original one, this method amends the action for each step in the original demo to accommodate the change in object pose T Onew W (T O old W ) −1 .The method includes two steps: (i) Divide the entire trajectory into several segments and compute the sensitivity of each segment; (ii) Modify the end effector pose trajectory based on the sensitivity to compute the new action.</p>
<p>(i) Sensitivity Analysis for Trajectory Segments.Sensitivity pertains to the robustness against action noise.For example, a pre-grasp state, when the hand is close to the object, has higher sensitivity compared to a state where the hand is far away.The critical insight is that it is simpler to modify the action of those states with lower sensitivity to handle the object pose variation
∆T = T Onew W (T O old W ) −1 .
The robustness (the multiplicative inverse of sensitivity) of a trajectory segment ψ can be mathematically defined as follows:
ψ seg = exp(maxδ a ) s.t. eval(τ ′ ) = 1 τ ′ = {a 1 , a 2 , ..., a ′ n , ..., a ′ n+K−1 , ..., a N } ∀i ∈ seg a ′ i = a i + δ a ϵ i , ϵ i ∼ N (0, 1)(1)
In this equation, we divide the original action trajectory τ with length N into M segments, each segment with size K = N/M .Then we perturb the action within a segment seg by adding Gaussian noise of scale δ a to the original action {a m , a n+1 , ..., a n+k−1 } while keeping all the actions outside of this segment unchanged to generate perturbed trajectory τ ′ .We assume the action space is already normalized to [−1, 1] and eval is a binary function indicating whether an action trajectory can successfully solve the task.Intuitively, a demonstration segment is more sensitive if a smaller perturbation can cause it to fail.This sensitivity guides us on how to adjust the action to handle a new object pose.In practice, we incrementally escalate the noise scale δ a applied to the original action trajectory until the task fails to determine max δ a (ii) New End Effector Pose Trajectory.To accommodate the new object pose, the total pose change of the end effector should be the same as the change in the object pose ∆T .Each action contributes a small part to this change.We distribute this "task" to each step based on sensitivity:
ψ segj = ψ segj M j=1 ψ segj , ∀seg j ∆T j = exp(ψ segj log(∆T )/K) a new i = a i f i (∆T j )(2)
In this equation, ψ segj is the normalized robustness, ∆T j represents the pose modification for each step, with all states in the same segment being responsible for the same amount of modification "task" to compute new action a new i .f i is a similarity transformation in SE(3) space that converts the motion from the world frame to the current end effector frame.Intuitively, segments with higher robustness are tasked with more significant changes.</p>
<p>Please note that all the actions discussed above pertain solely to the 6D delta pose of the end effector and do not include the finger motion of the dexterous hand.For tasks such as pick-and-place or pouring, which also involve a target pose (e.g., the plate pose in pick-and-place or the bowl pose in pouring), we can apply the same augmentation strategy to the target pose (as illustrated in Level 3 of Fig. 2).</p>
<p>.3. Learning Sim2Real Policy</p>
<p>Given an augmented simulation dataset, we train a visual manipulation policy that takes images and robot proprioception as input to predict the robot's actions.In human teleoperation demonstrations, robot movements are neither Moravian nor temporally correlated.To deal with this issue, our policy is trained to predict action chunks rather than per-step actions, using Action Chunking with Transformers(ACT) [85].This approach produces smoother trajectories and reduces compounding errors.</p>
<p>Despite our data augmentation's capacity to accommodate diverse visual and dynamic conditions, a sim2real gap remains for the robot controller.This gap becomes more challenging in our tasks, where the end effector is a high-DoF multi-finger dexterous hand.This controller gap can significantly impact non-quasi-static tasks like rotating a valve, as shown in the second row of Figure 1.To close this gap, we fine-tune our network using a small set of realworld demonstrations (3-minute trajectory).However, due to the discrepancies in data collection patterns of human demos between simulation and reality, direct fine-tuning on real data risks overfitting.To ensure a smoother sim2real transfer, we employ several techniques, which will be discussed subsequently.Automatic Curriculum Learning.Curriculum learning and data augmentation techniques are often used together to provide a smoother training process.Following the spirit of curriculum design in previous reinforcement learning work [1,24], we devise a curriculum learning strategy applicable to our imitation learning context.Prior to training, we group the augmentations in Section 3 .2into four levels of increasing complexity, as depicted in Figure 2. As per Algorithm 1, we begin training from the simplest level, L = 0, signifying no augmentation, and then evaluate the task success rate after several steps of training.The evaluation difficulty aligns with the current level of L. When the success rate surpasses a pre-defined threshold, we advance to the next level, which brings greater augmentation and harder evaluation.If the success rate fails to reach the threshold, we create additional augmented training data and stays at the current level.We continue this iterative process until all levels are completed.To prevent endless training, we introduce a fail-safe N max : if the policy repeatedly fails during evaluation for N max times, we also progress to the next level.This curriculum learning approach significantly depends on data augmentation techniques to generate training data dynamically with suitable levels of randomization.This concept stands in contrast to typical supervised learning scenarios, where data is pre-established prior to training.This on-demand data generation and customization highlights the advantage of simulation data over real-world demonstrations.Action Aggregation for Small Motion.Human demonstrations often include noise, especially during operations involving a dexterous hand.For example, minor shaking and unintentional halting can occur within the demonstration trajectory, potentially undermining the training process.</p>
<p>To solve this, we aggregate steps characterized by small motions, merging these actions into a single action.In practice, we set thresholds for both end-effector and finger motions to discern whether a given motion qualifies as small.Through the aggregation process, we can eliminate small operational noises from human actions, enabling the imitation learning policy to extract meaningful information from the state-action trajectory.</p>
<p>. Experiment Setups</p>
<p>Our experimental design aims to address the following key queries:</p>
<p>(i) How does simulation-based data augmentation compare to learning from real demonstrations in terms of both robustness and generalizability?</p>
<p>(ii) How does our automatic curriculum learning contribute to improved policy learning?</p>
<p>(iii) What is the ideal ratio between simulated and real data to train an effective policy for a real-world robot?</p>
<p>.1. Dexterous Manipulation Tasks</p>
<p>We have designed three types of manipulation tasks in both real-world and simulated environments, including two quasi-static tasks (pick and place, pour) and one non-quasistatic task (rotate).For the experiments, we utilize an Allegro hand attached to an XArm6.The action space comprises a 6-dim delta end effector pose of the robot arm and a 16-dim finger joint position of the dexterous hand, with PD control employed for both arm and hand.</p>
<p>Pick and Place.This task requires the robot to lift an object from the table and position it on a plate (first row of Figure 1).Success is achieved when the object is properly placed onto the red plate.We select two objects during data collection and testing on multiple different objects.</p>
<p>Rotate.This task requires the robot to rotate a valve on the table (second row of Figure 1).The valve is constructed with a fixed base and a moving valve geometry, connected via a revolute joint.The task is successful when the robot rotates the valve to 720 degrees.We use a tri-valve in data collection and test on tetra-valves and penta-valves.</p>
<p>Pour.This task requires the robot to pour small boxes from a bottle into a bowl (third row of Figure 1).It involves three steps: (i) Lift the bottle; (ii) Move it close to the bowl; (iii) Rotate the bottle to dispense the small boxes into the bowl.Success is achieved when all four boxes have been poured into the bowl.For each task, we have designed levels for both data augmentation ( Section 3 .2) and curriculum learning (Section 3 .3).More details regarding the design of task levels can be found in the supplementary material.</p>
<p>.2. Baselines</p>
<p>Our approach can be interpreted as an initial pretraining phase using augmented simulation demonstrations followed by fine-tuning with a limited set of real data.It is natural to compare our method with other pre-training models for robotic manipulation.We have chosen three representative vision pre-training models.For all of them, we utilize the pre-trained model provided by the author and then fine-tune it using our real-world demonstration dataset.PVR is built on MoCo-v2 [13], using a ResNet50 backbone [25] trained on ImageNet [57].MVP employs self-supervised learning from a Masked Autoencoder [27] to train visual representation on individual frames from an extensive human interaction dataset compiled from multiple existing datasets.MVP integrates a Vision Transformer [19] backbone that segments frames into 16x16 patches.R3M proposes a pre-training approach where a ResNet50 backbone is trained using a mix of time-contrastive learning, video-language alignment, and L1 regularization.This model is trained on a large-scale human interaction videos dataset from Ego4D [22].</p>
<p>. Results</p>
<p>.1. Main Comparison</p>
<p>Augmented simulation data markedly boosts real-world dexterous manipulation.As depicted in Table 1, our methodology outperforms the baselines trained exclusively on real data in the in-domain setting (Level 1), exhibiting an average performance boost of 31.67%averaged on all tasks.Additionally, in other settings like random lighting (Level 2), out of position (Level 3), combined random lighting and out of position (Level 3) R3M, and MVP, the baselines exhibit a significant drop in success rates.In contrast, our method shows resilience to these variations, underscoring the efficacy of simulation data augmentation.Not only  We compare our approach with the baselines in scenarios involving novel objects, random light disturbances, and random object positions.</p>
<p>does this approach bridge the sim2real gap and amplify performance in in-domain real-world tasks, but it also significantly improves manipulations in out-of-domain realworld scenarios.By integrating augmentation for both visual and dynamic variations, our method successfully navigates challenges and delivers impressive results.</p>
<p>.2. Generalization to Novel Objects</p>
<p>By incorporating data augmentation techniques, such as including diverse objects in simulation, our model can effectively manipulate unfamiliar objects, even when transitioning to a real-world context.As shown in Figure 4 and 5, the baseline methods grapple with more complex real-world situations.In the most challenging scenario, rotating novel objects under random light conditions and new object positions, only one baseline method manages to solve it by chance with a 2.5% success rate.In contrast, our method still accomplishes the task with a success rate of 30%.</p>
<p>.3. Ablation on Data Augmentation</p>
<p>To evaluate the effectiveness of the data augmentation techniques, we perform an ablation study where our policy is trained with four levels of augmentation.As depicted in Ta-   ble 2, the policy performs better in both the simulation and real-world settings with increased data augmentation, and the policy trained on all four levels excels in all metrics.Interestingly, the policy manages to solve simpler settings more effectively even in simulation when more randomness is introduced in the training data.These experiments underscore the importance of simulator-based data augmentation.</p>
<p>.4. Ablation on Auto-Curriculum Learning</p>
<p>In this experiment, we evaluate the policy's effectiveness by testing it 200 times in simulations and conducting 20 realworld tests.As shown in Table 3, employing curriculum learning with auto-domain randomization solely based on the data generation rate yields inferior results compared to the approach based on model performance.</p>
<p>.5. Ablation on Ratio of Sim and Real Demos</p>
<p>To determine the optimal ratio between sim and real demos, we conducted tests using different combinations of sim and real demonstrations, as shown in Table 4.We observe that training solely on 50 real demonstrations results in poor performance, and the policy overfits to joint positions rather than utilizing the visual information in images.The best results were obtained with a combination of 15 simulation   demonstrations and 35 real demonstrations.These results highlight that collecting simulation data can be exceptionally valuable, even more so considering the significantly lower data collection costs.</p>
<p>. Discussion</p>
<p>We propose CyberDemo, a novel pipeline for imitation learning in robotic manipulation, leveraging demonstrations collected in simulation.While the common belief suggests that real-world demonstrations are the optimal way to solve real-world problems, we challenge this notion by demonstrating that extensive data augmentation can make simulation data even more valuable than real-world demonstrations, a fact also supported by our experiments.One limitation is the necessity to design a simulated environment for each real-world task, thereby increasing the human effort involved.However, since our method doesn't demand the design of specific rewards as in reinforcement learning tasks, which is often the most challenging aspect, the overall effort required is not as significant.</p>
<p>B .1. Human Demonstration Collection</p>
<p>The human play data is gathered through a teleoperation setup, where a human operator controls the system using a single real-sense camera in both the simulated and real environments.The entire trajectory is recorded at a rate of 30 frames per second, with each trajectory spanning approximately 20-30 seconds.</p>
<p>In the real-world setting, an additional real-sense camera is used to capture RGB images, which serve as the observations in the dataset.To ensure alignment between the simulated and real environments, we perform hand-eye calibration in the real world.This calibration process allows us to determine the relative position between the camera and the robot arm, enabling us to apply this transformation in the simulation.</p>
<p>B .2. Real World Setup</p>
<p>The system design for data collection is shown in Figure 6.As represented in the figure, the collection of human play data incorporates a human operator and a camera.The camera captures video footage at a frequency of 30 frames per second.Throughout the data collection process, the human operator interacts with the scene without any defined task objective.Instead, they interact freely with the environment, motivated by curiosity and the intent to observe intriguing behaviors.</p>
<p>In our experiments, human play data is collected by recording 30 seconds of uninterrupted interaction in each demonstration.This timeframe permits ample data to be</p>
<p>B .3. Policy Training</p>
<p>We use a conditional VAE [68] for training on 100 simulation demonstrations.The action chunking size was fixed at 50, in line with the methodology adopted in [85].Following the simulation data training, the model was fine-tuned with 15 real-world demonstrations, using a smaller learning rate and distinct batch norms for the real-world data.</p>
<p>Hyperparameters related to policy learning are displayed in Table 5, whereas Table 6 lists the hyperparameters pertinent to auto-curriculum learning.</p>
<p>To infuse diversity into the augmentation process, we have incorporated a randomness scale that ranges from 0 to 10 for each augmentation.In the context of auto-curriculum learning, this randomness scale progressively rises with a constant variance throughout each testing cycle.</p>
<p>In the course of auto-curriculum learning, the policy's performance is assessed across all four simulation levels, and the success rate is averaged.If the success rate falls below the success rate threshold, the increase in randomness scale is halted.This strategy aids in maintaining a balance between introducing randomness and ensuring the policy consistently accomplishes its tasks.</p>
<p>In summary, these hyperparameters and the evaluation procedure in auto-curriculum learning allow the policy to evolve and enhance over time, gradually escalating the randomness scale while preserving a satisfactory success rate.</p>
<p>B .4. Policy Testing</p>
<p>During the real-world testing phase, we perform both indomain and out-of-domain tests to evaluate the performance of the model.For out-of-domain tests, we significantly randomize the positions of objects, consciously choosing locations not included in the original data.This step guarantees that the model is examined in unfamiliar situations, evaluating its capacity to generalize and adjust to novel object arrangements.Moreover, to introduce visual disruptions and test the robustness of the model, we incorporate a disco light.The disco light generates visual disturbances and adds an extra layer of complexity to the test environment.This approach enables us to assess the model's resilience in dealing with unexpected visual inputs and its ability to sustain performance amidst such disruptions.</p>
<p>In the concluding stage, we evaluate the policy's ability to generalize across a range of objects, as illustrated in Figure 7.To carry out this generalizability test, we enhance the initial 100 simulation demonstrations by introducing 10 unique objects (adding 10 additional demonstrations for each object), and then re-run our pipeline.For the pick and place task in the real-world setting, we collected 15 demonstrations involving three different objects (with five demon-strations performed for each object within the red frame).For the rotating task, the real-world dataset includes only one object, identical to the original testing case.</p>
<p>By conducting these assessments and incorporating a variety of objects, we aim to evaluate the policy's adaptability and performance in diverse situations, ensuring its robustness and flexibility.A selection of demos is displayed in Figure 8.</p>
<p>B .5. Details of Simulation Evaluation Level designs</p>
<p>In the Pick &amp; Place and Pour tasks, we have defined different levels to introduce varying degrees of randomness:</p>
<p>• Level 1 signifies the original domain and encompasses slight randomization of the pose of the manipulated objects, including the end-effector pose and orientation.• Level 2 includes randomization of lighting and texture.</p>
<p>• Level 3 incorporates minimal randomization of the target objects (plate in pick place, bowl in pouring).• Level 4 escalates the randomness scale of both the manipulated and target objects.For Rotate task since there is only one object, things are different for the Rotate task: • Level 1 is set as randomizing the orientation of the objects, which is also the original domain.• Level 2 is the same as pick place and pouring tasks.• Level 3 is adding the randomization of the end-effector pose of the manipulated objects.• Level 4 increases the randomness scale of the manipulated objects.Below are the defined randomness parameters for each task, with all numbers listed in international units if without a statement.In our settings, the position (0,0) represents the center of the table.Level Design for Pick and Place • Random Manipulated Object Pose with a small scale:</p>
<p>-The x-coordinate of the Manipulated Object ranges from -0.1 to 0.</p>
<p>C . More Experimental Results</p>
<p>C .1. Comparision of Data Generation Method</p>
<p>To test our data augmentation approach's effectiveness, we pretrained using simulation data augmented by Mimic-Gen [44] and then fine-tuned with real-world teleoperation data, a sim2real transfer not included in the original Mimic-Gen framework.As shown in Figure 7, MimicGen adds an interpolated trajectory (in purple) to new object poses, potentially causing abrupt transitions.Our method, however, seamlessly integrates the entire sequence, resulting in more fluid motion.The imitation learning policy trained with our data thus outperforms others, as evidenced by the improved results in simulation and reality shown in the table.</p>
<p>C .2. Ablation on Action Aggregation</p>
<p>As illustrated in Figure 9, the use of action aggregation with Small Motion extends beyond its advantages in imitation learning within a single domain.It also functions as an effective instrument in closing the gap between simulated and real environments.As illustrated in Figure 10, the success  rate of the policy becomes more pronounced as the level of difficulty escalates.This suggests that action aggregation becomes increasingly beneficial for more challenging tasks.</p>
<p>C .3. Ablation on Novel Camera Views</p>
<p>As demonstrated in Figure 11, the application of random camera views augmentation improves the policy's robustness, particularly in situations involving camera view changes.In this method, all levels remain consistent while minor alterations to the camera view are incorporated.These changes involve a combined rotation along the yaxis and z-axis, plus a slight shift in the x, y, and z directions.The rotation Euler angle is sampled within the range of [−15, 15], enabling managed variation in the camera's alignment.Additionally, the translation is sampled within the range of [−0.05m, 0.05m], allowing minor adjustments in the camera's placement.</p>
<p>By integrating these alterations, the multi-view augmentation method introduces realistic variations in camera perspectives, thereby enhancing the policy's resilience to changes in the viewpoint.This strategy boosts the model's capability to adapt and perform efficiently, even when confronted with varied camera angles and positions.</p>
<p>C .4. Ablation on Kinematics Augmentation</p>
<p>We ablate the augmentation methods with or without sensitivity analysis (in contrast, simply relocating the endeffector to a new pose).We test both methods in an environment where object poses are extensively randomized, to verify the effectiveness of these two kinematic augmentation approaches.Figure 12 illustrates the success rate during training using datasets generated via these distinct augmentation strategies.Our findings demonstrate that, through the application of our proposed techniques, the model demonstrates consistent improvement over all three manipulation tasks.These outcomes underscore the efficacy of incorporating sensitivity analysis into pose data augmentation.</p>
<p>D . Derivation of Data Augmentation</p>
<p>In this section, we delve deeper into the derivation of the formula applied in the data augmentation of random object pose.We reproduce Equation 2 from the main paper and provide a detailed explanation:
ψ segj = ψ segj M j=1 ψ segj
, ∀seg j ∆T j = exp(ψ segj log(∆T )/K)
a new i = a i f i (∆T j )(3)
The first line of the equation normalizes the robustness score computed from Equation 1 in the main paper, ensuring that the sum of all scores equals 1.This parameter can be interpreted as a weight for each action chunk, symbolizing the proportion of modification each chunk should undertake to guide the robot to the new pose.</p>
<p>The second line calculates the relative pose modification for each step in chunk j.There are K steps in chunk j, and each step is allocated the same quantity of modification within the same chunk.Here, log() maps the SE(3) Lie Group to its se(3) Lie algebra, where exp is its inverse, mapping se(3) back to SE(3).</p>
<p>The third line of this equation computes the new action based on the pose modification.Here, f i is a similarity transformation in the SE(3) space that transitions the motion from the world frame to the current end-effector frame.We now provide a detailed derivation of f i .Since ∆T = T Onew W (T O old W ) −1 = T Onew O old , this relative pose change is a representation in the old object pose frame.To use this pose transformation to modify the action, we need to transform this relative pose into the frame corresponding to the action, which is the frame of the current end-effector pose.Considering the similarity transformation T A B X(T A B ) −1 , which transforms a SE(3) motion X represented in frame B to frame A, f i can be derived as
f i (∆T ) = T O old
Ri ∆T (T O old Ri ) −1 , where T Ri is the robot end-effector pose in frame i.</p>
<p>Figure 2 .
2
Figure 2. CyberDemo Pipeline.First, we collect both simulated and real demonstrations via vision-based teleoperation.Following this, we train the policy on simulated data, incorporating the proposed data augmentation techniques.During training, we apply automatic curriculum learning, which incrementally enhances the randomness scale based on task performance.Finally, the policy is fine-tuned with a few real demos before being deployed to the real world.</p>
<p>Figure 3 .
3
Figure 3. Data Augmentation.Our dataset augmentation encompasses four dimensions: (a) random camera views, (b) diverse objects, (c) random object pose, (d) random light and texture.</p>
<p>Algorithm 1 3 : 4 :
134
Auto Curriculum Learning Input: human demo D h , training set D, policy network π, curriculum level L, evaluation function eval L (), data aug.function aug L (), success rate threshold r up , number of failure N f ail , max number of failure N max Output: Trained policy π 1: Initialize π and set L = 0, N f ail = 0, D = {} 2: while L ≤ 4 do Generate augmented data aug L (D h ) Append into training set D ← D + aug L (D h )</p>
<p>B A ∈ SE( 3 )
3
as the pose of frame B relative to frame A. The original object pose is T O old W , the newly randomized object pose is T Onew W , and the original end effector pose is T R old W .The objective is to handle the object pose change T Onew W (T O old W ) −1 .A simple strategy can be first moving the robot end effector to a new initial pose, T Rnew W</p>
<p>Light Unseen Objects + Random Light + Out of Position</p>
<p>Figure 4 .
4
Figure 4. Generalization to Novel Objects for Pick and Place.We compare our approach with the baselines in scenarios involving novel objects, random light disturbances, and random object positions.</p>
<p>Light Unseen Objects + Random Light + Out of Position</p>
<p>Figure 5 .
5
Figure 5. Generalization to Novel Objects for Rotating.The experimental setup for this task mirrors that of the "Generalization to Novel Objects for Pick and Place" experiments.</p>
<p>Figure 6 .
6
Figure 6.Details of System Setups in Real World</p>
<p>1 .-Figure 7 .
17
Figure 7. Object Sets in Real World.The objects located within the red frame are allocated for training, while the remaining objects are set aside for testing on previously unseen objects.</p>
<p>Figure 8 .
8
Figure 8. Pick and Place Evaluation on diverse real-world objects.</p>
<p>Figure 11 .
11
Figure 11.Ablation on Multi-View Augmentaion</p>
<p>Figure 12 .
12
Figure 12.Augmentation Comparison We apply kinematic augmentation to one selected original demo, adapting it to a new position utilizing both the MimicGen method and ours.</p>
<p>Random Light &amp; Texture Diverse Objects Random Camera Views Original Demos Random Object Pose</p>
<p>Table 1 .
1
Main Comparison on Real Robot.In our study, we compare the performance across four distinct tasks: (a) Pick and Place
Pick and PlacePick and PlaceMustard BottleTomato Soup CanPouringRotating(Single Object)(Single Object)Level 1 Level 2 Level 3 Level 4 Level 1 Level 2 Level 3 Level 4 Level 1 Level 2 Level 3 Level 4 Level 1 Level 2 Level 3 Level 4R3M2 / 200 / 200 / 200 / 207 / 203 / 204 / 200 / 203 / 200 / 200 / 200 / 2011 / 202 / 206 / 202 / 20PVR4 / 200 / 200 / 200 / 204 / 200 / 203 / 200 / 202 / 200 / 201 / 200 / 208 / 203 / 205 / 201 / 20MVP2 / 200 / 203 / 201 / 207 / 202 / 204 / 202 / 201 / 201 / 203 / 202 / 208 / 204 / 2010 / 206 / 20Ours7 / 206 / 208 / 205 / 2014 / 20 11 / 20 13 / 20 13 / 209 / 204 / 2010 / 207 / 2015 / 20 10 / 20 17 / 20 13 / 20Bottle, (b) Pick and Place Can (exploring different grasping approaches), (c) Pouring (grasping a bottle and pouring its contents into abowl), and (d) Rotating the tri-valve. We perform evaluations of the models in four levels of real-world scenarios. These levels included:(a) Level 1: In Domain, (b) Level 2: Out of Position, (c) Level 3: Random Light, and (d) Level 4: Out of Position and Random Light.</p>
<p>Table 2 .
2
Ablation on Data Augmentation.To demonstrate the benefits of data augmentation, we employed auto-curriculum learning on various sets of levels.We performed 200 simulations to test its impact in a simulated environment and conducted 20 real-world tests to evaluate its effectiveness in a practical setting.
Test in SimTest in RealSet of Levels / Num of demos Level 1 Level 2 Level 3 Level 4 In Domain Random Light Out of PositionOut of Position + Random Light[1] / 10078%0%0%0%20%5%0%0%[1, 2] / 33073%75%10.5%7.5%15%25%0%0%[1,2,3] / 55058%66.5%43.5%21%15%15%5%15%[1,2,3,4] / 81092.5%81%63%49%35%30%30%40%</p>
<p>Table 3 .
3
Ablation on Auto-Curriculum Learning.We compare three different settings: (1) Auto Curriculum Learning based on the success rate.(2) Auto Curriculum Learning based on Data Generation Rate(the ratio of successfully generated trajectories to the total number of attempts).(3) Automatic Domain Randomization only based on Data Generation Rate.
Test in SimTest in RealDatasetLevel 1 Level 2 Level 3 Level 4 In Domain Out of Position50 sim73.5%80%63.5%36%0%0%35 sim + 15 real77%70.5%61%45.5%25%35%15 sim + 35 real63%74%55%33.5%50%15%50 real0%0%0%0%10%0%</p>
<p>Table 4 .
4
Ablation on Ratio of Sim and Real Demos.We compare the performance resulting from various quantities of simulated and real demonstrations, keeping the total number of demonstrations constant.</p>
<p>Table 5 .
5
Hyperparameters of Policy Network
HyperparameterDefaultBatch Size128Num of EpochsNoneFinetuning Epochs3000OptimizerAdamWLearning Rate (LR)1e-5Finetuning LR1e-6Weight Decay1e-2Evaluation Frequency100 epochsEncoder Layers4Decoder Layers7Heads8Feedforward Dimension 3200Hidden Dimension256Chunk Size50Dropout0.1Hyper ParametersDefaultTest Cycles300Evaluation Freq100 epochsRandomness Variance For Each Cycle 0.2Success Rate Threshold15%Data Generation Rate Threshold30%</p>
<p>Table 6 .
6
Hyperparameters for Auto Curriculum Learning</p>
<p>. AcknowledgementWe express our gratitude to Haichuan Che and Chengzhe Jia for their insightful conversations regarding the repair of the Allegro hand hardware.We are also thankful to Nicklas Hansen and Jiarui Xu for their valuable input on visual pretraining.Special thanks are due to Tony Zhao for his generous contribution to the community through his opensource projects on Action Chunking with Transformers.Supplementary MaterialA . OverviewThis supplementary document offers further information, results, and visualizations to complement the primary paper.Specifically, we encompass:B . Implementation detailsIn this section, we provide an overview of the data collection, training, and testing processes.
Solving rubik's cube with a robot hand. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob Mcgrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, arXiv:1910.07113201936arXiv preprint</p>
<p>Holo-dex: Teaching dexterity with immersive mixed reality. Irmak Sridhar Pandian Arunachalam, Güzey, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023Soumith Chintala, and Lerrel Pinto</p>
<p>Dexterous imitation made easy: A learningbased framework for efficient dexterous manipulation. Sneha Sridhar Pandian Arunachalam, Ben Silwal, Lerrel Evans, Pinto, 2023 ieee international conference on robotics and automation (icra). 2023</p>
<p>Playing hard exploration games watching youtube. Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, Nando De Freitas, Advances in neural information processing systems. 201831</p>
<p>Visual affordance prediction for guiding robot exploration. Homanga Bharadhwaj, Abhinav Gupta, Shubham Tulsiani, arXiv:2305.177832023arXiv preprint</p>
<p>Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar, arXiv:2309.0191820231arXiv preprint</p>
<p>Zero-shot robotic manipulation with pretrained imageediting diffusion models. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2310.106392023arXiv preprint</p>
<p>Unsupervised pixellevel domain adaptation with generative adversarial networks. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.0681720221arXiv preprint</p>
<p>Emerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Science Robotics. 88492442023</p>
<p>Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He, arXiv:2003.04297Improved baselines with momentum contrastive learning. 2020arXiv preprint</p>
<p>Learning robust real-world dexterous grasping policies via implicit shape augmentation. Zoey Chen, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, Dieter Fox, arXiv:2210.136382022arXiv preprint</p>
<p>Zoey Chen, Sho Kiami, arXiv:2302.06671Abhishek Gupta, and Vikash Kumar. Genaug: Retargeting behaviors to unseen situations via generative augmentation. 2023arXiv preprint</p>
<p>Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. Barret Ekin D Cubuk, Zoph, arXiv:1805.095011805. 22018arXiv preprint</p>
<p>Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, Dieter Fox, arXiv:2305.16309Imitating task and motion planning with visuomotor transformers. 2023arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.119292020arXiv preprint</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, Sergey Levine, arXiv:2109.133962021arXiv preprint</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Mario Franc ¸ois Laviolette, Victor Marchand, Lempitsky, The journal of machine learning research. 1712016</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202227</p>
<p>Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, arXiv:2302.04659A unified benchmark for generalizable manipulation skills. 2023arXiv preprint</p>
<p>Dextreme: Transfer of agile in-hand manipulation from simulation to reality. Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE202336</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202227</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, Yunfei Bai, 2021 IEEE International Conference on Robotics and Automation (ICRA). 202123</p>
<p>Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang, arXiv:2309.05655Dynamic handover: Throw and catch with bimanual hands. 2023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. 2022</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. 2022</p>
<p>Meta-sim: Learning to generate synthetic datasets. Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Sim2real transfer for reinforcement learning without dynamics randomization. Manuel Kaspar, Juan D Muñoz Osorio, Jürgen Bock, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Graph inverse reinforcement learning from diverse videos. Sateesh Kumar, Jonathan Zamora, Nicklas Hansen, Rishabh Jangir, Xiaolong Wang, PMLR, 2023. 3Conference on Robot Learning. </p>
<p>Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Conference on Robot Learning. PMLR2023</p>
<p>Learning active task-oriented exploration policies for bridging the simto-real gap. Jacky Liang, Saumya Saxena, Oliver Kroemer, arXiv:2006.019522020arXiv preprint</p>
<p>Interactive language: Talking to robots in real time. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence, IEEE Robotics and Automation Letters. 22023</p>
<p>Cacti: A framework for scalable multi-task multi-scene visual imitation learning. Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, Shuran Song, Aravind Rajeswaran, Vikash Kumar, arXiv:2212.057112022arXiv preprint</p>
<p>Dexvip: Learning dexterous grasping with human hand pose priors from video. Priyanka Mandikal, Kristen Grauman, PMLR, 2022. 3Conference on Robot Learning. </p>
<p>Roboturk: A crowdsourcing platform for robotic skill learning through imitation. Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, Conference on Robot Learning. 2018</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín, arXiv:2108.0329820211arXiv preprint</p>
<p>Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox, arXiv:2310.17596Mimicgen: A data generation system for scalable robot learning using human demonstrations. 2023215arXiv preprint</p>
<p>Adversarial skill networks: Unsupervised robot skill learning from video. Oier Mees, Markus Merklinger, Gabriel Kalweit, Wolfram Burgard, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>Translating videos to commands for robotic manipulation with deep recurrent neural networks. Anh Nguyen, Dimitrios Kanoulas, Luca Muratore, Darwin G Caldwell, Nikos G Tsagarakis, 2018 IEEE International Conference on Robotics and Automation (ICRA). 2018</p>
<p>Open xembodiment: Robotic learning datasets and rt-x models. Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, arXiv:2310.0886420231arXiv preprint</p>
<p>The unsurprising effectiveness of pretrained vision models for control. Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, Abhinav Gupta, International Conference on Machine Learning. 2022</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, Pieter Abbeel, IEEE international conference on robotics and automation (ICRA). 32018. 2018IEEE</p>
<p>In-hand object rotation via rapid motor adaptation. Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, Jitendra Malik, PMLR, 2023. 3Conference on Robot Learning. </p>
<p>General inhand object rotation with vision and touch. Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, Jitendra Malik, Conference on Robot Learning. PMLR2023</p>
<p>From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation. Yuzhe Qin, Hao Su, Xiaolong Wang, RA-L. 742022</p>
<p>Dexmv: Imitation learning for dexterous manipulation from human videos. Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, Xiaolong Wang, European Conference on Computer Vision. Springer2022</p>
<p>Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system. Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, Dietor Fox, arXiv:2307.045772023arXiv preprint</p>
<p>Rl-cyclegan: Reinforcement learning aware simulation-to-real. Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, Mohi Khansari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Imagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 11572015</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Reinforcement learning with videos: Combining offline observations with interaction. Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, Chelsea Finn, arXiv:2011.065072020arXiv preprint</p>
<p>Pretraining representations for data-efficient reinforcement learning. Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bachman, Aaron C Courville, Advances in Neural Information Processing Systems. 202134</p>
<p>Time-contrastive networks: Self-supervised learning from video. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, Google Brain, IEEE international conference on robotics and automation (ICRA). 22018. 2018IEEE</p>
<p>Rrl: Resnet as representation for reinforcement learning. Rutav Shah, Vikash Kumar, arXiv:2107.033802021arXiv preprint</p>
<p>Concept2robot: Learning manipulation concepts from instructions and human demonstrations. Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, Jeannette Bohg, The International Journal of Robotics Research. 4012-142021</p>
<p>Videodex: Learning dexterity from internet videos. Kenneth Shaw, Shikhar Bahl, Deepak Pathak, Conference on Robot Learning. 2023</p>
<p>A survey on image data augmentation for deep learning. Connor Shorten, M Taghi, Khoshgoftaar, Journal of big data. 612019</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. 2022</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Learning structured output representation using deep conditional generative models. Kihyuk Sohn, Honglak Lee, Xinchen Yan, Advances in neural information processing systems. 28132015</p>
<p>Languageconditioned imitation learning for robot manipulation tasks. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, Heni Ben Amor, Advances in Neural Information Processing Systems. 202033</p>
<p>Decoupling representation learning from reinforcement learning. Adam Stooke, Kimin Lee, Pieter Abbeel, Michael Laskin, International Conference on Machine Learning. 2021</p>
<p>Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Bidirectional domain adaptation for sim2real transfer of embodied navigation agents. Joanne Truong, Sonia Chernova, Dhruv Batra, IEEE Robotics and Automation Letters. 622021</p>
<p>A real2sim2real method for robust object grasping with neural surface reconstruction. Luobin Wang, Runlin Guo, Quan Vuong, Yuzhe Qin, Hao Su, Henrik Christensen, 2023 IEEE 19th International Conference on Automation Science and Engineering (CASE). IEEE2023</p>
<p>Sapien: A simulated part-based interactive environment. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Masked visual pre-training for motor control. Tete Xiao, Ilija Radosavovic, Trevor Darrell, Jitendra Malik, arXiv:2203.061732022arXiv preprint</p>
<p>Learning to see before learning to act: Visual pre-training for manipulation. Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, Tsung-Yi Lin, 2020 IEEE International Conference on Robotics and Automation (ICRA). 2020</p>
<p>Binghao Zhao-Heng Yin, Yuzhe Huang, Qifeng Qin, Xiaolong Chen, Wang, arXiv:2303.10880Rotating without seeing: Towards in-hand dexterity through touch. 2023arXiv preprint</p>
<p>Scaling robot learning with semantically imagined experience. Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, arXiv:2302.115502023arXiv preprint</p>
<p>Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Kang-Won Zhao-Heng Yin, Yi Lee, Soo-Chul Wu, Xiaolong Lim, Wang, arXiv:2312.01853Robot synesthesia: In-hand manipulation with visuotactile sensing. 2023arXiv preprint</p>
<p>Pre-trained image encoder for generalizable visual reinforcement learning. Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, Huazhe Xu, Advances in Neural Information Processing Systems. 202235</p>
<p>Deceptionnet: Network-driven domain randomization. Sergey Zakharov, Wadim Kehl, Slobodan Ilic, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Xirl: Crossembodiment inverse reinforcement learning. Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi, PMLR, 2022. 3Conference on Robot Learning. </p>
<p>Visual reinforcement learning with selfsupervised 3d representations. Yanjie Ze, Nicklas Hansen, Yinbo Chen, Mohit Jain, Xiaolong Wang, IEEE Robotics and Automation Letters. 852023</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. 2021</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. Tony Z Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn, arXiv:2304.137052023613arXiv preprint</p>
<p>Diffusion models for reinforcement learning: A survey. Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, Weinan Zhang, arXiv:2311.012232023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
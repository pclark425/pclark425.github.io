<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9226 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9226</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9226</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-277634322</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.06969v1.pdf" target="_blank">Towards LLMs Robustness to Changes in Prompt Format Styles</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have gained popularity in recent years for their utility in various applications. However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations. In the literature, this problem is commonly referred to as prompt brittleness. Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks. Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge. We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt few-shot examples. MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable. Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9226.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9226.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that presents each few-shot example in a distinct style and instructs the model to rewrite examples in different styles to reduce style-induced prompt brittleness and improve robustness across prompt-format variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B; 13b; 13b; 70b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple SuperNaturalInstructions tasks (16 selected datasets, e.g., task280, task1186, task905, task190, task1612, task320, ...)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A variety of NLP tasks drawn from SuperNaturalInstructions including text categorization (stereotype detection), textual entailment, text quality evaluation, toxic language detection, counting tasks, and stereotype detection.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompting where each prompt contains 5 few-shot examples; MOF variation: each of the 5 examples is presented in a distinct style and the prompt asks the model to rewrite each example in a different style. For evaluation, 10 MOF prompt variations were created (modified FormatSpread).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Traditional few-shot prompts (regular few-shot examples all in the same format) created using FormatSpread to generate 10 traditional prompt variations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>aggregate: MOF prompts perform comparable to or better than traditional prompts across most datasets and models; specific numeric values not fully reported in-text except where noted (see format_effect_size).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared across 10 prompt variations, MOF generally reduced the performance spread and often increased mean accuracy relative to traditional prompts; exceptions exist where traditional prompts performed better on some dataset/model pairs (see negative cases).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to 46% reduction in performance spread reported (task280 with Llama-2-13b). Other numeric effect sizes are not specified in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>MOF reduces prompt brittleness by exposing the model to diverse format styles so the model cannot associate spurious style features with the target variable; inspired by computer vision strategies that learn from style-diverse data to disentangle style and content (invariant learning). The rewriting task reinforces that examples vary in style.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each prompt used 5 few-shot examples. For each dataset the authors created 10 traditional prompt variations (FormatSpread) and 10 MOF prompt variations (modified FormatSpread). Experiments used 16 randomly selected SuperNaturalInstructions datasets, 1000 samples per dataset, batch size 100. Four LLMs were evaluated: falcon-11B, Llama-2-13b-hf, Llama-2-13b-chat-hf, and llama-3-70b-instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9226.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9226.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13b | task280</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-13b-hf evaluated on task280 (stereotype detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of MOF vs traditional few-shot prompting on a text categorization task that classifies sentences into stereotypes (gender, profession, race, religion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task280</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text categorization: classify sentences into four types of stereotypes (gender, profession, race, religion).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting (5-shot) with MOF style: each example in a distinct format and rewrite instruction; evaluated with 10 MOF prompt variations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Traditional few-shot prompting: 10 prompt variations generated by FormatSpread (all examples in consistent format).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>MOF achieved a reported 46% reduction in performance spread relative to traditional prompts on this dataset/model pairing (spread = max accuracy - min accuracy across 10 formats). Exact accuracy values are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>46% reduction in spread (MOF vs traditional) reported for task280 with Llama-2-13b.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Exposure to multiple styles prevents the model from relying on spurious format cues; rewriting examples in different styles helps the model generalize across format variations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>5 few-shot examples per prompt; 10 prompt variations per condition; evaluation measured accuracies across variations and computed spread (max - min accuracy) and mean accuracy. 1000 samples per dataset; batch size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9226.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9226.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama-3-70b | task190 (negative case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llama-3-70b-instruct evaluated on task190 (textual entailment) — MOF performs worse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported instance where MOF prompting did not improve performance: on textual entailment task190, the traditional prompt outperformed MOF for llama-3-70b-instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama-3-70b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task190</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Textual entailment: classifying whether two sentences agree, disagree, or neither.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting with MOF (distinct styles per example) and traditional few-shot prompts; evaluated across 10 prompt variations each.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Traditional few-shot prompts (FormatSpread generated variations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports that traditional prompts had better performance than MOF on this dataset/model pair; numeric accuracies and spreads are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that MOF fails on some datasets and that investigating these failures is important future work; no concrete hypothesis for this specific failure is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same general experimental setup: 5-shot prompts, 10 prompt variations per format type, 1000 samples per dataset, batch size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9226.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9226.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13b-chat | task1612 (negative case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-13b-chat-hf evaluated on task1612 (textual entailment variant) — MOF worse than traditional</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported case where MOF underperforms traditional few-shot prompting: on task1612 with Llama-2-13b-chat, MOF did not outperform the traditional prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task1612</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A textual entailment dataset derived from SICK, requiring classification of the relationship between two sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot MOF prompting (distinct styles per example) vs traditional few-shot prompts; 10 variations each.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Traditional few-shot prompts generated with FormatSpread.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports traditional prompts outperformed MOF for this dataset/model; numeric performance values are not included in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>No specific explanation provided for this dataset/model failure in-text; authors flag such failures as important future work.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>5few-shot examples; 10 prompt variations; evaluation computed max/min accuracies, spread, and mean accuracy over variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9226.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9226.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>falcon-11B | task320 (negative case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>falcon-11B evaluated on task320 (stereotype detection) — MOF underperforms in this case</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Example where MOF prompting did not yield better results: on task320 with falcon-11B the traditional prompt outperformed MOF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task320</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Stereotype detection: determine whether a given target pertaining to race in two sentences is a stereotype.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot MOF (distinct-style examples and rewrite instruction) vs traditional few-shot formats; 10 variations each.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Traditional few-shot prompts (FormatSpread variations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports traditional prompts performed better than MOF on this dataset/model pair; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>No dataset-specific explanation in-text; authors suggest investigating MOF failures as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same evaluation protocol: 5-shot prompts, 10 prompt variations, 1000 samples, batch size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9226.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9226.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FormatSpread</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FormatSpread (prompt-format variation generator / evaluation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method / tool used to generate multiple prompt format variations and to quantify format-induced performance spread by measuring difference between best and worst performing prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple LLMs used in study (see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>prompt-format evaluation / generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generating multiple prompt-format variants for few-shot evaluation and computing performance spread across them.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>FormatSpread produces diverse prompt templates/formats (e.g., 'Passage:: {} , Answer:: {}', 'SYSTEM REFERENCE : {}. ORIGINAL REFERENCE : {}. ANSWER : {}', 'Tweet:{} , Label:{} , Answer:{}') used for creating the 10 traditional prompt variations; the authors modified FormatSpread to create MOF variations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>FormatSpread is used as an evaluation mechanism to quantify prompt brittleness by computing spread (max accuracy - min accuracy) across format variations; it enables identifying how format changes affect model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used to create 10 traditional prompt variations per dataset. The authors modified FormatSpread to incorporate diverse per-example styles to produce 10 MOF prompt variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. <em>(Rating: 2)</em></li>
                <li>Ask me anything: A simple strategy for prompting language models. <em>(Rating: 2)</em></li>
                <li>Template Ensembles <em>(Rating: 1)</em></li>
                <li>Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks <em>(Rating: 2)</em></li>
                <li>Simple disentanglement of style and content in visual representations <em>(Rating: 1)</em></li>
                <li>Invariant risk minimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9226",
    "paper_id": "paper-277634322",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "MOF",
            "name_full": "Mixture of Formats",
            "brief_description": "A prompting technique that presents each few-shot example in a distinct style and instructs the model to rewrite examples in different styles to reduce style-induced prompt brittleness and improve robustness across prompt-format variations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct",
            "model_size": "11B; 13b; 13b; 70b",
            "task_name": "Multiple SuperNaturalInstructions tasks (16 selected datasets, e.g., task280, task1186, task905, task190, task1612, task320, ...)",
            "task_description": "A variety of NLP tasks drawn from SuperNaturalInstructions including text categorization (stereotype detection), textual entailment, text quality evaluation, toxic language detection, counting tasks, and stereotype detection.",
            "presentation_format": "Few-shot in-context prompting where each prompt contains 5 few-shot examples; MOF variation: each of the 5 examples is presented in a distinct style and the prompt asks the model to rewrite each example in a different style. For evaluation, 10 MOF prompt variations were created (modified FormatSpread).",
            "comparison_format": "Traditional few-shot prompts (regular few-shot examples all in the same format) created using FormatSpread to generate 10 traditional prompt variations.",
            "performance": "aggregate: MOF prompts perform comparable to or better than traditional prompts across most datasets and models; specific numeric values not fully reported in-text except where noted (see format_effect_size).",
            "performance_comparison": "Compared across 10 prompt variations, MOF generally reduced the performance spread and often increased mean accuracy relative to traditional prompts; exceptions exist where traditional prompts performed better on some dataset/model pairs (see negative cases).",
            "format_effect_size": "Up to 46% reduction in performance spread reported (task280 with Llama-2-13b). Other numeric effect sizes are not specified in the text.",
            "explanation_or_hypothesis": "MOF reduces prompt brittleness by exposing the model to diverse format styles so the model cannot associate spurious style features with the target variable; inspired by computer vision strategies that learn from style-diverse data to disentangle style and content (invariant learning). The rewriting task reinforces that examples vary in style.",
            "null_or_negative_result": true,
            "experimental_details": "Each prompt used 5 few-shot examples. For each dataset the authors created 10 traditional prompt variations (FormatSpread) and 10 MOF prompt variations (modified FormatSpread). Experiments used 16 randomly selected SuperNaturalInstructions datasets, 1000 samples per dataset, batch size 100. Four LLMs were evaluated: falcon-11B, Llama-2-13b-hf, Llama-2-13b-chat-hf, and llama-3-70b-instruct.",
            "uuid": "e9226.0",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-2-13b | task280",
            "name_full": "Llama-2-13b-hf evaluated on task280 (stereotype detection)",
            "brief_description": "Evaluation of MOF vs traditional few-shot prompting on a text categorization task that classifies sentences into stereotypes (gender, profession, race, religion).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13b-hf",
            "model_size": "13b",
            "task_name": "task280",
            "task_description": "Text categorization: classify sentences into four types of stereotypes (gender, profession, race, religion).",
            "presentation_format": "Few-shot prompting (5-shot) with MOF style: each example in a distinct format and rewrite instruction; evaluated with 10 MOF prompt variations.",
            "comparison_format": "Traditional few-shot prompting: 10 prompt variations generated by FormatSpread (all examples in consistent format).",
            "performance": null,
            "performance_comparison": "MOF achieved a reported 46% reduction in performance spread relative to traditional prompts on this dataset/model pairing (spread = max accuracy - min accuracy across 10 formats). Exact accuracy values are not provided in-text.",
            "format_effect_size": "46% reduction in spread (MOF vs traditional) reported for task280 with Llama-2-13b.",
            "explanation_or_hypothesis": "Exposure to multiple styles prevents the model from relying on spurious format cues; rewriting examples in different styles helps the model generalize across format variations.",
            "null_or_negative_result": false,
            "experimental_details": "5 few-shot examples per prompt; 10 prompt variations per condition; evaluation measured accuracies across variations and computed spread (max - min accuracy) and mean accuracy. 1000 samples per dataset; batch size 100.",
            "uuid": "e9226.1",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "llama-3-70b | task190 (negative case)",
            "name_full": "llama-3-70b-instruct evaluated on task190 (textual entailment) — MOF performs worse",
            "brief_description": "Reported instance where MOF prompting did not improve performance: on textual entailment task190, the traditional prompt outperformed MOF for llama-3-70b-instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "llama-3-70b-instruct",
            "model_size": "70b",
            "task_name": "task190",
            "task_description": "Textual entailment: classifying whether two sentences agree, disagree, or neither.",
            "presentation_format": "Few-shot prompting with MOF (distinct styles per example) and traditional few-shot prompts; evaluated across 10 prompt variations each.",
            "comparison_format": "Traditional few-shot prompts (FormatSpread generated variations).",
            "performance": null,
            "performance_comparison": "Paper reports that traditional prompts had better performance than MOF on this dataset/model pair; numeric accuracies and spreads are not provided in-text.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors note that MOF fails on some datasets and that investigating these failures is important future work; no concrete hypothesis for this specific failure is provided.",
            "null_or_negative_result": true,
            "experimental_details": "Same general experimental setup: 5-shot prompts, 10 prompt variations per format type, 1000 samples per dataset, batch size 100.",
            "uuid": "e9226.2",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama-2-13b-chat | task1612 (negative case)",
            "name_full": "Llama-2-13b-chat-hf evaluated on task1612 (textual entailment variant) — MOF worse than traditional",
            "brief_description": "Reported case where MOF underperforms traditional few-shot prompting: on task1612 with Llama-2-13b-chat, MOF did not outperform the traditional prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13b-chat-hf",
            "model_size": "13b",
            "task_name": "task1612",
            "task_description": "A textual entailment dataset derived from SICK, requiring classification of the relationship between two sentences.",
            "presentation_format": "Few-shot MOF prompting (distinct styles per example) vs traditional few-shot prompts; 10 variations each.",
            "comparison_format": "Traditional few-shot prompts generated with FormatSpread.",
            "performance": null,
            "performance_comparison": "Paper reports traditional prompts outperformed MOF for this dataset/model; numeric performance values are not included in the text.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "No specific explanation provided for this dataset/model failure in-text; authors flag such failures as important future work.",
            "null_or_negative_result": true,
            "experimental_details": "5few-shot examples; 10 prompt variations; evaluation computed max/min accuracies, spread, and mean accuracy over variations.",
            "uuid": "e9226.3",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "falcon-11B | task320 (negative case)",
            "name_full": "falcon-11B evaluated on task320 (stereotype detection) — MOF underperforms in this case",
            "brief_description": "Example where MOF prompting did not yield better results: on task320 with falcon-11B the traditional prompt outperformed MOF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "falcon-11B",
            "model_size": "11B",
            "task_name": "task320",
            "task_description": "Stereotype detection: determine whether a given target pertaining to race in two sentences is a stereotype.",
            "presentation_format": "Few-shot MOF (distinct-style examples and rewrite instruction) vs traditional few-shot formats; 10 variations each.",
            "comparison_format": "Traditional few-shot prompts (FormatSpread variations).",
            "performance": null,
            "performance_comparison": "Paper reports traditional prompts performed better than MOF on this dataset/model pair; no numeric metrics provided.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "No dataset-specific explanation in-text; authors suggest investigating MOF failures as future work.",
            "null_or_negative_result": true,
            "experimental_details": "Same evaluation protocol: 5-shot prompts, 10 prompt variations, 1000 samples, batch size 100.",
            "uuid": "e9226.4",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FormatSpread",
            "name_full": "FormatSpread (prompt-format variation generator / evaluation protocol)",
            "brief_description": "A method / tool used to generate multiple prompt format variations and to quantify format-induced performance spread by measuring difference between best and worst performing prompt formats.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "multiple LLMs used in study (see paper)",
            "model_size": null,
            "task_name": "prompt-format evaluation / generation",
            "task_description": "Generating multiple prompt-format variants for few-shot evaluation and computing performance spread across them.",
            "presentation_format": "FormatSpread produces diverse prompt templates/formats (e.g., 'Passage:: {} , Answer:: {}', 'SYSTEM REFERENCE : {}. ORIGINAL REFERENCE : {}. ANSWER : {}', 'Tweet:{} , Label:{} , Answer:{}') used for creating the 10 traditional prompt variations; the authors modified FormatSpread to create MOF variations.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "FormatSpread is used as an evaluation mechanism to quantify prompt brittleness by computing spread (max accuracy - min accuracy) across format variations; it enables identifying how format changes affect model outputs.",
            "null_or_negative_result": null,
            "experimental_details": "Used to create 10 traditional prompt variations per dataset. The authors modified FormatSpread to incorporate diverse per-example styles to produce 10 MOF prompt variations.",
            "uuid": "e9226.5",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Ask me anything: A simple strategy for prompting language models.",
            "rating": 2,
            "sanitized_title": "ask_me_anything_a_simple_strategy_for_prompting_language_models"
        },
        {
            "paper_title": "Template Ensembles",
            "rating": 1,
            "sanitized_title": "template_ensembles"
        },
        {
            "paper_title": "Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks",
            "rating": 2,
            "sanitized_title": "supernaturalinstructionsgeneralization_via_declarative_instructions_on_1600_tasks"
        },
        {
            "paper_title": "Simple disentanglement of style and content in visual representations",
            "rating": 1,
            "sanitized_title": "simple_disentanglement_of_style_and_content_in_visual_representations"
        },
        {
            "paper_title": "Invariant risk minimization",
            "rating": 1,
            "sanitized_title": "invariant_risk_minimization"
        }
    ],
    "cost": 0.011826,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards LLMs Robustness to Changes in Prompt Format Styles
9 Apr 2025</p>
<p>Lilian Ngweta ngwetl@rpi.edu 
Rensselaer Polytechnic Institute</p>
<p>Kiran Kate 
IBM Research</p>
<p>Jason Tsay 
IBM Research</p>
<p>Yara Rizk 
IBM Research</p>
<p>Towards LLMs Robustness to Changes in Prompt Format Styles
9 Apr 202568AFF47AE1A2DD639148D7D5E969C65BarXiv:2504.06969v1[cs.CL]
Large language models (LLMs) have gained popularity in recent years for their utility in various applications.However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations.In the literature, this problem is commonly referred to as prompt brittleness.Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks.Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge.We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt fewshot examples.MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable.Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are useful for many applications and tasks i.e., content generation, translation, text analysis, etc.One of the popular techniques for adapting pre-trained LLMs to specific tasks that has emerged in recent years is prompt engineering (Liu et al., 2023;Tonmoy et al., 2024;Chen et al., 2023).Prompt engineering involves carefully crafting task-specific instructions and a few input-output demonstrations (prompts) to guide LLMs without changing their parameters (Sahoo et al., 2024).The popularity of prompt engineering can be attributed to the fact that it does not require labeled data and only needs a few demonstrations in prompts containing few-shot examples (Liu et al., 2023).Prompting is also generally computationally cheaper than supervised fine-tuning techniques since the model parameters are not modified (Sahoo et al., 2024).</p>
<p>Existing prompting techniques include zero-shot prompting (Radford et al., 2019), few-shot prompting (Brown et al., 2020), chain-of-thought (CoT) prompting (Wei et al., 2022), and automatic chainof-thought (Auto-CoT) prompting (Zhang et al., 2023).Most research on prompting techniques has focused on identifying or designing good prompts for specific tasks (Zhou et al., 2023b;Wan et al., 2023).However, a key problem often overlooked by these techniques is the sensitivity of LLMs to meaning-preserving changes in prompts.Examples of such changes include adding extra spaces, replacing two colons with one, changing the order of few-shot examples, or varying the choice of fewshot examples (He et al., 2024;Sclar et al., 2024;Lu et al., 2022;Wan et al., 2023).This problem is sometimes referred to as prompt brittleness (Zhou et al., 2023a).Prompt brittleness contributes to LLMs being unreliable and prevents their adoption in high-risk domains such as healthcare.</p>
<p>In this work, we focus on style-induced prompt brittleness as illustrated in Figure 1, and propose Mixture of Formats (MOF) to address it.MOF is a simple and computationally efficient prompting technique where each few-shot example in the prompt is presented in a distinct style.Furthermore, the model is instructed to rewrite each example using a different style, as shown in Figure 2. MOF was inspired by ideas from computer vision that involve learning from datasets with diverse styles to prevent models from associating styles with the target variable (Arjovsky et al., 2019;Kamath et al., 2021;Yin et al., 2021;Wald et al., 2021;Ngweta et al., 2023;Li et al., 2021).We evaluate the effectiveness of MOF prompting using datasets from var-ious tasks within SuperNaturalInstructions (Wang et al., 2022), comparing its performance against traditional prompts.Our experiments focus on fewshot prompting, where a traditional prompt refers to a regular few-shot prompt, and a MOF prompt is a few-shot prompt that has been converted into the MOF style, as demonstrated in Figure 2.</p>
<p>Related work</p>
<p>Traditional prompt engineering techniques.Several prompt engineering techniques have been proposed in recent years.Zero-shot prompting is a technique in which a prompt contains a description of the task and no training data is required (Radford et al., 2019).Unlike zero-shot prompting, few-shot prompting adds a few input-output demonstrations to the prompt to further help the model understand the task (Brown et al., 2020).Both zero-shot and few-shot prompting techniques enable the application of LLMs on new tasks without extensive training (Sahoo et al., 2024).For reasoning and logic tasks, prompting techniques that have been proposed include chain-of-thought (CoT) (Wei et al., 2022) and automatic chain-of-thought (Auto-CoT) (Zhang et al., 2023).CoT is a prompting technique that encourages LLMs to do step-by-step reasoning (Wei et al., 2022).Since manually creating CoT examples is time-consuming and not easily scalable, Zhang et al. (2023) proposed Auto-CoT to automatically guide LLMs to generate reasoning steps using a "Let's think step by step" statement in the prompt.</p>
<p>These traditional prompting techniques can be adapted to the MOF format by applying differ-ent formatting styles to each prompt example, as demonstrated in Figure 2. In this paper, we focus on the application of MOF to few-shot prompting.</p>
<p>Optimizing for the best prompt.This line of work focuses on optimizing and identifying the most effective prompt for a given task.Zhou et al. (2023b) propose the automatic prompt engineer (APE), an approach that enables the generation and selection of prompt instructions automatically.APE involves analyzing input queries, generating candidate prompt instructions, and then using reinforcement learning to select the best prompt (Zhou et al., 2023b).Similarly, Wan et al. (2023) propose a method where an LLM generates zero-shot outputs for given inputs, followed by selecting high-quality few-shot examples to construct an improved prompt, focusing on consistency, diversity, and repetition.Since automatic prompt optimization (APO) methods focus on optimizing instruction or optimizing few-shot examples, Wan et al. (2024) propose a technique to optimize for both, and compare its performance with the performance of techniques that only optimize instructions or examples.Yang et al. (2024) present Optimization by PROmpting (OPRO), a method that leverages LLMs as optimizers by describing the optimization task in natural language (Yang et al., 2024).Pryzant et al. (2023) propose Prompt Optimization with Textual Gradients (ProTeGi), which employs text gradients guided by beam search and bandit selection techniques for automatic prompt optimization (Pryzant et al., 2023).Additionally, Khattab et al. (2024) introduce DSPy, a framework that replaces hard-coded prompt templates with a systematic approach for building language model pipelines.Other methods for identifying optimal prompts include (Feffer et al., 2024;Sorensen et al., 2022;Yin et al., 2023).</p>
<p>Unlike existing methods in this area that repeatedly search for optimal prompts per task and model, our goal is to reduce style-induced prompt brittleness using an efficient and straightforward recipe illustrated in Figure 2.</p>
<p>Quantifying prompt brittleness in LLMs.Several works have shown that LLMs are sensitive to changes in prompt formats (Sclar et al., 2024;He et al., 2024;Voronov et al., 2024) and to the order of few-shot examples in the prompt (Lu et al., 2022).Sclar et al. (2024) propose FormatSpread, a method to efficiently measure performance variations in LLMs caused by prompt format changes, ANSWER : {} for dataset task1186, and Tweet:{} , Label:{} , Answer:{} for dataset task905.These formats are generated using FormatSpread (Sclar et al., 2024), as described in Section 3.1.The datasets used are described in Table 3.</p>
<p>by computing the performance difference (spread) between the best-performing format and the worstperforming format.Due to the sensitivity of LLMs to prompt format variations, Polo et al. (2024) propose PromptEval, an efficient method for evaluating LLMs on multiple prompts instead of a single prompt.Similarly, Mizrahi et al. (2024) propose metrics for multi-prompt evaluation of LLMs.</p>
<p>While these approaches are valuable tools for quantifying prompt brittleness, our proposed method focuses on mitigating it, particularly the brittleness arising from style variations in prompt formats.</p>
<p>Prompt ensembles.Arora et al. (2022) introduce Ask Me Anything (AMA), a prompting approach that transforms inputs into a question-answering format to encourage open-ended responses.AMA generates multiple imperfect prompts and combines the responses using a weak supervision strategy to produce the final output (Arora et al., 2022).Similarly, Voronov et al. (2024) propose Template Ensembles, an approach that aggregates model predictions across multiple prompt templates.However, both methods are computationally expensive, as they require aggregating predictions from multiple prompts.Furthermore, unlike our proposed method, they do not specifically address prompt brittleness caused by style variations in prompt formats.</p>
<p>Mixture of Formats</p>
<p>Style-induced prompt brittleness in LLMs is similar to problems observed in computer vision, where small changes to an image's style (eg.color or background) can affect the model's ability to make accurate predictions (Nagarajan et al., 2020).In computer vision, various approaches have been developed to address this issue, often involving learning from diverse datasets (Arjovsky et al., 2019;Ngweta et al., 2023;Kamath et al., 2021;Yin et al., 2021;Wald et al., 2021;Li et al., 2021).The underlying idea is that exposure to diverse data points helps the model disassociate styles from the target variable.Drawing inspiration from these techniques, we propose Mixture of Formats (MOF), a novel prompting strategy that deviates from traditional ways of crafting prompts by employing a distinct style format for each few-shot example in the prompt.To further reinforce model understanding, we have the model rewrite the question and answer of each example using a different format style, as illustrated in Figure 2. The effectiveness of this approach is evaluated in the subsequent subsections.</p>
<p>Experiments</p>
<p>Let X denote input queries for a task, and Y denote the target variable.Given N observations of inputs X and their corresponding targets Y as data D = {X n , Y n } N n=1 , we automatically build a traditional prompt and its MOF prompt version, each containing 5 few-shot examples, and use them for inference with an LLM.The traditional prompt is created using FormatSpread (Sclar et al., 2024), while the MOF prompt is generated by modifying FormatSpread to incorporate diverse formats within the few-shot examples, as illustrated in Figure 2.</p>
<p>Using FormatSpread, we create 10 traditional Spread is a metric for quantifying style-induced prompt brittleness and it is obtained by taking the difference between the best performing prompt (maximum accuracy) and the worst performing prompt (minimum accuracy).MOF prompts perform comparably or outperform traditional prompts in most datasets and in some datasets, traditional prompts have better performance.</p>
<p>prompt variations and 10 MOF prompt variations.</p>
<p>From the 10 prompt variations, for both traditional and MOF prompts, we compute performance accuracies for each prompt format across various tasks.</p>
<p>The goal is to compare the style-induced prompt brittleness between traditional prompts and MOF prompts.As in Sclar et al. (2024), we measure brittleness by calculating the performance spread, defined as the accuracy difference between the bestperforming and worst-performing prompt formats.The evaluation pipelines for traditional and MOF prompts are summarized in Algorithm 1 and Algorithm 2, respectively.</p>
<p>Datasets</p>
<p>We perform experiments on datasets covering various tasks from SuperNaturalInstructions (Mishra et al., 2022;Wang et al., 2022).Due to limited computational resources, we randomly selected 16 datasets and for each dataset we use 1000 samples and a batch size of 100.The datasets used are described in Table 3.</p>
<p>Baselines, metrics, and LLMs used In our experiments, we use traditional few-shot prompts as our baselines, where we compare the performance of LLMs when using traditional prompts versus MOF prompts.A primary focus of this work is to determine whether MOF prompting can minimize performance variations (spread) in LLMs when prompt format styles change.The performance spread is obtained by taking the difference between the highest performing prompt (denoted as "Max Accuracy" in the results tables) and the minimum performing prompt (denoted as "Min Accuracy").The spread value ranges from 0.0 to 1.0, where values closer to 0.0 indicate that the LLM is more robust and less sensitive to style changes, while values closer to 1.0 suggest that the LLM is highly sensitive to these changes.Additionally, for both traditional and MOF prompts, we compute the average accuracy across all 10 prompt variations to assess the overall performance of MOF prompts relative to traditional prompts.We use four LLMs in our experiments: falcon-11B, Llama-2-13b-hf, Llama-2-13b-chat-hf, and llama-3-70b-instruct.</p>
<p>We emphasize that while MOF prompting can be applied and compared with other existing traditional prompting techniques, such as automatic 1: Best performing format (Max Accuracy) and worst performing format (Min Accuracy) results for both traditional prompts and MOF prompts for llama-3-70b-instruct. MOF prompts improve the Min Accuracy and the Max Accuracy over traditional prompts in most cases.</p>
<p>Task</p>
<p>Traditional and the automatic prompt engineer (APE) (Zhou et al., 2023b), this paper focuses on applying MOF prompting to regular few-shot prompting and comparing their performances, due to limited computational resources.</p>
<p>Generating responses for evaluation To generate a response for a given question, a traditional or MOF prompt is combined with the question and then passed to an LLM to generate the response.The generated response is then compared to the ground-truth answer to calculate the model's accuracy.</p>
<p>Results</p>
<p>We perform experiments to evaluate whether MOF prompts reduce prompt brittleness in LLMs by comparing their spread with traditional prompts.We also assess improvements by analyzing the best (Max Accuracy) and worst (Min Accuracy) performing prompts.Finally, we evaluate overall performance by comparing the mean accuracies across all 10 prompt variations for both prompt types.</p>
<p>Minimizing prompt brittleness Figure 3 shows that MOF prompting effectively reduces style-induced prompt brittleness across several datasets and LLMs, with a notable 46% reduction in task280 using Llama-2-13b.While MOF prompts generally perform as well or better than traditional prompts, exceptions occur in task190 (llama-3-70b-instruct), task1612 (llama-2-13b-chat), and task320 (falcon-11B), where traditional prompts perform better.Investigating why MOF fails on these datasets is an important future direction.</p>
<p>Best and worst performing prompts Results for the best-performing prompt (Max Accuracy) and worst-performing prompt (Min Accuracy) for both traditional and MOF prompting are reported in Table 1.We observe that MOF prompting not only reduces spread but also improves both minimum and maximum accuracies.Average accuracy results across all 10 prompt variations for both traditional and MOF prompts are discussed in Appendix A.</p>
<p>Conclusion and future work</p>
<p>Addressing prompt brittleness remains a challenge, particularly when caused by changes in prompt format styles.In this work, we introduce a simple and efficient prompting technique, MOF, and evaluate its effectiveness in addressing style-induced prompt brittleness.The preliminary results are promising, with significant improvements over traditional prompting in many datasets, as shown in Figure 3. Future directions include integrating MOF with techniques like chain-of-thought (CoT) and automatic prompt engineer (APE), comparing its performance with methods that aggregate results from multiple prompts such as AMA (Arora et al., 2022) and Template Ensembles (Voronov et al., 2024), and conducting experiments with larger LLMs like GPT-4, Claude 3.5 Sonnet, Falcon 40B, and Llama 3.1 405B.Additionally, analyzing MOF's failures on certain datasets is a crucial area for further exploration.</p>
<p>We hope this work will inspire further research into addressing prompt brittleness in LLMs, and the code for this project is publicly available on GitHub. 1 1 Code: github.com/lilianngweta/mof.</p>
<p>2: Average accuracy results across 10 prompt variations for traditional prompts (denoted as Trad Mean Acc) and MOF prompts (denoted as MOF Mean Acc).For all LLMs, MOF prompts perform comparable and in most cases have a higher overall average accuracy than traditional prompts.task1347 A text matching dataset that involves classifying the semantic similarity of two sentences on a scale of 0 -5.</p>
<p>task1612</p>
<p>A textual entailment dataset derived from the SICK dataset, that involves accurately classifying labels to show the relationship between two sentences.task1502 A toxic language detection dataset that involves classifying the type of tweet in HateXplain.</p>
<p>task161</p>
<p>A dataset focused on counting the words in a sentence that contain a specified letter.</p>
<p>task158</p>
<p>A dataset that involves counting the number of times a word occurs in a sentence.</p>
<p>task1186 A text quality evaluation dataset that involves evaluating the naturalness of system generated reference.task190 A textual entailment dataset that involves choosing whether two given sentences agree, disagree, or neither with each other.</p>
<p>task1284 A text quality evaluation dataset that involves evaluating the informativeness of system generated reference.</p>
<p>task607</p>
<p>A toxic language detection that involves determining whether or not the post is intentionally offensive.</p>
<p>task163</p>
<p>A dataset that involves counting the number of words in the sentence that end with a specified letter.</p>
<p>task905</p>
<p>A toxic language detection dataset that involves determining whether the given category of a tweet is true or false.task320 A stereotype detection dataset that involves determining whether a given target pertaining to race in two sentences is a stereotype.</p>
<p>task316</p>
<p>A stereotype detection dataset that involves classifying whether a sentence is stereotype or anti-stereotype.</p>
<p>task162</p>
<p>A dataset that involves counting the words in a sentence that begin with a specified letter.</p>
<p>Figure 1 :
1
Figure 1: A demonstration of how small changes to the prompt format style can sometimes lead to incorrect predictions in LLMs.</p>
<p>Figure 2 :
2
Figure 2: An illustration of how to convert a traditional prompt into a MOF prompt.This example serves as a simple demonstration of the conversion process.In the actual experiments, datasets use various formats such as Passage:: {} , Answer:: {} for dataset task280, SYSTEM REFERENCE : {}.ORIGINAL REFERENCE : {}.ANSWER : {} for dataset task1186, and Tweet:{} , Label:{} , Answer:{} for dataset task905.These formats are generated using FormatSpread(Sclar et al., 2024), as described in Section 3.1.The datasets used are described in Table3.</p>
<p>Figure 3 :
3
Figure3: Comparing the performance spread of traditional prompts and MOF prompts.Spread is a metric for quantifying style-induced prompt brittleness and it is obtained by taking the difference between the best performing prompt (maximum accuracy) and the worst performing prompt (minimum accuracy).MOF prompts perform comparably or outperform traditional prompts in most datasets and in some datasets, traditional prompts have better performance.</p>
<p>Table 3 :
3
(Mishra et al., 2022;Wang et al., 2022)Mishra et al., 2022;Wang et al., 2022)that we used in our experiments.
Dataset IDDataset Descriptiontask280A text categorization dataset that involves classifying sentences into four types of stereotypes: gender, profession, race, and religion.task317A stereotype detection dataset that involves classifying sentences into various types of stereotypes.</p>
<p>Martin Arjovsky, Léon Bottou, Ishaan , David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>Ask me anything: A simple strategy for prompting language models. Simran Arora, Avanika Narayan, Laurel Mayee F Chen, Neel Orr, Kush Guha, Ines Bhatia, Christopher Chami, Re, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '20Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; Red Hook, NY, USACurran Associates Inc2020Language models are few-shot learners</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, Shengxin Zhu, arXiv:2310.147352023arXiv preprint</p>
<p>Prompt exploration with prompt regression. Michael Feffer, Ronald Xu, Yuekai Sun, Mikhail Yurochkin, arXiv:2405.110832024arXiv preprint</p>
<p>Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, Sadid Hasan, arXiv:2411.10541Does prompt formatting have any impact on llm performance? arXiv preprint. 2024</p>
<p>Does invariant risk minimization capture invariance?. Pritish Kamath, Akilesh Tangella, Danica Sutherland, Nathan Srebro, International Conference on Artificial Intelligence and Statistics. PMLR2021</p>
<p>DSPy: Compiling declarative language model calls into stateof-the-art pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, A , Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Emmanuel Candès, and Chiara Sabatti. Shuangning Li, Matteo Sesia, Yaniv Romano, arXiv:2106.04118Searching for consistent associations with a multi-environment knockoff filter. 2021arXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ACL. 2022</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Understanding the failure modes of out-of-distribution generalization. Anders Vaishnavh Nagarajan, Behnam Andreassen, Neyshabur, arXiv:2010.157752020arXiv preprint</p>
<p>Simple disentanglement of style and content in visual representations. Lilian Ngweta, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin, International Conference on Machine Learning. PMLR2023</p>
<p>Felipe Maia Polo, Ronald Xu, Lucas Weber, Mírian Silva, Onkar Bhardwaj, Leshem Choshen, arXiv:2405.17202Efficient multi-prompt evaluation of llms. Yuekai Oliveira, Mikhail Sun, Yurochkin, 2024arXiv preprint</p>
<p>Automatic prompt optimization with "gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, </p>
<p>arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, The Twelfth International Conference on Learning Representations. 2024</p>
<p>An information-theoretic approach to prompt without ground truth labels. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, David Wingate, 10.18653/v1/2022.acl-long.60Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsPapers; Dublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, arXiv:2401.067662024arXiv preprint</p>
<p>On calibration and out-of-domain generalization. Yoav Wald, Amir Feder, Daniel Greenfeld, Uri Shalit, Advances in Neural Information Processing Systems. 202134</p>
<p>Better zero-shot reasoning with self-adaptive prompting. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, Tomas Pfister, 10.18653/v1/2023.findings-acl.216Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Teach better or show smarter? on instructions and exemplars in automatic prompt optimization. Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O Arik, arXiv:2406.157082024arXiv preprint</p>
<p>Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, EMNLP. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, arXiv:2309.034092024Preprint</p>
<p>Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu, 10.18653/v1/2023.acl-long.172Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Optimization-based causal estimation from heterogenous environments. Mingzhang Yin, Yixin Wang, David M Blei, arXiv:2109.11990The Eleventh International Conference on Learning Representations. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, 2021. 2023arXiv preprintAutomatic chain of thought prompting in large language models</p>
<p>Batch calibration: Rethinking calibration for in-context learning and prompt engineering. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy, arXiv:2309.172492023aarXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>In this section, we focus on the performance of traditional and MOF prompts across all 10 prompt variations for each. The average accuracy across these 10 prompt variations for both traditional and MOF prompts is reported in Table 2. For all LLMs, we find that MOF prompts perform nearly as well as traditional prompts, with MOF prompts generally leading to significant overall mean accuracy improvements. Algorithm 1 Traditional prompts evaluation pipeline 1: Input: Data D 2: Create 10 variations of traditional prompts using FormatSpread. Sclar, 20247Output: Return accuracies for the best performing prompt (max accuracy), worst performing prompt (min accuracy), the spread, and the average accuracy across all 10 traditional prompt variations. Algorithm 2 MOF prompts evaluation pipeline 1: Input: Data D 2: Create 10 variations of MOF prompts using a modified FormatSpread. Sclar et al., 2024) that incorporates diverse styles in the few-shot examples as illustrated in Figure 2</p>
<p>Use the created MOF prompt variations to generate responses. 4: Evaluate each of the 10 MOF prompts and save results. 5: Compute the average accuracy across all 10 MOF prompt variations. 6: Identify the best performing prompt. worst performing prompt, and compute the spread. 7: Output: Return accuracies for the best performing prompt (max accuracy. worst performing prompt (min accuracy), the spread, and the average accuracy across all 10 MOF prompt variations</p>            </div>
        </div>

    </div>
</body>
</html>
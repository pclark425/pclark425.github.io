<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-d1da8fa082b16714780f63fa8275529d425bca6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d1da8fa082b16714780f63fa8275529d425bca6a" target="_blank">Imitation Learning: Progress, Taxonomies and Challenges</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Neural Networks and Learning Systems</p>
                <p><strong>Paper TL;DR:</strong> This survey provides an insightful review of imitation learning and presents research opportunities with learning policy from suboptimal demonstration, voice instructions, and other associated optimization schemes.</p>
                <p><strong>Paper Abstract:</strong> Imitation learning (IL) aims to extract knowledge from human experts’ demonstrations or artificially created agents to replicate their behaviors. It promotes interdisciplinary communication and real-world automation applications. However, the process of replicating behaviors still exhibits various problems, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. In this survey, we provide an insightful review on IL. We first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within IL and key milestones of the field. We then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions, and other associated optimization schemes.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2360.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2360.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioural Cloning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised-learning approach to imitation that directly maps observed states/contexts to actions/trajectories using expert demonstration, typically optimized with negative log-likelihood or regression losses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A framework for behavioural cloning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Robotics, autonomous driving, surgical automation, discrete control (games), scheduling/cache management</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a policy that reproduces expert behaviour by fitting a mapping from states (or observations) to actions using recorded demonstration trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically requires labeled state-action demonstration pairs; high-quality demonstrations are often limited/expensive (survey notes many methods assume high-quality demos); offline datasets commonly used, though online data-collection is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequence data: trajectories of (state, action) pairs; in some applications visual inputs (images/video) are used (multimodal), high-dimensional continuous control signals for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Can be high (high-DOF robots, continuous control, partially observed settings); main challenges include compounding error due to distribution shift and high-dimensional input/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied and mature in many engineering tasks (autonomous driving, robotics) as a pragmatic approach; methods and benchmarks well-developed but still active research for robustness and representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — often black-box policies are acceptable for control tasks, though interpretability can be desirable in safety-critical domains (e.g., surgical robotics).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Behavioural Cloning (supervised policy learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train a policy model πθ to minimize supervised loss (e.g., negative log-likelihood, ℓ1/ℓ2, KL divergence) on expert state-action pairs; model choices include neural networks for high-dimensional inputs; variants include model-free BC and model-based BC (which learns forward dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / Imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where reliable state-action demonstrations and controllers are available; suitable for problems where trial-and-error is costly; less suitable when distribution shift or unseen states dominate without corrective expert feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Efficient and straightforward; good sample efficiency when demonstrations are abundant and representative; suffers empirical failures from compounding error and poor generalization to unseen states without dataset augmentation (DAgger-like fixes).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables rapid policy acquisition for robotics and driving from demonstrations, lowers need for hand-crafted rewards; practical in industry where controllers and labeled demos exist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to IRL: BC is more time-efficient and requires less environment interaction but cannot recover reward structure; compared to adversarial methods: BC is computationally cheaper but less robust to suboptimal demos and dynamic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and coverage of demonstration data, choice of policy representation (neural nets for high-dim inputs), incorporation of online corrections or dataset-aggregation methods (e.g., DAgger) to mitigate covariate shift.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Behavioural cloning is an effective, sample-efficient supervised approach when plentiful, high-quality state-action demonstrations exist, but its effectiveness is limited by distribution shift and unseen-state compounding errors unless supplemented by expert interaction or dataset-aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2360.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that infer the (implicit) reward/cost function underlying expert demonstrations and then optimize a policy under that recovered reward, often via an inner-loop reinforcement learning procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning agents for uncertain environments (extended abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Simulated control tasks, robotics, problems where dynamics/future prediction matter (e.g., trajectory planning), high-level planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover a reward function R from demonstrations so that an agent optimizing R reproduces expert behaviour; useful when reward is unknown or hard to specify manually.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires demonstration trajectories (state-action sequences); can require substantial environment interaction for inner-loop RL updates; demonstration quantity can vary — survey notes IRL often simulated due to computational needs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Trajectory data (state-action pairs) and often environment models or samples; can include feature vectors φ(s) instead of raw reward values; may involve high-dimensional observations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: iterative inner-loop RL increases computational cost, ill-posedness (many rewards can explain same behavior), high-dimensional state/action spaces worsen computation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Conceptually established but computationally intensive; many recent advances (neural reward representations, max-entropy IRL, guided cost learning) improve scalability; more mature in simulated domains than in some real-world settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-to-high — IRL explicitly seeks a reward interpretation, so interpretability or causal insight into objective is often desired.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Inverse Reinforcement Learning (maximum-entropy IRL, guided cost learning, feature-matching IRL)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Iterative optimization: estimate policy πθ and reward parameters ω, compute state-action visitation u under current policy (via RL), update ω to match demonstration feature expectations (or maximize likelihood under max-ent), repeat; modern variants use neural networks to parameterize rewards and sample-based RL (TRPO/HER) for policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning / Inverse problem (generative modeling of rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when reward is unknown and domain dynamics matter; requires computational resources and environment interaction; suited to settings where recovered reward provides useful structure for transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Can produce policies that generalize better to dynamic contexts and future prediction compared to BC, but historically limited by computation and ill-posed reward ambiguity; neural reward models and improved RL algorithms have mitigated some issues.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Offers interpretable objective functions enabling transfer, debugging, and better long-horizon behavior prediction; potential in domains where reward engineering is infeasible (e.g., complex manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to BC: IRL requires more computation and environment interaction but can better capture long-term objectives; compared to adversarial IL: close relation (GAIL connects to max-ent IRL) but adversarial methods may avoid explicit reward recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Expressive reward parameterization (neural nets), efficient inner-loop RL (TRPO, HER), good feature representations; availability of environment simulators to sample policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>IRL trades greater computational and interaction cost for the ability to recover objective functions that enable better prediction and generalization in dynamics-sensitive tasks, making it preferable when interpretability of intent and long-horizon planning are important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2360.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAgger</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset Aggregation (DAgger)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative behavioural cloning algorithm that reduces compounding error by aggregating data collected from the learner's own rollouts with expert corrections to retrain the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A reduction of imitation learning and structured prediction to no-regret online learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Robotics, control, sequential decision-making tasks in which distribution shift from training to deployment causes failures</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Mitigate compounding error in BC by collecting additional labels from the expert on states visited by the learner, then aggregating these into the training set to improve generalization to learner-induced distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires access to an expert/oracle during iterative training to provide corrective labels; demonstrated data grows over iterations; expert interaction can be expensive or impractical in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Trajectory data (states visited by learner) with expert-provided actions/labels; sequential/time-series structure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Addresses distribution shift complexity; complexity depends on task dimensionality but algorithmic complexity increases due to repeated data collection and retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-known and widely referenced approach in the IL literature; applied in multiple domains and extended by many works.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — method is algorithmic and focuses on reducing statistical error; interpretability of learned policy not primary concern.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DAgger (iterative dataset aggregation for imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Initialize policy via BC, then iteratively: execute mixture policy (blend of expert and learner), collect states encountered, query expert for correct actions on those states, aggregate into dataset, and retrain policy; mixing parameter β decays to reduce reliance on expert.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Online supervised learning / Imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when an expert oracle is accessible during training and cost per query is affordable; less applicable when expert queries are expensive or impossible.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Empirically alleviates compounding error and improves generalization versus naive BC; drawback is the need for repeated expert queries and potential action cost inequality.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables deployment of BC in environments where learner-induced data distributions differ from demonstrations by actively correcting policy on novel states; practical for robotics labs and simulated settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over vanilla BC by reducing covariate shift; alternatives (e.g., offline augmentation, self-supervised approaches) aim to avoid expert queries at cost of potentially worse correction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of expert for iterative labeling, adequate coverage of learner-visited states, appropriate scheduling of mixing parameter β, and sufficient model capacity to absorb aggregated data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>DAgger demonstrates that iterative aggregation of learner-visited states with expert corrections empirically reduces compounding error in BC, trading expert interaction cost for improved robustness to distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2360.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Imitation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial imitation framework that frames imitation as distribution matching between expert and learner trajectories using a generator (policy) and discriminator, borrowing GAN techniques and using policy optimization (e.g., TRPO) for updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative Adversarial Imitation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>High-dimensional continuous control (robotics), autonomous driving, simulated games, multi-agent systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Match the distribution of expert state-action trajectories without requiring explicit reward labels or expert queries, enabling imitation from demonstrations by adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses expert trajectory data (state-action pairs) but does not require expert interaction during training; needs environment interaction to sample learner rollouts for discriminator updates.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Trajectory distributions over state-action pairs; can operate with high-dimensional observations including images.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: adversarial saddle-point optimization, requirement for many environment rollouts, high-dimensional policy parameter spaces; training can be unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent but rapidly adopted; spawned many variants (InfoGAIL, MGAIL, GAIfO) and used as a baseline in IL research.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — GAIL recovers behavior without explicit reward interpretability; interpretability of discriminator or learned policy is limited unless additional structure is used.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Generative Adversarial Imitation Learning (adversarial imitation using GAN-like objective)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train a discriminator Dω to distinguish expert vs learner state-action samples and update policy πθ to maximize confusion (minimize discriminator-based cost) often using TRPO or other policy gradient methods; training alternates discriminator updates and policy optimization, with optional entropy regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Adversarial generative modeling / Reinforcement learning / Imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for high-dimensional tasks and when expert interaction is unavailable or costly; requires substantial environment interaction and careful optimization tuning to avoid instability.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated robustness in high-dimensional domains and to distributional changes; more sample-efficient in use of expert data than on-policy RL from scratch but can be fragile due to adversarial training dynamics; does not explicitly recover reward function in many variants.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant: enabled adversarial approaches for imitation, improved performance on complex tasks, inspired many derivatives and cross-domain applications (multi-agent, IfO).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to BC: GAIL avoids expert queries and handles high-dim inputs better; compared to classic IRL: GAIL bypasses explicit reward recovery and uses adversarial matching, often yielding more scalable methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Expressive policy and discriminator networks, stable policy optimization (TRPO/other trust-region methods), good initialization (BC warm-start), and sufficient environment sampling; architectural variants (MGAIL, InfoGAIL) can improve differentiability or interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adversarial imitation reframes IL as trajectory-distribution matching, offering robustness in high-dimensional settings and removing reliance on expert queries, at the expense of increased environment interaction and potential training instability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2360.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IfO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imitation from Observation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that learn to imitate behaviors using observation-only demonstrations (e.g., raw video) without ground-truth action labels, often via representation translation or observation-distance reward surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Imitation from observation: Learning to imitate behaviors from raw video via context translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Robotics (manipulation), simulated games (Atari), video-based learning, visual navigation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn policies from demonstrations where only observations (images/video) are available, by deriving surrogates for actions or reward via feature translation, VAE-based context mapping, adversarial objectives, or observation distance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often exploits abundant unlabeled video sources (e.g., YouTube gameplay), which are plentiful but unaligned, noisy, and unlabeled; labeled state-action pairs may be unavailable or scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured visual data (images, video sequences), often unaligned across domains; multimodal when combined with proprioception or language.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: domain gap and viewpoint differences, unaligned temporal correspondence, high-dimensional pixel inputs, and potential partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging and rapidly growing; many recent works (GAIfO, BCO, One-Shot IfO) expand methodology and applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — surrogate reward or representation alignment is used; interpretability can help validate correctness but black-box representations are common.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Imitation from Observation (VAE-based context translation, adversarial IfO, observation-distance reward)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Approaches include VAE encoders for source and target contexts with a translator to map expert features into learner context and decode predicted observations, self-supervised domain-invariant representation learning from unlabelled videos, adversarial methods (GAIfO) applying discriminator on observations, and distance-based reward penalties between encoded features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Self-supervised / Unsupervised representation learning + Reinforcement learning / Adversarial methods</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable in scenarios with abundant unlabelled video but lacking action labels; enables leveraging internet-scale demonstrations for tasks like games and some robotics; requires domain-invariant features or translation mechanisms to succeed across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective at using unlabelled, noisy, or unaligned video (e.g., YouTube) to bootstrap imitation in hard-exploration games and robotics; performance depends on quality of representation learning and alignment mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to democratize imitation learning by opening vast unlabeled video resources (e.g., Internet videos) as training data, reducing need for expensive action annotation or expert instrumentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to standard IL (state-action BC/IRL): IfO relaxes the need for action labels and can scale to large unlabeled datasets; compared to methods that use proprioception, IfO may be less sample-efficient unless proprioceptive signals are available.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Robust, domain-invariant visual representations (self-supervised objectives), effective translation between source and target contexts (VAE translators), and downstream RL algorithms able to exploit surrogate rewards; leveraging proprioception or multi-view data improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>IfO shows that representation learning and observation-distance objectives can unlock imitation from abundant unlabeled videos, trading explicit action labels for stronger representation and alignment requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2360.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-Contrastive Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised method that learns viewpoint-invariant visual representations from video by contrasting frames across time and viewpoints, used to provide reward or state features for imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Time-contrastive networks: Self-supervised learning from video</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Robotic manipulation and imitation from multi-view video data</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn invariant embeddings of observations across viewpoints so the agent can match observed expert behaviors despite view changes, enabling imitation without action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses multi-view video recordings; data can be collected autonomously (e.g., robot viewing itself) or curated; amount can be moderate to large depending on collection effort.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured visual time-series (video frames) with multi-view correspondences; temporal structure used for contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high due to viewpoint variability, high-dimensional image inputs, and need for invariance across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent but practical; validated in robotics experiments (self-supervised imitation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — learned embeddings are used as features/rewards; interpretability of embeddings is limited though their alignment properties are evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Time-Contrastive Networks (self-supervised contrastive embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train an embedding network by pulling temporally-aligned frames (across views) together and pushing non-aligned frames apart (contrastive loss), producing a representation invariant to viewpoint; representations used to compute distances for imitation rewards or state features.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Self-supervised representation learning / Contrastive learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited when multi-view video is available and viewpoint invariance is necessary for imitation; effective as a pretraining or feature-extraction module for downstream IL.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enables robots to 'look at themselves in the mirror' to learn mappings between visual observations and proprioception, improving the feasibility of learning from video demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Facilitates scalable, annotation-free representation learning for visual imitation, lowering data-collection barriers in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to supervised feature engineering, TCNs require no labels and produce robust invariances; compared to VAE translation, contrastive objectives focus on discriminative alignment rather than reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of multi-view or synchronized video, appropriate contrastive sampling strategies, and sufficient model capacity to capture invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Self-supervised contrastive embeddings learned from multi-view, temporally-aligned video can provide robust, viewpoint-invariant features that materially aid imitation from observation in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2360.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DQfD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Q-learning from Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning approach that integrates a small set of expert demonstrations into Deep Q-Learning to accelerate training by pre-training and by mixing demonstration and self-generated experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Q-learning from Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Simulated control and game-playing tasks (Atari, hard-exploration environments), tasks where RL is expensive</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Use demonstration data to bootstrap value-based RL, reducing exploration burden and improving sample efficiency in sparse or hard-exploration reward environments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed to work with a small amount of high-quality demonstrations combined with abundant self-generated experience; demonstrations are labeled (state-action-reward) and thus more informative.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>State-action(-reward) trajectories suitable for value-function learning; can include pixel observations for Atari-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High in sparse-reward/hard-exploration tasks; complexity addressed by combining demonstration guidance with RL updates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Adopted in research as an effective practical technique to speed RL with demonstrations; many follow-up works build on the idea.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — primarily performance-driven; interpretability of learned Q-values less critical in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep Q-learning augmented with demonstrations (DQfD)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Pre-train Q-network on demonstration data and continue training with a replay buffer that mixes demonstrations and self-generated transitions, using auxiliary losses to preserve demo knowledge and shape exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning with demonstration / Hybrid supervised+RL</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where a small set of quality demonstrations are available and exploration is challenging; particularly useful in sparse reward or hard-exploration domains.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to significantly accelerate training and improve initial performance compared to pure RL, by 'kick-starting' learning with demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for practical RL tasks where obtaining many environment samples is expensive or dangerous; reduces sample complexity and stabilizes early learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pure RL: faster convergence and better initial policies; compared to pure BC: retains ability to improve beyond demonstrations via exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality of demonstrations, balance between demo and self-generated data in replay buffer, appropriate auxiliary losses to prevent demo forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating a small number of demonstrations into deep RL can substantially accelerate learning in sparse or hard-exploration tasks by providing informative priors while preserving capacity for further improvement via exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2360.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-IL / SIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Imitation Learning (and Self-supervised Imitation Learning variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that learn from the agent's own past good experiences by storing high-return trajectories and shaping learning to reproduce them, often combined with policy gradients and off-policy value updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hard-exploration RL tasks, robotics, navigation with language instructions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Exploit the agent's own successful past behaviors as additional demonstrations to bias learning toward high-return regions and improve exploration and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on internally generated trajectories; no external expert data required, though initial policies or small demonstrations can be used; amount of 'good' experience may be limited early on.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Replay buffers of trajectories with associated returns (R), possibly augmented with auxiliary modalities like language or vision.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Addresses difficulties in exploration and sparse rewards; complexity depends on environment dynamics and dimensionality but reduces reliance on external demos.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging technique with evidence of benefit in several domains; extensions incorporate planning or cross-modal signals.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — focuses on practical performance improvements; some variants use matching critics or ranking of experiences which can be interpretable to some degree.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Self-Imitation Learning / Self-supervised Imitation Learning (SIL)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Collect agent rollouts, compute accumulated returns, store high-return trajectories in a buffer, and use policy-gradient or actor-critic updates augmented with an objective that reproduces these high-return actions (e.g., using R−V as a weighting); can combine language encoders and visual trajectory encoders for multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning / Self-supervised imitation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful where external demonstrations are scarce and the agent can occasionally find good trajectories; helps in sparse-reward and hard exploration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improves exploration and stability by reinforcing past successes; performance depends on ability to discover initial high-return experiences and on replay management.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Promising for reducing reliance on external demonstrations and improving sample efficiency in difficult RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to demonstration-based methods: does not require expert data but may be less effective early unless good experiences emerge; complements other techniques like HER or DQfD.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Mechanisms to reliably identify and store useful experiences, proper balance between imitation of past successes and exploration, and integration with stable RL optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Self-imitation leverages an agent's own high-return trajectories as a form of demonstration to bootstrap learning in sparse-reward and hard-exploration tasks, reducing dependence on external expert data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2360.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2360.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE-based Diverse IL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VAE + Adversarial / Diverse Imitation (e.g., Diverse GAIL / VAE latent policy methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods combining variational autoencoders with adversarial or imitation learning to learn diverse latent-conditioned policies and perform one-shot or few-shot imitation from limited demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust Imitation of Diverse Behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Robotic behavior diversity and few-shot imitation, tasks requiring multiple modes of behavior (e.g., varied manipulation or driving styles)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a latent embedding of trajectories (via VAE) such that sampling or conditioning on latent z yields diverse policies matching multiple modes present in demonstrations, enabling one-shot or few-shot adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Can work with fewer demonstrations per mode by leveraging structured latent representations; needs trajectory datasets covering diverse behaviors to learn multimodal mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Trajectory sequences encoded into latent vectors; high-dimensional continuous control signals and possibly visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: multimodality requires capturing multiple modes in behavior distribution, avoiding mode collapse in generative models, and conditioning policy networks on latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent research direction; several successful prototypes demonstrate diverse behavior replication and one-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — latent factors may be interpretable but policies are often black-box conditional generators.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>VAE-augmented Imitation Learning / Diverse GAIL / Latent-policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train a VAE to map trajectories to latent z; jointly or subsequently train a conditional policy (or adversarially train generator/discriminator) so that conditioned on z the policy reproduces corresponding behavior; enables disentangled or structured latent control for diversity and few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Generative modeling + Imitation learning / Representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for tasks requiring modeling multi-modal behavior distributions and for enabling diverse or one-shot imitation from limited examples.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to enable diverse behavior reproduction and one-shot imitation in benchmark tasks; success conditioned on quality and coverage of the demonstration set and on mitigation of generative model pathologies.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for practical systems that must exhibit diverse styles (e.g., different driving styles, varied manipulation strategies) and for reducing demonstration requirements per behavior mode.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over single-mode imitation (BC/GAIL) by explicitly modeling multimodality; provides better sample-efficiency for diversity than naive augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Expressive latent models (VAE), proper regularization to avoid mode collapse, conditional policy capacity, and sufficient coverage of behavior modes in demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Augmenting imitation learning with latent generative models (VAEs) enables modeling and reproduction of diverse multimodal behaviors, allowing few-shot or one-shot adaptation when demonstrations per mode are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Imitation Learning: Progress, Taxonomies and Challenges', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Adversarial Imitation Learning <em>(Rating: 2)</em></li>
                <li>A reduction of imitation learning and structured prediction to no-regret online learning <em>(Rating: 2)</em></li>
                <li>Imitation from observation: Learning to imitate behaviors from raw video via context translation <em>(Rating: 2)</em></li>
                <li>A framework for behavioural cloning <em>(Rating: 1)</em></li>
                <li>Time-contrastive networks: Self-supervised learning from video <em>(Rating: 2)</em></li>
                <li>Deep Q-learning from Demonstrations <em>(Rating: 2)</em></li>
                <li>Self-imitation learning <em>(Rating: 2)</em></li>
                <li>Robust Imitation of Diverse Behaviors <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2360",
    "paper_id": "paper-d1da8fa082b16714780f63fa8275529d425bca6a",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "BC",
            "name_full": "Behavioural Cloning",
            "brief_description": "A supervised-learning approach to imitation that directly maps observed states/contexts to actions/trajectories using expert demonstration, typically optimized with negative log-likelihood or regression losses.",
            "citation_title": "A framework for behavioural cloning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Robotics, autonomous driving, surgical automation, discrete control (games), scheduling/cache management",
            "problem_description": "Learn a policy that reproduces expert behaviour by fitting a mapping from states (or observations) to actions using recorded demonstration trajectories.",
            "data_availability": "Typically requires labeled state-action demonstration pairs; high-quality demonstrations are often limited/expensive (survey notes many methods assume high-quality demos); offline datasets commonly used, though online data-collection is possible.",
            "data_structure": "Sequence data: trajectories of (state, action) pairs; in some applications visual inputs (images/video) are used (multimodal), high-dimensional continuous control signals for robotics.",
            "problem_complexity": "Can be high (high-DOF robots, continuous control, partially observed settings); main challenges include compounding error due to distribution shift and high-dimensional input/outputs.",
            "domain_maturity": "Applied and mature in many engineering tasks (autonomous driving, robotics) as a pragmatic approach; methods and benchmarks well-developed but still active research for robustness and representation.",
            "mechanistic_understanding_requirements": "Low-to-medium — often black-box policies are acceptable for control tasks, though interpretability can be desirable in safety-critical domains (e.g., surgical robotics).",
            "ai_methodology_name": "Behavioural Cloning (supervised policy learning)",
            "ai_methodology_description": "Train a policy model πθ to minimize supervised loss (e.g., negative log-likelihood, ℓ1/ℓ2, KL divergence) on expert state-action pairs; model choices include neural networks for high-dimensional inputs; variants include model-free BC and model-based BC (which learns forward dynamics).",
            "ai_methodology_category": "Supervised learning / Imitation learning",
            "applicability": "Applicable where reliable state-action demonstrations and controllers are available; suitable for problems where trial-and-error is costly; less suitable when distribution shift or unseen states dominate without corrective expert feedback.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Efficient and straightforward; good sample efficiency when demonstrations are abundant and representative; suffers empirical failures from compounding error and poor generalization to unseen states without dataset augmentation (DAgger-like fixes).",
            "impact_potential": "Enables rapid policy acquisition for robotics and driving from demonstrations, lowers need for hand-crafted rewards; practical in industry where controllers and labeled demos exist.",
            "comparison_to_alternatives": "Compared to IRL: BC is more time-efficient and requires less environment interaction but cannot recover reward structure; compared to adversarial methods: BC is computationally cheaper but less robust to suboptimal demos and dynamic changes.",
            "success_factors": "Quality and coverage of demonstration data, choice of policy representation (neural nets for high-dim inputs), incorporation of online corrections or dataset-aggregation methods (e.g., DAgger) to mitigate covariate shift.",
            "key_insight": "Behavioural cloning is an effective, sample-efficient supervised approach when plentiful, high-quality state-action demonstrations exist, but its effectiveness is limited by distribution shift and unseen-state compounding errors unless supplemented by expert interaction or dataset-aggregation.",
            "uuid": "e2360.0",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "IRL",
            "name_full": "Inverse Reinforcement Learning",
            "brief_description": "A family of methods that infer the (implicit) reward/cost function underlying expert demonstrations and then optimize a policy under that recovered reward, often via an inner-loop reinforcement learning procedure.",
            "citation_title": "Learning agents for uncertain environments (extended abstract)",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Simulated control tasks, robotics, problems where dynamics/future prediction matter (e.g., trajectory planning), high-level planning tasks",
            "problem_description": "Recover a reward function R from demonstrations so that an agent optimizing R reproduces expert behaviour; useful when reward is unknown or hard to specify manually.",
            "data_availability": "Requires demonstration trajectories (state-action sequences); can require substantial environment interaction for inner-loop RL updates; demonstration quantity can vary — survey notes IRL often simulated due to computational needs.",
            "data_structure": "Trajectory data (state-action pairs) and often environment models or samples; can include feature vectors φ(s) instead of raw reward values; may involve high-dimensional observations.",
            "problem_complexity": "High: iterative inner-loop RL increases computational cost, ill-posedness (many rewards can explain same behavior), high-dimensional state/action spaces worsen computation.",
            "domain_maturity": "Conceptually established but computationally intensive; many recent advances (neural reward representations, max-entropy IRL, guided cost learning) improve scalability; more mature in simulated domains than in some real-world settings.",
            "mechanistic_understanding_requirements": "Medium-to-high — IRL explicitly seeks a reward interpretation, so interpretability or causal insight into objective is often desired.",
            "ai_methodology_name": "Inverse Reinforcement Learning (maximum-entropy IRL, guided cost learning, feature-matching IRL)",
            "ai_methodology_description": "Iterative optimization: estimate policy πθ and reward parameters ω, compute state-action visitation u under current policy (via RL), update ω to match demonstration feature expectations (or maximize likelihood under max-ent), repeat; modern variants use neural networks to parameterize rewards and sample-based RL (TRPO/HER) for policy updates.",
            "ai_methodology_category": "Reinforcement learning / Inverse problem (generative modeling of rewards)",
            "applicability": "Appropriate when reward is unknown and domain dynamics matter; requires computational resources and environment interaction; suited to settings where recovered reward provides useful structure for transfer/generalization.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Can produce policies that generalize better to dynamic contexts and future prediction compared to BC, but historically limited by computation and ill-posed reward ambiguity; neural reward models and improved RL algorithms have mitigated some issues.",
            "impact_potential": "Offers interpretable objective functions enabling transfer, debugging, and better long-horizon behavior prediction; potential in domains where reward engineering is infeasible (e.g., complex manipulation).",
            "comparison_to_alternatives": "Compared to BC: IRL requires more computation and environment interaction but can better capture long-term objectives; compared to adversarial IL: close relation (GAIL connects to max-ent IRL) but adversarial methods may avoid explicit reward recovery.",
            "success_factors": "Expressive reward parameterization (neural nets), efficient inner-loop RL (TRPO, HER), good feature representations; availability of environment simulators to sample policies.",
            "key_insight": "IRL trades greater computational and interaction cost for the ability to recover objective functions that enable better prediction and generalization in dynamics-sensitive tasks, making it preferable when interpretability of intent and long-horizon planning are important.",
            "uuid": "e2360.1",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DAgger",
            "name_full": "Dataset Aggregation (DAgger)",
            "brief_description": "An iterative behavioural cloning algorithm that reduces compounding error by aggregating data collected from the learner's own rollouts with expert corrections to retrain the policy.",
            "citation_title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Robotics, control, sequential decision-making tasks in which distribution shift from training to deployment causes failures",
            "problem_description": "Mitigate compounding error in BC by collecting additional labels from the expert on states visited by the learner, then aggregating these into the training set to improve generalization to learner-induced distributions.",
            "data_availability": "Requires access to an expert/oracle during iterative training to provide corrective labels; demonstrated data grows over iterations; expert interaction can be expensive or impractical in some domains.",
            "data_structure": "Trajectory data (states visited by learner) with expert-provided actions/labels; sequential/time-series structure.",
            "problem_complexity": "Addresses distribution shift complexity; complexity depends on task dimensionality but algorithmic complexity increases due to repeated data collection and retraining.",
            "domain_maturity": "Well-known and widely referenced approach in the IL literature; applied in multiple domains and extended by many works.",
            "mechanistic_understanding_requirements": "Low — method is algorithmic and focuses on reducing statistical error; interpretability of learned policy not primary concern.",
            "ai_methodology_name": "DAgger (iterative dataset aggregation for imitation)",
            "ai_methodology_description": "Initialize policy via BC, then iteratively: execute mixture policy (blend of expert and learner), collect states encountered, query expert for correct actions on those states, aggregate into dataset, and retrain policy; mixing parameter β decays to reduce reliance on expert.",
            "ai_methodology_category": "Online supervised learning / Imitation learning",
            "applicability": "Highly applicable when an expert oracle is accessible during training and cost per query is affordable; less applicable when expert queries are expensive or impossible.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Empirically alleviates compounding error and improves generalization versus naive BC; drawback is the need for repeated expert queries and potential action cost inequality.",
            "impact_potential": "Enables deployment of BC in environments where learner-induced data distributions differ from demonstrations by actively correcting policy on novel states; practical for robotics labs and simulated settings.",
            "comparison_to_alternatives": "Improves over vanilla BC by reducing covariate shift; alternatives (e.g., offline augmentation, self-supervised approaches) aim to avoid expert queries at cost of potentially worse correction.",
            "success_factors": "Availability of expert for iterative labeling, adequate coverage of learner-visited states, appropriate scheduling of mixing parameter β, and sufficient model capacity to absorb aggregated data.",
            "key_insight": "DAgger demonstrates that iterative aggregation of learner-visited states with expert corrections empirically reduces compounding error in BC, trading expert interaction cost for improved robustness to distribution shift.",
            "uuid": "e2360.2",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "GAIL",
            "name_full": "Generative Adversarial Imitation Learning",
            "brief_description": "An adversarial imitation framework that frames imitation as distribution matching between expert and learner trajectories using a generator (policy) and discriminator, borrowing GAN techniques and using policy optimization (e.g., TRPO) for updates.",
            "citation_title": "Generative Adversarial Imitation Learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "High-dimensional continuous control (robotics), autonomous driving, simulated games, multi-agent systems",
            "problem_description": "Match the distribution of expert state-action trajectories without requiring explicit reward labels or expert queries, enabling imitation from demonstrations by adversarial training.",
            "data_availability": "Uses expert trajectory data (state-action pairs) but does not require expert interaction during training; needs environment interaction to sample learner rollouts for discriminator updates.",
            "data_structure": "Trajectory distributions over state-action pairs; can operate with high-dimensional observations including images.",
            "problem_complexity": "High: adversarial saddle-point optimization, requirement for many environment rollouts, high-dimensional policy parameter spaces; training can be unstable.",
            "domain_maturity": "Relatively recent but rapidly adopted; spawned many variants (InfoGAIL, MGAIL, GAIfO) and used as a baseline in IL research.",
            "mechanistic_understanding_requirements": "Low-to-medium — GAIL recovers behavior without explicit reward interpretability; interpretability of discriminator or learned policy is limited unless additional structure is used.",
            "ai_methodology_name": "Generative Adversarial Imitation Learning (adversarial imitation using GAN-like objective)",
            "ai_methodology_description": "Train a discriminator Dω to distinguish expert vs learner state-action samples and update policy πθ to maximize confusion (minimize discriminator-based cost) often using TRPO or other policy gradient methods; training alternates discriminator updates and policy optimization, with optional entropy regularization.",
            "ai_methodology_category": "Adversarial generative modeling / Reinforcement learning / Imitation learning",
            "applicability": "Appropriate for high-dimensional tasks and when expert interaction is unavailable or costly; requires substantial environment interaction and careful optimization tuning to avoid instability.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated robustness in high-dimensional domains and to distributional changes; more sample-efficient in use of expert data than on-policy RL from scratch but can be fragile due to adversarial training dynamics; does not explicitly recover reward function in many variants.",
            "impact_potential": "Significant: enabled adversarial approaches for imitation, improved performance on complex tasks, inspired many derivatives and cross-domain applications (multi-agent, IfO).",
            "comparison_to_alternatives": "Compared to BC: GAIL avoids expert queries and handles high-dim inputs better; compared to classic IRL: GAIL bypasses explicit reward recovery and uses adversarial matching, often yielding more scalable methods.",
            "success_factors": "Expressive policy and discriminator networks, stable policy optimization (TRPO/other trust-region methods), good initialization (BC warm-start), and sufficient environment sampling; architectural variants (MGAIL, InfoGAIL) can improve differentiability or interpretability.",
            "key_insight": "Adversarial imitation reframes IL as trajectory-distribution matching, offering robustness in high-dimensional settings and removing reliance on expert queries, at the expense of increased environment interaction and potential training instability.",
            "uuid": "e2360.3",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "IfO",
            "name_full": "Imitation from Observation",
            "brief_description": "A class of methods that learn to imitate behaviors using observation-only demonstrations (e.g., raw video) without ground-truth action labels, often via representation translation or observation-distance reward surrogates.",
            "citation_title": "Imitation from observation: Learning to imitate behaviors from raw video via context translation",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Robotics (manipulation), simulated games (Atari), video-based learning, visual navigation",
            "problem_description": "Learn policies from demonstrations where only observations (images/video) are available, by deriving surrogates for actions or reward via feature translation, VAE-based context mapping, adversarial objectives, or observation distance metrics.",
            "data_availability": "Often exploits abundant unlabeled video sources (e.g., YouTube gameplay), which are plentiful but unaligned, noisy, and unlabeled; labeled state-action pairs may be unavailable or scarce.",
            "data_structure": "Unstructured visual data (images, video sequences), often unaligned across domains; multimodal when combined with proprioception or language.",
            "problem_complexity": "High: domain gap and viewpoint differences, unaligned temporal correspondence, high-dimensional pixel inputs, and potential partial observability.",
            "domain_maturity": "Emerging and rapidly growing; many recent works (GAIfO, BCO, One-Shot IfO) expand methodology and applicability.",
            "mechanistic_understanding_requirements": "Medium — surrogate reward or representation alignment is used; interpretability can help validate correctness but black-box representations are common.",
            "ai_methodology_name": "Imitation from Observation (VAE-based context translation, adversarial IfO, observation-distance reward)",
            "ai_methodology_description": "Approaches include VAE encoders for source and target contexts with a translator to map expert features into learner context and decode predicted observations, self-supervised domain-invariant representation learning from unlabelled videos, adversarial methods (GAIfO) applying discriminator on observations, and distance-based reward penalties between encoded features.",
            "ai_methodology_category": "Self-supervised / Unsupervised representation learning + Reinforcement learning / Adversarial methods",
            "applicability": "Highly applicable in scenarios with abundant unlabelled video but lacking action labels; enables leveraging internet-scale demonstrations for tasks like games and some robotics; requires domain-invariant features or translation mechanisms to succeed across contexts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective at using unlabelled, noisy, or unaligned video (e.g., YouTube) to bootstrap imitation in hard-exploration games and robotics; performance depends on quality of representation learning and alignment mechanisms.",
            "impact_potential": "High potential to democratize imitation learning by opening vast unlabeled video resources (e.g., Internet videos) as training data, reducing need for expensive action annotation or expert instrumentation.",
            "comparison_to_alternatives": "Compared to standard IL (state-action BC/IRL): IfO relaxes the need for action labels and can scale to large unlabeled datasets; compared to methods that use proprioception, IfO may be less sample-efficient unless proprioceptive signals are available.",
            "success_factors": "Robust, domain-invariant visual representations (self-supervised objectives), effective translation between source and target contexts (VAE translators), and downstream RL algorithms able to exploit surrogate rewards; leveraging proprioception or multi-view data improves performance.",
            "key_insight": "IfO shows that representation learning and observation-distance objectives can unlock imitation from abundant unlabeled videos, trading explicit action labels for stronger representation and alignment requirements.",
            "uuid": "e2360.4",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "TCN",
            "name_full": "Time-Contrastive Network",
            "brief_description": "A self-supervised method that learns viewpoint-invariant visual representations from video by contrasting frames across time and viewpoints, used to provide reward or state features for imitation learning.",
            "citation_title": "Time-contrastive networks: Self-supervised learning from video",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Robotic manipulation and imitation from multi-view video data",
            "problem_description": "Learn invariant embeddings of observations across viewpoints so the agent can match observed expert behaviors despite view changes, enabling imitation without action labels.",
            "data_availability": "Uses multi-view video recordings; data can be collected autonomously (e.g., robot viewing itself) or curated; amount can be moderate to large depending on collection effort.",
            "data_structure": "Unstructured visual time-series (video frames) with multi-view correspondences; temporal structure used for contrastive learning.",
            "problem_complexity": "Moderate-to-high due to viewpoint variability, high-dimensional image inputs, and need for invariance across contexts.",
            "domain_maturity": "Relatively recent but practical; validated in robotics experiments (self-supervised imitation).",
            "mechanistic_understanding_requirements": "Low-to-medium — learned embeddings are used as features/rewards; interpretability of embeddings is limited though their alignment properties are evaluated.",
            "ai_methodology_name": "Time-Contrastive Networks (self-supervised contrastive embedding)",
            "ai_methodology_description": "Train an embedding network by pulling temporally-aligned frames (across views) together and pushing non-aligned frames apart (contrastive loss), producing a representation invariant to viewpoint; representations used to compute distances for imitation rewards or state features.",
            "ai_methodology_category": "Self-supervised representation learning / Contrastive learning",
            "applicability": "Well-suited when multi-view video is available and viewpoint invariance is necessary for imitation; effective as a pretraining or feature-extraction module for downstream IL.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Enables robots to 'look at themselves in the mirror' to learn mappings between visual observations and proprioception, improving the feasibility of learning from video demonstrations.",
            "impact_potential": "Facilitates scalable, annotation-free representation learning for visual imitation, lowering data-collection barriers in robotics.",
            "comparison_to_alternatives": "Compared to supervised feature engineering, TCNs require no labels and produce robust invariances; compared to VAE translation, contrastive objectives focus on discriminative alignment rather than reconstruction.",
            "success_factors": "Availability of multi-view or synchronized video, appropriate contrastive sampling strategies, and sufficient model capacity to capture invariant features.",
            "key_insight": "Self-supervised contrastive embeddings learned from multi-view, temporally-aligned video can provide robust, viewpoint-invariant features that materially aid imitation from observation in robotics.",
            "uuid": "e2360.5",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DQfD",
            "name_full": "Deep Q-learning from Demonstrations",
            "brief_description": "A reinforcement learning approach that integrates a small set of expert demonstrations into Deep Q-Learning to accelerate training by pre-training and by mixing demonstration and self-generated experiences.",
            "citation_title": "Deep Q-learning from Demonstrations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Simulated control and game-playing tasks (Atari, hard-exploration environments), tasks where RL is expensive",
            "problem_description": "Use demonstration data to bootstrap value-based RL, reducing exploration burden and improving sample efficiency in sparse or hard-exploration reward environments.",
            "data_availability": "Designed to work with a small amount of high-quality demonstrations combined with abundant self-generated experience; demonstrations are labeled (state-action-reward) and thus more informative.",
            "data_structure": "State-action(-reward) trajectories suitable for value-function learning; can include pixel observations for Atari-like tasks.",
            "problem_complexity": "High in sparse-reward/hard-exploration tasks; complexity addressed by combining demonstration guidance with RL updates.",
            "domain_maturity": "Adopted in research as an effective practical technique to speed RL with demonstrations; many follow-up works build on the idea.",
            "mechanistic_understanding_requirements": "Low — primarily performance-driven; interpretability of learned Q-values less critical in benchmarks.",
            "ai_methodology_name": "Deep Q-learning augmented with demonstrations (DQfD)",
            "ai_methodology_description": "Pre-train Q-network on demonstration data and continue training with a replay buffer that mixes demonstrations and self-generated transitions, using auxiliary losses to preserve demo knowledge and shape exploration.",
            "ai_methodology_category": "Reinforcement learning with demonstration / Hybrid supervised+RL",
            "applicability": "Applicable where a small set of quality demonstrations are available and exploration is challenging; particularly useful in sparse reward or hard-exploration domains.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to significantly accelerate training and improve initial performance compared to pure RL, by 'kick-starting' learning with demonstrations.",
            "impact_potential": "High for practical RL tasks where obtaining many environment samples is expensive or dangerous; reduces sample complexity and stabilizes early learning.",
            "comparison_to_alternatives": "Compared to pure RL: faster convergence and better initial policies; compared to pure BC: retains ability to improve beyond demonstrations via exploration.",
            "success_factors": "Quality of demonstrations, balance between demo and self-generated data in replay buffer, appropriate auxiliary losses to prevent demo forgetting.",
            "key_insight": "Incorporating a small number of demonstrations into deep RL can substantially accelerate learning in sparse or hard-exploration tasks by providing informative priors while preserving capacity for further improvement via exploration.",
            "uuid": "e2360.6",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Self-IL / SIL",
            "name_full": "Self-Imitation Learning (and Self-supervised Imitation Learning variants)",
            "brief_description": "Methods that learn from the agent's own past good experiences by storing high-return trajectories and shaping learning to reproduce them, often combined with policy gradients and off-policy value updates.",
            "citation_title": "Self-imitation learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Hard-exploration RL tasks, robotics, navigation with language instructions",
            "problem_description": "Exploit the agent's own successful past behaviors as additional demonstrations to bias learning toward high-return regions and improve exploration and sample efficiency.",
            "data_availability": "Relies on internally generated trajectories; no external expert data required, though initial policies or small demonstrations can be used; amount of 'good' experience may be limited early on.",
            "data_structure": "Replay buffers of trajectories with associated returns (R), possibly augmented with auxiliary modalities like language or vision.",
            "problem_complexity": "Addresses difficulties in exploration and sparse rewards; complexity depends on environment dynamics and dimensionality but reduces reliance on external demos.",
            "domain_maturity": "Emerging technique with evidence of benefit in several domains; extensions incorporate planning or cross-modal signals.",
            "mechanistic_understanding_requirements": "Low — focuses on practical performance improvements; some variants use matching critics or ranking of experiences which can be interpretable to some degree.",
            "ai_methodology_name": "Self-Imitation Learning / Self-supervised Imitation Learning (SIL)",
            "ai_methodology_description": "Collect agent rollouts, compute accumulated returns, store high-return trajectories in a buffer, and use policy-gradient or actor-critic updates augmented with an objective that reproduces these high-return actions (e.g., using R−V as a weighting); can combine language encoders and visual trajectory encoders for multimodal tasks.",
            "ai_methodology_category": "Reinforcement learning / Self-supervised imitation",
            "applicability": "Useful where external demonstrations are scarce and the agent can occasionally find good trajectories; helps in sparse-reward and hard exploration settings.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Improves exploration and stability by reinforcing past successes; performance depends on ability to discover initial high-return experiences and on replay management.",
            "impact_potential": "Promising for reducing reliance on external demonstrations and improving sample efficiency in difficult RL tasks.",
            "comparison_to_alternatives": "Compared to demonstration-based methods: does not require expert data but may be less effective early unless good experiences emerge; complements other techniques like HER or DQfD.",
            "success_factors": "Mechanisms to reliably identify and store useful experiences, proper balance between imitation of past successes and exploration, and integration with stable RL optimizers.",
            "key_insight": "Self-imitation leverages an agent's own high-return trajectories as a form of demonstration to bootstrap learning in sparse-reward and hard-exploration tasks, reducing dependence on external expert data.",
            "uuid": "e2360.7",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "VAE-based Diverse IL",
            "name_full": "VAE + Adversarial / Diverse Imitation (e.g., Diverse GAIL / VAE latent policy methods)",
            "brief_description": "Methods combining variational autoencoders with adversarial or imitation learning to learn diverse latent-conditioned policies and perform one-shot or few-shot imitation from limited demonstrations.",
            "citation_title": "Robust Imitation of Diverse Behaviors",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Robotic behavior diversity and few-shot imitation, tasks requiring multiple modes of behavior (e.g., varied manipulation or driving styles)",
            "problem_description": "Learn a latent embedding of trajectories (via VAE) such that sampling or conditioning on latent z yields diverse policies matching multiple modes present in demonstrations, enabling one-shot or few-shot adaptation.",
            "data_availability": "Can work with fewer demonstrations per mode by leveraging structured latent representations; needs trajectory datasets covering diverse behaviors to learn multimodal mapping.",
            "data_structure": "Trajectory sequences encoded into latent vectors; high-dimensional continuous control signals and possibly visual inputs.",
            "problem_complexity": "High: multimodality requires capturing multiple modes in behavior distribution, avoiding mode collapse in generative models, and conditioning policy networks on latent variables.",
            "domain_maturity": "Relatively recent research direction; several successful prototypes demonstrate diverse behavior replication and one-shot generalization.",
            "mechanistic_understanding_requirements": "Low-to-medium — latent factors may be interpretable but policies are often black-box conditional generators.",
            "ai_methodology_name": "VAE-augmented Imitation Learning / Diverse GAIL / Latent-policy learning",
            "ai_methodology_description": "Train a VAE to map trajectories to latent z; jointly or subsequently train a conditional policy (or adversarially train generator/discriminator) so that conditioned on z the policy reproduces corresponding behavior; enables disentangled or structured latent control for diversity and few-shot generalization.",
            "ai_methodology_category": "Generative modeling + Imitation learning / Representation learning",
            "applicability": "Appropriate for tasks requiring modeling multi-modal behavior distributions and for enabling diverse or one-shot imitation from limited examples.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to enable diverse behavior reproduction and one-shot imitation in benchmark tasks; success conditioned on quality and coverage of the demonstration set and on mitigation of generative model pathologies.",
            "impact_potential": "High for practical systems that must exhibit diverse styles (e.g., different driving styles, varied manipulation strategies) and for reducing demonstration requirements per behavior mode.",
            "comparison_to_alternatives": "Improves over single-mode imitation (BC/GAIL) by explicitly modeling multimodality; provides better sample-efficiency for diversity than naive augmentation.",
            "success_factors": "Expressive latent models (VAE), proper regularization to avoid mode collapse, conditional policy capacity, and sufficient coverage of behavior modes in demonstrations.",
            "key_insight": "Augmenting imitation learning with latent generative models (VAEs) enables modeling and reproduction of diverse multimodal behaviors, allowing few-shot or one-shot adaptation when demonstrations per mode are limited.",
            "uuid": "e2360.8",
            "source_info": {
                "paper_title": "Imitation Learning: Progress, Taxonomies and Challenges",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Adversarial Imitation Learning",
            "rating": 2
        },
        {
            "paper_title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "rating": 2
        },
        {
            "paper_title": "Imitation from observation: Learning to imitate behaviors from raw video via context translation",
            "rating": 2
        },
        {
            "paper_title": "A framework for behavioural cloning",
            "rating": 1
        },
        {
            "paper_title": "Time-contrastive networks: Self-supervised learning from video",
            "rating": 2
        },
        {
            "paper_title": "Deep Q-learning from Demonstrations",
            "rating": 2
        },
        {
            "paper_title": "Self-imitation learning",
            "rating": 2
        },
        {
            "paper_title": "Robust Imitation of Diverse Behaviors",
            "rating": 2
        }
    ],
    "cost": 0.0217505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Imitation Learning: Progress, Taxonomies and Challenges</h1>
<p>BOYUAN ZHENG, SUNNY VERMA, JIANLONG ZHOU, IVOR TSANG, and FANG CHEN, University of Technology Sydney, Autralia</p>
<p>Imitation learning aims to extract knowledge from human experts' demonstrations or artificially created agents in order to replicate their behaviours. Its success has been demonstrated in areas such as video games, autonomous driving, robotic simulations and object manipulation. However, this replicating process could be problematic, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. In this survey, we provide a systematic review on imitation learning. We first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within Imitation Learning and key milestones of the field. We then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions and other associated optimization schemes.
Additional Key Words and Phrases: datasets, neural networks, gaze detection, text tagging</p>
<h2>Reference Format:</h2>
<p>Boyuan Zheng, Sunny Verma, Jianlong Zhou, Ivor Tsang, and Fang Chen. 2022. Imitation Learning: Progress, Taxonomies and Challenges. (October 2022), 21 pages.</p>
<h2>1 INTRODUCTION</h2>
<p>Imitation learning (IL), also known as learning from demonstration, makes responses by mimicking behavior in a relatively simple approach. It extracts useful knowledge to reproduce the behavior in the environment which is similar to the demonstrations'. The presence of IL facilitates the research on autonomous control system and designing artificially intelligent agents, as it demonstrates good promise in real-world scenario and efficiency to train a policy. Recent developments in machine learning field like deep learning, online learning and Generative Adversarial Network (GAN) [23] make further improvement on IL, not only alleviating existing problems like dynamic environment, frequent inquiries and high-dimensional computation, but also achieving faster convergence, more robust to the noise and more sample-efficient learning process. These improvements of IL promote the applications in both continuous and discrete control domains. For example, in the continuous control domain, imitation learning could be applied to autonomous vehicle manipulation to reproduce appropriate driving behavior in a dynamic environment[11, 13, 14, 22, 31, 52, 53, 80]. In addition, imitation learning is also applied to robotic, ranging from basic grabbing and placing to surgical assistance[21, 37, 43, 46, 48, 49, 67, 79]. In the discrete control domain, imitation learning makes contribution to fields like game theory[5, 19, 24, 55], navigation tasks[28, 62, 76], cache management[38] and so on.</p>
<p>It is worth noting that the demonstrations could be gathered either from human experts or artificial agents. In most cases, the demonstration is collected from human experts, but there are also some studies that obtain the demonstration through another artificial agent. For example, Chen et al.[13] proposed a teacher-student training structure, they train a teacher agent with additional information and use this trained agent to teach a student agent without additional information. This process is not redundant, using the demonstration from other agent benefits the training process as student agents can rollout their own policy by frequently querying trained agents and learn policies from similar configurations while classic IL needs to overcome the kinematic shifting problem.</p>
<p>Authors' address: Boyuan Zheng, 14055661@student.uts.edu.au; Sunny Verma; Jianlong Zhou; Ivor Tsang; Fang Chen, University of Technology Sydney, PO Box 123, Sydney, New South Wales, Autralia, 2007.</p>
<p>Published in The IEEE Transactions on Neural Networks and Learning Systems.</p>
<p>IL has a close relationship with Reinforcement Learning (RL). Both IL and RL commonly solve the problem under Markov Decision Process, and improvements like TRPO[60] in RL could benefit IL as well, but they reproduce the behavior in a different manner. In comparing to RL, IL is more efficient, accessible, and human-interactive. In terms of efficiency, comparing with trial and error, the IL agents usually spend less time to produce the desired behavior by using the demonstrations as guidance. In terms of accessibility, achieving autonomous behavior in the RL approach requires human experts who are familiar with the problem setting, together with hard-coded reward functions which could be impractical and non-intuitive in some settings. For example, people learn to swim and walk almost from demonstration instead of math functions, and it is hard to formulate these behavior mathematically. IL also prompts interdisciplinary integration, experts who are novice to programming can contribute to the design and evaluating paradigms. In terms of human-interaction, IL highlights human's influence through providing demonstration or preference to accelerate the learning process, which efficiently leverages and transfers the experts' knowledge. Although IL presents the above merits, it also faces challenges and opportunities, and this content will be detailed in the following sections.</p>
<p>This survey is organized as follows:</p>
<ul>
<li>Systematic review This survey presents research in imitation learning under categories behavioural cloning vs. inverse reinforcement learning and model-free vs. model-based. It then summarizes IL research into two new categories namely low-level tasks vs. high-level tasks and $B C$ vs. IRL vs. Adversarial Structured IL, which are more adapted to the development of IL.</li>
<li>Background knowledge A comprehensive description of IL's evolution is presented in Section 2, followed by fundamental knowledge in Section 3 and the most common learning framework in Sections 5.</li>
<li>Future direction This survey presents the remaining challenges of IL, like learning diverse behavior, leveraging various demonstration and better representation. Then we discuss the future directions with respect to methods like transfer learning and importance sampling.</li>
</ul>
<h1>2 BACKGROUND</h1>
<p>One of the earliest well-known research on IL is the Autonomous Land Vehicle In a Neural Network (ALVINN) project at Carnegie Mellon University proposed by Pomerleau[52]. In 1998, a formal definition of Inverse Reinforcement Learning (IRL) was proposed by Russell[58]. Inverse reinforcement learning aims to recover reward function from demonstrations. A year after, a formal definition of another important category - Behavioural Cloning (BC) was proposed in[6]. BC works in a supervised learning fashion and seeks to learn a policy that builds a direct mapping between states and actions, then output a control strategy for control tasks. Although BC demonstrates significant advantage in efficiency, it also suffers from various problems. In 2010, SMiLe[55] was proposed, it mixed a new policy $\hat{\pi}^{n+1}$ with a fixed probability $\alpha$ as next policy, this method promotes the development of IL and set up the foundation for the later proposed DAgger[57]. DAgger was proposed by Ross et al. and it updates the dataset in each iteration and trains a new policy in the subsequent iteration based on the updated dataset. Compared with previous methods like SMILE [55] and SEARN [16], DAgger alleviates the problem on the unseen scenario and achieve data-efficiency. Later research like[38, 56, 67] were proposed to make improvements on DAgger. Besides DAgger and its derivatives, other BC methods also make contribution to the development of IL like MMD-IL[32], LOLS[12]. As for applications, one of the notable applications of BC was proposed by Abbeel et al.[1], a model-free BC method on autonomous helicopter project, developed an open-loop iterative learning control. Another famous BC application was an autonomous surgical knot-tying robotic proposed by Osa et al.[49], which achieved online trajectory planning</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Featured approaches and annual publication numbers for each class of approaches. The blue text indicates some of the most active research topics in IL and the background histogram plot is the number of annual publications. The data was collected from Web of Science until 31 May 2021, filtered by setting up each class and their abbreviation as keywords (like "Behavioural Cloning OR BC", only cover records within computer science).
and updating in a dynamic system. Besides these real-world applications, BC was also implemented into other research fields like cybernetics, for example, DAgger was used for scheduling in [75] and Liu et al. leveraged Belagy's optimal policy (proof-of-concept) as oracle to solve the cache replacement problem by predicting reuse distance when cache miss happens[38].</p>
<p>In terms of IRL, Ziebart et al.[82] proposed Maximum Entropy IRL, which uses maximum entropy distribution to develop a convex procedure for good promise and efficient optimization. This method played a pivotal role in the development of subsequent IRL and GAIL. In 2016, Finn et al.[21] made significant contributions to IRL and proposed a model-based IRL method called guided cost learning, neural network is used for representing cost to enhance expressive power, combining with sample-based IRL to handle the unknown dynamics. Later in 2017, Hester et al. proposed DQfD[24] which uses small amount of demonstration to significantly accelerate the training process by doing pre-training to kick-off and learning from both demonstration and self-generated data. Later methods like T-REX[9], SQIL[54], SILP[41] make improvements on IRL from different aspects.</p>
<p>Another novel method called Generative Adversarial Imitation Learning (GAIL), it was proposed in 2016 by Ho and Ermon[25] and became one of the hot topics in IL. Later research like[17, 33, 65, 76] were proposed inspired by GAIL and other generative models were gradually adopted in IL. Besides GAIL, another important research direction is inspired by Stadie et al.[65]. Since first-person demonstrations are hard to obtain in practice, and people usually learn by observing the demonstration of others through the perspective of a third party, learning from third-person viewpoint demonstrations was proposed. The change of viewpoint facilitates the following research like[9, 19], which includes IfO[40]. IfO focus on simplifying input to use raw video only (i.e. no longer use state-action pairs), many following methods advocate this new setting. These methods measure the distance between observations to replace the need for ground-truth actions and widen the available input for training, foe example, using YouTube videos for training[5]. Other interesting research fields like meta-learning $[18,20,27]$, multi-agent learning[78] are also thrived because of</p>
<p>the development of IL. Figure 1 shows some featured approaches and annual publication numbers for each class and focuses on the research after 2016, it shows that the class of BC(Behavioural Cloning) has maintained a stable increment in publications, while the research in the class of Adversarial Structured IL and IRL(Inverse Reinforcement Learning) have grown rapidly due to the recent advance in other research fields like deep learning.</p>
<h1>3 PRELIMINARY KNOWLEDGE</h1>
<p>This section provides some basic concepts for better understanding of the IL methodology.
In IL, the demonstrated trajectories are commonly represented as pairs of states $s$ and actions $a$, sometimes other parameters such as high-level commands and conditional goals will also be included to form the dataset. The way to collect the dataset could be either online or offline. Offline IL prepares the dataset in advance and obtains policies from the dataset while involves fewer interactions with the environment. This could be beneficial when interacting with the environment is expensive or risky. Contrary to offline learning, online learning assumes the data would be accessible in sequence and uses this updated data to learn the best predictor for future data. This method facilitates imitation learning to be more robust in a dynamic system. For example, in [48, 49, 57], online learning is used in surgical robotics. The online learning agent will provide a policy in iteration $n$, then the opponent will choose a loss function $l_{n}$ based on current policy and the new observed loss will affect the choice of next iteration $n+1$ 's policy. The performance is measured through regret, i.e.</p>
<p>$$
\sum_{n=1}^{N} l_{n}\left(\pi_{n}\right)-\min <em n="1">{\pi \in \Pi} \sum</em>(\pi)
$$}^{N} l_{n</p>
<p>and the loss function could vary from iteration to iteration. One of the most common ways to calculate loss is Kullback-Leibler (KL) Divergence. KL Divergence measures the difference between 2 probability distribution, i.e.,</p>
<p>$$
D_{K L}(p(x) | q(x))=\int p(x) \ln \frac{p(x)}{q(x)} d x
$$</p>
<p>KL divergence is not symmetric, i.e., $D_{K L}(p(x) | q(x)) \neq D_{K L}(q(x) | p(x))$. Many algorithms such as $[8,60]$ use KL divergence as the loss function as it could be useful when dealing with the stochastic policy learning problem.</p>
<p>For many methods, especially those under the class of IRL and Adversarial structured IL, the environment is modeled as Markov Decision Process(MDP). MDP is the process satisfying the property that the next state $s_{t+1}$ only depends on the current state $s_{t}$ at any time $t$. Typically, a MDP is defined as a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \gamma, \mathcal{D}, \mathcal{R})$, where $\mathcal{S}$ is the finite set of states, $\mathcal{A}$ is the corresponding set of actions, $\mathcal{P}$ is the set of state transition probabilities and the successor states $s_{t+1}$ is drawn from this transition model, i.e. $s_{t+1}=P\left(\cdot \mid s_{t}, a_{t}\right), \gamma \in[1,0)$ is the discount factor, $\mathcal{D}$ is the set of initial state distribution and $\mathcal{R}$ is the reward function $\mathcal{S} \mapsto \mathbb{R}$, and in IL setting, the reward function is not available. The Markov property assists imitation learning to simplify the input since the earlier state is helpless to determine the next state. The use of MDP inspires research to make use of other MDP variants to solve various problems, for example, Partially Observable MDP is used to model the scheduling problem in [75] and Markov games is used in multi-agent scenario[63].</p>
<p>The learning process of IL could be either on-policy or off-policy (there exists research using a hierarchical combination of these two[13]). On-policy learning estimates the return and updates the action using the same policy, the agent adopting on-policy will pick actions by themselves and rollout their own policy while training; Off-policy learning estimates the return and chooses the action using different policy, the agent adopting off-policy will update their policy greedily and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Taxonomies in this review.</p>
<p>Table 1. Categorization of IL: BC vs. IRL</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Classes</th>
<th style="text-align: center;">Examples and Publications</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Behavioural Cloning</td>
<td style="text-align: center;">Few-shots learning[18]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Input optimization[13]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Latent policy learning[42]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Real-world application[79]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Improving efficiency[9]</td>
</tr>
<tr>
<td style="text-align: center;">Inverse Reinforcement Learning</td>
<td style="text-align: center;">Raw video as inputs[61]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adversarial structured[66]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sparse reward problem[44]</td>
</tr>
</tbody>
</table>
<p>imitate action with the help of other sources. Some recent IL research such as [84?? ] advocates off-policy actor-critic architecture to optimize the agent policy and achieve sample efficiency comparing with on-policy learning.</p>
<h1>4 CATEGORIZATION AND FRAMEWORKS</h1>
<p>In this section, four kinds of taxonomies are presented (see Figure 2). The first two taxonomies (BC vs. IRL and model-free vs. model-based) follow the classifications in[47, 72] and the other two (Low=level Manipulation Tasks vs. High-Level Tasks and BC vs. IRL vs. adversarial structured IL are new proposed taxonomies.</p>
<h3>4.1 Behavioural Cloning vs. Inverse Reinforcement Learning</h3>
<p>IL is conventionally divided into BC and IRL. These two classes flourish by combining various techniques and then extend into different domains. Generally speaking, BC and IRL methods use different methodology to reproduce the expert behavior. BC commonly uses a direct mapping from the states to the actions, while IRL tries to recover the reward function from the demonstrations. This difference could be why BC methods are commonly applied to real-world problems while most IRL methods still do simulations in the environment with less invention.</p>
<p>Compared with direct mapping, recovering a reward function needs stronger computational power and technologies to obtain the unique reward function and solve the sparse reward problem.</p>
<p>Table 2. Categorization of IL: Model-based vs. Model-free</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Classes</th>
<th style="text-align: left;">Examples and Publications</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model-based IL</td>
<td style="text-align: left;">Forward model $[19,21]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Inverse model $[43]$</td>
</tr>
<tr>
<td style="text-align: left;">Model-free IL</td>
<td style="text-align: left;">BC method $[42]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Reward engineering $[9]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Adversarial style $[70]$</td>
</tr>
</tbody>
</table>
<p>The inner loop reinforcement learning could also cause IRL methods to be impractical in realworld problems. For the computational problem, recent development in GPU gradually alleviate the problem of high-dimensional computation; for the technology aspect, recent algorithms like Trust Region Policy Optimization[60] and attention models[26] provide more robust and efficient approaches for IRL methods; as for the sparse reward function, Hindsight Experience Replay[2] is commonly adopted for this problem. On the other hand, BC also suffers from the "compounding error"[57] where a small error could destroy the final performance. Besides these problems, other problems like better representation and diverse behavior learning are still open, many approaches are proposed for these problems, such as $[29,39,76]$.</p>
<p>Table 1 lists some of the recent research in IL categorized into BC and IRL. Recent BC methods mainly focus on the topics such as: meta-learning that the agent is learning to learn by pretraining on a broader range of behaviors[18]; combining BC with other technique like VR equipment[79]. On the other hand, recent IRL methods mainly focus on the topics such as: extending GAIL with other methods or problem settings[17]; recovering reward function from raw videos[5]; developing more efficient model-based IRL approaches by using the current development in reinforcement learning like TRPO[60] and HER[2].</p>
<h1>4.2 Model-Based vs. Model-Free</h1>
<p>Another classical taxonomy divides IL into model-based and model-free methods. The main difference between these two classes is whether the algorithm adopts a forward model to learn from the environmental context/dynamics. Before GAIL[25] was proposed, most IRL methods are developed in the model-based setting because IRL methods commonly involve iterative algorithms evaluate the environment, while BC methods are commonly model-free since the low-level controller is commonly available. After GAIL was proposed, various adversarial structured IL are proposed following the GAIL's model-free setting. Although learning from the environment sounds beneficial for all kinds of methods, it might not be necessary for a given problem setting or impractical to apply. Integrating environment context/dynamics could obtain more useful information so that the algorithm can achieve data-efficiency and feasibility, while the drawback is learning the model is expensive and challenging. For example, in robotics, the equipment is commonly precise, the spatial position, velocity and other parameters could be easily obtained, the system dynamics might provides relatively little help to reproduce the behavior. On the other hand, in autonomous car tasks, the system dynamics might be crucial to avoid hitting pedestrians. In this case, the choice of model-free or model-based depends on the tasks. Table 2 lists some of the recent research topics in IL categorized into model-based and model-free.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Prevalent Tasks in IL. Top-left: HalfCheetah in Mujoco; Top-mid: CARLA simulator; Top-right: Minecraft scenario in MineRL dataset; Bottom-left: FetchPickAndPlace-v1 in OpenAI Gym; Bottom-mid: Driving scenario in Xi'an [80]; Bottom-right: Atari game-MontezumaRevenge</p>
<h1>4.3 Low-Level Tasks vs. High-Level Tasks</h1>
<p>This subsection introduces a novel taxonomy, which divides IL into manipulation tasks and highlevel tasks according to their evaluation approach. The idea is inspired by a control diagram (See Figure 4) in[47]. Although some IL benchmark systems are proposed, such as[34], there is still no widely accepted one. In this case, the evaluation approaches and focus could vary from method to method, ranging from performance in sparse reward scenario to the smoothness of autonomous driving in dynamic environment. This taxonomy could draw clearer boundary and might alleviate the difficulty of designing appropriate benchmark from performance perspective.</p>
<p>The low-level manipulator tasks could be either real-world or virtual, and are not limited to robotics and autonomous driving problems. The robotic task can be object manipulation by robotic arm like PR2, KUKA robot arm, and simulation tasks commonly experimented on OPEN AI gym, MuJoCo simulation platform and so on. For real-world object manipulation tasks, the tasks could be push the object to the desired area, avoiding obstacles and operation soft object like rope. The autonomous driving tasks commonly implemented by simulation, and which is more related to the high-level planning. There are two widely-used benchmark system for simulation: CARLA CoRL2017 and NoCrash benchmark system, these two benchmark systems mainly focus on the urban scenario under various weather condition while the agent is evaluated on whether it can reach the destination on time, but CARLA CoRL2017 ignores the collision and traffic rules violation. Besides simulation, there are also some research doing experiment in real-world using cars[80] and smaller remote-controlled cars[14], but other kinds of equipment are also used like remote control helicopter[1]. As for the high-level controller, the tasks could be navigation tasks and gameplay. The navigation tasks are mainly route recommendation and in-door room-to-room navigation. Most of the evaluated games are 2D Atari games on OpenAI Gym, such as MontezumaRevenge is commonly</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Control diagram adapted from [47]
evaluated for performance on hard expolration and sparse reward scenario. Others are evaluated on 3D games like GTAV or Minecraft for evaluation. This taxonomy could be meaningful since it clearly reflects the target domain of the proposed algorithm, as the variance on their evaluation methods could be smaller, this may help to design a unified evaluation metric for IL. Figure 3 provides various popular evaluation tasks in IL.</p>
<p>From the Figure 4, the target of imitation could be either learning a policy for high-level controllers while assuming the low-level manipulator is working correctly or learning a policy to reproduce the simpler behavior on the low-level controller. Generally speaking, the high-level controller learns a policy to plan a sequence of motion primitives, such as [49]. As for the low-level controller, it learns a policy to reproduce the primitive behavior, such as [61], this forms the hierarchical structure of IL. Although some of the methods propose general frameworks which are evaluated on both domains, most of them are presenting "bias" on selecting tasks to demonstrate their improvement in either higher-level or low-level domain. For example, in [10], the proposed algorithm is evaluated on both Atari and Mujoco environments, but the amount of the evaluated tasks in each environment is obviously unequal. In this case, the ambiguity of classifying these general methods could be simply eliminated based on their tendency on evaluation tasks.</p>
<p>Table 3 lists some of the recent research under this taxonomy. The majority of current imitation methods tend to use low-level manipulation tasks to evaluate the proposed method, since reinforcement learning performs acceptably in high-level controller tasks like games, and commonly performs poorly on the low-level manipulation tasks where the reward function might be impractical to obtain. Nevertheless, IL in the high-level controller tasks is non-trivial, since for the 3D tasks or hard exploration games, reinforcement learning can be time-consuming on the huge state and action space.</p>
<h1>4.4 BC vs. IRL vs. Adversarial Structured IL</h1>
<p>This taxonomy is extended from the first taxonomy (BC vs. IRL). This new taxonomy divides IL into three categories: Behavioural Cloning (BC), Inverse Reinforcement Learning (IRL) and adversarial structured IL. With the recent development of IL, adversarial structured IL brings new insights for researchers and alleviate problems existing in previous work, such as high-dimensional problem. Inspired by the presence of GAIL, many recent papers adopt this adversarial structure, and inevitably, GAIL becomes baseline for comparison. But this is not enough to establish an</p>
<p>Table 3. Categorization of IL: Low-level Tasks vs. High-level Tasks</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Classes</th>
<th style="text-align: left;">Examples and Publications</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Surgical assistance $[49,68]$</td>
</tr>
<tr>
<td style="text-align: left;">Low-level manipulation</td>
<td style="text-align: left;">Vehicle manipulation[80]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Robotic arm[61]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">VR teleoperation[79]</td>
</tr>
<tr>
<td style="text-align: left;">High-level tasks</td>
<td style="text-align: left;">2D gameplay[59]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">3D gameplay[4]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Navigation[28]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Sports analysis[78]</td>
</tr>
</tbody>
</table>
<p>independent category in IL, the true reason making it distinguishable is that GAIL is not belongs to either BC or IRL. Although adversarial structured IL has close connection with IRL, most adversarial structured IL does not recover the reward function. In this case, the taxonomy of IL could be more specific. GAIL and its derivations are separated from the traditional IRL category and classified as adversarial structured IL in this survey. Compared with the traditional taxonomies, the proposed new taxonomy is more adapted to the development of IL and eliminates the vagueness of classifying these adversarial structured methods.</p>
<p>Figure 5 roughly evaluate the proposed three classes through two kinds of aspects which are commonly compared between research. Since different methods evaluate on various tasks, the overall performance is hard to quantify and rank, in this case, we evaluate three classes from Efficiency and Robustness from an empirical perspective.</p>
<p>In terms of Efficiency, we mainly focus on environmental interaction, computation, and expert interaction. BC methods commonly take advantage of interaction with expert while have less interaction in the environment, and due to these characteristics, the computational cost for BC is more likely to be the lowest; IRL methods commonly have abundant interaction with the environment in their inner-loop, and the evaluation on system dynamic makes IRL suffers from high computational cost, but IRL methods hardly enquiry the expert during training; Adversarial structured IL methods also involve frequent interaction with the environment when they iteratively update the policy parameter and discriminator parameter, and get rid of the interaction with expert. As adversarial structured IL methods are commonly model-free, in the evaluation of computational efficiency, we rank it as the second.</p>
<p>In terms of Robustness, we mainly focus on robustness in high-dimensional space, robustness when demonstrations are suboptimal (includes the consideration on noise in demonstration), and robustness in dynamic system. BC methods commonly have better performance in highdimensional space so that they are widely evaluation on robotics, while the performance in dynamic environment and suboptimal dataset are limited; IRL methods optimize the parameter in their inner-loop, which becomes a burden limiting their performance in high-dimensional space, but the recovered reward function would benefit the agent to do prediction in dynamic system. Since adversarial structured IL methods commonly derive from GAIL, they inherit the merits of GAIL: robustness in high-dimensional space and when changes occur in distribution. Because recent research such as $[9,17,84]$ in both IRL and Adversarial structured IL make progress in suboptimal</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Web plot for taxonomy: BC vs. IRL vs. Adversarial Structured IL. We collected 6 popular evaluation criteria from the research and empirically ranked them into three levels based on research consensus. The outer the point, the higher the ranking, which means that it scores higher in the evaluation from the empirical perspective.
demonstration problem, we give them the same rank in the evaluation of robustness on suboptimal demonstration.</p>
<h1>5 MAIN RESEARCH TOPICS AND METHODS</h1>
<h3>5.1 Behavioural Cloning</h3>
<p>Behavioural Cloning directly maps the states/contexts to actions/trajectories by leveraging the demonstration provided by expert/oracle. After generating the control input or trajectories, the loss function $\mathcal{L}$ will be designed according to the problem formulation and optimized in a supervised learning fashion. The state-of-the-art behavioural cloning uses negative log-likelihood loss to update the policy, i.e.</p>
<p>$$
\underset{\pi}{\operatorname{argmin}} \mathcal{L}(\pi)=-\frac{1}{N} \sum_{k=1}^{N} \log \pi\left(a_{k} \mid s_{k}\right)
$$</p>
<p>Algorithm 1 outlines the state-of-the-art behavioural cloning process. As traditional BC has less connection to MDP comparing with other prevalent methods, its efficiency is guaranteed, the tradeoff is that it suffers from the scenario when the agent visits an unseen state. Loss function $\mathcal{L}$ could be customized for specific problem formulation. Loss function (objective function) significantly influences the training process and there are many existing lost function available to measure the differences (in most cases, the difference means the 1 step deviation) such as $\ell_{1}$ loss, $\ell_{2}$ loss, KL divergence, Hinge Loss, etc. For example, when using KL divergence as the loss function, the objective policy could be obtained by minimizing the deviation between expert distribution $q \pi_{E}$</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Basic behavioural cloning method
    Collect expert demonstration into dataset \(\mathcal{D}\);
    Select policy representation \(\pi_{\theta}\) and loss function \(\mathcal{L}\);
    Use \(\mathcal{D}\) to optimize the loss function \(\mathcal{L}\) based on policy representation \(\pi_{\theta}\);
    return optimized policy representation \(\pi_{\theta}\);
</code></pre></div>

<p>and induced distribution $q(\pi)$, i.e.</p>
<p>$$
\pi^{*}=\underset{\pi}{\operatorname{argmin}} D_{K L}\left(q\left(\pi_{E}\right) | q(\pi)\right)
$$</p>
<p>BC could be subdivided into model-free BC and model-based BC methods. The main difference is whether the method learns a forward model to estimate the system dynamics. Since model-free BC methods take no consideration on the context, model-free BC methods perform well in industry applications where accurate controllers are available and experts could control and modify the robot joints. However, model-free BC methods typically are hard to predict future states and could not guarantee the output's feasibility under the environment that an accurate controller is not available. Under this kind of "imperfect" environment, the agent would have limited information of system dynamics and usually gets stuck into the unseen scenarios due to the "compounding error"[55]. While model-based BC methods leverage the environment information and learn the dynamics iteratively to produce feasible output, the trade-off is that model-based BC methods usually have greater time-complexity since the iterative learning involvement process.</p>
<p>One of the significant BC method is DAgger, which is a model-free BC method proposed by Ross et al.[57] and the idea is to use dataset aggregation to improve the generalization on unseen scenario. Algorithm 2 presents the abstract process of DAgger. DAgger adopts iterative learning process and mixes a new policy $\hat{\pi}^{n+1}$ with probability $\beta$ to construct the next policy. The mixing parameter is a set of $\left{\beta_{i}\right}$ that satisfies $\frac{1}{N} \sum_{i=1}^{N} \beta_{i} \rightarrow 0$. The start-up policy is learned by BC and records the trajectory into the dataset. Since a small difference can lead to compounding error, new unseen trajectories will be recorded combining with the expert's corrections. In this case, the algorithm gradually updates the possible state and fully leverages the presence of expert. Later research like[29, 38, 56, 67, 73, 75] were proposed to make improvements on DAgger. This method alleviates the problem that traditional BC methods perform poorly on the unseen scenario and achieve data-efficiency comparing with previous methods like SMILe[55]. However, it does have drawbacks, such as DAgger involves frequent interaction with the expert which might not be available and expensive in some cases (e.g., enquiring expert correction could be expensive in interdisciplinary tasks). Recent methods such as[13, 25] successfully alleviate this problem. Another problem of DAgger could be that cost of each action is ignored. Since DAgger is evaluated on video games where the actions have equal cost, the cost of implementing each action is not obvious like tasks such as navigation tasks. This problem is solved later by Ross and Bagnell[56].</p>
<h1>5.2 Inverse Reinforcement Learning</h1>
<p>Inverse reinforcement learning was firstly proposed by Russell[58]. Unlike BC, the IRL agent is recovering and evaluating the reward function from expert demonstrations iteratively instead of establishing a mapping from states to actions. The choice of choosing BC or IRL depends on the problem settings. When the problem setting weights more on system dynamics and future prediction is necessary, choosing IRL methods can be more likely to evaluate the given context iteratively and provide a more accurate prediction. On the other hand, when an accurate controller</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">DAgger</span><span class="w"> </span><span class="p">[</span><span class="mi">57</span><span class="p">]</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">);</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">hat</span><span class="p">{</span>\<span class="n">pi</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Pi</span>\<span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span>\<span class="n">rightarrow</span><span class="w"> </span><span class="n">N</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">Let</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">=</span>\<span class="n">beta_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="o">+</span>\<span class="n">left</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span>\<span class="n">beta_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">hat</span><span class="p">{</span>\<span class="n">pi</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">T</span><span class="o">-</span><span class="n">step</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="n">using</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Get</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">=</span>\<span class="n">left</span>\<span class="p">{</span>\<span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}(</span><span class="n">s</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">visited</span><span class="w"> </span><span class="n">states</span><span class="w"> </span><span class="n">by</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="ow">and</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">expert</span><span class="o">.</span>
<span class="w">        </span><span class="n">Aggregate</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Train</span><span class="w"> </span><span class="n">classifier</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">hat</span><span class="p">{</span>\<span class="n">pi</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">on</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">best</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">hat</span><span class="p">{</span>\<span class="n">pi</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">validation</span><span class="o">.</span>
</code></pre></div>

<p>Algorithm 3 Classic feature matching IRL method
Require: The set of demonstrated trajectories $\mathcal{D}$;
1: Initialize reward function parameter $\omega$ and policy parameter $\theta$;
2: repeat
3: $\quad$ Evaluate current policy $\pi_{\theta}$ state-action visitation
3: $\quad$ frequency $u$;
4: $\quad$ Evaluate loss function $\mathcal{L}$ w.r.t. $u$ and the dataset $\mathcal{D}$
4: distribution;
5: Update the reward function parameter $\omega$ based on
5: $\quad$ the loss function;
6: Update the policy parameter $\theta$ in the inner loop RL
6: $\quad$ method using the updated reward parameter $\omega$;
7: until
8: return optimized policy representation $\pi_{\theta}$;
and abundant demonstrations are available, choosing BC methods usually takes less time and performs better.</p>
<p>IRL commonly assumes that the demonstrations are under Markov Decision Process setting and since the reward $\mathbb{R}$ is unknown, the set of states is used to estimate the feature vector (i.e. $\phi: \mathcal{X} \mapsto[0,1]^{k}$ ) instead of the true reward function (i.e. $\mathcal{X} \mapsto \mathbb{R}$ ). The process of classic IRL method (see Algorithm 3) is based on iteratively update the reward function parameter $\omega$ and policy parameter $\theta$. The reward function parameter $\omega$ is updated after the state-action visitation frequency $u$ are evaluated, and the way that $\omega$ is updated could vary, for example, Ziebart et al.[82] updated $\omega$ by maximizing the likelihood of the demonstration over maximum entropy distribution, i.e. $\omega^{*}=\operatorname{argmax}<em D="D" _in="\in" _tau="\tau">{\omega} \sum</em> \log P(\tau | \omega)$. On the other hand, the policy parameter $\theta$ is updated in the inner loop reinforcement learning process. This iterative and embedded structure can be problematic: the learning process could be time-consuming and impractical for high-dimensional problems like the high Degree Of Freedom (DOF) robotic problem. Another significant problem is "ill-posed" which means the many different cost functions could lead to the same action. In this case, the good IRL methods need to have more expressive power and a more efficient framework. Research such as $[9,15,21,30,41,50,54]$ was proposed to alleviate the above problems by using more expressive models like neural network and optimizing the input like ranking the demonstration in advance.</p>
<p>Several recent IRL methods are gradually integrated with various novel methods such as selfsupervised learning. Self-supervised learning means learning a function from a partially given context to the remaining or surrounding context. Nair et al. 43 could be one of the earliest researchers who adopt self-supervised learning into imitation learning. One important problem that integrating self-supervised learning with imitation learning has to solve is the huge amount of data, since the state and action space is extensive for real-world manipulation tasks. Nair et al. solved this problem by using the Baxter robot which automatically records data for a rope manipulation task. This method achieves practical improvement and provides a novel viewpoint for later research and leads the tendency of learning from the past. In 2018, Oh et al. [45] proposed self-IL, which tries to leverage past good experience to get better exploration result. The proposed method takes a initial policy as input. It then iteratively uses the current policy to generate trajectories, calculates the accumulated return value $R$, update the dataset $D \leftarrow D \bigcup\left{\left(s_{t}, a_{t}, R\right)\right}<em _theta="\theta">{t=0}^{T}$ and finally uses the deviation between accumulated return and the agent estimate value $R-V</em>\right}}$ to optimize the policy parameter $\theta$. The process gradually ranks the state-action pairs and updates the policy parameter from the high-ranked pairs. In addition, Self-IL integrates Q learning with policy gradient under the actor-critic framework. As the component of the loss function, policy gradient loss was used to determine the good experience and lower bound $Q$ learning was used to exploit the good experience, this helps Self-IL perform better in the hard exploration tasks. Similarly, in 74, Selfsupervised Imitation Learning (SIL) also tries to learn from its good experience but in a different structure. SIL creatively uses voice instruction in the imitation learning process. One language encoder is used to extract textual feature $\left{\omega_{t<em j="j">{t=1}^{n}$ and an attention-based trajectory encoder LSTM is use to encode the previous state-action as a history context vector from visual state $\left{v</em>\right}<em t="t">{j=1}^{m}$, i.e. $h</em>$ could be obtained based on the historical context vector, finally the action is predicted based on these parameters. The obtained experience is evaluated on a match critic, and the "good" experience is stored in a replay buffer for future prediction.}=L S T M\left(\left[v_{t}, a_{t-1}\right], h_{t-1}\right)$. Then visual context $c_{t}^{\text {visual }}$ and language context $c_{t}^{\text {text }</p>
<h1>5.3 Generative Adversarial Imitation Learning (GAIL)</h1>
<p>In order to mitigate problems in BC and IRL, Ho and Ermon [25] proposed a novel general framework called Generative adversarial imitation learning in 2016. GAIL builds a connection between GAN [23] and maximum entropy IRL [82]. Inheriting from the structure of GAN, GAIL consists of a generative model G and a discriminator D , while G generates data distribution $\rho_{\pi}$ integrating with true data distribution $\rho_{\pi E}$ to confuse D. GAIL works in an iterative fashion, and the formal objective of GAIL could be denoted as</p>
<p>$$
\min <em D="D" _in_0_1_S="\in(0,1)^{S" _pi="\pi" _times="\times">{\pi} \max </em>}} \hat{\mathbb{E}<em t="t">{\tau</em>}}\left[\log \left(D_{\omega}(s, a)\right)\right]+\hat{\mathbb{E}<em E="E">{\tau</em>(s, a)\right)\right]
$$}}\left[\log \left(1-D_{\omega</p>
<p>GAIL firstly samples trajectories from initial policy, then these generated trajectories are used to update the discriminator weight $\omega$ by applying an Adam gradient step on equation</p>
<p>$$
\hat{\mathbb{E}}<em t="t">{\tau</em>}}\left[\nabla_{\omega} \log \left(D_{\omega}(s, a)\right)\right]+\hat{\mathbb{E}<em E="E">{\tau</em>(s, a)\right)\right]
$$}}\left[\nabla_{\omega} \log \left(1-D_{\omega</p>
<p>and maximize this equation with respect to D. Then adopting the TRPO 60 with the cost function $\log \left(D_{\omega_{t+1}}(s, a)\right)$ to update the policy parameter $\theta$ and minimize the above function with respect to $\pi$, combining with a causal entropy regularizer controlled by non-negative parameter $\lambda$, i.e.</p>
<p>$$
\hat{\mathbb{E}}<em t="t">{\tau</em>\right)
$$}}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) \mathcal{Q}(s, a))\right]-\lambda \nabla_{\theta} H\left(\pi_{\theta</p>
<p>where $\mathcal{Q}(\bar{s}, \bar{a})=\hat{\mathbb{E}}<em t="t">{\tau</em>\right]$.
The abstract training process is presented in Algorithm 4. By adopting TRPO, the policy could be more resistant and stable to the noise in the policy gradient. Unlike DAgger and other previous}}\left[\log \left(D_{\omega_{t+1}}(s, a)\right) \mid s_{0}=\bar{s}, a_{0}=\bar{a</p>
<div class="codehilite"><pre><span></span><code>Algorithm 4 GAIL [25]
Require: Expert trajectories \(\tau_{E} \sim \pi_{E}\), initial policy and discriminator parameter \(\theta_{0}, \omega_{0}\)
    for \(i=0,1,2, \ldots\) do
        Sample trajectories \(\tau_{i} \sim \pi_{\theta_{i}}\).
        Update the discriminator parameters \(\omega_{i}\) to \(\omega_{i+1}\).
        Update the policy parameter \(\theta_{i}\) to \(\theta_{i+1}\).
    end for
</code></pre></div>

<p>Table 4. Different Kinds of Derivative on GAIL</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GAILs</th>
<th style="text-align: left;">Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Make further improvement</td>
<td style="text-align: left;">MGAIL[7], InfoGAIL[35]</td>
</tr>
<tr>
<td style="text-align: left;">Apply to other research question</td>
<td style="text-align: left;">MAGAIL[63], GAIfO[70]</td>
</tr>
<tr>
<td style="text-align: left;">Other generative model</td>
<td style="text-align: left;">Diverse GAIL[76], GIRL[77]</td>
</tr>
</tbody>
</table>
<p>algorithms, GAIL is more sample-efficiency from the perspective of using expert data and does not require expert interaction during the training process, it also presents adequate capacity dealing with the high-dimensional domain and changes in distribution. While the trade-off is the training process involves frequent interaction with the environment and could be more fragile and not stable for saddle point problem. As for the first problem, the authors suggested to initialize the policy with BC so that the amount of environment interaction would reduce. As for the second problem, recent research such as[3] tries to alleviate this problem by formulating the distribution-matching problem as an iterative lower-bound optimization problem.</p>
<p>Inspired by GAIL's presence, there is a bunch of research proposed to make further development on GAIL (see Table 4) and adversarial structured IL gradually becomes a category. In terms of "make further improvement", many proposed methods modify and improve GAIL from different perspectives. For example, MGAIL[7] uses an advanced forward model to make the model differentiable so that the Generator could use the exact gradient of the Discriminator. InfoGAIL[35] modifies GAIL by adopting WGAN instead of GAN. Other recent work like GoalGAIL [17], TRGAIL[33] and DGAIL[83] are all making improvement on GAIL by combining with other method like hindsight relabeling and Deep Deterministic Policy Gradient (DDPG) [36] to achieve faster convergence and better final performance. In terms of "apply to other research question", some of the proposed methods combine other method with GAIL and apply to various problems. For example, in[66], FAIL outperforms GAIL on sparse reward problem without using the ground truth action and achieves both sample and computational efficiency. It integrates adversarial structure with minimax theory, which is used to determines the next time step policy $\pi_{h}$ under the assumption that $\left{\pi_{1}, \pi_{2}, \ldots, \pi_{h-1}\right}$ is learned and fixed. GAIL is also applied into the other research area, such as multiagent settings $[8,63,78]$ and IfO settings[70] to effectively deal with more dynamic environment. In terms of "combine IL with other generative model", a number of recent research adopt other generative models to facilitate learning process, for example, in[76], Variational AutoEncoder(VAE) is integrated with IL by using encoder to map from trajectories to an embedding vector $z$, which makes the proposed algorithm to behave diversely with relatively less demonstration and achieve one-shot learning for the new trajectory. Other research like GIRL[77] also achieves the outstanding performance from limited demonstrations using VAE.</p>
<p>Table 5. Publication Related to IfO</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Publication</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">IfO[40]</td>
<td style="text-align: center;">Learning policy from aligned observation only</td>
</tr>
<tr>
<td style="text-align: center;">BCO[69]</td>
<td style="text-align: center;">Adopting IfO setting and integrating with BC</td>
</tr>
<tr>
<td style="text-align: center;">TCN[61]</td>
<td style="text-align: center;">Multi-viewpoint self-supervised IfO method</td>
</tr>
<tr>
<td style="text-align: center;">One-shot IfO[5]</td>
<td style="text-align: center;">Extracting features from unlabeled and <br> unaligned gameplay footage</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot Visual Imitation[51]</td>
<td style="text-align: center;">Using distance between observations to predict and <br> penalize the actions</td>
</tr>
<tr>
<td style="text-align: center;">IfO survey[72]</td>
<td style="text-align: center;">Detailed classified recent IfO methods</td>
</tr>
<tr>
<td style="text-align: center;">Imitating Latent Policies <br> from Observation[19]</td>
<td style="text-align: center;">Infering latent policies directly from state observations</td>
</tr>
<tr>
<td style="text-align: center;">GAIfO[70]</td>
<td style="text-align: center;">Generative adversarial structure aggregating with IfO</td>
</tr>
<tr>
<td style="text-align: center;">IfO Leveraging Proprioception[71]</td>
<td style="text-align: center;">Leveraging internal information of the agent</td>
</tr>
<tr>
<td style="text-align: center;">OPOLO[81]</td>
<td style="text-align: center;">Using dual-form of the expectation function and <br> adversarial structure to achieve off-policy IfO</td>
</tr>
</tbody>
</table>
<h1>5.4 Imitation from Observation (IfO)</h1>
<p>The prevalent methods introduced above is almost using sequences of state-action pairs to form trajectories as the input data. This kind of data preparation process could be laborious and this is a kind of waste for the abundant raw unlabeled videos. This problem got mitigated after IfO[40] was proposed, and IL algorithms start to advocate this novel settings and make use of raw videos to learn policies. Comparing with traditional IL methods, this algorithm is more intuitive, and it follows the nature of how human and animal imitate. For example, people learn to dance by following a video, this kind of following process is achieved though detecting the changes of poses and taking actions to match the pose, which is similar to how IfO solves the problem. Different from traditional IL, the ground truth action sequence is not given. Similar to IRL, the main objective of IfO is the reward function from demonstration videos. Imitation from observation tries to build connection for different context so that the VAE structure is adopted to encode both the context (environment) of demonstrator (expert) $s_{1}$ and target context $s_{2}$. The proposed model has four components: a source observation encoder $\operatorname{Enc}<em t="t">{1}\left(o</em>}^{i}\right)$ which extracts feature vector $z_{1}$, a target observation encoder $\operatorname{Enc<em 0="0">{2}\left(o</em>\right]}^{j}\right)$ which extracts feature vector $z_{2}$, a translator $z_{3}$ and a target context decoder $\operatorname{Dec}\left(z_{3}\right)$. The model takes two sets of observations $\left(D_{i}=\left[o_{t}^{i<em j="j">{t=0}^{T}\right.$ and $D</em>\right]}=\left[o_{t}^{j<em 3="3">{t=0}^{T}$ as source observation and target observation respectively) as input, then using these two sets to predict the future observation in target context under the assumption that source observation and target observation are time aligned. The translator $z</em>}$ translates features in $z_{1}$ produced by source encoder into the context of $z_{2}$ produced by another encoder, i.e. $z_{3}=T\left(z_{1}, z_{2}\right)$, then the translated feature vector $z_{3}$ is decoded into the observation $\hat{o<em _text="\text" _trans="{trans">{t}^{j}$. The model is working in a supervised learning process with the loss function $\mathcal{L}</em>}}=\left|\left(\hat{o<em t="t">{t}^{j}\right)-o</em>$. To improve the performance, the final objective of the proposed model is}^{j}\right|_{2}^{2</p>
<p>combined with the loss of VAE reconstruction and the loss of time alignment, i.e.</p>
<p>$$
\mathcal{L}=\sum_{(i, j)}\left(\mathcal{L}<em 1="1">{\text {trans }}+\lambda</em>} \mathcal{L<em 2="2">{\text {rec }}+\lambda</em>\right)
$$} \mathcal{L}_{\text {align }</p>
<p>where $\lambda_{1}$ and $\lambda_{2}$ are the hyperparameter predetermined in advance. The output reward function consists of two parts, the first one is deviation penalty on squared Euclidean distance, which measures the difference between the encoded learner's observation feature and translated expert observation feature in learner's context, i.e.</p>
<p>$$
\hat{R}<em t="t">{\text {feat }}\left(0</em>
$$}^{l}\right)=-\left|E n c_{1}\left(o_{t}^{l}\right)-\frac{1}{n} \sum_{t=0}^{T} T\left(o_{t}^{i}, o_{0}^{l}\right)\right|_{2}^{2</p>
<p>The second part is the penalty which ensures the current observation keeping similar with translated observations, i.e.</p>
<p>$$
\hat{R}<em t="t">{\text {img }}\left(0</em>
$$}^{l}\right)=-\left|o_{t}^{l}-\frac{1}{n} \sum_{t=0}^{T} M\left(o_{t}^{i}, o_{0}^{l}\right)\right|_{2}^{2</p>
<p>where M is the full observation translation model. The proposed reward function could be applied into the any reinforcement learning algorithm, Liu et al. uses TRPO[60] for the simulation experiments.</p>
<p>After IfO being proposed, measuring observation distance to replace the ground truth action becomes a prevalent setting in imitation learning. In Table 5, we present some of the research advocate this new insight and apply this idea into various domain. Both BC, IRL and GAIL start to adopt this setting to simplify the input. For example, in [5], raw unaligned YouTube videos are used for imitation to reproduce the behavior for games. YouTube videos are relatively noisy and varying in settings like resolution. The proposed method successfully handled these problems by using a novel self-supervised objective to learn a domain-invariant representation from videos. Similarly, in[61], multi-viewpoint self-supervised IL method Time-Contrastive Network (TCN) was proposed. Different viewpoints introduce a wide range of contexts about the task environment and the goal is to learn invariant representation about the task. By measuring the distance between the input video frames and "looking at itself in the mirror", the robot could learn its internal joint to learn the mapping and achieve imitating demonstration.</p>
<h1>6 CHALLENGES AND OPPORTUNITIES</h1>
<p>Although improvements like integrating novel techniques, reducing human interaction during training and simplifying inputs alleviate difficulties in learning behaviour, there are still some open challenges for IL:</p>
<p>Diverse behavior learning: Current IL methods commonly use task-specific training datasets to learn to reproduce single behavior. Research like[76] presented diverse behavior learning by combining adversarial structure and variational autoencoder, but this is still an open challenge. Other methods could be adopted to optimize IL, such as transfer learning might help the agent to learn from similar tasks so that the training process could be more efficient.</p>
<p>Sub-optimal demonstration for training: Current IL methods generally require a highquality set of demonstrations for training. However, the number of high-quality demonstrations could be limited and expensive to obtain. Existing research like $[9,17,64]$ have shown the possibility to use sub-optimal demonstration for training, but performance can be improved by extracting common intent from the dataset.</p>
<p>Imitation not just from observation: Current IfO methods commonly use raw videos and the deviation of observations to recover the reward function. But the video is not just observation,</p>
<p>maybe the voice instruction could also be used to get a better reward function. Wang et al. 74 ] demonstrated using natural language for navigation tasks, but it could be an interesting topic to explore in the IfO settings.</p>
<p>Better representation: Good policy representation could benefit the training process to achieve data-efficiency and computation-efficiency. Finding better policy representation is still an active research topic for IL. Besides policy representation, how to represent the demonstration is another problem in IL. The representation of demonstration needs to be more efficient and expressive.</p>
<p>Find globally optimal solution: Most research is finding a locally optimal solution based on demonstration, which might set the upper-bound for the agent performance. The future direction could be finding the global optimal for a specific task, which requires the agent to understand the intent of the behavior instead of copy-pasting. Current research like[77] successfully surpasses the demonstrator's performance, but finding the global optimal still needs effort.</p>
<h1>7 CONCLUSION</h1>
<p>Imitation learning achieves outstanding performance in a wide range of problems, ranging from solving hard exploration Atari games to achieving object manipulation while avoiding obstacles by robotic arm. Different kinds of imitation learning methods make contribution to this significant development, such as BC methods replicate behavior more intuitively where the environmental parameters could be easily obtained; IRL methods achieve data-efficiency and future behavior prediction when problems weight more on environment dynamics and care less about training time; adversarial structured IL methods eliminate expert interaction during the training process and present adequate capacity dealing with the high-dimensional problem. While IL methods continue to grow and develop, IL is also seeking breakthroughs in settings, like IfO methods simplify the input by replacing the need of action labels when the input demonstrations are raw video. Although recent work presents a superior advantage in replicating behavior, taxonomy ambiguity exists as the presence of GAIL and its derivatives break out of the previous classification framework. To alleviate this ambiguity, we analyzed the traditional taxonomies of IL and proposed new taxonomies that draw clearer boundaries between methods. Despite the success of IL, challenges and opportunities exist, such as diverse behavior learning, leveraging sub-optimal demonstration and voice instruction, better representation, and finally finding the globally optimal solution. Future work is expected to unravel IL and its practical applications.</p>
<h2>REFERENCES</h2>
<p>[1] Pieter Abbeel, Adam Coates, and Andrew Y. Ng. 2010. Autonomous Helicopter Aerobatics through Apprenticeship Learning. The International Journal of Robotics Research 29, 13 (Nov. 2010), 1608-1639. https://doi.org/10.1177/ 0278364910371999
[2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in neural information processing systems. 5048-5058.
[3] Oleg Arenz and Gerhard Neumann. 2020. Non-Adversarial Imitation Learning and its Connections to Adversarial Methods. arXiv:2008.03525 [cs, math, stat] (Aug. 2020). http://arxiv.org/abs/2008.03525 arXiv: 2008.03525.
[4] Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. 2019. Deep reinforcement learning from policydependent human feedback. arXiv preprint arXiv:1902.04257 (2019).
[5] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando de Freitas. 2018. Playing hard exploration games by watching YouTube. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Associates, Inc., 2930-2941. http://papers. nips.cc/paper/7557-playing-hard-exploration-games-by-watching-youtube.pdf
[6] Michael Bain and Claude Sammut. 1999. A framework for behavioural cloning. In Machine Intelligence 15. Oxford University Press, 103-129.
[7] Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. 2017. End-to-end differentiable adversarial imitation learning. In International Conference on Machine Learning. 390-399.</p>
<p>[8] Raunak P. Bhattacharyya, Derek J. Phillips, Blake Wulfe, Jeremy Morton, Alex Kuefler, and Mykel J. Kochenderfer. 2018. Multi-Agent Imitation Learning for Driving Simulation. arXiv:1803.01044 [cs] (March 2018). http://arxiv.org/abs/ 1803.01044 arXiv: 1803.01044.
[9] Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. 2019. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In International Conference on Machine Learning. PMLR, $783-792$.
[10] Daniel S. Brown, Wonjoon Goo, and Scott Niekum. 2019. Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations. arXiv:1907.03976 [cs.LG]
[11] Andreas Bühler, Adrien Gaidon, Andrei Cramariuc, Rares Ambrus, Guy Rosman, and Wolfram Burgard. 2020. Driving Through Ghosts: Behavioral Cloning with False Positives. arXiv:2008.12969 [cs.CV]
[12] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. 2015. Learning to search better than your teacher. In International Conference on Machine Learning. PMLR, 2058-2066.
[13] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl. 2020. Learning by cheating. In Conference on Robot Learning. PMLR, 66-75.
[14] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and Alexey Dosovitskiy. 2018. End-to-End Driving Via Conditional Imitation Learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA). 4693-4700. https://doi.org/10.1109/ICRA.2018.8460487 ISSN: 2577-087X.
[15] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayaraman, Akshara Rai, and Franziska Meier. 2020. Model-Based Inverse Reinforcement Learning from Visual Demonstrations. arXiv:2010.09034 [cs] (Oct. 2020). http://arxiv.org/abs/ 2010.09034 arXiv: 2010.09034.
[16] Hal Daumé, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine learning 75, 3 (2009), $297-325$.
[17] Yiming Ding, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. 2019. Goal-conditioned imitation learning. arXiv preprint arXiv:1906.05838 (2019).
[18] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. 2017. One-Shot Imitation Learning. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 1087-1098. http://papers.nips.cc/paper/6709-one-shot-imitation-learning.pdf
[19] Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. 2019. Imitating latent policies from observation. In International Conference on Machine Learning. PMLR, 1755-1763.
[20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. PMLR, 1126-1135.
[21] Chelsea Finn, Sergey Levine, and Pieter Abbeel. 2016. Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning. PMLR, 49-58.
[22] Laurent George, Thibault Buhet, Emilie Wirbel, Gaetan Le-Gall, and Xavier Perrotton. 2018. Imitation Learning for End to End Vehicle Longitudinal Control with Forward Camera. arXiv:1812.05841 [cs] (Dec. 2018). http://arxiv.org/ abs/1812.05841 arXiv: 1812.05841.
[23] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Networks. arXiv:1406.2661 [stat.ML]
[24] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z. Leibo, and Audrunas Gruslys. 2017. Deep Q-learning from Demonstrations. arXiv:1704.03732 [cs] (Nov. 2017). http://arxiv.org/abs/1704.03732 arXiv: 1704.03732.
[25] Jonathan Ho and Stefano Ermon. 2016. Generative Adversarial Imitation Learning. In Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Eds.). Curran Associates, Inc., 4565-4573. http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf
[26] X. Hu, J. Liu, J. Ma, Y. Pan, and L. Zhang. 2020. Fine-Grained 3D-Attention Prototypes for Few-Shot Learning. Neural Computation 32, 9 (2020), 1664-1684. https://doi.org/10.1162/neco_a_01302
[27] Z. Hu, Z. Gan, W. Li, J. Z. Wen, D. Zhou, and X. Wang. 2020. Two-Stage Model-Agnostic Meta-Learning With Noise Mechanism for One-Shot Imitation. IEEE Access 8 (2020), 182720-182730. https://doi.org/10.1109/ACCESS.2020.3029220 Conference Name: IEEE Access.
[28] Ahmed Hussein, Eyad Elyan, Mohamed Medhat Gaber, and Chrisina Jayne. 2018. Deep imitation learning for 3D navigation tasks. Neural Comput \&amp; Applic 29, 7 (April 2018), 389-404. https://doi.org/10.1007/s00521-017-3241-z
[29] Mostafa Hussein, Brendan Crowe, Marek Petrik, and Momotaz Begum. 2021. Robust Maximum Entropy Behavior Cloning. arXiv preprint arXiv:2101.01251 (2021).
[30] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. 2018. Reward learning from human preferences and demonstrations in atari. arXiv preprint arXiv:1811.06521 (2018).</p>
<p>[31] Parham M. Kebria, Abbas Khosravi, Syed Moshfeq Salaken, and Saeid Nahavandi. 2020. Deep imitation learning for autonomous vehicles based on convolutional neural networks. IEEE/CAA Journal of Automatica Sinica 7, 1 (Jan. 2020), 82-95. https://doi.org/10.1109/JAS.2019.1911825 Conference Name: IEEE/CAA Journal of Automatica Sinica.
[32] Beomjoon Kim and Joelle Pineau. 2013. Maximum Mean Discrepancy Imitation Learning. In Robotics: Science and Systems IX. Robotics: Science and Systems Foundation. https://doi.org/10.15607/RSS.2013.IX. 038
[33] Akira Kinose and Tadahiro Taniguchi. 2020. Integration of imitation learning using GAIL and reinforcement learning using task-achievement rewards via probabilistic graphical model. Advanced Robotics (June 2020), 1-13. https: //doi.org/10.1080/01691864.2020.1778521
[34] A. Lemme, Y. Meirovitch, M. Khansari-Zadeh, T. Flash, A. Billard, and J. J. Steil. 2015. Open-source benchmarking for learned reaching motion generation in robotics. Paladyn, Journal of Behavioral Robotics 6, 1 (Jan. 2015). https: //doi.org/10.1515/pjbr-2015-0002
[35] Yunzhu Li, Jiaming Song, and Stefano Ermon. 2017. InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 3812-3822. http://papers.nips.cc/paper/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations.pdf
[36] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. Continuous control with deep reinforcement learning. arXiv:1509.02971 [cs.LG]
[37] Rudolf Lioutikov, Gerhard Neumann, Guilherme Maeda, and Jan Peters. 2017. Learning movement primitive libraries through probabilistic segmentation. The International Journal of Robotics Research 36, 8 (July 2017), 879-894. https: //doi.org/10.1177/0278364917713116 Publisher: SAGE Publications Ltd STM.
[38] Evan Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, and Junwhan Ahn. 2020. An imitation learning approach for cache replacement. In International Conference on Machine Learning. PMLR, 6237-6247.
[39] Mengyue Liu, Jun Liu, Yihe Chen, Meng Wang, Hao Chen, and Qinghua Zheng. 2019. AHNG: representation learning on attributed heterogeneous network. Information Fusion 50 (2019), 221-230.
[40] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. 2018. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 1118-1125.
[41] Sha Luo, Hamidreza Kasaei, and Lambert Schomaker. 2021. Self-Imitation Learning by Planning. arXiv preprint arXiv:2103.13834 (2021).
[42] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. 2020. Learning latent plans from play. In Conference on Robot Learning. PMLR, 1113-1132.
[43] Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, and Sergey Levine. 2017. Combining self-supervised learning and imitation for vision-based rope manipulation. In 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2146-2153.
[44] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. 2018. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 6292-6299.
[45] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. 2018. Self-imitation learning. In International Conference on Machine Learning. PMLR, 3878-3887.
[46] Takayuki Osa, Amir M. Ghalamzan Esfahani, Rustam Stolkin, Rudolf Lioutikov, Jan Peters, and Gerhard Neumann. 2017. Guiding Trajectory Optimization by Demonstrated Distributions. IEEE Robotics and Automation Letters 2, 2 (April 2017), 819-826. https://doi.org/10.1109/LRA.2017.2653850 Conference Name: IEEE Robotics and Automation Letters.
[47] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, and Jan Peters. 2018. An Algorithmic Perspective on Imitation Learning. FNT in Robotics 7, 1-2 (2018), 1-179. https://doi.org/10.1561/2300000053
[48] Takayuki Osa, Naohiko Sugita, and Mamoru Mitsuishi. 2014. Online Trajectory Planning in Dynamic Environments for Surgical Task Automation. In Robotics: Science and Systems X. Robotics: Science and Systems Foundation. https: //doi.org/10.15607/RSS.2014.X. 011
[49] Takayuki Osa, Naohiko Sugita, and Mamoru Mitsuishi. 2018. Online Trajectory Planning and Force Control for Automation of Surgical Tasks. IEEE Transactions on Automation Science and Engineering 15, 2 (April 2018), 675691. https://doi.org/10.1109/TASE.2017.2676018 Conference Name: IEEE Transactions on Automation Science and Engineering.
[50] Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and Dorsa Sadigh. 2019. Learning Reward Functions by Integrating Human Demonstrations and Preferences. arXiv:1906.08928 [cs] (June 2019). http://arxiv.org/abs/1906.08928 arXiv: 1906.08928.
[51] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Fred Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. 2018. Zero-Shot Visual Imitation. In 2018 IEEE/CVF Conference</p>
<p>on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, Salt Lake City, UT, USA, 2131-21313. https: //doi.org/10.1109/CVPRW.2018.00278
[52] Dean A. Pomerleau. 1989. ALVINN: An Autonomous Land Vehicle in a Neural Network. In Advances in Neural Information Processing Systems 1, D. S. Touretzky (Ed.). Morgan-Kaufmann, 305-313. http://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf
[53] Dean A. Pomerleau. 1991. Efficient training of artificial neural networks for autonomous navigation. Neural computation 3, 1 (1991), 88-97.
[54] Siddharth Reddy, Anca D. Dragan, and Sergey Levine. 2019. SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards. arXiv:1905.11108 [cs, stat] (Sept. 2019). http://arxiv.org/abs/1905.11108 arXiv: 1905.11108.
[55] Stéphane Ross and Drew Bagnell. 2010. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 661-668.
[56] Stephane Ross and J Andrew Bagnell. 2014. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979 (2014).
[57] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 627-635.
[58] Stuart Russell. 1998. Learning agents for uncertain environments (extended abstract). In Proceedings of the eleventh annual conference on Computational learning theory - COLT' 98. ACM Press, Madison, Wisconsin, United States, 101-103. https://doi.org/10.1145/279943.279964
[59] Tim Salimans and Richard Chen. 2018. Learning Montezuma's Revenge from a Single Demonstration. arXiv preprint arXiv:1812.03381 (2018).
[60] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International conference on machine learning. PMLR, 1889-1897.
[61] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. 2018. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 1134-1141.
[62] Zhenyu Shou, Xuan Di, Jieping Ye, Hongtu Zhu, Hua Zhang, and Robert Hampshire. 2020. Optimal passenger-seeking policies on E-hailing platforms using Markov decision process and imitation learning. Transportation Research Part C: Emerging Technologies 111 (Feb. 2020), 91-113. https://doi.org/10.1016/j.trc.2019.12.005
[63] Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. 2018. Multi-Agent Generative Adversarial Imitation Learning. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Associates, Inc., 7461-7472. http://papers.nips.cc/paper/7975-multi-agent-generative-adversarial-imitation-learning.pdf
[64] Lingyun Song, Jun Liu, Mingxuan Sun, and Xuequn Shang. 2020. Weakly Supervised Group Mask Network for Object Detection. International Journal of Computer Vision (2020). https://doi.org/10.1007/s11263-020-01397-w
[65] Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. 2017. Third-person imitation learning. arXiv preprint arXiv:1703.01703 (2017).
[66] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. 2019. Provably efficient imitation learning from observation alone. In International Conference on Machine Learning. PMLR, 6036-6045.
[67] Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. 2017. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In International Conference on Machine Learning. PMLR, $3309-3318$.
[68] Ajay Kumar Tanwani, Pierre Sermanet, Andy Yan, Raghav Anand, Mariano Phielipp, and Ken Goldberg. 2020. Motion2Vec: Semi-supervised representation learning from surgical videos. In 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2174-2181.
[69] Faraz Torabi, Garrett Warnell, and Peter Stone. 2018. Behavioral cloning from observation. arXiv preprint arXiv:1805.01954 (2018).
[70] Faraz Torabi, Garrett Warnell, and Peter Stone. 2018. Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158 (2018).
[71] Faraz Torabi, Garrett Warnell, and Peter Stone. 2019. Imitation Learning from Video by Leveraging Proprioception. arXiv:1905.09335 [cs, stat] (June 2019). http://arxiv.org/abs/1905.09335 arXiv: 1905.09335.
[72] Faraz Torabi, Garrett Warnell, and Peter Stone. 2019. Recent advances in imitation learning from observation. arXiv preprint arXiv:1905.13566 (2019).
[73] Stephen Tu, Alexander Robey, and Nikolai Matni. 2021. Closing the closed-loop distribution shift in safe imitation learning. arXiv preprint arXiv:2102.09161 (2021).
[74] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. 2019. Reinforced cross-modal matching and self-supervised imitation learning for vision-language</p>
<p>navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6629-6638.
[75] Xiaojie Wang, Zhaolong Ning, Song Guo, Miaowen Wen, and Vincent Poor. 2021. Minimizing the age-of-criticalinformation: an imitation learning-based scheduling approach under partial observations. IEEE Transactions on Mobile Computing (2021).
[76] Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. 2017. Robust Imitation of Diverse Behaviors. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 5320-5329. http://papers.nips. cc/paper/7116-robust-imitation-of-diverse-behaviors.pdf
[77] Xingrui Yu, Yueming Lyu, and Ivor Tsang. 2020. Intrinsic reward driven imitation learning via generative model. In International Conference on Machine Learning. PMLR, 10925-10935.
[78] Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. 2018. Generating multi-agent trajectories using programmatic weak supervision. arXiv preprint arXiv:1803.07612 (2018).
[79] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. 2018. Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA). 5628-5635. https://doi.org/10.1109/ICRA.2018.8461249 ISSN: 2577-087X.
[80] Yang Zhou, Rui Fu, Chang Wang, and Ruibin Zhang. 2020. Modeling Car-Following Behaviors and Driving Styles with Generative Adversarial Imitation Learning. Sensors 20, 18 (Sept. 2020), 5034. https://doi.org/10.3390/s20185034
[81] Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. 2021. Off-policy imitation learning from observations. arXiv preprint arXiv:2102.13185 (2021).
[82] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. 2008. Maximum entropy inverse reinforcement learning.. In Aaai, Vol. 8. Chicago, IL, USA, 1433-1438.
[83] Guoyu Zuo, Kexin Chen, Jiahao Lu, and Xiangsheng Huang. 2020. Deterministic generative adversarial imitation learning. Neurocomputing 388 (May 2020), 60-69. https://doi.org/10.1016/j.neucom.2020.01.016
[84] Guoyu Zuo, Qishen Zhao, Kexin Chen, Jiangeng Li, and Daoxiong Gong. 2020. Off-policy adversarial imitation learning for robotic tasks with low-quality demonstrations. Applied Soft Computing Journal 97 (2020), 106795. https: //doi.org/10.1016/j.asoc.2020.106795</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1850 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1850</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1850</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-8199c9d55dd998f69f703e0ad250ca0697e3ad27</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8199c9d55dd998f69f703e0ad250ca0697e3ad27" target="_blank">NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The NavGPT is introduced, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN).</p>
                <p><strong>Paper Abstract:</strong> Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1850.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1850.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NavGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NavGPT — an LLM-based instruction-following navigation agent using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses a large pretrained language model (GPT-4) as the decision-making core to perform zero-shot Vision-and-Language Navigation (VLN) by consuming language descriptions of visual observations and emitting explicit reasoning and discrete navigation actions (viewpoint IDs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4 (used inside NavGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>GPT-4, a large pre-trained generative language model, is used as the reasoning and policy module in NavGPT. The model receives prompts containing (1) natural-language descriptions of current multi-view visual observations (from BLIP-2 + detectors), (2) a summarized navigation history, (3) navigation system rules, and (4) the instruction, and it outputs a textual reasoning trace and a concrete action by selecting an exact viewpoint ID (Action Input). Integration is implemented via a prompt manager + an action_maker tool that executes viewpoint-ID actions in the simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale natural language corpora / language data (text)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper beyond 'language corpus' used to train GPT-4; the paper explicitly states NavGPT does not learn from VLN datasets and that the LLM's parameters Θ originate from pretraining on language data (details of GPT-4 pretraining are not reported in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) — R2R (Room-to-Room) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Indoor instruction-following navigation in the Matterport3D-based R2R dataset: given a natural-language instruction describing a path, the agent navigates a pre-defined viewpoint graph (discrete viewpoints, each with multiple egocentric views) to reach a target viewpoint within 3 meters. Evaluation metrics include Trajectory Length (TL, meters), Navigation Error (NE, meters), Success Rate (SR, % within 3m), Oracle SR (OSR, %) and SPL (%).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural-language reasoning and action specification: the LLM emits textual 'Thought' traces followed by a textual 'Action' and exact 'Action Input' which is the target viewpoint ID (a discrete identifier); the LLM thus operates in a high-level symbolic / language action space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation on a pre-defined navigation graph: selecting the next adjacent viewpoint by its unique ID (discrete viewpoint transitions) and implicit relative-heading changes; includes an explicit stop/finished output.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct symbolic mapping: the prompt manager translates visual input into natural-language observations; the LLM selects a navigable viewpoint by emitting the exact viewpoint ID as 'Action Input'; an action_maker tool executes that viewpoint transition in the simulator. The system follows a ReAct-style loop (LLM produces Thoughts then an Action).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Language descriptions of RGB views (BLIP-2 captions), object detections (Faster-RCNN bounding boxes + class labels), and depth at object center from the Matterport3D simulator; a summarizer (GPT-3.5) is used to compress history into sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Zero-shot NavGPT with GPT-4 on R2R val-unseen: Trajectory Length (TL)=11.45 m, Navigation Error (NE)=6.46 m, Oracle Success Rate (OSR)=42%, Success Rate (SR)=34%, SPL=29% (values reported in the paper's Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-shot usage: no task-specific finetuning on VLN (0 training episodes on R2R); the agent operates purely from pretraining and prompt-time inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified in sample-count terms; NavGPT demonstrates zero-shot competence (no VLN training) but no measured sample-efficiency comparison because no finetuning curve is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale language knowledge, emergent multi-step reasoning/planning ability from the LLM, ability to generate explicit reasoning traces and map those to discrete viewpoint IDs, and integration with visual translators (BLIP-2 + detectors) that provide language-form perceptual input.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Information loss in translating high-dimensional visual observations into compact language captions and further loss via history summarization; mismatch between rich visual grounding required for precise localization and the coarse language descriptions; limited object-tracking from language summaries; lack of direct multimodal end-to-end training for the visual-to-action mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained LLMs (GPT-4) can be used zero-shot to perform high-level planning and sequential action prediction in 3D embodied navigation when given language descriptions of perception and an API to execute discrete viewpoint actions; however, transfer is partial — NavGPT exhibits explicit, interpretable reasoning but underperforms specialized, finetuned VLN models due primarily to information loss in visual→language translation and history summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1850.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NavGPT (GPT-3.5 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NavGPT variant using GPT-3.5 as the language core</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A budget-friendly NavGPT configuration that substitutes GPT-4 with GPT-3.5 for reasoning and decision-making; used for ablation studies of visual input granularity and perceptual components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-3.5 (used inside NavGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>GPT-3.5 (a large pretrained language model) is used similarly to GPT-4 within the NavGPT system for zero-shot VLN; prompts include observations (BLIP-2 captions + detector outputs), history summaries, and the navigation system principle, and the model outputs Thoughts and Action Inputs (viewpoint IDs).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale natural language corpora / language data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; used as an off-the-shelf pretrained LLM for prompting and for summarization in components.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) — used for ablations</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same R2R VLN task; GPT-3.5 variant used in ablation experiments over a sampled 216-sample split (72 scenes, one trajectory each) to evaluate effect of visual granularity and adding object/depth information.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural-language outputs (Thoughts) and discrete viewpoint ID selection as Action Input.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation on R2R viewpoint graph (select next viewpoint ID).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Same action_maker tool mapping textual viewpoint-ID outputs to simulator transitions; the prompt manager forms the mapping by arranging candidate navigable viewpoints and their relative headings in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>BLIP-2 captions (multi-view summarised per heading), Faster-RCNN detections, and simulator depth for objects when included in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Quantitative full-run numbers not reported for GPT-3.5 full R2R val-unseen; ablation results report relative improvements: adding object information to BLIP-2 captions increased SR by 4.86% (relative) over the BLIP-2-only baseline; adding depth further increased SR by 0.7% and SPL by 1.29 (absolute percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-shot (no VLN finetuning). Ablation run used 216 sampled episodes for evaluation; no training samples required.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified (zero-shot evaluation only).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Same as GPT-4 case but with lower-capacity reasoning; benefits observed from adding structured perceptual signals (object lists and depth) into language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lower reasoning/planning capacity than GPT-4; still hampered by visual→language information bottleneck and history summarization losses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 can perform zero-shot VLN within the NavGPT framework and shows measurable sensitivity to richer perceptual inputs (object labels and depth) — adding these signals improves SR and SPL — but lacks the stronger planning/reasoning performance of GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1850.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DuET (Init. LXMERT) — No Train baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DuET model initialized with pre-trained LXMERT, evaluated without VLN fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparison baseline in Table 1 showing performance when a vision-language VLN architecture is initialized with a pre-trained V+L encoder (LXMERT) but not trained/fine-tuned on the VLN task (i.e., zero-shot transfer from V+L pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DuET (initialized with LXMERT weights, no VLN training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>DuET is a VLN architecture (dual-scale graph transformer) normally trained for VLN; in the table this variant is evaluated with only its LXMERT initialization and no subsequent VLN training to probe zero-shot transfer from vision-language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-and-language pretraining (LXMERT), i.e., image-text pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the entry in Table 1 denotes DuET initialized from an off-the-shelf LXMERT pre-trained encoder but without VLN finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) — val-unseen</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same R2R instruction-following task used for comparison in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Model expects cross-modal inputs (visual features + text instruction) and produces discrete navigation decisions (as typical for DuET), but in this zero-training setting no task-specific policy learning occurred.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation on R2R viewpoint graph.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Standard DuET internal mapping from cross-modal features to discrete navigation decisions; in the 'No Train' condition no VLN-specific adaptation was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual features extracted by the standard DuET/LXMERT pipeline (image regions + text); details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in Table 1 (No Train row): TL=22.03 m, NE=9.74 m, OSR=7%, SR=1%, SPL=0% — indicating very poor zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-shot (no VLN finetuning) — poor performance despite V+L pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Negative/none in this zero-train setting — pretraining alone (LXMERT init) did not yield useful zero-shot VLN behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained cross-modal representations exist in LXMERT that are useful when followed by finetuning, but alone they do not provide the structured decision-making needed for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lack of task-specific finetuning; mismatch between pretrained V+L feature objectives and sequential decision-making required for navigation; no explicit action-mapping mechanism like viewpoint-ID emission was provided in zero-train evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A vision-and-language pretrained encoder (LXMERT) without VLN finetuning yields negligible zero-shot navigation performance, underscoring that pretraining alone (without task adaptation) is insufficient for effective embodied navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1850.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E — an embodied multimodal language model (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of integrating large language models into embodied robotics; referenced as prior work exploring multimodal LLMs for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E (reference mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as an example in related work of an LLM integrated into embodied robotics; the present paper does not use or evaluate PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal (language + vision / embodied data) — not detailed in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotics / embodied tasks (general) — not specific in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as part of the trend of integrating LLMs into robotics; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1850.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan — grounding language in robotic affordances (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in the introduction as prior work that integrates language models with robotic affordance grounding; cited to motivate using LLM knowledge for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SayCan (reference mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as an example of combining LLMs with low-level affordance modules for robotics; not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language models + robotic affordance datasets (not detailed here)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic manipulation/navigation (general) — not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work demonstrating the potential of LLMs + affordances for real-world robotics; no details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1850.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-Nav (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-Nav — Robotic navigation with large pre-trained models of language, vision, and action (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as recent prior work exploring the use of large pretrained models for robotic navigation and code generation; cited in context of adopting LLMs for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LM-Nav (reference mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as an example of leveraging large pretrained models for navigation (reference only); not used or evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language/vision/action models (details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic navigation (general) — not specified</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work illustrating attempts to use large pretrained models for navigation; no further details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1850.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-Nav / CLIP (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP / CLIP-Nav — text-image matching based zero-shot navigation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited works that use CLIP-style text-image matching to perform zero-shot VLN or object navigation by matching instruction phrases to image views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP / CLIP-Nav (reference mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced as an approach relying on image-text matching for zero-shot VLN (e.g., chunking instructions into keyphrases and using CLIP for matching). Not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Contrastive image-text pretraining (image-caption corpora) — not detailed here</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Zero-shot Vision-and-Language Navigation / object navigation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as alternative zero-shot approaches based on text-image matching; no experimental details reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1850.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1850.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LGX / Dorbala et al. (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LGX — LLM-based zero-shot object navigation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work that uses LLMs for zero-shot object navigation (object-goal navigation) and GLIP for stop decision; mentioned to contrast NavGPT's instruction-following VLN focus and the ability to inspect LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LGX (reference mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Cited as work using LLMs for object navigation where agents are not required to follow instructions; this paper notes LGX uses GLIP and does not consider explicit memorization of navigation history + LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>LLM pretrained on language data; GLIP is used for visual grounding (details not provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Zero-shot object navigation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned to highlight differences (object navigation vs instruction-following and history handling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as I can and not as I say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>Can an embodied agent find your "cat-shaped mug"? llm-based zero-shot object navigation <em>(Rating: 2)</em></li>
                <li>Clip-n-Nav: Using CLIP for zero-shot vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>LXMERT: Learning cross-modality encoder representations from transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1850",
    "paper_id": "paper-8199c9d55dd998f69f703e0ad250ca0697e3ad27",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "NavGPT (GPT-4)",
            "name_full": "NavGPT — an LLM-based instruction-following navigation agent using GPT-4",
            "brief_description": "A system that uses a large pretrained language model (GPT-4) as the decision-making core to perform zero-shot Vision-and-Language Navigation (VLN) by consuming language descriptions of visual observations and emitting explicit reasoning and discrete navigation actions (viewpoint IDs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4 (used inside NavGPT)",
            "model_agent_description": "GPT-4, a large pre-trained generative language model, is used as the reasoning and policy module in NavGPT. The model receives prompts containing (1) natural-language descriptions of current multi-view visual observations (from BLIP-2 + detectors), (2) a summarized navigation history, (3) navigation system rules, and (4) the instruction, and it outputs a textual reasoning trace and a concrete action by selecting an exact viewpoint ID (Action Input). Integration is implemented via a prompt manager + an action_maker tool that executes viewpoint-ID actions in the simulator.",
            "pretraining_data_type": "Large-scale natural language corpora / language data (text)",
            "pretraining_data_details": "Not specified in this paper beyond 'language corpus' used to train GPT-4; the paper explicitly states NavGPT does not learn from VLN datasets and that the LLM's parameters Θ originate from pretraining on language data (details of GPT-4 pretraining are not reported in this work).",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) — R2R (Room-to-Room) benchmark",
            "embodied_task_description": "Indoor instruction-following navigation in the Matterport3D-based R2R dataset: given a natural-language instruction describing a path, the agent navigates a pre-defined viewpoint graph (discrete viewpoints, each with multiple egocentric views) to reach a target viewpoint within 3 meters. Evaluation metrics include Trajectory Length (TL, meters), Navigation Error (NE, meters), Success Rate (SR, % within 3m), Oracle SR (OSR, %) and SPL (%).",
            "action_space_text": "High-level natural-language reasoning and action specification: the LLM emits textual 'Thought' traces followed by a textual 'Action' and exact 'Action Input' which is the target viewpoint ID (a discrete identifier); the LLM thus operates in a high-level symbolic / language action space.",
            "action_space_embodied": "Discrete navigation on a pre-defined navigation graph: selecting the next adjacent viewpoint by its unique ID (discrete viewpoint transitions) and implicit relative-heading changes; includes an explicit stop/finished output.",
            "action_mapping_method": "Direct symbolic mapping: the prompt manager translates visual input into natural-language observations; the LLM selects a navigable viewpoint by emitting the exact viewpoint ID as 'Action Input'; an action_maker tool executes that viewpoint transition in the simulator. The system follows a ReAct-style loop (LLM produces Thoughts then an Action).",
            "perception_requirements": "Language descriptions of RGB views (BLIP-2 captions), object detections (Faster-RCNN bounding boxes + class labels), and depth at object center from the Matterport3D simulator; a summarizer (GPT-3.5) is used to compress history into sentences.",
            "transfer_successful": true,
            "performance_with_pretraining": "Zero-shot NavGPT with GPT-4 on R2R val-unseen: Trajectory Length (TL)=11.45 m, Navigation Error (NE)=6.46 m, Oracle Success Rate (OSR)=42%, Success Rate (SR)=34%, SPL=29% (values reported in the paper's Table 1).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Zero-shot usage: no task-specific finetuning on VLN (0 training episodes on R2R); the agent operates purely from pretraining and prompt-time inputs.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Not quantified in sample-count terms; NavGPT demonstrates zero-shot competence (no VLN training) but no measured sample-efficiency comparison because no finetuning curve is reported.",
            "transfer_success_factors": "Large-scale language knowledge, emergent multi-step reasoning/planning ability from the LLM, ability to generate explicit reasoning traces and map those to discrete viewpoint IDs, and integration with visual translators (BLIP-2 + detectors) that provide language-form perceptual input.",
            "transfer_failure_factors": "Information loss in translating high-dimensional visual observations into compact language captions and further loss via history summarization; mismatch between rich visual grounding required for precise localization and the coarse language descriptions; limited object-tracking from language summaries; lack of direct multimodal end-to-end training for the visual-to-action mapping.",
            "key_findings": "Pretrained LLMs (GPT-4) can be used zero-shot to perform high-level planning and sequential action prediction in 3D embodied navigation when given language descriptions of perception and an API to execute discrete viewpoint actions; however, transfer is partial — NavGPT exhibits explicit, interpretable reasoning but underperforms specialized, finetuned VLN models due primarily to information loss in visual→language translation and history summarization.",
            "uuid": "e1850.0",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NavGPT (GPT-3.5 baseline)",
            "name_full": "NavGPT variant using GPT-3.5 as the language core",
            "brief_description": "A budget-friendly NavGPT configuration that substitutes GPT-4 with GPT-3.5 for reasoning and decision-making; used for ablation studies of visual input granularity and perceptual components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "GPT-3.5 (used inside NavGPT)",
            "model_agent_description": "GPT-3.5 (a large pretrained language model) is used similarly to GPT-4 within the NavGPT system for zero-shot VLN; prompts include observations (BLIP-2 captions + detector outputs), history summaries, and the navigation system principle, and the model outputs Thoughts and Action Inputs (viewpoint IDs).",
            "pretraining_data_type": "Large-scale natural language corpora / language data",
            "pretraining_data_details": "Not specified in this paper; used as an off-the-shelf pretrained LLM for prompting and for summarization in components.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) — used for ablations",
            "embodied_task_description": "Same R2R VLN task; GPT-3.5 variant used in ablation experiments over a sampled 216-sample split (72 scenes, one trajectory each) to evaluate effect of visual granularity and adding object/depth information.",
            "action_space_text": "High-level natural-language outputs (Thoughts) and discrete viewpoint ID selection as Action Input.",
            "action_space_embodied": "Discrete navigation on R2R viewpoint graph (select next viewpoint ID).",
            "action_mapping_method": "Same action_maker tool mapping textual viewpoint-ID outputs to simulator transitions; the prompt manager forms the mapping by arranging candidate navigable viewpoints and their relative headings in the prompt.",
            "perception_requirements": "BLIP-2 captions (multi-view summarised per heading), Faster-RCNN detections, and simulator depth for objects when included in ablations.",
            "transfer_successful": true,
            "performance_with_pretraining": "Quantitative full-run numbers not reported for GPT-3.5 full R2R val-unseen; ablation results report relative improvements: adding object information to BLIP-2 captions increased SR by 4.86% (relative) over the BLIP-2-only baseline; adding depth further increased SR by 0.7% and SPL by 1.29 (absolute percentage points).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Zero-shot (no VLN finetuning). Ablation run used 216 sampled episodes for evaluation; no training samples required.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Not quantified (zero-shot evaluation only).",
            "transfer_success_factors": "Same as GPT-4 case but with lower-capacity reasoning; benefits observed from adding structured perceptual signals (object lists and depth) into language prompts.",
            "transfer_failure_factors": "Lower reasoning/planning capacity than GPT-4; still hampered by visual→language information bottleneck and history summarization losses.",
            "key_findings": "GPT-3.5 can perform zero-shot VLN within the NavGPT framework and shows measurable sensitivity to richer perceptual inputs (object labels and depth) — adding these signals improves SR and SPL — but lacks the stronger planning/reasoning performance of GPT-4.",
            "uuid": "e1850.1",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DuET (Init. LXMERT) — No Train baseline",
            "name_full": "DuET model initialized with pre-trained LXMERT, evaluated without VLN fine-tuning",
            "brief_description": "A comparison baseline in Table 1 showing performance when a vision-language VLN architecture is initialized with a pre-trained V+L encoder (LXMERT) but not trained/fine-tuned on the VLN task (i.e., zero-shot transfer from V+L pretraining).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "DuET (initialized with LXMERT weights, no VLN training)",
            "model_agent_description": "DuET is a VLN architecture (dual-scale graph transformer) normally trained for VLN; in the table this variant is evaluated with only its LXMERT initialization and no subsequent VLN training to probe zero-shot transfer from vision-language pretraining.",
            "pretraining_data_type": "Vision-and-language pretraining (LXMERT), i.e., image-text pretraining",
            "pretraining_data_details": "Not specified in this paper; the entry in Table 1 denotes DuET initialized from an off-the-shelf LXMERT pre-trained encoder but without VLN finetuning.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) — val-unseen",
            "embodied_task_description": "Same R2R instruction-following task used for comparison in Table 1.",
            "action_space_text": "Model expects cross-modal inputs (visual features + text instruction) and produces discrete navigation decisions (as typical for DuET), but in this zero-training setting no task-specific policy learning occurred.",
            "action_space_embodied": "Discrete navigation on R2R viewpoint graph.",
            "action_mapping_method": "Standard DuET internal mapping from cross-modal features to discrete navigation decisions; in the 'No Train' condition no VLN-specific adaptation was applied.",
            "perception_requirements": "Visual features extracted by the standard DuET/LXMERT pipeline (image regions + text); details not given in this paper.",
            "transfer_successful": false,
            "performance_with_pretraining": "Reported in Table 1 (No Train row): TL=22.03 m, NE=9.74 m, OSR=7%, SR=1%, SPL=0% — indicating very poor zero-shot transfer.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Zero-shot (no VLN finetuning) — poor performance despite V+L pretraining.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Negative/none in this zero-train setting — pretraining alone (LXMERT init) did not yield useful zero-shot VLN behavior.",
            "transfer_success_factors": "Pretrained cross-modal representations exist in LXMERT that are useful when followed by finetuning, but alone they do not provide the structured decision-making needed for VLN.",
            "transfer_failure_factors": "Lack of task-specific finetuning; mismatch between pretrained V+L feature objectives and sequential decision-making required for navigation; no explicit action-mapping mechanism like viewpoint-ID emission was provided in zero-train evaluation.",
            "key_findings": "A vision-and-language pretrained encoder (LXMERT) without VLN finetuning yields negligible zero-shot navigation performance, underscoring that pretraining alone (without task adaptation) is insufficient for effective embodied navigation.",
            "uuid": "e1850.2",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PaLM-E (mentioned)",
            "name_full": "PaLM-E — an embodied multimodal language model (mentioned)",
            "brief_description": "Mentioned as an example of integrating large language models into embodied robotics; referenced as prior work exploring multimodal LLMs for embodied tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E (reference mention only)",
            "model_agent_description": "Cited as an example in related work of an LLM integrated into embodied robotics; the present paper does not use or evaluate PaLM-E.",
            "pretraining_data_type": "Multimodal (language + vision / embodied data) — not detailed in this paper",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotics / embodied tasks (general) — not specific in this paper",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as part of the trend of integrating LLMs into robotics; no experimental details provided in this paper.",
            "uuid": "e1850.3",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SayCan (mentioned)",
            "name_full": "SayCan — grounding language in robotic affordances (mentioned)",
            "brief_description": "Referenced in the introduction as prior work that integrates language models with robotic affordance grounding; cited to motivate using LLM knowledge for embodied agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "SayCan (reference mention only)",
            "model_agent_description": "Cited as an example of combining LLMs with low-level affordance modules for robotics; not used or evaluated in this paper.",
            "pretraining_data_type": "Language models + robotic affordance datasets (not detailed here)",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic manipulation/navigation (general) — not specified here",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited as related work demonstrating the potential of LLMs + affordances for real-world robotics; no details in this paper.",
            "uuid": "e1850.4",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LM-Nav (mentioned)",
            "name_full": "LM-Nav — Robotic navigation with large pre-trained models of language, vision, and action (mentioned)",
            "brief_description": "Mentioned as recent prior work exploring the use of large pretrained models for robotic navigation and code generation; cited in context of adopting LLMs for navigation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "LM-Nav (reference mention only)",
            "model_agent_description": "Cited as an example of leveraging large pretrained models for navigation (reference only); not used or evaluated in this work.",
            "pretraining_data_type": "Pretrained language/vision/action models (details not provided in this paper)",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic navigation (general) — not specified",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as related work illustrating attempts to use large pretrained models for navigation; no further details in this paper.",
            "uuid": "e1850.5",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CLIP-Nav / CLIP (mentioned)",
            "name_full": "CLIP / CLIP-Nav — text-image matching based zero-shot navigation (mentioned)",
            "brief_description": "Cited works that use CLIP-style text-image matching to perform zero-shot VLN or object navigation by matching instruction phrases to image views.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "CLIP / CLIP-Nav (reference mention only)",
            "model_agent_description": "Referenced as an approach relying on image-text matching for zero-shot VLN (e.g., chunking instructions into keyphrases and using CLIP for matching). Not used experimentally in this paper.",
            "pretraining_data_type": "Contrastive image-text pretraining (image-caption corpora) — not detailed here",
            "pretraining_data_details": null,
            "embodied_task_name": "Zero-shot Vision-and-Language Navigation / object navigation (mentioned)",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as alternative zero-shot approaches based on text-image matching; no experimental details reported in this paper.",
            "uuid": "e1850.6",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LGX / Dorbala et al. (mentioned)",
            "name_full": "LGX — LLM-based zero-shot object navigation (mentioned)",
            "brief_description": "A referenced prior work that uses LLMs for zero-shot object navigation (object-goal navigation) and GLIP for stop decision; mentioned to contrast NavGPT's instruction-following VLN focus and the ability to inspect LLM reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "LGX (reference mention only)",
            "model_agent_description": "Cited as work using LLMs for object navigation where agents are not required to follow instructions; this paper notes LGX uses GLIP and does not consider explicit memorization of navigation history + LLM reasoning.",
            "pretraining_data_type": "LLM pretrained on language data; GLIP is used for visual grounding (details not provided here)",
            "pretraining_data_details": null,
            "embodied_task_name": "Zero-shot object navigation (mentioned)",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned to highlight differences (object navigation vs instruction-following and history handling).",
            "uuid": "e1850.7",
            "source_info": {
                "paper_title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as I can and not as I say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 2
        },
        {
            "paper_title": "Can an embodied agent find your \"cat-shaped mug\"? llm-based zero-shot object navigation",
            "rating": 2
        },
        {
            "paper_title": "Clip-n-Nav: Using CLIP for zero-shot vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "LXMERT: Learning cross-modality encoder representations from transformers",
            "rating": 2
        }
    ],
    "cost": 0.021326249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</h1>
<p>Gengze Zhou ${ }^{1}$ Yicong Hong ${ }^{2}$ Qi Wu ${ }^{1}$<br>${ }^{1}$ The University of Adelaide ${ }^{2}$ The Australian National University<br>{gengze.zhou, qi.wu01}@adelaide.edu.au yicong.hong@anu.edu.au https://github.com/GengzeZhou/NavGPT</p>
<h4>Abstract</h4>
<p>Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instructionfollowing navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.</p>
<h2>1 Introduction</h2>
<p>Amid the remarkable advances in large language model (LLM) training [54, 3, 9, 67, 61, 8, 4, 40], we note a shift towards integrating LLMs into embodied robotics tasks such as SayCan [1] and PaLM-E [13]. This trend stems from two primary considerations: the scale of training data and the scale of models. First, the development of techniques for processing textual information provides an abundant source of natural language training data for learning interdisciplinary and generalizable knowledge. Furthermore, by accessing unlimited language data, significant emergent abilities [62] are observed when scaling up the model, resulting in a remarkable enhancement in the reasoning capabilities when solving problems across wide domains. Consequently, training an LLM with unlimited language data is seen as a viable pathway toward realizing a universal embodied agent.
This insight has spurred the integration of LLMs into vision-and-language navigation (VLN) [2], an exploratory task toward achieving real-world instruction-following embodied agents. The latest research attempt to leverage GPT models [40, 3] to benefit navigation. For example, using LLMs as a parser for diverse language input [50] - extracting landmarks from instruction to support visual matching and planning, or leveraging LLMs' commonsense reasoning abilities [68, 11] to incorporate prior knowledge of inter-object correlations to extend agents' perception and facilitate</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The architecture of NavGPT. NavGPT synergizes reasoning and actions in LLMs to perform zero-shot Vision-and-Language Navigation following navigation system principles. It interactives with different visual foundation models to adapt multi-modality inputs, handle the length of history with a history buffer and a GPT-3.5 summarizer, and aggregate various sources of information through a prompt manager. NavGPT parse the generated results from LLMs (LLM Thoughts and LLM Action) to move to the next viewpoint.</p>
<p>The decision making. However, we notice that the reasoning ability of LLMs in navigation is still under-explored, <em>i.e.,</em> can LLMs understand the interactive world, the actions, and consequences in text form, and use all the information to solve a navigation task?</p>
<p>In light of this, we introduce NavGPT, a fully automatic LLM-based system designed for language-guided visual navigation, with the capability to handle multi-modality inputs, unconstrained language guidance, interaction with an open-world environment, and progress tracking with navigation history. NavGPT perceives the visual world by reading descriptions of observations generated by visual foundation models (VFMs), and synergizing Thoughts (reasoning) and Actions (decision making) in an explicit text form. To an extreme extent, we use NavGPT to perform zero-shot VLN to clearly reveal the reasoning process of LLMs during navigation.</p>
<p>Through comprehensive experiments, we found that LLMs possess the capability to execute complex navigational planning. This includes the deconstruction of instructions into distinct sub-goals, assimilation of commonsense knowledge pertinent to navigational tasks, identification of landmarks within the context of observed environments, continuous monitoring of navigational progression, and responding to anomalies by modifying their initial plan. The aforementioned phenomenon reflects an astonishing reasoning ability in understanding and solving navigation problems. Furthermore, we show that LLMs have the ability to draw navigation trajectories in a metric map and regenerate navigation instruction based on navigation history, revealing the historical and spatial awareness of LLMs for navigation tasks. However, there remains a significant gap between the zero-shot performance of current open-sourced LLMs in VLN compared to the fine-tuned models, where the bottleneck of NavGPT lies in the information loss while translating visual signals into natural language and summarizing observations into history. As a result, we suggest the future direction of building general VLN agents to be LLMs with multi-modality inputs or a navigation system making use of high-level navigation planning, historical and spatial awareness from LLMs.</p>
<p>Our contributions can be summarized as follow: (1) We introduce a novel instruction-following LLMs agent for visual navigation with a supportive system to interact with the environment and track navigation history. (2) We investigate the capabilities and limitations of current LLMs' reasoning for making navigation decisions. (3) We reveal the capability of LLMs in high-level planning for</p>
<p><sup>1</sup>Our NavGPT is solely powered by off-the-shelf LLMs, without any learnable module or any prior experience in solving interactive navigation. Hence, all navigation tasks defined in this paper are novel to NavGPT.</p>
<p>navigation, by observing the thoughts of LLMs, making the planning process of navigation agents accessible and explainable.</p>
<h1>2 Related Work</h1>
<p>Vision-and-Language Navigation Language-driven vision navigation is demanded by widely applicable embodied navigation agents. Previous study shows the essentials of modules to achieve such a goal [2, 46, 29, 30, 22, 19, 60, 72, 23, 25], whereas a large number of research reveal the crucial effect of training strategies [59, 53]. Importantly, the main problem lies in VLN is the generalizability of agents in unseen environments. Data augmentation [36, 58, 32, 53, 41, 15, 56], memory mechanism [6, 57, 42], pre-training [39, 21, 20, 65, 44] have been adopted to alleviate data scarcity. However, those augmentations and pre-training are limited to the sampled data from a fixed number of scenes, which is not enough to reflect a realistic application scene where objects could be out of the domains and language instructions are more diverse. In our work, we utilize the reasoning and knowledge storage of LLMs and perform VLN in a zero-shot manner as an initial attempt to reveal the potential usage of LLMs for VLN in the wild. A number of studies [5, 10, 7, 57] have presented compelling methodologies that underscore the significance of topological maps in facilitating long-term planning, specifically in the aspect of backtracking to prior locations. In addition, Dorbala et al. [12] use CLIP [47] to perform zero-shot VLN by chunking instructions into keyphrases and completely rely on the text-image matching capability from CLIP to navigate. However, the planning and decision making processes of the agents above are implicit and not accessible. On the contrary, benefiting from the intrinsic of LLMs, we are able to access the reasoning process of agents, making it explainable and controllable.</p>
<p>Large language models. With the massive success in large-scale language model training [54, $3,9,67,61,8]$, a new cohort of Large Language Models (LLMs) has shown evolutionary progress toward achieving Artificial General Intelligence (AGI) [4, 40]. This burgeoning class of LLMs, underpinned by increasingly sophisticated architectures and training methodologies [8, 48], has the potential to revolutionize various domains by offering unprecedented capabilities in natural language understanding and generation. The main concern for LLMs is that their knowledge is limited and confined after training is finished. The latest works study how to utilize LLMs interacting with tools to expand their knowledge as a plugin, including extending LLM to process multimodality content [64, 51], teaching LLMs to access the internet with correct API calls [49], and expanding their knowledge with local databases to accomplish QA tasks [43]. Another stream of works studies how to prompt LLMs in a hierarchical system to facilitate the alignment of reasoning and corresponding actions [66, 28] beyond the Chain of Thought (CoT) [63]. These works set up the preliminaries for building an embodied agent directly using LLMs.</p>
<p>LLMs in Robotics Navigation. The employment of Large Language Models (LLMs) in the field of robotics remains in the primary stage [55, 4]. A handful of contemporary studies, however, have begun to explore the utilization of generative models for navigation. Shah et al. [50] employs GPT-3 [3] in an attempt to identify "landmarks" or subgoals, while Huang et al. [27] concentrates its efforts on the application of an LLM for the generation of code. Zhou et al. [68] use LLM to extract the commonsense knowledge of the relations between targets and objects in observations to perform zero-shot object navigation (ZSON) [16, 38]. Despite these recent advancements, our study diverges in its concentration on converting visual scene semantics into input prompts for the LLM, directly performing VLN based on the commonsense knowledge and reasoning ability of LLMs. The work closest to ours is LGX [11], but they are doing object navigation where agents are not required to follow the instruction and in their method, they use the GLIP [33] model to decide the stop probability and did not consider memorization of navigation history, action, and reasoning between LLM.</p>
<h2>3 Method</h2>
<p>VLN Problem Formulation. We formulate the VLN problem as follows: Given a natural language instruction $\mathcal{W}$, composed of a series of words $\left{w_{1}, w_{2}, w_{3}, \ldots, w_{n}\right}$, at every step $s_{t}$, the agent interprets the current location via the simulator to obtain an observation $\mathcal{O}$. This observation comprises $N$ alternative viewpoints, representing the egocentric perspectives of agents in varying orientations.</p>
<p>Each unique view observation is denoted as $o_{i}(i \leqslant N)$, with its associated angle direction represented as $a_{i}(i \leqslant N)$. The observation can thus be defined as $\mathcal{O}<em 1="1">{t} \triangleq\left[\left\langle o</em>}, a_{1}\right\rangle,\left\langle o_{2}, a_{2}\right\rangle, \ldots,\left\langle o_{N}, a_{N}\right\rangle\right]$. Throughout the navigation process, the agents' action space is confined to the navigation graph $G$. The agent must select from the $M=\left|C_{t+1}\right|$ navigable viewpoints, where $C_{t+1}$ indicates the set of candidate viewpoints, by aligning the observation $\mathcal{O<em 1="1">{t}^{C} \triangleq\left[\left\langle o</em>}^{C}, a_{1}^{C}\right\rangle,\left\langle o_{2}^{C}, a_{2}^{C}\right\rangle, \ldots,\left\langle o_{M}^{C}, a_{M}^{C}\right\rangle\right]$ with the oracle $\mathcal{W}$. The agent prognosticates the subsequent action by selecting the relative angle $a_{i}^{C}$ from $\mathcal{O<em t="t">{t}^{C}$, then enacts this action through interaction with the simulator to transition from the current state $s</em>}=\left\langle v_{t}, \theta_{t}, \phi_{t}\right\rangle$ to $s_{t+1}=\left\langle v_{t+1}, \theta_{t+1}, \phi_{t+1}\right\rangle$, where $v, \theta$ and $\phi$ denotes the current viewpoint location, the current heading and elevation angle of the agent respectively. The agent also maintains a record of the state history $h_{t}$ and adjusts the conditional transition probability between states $\mathcal{S<em t_1="t+1">{t}=T\left(s</em>\right)$, where function $T$ denotes the conditional transition probability distribution.} \mid a_{i}^{C}, s_{t}, h_{t</p>
<p>In summary, the policy $\pi$ parametrized by $\Theta$ that the agent is required to learn is based on the oracle $\mathcal{W}$ and the current observation $\mathcal{O}<em i="i">{t}^{C}$, which is $\pi\left(a</em>} \mid \mathcal{W}, \mathcal{O<em t="t">{t}, \mathcal{O}</em> ; \Theta\right)$. In this study, NavGPT conducts the VLN task in a zero-shot manner, where the $\Theta$ is not learned from the VLN datasets, but from the language corpus that the LLMs are trained on.}^{C}, \mathcal{S}_{t</p>
<h1>3.1 NavGPT</h1>
<p>NavGPT is a system that interacts with environments, language guidance, and navigation history to perform action prediction. Let $\mathcal{H}<em 1="1">{&lt;t+1} \triangleq\left[\left\langle\mathcal{O}</em>}, \mathcal{R<em 1="1">{1}, \mathcal{A}</em>}\right\rangle,\left\langle\mathcal{O<em 2="2">{2}, \mathcal{R}</em>}, \mathcal{A<em t="t">{2}\right\rangle, \ldots,\left\langle\mathcal{O}</em>}, \mathcal{R<em t="t">{t}, \mathcal{A}</em>$, define as follow:}\right\rangle\right]$ be the navigation history of observation $\mathcal{O}$, LLM reasoning $\mathcal{R}$ and action $\mathcal{A}$ triplets for the previous $t$ steps. To obtain the navigation decision $\mathcal{A}_{t+1}$, NavGPT needs to synergize the visual perception from VFMs $\mathcal{F}$, language instruction $\mathcal{W}$, history $\mathcal{H}$ and navigation system principle $\mathcal{P}$ with the help of prompt manager $\mathcal{M</p>
<p>$$
\left\langle\mathcal{R}<em t_1="t+1">{t+1}, \mathcal{A}</em>}\right\rangle=L L M(\mathcal{M}(\mathcal{P}), \mathcal{M}(\mathcal{W}), \mathcal{M}\left(\mathcal{F}\left(\mathcal{O<em _t_1="&lt;t+1">{t}\right)\right), \mathcal{M}\left(\mathcal{H}</em>\right))
$$</p>
<p>Navigation System Principle $\mathcal{P}$. The Navigation System Principle formulates the behavior of LLM as a VLN agent. It clearly defines the VLN task and the basic reasoning format and rules for NavGPT at each navigation step. For example, NavGPT should move among the static viewpoints (positions) of a pre-defined graph of the environment by identifying the unique viewpoint ID. NavGPT should not fabricate nonexistent IDs. Details are discussed in section 3.4.
Visual Foundation Models $\mathcal{F}$. NavGPT as an LLM agent requires visual perception and expression ability from VFMs to translate the current environment's visual observation into natural language description. The VFMs here play the role of translator, to translate visual observations using their own language, e.g. natural language, objects' bounding boxes, and objects' depth. Through the process of prompt management, the visual perception results will be reformated and translated into pure natural language for LLMs to understand, discussed in section 3.2.
Navigation History $\mathcal{H}<em _t_1="&lt;t+1">{&lt;t+1}$. The navigation history is essential for NavGPT to evaluate the progress of the completion of the instruction, to update the current state, and make the following decisions. The history is composed of summarized descriptions of previous observations $\mathcal{O}</em>}$ and actions $\mathcal{A<em _t_1="&lt;t+1">{&lt;t+1}$, along with the reasoning thoughts $\mathcal{R}</em>$ from LLM, discussed in section 3.3.</p>
<p>Prompt Manager $\mathcal{M}$. The key to using LLM as a VLN agent is to convert all the above content into a natural language that LLM can understand. This process is done by the prompt manager, which collects the results from different components and parses them into a single prompt for LLM to make navigation decisions, discussed in section 3.4.</p>
<h3>3.2 Visual Perceptron for NavGPT</h3>
<p>In this section, we introduce the visual perception process of NavGPT. We take visual signals as a foreign language and handle the visual input using different visual foundation models to translate them into natural language, shown in figure 2.</p>
<p>For an agent standing at any viewpoint in the environment, the observation is composed of egocentric views from different orientations. The number of total views is defined by the field of view of each view image and the relative angle of each view. In our work, we set the field of view of each view as $45^{\circ}$, and turn the heading angle $\theta 45^{\circ}$ per view from $0^{\circ}$ to $360^{\circ}, 8$ directions in total. Besides, we</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The process of forming natural language description from visual input. We used 8 directions to represent a viewpoint and show the process of forming the descriptions for one of the directions.</p>
<p>turn the elevation angle $\phi 30^{\circ}$ per view from $30^{\circ}$ above the horizontal level to $30^{\circ}$ below, 3 levels in total. As a result, we obtain $3 \times 8=24$ egocentric views for each viewpoint.</p>
<p>To translate visual observation into natural language, we first utilize the BLIP-2 [31] model as the translator. With the strong text generation capability of LLMs, BLIP-2 can achieve stunning zero-shot image-to-text generation quality. By carefully setting the granularity of visual observation (field of views and the total view number in each observation), we prompt BILP-2 to generate a decent language description of each view with a detailed depiction of the shapes and color of objects and the scenes they are in while avoiding useless caption of views from a smaller FoV, from which partial observation is available and it is hard to recognize even for humans. See appendix for details.</p>
<p>Notice that for the heading direction, the rotation interval is equal to the field of view, therefore there is no overlapping between each orientation. For the elevations, there is a $15^{\circ}$ 's overlapping between the top, middle, and down views. In NavGPT we mainly focus on the heading angle of agents during navigation, therefore, we prompt GPT-3.5 to summarize the scenes from the top, middle, and down views for each orientation into a sentence of description.</p>
<p>Besides natural language descriptions of the scene from BLIP-2, we also excavate the lower-level feature extracted by other vision models. These vision models serve as auxiliary translators, translating visual input into their own "language" like the class of objects and corresponding bounding boxes. The detection results will be aggregated by the prompt manager into prompts for LLMs. In this work, we utilize Fast-RCNN [18] to extract the bounding boxes of objects in each egocentric view. After locating the objects, we calculate the relative heading angle for each object and the agent. We also extract the depth information of the center pixel of the object provided by the Matterport3D simulator [2]. With the depth, objects' relative orientation, and class, we filter the detection results by leaving the object within 3 meters from the current viewpoint. The results from VFMs will be processed by the prompt manager into observation for the current viewpoint in natural language.</p>
<h3>3.3 Synergizing reasoning and actions in LLMs</h3>
<p>In the VLN task, the agent needs to learn the policy $\pi(a_{t} \mid \mathcal{W}, \mathcal{O}<em t="t">{t}, \mathcal{O}</em>$, denoting the thought or reasoning trace of the agent.}^{C}, \mathcal{S}_{t} ; \Theta)$, which is difficult because the implicit connection between actions and observations and demain intensive computation. In order to explicitly access and enhance the agent's comprehension of the current state during navigation, we follow the ReAct paper [66] to expand the agent's action space to $\tilde{\mathcal{A}}=\mathcal{A} \cup \mathcal{R}$, where $\mathcal{R} \in \mathcal{L}$ is in the entire language space $\mathcal{L</p>
<p>The reasoning traces $\mathcal{R}$ of the agent will not trigger any interaction with the external environment, therefore no observation will be returned when the agent is outputting the reasoning during each navigation step. We synergize the NavGPT's actions and thoughts by prompting it to make navigation decisions after outputting the reasoning trace at each step. Introducing the reasoning traces aims to bootstrap the LLMs in two aspects:</p>
<p>Firstly, prompting the LLMs to think before choosing an action, enables LLMs to perform complex reasoning in planning and creating strategies to follow the instructions under the new observations. For example, as shown in figure 3, NavGPT can generate a long-term navigation plan by analyzing the current observation and the instruction, performing higher-level planning such as decomposing instruction and planning to reach the sub-goal, which is never seen explicitly in previous works.</p>
<p>Secondly, including reasoning traces $\mathcal{R}$ in the navigation history $\mathcal{H}_{&lt;t}$ enhances the problem-solving ability of NavGPT. By injecting reasoning traces into navigation history, NavGPT inherits from the previous reasoning traces, to reach a sub-goal with high-level planning consistently through steps, and can track the navigation progress with exception-handling abilities like adjusting the plan.</p>
<h1>3.4 NavGPT prompt manager</h1>
<p>With the Navigation System Principle $\mathcal{P}$, translated results from VFMs, and the History of Navigation $\mathcal{H}_{&lt;t}$, the prompt manager parses and reformates them into prompts for LLMs. Details of the prompt are presented in the appendix.</p>
<p>Specifically, for Navigation System Principle $\mathcal{P}$, NavGPT prompt manager will create a prompt to convey LLMs with the rules, declaring the VLN task definition, defining the simulation environment for NavGPT, and restricting LLMs' behavior in the given reasoning format.</p>
<p>For perception results from VFMs $\mathcal{F}$, the prompt manager gathers the results from each direction and orders the language description by taking the current orientation of NavGPT as the front, shown in figure 2, arranging the description from 8 directions into prompt by concatenating them clockwise.</p>
<p>For navigation history $\mathcal{H}<em i="i">{&lt;t+1}$, the observation, reasoning, and actions triples $\left\langle\mathcal{O}</em>}, \mathcal{R<em i="i">{i}, \mathcal{A}</em>\right\rangle$ are stored in a history buffer, shown in figure 1. Directly extracting all triples in the buffer will create too long a prompt for LLMs to accept. To handle the length of history, the prompt manager utilizes GPT-3.5 to summarize the observations from viewpoints in the trajectory, inserting the summarized observations into the observation, reasoning, and actions triples in the prompt.</p>
<h2>4 Experiment</h2>
<p>Implementation Details. We evaluate NavGPT based on GPT-4 [40] and GPT-3.5 on the R2R-VLN dataset [2]. The R2R dataset is composed of 7189 trajectories, each corresponding to three finegrained instructions. The dataset is separated into the train, val seen, val unseen, and test unseen splits, with $61,56,11$, and 18 indoor scenes, respectively. We apply the 783 trajectories in the 11 val unseen environments in all our experiments and for comparison to previous supervised approaches. We utilize BLIP-2 ViT-G FlanT5 ${ }_{\mathrm{XL}}$ [31] as images translator and Faster-RCNN [18] as object detector. The depth information of objects is extracted from the Mattport3D simulator [2] by taking the depth of the center pixel in the bounding box.</p>
<p>Evaluation Metrics. The evaluation of NavGPT utilizes standardized metrics from the R2R dataset. These include Trajectory Length (TL), denoting the average distance traveled by the agent; Navigation Error (NE), representing the mean distance from the agent's final location to the destination; Success Rate (SR), indicating the proportion of navigation episodes where the agent successfully reaches the target location within a 3-meter margin of error; Oracle Success Rate (OSR), the success rate of agent stopped at the closest point to the goal on its trajectory; and Success Rate weighted by the normalized inverse of Path Length (SPL), which is a more nuanced measure that balances navigation precision and efficiency by adjusting the success rate based on the ratio of the optimal path length to the agent's predicted path length.</p>
<h3>4.1 Qualitive Results</h3>
<p>We elaborately study the qualitative results of the reason trace from NavGPT. We reveal the potential high-level planning capability of GPT-4 under embodied navigation tasks.</p>
<p>Reasoning capability of GPT-4 for Language-guide Navigation As shown in figure 3, with GPT-4, NavGPT can perform various types of reasoning and high-level planning during navigation. For short instructions, NavGPT can track the navigation progress through steps to accomplish a single action described in the instructions, similar to the self-monitoring VLN agents [37, 70]. For long</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The qualitative of NavGPT. NavGPT can explicitly perform high-level planning for sequential action prediction, including decomposing instruction into sub-goal, integrating commonsense knowledge, identifying landmarks from observed scenes, tracking navigation progress, exceptions handling with plan adjustment.
instructions, NavGPT can break it down with sub-goals, similar to previous works on fine-graining R2R data [24, 22, 71], and plan to reach the destination by effectively identifying landmarks from observations, similar to works on utilizing objects information to perform cross-modality matching in VLN [17, 45, 44]. When navigating to a viewpoint with unexpected observation, NavGPT can plan to explore the environment and use commonsense knowledge to assist decision-making, similar to VLN methods incorporate external knowledge $[35,17,34]$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: We evaluate GPT-4 on a case where NavGPT successfully follows the ground truth path, using only the historical actions A&lt;t+1 and observations O&lt;t+1 to generate an instruction (without reasoning trace R&lt;t+1 to avoid information leaking), and using the entire navigation history H&lt;t+1 to draw a top-down trajectory.</p>
<h3>History and spatial relative relation awareness for LLMs during navigation</h3>
<p>We examined NavGPT's awareness of historical and spatial relations by employing GPT-4 to delineate the trajectory in navigational history and to construct a map of visited viewpoints utilizing pyplot. The process involved extracting exclusively the actions A&lt;t+1, observations O&lt;t+1, and the entire navigation history H&lt;t+1. The specifics of the prompt are presented in the appendix.</p>
<p>As shown in figure 4, we observed that GPT-4 could effectively extract landmarks from the redundant observation descriptions and generate navigation history descriptions with actions. This could be a potential way of generating new trajectory instructions for VLN. Besides, the result shows GPT-4 can comprehensively understand the history of navigation, and thus can perform the essential progress tracking during navigation. Moreover, shown in figure 4, GPT-4 can successfully catch the relative position relations between viewpoints and draw a top-down view of the trajectory for visited viewpoints. By providing language descriptions of actions taken by the agents, including the turning angle and relative distances between viewpoints, GPT-4 shows a stunning awareness of spatial relations. Such impressive reasoning ability support NavGPT to perform high-level planning shown in figure 3, underlines the significant potential LLMs hold for embodied navigation tasks.</p>
<h3>4.2 Comparison with Supervised Methods</h3>
<p>We compare the results of using NavGPT with GPT-4 to zero-shot the sequential navigation tasks with previous models trained on the R2R dataset. As shown in table 1, a significant discrepancy can be discerned. We suggest the limitations inhibiting the performance of LLMs in solving VLN can be primarily attributed to two factors: the precision of language-based depiction of visual scenes and the tracking capabilities regarding objects.</p>
<p>Table 1: Comparison with previous methods on R2R validation unseen split.</p>
<table>
<thead>
<tr>
<th>Training Schema</th>
<th>Method</th>
<th>TL</th>
<th>NE↓</th>
<th>OSR↑</th>
<th>SR↑</th>
<th>SPL↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train Only</td>
<td>Seq2Seq [2]</td>
<td>8.39</td>
<td>7.81</td>
<td>28</td>
<td>21</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Speaker Follower [14]</td>
<td>-</td>
<td>6.62</td>
<td>45</td>
<td>35</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>EnvDrop [53]</td>
<td>10.70</td>
<td>5.22</td>
<td>-</td>
<td>52</td>
<td>48</td>
</tr>
<tr>
<td>Pretrain + Finetune</td>
<td>PREVALENT [21]</td>
<td>10.19</td>
<td>4.71</td>
<td>-</td>
<td>58</td>
<td>53</td>
</tr>
<tr>
<td></td>
<td>VLN↑BERT [26]</td>
<td>12.01</td>
<td>3.93</td>
<td>69</td>
<td>63</td>
<td>57</td>
</tr>
<tr>
<td></td>
<td>HAMT [6]</td>
<td>11.46</td>
<td>2.29</td>
<td>73</td>
<td>66</td>
<td>61</td>
</tr>
<tr>
<td></td>
<td>DuET [7]</td>
<td>13.94</td>
<td>3.31</td>
<td>81</td>
<td>72</td>
<td>60</td>
</tr>
<tr>
<td>No Train</td>
<td>DuET (Init. LXMERT [52])</td>
<td>22.03</td>
<td>9.74</td>
<td>7</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>NavGPT (Ours)</td>
<td>11.45</td>
<td>6.46</td>
<td>42</td>
<td>34</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>NavGPT's functionality is heavily reliant on the quality of captions generated from VFMs. If the target object delineated in the instruction is absent in the observation description, NavGPT is compelled to explore the environment. The ideal circumstance entails all target objects being visible pursuant to the instruction. However, the inherent granularity of language description inevitably incurs a loss of information. Moreover, NavGPT must manage the length of the navigation history to prevent</p>
<p>excessively verbose descriptions as the steps accrue. To this end, a summarizer is implemented, albeit at the cost of further information loss. This diminishes NavGPT's tracking ability, impeding the formation of seamless perceptions of the entire environment as the trajectory lengthens.</p>
<h1>4.3 Effect of Visual Components</h1>
<p>We perform additional experiments to investigate the effectiveness of visual components in NavGPT, we construct a baseline with GPT-3.5 for its easier access and budget-friendly costs. To evaluate the zero-shot ability in various environments, we construct a new validation split sampling both from the original training set and the validation unseen set. The scenes from the training and validation unseen set are 61 and 11 respectively, 72 scenes in total. We randomly picked 1 trajectory from the 72 environment, each is associated with 3 instructions. In total, we sample 216 samples to conduct the ablation study.</p>
<h2>Effect of granularity in visual observation descriptions. The Field of View</h2>
<p>(FoV) of an image critically influences BILP-2's captioning ability, with an overly large FoV leading to generalized room descriptions and an extremely small FoV hindering object recognition due to limited content. As shown in table 2, we investigate 3 granularity of visual representation from a viewpoint. Specifically, variant $# 1$ utilizes an image with 60 FoV , turn heading angle 30 degrees clock-wise to obtain 12 views from a viewpoint, while variant $# 2$ and $# 3$ utilize an image with $30,45 \mathrm{FoV}$, turn elevation angle 30 degrees from top to down, and turn heading angle 30,45 degrees clockwise to form 36 views, 24 views respectively. From the results, we found that using FoV 45 with 24 views for a viewpoint generates the most suitable natural language description for navigation from the BILP-2 model. Using description under such granularity surpasses variant $# 1$ and $# 2$ by $6.48 \%$ and $2.78 \%$ respectively.</p>
<h2>Effect of semantic scene understanding</h2>
<p>and depth estimation. In addition to the granularity of natural language description of the environment, NavGPT also collaborates with other visual foundation models like object detectors and depth estimators to enhance the perception of the current environment. We investigate the effectiveness of adding the object information and the relative distance between the agent and the detected objects. We constructed a baseline method based on the caption results from BILP-2 and powered by GPT-3.5. As shown in table 3, by adding object information, the SR increase by $4.86 \%$ compared with the baseline, for the additional object information emphasizes the salient object in the scenes. Moreover, we observed a phenomenon in that agents failed to reach the destination because they do not know how close they are to the destination. Once the target viewpoint is visible in sight, they tend to stop immediately. Therefore by adding depth information, the agent has a better understanding of the current position and further rise the SR by $0.7 \%$ and SPL by 1.29 .</p>
<h2>5 Conclusion</h2>
<p>In this work, we explore the potential of utilizing LLMs in embodied navigation tasks. We present NavGPT, an autonomous LLM system specifically engineered for language-guided navigation, possessing the ability to process multi-modal inputs and unrestricted language guidance, engage with open-world environments, and maintain the navigation history. Limited by the quality of language description of visual scenes and the tracking abilities of objects, NavGPT's zero-shot performance on VLN is still not compatible with trained methods. However, the reasoning trace of GPT-4 illuminates the latent potential of LLMs in embodied navigation planning. Interaction of LLMs with downstream specialized models or the development of multi-modal LLMs for navigation, heralding the future of versatile VLN agents.</p>
<h1>References</h1>
<p>[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.1
[2] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $3674-3683,2018.1,3,5,6,8,15$
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1, 3
[4] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 1, 3
[5] K. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese. Topological planning with transformers for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11276-11286, 2021. 3
[6] S. Chen, P.-L. Guhur, C. Schmid, and I. Laptev. History aware multimodal transformer for vision-andlanguage navigation. Advances in Neural Information Processing Systems, 34:5834-5847, 2021. 3, 8
[7] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev. Think global, act local: Dual-scale graph transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16537-16547, 2022. 3, 8
[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. 1, 3
[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 1, 3
[10] Z. Deng, K. Narasimhan, and O. Russakovsky. Evolving graphical planner: Contextual global planning for vision-and-language navigation. Advances in Neural Information Processing Systems, 33:20660-20672, 2020. 3
[11] V. S. Dorbala, J. F. Mullen Jr, and D. Manocha. Can an embodied agent find your" cat-shaped mug"? llm-based zero-shot object navigation. arXiv preprint arXiv:2303.03480, 2023. 1, 3
[12] V. S. Dorbala, G. Sigurdsson, R. Piramuthu, J. Thomason, and G. S. Sukhatme. Clip-nav: Using clip for zero-shot vision-and-language navigation. arXiv preprint arXiv:2211.16649, 2022. 3
[13] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1
[14] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navigation. Advances in Neural Information Processing Systems, 31, 2018. 8
[15] T.-J. Fu, X. E. Wang, M. F. Peterson, S. T. Grafton, M. P. Eckstein, and W. Y. Wang. Counterfactual vision-and-language navigation via adversarial path sampler. In European Conference on Computer Vision, pages 71-86. Springer, 2020. 3
[16] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Open-vocabulary models are (almost) zero-shot object navigators. arXiv, 2022. 3
[17] C. Gao, J. Chen, S. Liu, L. Wang, Q. Zhang, and Q. Wu. Room-and-object aware knowledge reasoning for remote embodied referring expression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3064-3073, 2021. 7</p>
<p>[18] R. Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages $1440-1448,2015.5,6$
[19] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. Wang. Vision-and-language navigation: A survey of tasks, methods, and future directions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7606-7623, 2022. 3
[20] P.-L. Guhur, M. Tapaswi, S. Chen, I. Laptev, and C. Schmid. Airbert: In-domain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1634-1643, 2021. 3
[21] W. Hao, C. Li, X. Li, L. Carin, and J. Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13137-13146, 2020. 3, 8
[22] K. He, Y. Huang, Q. Wu, J. Yang, D. An, S. Sima, and L. Wang. Landmark-rxr: Solving vision-andlanguage navigation with fine-grained alignment supervision. Advances in Neural Information Processing Systems, 34:652-663, 2021. 3, 7
[23] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu, and S. Gould. Language and visual entity relationship graph for agent navigation. Advances in Neural Information Processing Systems, 33:7685-7696, 2020. 3
[24] Y. Hong, C. Rodriguez-Opazo, Q. Wu, and S. Gould. Sub-instruction aware vision-and-language navigation. arXiv preprint arXiv:2004.02707, 2020. 7
[25] Y. Hong, Z. Wang, Q. Wu, and S. Gould. Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15439-15449, 2022. 3
[26] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould. VLN $\circlearrowleft$ BERT: A recurrent vision-andlanguage bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1643-1653, 2021. 8
[27] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation. arXiv preprint arXiv:2210.05714, 2022. 3
[28] E. Karpas, O. Abend, Y. Belinkov, B. Lenz, O. Lieber, N. Ratner, Y. Shoham, H. Bata, Y. Levine, K. LeytonBrown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445, 2022. 3
[29] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In European Conference on Computer Vision, pages 104-120. Springer, 2020. 3
[30] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4392-4412, 2020. 3
[31] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 5, 6, 14
[32] J. Li, H. Tan, and M. Bansal. Envedit: Environment editing for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1540715417, 2022. 3
[33] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965-10975, 2022. 3
[34] M. Li, Z. Wang, T. Tuytelaars, and M.-F. Moens. Layout-aware dreamer for embodied referring expression grounding. In AAAI, 2023. 7
[35] X. Li, Y. Zhang, W. Yuan, and J. Luo. Incorporating external knowledge reasoning for vision-and-language navigation with assistant's help. Applied Sciences, 12(14):7053, 2022. 7
[36] C. Liu, F. Zhu, X. Chang, X. Liang, Z. Ge, and Y.-D. Shen. Vision-language navigation with random environmental mixup. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages $1644-1654,2021.3$</p>
<p>[37] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035, 2019. 6
[38] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra. Zson: Zero-shot object-goal navigation using multimodal goal embeddings. arXiv preprint arXiv:2206.12403, 2022. 3
[39] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra. Improving vision-and-language navigation with image-text pairs from the web. In European Conference on Computer Vision, pages 259-274. Springer, 2020. 3
[40] OpenAI. Gpt-4 technical report, 2023. 1, 3, 6
[41] A. Parvaneh, E. Abbasnejad, D. Teney, J. Q. Shi, and A. van den Hengel. Counterfactual vision-andlanguage navigation: Unravelling the unseen. Advances in Neural Information Processing Systems, 33:5296-5307, 2020. 3
[42] A. Pashevich, C. Schmid, and C. Sun. Episodic transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15942-15952, 2021. 3
[43] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023. 3
[44] Y. Qi, Z. Pan, Y. Hong, M.-H. Yang, A. van den Hengel, and Q. Wu. The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1655-1664, 2021. 3, 7
[45] Y. Qi, Z. Pan, S. Zhang, A. v. d. Hengel, and Q. Wu. Object-and-action aware model for visual language navigation. In European Conference on Computer Vision, pages 303-317. Springer, 2020. 7
[46] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. v. d. Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982-9991, 2020. 3
[47] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 3
[48] T. L. Scao, T. Wang, D. Hesslow, L. Saulnier, S. Bekman, M. S. Bari, S. Bideman, H. Elsahar, N. Muennghoff, J. Phang, et al. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022. 3
[49] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 3
[50] D. Shah, B. Osiński, S. Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on Robot Learning, pages 492-504. PMLR, 2023. 1, 3
[51] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface, 2023. 3
[52] H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100-5111, 2019. 8
[53] H. Tan, L. Yu, and M. Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In Proceedings of NAACL-HLT, pages 2610-2621, 2019. 3, 8
[54] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3
[55] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model abilities. 2023, 2023. 3
[56] H. Wang, W. Liang, J. Shen, L. Van Gool, and W. Wang. Counterfactual cycle-consistent learning for instruction following and generation in vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15471-15481, 2022. 3</p>
<p>[57] H. Wang, W. Wang, W. Liang, C. Xiong, and J. Shen. Structured scene memory for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8455-8464, 2021. 3
[58] S. Wang, C. Montgomery, J. Orbay, V. Birodkar, A. Faust, I. Gur, N. Jaques, A. Waters, J. Baldridge, and P. Anderson. Less is more: Generating grounded navigation instructions from landmarks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15428-15438, 2022. 3
[59] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y. Wang, and L. Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6629-6638, 2019. 3
[60] X. Wang, W. Xiong, H. Wang, and W. Y. Wang. Look before you leap: Bridging model-free and modelbased reinforcement learning for planned-ahead vision-and-language navigation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37-53, 2018. 3
[61] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 1, 3
[62] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 1
[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. 3
[64] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3
[65] S. Wu, X. Fu, F. Wu, and Z.-J. Zha. Cross-modal semantic alignment pre-training for vision-and-language navigation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4233-4241, 2022. 3
[66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 3, 5
[67] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 1, 3
[68] K. Zhou, K. Zheng, C. Pryor, Y. Shen, H. Jin, L. Getoor, and X. E. Wang. Esc: Exploration with soft commonsense constraints for zero-shot object navigation. arXiv preprint arXiv:2301.13166, 2023. 1, 3
[69] D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions, 2023. 18
[70] F. Zhu, Y. Zhu, X. Chang, and X. Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012-10022, 2020. 6
[71] W. Zhu, H. Hu, J. Chen, Z. Deng, V. Jain, E. Ie, and F. Sha. BabyWalk: Going farther in vision-andlanguage navigation by taking baby steps. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2539-2556. Association for Computational Linguistics, 2020. 7
[72] W. Zhu, Y. Qi, P. Narayana, K. Sone, S. Basu, X. E. Wang, Q. Wu, M. P. Eckstein, and W. Y. Wang. Diagnosing vision-and-language navigation: What really matters. In NAACL, 2022. 3</p>
<h1>Supplementary Material for NavGPT</h1>
<p>Section A provides additional details for NavGPT, including each component's prompt and examples of observation descriptions. The experimental setup for prompting GPT-4 to generate instructions and draw top-down trajectory is described in Section B. Section C illustrates the limitation of NavGPT with some failure cases. Finally, Section D discusses the broader impacts of our work.</p>
<h2>A Implementation Details</h2>
<h2>A. 1 Convert Visual Perception to Language Description ( $\S 3.2^{2}$ )</h2>
<p>For each viewpoint, given a heading direction $\theta$, we use elevation angles $-30^{\circ}, 0^{\circ}$ and $30^{\circ}$ to capture three egocentric images from down, middle and top to form the observation for this direction. The field of view of each image is $45^{\circ}$, so there is an overlapping of $15^{\circ}$ of the there images in the same direction. The visual perception process for each direction includes two steps, including using BILP-2 [31] to caption the three images, then, summarizing the descriptions by the GPT-3.5 summarizer.</p>
<p>BILP-2 Prompt We tried various ways to prompt the BILP-2 model given the images from a viewpoint. Such as no prompt, prompting it with "Detailly describe the scene." or "This is a scene of". Ultimately, we selected "This is a scene of" as the preferred prompt for BILP-2 to generate descriptions for each image. Utilizing no prompt can lead to inconsistent description lengths, occasionally rendering the description excessively brief. When applying the prompt "Detailly describe the scene.", the resulting description primarily centers on the room type, neglecting object details. In contrast, our chosen prompt yields language descriptions that are highly pertinent to indoor scenes and emphasize object depictions. The examples of the caption results are shown in figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The prompt for GPT-3.5 summarizer and the summarized results. The original descriptions from BLIP-2 are in orange.</p>
<p>GPT-3.5 Summarizer Prompt Descriptions from BILP-2 could have a substantial amount of redundancy because the same object could show up in the three images simultaneously. We adopt a GPT-3.5 summarizer to summarize them into one sentence following the template: "Here is a single scene view from top, down and middle: $\mathcal{A}$ description $\mathcal{B}$ Summarize the scene in one sentence:", where the " $[$ description $]$ " is replaced with the generated text of top, middle and down images from BILP-2, shown in figure 5.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Observation description examples Given the summarized description of each direction, along with the objects detected from the object detector, the agent interacts with the Matterport Simulator [2] to extract the depth and the navigable viewpoints information. The prompt manager will take the current heading of the agent as the "front" direction, and calculate the relative angle between the agent's current heading and the detected objects as well as the navigable viewpoints, concatenating the descriptions from each direction clockwise. The overall observation for a single viewpoint is shown in figure 6 .</p>
<h1>Observation for a viewpoint</h1>
<div class="codehilite"><pre><span></span><code>Front, range (left 23.50 to right 21.50):
    &#39;The scene features a brick wall with a fireplace, displaying a mix of brown, white, and yellow
    colors.&#39;
    Front Objects in 3m: None
    Front Navigable Viewpoints: None
    Front Right, range (right 21.50 to right 66.50):
    &#39;A brick wall with a doorway is illuminated by a light shining down on it.&#39;
    Front Right Objects in 3m: None
    Front Right Navigable Viewpoints: None
    Right, range (right 66.50 to right 111.50):
    &#39;A staircase with a bottle in it leads up to a brick archway with a staircase inside, all surrounded
    by a brick wall with a white circle on it.&#39;
    Right Objects in 3m: None
    Right Navigable Viewpoints:{&#39;1d337fde52e84923871db95009731c41&#39;: &#39;right 86.26, 2.03m&#39;}
    Rear Right, range (right 111.50 to right 156.50):
    &#39;A brick wall with a light shining through it and a brick pillar stands beneath a brick archway with
    a surrounding brick wall.&#39;
    Rear Right Objects in 3m: None
    Rear Right Navigable Viewpoints: None
    Rear, range (right 156.50 to left 158.50):
    &#39;A small room with a fireplace, vase, and hanging light fixture surrounded by brick walls.&#39;
    Rear Objects in 3m: {&#39;table&#39;: &#39;left 165.41, 1.75m&#39;}
    Rear Navigable Viewpoints:{&#39;4b587c327d8040feaa14adfeaaf6e84d&#39;: &#39;right 180.00, 1.07m&#39;}
    Rear Left, range (left 158.50 to left 113.50):
    &#39;A scene with a brick wall featuring vases, a picture of a man, and another brick wall from
    different perspectives.&#39;
    Rear Left Objects in 3m: {&#39;table&#39;: &#39;left 137.15, 1.75m&#39;}
    Rear Left Navigable Viewpoints: None
    Left, range (left 113.50 to left 68.50):
    &#39;The scene features a brick wall with a brown color and a white ceiling viewed from both the top and
    middle perspectives.&#39;
    Left Objects in 3m: None
    Left Navigable Viewpoints: None
    Front Left, range (left 68.50 to left 23.50):
    &#39;The scene features a brick wall with a corner, a brick floor, and a yellow and brown color scheme.&#39;
    Front Left Objects in 3m: None
    Front Left Navigable Viewpoints: None
</code></pre></div>

<h2>Summarized Observation in History</h2>
<p>Scene from the viewpoint is a small room with a brick wall, fireplace, vase, picture of a man, and various colors on the walls and ceiling, as well as a staircase and archway with brick walls and pillars.</p>
<p>Figure 6: The language description of observation for a single viewpoint.</p>
<h2>A. 2 NavGPT Prompt (\$3.4)</h2>
<p>Navigation System Principle The Navigation system principle for NavGPT is shown in figure 7, it defines the VLN task and the basic reasoning format and rules for NavGPT at each navigation step.
The " $[i n s t r u c t i o n]$ " and " $[i n i t _o b s e r v a t i o n]$ " in figure 7 will be replaced with the specific instruction and the language description of the starting point respectively.</p>
<p>History with Summarizer For history during navigation, directly using the description shown in figure 6 will be too long for LLMs to accept. We adopt a GPT-3.5 summarizer to summarize the observation into a sentence to replace the $\mathcal{O}<em _t_1="&lt;t+1">{&lt;t+1}$ in $\mathcal{H}</em>$. The prompt template we used is: "Given the description of a viewpoint. Summarize the scene from the viewpoint in one concise</p>
<p>You are an intelligent embodied agent that follows an instruction to navigate in an indoor environment. Your task is to move among the static viewpoints (positions) of a pre-defined graph of the environment, and try to reach the target viewpoint as described by the given instruction with the least steps.</p>
<p>At the beginning of the navigation, you will be given an instruction of a trajectory which describes all observations and the action you should take at each step.
During navigation, at each step, you will be at a specific viewpoint and receive the history of previous steps you have taken (containing your "Thought", "Action", "Action Input" and
"Observation" after the "Begin!" sign) and the observation of current viewpoint (including scene descriptions, objects, and navigable directions/distances within 3 meters).
Orientations range from -180 to 180 degrees: "0" signifies forward, "right 90" rightward, "right (or left) 180" backward, and "left 90" leftward.</p>
<p>You make actions by selecting navigable viewpoints to reach the destination. You are encouraged to explore the environment while avoiding revisiting viewpoints by comparing current navigable and previously visited IDs in previous "Action Input". The ultimate goal is to stop within 3 meters of the destination in the instruction. If destination visible but the target object is not detected within 3 meters, move closer.</p>
<p>At each step, you should consider:
(1) According to Current Viewpoint observation and History, have you reached the destination? If yes you should stop, output the 'Final Answer! Finished!' to stop.
If not you should continue:
(2) Consider where you are on the trajectory and what should be the next viewpoint to navigate according to the instruction.
Use the action_maker tool, input the next navigable viewpoint ID to move to that location. Show your reasoning in the Thought section.</p>
<p>Here are the descriptions of the action_maker tool:
Can be used to move to next adjacent viewpoint.
The input to this tool should be a viewpoint ID string of the next viewpoint you wish to visit. For example:
Action: action_maker
Action Input: "4a153b13a3f6424784cb8e5dabbb3a2c".
Every viewpoint has a unique viewpoint ID. You are very strict to the viewpoint ID and will never fabricate nonexistent IDs.</p>
<hr />
<p>Starting below, you should follow this format:
Instruction: an instruction of a trajectory which describes all observations and the actions should be taken
Initial Observation: the initial observation of the environment
Thought: you should always think about what to do next and why
Action: the action to take, must be one of the tools [action_maker]
Action Input: "Viewpoint ID"
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I have reached the destination, I can stop.
Final Answer! Finished!</p>
<hr />
<p>Begin!
Instruction: {instruction}
Initial Observation: {init_observation}
Thought: I should start navigation according to the instruction,</p>
<p>Figure 7: Navigation system principle prompt.
sentence. $\backslash n$ Description: $\backslash n$ [description] $\backslash n$ Summarization: The scene from the viewpoint is $a^{\prime \prime}$. where the " $[$ description $]^{\text {" }}$ is replaced with the textual description of the viewpoint. The summarized observation for viewpoints is shown in figure 6 . The observation in history $\mathcal{O}<em t_1="t+1">{&lt;t+1}$ is the summarized observation of viewpoint, while the current viewpoint observation $\mathcal{O}</em>$ is the full description.</p>
<h1>B Prompting GPT-4 to Generate Instruction and Draw Top-down Trajectory (§4.1)</h1>
<p>We choose a case where NavGPT successfully followed the ground truth trajectory, the history of NavGPT is shown in figure 8. The prompts for GPT-4 are shown in figure 9 .</p>
<p>Init Observation: Navigation start, no actions taken yet.
Current viewpoint "7e94b116c45043369954c6691acaeacd": Scene from the viewpoint is a view of a small apartment, including a sewing room, kitchen, bathroom, closet, and office.
Thought: first, I need to exit the sewing room. The only navigable viewpoint is in the front right direction, so I will move there.
Action: action_maker
Action Input: "cc0df704278a4f009b2239afd9a2ec9d"
Observation: Turn heading direction 61.44 degrees from right 26.01 to right 87.45 .
Current viewpoint "cc0df704278a4f009b2239afd9a2ec9d": Scene from the viewpoint is a view of a house with various hallways, rooms, and closets, featuring pictures, mirrors, figurines, and a violin hanging from the ceiling.
Thought: I have exited the sewing room. Now, I need to turn right and go toward the glass cabinet with the dolls in it. I see a room with a glass cabinet filled with figurines to my right. I will move to the navigable viewpoint in that direction.
Action: action_maker
Action Input: "da3ab1ba02824248bab4f6b3299b209d"
Observation: Turn heading direction 91.89 degrees from right 87.45 to right 179.34 .
Current viewpoint "da3ab1ba02824248bab4f6b3299b209d": Scene from the viewpoint is a view of a building with various hallways, rooms, and displays featuring chandeliers, pictures, mirrors, statues, and wooden floors.
Thought: I have reached the glass cabinet with the dolls in it. Now, I need to turn into the doorway on the left. I see a navigable viewpoint to my left, so I will move there.
Action: action_maker
Action Input: "d30ea508d12b4abdad3075f88e421c0b"
Observation: Turn heading direction -79.55 degrees from right 179.34 to right 99.78 .
Current viewpoint "d30ea508d12b4abdad3075f88e421c0b": Scene from the viewpoint is a room with wooden doors, a painting, a dresser, a bed, a bedside lamp, a glass case with figurines, and a statue of a horse on a shelf, with a ceiling fan and a light fixture.
Thought: I have turned into the doorway on the left. Now, I need to pass the bed and go through the next doorway on the left into the bathroom. I see a navigable viewpoint in the front right direction, closer to the bed. I will move there.
Action: action_maker
Action Input: "e841279ffc134a21b4b106cbda20862a"
Observation: Turn heading direction 14.29 degrees from right 99.78 to right 114.07 .
Current viewpoint "e841279ffc134a21b4b106cbda20862a": Scene from the viewpoint is a series of paintings, bedrooms, and rooms with various furnishings and decor, including hardwood floors, rugs, windows, lamps, and ceiling fans.
Thought: I have passed the bed. Now, I need to go through the next doorway on the left into the bathroom. I see a navigable viewpoint to my left, so I will move there.
Action: action_maker
Action Input: "408abc6abcd249ad92654606ba41fb80"
Observation: Turn heading direction -52.89 degrees from right 114.07 to right 61.18 .
Current viewpoint "408abc6abcd249ad92654606ba41fb80": Scene from the viewpoint is a view of a house, including various rooms and details such as doors, rugs, sinks, and paintings."
Thought: I have reached the bathroom. Now, I need to wait by the sink. I see a navigable viewpoint to my front left, closer to the sink. I will move there.
Action: action_maker
Action Input: "2389e105b2584b3997a819cfe0b99d30}
Observation: Turn heading direction -50.82 degrees from right 61.18 to right 10.36 .
Current viewpoint "2389e105b2584b3997a819cfe0b99d30": Scene from the viewpoint is a view of a house, including various rooms and hallways, with a focus on doors, bathroom fixtures, and framed pictures on the walls.
Thought: I have reached the sink as instructed, and the navigation is finished.
Action: Finished!
Figure 8: The history $\mathcal{H}_{&lt;t+1}$ of NavGPT.</p>
<h1>Prompt for drawing top-down trajectory</h1>
<div class="codehilite"><pre><span></span><code>Given the navigation history of an embodied agent, use pyplot to draw a top-down map of the
viewpoints, showing their relative positions.
Here is the prompt received by the agent:
{Navigation system prompt}
Here is its navigation history:
{Navigation History}
Use pyplot to draw a top-down map of the viewpoints, by recognizing the relative positions of
viewpoints in the agent&#39;s thoughts and observations.
</code></pre></div>

<h2>Prompt for Instruction Generation</h2>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">navigation</span><span class="w"> </span><span class="nv">history</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">embodied</span><span class="w"> </span><span class="nv">agent</span>,<span class="w"> </span><span class="nv">write</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">instruction</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">another</span><span class="w"> </span><span class="nv">agent</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">follow</span>
<span class="nv">such</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">trajectory</span>.
<span class="nv">Here</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">prompt</span><span class="w"> </span><span class="nv">received</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">agent</span>:
{<span class="nv">Navigation</span><span class="w"> </span><span class="nv">system</span><span class="w"> </span><span class="nv">prompt</span>}
<span class="nv">Here</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">its</span><span class="w"> </span><span class="nv">navigation</span><span class="w"> </span><span class="nv">history</span>:
{<span class="nv">Navigation</span><span class="w"> </span><span class="nv">History</span>}
</code></pre></div>

<p>Figure 9: Prompts for GPT-4 to generate instruction and draw top-down trajectory.</p>
<p>We further show the detail of prompts and NavGPT's response at each step in this example in figure 11-17. The beginning of each prompt, the "[Navigation system principles]" is replaced with the Navigation system principle prompt shown in figure 7.</p>
<h2>C NavGPT Failure cases (\$4.2)</h2>
<p>In this section, we show some failure cases of NavGPT to illustrate the limitation of our method, specifically pertaining to the information degradation in the linguistic representation of visual scenes and the object tracking abilities.
As depicted in Figure 10, the top example demonstrates that if the target object outlined in the instruction is missing from the observation description, NavGPT is necessitated to explore the environment. Ideally, all target objects should be discernible in accordance with the instruction. Nonetheless, the inherent granularity of language description compared to visual perception invariably results in information loss. In the lower example, the agent is required to "turn right into the next room". However, since the historical viewpoint observations are summarized into a sentence, some details have been omitted. NavGPT cannot compare with the comprehensive observation descriptions between viewpoints to construct a continuous understanding of the environment. The lacking a reference substance between viewpoints prevents NavGPT from determining whether it has completed the previous action. In the lower example, the agent has not entered the room yet while it erroneously believed that "I have turned right into the next room".
A potential way to solve the problem is to generate the language descriptions dynamically, similar to the ChatCaptioner[69]. Unlike the current implementation, where visual information is compromised during translation into static language descriptions, through the interaction between LLMs [69], the agent can extract essential information from the environment during navigation.</p>
<h1>Failure Case Examples</h1>
<p>Thought: I have passed the stairs. Now, I need to walk towards another door that is on the right of me. I see a doorway in the right direction. I will move to the viewpoint
'a5026cf1294d4090bbe1bf3b25643160' which is in the front left direction and closer to the doorway.
Thought: I have moved closer to the doorway on the right. Now, I need to enter the room through the doorway. I will move to the viewpoint '60e8b51598084054b9afe9f485d7898b' which is in the front right direction and closer to the doorway.
Thought: I have entered the room through the doorway. Now, I need to walk ahead and turn right at the first corner. I see a navigable viewpoint '919f4027406c421b99abb92b48917eba' in the left direction, which seems to be the first corner. I will move to this viewpoint.</p>
<p>Thought: I have reached the first corner. Now, I need to turn right and stop in front of the animal showpiece. I see a hallway in the right direction. I will move to the viewpoint
'ec76b2dc0e8248188d897e3e487df74e' which is in the right direction and closer to the animal showpiece.</p>
<p>Thought: I have turned right at the first corner, but I don't see the animal showpiece yet. I will move to the only available navigable viewpoint 'd2d25247a3c34f5382e3e1ab33079036' in the front direction to get closer to the animal showpiece.</p>
<p>Thought: I still don't see the animal showpiece, but there is only one available navigable viewpoint '835f9d83bc6d452a97ee77b90bedeea3' in the rear right direction. I will move to this viewpoint to continue searching for the animal showpiece.
Thought: I have reached a dead-end and still haven't found the animal showpiece. I will go back to the previous viewpoint 'd2d25247a3c34f5382e3e1ab33079036' and try to find another way to reach the animal showpiece.</p>
<p>Observation: Turn heading direction 42.76 degrees from left 172.95 to left 130.19.
Current viewpoint "f320ae084f3a447da3e8ab11dd5f9320": Scene from the viewpoint is a view of a house, including a bedroom, bathroom, staircase, and various doors and walls, all in shades of white with some wooden accents.
Thought: I have walked past the bathroom. Now, I need to turn right into the next room. The next room is in the Front Right direction, so I should move towards it.
Action: ..
Observation: Turn heading direction -82.29 degrees from left 130.19 to right 147.52.
Current viewpoint "c53934a9642942ae9b8d9766ee1e51ac": Scene from the viewpoint is a view of a house, including a hallway with hardwood floors and a staircase, a bathroom with mirrors and a ceiling light, a bedroom with a bed and window, and various decorative elements such as plants and wall art. Thought: I have turned right into the next room. Now, I need to proceed to the exiting doorway and wait there. The exiting doorway is in the Front Right direction, so I should move towards it.</p>
<p>Figure 10: Failure cases in NavGPT.</p>
<h2>D Broader Impact</h2>
<p>Our work is the initial attempt to use GPT models toward building versatile VLN agents. We believe the reasoning capability of LLMs is the foundation for generalizable embodied navigation agents. NavGPT reveals the reasoning trace of LLMs during navigation, making the process explicit and explainable. For safety and ethical concerns, at the current stage, all the experiments are done on the open-source Vision-and-Language Navigation dataset in a simulated environment, which ensures the controllability of the agent's behaviors. At the same time, for exploration of the practical implementation of this technology in the future, the robustness of the performance of generative models cannot yet be guaranteed. Further research is required including how to prompt LLMs to increase the accuracy and precision in planning and sequential action predictions, which is the key consideration for safety issues in real-world deployment.</p>
<h1>NavGPT Prompt $(t=0)$</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>NavGPT Response</h2>
<p>first, I need to exit the sewing room. The only navigable viewpoint is in the front right direction, so I will move there.
Action: action_naker
Action Input:
<em>cc8df784278a4f089b2239afd9a2ec9d</em></p>
<h2>[Navigation system principles]</h2>
<h2>Begin!</h2>
<p>Instruction: Exit the sewing room. Turn right. Go toward the glass cabinet with the dolls in it. Turn into the doorway on the left. Pass the bed and go through the next doorway on the left into the bathroom. Wait by the sink. Init Observation:
Front, range (left 3.51 to right 41.49 ):
'A bathroom with a wooden door and tile floor has a doorway with a picture of a pigeon on it and a wooden archway with a light fixture above it.'
Front Objects in 3m: ['cabinet': 'right 3.31, 1.82m']
Front Navigable Viewpoints: None
Front Right, range (right 41.49 to right 86.49):
'A view of a bathroom with a white tile floor and door, a kitchen with a wooden door and pictures, and a ceiling fan with a light in it from a top, down, and middle perspective.'
Front Right Objects in 3m: None
Front Right Navigable
Viewpoints:{^cc8df784278a4f089b2239afd9a2ec9d': 'right $61.44,1.48 \mathrm{~m}^{\prime}$ }
Right, range (right 86.49 to right 131.49):
'A wooden door with a metal handle and a picture on it is framed by a wooden frame.'
Right Objects in 3m: None
Right Navigable Viewpoints: None
Rear Right, range (right 131.49 to right 176.49):
'A wooden door with a handle on it is shown from different angles, including a close-up and with a light shining through it.'
Rear Right Objects in 3m: {'light': 'right 167.02, 1.23m'}
Rear Right Navigable Viewpoints: None
Rear, range (right 176.49 to left 138.51):
'A wooden closet with a metal rack and hanger rack, and a small bathroom with a wooden cabinet and light.'
Rear Objects in 3m: {'light': 'left 160.33, 1.23m'}
Rear Navigable Viewpoints: None
Rear Left, range (left 138.51 to left 93.51):
'A view of a metal rod with a wooden floor and a metal handle, under a white ceiling with a gold and brown pattern.'
Rear Left Objects in 3m: {'light_1': 'left 109.68, 1.27m', 'light_2': 'left 124.96, 1.23m'}
Rear Left Navigable Viewpoints: None
Left, range (left 93.51 to left 48.51):
'The scene depicts a small office with a desk and chair, adjacent to a small bathroom with a sink and cabinets, and a light fixture hanging above the toilet.'
Left Objects in 3m: {'cabinet': 'left 64.14, 1.82m', 'armchair': 'left 60.39, 1.58m', 'sewing machine ': 'left $69.51,1.77 \mathrm{~m}^{\prime}, \cdot$ 'light': 'left $66.11,1.27 \mathrm{~m}^{\prime}$ }
Left Navigable Viewpoints: None
Front Left, range (left 48.51 to left 3.51):
'The scene depicts a small room with a sewing machine and chair, a small kitchen with a sink and wooden cabinet, and a closet with a light fixture and wooden cabinets.'
Front Left Objects in 3m: {'cabinet': 'left 25.28, 1.82m', 'armchair': 'left 17.36, 1.58m', 'sewing machine ': 'left 17.26, 1.77m', 'light': 'left 20.54, 1.27m'}</p>
<p>Front Left Navigable Viewpoints: None
Thought: I should start navigation according to the instruction,</p>
<p>Figure 11: The prompt and response of NavGPT at step 0. All the text in NavGPT's response is generated by GPT-4. The " ${$ Navigation system principles $}$ " is shown in figure 7.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h1>NavGPT Prompt $(1-1)$</h1>
<p>(Navigation system principles)
Begin!
Instruction! Exit the sewing room. Turn right. Go toward the glass cabinet with the dolls in it. Turn into the doorway on the left. Pass the bed and go through the next doorway on the left into the bathroom. Wait by the sink. Init Observation!
Navigation start, no actions taken yet.
Current viewpoint "7e94b116c45043369954c6691acaeacd": Scene from the viewpoint is a view of a small apartment, including a sewing room, kitchen, bathroom, closet, and office.
Thought: I should start navigation according to the instruction, first, I need to exit the sewing room. The only navigable viewpoint is in the front right direction, on I will move there.
Action: action_maker
Action Input: "cc0df704278a4f009b2239afd9a2ec9d" Observation!
Front, range (left 19.95 to right 25.05):
'A laundry room with a washer and dryer is visible from both the top and middle views, while the top view also shows an arched doorway in the kitchen.'
Front Objects in 3m: {'picture_1': 'right 22.38, 1.54m', 'picture_2': 'left 5.07, 1.28m', 'picture_3': 'left 5.79, $1.29 \mathrm{~m}^{\prime}$ }
Front Navigable
Viewpoints: {'e4aa820229c5437096e4f41abf33c452': 'right 6.21, $1.69 \mathrm{~m}^{\prime}$ }
Front Right, range (right 25.05 to right 70.05):
'The scene depicts a room with a laundry area and family photos displayed on a wooden-framed wall, leading to a hallway with a picture frame and mirror.'
Front Right Objects in 3m: {'picture': 'right 47.76, 1.54m'}
Front Right Navigable Viewpoints: None
Right, range (right 70.05 to right 115.05):
'A room with a wooden floor contains a glass cabinet filled with figurines, while a ceiling light with two statues illuminates the space.'
Right Objects in 3m: {'picture_1': 'right 86.57, 1.54m', .. Right Navigable
Viewpoints: {'da3ab1ba02824248bab4f6b3299b209d': 'right $91.89,2.41 \mathrm{~m}^{\prime}$ }
Rear Right, range (right 115.05 to right 160.05):
'A hallway with numerous pictures and doors, featuring a wooden ironing board with headphones on it.'
Rear Right Objects in 3m: {'picture_1': 'right 135.22, .. Rear Right Navigable Viewpoints: None
Rear, range (right 160.05 to left 154.95):
'A closet with a clothes rack and metal rack, a mirror and a light shining through it.'
Rear Objects in 3m: {'picture_1': 'left 159.73, 1.21m', .. Rear Navigable
Viewpoints: {'7e94b116c45043369954c6691acaeacd': 'right $188.08,1.48 \mathrm{~m}^{\prime}$ }
Rear Left, range (left 154.95 to left 109.95):
'The scene depicts a hallway with framed pictures and a clock leading to a room with a yellow archway and more framed pictures.'
Rear Left Objects in 3m: {'light': 'left 112.76, 1.43m', .. Rear Left Navigable Viewpoints: None
Left, range (left 109.95 to left 64.95):
'A hallway with a circular tiled floor and a violin hanging from the ceiling, illuminated by a circular ceiling light.' Left Objects in 3m: {'light': 'left 87.36, 1.43m', .. Left Navigable
Viewpoints: {'0942757fbce9474590d047d55e04868d': 'left 84.6.. Front Left, range (left 64.95 to left 19.95):
'A man is sitting in a room with a picture on the wall, while a hallway with a picture of a man and woman and a mirror with framed photos on it can be seen from different angles.'
Front Left Objects in 3m: {'light': 'left 63.68, 1.43m', ..} Front Left Navigable Viewpoints: None Thought!</p>
<p>Figure 12: The prompt and response of NavGPT at step 1. All the text in NavGPT's response is generated by GPT-4. The "[Navigation system principles]" is shown in figure 7.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Refer to section 3.2 in main paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1276 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1276</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1276</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-3a13bb6e950dc39d62f816ac1584b68215b0d815</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3a13bb6e950dc39d62f816ac1584b68215b0d815" target="_blank">Belief space planning assuming maximum likelihood observations</a></p>
                <p><strong>Paper Venue:</strong> Robotics: Science and Systems</p>
                <p><strong>Paper TL;DR:</strong> This work casts the partially observable control problem as a fully observable underactuated stochastic control problem in belief space and applies standard planning and control techniques to define deterministic belief system dynamics based on an assumption that the maximum likelihood observation is always obtained.</p>
                <p><strong>Paper Abstract:</strong> We cast the partially observable control problem as a fully observable underactuated stochastic control problem in belief space and apply standard planning and control techniques. One of the difficulties of belief space planning is modeling the stochastic dynamics resulting from unknown future observations. The core of our proposal is to define deterministic beliefsystem dynamics based on an assumption that the maximum likelihood observation (calculated just prior to the observation) is always obtained. The stochastic effects of future observations are modeled as Gaussian noise. Given this model of the dynamics, two planning and control methods are applied. In the first, linear quadratic regulation (LQR) is applied to generate policies in the belief space. This approach is shown to be optimal for linearGaussian systems. In the second, a planner is used to find locally optimal plans in the belief space. We propose a replanning approach that is shown to converge to the belief space goal in a finite number of replanning steps. These approaches are characterized in the context of a simple nonlinear manipulation problem where a planar robot simultaneously locates and grasps an object.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1276.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1276.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief-space planner (B-LQR + Direct Transcription + Replan)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief-space planning with maximum-likelihood observation determinization, B-LQR stabilization, direct transcription planning, and replanning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated belief-space planning and control agent that (1) plans in Gaussian belief space using a determinize-and-replan approximation that assumes future observations equal their maximum-likelihood values, (2) computes locally optimal belief trajectories via direct transcription (nonlinear optimization) and stabilizes them with time-varying LQR in belief space (B-LQR), and (3) executes with an EKF belief update and replans when observed belief mean deviates past a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief-space planner (ML-observation determinize + B-LQR + Direct Transcription + Replanning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: (a) offline/online planner using direct transcription (SQP) to solve for a locally optimal belief trajectory under simplified deterministic belief dynamics obtained by assuming maximum-likelihood observations; (b) time-varying Belief-space LQR (B-LQR) controller computed by linearizing the simplified belief dynamics about the nominal trajectory to stabilize execution; (c) execution module using an Extended Kalman Filter (EKF) to update the true Gaussian belief with actual observations; (d) a replanning loop that triggers when the belief mean deviates more than a threshold theta and treats unexpected observation-induced deviations as Gaussian process noise. Key components: EKF belief tracking, simplified (ML) belief dynamics, direct transcription optimizer (SQP), time-varying LQR in belief space, and a deterministic replanning policy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active information-seeking via belief-space planning (explicit minimization of posterior covariance / final-covariance cost) combined with determinize-and-replan (maximum-likelihood observation determinization).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During planning the agent uses a cost that penalizes final covariance (and mean deviation) to generate information-gathering actions; it determinizes future observations by assuming z = z_ml = g(f(m,u)), enabling planning with deterministic belief dynamics. At execution the EKF updates belief using actual observations; if the belief mean departs from the planned trajectory by more than a threshold theta, the agent stops execution, re-runs the direct-transcription planner from the current belief, and resumes with a new plan stabilized by B-LQR. Unexpected observations are modeled as Gaussian noise around the nominal (ML) observation, and the replanning mechanism reduces uncertainty in finite steps under stated assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Light–dark domain and Laser–grasp (planar) domain</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (object/robot pose uncertain), continuous state and action spaces, deterministic process dynamics (x_{t+1}=f(x_t,u_t) in experiments), stochastic observations with state-dependent Gaussian noise (observation covariance varies with position); belief represented as Gaussian (EKF); underactuated belief dynamics (belief dimension > control inputs); nonlinear measurement functions in some domains (laser-grasp has nonlinear/squashed measurement with scalar range reading).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Light–dark: 2D state (x in R^2), 2D continuous actions (velocity), isotropic Gaussian belief (mean m in R^2, scalar covariance s) ; Laser–grasp: 2D object position state, 2D action (end-effector velocity), belief covariance parameterized in R^3 (symmetric 2x2), scalar observation (range) giving 1D information per timestep; episode/plan horizon T is finite but unspecified; planning requires nonlinear optimization (direct transcription) over k segments (k = T/δ).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative experimental outcomes reported: direct transcription + replanning produces locally optimal plans that linger in high-information regions (e.g., in light-max or optimal sensor range) and actively 'scan' to reduce covariance; B-LQR provides fast, locally stabilizing policies and performs reasonably well but can be suboptimal under poor linearization (e.g., overshoots optimal sensing location). No quantitative metrics (numeric success rates, cumulative reward, or sample-efficiency numbers) are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported numerically. Qualitative: B-LQR is computationally cheaper to compute online (local linear controller) while direct transcription is more computationally intensive (SQP) but yields higher-quality information-gathering plans; the replanning scheme is argued to converge in a finite number of replans because covariance strictly decreases by a lower-bounded amount when replanning is triggered.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly encoded in the belief-space cost: recurring Q penalizes deviation from desired mean/action (exploitation toward goal), and final covariance penalty Λ (and directional weights) encourages information-gathering (exploration) actions; planner trades off moving toward the goal vs actions that reduce posterior covariance. Execution uses a determinize-and-replan policy: follow plan (exploit) while EKF belief remains near nominal, replan (explore/new information-seeking) when surprising observations cause belief to deviate beyond threshold theta.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Within paper experiments and analysis compare: (1) B-LQR (belief-space LQR stabilization) vs (2) Direct transcription (nonlinear locally optimal planning) + B-LQR stabilization; related approaches discussed in related work include 'most-likely state' approximations (Q-MDP, FIB), nominal belief-state observation (NBO), Erez & Smart local approximation, belief roadmap approaches, and determinize-and-replan (FF-Replan) approximations, but these are not empirically benchmarked in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Formulating belief dynamics by determinizing future observations to their ML values allows use of conventional planning/control methods (LQR, nonlinear optimization) in belief space. 2) B-LQR is provably optimal when the underlying process and observation models are linear-Gaussian; for nonlinear domains it yields reasonable local controllers but can be suboptimal due to linearization (demonstrated by overshoot in the light–dark example). 3) Direct transcription finds locally optimal belief trajectories that actively gather information (e.g., linger in high-information regions and execute 'scanning' motions when observation dimension is smaller than state dimension). 4) The replanning strategy (plan → execute with EKF → replan if mean deviates > theta) is shown analytically to converge to the goal mean in a finite number of replans under stated assumptions (positive-definite observation covariance lower bound, deterministic dynamics, bounded observations, A_t eigenvalues ≤ 1). 5) Experiments qualitatively demonstrate information-gathering behaviors and successful localization + approach (light–dark and laser-grasp) but do not provide scalar performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations include: B-LQR is only locally valid and can be suboptimal if the linearization point is poor; the method assumes deterministic process dynamics (paper ignores process noise) which simplifies analysis and may limit applicability to noisy dynamics; the determinize-to-ML observation approximation can fail for highly multimodal or heavy-tailed observation distributions; practical performance depends on threshold θ selection for replanning; computational cost of direct transcription (SQP) may be high; no quantitative sample-efficiency or success-rate data are reported, limiting direct empirical comparison to other adaptive-design methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Belief space planning assuming maximum likelihood observations', 'publication_date_yy_mm': '2010-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Coordinated guidance of autonomous uavs via nominal belief-state optimization <em>(Rating: 2)</em></li>
                <li>A scalable method for solving high-dimensional continuous pomdps using local approximation <em>(Rating: 2)</em></li>
                <li>The belief roadmap: Efficient planning in linear pomdps by factoring the covariance <em>(Rating: 2)</em></li>
                <li>Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information <em>(Rating: 2)</em></li>
                <li>LQR-trees: Feedback motion planning on sparse randomized trees <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1276",
    "paper_id": "paper-3a13bb6e950dc39d62f816ac1584b68215b0d815",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Belief-space planner (B-LQR + Direct Transcription + Replan)",
            "name_full": "Belief-space planning with maximum-likelihood observation determinization, B-LQR stabilization, direct transcription planning, and replanning",
            "brief_description": "An integrated belief-space planning and control agent that (1) plans in Gaussian belief space using a determinize-and-replan approximation that assumes future observations equal their maximum-likelihood values, (2) computes locally optimal belief trajectories via direct transcription (nonlinear optimization) and stabilizes them with time-varying LQR in belief space (B-LQR), and (3) executes with an EKF belief update and replans when observed belief mean deviates past a threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief-space planner (ML-observation determinize + B-LQR + Direct Transcription + Replanning)",
            "agent_description": "Architecture: (a) offline/online planner using direct transcription (SQP) to solve for a locally optimal belief trajectory under simplified deterministic belief dynamics obtained by assuming maximum-likelihood observations; (b) time-varying Belief-space LQR (B-LQR) controller computed by linearizing the simplified belief dynamics about the nominal trajectory to stabilize execution; (c) execution module using an Extended Kalman Filter (EKF) to update the true Gaussian belief with actual observations; (d) a replanning loop that triggers when the belief mean deviates more than a threshold theta and treats unexpected observation-induced deviations as Gaussian process noise. Key components: EKF belief tracking, simplified (ML) belief dynamics, direct transcription optimizer (SQP), time-varying LQR in belief space, and a deterministic replanning policy.",
            "adaptive_design_method": "Active information-seeking via belief-space planning (explicit minimization of posterior covariance / final-covariance cost) combined with determinize-and-replan (maximum-likelihood observation determinization).",
            "adaptation_strategy_description": "During planning the agent uses a cost that penalizes final covariance (and mean deviation) to generate information-gathering actions; it determinizes future observations by assuming z = z_ml = g(f(m,u)), enabling planning with deterministic belief dynamics. At execution the EKF updates belief using actual observations; if the belief mean departs from the planned trajectory by more than a threshold theta, the agent stops execution, re-runs the direct-transcription planner from the current belief, and resumes with a new plan stabilized by B-LQR. Unexpected observations are modeled as Gaussian noise around the nominal (ML) observation, and the replanning mechanism reduces uncertainty in finite steps under stated assumptions.",
            "environment_name": "Light–dark domain and Laser–grasp (planar) domain",
            "environment_characteristics": "Partially observable (object/robot pose uncertain), continuous state and action spaces, deterministic process dynamics (x_{t+1}=f(x_t,u_t) in experiments), stochastic observations with state-dependent Gaussian noise (observation covariance varies with position); belief represented as Gaussian (EKF); underactuated belief dynamics (belief dimension &gt; control inputs); nonlinear measurement functions in some domains (laser-grasp has nonlinear/squashed measurement with scalar range reading).",
            "environment_complexity": "Light–dark: 2D state (x in R^2), 2D continuous actions (velocity), isotropic Gaussian belief (mean m in R^2, scalar covariance s) ; Laser–grasp: 2D object position state, 2D action (end-effector velocity), belief covariance parameterized in R^3 (symmetric 2x2), scalar observation (range) giving 1D information per timestep; episode/plan horizon T is finite but unspecified; planning requires nonlinear optimization (direct transcription) over k segments (k = T/δ).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative experimental outcomes reported: direct transcription + replanning produces locally optimal plans that linger in high-information regions (e.g., in light-max or optimal sensor range) and actively 'scan' to reduce covariance; B-LQR provides fast, locally stabilizing policies and performs reasonably well but can be suboptimal under poor linearization (e.g., overshoots optimal sensing location). No quantitative metrics (numeric success rates, cumulative reward, or sample-efficiency numbers) are provided in the paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Not reported numerically. Qualitative: B-LQR is computationally cheaper to compute online (local linear controller) while direct transcription is more computationally intensive (SQP) but yields higher-quality information-gathering plans; the replanning scheme is argued to converge in a finite number of replans because covariance strictly decreases by a lower-bounded amount when replanning is triggered.",
            "exploration_exploitation_tradeoff": "Explicitly encoded in the belief-space cost: recurring Q penalizes deviation from desired mean/action (exploitation toward goal), and final covariance penalty Λ (and directional weights) encourages information-gathering (exploration) actions; planner trades off moving toward the goal vs actions that reduce posterior covariance. Execution uses a determinize-and-replan policy: follow plan (exploit) while EKF belief remains near nominal, replan (explore/new information-seeking) when surprising observations cause belief to deviate beyond threshold theta.",
            "comparison_methods": "Within paper experiments and analysis compare: (1) B-LQR (belief-space LQR stabilization) vs (2) Direct transcription (nonlinear locally optimal planning) + B-LQR stabilization; related approaches discussed in related work include 'most-likely state' approximations (Q-MDP, FIB), nominal belief-state observation (NBO), Erez & Smart local approximation, belief roadmap approaches, and determinize-and-replan (FF-Replan) approximations, but these are not empirically benchmarked in the experiments.",
            "key_results": "1) Formulating belief dynamics by determinizing future observations to their ML values allows use of conventional planning/control methods (LQR, nonlinear optimization) in belief space. 2) B-LQR is provably optimal when the underlying process and observation models are linear-Gaussian; for nonlinear domains it yields reasonable local controllers but can be suboptimal due to linearization (demonstrated by overshoot in the light–dark example). 3) Direct transcription finds locally optimal belief trajectories that actively gather information (e.g., linger in high-information regions and execute 'scanning' motions when observation dimension is smaller than state dimension). 4) The replanning strategy (plan → execute with EKF → replan if mean deviates &gt; theta) is shown analytically to converge to the goal mean in a finite number of replans under stated assumptions (positive-definite observation covariance lower bound, deterministic dynamics, bounded observations, A_t eigenvalues ≤ 1). 5) Experiments qualitatively demonstrate information-gathering behaviors and successful localization + approach (light–dark and laser-grasp) but do not provide scalar performance metrics.",
            "limitations_or_failures": "Reported limitations include: B-LQR is only locally valid and can be suboptimal if the linearization point is poor; the method assumes deterministic process dynamics (paper ignores process noise) which simplifies analysis and may limit applicability to noisy dynamics; the determinize-to-ML observation approximation can fail for highly multimodal or heavy-tailed observation distributions; practical performance depends on threshold θ selection for replanning; computational cost of direct transcription (SQP) may be high; no quantitative sample-efficiency or success-rate data are reported, limiting direct empirical comparison to other adaptive-design methods.",
            "uuid": "e1276.0",
            "source_info": {
                "paper_title": "Belief space planning assuming maximum likelihood observations",
                "publication_date_yy_mm": "2010-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Coordinated guidance of autonomous uavs via nominal belief-state optimization",
            "rating": 2
        },
        {
            "paper_title": "A scalable method for solving high-dimensional continuous pomdps using local approximation",
            "rating": 2
        },
        {
            "paper_title": "The belief roadmap: Efficient planning in linear pomdps by factoring the covariance",
            "rating": 2
        },
        {
            "paper_title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information",
            "rating": 2
        },
        {
            "paper_title": "LQR-trees: Feedback motion planning on sparse randomized trees",
            "rating": 1
        }
    ],
    "cost": 0.010083,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MIT <br> Libraries</h1>
<h2>DSpace@MIT</h2>
<h2>MIT Open Access Articles</h2>
<h2>Belief space planning assuming maximum likelihood observations</h2>
<p>The MIT Faculty has made this article openly available. Please share how this access benefits you. Your story matters.</p>
<p>Citation: Platt, Robert, Jr., et al. "Belief space planning assuming maximum likelihood observations" Proceedings of the Robotics: Science and Systems Conference, 6th, 2010.</p>
<p>As Published: http://www.roboticsproceedings.org/rss06/p37.pdf
Persistent URL: http://hdl.handle.net/1721.1/62571
Version: Author's final manuscript: final author's manuscript post peer review, without publisher's formatting or copy editing</p>
<p>Terms of use: Creative Commons Attribution-Noncommercial-Share Alike 3.0</p>
<h1>Belief space planning assuming maximum likelihood observations</h1>
<p>Robert Platt Jr., Russ Tedrake, Leslie Kaelbling, Tomas Lozano-Perez<br>Computer Science and Artificial Intelligence Laboratory<br>Massachusetts Institute of Technology<br>{rplatt,russt,lpk,tlp}@csail.mit.edu</p>
<h4>Abstract</h4>
<p>We cast the partially observable control problem as a fully observable underactuated stochastic control problem in belief space and apply standard planning and control techniques. One of the difficulties of belief space planning is modeling the stochastic dynamics resulting from unknown future observations. The core of our proposal is to define deterministic beliefsystem dynamics based on an assumption that the maximum likelihood observation (calculated just prior to the observation) is always obtained. The stochastic effects of future observations are modeled as Gaussian noise. Given this model of the dynamics, two planning and control methods are applied. In the first, linear quadratic regulation (LQR) is applied to generate policies in the belief space. This approach is shown to be optimal for linearGaussian systems. In the second, a planner is used to find locally optimal plans in the belief space. We propose a replanning approach that is shown to converge to the belief space goal in a finite number of replanning steps. These approaches are characterized in the context of a simple nonlinear manipulation problem where a planar robot simultaneously locates and grasps an object.</p>
<h2>I. INTRODUCTION</h2>
<p>Control problems in partially observable environments are important to robotics because all robots ultimately perceive the world through limited and imperfect sensors. In the context of robotic manipulation, tactile and range sensors mounted on the manipulator near the contact locations can provide a tremendous advantage in precision over remote sensing [1, 2]. However, co-locating the sensors with the contacts this way complicates planning and control because it forces the system to trade off sensing and acting. It essentially requires the system to solve a difficult instance of the partially observable control problem, often modeled as a partially observable Markov decision process (POMDP). Unfortunately this problem has been shown to be PSPACE complete, even for a finite planning horizon, discrete states, actions, and observations [3].</p>
<p>One solution to the partially observable control problem is to form plans in the "belief space" of the manipulator the space of all possible distributions over the state space. The controller then selects actions based not only on the current most-likely state of the robot, but more generically on the information available to the robot. A hallmark of belief-space planning is the ability to generate informationgathering actions. However, planning in the belief space is challenging for a number of reasons. Even coarse finitedimensional approximations of the belief state distributions require planning in dimensions that are much larger than the
original state space. Furthermore, the resulting belief state dynamics are nonlinear, underactuated (number of control inputs is smaller than the dimension of the belief space), and stochastic (transitions depend on observations which have not yet been made).</p>
<p>A number of powerful tools exist for planning and control of high-dimensional non-linear underactuated systems. In order to apply these tools, this paper defines nominal belief space dynamics based on an assumption that all future observations will obtain their maximum likelihood values (this assumption is also made in [4, 5]). During execution, the system tracks the true belief based on the observations actually obtained. Departures from the nominal belief dynamics caused by these unexpected observations are treated as Gaussian process noise. As a result, it is possible to apply standard control and planning techniques. In particular, we use linear quadratic regulation (LQR) to calculate belief space policies based on a local linearization of the belief space dynamics. In spite of this linearization, the resulting belief space policy is shown to be optimal for underlying linear-Gaussian systems. For nonlinear systems, it produces reasonable policies within a local region about the linearization point. When large observations cause system belief to leave the locally stabilized region, we propose replanning from the new belief state. We analyze this replanning approach and demonstrate that, under certain conditions, it is guaranteed to ultimately reach a goal region in belief space in a finite number of replanning steps.</p>
<h2>A. Related Work</h2>
<p>Finding an exact optimal solution to a POMDP is an intractable problem [3]. As a result, research has focused on various approaches to approximating the solution. One approach is the 'most-likely state' approximation. This method assumes that the true state of the MDP that underlies the POMDP is in fact the mode of the current belief state. Actions are taken according to the optimal policy in the underlying MDP. The approach considers stochasticity in the underlying process dynamics, but assumes no state uncertainty exists in the future. More sophisticated versions of this approximation include Q-MDP [6] and FIB [7]. A fundamental failing of these approaches is that the system never takes actions for the explicit purpose of reducing uncertainty because the planner assumes that no uncertainty exists.</p>
<p>Another approach that is applicable to some kinds of</p>
<p>POMDPs with continuous state, action, and observation spaces is the belief roadmap approach [8]. This method ranks paths through a probabilistic roadmap defined in the underlying state space in terms of the change in covariance over the path. Van der Berg et. al. propose a related approach where a set of potential trajectories are evaluated by tracking the belief state along each trajectory and selecting the one that minimizes the likelihood of failure [9]. In contrast to this class of work, the current paper proposes planning directly in the belief space. This enables our planner to utilize knowledge of the belief dynamics during the planning process rather than evaluating a set of paths through the underlying space.</p>
<p>Our approach is also related to the ‘determinize-and-replan’ approximation to MDPs that assumes world dynamics are deterministic for the purposes of planning. It takes the first action, observes the actual resulting state, and then replans. This approach, as embodied in FF-Replan, has been very successful (won the ICAPS06 planning competition) in a variety of contexts [10]. Our approach can be viewed as ‘determinize-and-replan’, applied to POMDPs. It has the significant advantage over the most-likely state approach that it can and does explicitly plan to gain information. And, by replanning when surprising observations are obtained, it remains robust to unlikely outcomes.</p>
<p>Two approaches closely related to our current proposal are the nominal belief-state observation (NBO) of Miller et. al. [4] and the work of Erez and Smart [5]. Both approaches plan in belief space using extended Kalman filter dynamics that incorporate an assumption that observations will be consistent with a maximum likelihood state. In relation to these works, this paper has a greater focus on methods for control and replanning. We analyze the conditions under which LQR in belief space is optimal and show that our replanning framework must eventually converge.</p>
<h2>II. Problem specification</h2>
<p>We reformulate the underlying partially observable problem as a fully observable belief space problem with an associated cost function.</p>
<h2>A. Underlying system</h2>
<p>Consider the following partially observable control problem. Let $x_{t} \in X$ be the unobserved $d$-dimensional combined state of the robot and the environment at time $t$. Although the state is not observed directly, noisy observations, $z_{t} \in Z$, are available as a non-linear stochastic function of $x_{t}$ :</p>
<p>$$
z_{t}=g\left(x_{t}\right)+\omega
$$</p>
<p>where $g$ is the deterministic component of the measurement function and $\omega$ is zero-mean Gaussian noise with possibly state-dependent covariance, $W_{t}$. This paper only considers the case where the underlying process dynamics are deterministic:</p>
<p>$$
x_{t+1}=f\left(x_{t}, u_{t}\right)
$$</p>
<p>Both $g$ and $f$ are required to be a differentiable functions of $x_{t}$ and $u_{t}$. Although we expect our technique to extend to
systems with noisy process dynamics, consideration is limited to deterministic systems to simplify the analysis.</p>
<h2>B. Belief system</h2>
<p>Although the underlying state of the system is not directly observed, we assume that the controller tracks a fixed parameterization of a probability distribution over the state, $P(x)$. The parameters of the probability density function (pdf) of this distribution will be referred to as the "belief state" and can be tracked using Bayesian filtering as a function of control actions and observations:</p>
<p>$$
P\left(x_{t+1}\right)=\eta P\left(z_{t+1} \mid x_{t+1}\right) \int_{x} P\left(x_{t+1} \mid x, u_{t}\right) P(x)
$$</p>
<p>where $\eta$ is a normalization constant. Notice that since the belief update is a function of the measured observations accrued during execution, it is impossible in general to predict ahead of time exactly how system belief will change until the observations are made.</p>
<p>For the rest of this paper, we will focus on Gaussian belief state dynamics where the extended Kalman filter belief update is used. On each update, the extended Kalman filter linearizes the process and observation dynamics (Equations 2 and 1) about the current mean of the belief distribution:</p>
<p>$$
x_{t+1} \approx A_{t}\left(x_{t}-m_{t}\right)+f\left(m_{t}, u_{t}\right)
$$</p>
<p>and</p>
<p>$$
z_{t} \approx C_{t}\left(x_{t}-m_{t}\right)+g\left(m_{t}\right)+\omega
$$</p>
<p>where $m_{t}$ is the belief state mean, and $A_{t}=\frac{\partial f}{\partial x}\left(m_{t}, u_{t}\right)$, and $C_{t}=\frac{\partial g}{\partial x}\left(m_{t}\right)$, are Jacobian matrices linearized about the mean and action.</p>
<p>For the Gaussian belief system, the distribution is a Gaussian with mean, $m_{t}$, and covariance, $\Sigma_{t}: P(x)=$ $\mathcal{N}\left(x \mid m_{t}, \Sigma_{t}\right)$. This belief state will be denoted by a vector, $b_{t}=\left(m_{t}^{T}, s_{t}^{T}\right)^{T}$, where $s=\left(s_{1}^{T}, \ldots, s_{d}^{T}\right)^{T}$ is a vector composed of the $d$ columns of $\Sigma=\left(s_{1}, \ldots, s_{d}\right)$. If the system takes action, $u_{t}$, and makes observation, $z_{t}$, then the EKF belief state update is:</p>
<p>$$
\begin{gathered}
\Sigma_{t+1}=\Gamma_{t}-\Gamma_{t} C_{t}^{T}\left(C_{t} \Gamma_{t} C_{t}^{T}+W_{t}\right)^{-1} C_{t} \Gamma_{t} \
m_{t+1}=f_{t}+\Gamma_{t} C_{t}^{T}\left(C_{t} \Gamma_{t} C_{t}^{T}+W_{t}\right)^{-1}\left(z_{t+1}-g\left(f_{t}\right)\right)
\end{gathered}
$$</p>
<p>where</p>
<p>$$
\Gamma_{t}=A_{t} \Sigma_{t} A_{t}^{T}
$$</p>
<p>and $f_{t}$ denotes $f\left(m_{t}, u_{t}\right)$.</p>
<h2>C. Cost function</h2>
<p>In general, we are concerned with the problem of reaching a given region of state space with high certainty. For a Gaussian belief space, this corresponds to a cost function that is minimized at zero covariance. However, it may be more important to reduce covariance in some directions over others. Let $\left{\hat{n}<em k="k">{1}, \ldots, \hat{n}</em>$. For a given state-action trajectory,}\right}$ be a set of unit vectors pointing in $k$ directions in which it is desired to minimize covariance and let the relative importance of these directions be described by the weights, $w_{1}, \ldots, w_{k</p>
<p>$b_{\tau:T},u_{\tau:T}$, we define a finite horizon quadratic cost-to-go function:</p>
<p>$J(b_{\tau:T},u_{\tau:T})=\sum_{i=1}^{k}{w_{i}\left(\hat{n}<em T="T">{i}^{T}\Sigma</em>}\hat{n<em t="\tau">{i}\right)^{2}+\sum</em>}^{T-1}{\tilde{m<em t="t">{t}^{T}Q\tilde{m}</em>}+\tilde{u<em t="t">{t}^{T}R\tilde{u}</em>,$ (7)}</p>
<p>where $\tilde{m}<em t="t">{t}=m</em>}-\bar{m<em t="t">{t}$ and $\tilde{u}</em>}=u_{t}-\bar{u<em t="t">{t}$ are the mean and action relative to a desired state-action point or trajectory, $\bar{m}</em>}$ and $\bar{u<em T="T">{t}$, and $\Sigma</em>$ is the covariance matrix at the end of the planning horizon. The first summation penalizes departures from zero covariance along the specified directions on the final timestep. The second summation over the planning horizon penalizes departures from a desired mean and action with positive definite cost matrices $Q$ and $R$. While Equation 7 is quadratic, the first summation is not expressed in the standard form. However, its terms can be re-written in terms of $s$, the vector of the columns of $\Sigma$:</p>
<p>$\left(\hat{n}<em i="i">{i}^{T}\Sigma\hat{n}</em>s,$}\right)^{2}=s^{T}L_{i</p>
<p>where the cost matrix, $L_{i}$, is a function of $\hat{n}_{i}$:</p>
<p>$L_{i}=\left(\begin{array}{c}\hat{n}<em i_1="i,1">{i}{n</em>}} \ \vdots \ \hat{n<em i_d="i,d">{i}{n</em>}}\end{array}\right)\left(\begin{array}{c}\hat{n<em i_1="i,1">{i}{n</em>}} \ \vdots \ \hat{n<em i_d="i,d">{i}{n</em>,$}}\end{array}\right)^{T</p>
<p>where $n_{i,j}$ is the $j^{t h}$ element of $\hat{n}_{i}$. As a result, we re-write the cost-to-go function as:</p>
<p>$J(b_{\tau:T},u_{\tau:T})=s_{T}^{T}\Lambda s_{T}+\sum_{t=\tau}^{T-1}{\tilde{m}<em t="t">{t}^{T}Q\tilde{m}</em>}+\tilde{u<em t="t">{t}^{T}R\tilde{u}</em>,$ (8)}</p>
<p>where</p>
<p>$\Lambda=\sum_{i=1}^{K}{w_{i}L_{i}}.$</p>
<p>Although not included in Equation 8, all plans in the belief space are required to satisfy certain final value constraints. First, a constraint on the final value mean of the belief system is specified: $m_{T}=\bar{m}_{T}$. B-LQR (Section IV) incorporates this constraint by augmenting the cost-to-go function (Equation 14). Direct transcription (Section V-A) incorporates this constraint directly.</p>
<p>For a policy defined over the belief space,</p>
<p>$u_{t}=\pi(b_{t}),$</p>
<p>the expected cost-to-go from a belief state, $b_{\tau}$, at time $\tau$ with a planning horizon $T-\tau$ (the planning horizon could be infinite) is:</p>
<p>$J^{\pi}(b_{\tau})=\mathcal{E}<em _tau:T-1="\tau:T-1">{z</em>))\right},$ (9)}}\left{J(b_{\tau:T},\pi(b_{\tau:T-1</p>
<p>where the expectation is taken over future observations. The optimal policy minimizes expected cost: $\pi^{*}(b_{\tau})=$ $\arg \min_{\pi} J^{\pi}(b_{\tau})$.</p>
<h2>III. SIMPLIFIED BELIEF SPACE DYNAMICS</h2>
<p>In order to apply standard control and planning techniques to the belief space problem, it is necessary to define the dynamics of the belief system. Given our choice to use the EKF belief update, Equations 5 and 6 should be used. Notice that Equation 6 depends on the observation, $z_{t}$. Since these observations are unknown ahead of time, it should be necessary to evaluate the expected value of seeing each observation and take the expectation over all observations. However, since it is difficult to evaluate this marginalization, we make a key simplifying assumption for the purposes of planning and control: we assume that future observations are normally distributed about a maximum likelihood. If action $u_{t}$ is taken from belief state $b_{t}$, then the maximum likelihood observation is:</p>
<p>$z_{m l}=\arg \max_{z}{P(z|b_{t},u_{t})}.$ (10)</p>
<p>Evaluating the maximum likelihood observation for the EKF using Equation 10, we have:</p>
<p>$z_{m l}=$ $\arg \max_{z}{\int{P(z|x_{t+1})P(x_{t+1}|b_{t},u_{t})dx_{t+1}}}$
$\approx$ $\arg \max_{z}{\int{\mathcal{N}(z|C_{t}(x_{t+1}-f_{t})+g(f_{t}),W_{t})}}$
$\mathcal{N}(x_{t+1}|f_{t},\Gamma_{t})dx_{t+1}$
$=$ $\arg \max_{z}{\mathcal{N}(z|g(f_{t}),C_{t}\Gamma_{t}C_{t}^{T}+W_{t})}$
$=$ $g(f_{t})$.</p>
<p>Substituting into Equation 6, and restating Equation 5, the simplified dynamics are:</p>
<p>$b_{t+1}=F(b_{t},u_{t}),$</p>
<p>where $F$ evaluates to the $b_{t+1}$ corresponding to $m_{t+1}$ and $\Sigma_{t+1}$,</p>
<p>$m_{t+1}=f_{t}+v,$
$\Sigma_{t+1}=\Gamma_{t}-\Gamma_{t}C_{t}^{T}(C_{t}\Gamma_{t}C_{t}^{T}+W_{t})^{-1}C_{t}\Gamma_{t},$ (13)</p>
<p>and $v$ is Gaussian noise.</p>
<h2>IV. LQR IN BELIEF SPACE</h2>
<p>Linear quadratic regulation (LQR) is an important and simple approach to optimal control [11]. While LQR is optimal only for linear-Gaussian systems, it is also used to stabilize non-linear systems in a local neighborhood about a nominal point or trajectory in state space. LQR in belief space (BLQR) is the application of LQR to belief space control using the simplified belief system dynamics (Equations 12 and 13). Since the belief system dynamics are always non-linear, policies found by B-LQR can be expected to be only locally stable. However, as we show below, it turns out that B-LQR is optimal and equivalent to linear quadratic Gaussian (LQG) control for systems with linear-Gaussian process and observation dynamics. Moreover, B-LQR produces qualitatively interesting policies for systems with non-linear dynamics.</p>
<p>We are interested in the finite horizon problem with a final state constraint on the mean of the belief system. The final</p>
<p>state constraint is accommodated by adding an additional term to Equation 8 that assigns a large cost to departures from the desired final mean value. Also, the LQR cost function used in the current work is linear about the planned trajectory:</p>
<p>$$
\begin{aligned}
J\left(b_{\tau: T}, u_{\tau: T}\right)= &amp; \tilde{m}<em _large="{large" _text="\text">{T}^{T} Q</em>}} \tilde{m<em T="T">{T}+\tilde{s}</em>}^{T} \Lambda \tilde{s<em t="\tau">{T} \
&amp; +\sum</em>}^{T-1} \tilde{m<em t="t">{t}^{T} Q \tilde{m}</em>}+\tilde{u<em t="t">{t}^{T} R \tilde{u}</em>
\end{aligned}
$$</p>
<p>Notice that the second term in the above does not measure covariance cost about zero (that would be $s_{T}^{T} \Lambda s_{T}$ ). Instead, we keep the system linear by measuring cost relative to the belief trajectory, $\tilde{s}_{T}$. Nevertheless, note that it should be possible measure covariance cost relative to zero by using an affine rather than linear version of the LQR controller.</p>
<p>Since B-LQR is operating in belief space, is necessary to linearize Equation 11 about a nominal belief and action, denoted $\bar{b}<em t="t">{t}$ and $\bar{u}</em>$ :</p>
<p>$$
\mathbb{A}<em t="t">{t}=\frac{\partial F}{\partial b}\left(\bar{b}</em>\right)
$$}, \bar{u}_{t</p>
<p>and</p>
<p>$$
\mathbb{B}<em t="t">{t}=\frac{\partial F}{\partial u}\left(\bar{b}</em>\right)
$$}, \bar{u}_{t</p>
<p>Notice that the mean of the Gaussian belief system has the same nominal dynamics as the underlying system. As a result, $\mathbb{A}_{t}$ always has the form:</p>
<p>$$
\mathbb{A}<em t="t">{t}=\left(\begin{array}{cc}
A</em> \
\frac{\partial s_{t+1}}{\partial m_{t}} &amp; \frac{\partial s_{t+1}}{\partial s_{t}}
\end{array}\right)
$$} &amp; \mathbf{0</p>
<p>where $A_{t}=\frac{\partial f}{\partial x}\left(\bar{m}<em t="t">{t}, \bar{u}</em>$ always has the form}\right)$ (see Equation 3). Also, note that since the control input never directly affects covariance, $\mathbb{B</p>
<p>$$
\mathbb{B}<em t="t">{t}=\binom{B</em>
$$}}{0</p>
<p>where $B_{t}=\frac{\partial f}{\partial u}\left(\bar{m}<em t="t">{t}, \bar{u}</em>\right)$. Finally, since Equation 14 only assigns departures from the mean a recurring cost, the recurring cost matrix for belief state is</p>
<p>$$
\mathbb{Q}=\left(\begin{array}{cc}
Q &amp; \mathbf{0} \
\mathbf{0} &amp; \mathbf{0}
\end{array}\right)
$$</p>
<p>Using the above, the Riccati equation for the discrete time finite horizon problem is:</p>
<p>$$
\begin{aligned}
\mathbb{S}<em t="t">{t}= &amp; \mathbb{Q}+\mathbb{A}</em>}^{T} \mathbb{S<em t="t">{t+1} \mathbb{A}</em> \
&amp; -\mathbb{A}<em t_1="t+1">{t}^{T} \mathbb{S}</em>} \mathbb{B<em t="t">{t}\left(\mathbb{B}</em>}^{T} \mathbb{S<em t="t">{t+1} \mathbb{B}</em>}+R_{t}\right)^{-1} \mathbb{B<em t_1="t+1">{t}^{T} \mathbb{S}</em>
\end{aligned}
$$} \mathbb{A}_{t</p>
<p>where $\mathbb{S}<em t="t">{t}$ is the expected cost-to-go matrix for linear-Gaussian systems, $J^{s^{*}}\left(b</em>}\right)=b_{t}^{T} \mathbb{S<em t="t">{t} b</em>$. The optimal action for linearGaussian systems is:</p>
<p>$$
u^{*}=-\left(\mathbb{B}<em t_1="t+1">{t}^{T} \mathbb{S}</em>} \mathbb{B<em t="t">{t}+R</em>}\right)^{-1} \mathbb{B<em t_1="t+1">{t}^{T} \mathbb{S}</em>} \mathbb{A<em t="t">{t} b</em>
$$</p>
<p>The following theorem demonstrates that B-LQR is equivalent to LQG control for linear-Gaussian systems.</p>
<p>Theorem 1: If the cost function is of the form of Equation 14, and the underlying process and observation dynamics (Equations 1 and 2) are linear, then B-LQR is optimal.</p>
<p>Proof: We show that under the conditions of the theorem, B-LQR is equivalent to LQG and is optimal as a result.</p>
<p>First, if the underlying observation dynamics are linear, then $\frac{\partial s_{t+1}}{\partial m_{t}}=0$ and $\mathbb{A}<em t="t">{t}$ is block diagonal. Also, note that if the cost is of the form in Equation 14, then the state cost, $\mathbb{Q}$, is also block diagonal with respect to mean and covariance ( $m$ and $s$ ). As a result of these two facts, the solution to the belief space Riccati equation (Equation 15) at time $t, \mathbb{S}</em>$, always has the form,</p>
<p>$$
\mathbb{S}<em t="t">{t}=\left(\begin{array}{cc}
S</em> \
\mathbf{0} &amp; P_{t}
\end{array}\right)
$$} &amp; \mathbf{0</p>
<p>where $S_{t}$ is the solution of the Riccati equation in the underlying space,</p>
<p>$$
\begin{aligned}
S_{t}= &amp; Q+A_{t}^{T} S_{t+1} A_{t} \
&amp; -A_{t}^{T} S_{t+1} B_{t}\left(B_{t}^{T} S_{t+1} B_{t}+R_{t}\right)^{-1} B_{t}^{T} S_{t+1} A_{t}
\end{aligned}
$$</p>
<p>and $P_{t}$ is arbitrary. Substituting into Equation 16, we have:</p>
<p>$$
u^{*}=-\left(B_{t}^{T} S_{t+1} B_{t}+R_{t}\right)^{-1} B_{t}^{T} S_{t+1} A_{t} m_{t}
$$</p>
<p>Since this is exactly the solution of the LQG formulation, and LQG is optimal for linear-Gaussian process and observation dynamics, we conclude the B-LQR is optimal.</p>
<h2>V. Planning</h2>
<p>Since B-LQR is based on a local linearization of the simplified belief dynamics, it is clear that planning methods that work directly with the non-linear dynamics can have better performance. A number of planning methods that are applied to underactuated systems are relevant to the belief space planning problem: rapidly expanding random trees (RRTs) [12], LQR-trees [13], and nonlinear optimization methods [14]. Presently, we use an approach based on a nonlinear optimization technique known as direct transcription. The resulting trajectory is stabilized using time varying B-LQR.</p>
<h2>A. Direct transcription</h2>
<p>Direction transcription is an approach to transcribing the optimal control problem to a nonlinear optimization problem. Suppose we want to find a path from time 1 to $T$ starting at $b_{1}$ that optimizes Equation 8. Direction transcription parameterizes the space of possible trajectories by a series of $k$ segments. Let $\delta$ be a user-defined integer that defines the length of each segment in time steps. Then the number of segments is $k=\frac{T}{\delta}$. Let $b_{1: k}^{\prime}$ and $u_{1: k-1}^{\prime}$ be sets of belief state and action variables that parameterize the trajectory in terms of the segments. Segment $i$ begins at time $i \delta$ and ends at time $i \delta+\delta-1$. The cost function, Equation 8, is approximated in terms of these segments:</p>
<p>$$
\begin{aligned}
J\left(b_{1: T}, u_{1: T}\right) &amp; \approx \bar{J}\left(b_{1: k}^{\prime}, u_{1: k}^{\prime}\right) \
&amp; =s^{T} \Lambda s+\sum_{j=1}^{k} \tilde{m}<em i="i">{i}^{\prime T} Q \tilde{m}</em>}^{\prime}+\tilde{u<em i="i">{i}^{\prime T} R \tilde{u}</em>
\end{aligned}
$$}^{\prime</p>
<p>where, for the purposes of planning, $\tilde{m}<em i="i">{i}^{\prime}$ is measured with respect to the final value constraint, $\bar{m}</em>$. The belief}^{\prime}=\bar{m}_{T</p>
<p>state on the last time step of this segment can be found by integrating $F$ over $\delta$ :</p>
<p>$$
\phi\left(b_{i}^{\prime}\right)=F\left(b_{i}^{\prime}, u_{i}\right)+\sum_{t=i \delta}^{i \delta+\delta-1} F\left(b_{t+1}, u_{i}\right)-F\left(b_{t}, u_{i}\right)
$$</p>
<p>It is now possible to define the nonlinear optimization problem that approximates the optimal control problem. We want assignments to the variables, $b_{1: k}^{\prime}$ and $u_{1: k}^{\prime}$, that minimize the approximate cost, $\hat{J}\left(b_{1: k}^{\prime}, u_{1: k}^{\prime}\right)$ subject to constraints that require each segment to be dynamically consistent with its neighboring segments:</p>
<p>$$
\begin{aligned}
b_{2}^{\prime} &amp; =\phi\left(b_{1}^{\prime}, u_{1}^{\prime}\right) \
&amp; \vdots \
b_{k}^{\prime} &amp; =\phi\left(b_{k-1}^{\prime}, u_{k-1}^{\prime}\right)
\end{aligned}
$$</p>
<p>Since we have a final value constraint on the mean component of belief, an additional constraint is added:</p>
<p>$$
m_{k}^{\prime}=\bar{m}_{T}
$$</p>
<p>By approximating the optimal control problem using Equations 18 through 20, it is possible to apply any optimization method available. In the current work, we use sequential quadratic programming (SQP) to minimize the Lagrangian constructed from the costs and constraints in Equations 18- 20. At each iteration, this method takes a Newton step found by solving the Karush-Kuhn-Tucker (KKT) system of equations. Ultimately SQP converges to a local minimum. For more information, the reader is recommended to [14].</p>
<h2>B. Replanning strategy</h2>
<p>Input : initial belief state, $b$.
Output: vector of control actions, $u_{1: T}$.
while $b$ not at goal do
$\left(\bar{u}<em 1:="1:" T="T">{1: T}, \bar{b}</em>\right)=$ create_plan $(b)$;
for $t \leftarrow 1$ to $T-1$ do
$u_{t}=l q r \cdot \operatorname{control}\left(b_{t}, \bar{u}<em t="t">{t}, \bar{b}</em>\right)$
$b_{t+1}=E K F\left(b_{t}, u_{t}, z_{t}\right) ;$
if $\hat{m}<em t_1="t+1">{t+1}&gt;\theta$ then break;
end
$b=b</em>$
end
Algorithm 1: Belief space planning algorithm.</p>
<p>Since the actual belief transition dynamics are stochastic, a mechanism must exist for handling divergences from the planned trajectory. One approach is to use time varying BLQR about the planned path to stabilize the trajectory. While this works for small departures from the planned trajectory, replanning is needed to handle larger divergences. We propose the basic replanning strategy outlined in Algorithm 1 (above). In the create_plan step, the algorithm solves for a belief space trajectory that satisfies the final value constraints on the mean using direct transcription. Next, lqr_control solves for and
executes a locally stable policy about the trajectory. The $E K F$ step tracks belief state based on actual observations. When the mean component of belief departs from the planned trajectory by more than a given threshold, $\theta$,</p>
<p>$$
m_{t}-\bar{m}<em t="t">{t}=\hat{m}</em>&gt;\theta
$$</p>
<p>the for loop breaks and replanning occurs. It is assumed that the threshold, $\theta$, is chosen such that B-LQR is stable within the threshold.</p>
<h2>C. Analysis of belief space covariance during replanning</h2>
<p>Under Algorithm 1, the mean component of belief can be shown to reach the final value constraint in a finite number of replanning steps. Essentially, we show that each time the belief system deviates from the planned trajectory by more than $\theta$, covariance decreases by a finite amount. Each time this occurs, it becomes more difficult for the belief state mean to exceed the threshold again. After a finite number of replanning steps, this becomes impossible and the system converges to the final value constraint. In the following, we shall denote the spectral norm of a matrix, $A$, by $|A|_{2}$. The spectral norm of a matrix evaluates to its largest singular value.</p>
<p>We require the following conditions to be met:</p>
<p>1) The observation covariance matrix is positive definite and there exists a strictly smaller matrix, $W_{\text {min }}$, such that $W&gt;W_{\text {min }}$,
2) the underlying process dynamics are deterministic,
3) $A_{t}$ has eigenvalues no greater than one,
4) the observations are bounded, $\left|z_{t}\right|<z_{\max }$, Lemma 1: Suppose that Algorithm 1 executes under the conditions above. Then, the change in covariance between time $t_{a}$ when $\hat{m}_{a}=0$ and time $t_{b}$ when $\hat{m}_{b}>\theta$ (the replanning threshold) is lower bounded by</p>
<p>$$
\left|\Sigma_{a}-\Sigma_{b}\right|<em _min="\min">{2} \geq\left(\frac{\theta}{\tau\left|W</em>\right|}^{-\frac{1}{2}<em _max="\max">{2} z</em>
$$}}\right)^{2</p>
<p>where $\tau=t_{b}-t_{a}$.
Proof: Since $m$ changes by at least $\theta$ over $\tau$ timesteps, there is at least one timestep between $t_{a}$ and $t_{b}$ (time $t_{c}$ ) where $\Delta m_{c}=\left|\tilde{m}<em c="c">{c+1}-\hat{m}</em>$. This implies:}\right|&gt;\frac{\theta}{\tau</p>
<p>$$
\begin{aligned}
\frac{\theta}{\tau} &amp; \leq\left|\Gamma_{c} C_{c}^{T}\left(C_{c} \Gamma_{c} C_{c}^{T}+W_{c}\right)^{-1}\left(z_{c+1}-\bar{z}<em c="c">{c+1}\right)\right| \
&amp; \leq\left|\Gamma</em>\right|} C_{c}^{T}\left(C_{c} \Gamma_{c} C_{c}^{T}+W_{c}\right)^{-1<em _max="\max">{2} z</em>
\end{aligned}
$$</p>
<p>Since</p>
<p>$$
\left|\left(C_{c} \Gamma_{c} C_{c}^{T}+W_{c}\right)^{-\frac{1}{2}}\right|<em _min="\min">{2} \leq\left|W</em>
$$}^{-\frac{1}{2}}\right|_{2</p>
<p>and using the properties of the spectral norm, we have that:</p>
<p>$$
\frac{\theta}{\tau} \leq\left|\Gamma_{c} C_{c}^{T}\left(C_{c} \Gamma_{c} C_{c}^{T}+W_{c}\right)^{-\frac{1}{2}}\right|<em _min="\min">{2}\left|W</em>\right|}^{-\frac{1}{2}<em _max="\max">{2} z</em>
$$</p>
<p>Dividing through by $\left|W_{\text {min }}^{-\frac{1}{2}}\right|<em _max="\max">{2} z</em>$ and squaring the result, we have:</p>
<p>$$
\left(\frac{\theta}{\tau\left|W_{\min}^{-\frac{1}{2}}\right|<em _max="\max">{2} z</em>
$$}}\right)^{2} \leq\left|\Gamma_{c} C_{c}^{T}\left(C_{c} \Gamma_{c} C_{c}^{T}+W_{c}\right)^{-1} C_{c} \Gamma_{c}\right|_{2</p>
<p>Considering the covariance update equation (Equation 13), and considering that $\left|A_{c}\right|_{2}\leq 1$, it must be that</p>
<p>$\left|\Sigma_{a}-\Sigma_{b}\right|<em min="min">{2}\geq\left(\frac{\theta}{\tau\left|W</em>\right|}^{-\frac{1}{2}<em max="max">{2}z</em>.$}}\right)^{2</p>
<p>Since Lemma 1 establishes that $\Sigma$ decreases by a constant amount each time algorithm 1 replans, the following Theorem is able to conclude that after a finite number of replans, the belief state mean converges exponentially toward the final value constraint. We require the following additional assumptions:</p>
<ol>
<li>The planner always finds a trajectory such that the belief space mean satisfies the final value constraint on the final timestep, $T$.</li>
<li>B-LQR stabilizes the trajectory within the replanning threshold, $\theta$.</li>
</ol>
<p>Theorem 2: Under the conditions of Lemma 1 and the conditions above, Algorithm 1 causes the mean of belief state to be exponentially convergent to the final value condition after a finite number of replanning steps.</p>
<p>Proof: Under Algorithm 1, the system does one of two things: 1) the mean of the belief system never exceeds the replan threshold and B-LQR gives us exponential convergence of the mean to the final value condition, or 2) the mean exceeds the replan threshold. In the second case, we have by Lemma 1 that $\Sigma$ decreases by a fixed minimum amount in one or more directions. For an initial $\Sigma$ with finite variance, this is only possible a finite number of times. After that, condition 1 above becomes true and the mean of the belief system is exponentially convergent.</p>
<h2>VI. EXPERIMENTS</h2>
<p>We explored the capabilities of our approach to belief space planning in two experimental domains: the light-dark domain and the planar grasping domain.</p>
<h3>A. The light-dark domain</h3>
<p>In the light-dark domain, a robot must localize its position in the plane before approaching the goal. The robot’s ability to localize itself depends upon the amount of light present at its actual position. Light varies as a quadratic function of the horizontal coordinate. Depending upon the goal position, the initial robot position, and the configuration of the light, the robot may need to move away from its ultimate goal in order to localize itself. Figure 1 illustrates the configuration of the light-dark domain used in our experiments. The goal position is at the origin, marked by an $X$ in the figure. The intensity in the figure illustrates the magnitude of the light over the plane. The robot’s initial position is unknown.</p>
<p>The underlying state space is the plane, $x \in \mathbb{R}^{2}$. The robot is modeled as a first-order system such that robot velocity is determined by the control actions, $u \in \mathbb{R}^{2}$. The underlying system dynamics are linear with zero process noise, $f\left(x_{t}, u_{t}\right)=x_{t}+u$. The observation function is identity, $g\left(x_{t}\right)=x_{t}+\omega$, with zero-mean Gaussian observation noise a function of state, $\omega \sim \mathcal{N}(\cdot \mid 0, w(x))$, where</p>
<p>$$
w(x)=\frac{1}{2}\left(5-x_{x}\right)^{2}+\text { const. }
$$</p>
<p>has a minimum when $x_{x}=5$, where $x_{x}$ is the first element of $x$. Belief state was modeled as an isotropic Gaussian pdf over the state space: $b=(m, s) \in \mathbb{R}^{2} \times \mathbb{R}^{+}$. The cost function (Equation 8) used recurring state and action costs of $R=\operatorname{diag}(0.5,0.5)$ and $Q=\operatorname{diag}(0.5,0.5)$, and a final cost on covariance, $\Lambda=200$. B-LQR had an additional large final cost on mean. Direct transcription used a final value constraint on mean, $m=(0,0)$, instead. The true initial state was $x_{1}=(2.5,0)$ and the prior belief (initial belief state) was $b_{1}=(2,2,5)$. The replanning threshold was $\theta=0.1$. At each replanning step, direct transcription was initialized with a random trajectory. B-LQR linearized the belief system dynamics about $(0,0,0.5)$.</p>
<p>1) Results and discussion: Figure 1 shows solutions to the light-dark problem domain found by B-LQR (the dotted line) and by direct transcription (the solid line). The BLQR trajectory shows the integrated policy assuming that the assumed observation dynamics were always obtained. The direct transcription trajectory shows the initial plan found before replanning. Most importantly, notice that even though the B-LQR trajectory is based on a poor linearization of the belief dynamics, it performs surprisingly well (compare with the locally optimal direct transcription trajectory). However, it is clear that B-LQR is sub-optimal because whereas the locally optimal trajectory lingers in the minimum-noise region at $x_{x}=5$, the B-LQR trajectory overshoots past the minimum noise point to $x_{x}=6$.</p>
<p>Figure 2 illustrates the behavior of the replanning performed by Algorithm 1. The dotted line shows the mean of the belief space trajectory that was found on the first planning step. The solid line shows the actual trajectory. Whereas the system expected that it began execution at $x=(2,2)$, it</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. Comparison of the true robot trajectory (solid line) and the mean of the belief trajectory that was initially planned by direct transcription (dotted line).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. Laser-grasp domain. A range-finding laser (the dashed line) points out from the robot end effector. The objective is to move the end-effector to a point just in front of the puck on the left (the manipulator approach configuration).</p>
<p>actually began execution at x = (2.5, 0). As a result of this confusion, it initially took actions consistent with its initial belief. However, as it moved into the light, it quickly corrected its misperception. After reaching the point of maximum light intensity, the system subsequently followed a nearly straight line toward the goal.</p>
<h3><em>B. Laser-grasp domain</em></h3>
<p>In the laser-grasp domain, a planar robot manipulator must locate and approach a round puck as illustrated in Figure 3. The robot manipulator position is known, but the puck position is unknown. The robot locates the puck using a range-bound laser range finder that points out from the end-effector along a line. The end-effector always points horizontally as indicated. In order to solve the task, the manipulator must move back and forth in front of the puck so that the laser (the laser is always switched on) detects the puck location and then moves to the grasp approach point. The robot is controlled by specifying Cartesian end-effector velocities.</p>
<p><em>1) Setup:</em> The underlying state space, x ∈ R², denotes the position of the manipulator relative to an "approach point" defined directly in front of the object. Although the end-effector position is assumed to be known completely, state is</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Trajectory found using direct transcription for the laser-grasp domain. The dotted line denotes the mean of the belief state trajectory. The ellipses sample the covariance matrix at various points along the trajectory. The half circle on the left represents the puck. Just to the right of the puck, the end-effector is illustrated at the approach point.</p>
<p>not observed directly because the object position is unknown. The control action, u ∈ R², specifies the end-effector velocity:</p>
<p>$$f(x_t, u_t) = x_t + u_t.$$</p>
<p>In order to get a smooth measurement function that is not discontinuous at the puck edges, the puck was modeled using a symmetric squashing function about the origin. The result was roughly circular with a "radius" of approximately 0.65. The measurement gradient, C, was zero outside of that radius. State-dependent noise was defined to be large when the laser scan line was outside the radius of the puck (modeling an unknown and noisy background). The noise function also incorporated a low-amplitude quadratic about x<sup>x</sup> = 5 modeling a sensor with maximum measurement accuracy at 5 units away from the target. The belief space was modeled as a non-isotropic Gaussian in the plane: b = (m, s) ∈ R² × R³. The parameterization of covariance in R³ rather than R⁴ is a result of incorporating the symmetry of the covariance matrix into the representation. The cost function was of the form of Equation 8 with Q = diag(10, 10, 10, 10, 10), R = diag(10, 10), and Λ = diag(10000, 0, 0, 10000) (denoting no preference for covariance in one direction over another).</p>
<p><em>2) Results and discussion:</em> Figure 4 shows a representative plan found by direct transcription that moves the end-effector along the dotted line from its starting location in the upper right to the goal position in front of the puck (The geometry of the experiment roughly follows Figure 3). The initial belief state was b<sup>1</sup> = (9, 5, 5, 0, 5). The ellipses in Figure 4 illustrate the planned trajectory of the Gaussian. First, notice that the system plans to move the laser in front of the puck so that it may begin to localize it. Covariance does not change until the end-effector is actually in front of the puck. Also, notice that the plan lingers in front of the puck near the optimal sensor range. During this time, the trajectory makes short jumps up and down, apparently "scanning" the puck. Finally, as time</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Planned belief trajectory as a function of time step. The two black lines denote the mean of the belief. The three gray lines denote the elements of covariance. Notice that as the plan is "scanning" the puck, different elements of covariance change in alternation.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Comparison between the initially planned trajectory (the gray line) and the actual trajectory (the black line).
approaches the planning horizon, the end-effector moves off to the approach point. Figure 5 provides a more in-depth look at what is going on while the plan scans the puck. First, notice that the plan spends almost all its time in the "sweet spot" scanning the puck. Second, notice that different elements of covariance change during different phases of the scanning motion. This suggests that during scanning, the plan actively alternates between reducing covariance in different directions. The effect results from the fact that the dimension of the observation (a scalar scan depth) is one while the Gaussian belief is over a two-dimensional space. At a given point in time, it is only possible to minimize one dimension of covariance and the plan alternates between minimizing various different dimensions, resulting in the scanning behavior. Finally, Figure 6 illustrates the behavior the replanning strategy where time varying B-LQR stabilization was used. Initially, the mean of the system prior is at $m=(9,5)$ but the true state is at $x=(10,4.5)$. The incorrect belief persists until the system reaches a point in front of the puck. At this point, the incorrect prior is gradually corrected during the scanning process until the true state of the system finally reaches the origin.</p>
<h2>VII. CONCLUSION</h2>
<p>This paper explores the application of underactuated planning and control approaches to the belief space planning problem. Belief state dynamics are underactuated because the number of controlled dimensions (the parameters of a probability distribution) exceeds the number of independent control inputs. As a result, the dynamics are constrained in a way that can make planning difficult. Our contribution is to recast the belief space planning problem in such a way that conventional planning and control techniques are applicable. As a result, we are able to find belief space policies using linear quadratic regulation (LQR) and locally optimal belief space trajectories using direct transcription. We provide theoretical results characterizing the effectiveness of a plan-and-replan strategy. Finally, we show that the approach produces interesting and relevant behaviors on a simple grasp problem where it is necessary to acquire information before acting.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>This work was sponsored by the Office of Secretary of Defense under Air Force Contract FA8721-05-C-0002.</p>
<h2>REFERENCES</h2>
<p>[1] C. Corcoran and R. Platt, "Tracking object pose and shape during robot manipulation based on tactile information," in IEEE Int'l Conf. on Robotics and Automation, vol. 2, 2010.
[2] A. Petrovskaya, O. Khatib, S. Thrun, and A. Ng, "Bayesian estimation for autonomous object manipulation based on tactile sensors," in IEEE Int'l Conf. on Robotics and Automation, 2006, pp. 707-714.
[3] C. Papadimitriou and J. Tsitsiklis, "The complexity of markov decision processes," Mathematics of Operations Research, vol. 12, no. 3, pp. 441-450, 1987.
[4] S. Miller, A. Harris, and E. Chong, "Coordinated guidance of autonomous uavs via nominal belief-state optimization," in American Control Conference, 2009, pp. 2811-2818.
[5] T. Erez and W. Smart, "A scalable method for solving high-dimensional continuous pomdps using local approximation," in Proceedings of the International Conference on Uncertainty in Artificial Intelligence, 2010.
[6] M. Littman, A. Cassandra, and L. Kaelbling, "Learning policies for partially observable environments: Scaling up," in Proceedings of the Twelfth International Conference on Machine Learning, 1995.
[7] M. Hauskrecht, "Value-function approximations for partially observable markov decision processes," Journal of Artificial Intelligence Research, vol. 13, pp. 33-94, 2000.
[8] S. Prentice and N. Roy, "The belief roadmap: Efficient planning in linear pomdps by factoring the covariance," in 12th International Symposium of Robotics Research, 2008.
[9] J. Van der Berg, P. Abbeel, and K. Goldberg, "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information," in Proceedings of Robotics: Science and Systems (RSS), 2010.
[10] S. Yoon and R. Fern, A. Givan, "FF-replan: A baseline for probabilistic planning," in Proceedings of the International Conference on Automated Planning and Scheduling, 2007.
[11] D. Bertsekas, Dynamic Programming and Optimal Control: 3rd Edition. Athena Scientific, 2007.
[12] S. LaValle and J. Kuffner, "Randomized kinodynamic planning," International Journal of Robotics Research, vol. 20, no. 5, pp. 378-400, 2001.
[13] R. Tedrake, "LQR-trees: Feedback motion planning on sparse randomized trees," in Proceedings of Robotics: Science and Systems (RSS), 2009.
[14] J. Betts, Practical methods for optimal control using nonlinear programming. Siam, 2001.</p>            </div>
        </div>

    </div>
</body>
</html>
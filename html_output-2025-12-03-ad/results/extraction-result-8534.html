<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8534 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8534</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8534</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-de74100ac5a16f9169b8a7fe98dd05647b0e8777</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de74100ac5a16f9169b8a7fe98dd05647b0e8777" target="_blank">SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs’ capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ.</p>
                <p><strong>Paper Abstract:</strong> This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs’ capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8534",
    "paper_id": "paper-de74100ac5a16f9169b8a7fe98dd05647b0e8777",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00531875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning</h1>
<p>Roshanak Mirzaee<em> Hossein Rajaby Faghihi</em> Qiang Ning ${ }^{\text {a }}$ Parisa Kordjamshidi<em><br></em>Michigan State University ${ }^{\text {A }}$ Amazon<br>{mirzaeem, rajabyfa, kordjams}@msu.edu qning@amazon.com</p>
<h4>Abstract</h4>
<p>This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.</p>
<h2>1 Introduction</h2>
<p>Spatial reasoning is a cognitive process based on the construction of mental representations for spatial objects, relations, and transformations (Clements and Battista, 1992), which is necessary for many natural language understanding (NLU) tasks such as natural language navigation (Chen et al., 2019; Roman Roman et al., 2020; Kim et al., 2020), human-machine interaction (Landsiedel et al., 2017; Roman Roman et al., 2020), dialogue systems (Udagawa et al., 2020), and clinical analysis (Datta and Roberts, 2020).</p>
<p>Modern language models (LM), e.g., BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), and XLNet (Yang et al., 2019) have seen great successes in natural language processing (NLP). However, there has been limited investigation into spatial reasoning capabilities of LMs. To the best of our knowledge, bAbI (Weston et al., 2015) (Fig 9) is the only dataset with direct textual spatial question answering (QA) (Task 17), but it is synthetic</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and overly simplified: (1) The underlying scenes are spatially simple, with only three objects and relations only in four directions. (2) The stories for these scenes are two short, templated sentences, each describing a single relation between two objects. (3) The questions typically require up to two-steps reasoning due to the simplicity of those stories.</p>
<p>To address these issues, this paper proposes a new dataset, SPARTQA ${ }^{1}$ (see Fig. 1). Specifically, (1) SPARTQA is built on NLVR's (Suhr et al., 2017) images containing more objects with richer spatial structures (Fig. 1b). (2) SPARTQA's stories are more natural, have more sentences, and richer in spatial relations in each sentence. (3) SPARTQA's questions require deeper reasoning and have four types: find relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine-grained analysis of models' capabilities.</p>
<p>We showed annotators random images from NLVR, and instructed them to describe objects and relationships not exhaustively at the cost of naturalness (Sec. 3). In total, we obtained 1.1 k unique QA pair annotations on spatial reasoning, evenly distributed among the aforementioned types. Similar to bAbI, we keep this dataset in relatively small scale and suggest to use as little training data as possible. Experiments show that modern LMs (e.g., BERT) do not perform well in this low-resource setting.</p>
<p>This paper thus proposes a way to obtain distant supervision signals for spatial reasoning (Sec. 4). As spatial relationships are rarely mentioned in existing corpora, we take advantage of the fact that spatial language is grounded to the geometry of visual scenes. We are able to automatically generate stories for NLVR images (Suhr et al., 2017) via our newly designed context free grammars (CFG) and context-sensitive rules. In the process of story generation, we store the information about all ob-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>STORY:</h1>
<p>We have three blocks, A, B and C. Block B is to the right of block C and it is below block A. Block A has two black medium squares. Medium black square number one is below medium black square number two and a medium blue square. It is touching the bottom edge of this block. The medium blue square is below medium black square number two. Block B contains one medium black square. Block C contains one medium blue square and one medium black square. The medium blue square is below the medium black square.</p>
<h2>QUESTIONS:</h2>
<p>FB: Which block(s) has a medium thing that is below a black square? A, B, C
FB: Which block(s) doesn't have any blue square that is to the left of a medium square? A, B
FR: What is the relation between the medium black square which is in block C and the medium square that is below a medium black square that is touching the bottom edge of a block? Left
CO: Which object is above a medium black square? the medium black square which is in block C or medium black square number two? medium black square number two
YN: Is there a square that is below medium square number two above all medium black squares that are touching the bottom edge of a block? Yes
(a) An example story and corresponding questions and answers.
<img alt="img-0.jpeg" src="img-0.jpeg" />
(b) An example NLVR image and the scene created in Fig. 1a, where the blocks in the NLVR image are rearranged.</p>
<p>Figure 1: Example from SPARTQA (specifically from SPARTQA-AUTO)
jects and relationships, such that QA pairs can also be generated automatically. In contrast to bAbI, we use various spatial rules to infer new relationships in these QA pairs, which requires more complex reasoning capabilities. Hereafter, we call this automatically-generated dataset SPARTQA-AUTO, and the human-annotated one SPARTQA-HUMAN.</p>
<p>Experiments show that, by further pretraining on SPARTQA-AUTO, we improve LMs' performance on SPARTQA-HUMAN by a large margin. ${ }^{2}$ The spatially-improved LMs also show stronger performance on two external QA datasets, bAbI and boolQ (Clark et al., 2019): BERT further pretrained on SPARTQA-AUTO only requires half of the training data to achieve $99 \%$ accuracy on bAbI as compared to the original BERT; on boolQ's development set, this model shows better performance than BERT, with $2.3 \%$ relative error reduction. ${ }^{3}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Our contributions can be summarized as follows. First, we propose the first human-curated benchmark, SPARTQA-HUMAN, for spatial reasoning with richer spatial phenomena than the prior synthetic dataset bAbI (Task 17).</p>
<p>Second, we exploit the scene structure of images and design novel CFGs and spatial reasoning rules to automatically generate data (i.e., SPARTQAAuto) to obtain distant supervision signals for spatial reasoning over text.</p>
<p>Third, SPARTQA-AUTO proves to be a rich source of spatial knowledge that improved the performance of LMs on SPARTQA-HUMAN as well as on different data domains such as bAbI and boolQ.</p>
<h2>2 Related work</h2>
<p>Question answering is a useful format to evaluate machines' capability of reading comprehension (Gardner et al., 2019) and many recent works have been implementing this strategy to test machines' understanding of linguistic formalisms: He et al. (2015); Michael et al. (2018); Levy et al. (2017); Jia et al. (2018); Ning et al. (2020); Du</p>
<p>and Cardie (2020). An important advantage of QA is using natural language to annotate natural language, thus having the flexibility to get annotations on complex phenomena such as spatial reasoning. However, spatial reasoning phenomena have been covered minimally in the existing works.</p>
<p>To the best of our knowledge, Task 17 of the bAbI project (Weston et al., 2015) is the only QA dataset focused on textual spatial reasoning (examples in Appendix F). However, bAbI is synthetic and does not reflect the complexity of the spatial reasoning in natural language. Solving Task 17 of bAbI typically does not require sophisticated reasoning, which is an important capability emphasized by more recent works (e.g., Dua et al. (2019); Khashabi et al. (2018); Yang et al. (2018); Dasigi et al. (2019); Ning et al. (2020)).</p>
<p>Spatial reasoning is arguably more prominent in multi-modal QA benchmarks, e.g., NLVR (Suhr et al., 2017), VQA (Antol et al., 2015), GQA (Hudson and Manning, 2019), CLEVR (Johnson et al., 2017). However, those spatial reasoning phenomena are mostly expressed naturally through images, while this paper focuses on studying spatial reasoning on natural language. Some other works on visual-spatial reasoning are based on geographical information inside maps and diagrams (Huang et al., 2019) and navigational instructions (Chen et al., 2019; Anderson et al., 2018).</p>
<p>As another approach to evaluate spatial reasoning capabilities of models, a dataset proposed in Ghanimifard and Dobnik (2017) generates a synthetic training set of spatial sentences and evaluates the models' ability to generate spatial facts and sentences containing composition and decomposition of relations on grounded objects.</p>
<h2>3 SPARTQA-HUMAN</h2>
<p>To mitigate the aforementioned problems of Task 17 of bAbI, i.e., simple scenes, stories, and questions, we describe the data annotation process of SPARTQA-HUMAN, and explain how those problems were addressed in this section.</p>
<p>First, we randomly selected a subset of NLVR images, each of which has three blocks containing multiple objects (see Fig 1b). The scenes shown by these images are more complicated than those described by bAbI because (1) there are more objects in NLVR images; (2) the spatial relationships in NLVR are not limited to just four relative directions as objects are placed arbitrarily within blocks.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: For "A blue circle is above a big triangle. To the left of the big triangle, there is a square," if the question is: "Is the square to the left of the blue circle?", the answer is neither Yes nor No. Thus, the correct answer is "Do not Know" (DK) in our setting.</p>
<p>Second, two student volunteers produced textual description of those objects and their corresponding spatial relationships based on these images. Since the blocks are always horizontally aligned in each NLVR image, to allow for more flexibility, annotators could also rearrange these blocks (see Fig. 1a). Relationships between objects within the same block can take the forms of relative direction (e.g., left or above), qualitative distance (e.g., near or far), and topological relationship (e.g., touching or containing).</p>
<p>However, we instructed the annotators not to describe all objects and relationships, (1) to avoid unnecessarily verbose stories, and (2) to intentionally miss some information to enable more complex reasoning later. Therefore, annotators describe only a random subset of blocks, objects, and relationships.</p>
<p>To query more interesting phenomena, annotators were then encouraged to write questions requiring detecting relations and reasoning over them using multiple spatial rules. A spatial rule can be one of the transitivity $(A \rightarrow B, B \rightarrow C \Rightarrow$ $A \rightarrow C)$, symmetry $(A \rightarrow B \Rightarrow B \rightarrow A)$, converse $((A, R, B) \Rightarrow(B$, reverse $(R), A))$, inclusion (obj1 in $A$ ), and exclusion (obj1 not in $B$ ) rules.</p>
<p>There are four types of questions (Q-TYPE). (1) $F R$ : find relation between two objects. (2) $F B$ : find the block that contains certain object(s). (3) $C O$ : choose between two objects mentioned in the question that meets certain criteria. (4) $Y N$ : a yes/no question that tests if a claim on spatial relationship holds.</p>
<p>FB, FR, and CO questions are formulated as multiple-choice questions ${ }^{4}$ and receive a list of candidate answers, and YN questions' answer is choosing from Yes, No, or "DK" (Do not Know). The "DK" option is due to the open-world assumption of the stories, where if something is not described</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sets</th>
<th style="text-align: right;">FB</th>
<th style="text-align: right;">FR</th>
<th style="text-align: right;">YN</th>
<th style="text-align: right;">CO</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPARTQA-HUMAN:</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: right;">104</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">194</td>
<td style="text-align: right;">107</td>
<td style="text-align: right;">510</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">154</td>
<td style="text-align: right;">149</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">151</td>
<td style="text-align: right;">616</td>
</tr>
<tr>
<td style="text-align: left;">SPARTQA-AUTO:</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Seen Test</td>
<td style="text-align: right;">3872</td>
<td style="text-align: right;">3712</td>
<td style="text-align: right;">3896</td>
<td style="text-align: right;">3594</td>
<td style="text-align: right;">15074</td>
</tr>
<tr>
<td style="text-align: left;">Unseen Test</td>
<td style="text-align: right;">3872</td>
<td style="text-align: right;">3721</td>
<td style="text-align: right;">3896</td>
<td style="text-align: right;">3598</td>
<td style="text-align: right;">15087</td>
</tr>
<tr>
<td style="text-align: left;">Dev</td>
<td style="text-align: right;">3842</td>
<td style="text-align: right;">3742</td>
<td style="text-align: right;">3860</td>
<td style="text-align: right;">3579</td>
<td style="text-align: right;">15023</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">23654</td>
<td style="text-align: right;">23302</td>
<td style="text-align: right;">23968</td>
<td style="text-align: right;">22794</td>
<td style="text-align: right;">93673</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of questions per Q-TYPE
in the text, it is not considered as false (See Fig. 2).
Finally, annotators were able to create 1.1k QA pairs on spatial reasoning on the generated descriptions, distributed among the aforementioned types. We intentionally keep this data in a relatively small scale due to two reasons. First, there has been some consensus in our community that modern systems, given their sufficiently large model capacities, can easily find shortcuts and overfit a dataset if provided with a large training data (Gardner et al., 2020; Sen and Saffari, 2020). Second, collecting spatial reasoning QAs is very costly: The two annotators spent 45-60 mins on average to create a single story with 8-16 QA pairs. We estimate that SPARTQA-HUMAN costed about 100 human hours in total. The expert performance on 100 examples of SPARTQA-HUMAN's test set measured by their accuracy of answering the questions is $92 \%$ across four Q-TYPES on average, indicating its high quality.</p>
<h2>4 Distant Supervision: SPARTQA-AUTO</h2>
<p>Since human annotations are costly, it is important to investigate ways to generate distant supervision signals for spatial reasoning. However, unlike conventional distant supervision approaches (e.g., Mintz et al. (2009); Zeng et al. (2015); Zhou et al. (2020)) where distant supervision data can be selected from large corpora by implementing specialized filtering rules, spatial reasoning does not appear often in existing corpora. Therefore, similar to SPARTQA-HUMAN, we take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SPARTQAAuto, and below we describe its generation process in detail.</p>
<p>Story generation Since NLVR comes with structured descriptions of the ground truth locations of those objects, we were able to choose random
blocks and objects from each image programmatically. The benefit is two-fold. First, a random selection of blocks and objects allows us to create multiple stories for each image; second, this randomness also creates spatial reasoning opportunities with missing information.</p>
<p>Once we decide on a set of blocks and objects to be included, we determine their relationships: Those relationships between blocks are generated randomly; as for those between objects, we refer to the ground truth of these images to determine them.</p>
<p>Now we have a scene containing a set of blocks and objects and their associated relationships. To produce a story for this scene, we design CFGs to produce natural language sentences that describe those blocks/objects/relationships in various expressions (see Fig. 3 for two portions of our CFG describing relative and nested relations between objects).</p>
<h2>The big black shape is above the medium triangle.</h2>
<p>$S \rightarrow&lt;$ Article $&gt;&lt;$ Object $&gt;$ is $&lt;$ Relation $&gt;&lt;$ Article $&gt;&lt;$ Object $&gt;$.
Article $\rightarrow$ the $\mid a$
Relation $\rightarrow$ above $\mid$ left $\mid \ldots$
Object $\rightarrow&lt;$ Size $&gt;<em>&lt;$ Color $&gt;</em>&lt;$ Shape $\mid$ Ind_shape $&gt;$
Size $\rightarrow$ small $\mid$ medium $\mid$ big
Color $\rightarrow$ yellow $\mid$ blue $\mid$ black
Shape $\rightarrow$ square $\mid$ triangle $\mid$ circle
Ind_shape $\rightarrow$ shape $\mid$ object $\mid$ thing
(a) Part of the grammar describing relations between objects</p>
<p>The big black shape is above the object that is to the right of the medium triangle
$S \rightarrow&lt;$ Article $&gt;&lt;$ Object $&gt;$ is $&lt;$ Relation $&gt;&lt;$ Article $&gt;$ $&lt;$ Object $&gt;$.
Object $\rightarrow&lt;$ Size $&gt;<em>&lt;$ Color $&gt;</em>&lt;$ Shape $\mid$ Ind_shape $&gt;\mid$ $&lt;$ Ind_shape $&gt;$ that is $&lt;$ Relation $&gt;&lt;$ Object $&gt;$
(b) Part of the grammar describing nested relationships.</p>
<p>Figure 3: Two parts of our designed CFG
Being grounded to visual scenes guarantees spatial coherency in a story, and using CFGs helps to have correct sentences (grammatically) and various expressions. We also design context-sensitive rules to limited options for each CFG's variable based on the chosen entities (e.g. black circle), or what is described in the previous sentences (e.g. Block A has $a$ circle. The circle is below $a$ triangle.)</p>
<p>Question generation To generate questions based on a passage, there are rule-based sys-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Find the implicit relation between obj1 and obj 4 by Transitivity rule. (1) Find a set of objects that have a relation with obj1. Continue the same process on the new set until obj 4 is found. (2) Get the union of the intermediate relations between these two objects and it is the final answer.
tems (Heilman and Smith, 2009; Labutov et al., 2015), neural networks (Du et al., 2017), and their combinations (Dhole and Manning, 2020). However, in our approach, during generating each story, the program stores the information about the entities and their relationships. Thus, without processing the raw text, which is error-prone, we generate questions by only looking at the stored data. The question generation operates based on four primary functionalities, Choose-objects, Describe-objects, Find-all-relations, and Find-similar-objects. These modules are responsible to control the logical consistency, correctness, and the number of steps required for reasoning in each question.</p>
<p>Choose-objects randomly chooses up to three objects from the set of possible objects in a story under a set of constraints such as preventing selection of similar objects, or excluding objects with relations that are directly mentioned in the text.</p>
<p>Describe-Objects generates a mention phrase for an object using parts of its full name (presented in the story). The generated phrase is either pointing to a unique object or a group of objects such as "the big circle," or "big circles." To describe a unique object, it chooses an attribute or a group of attributes that apply to a unique object among others in the story. To increase the steps of reasoning, the description may include the relationship of the object to other objects instead of using a direct unique description. For example, "the circle which is above the black triangle."</p>
<p>Find-all-relations completes the relationship graph between objects by applying a set of spatial rules such as transitivity, symmetry, converse, inclusion, and exclusion on top of the direct relations described in the story. As shown in Fig. 4, it does an exhaustive search over all combinations of the relations that link two objects to each other.</p>
<p>Find-similar-objects finds all the mentions matching a description from the question to objects
in the story. For instance, for the question "is there any blue circle above the big blue triangle?", this module finds all the mentions in the story matching the description "a blue circle".</p>
<p>Similar to the SPARTQA-HUMAN, we provide four Q-TYPES FR, FB, CO, and YN. To generate FR questions, we choose two objects using Choose-objects module and question their relationships. The YN Q-TYPE is similar to FR, but the question specifies one relationship of interest chosen from all relation extracted by Find-all-relations module to be questioned about the objects. Since most of the time, Yes/No questions are simpler problems, we make this question type more complex by adding quantifiers (adding "all" and "any"). These quantifiers help to evaluates the models' capability to aggregate relations between more than two objects in the story and do the reasoning over all find relations to find the final answer. In FB Q-TYPE, we mention an object by its indirect relation to another object using the nested relation in Describe-objects module and ask to find the blocks containing or not containing this object. Finally, the CO question selects an anchor object (Choose-objects) and specifies a relationship ( using Find-all-relations) in the question. Two other objects are chosen as candidates to check whether the specified relationship holds between them and the anchor object. We tend to force the algorithm to choose objects as candidates that at least have one relationship to the anchor object. To see more details about different question' templates see Table 7 in the Appendix.</p>
<p>Answer generation We compute all direct and indirect relationships between objects using Find-all-relations function and based on the Q-TYPES generate the final answer.</p>
<p>For instance, in YN Q-TYPE if the asked relation exists in the found relations, the answer is "Yes", if the inverse relation exists it must be "No", and otherwise, it is "DK"5.</p>
<h3>4.1 Corpus Statistics</h3>
<p>We generate the train, dev, and test set splits based on the same splits of the images in the NLVR dataset. On average, each story contains 9 sentences (Min:3, Max: 22) and 118 tokens (Min: 66,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Max: 274). Also, the average tokens of each question (on all Q-TYPE ) is 23 (Min:6, Max: 57).</p>
<p>Table 1 shows the total number of each question type in SPARTQA-AUTO (Check Appendix to see more statistic information about the labels in Tab 8.)</p>
<h2>5 Models for Spatial Reasoning over Language</h2>
<p>This section describes the model architectures on different Q-TYPES: FR, YN, FB, and CO. All QTYPES can be cast into a sequence classification task, and the three transformer-based LMs tested in this paper, BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), and XLNet (Yang et al., 2019), can all handle this type of tasks by classifying the representation of [CLS], a special token prepended to each target sequence (see Appendix E). Depending on the Q-TYPE, the input sequence and how we do inference may be different.</p>
<p>FR and YN both have a predefined label set as candidate answers, and their input sequences are both the concatenation of a story and a question. While the answer to a YN question is a single label chosen from Yes, No, and DK, FR questions can have multiple correct answers. Therefore, we treat each candidate answer to FR as an independent binary classification problem, and take the union as the final answer. As for YN, we choose the label with the highest confidence (Fig 8b).</p>
<p>As the candidate answers to FB and CO are not fixed and depend on each story and its question the input sequences to these Q-TYPES are concatenated with each candidate answer. Since the defined YN and FR model has moderately less accurate results on FB and CO Q-TYPES, we add a LSTM (Hochreiter and Schmidhuber, 1997) layer to improve it. Hence, to find the final answer, we run the model with each candidate answer and then apply an LSTM layer on top of all token representations. Then, we use the last vector of the LSTM outputs for classification (Fig 8a). The final answers are selected based on Eq. (1).</p>
<p>$$
\begin{aligned}
x_{i} &amp; =\left[s, c_{i}, q\right] \
\vec{T}<em 1="1">{i} &amp; =\left[t</em>\right) \
{\left[\vec{h}}^{j}, \ldots, t_{m^{\prime}}^{r^{\prime}}\right]=L M\left(x_{i<em m_prime="m^{\prime">{1}^{i}, \ldots, \vec{h}</em>}}^{i}\right] } &amp; =\operatorname{LSTM}\left(\vec{T<em i="i">{i}\right) \
\vec{y}</em> W\right))
\end{aligned}
$$}=\left[y_{i}^{0}, y_{i}^{1}\right] &amp; =\operatorname{Softmax}\left(\vec{h}_{m^{\prime}}^{T</p>
<p>where $s$ is the story, $c_{i}$ is the candidate answer, $q$ is the question, [ ] indicates the concatenation of the listed vectors, and $m_{i}$ is tokens' number in $x_{i}$. The parameter vector, $W$, is shared for all candidates.</p>
<h3>5.1 Training and Inference</h3>
<p>We train the models based on the summation of the cross-entropy losses of all binary classifiers in the architecture. For FR and YN Q-TYPES, there are multiple classifiers, while there is only one classifier used for CO and FB Q-TYPES.</p>
<p>We remove inconsistent answers in postprocessing for FR and YN Q-TYPES during inference phase. For instance on FR, left and right relations between two objects cannot be valid at the same time. For YN, as there is only one valid answer amongst the three candidates, we select the candidate with the maximal predicted probability of being the true answer.</p>
<h2>6 Experiments</h2>
<p>As fine-tuning LMs has become a common baseline approach to knowledge transfer from a source dataset to a target task, including but not limited to Phang et al. (2018); Zhou et al. (2020); He et al. (2020b), we study the capability of spatial reasoning of modern LMs, specifically BERT, ALBERT, and XLNet, after fine-tuning them on SPARTQAAuto. This fine-tuning process is also known as further pretraining, to distinguish with the finetuning process on one's target task. It is an open problem to find out better transfer learning techniques than simple further pretraining, as suggested in He et al. (2020a); Khashabi et al. (2020), which is beyond the scope of this work. All experiments use the models proposed in Sec. 5. We use AdamW (Loshchilov and Hutter, 2017) with $2 \times 10^{-6}$ learning rate and Focal Loss (Lin et al., 2017) with $\gamma=2$ for training all the models. ${ }^{6}$</p>
<h3>6.1 Further pretraining on SPARTQA-AUTO improves spatial reasoning</h3>
<p>Table 2 shows performance on SPARTQA-HUMAN in a low-resource setting, where 0.6 k QA pairs from SPARTQA-HUMAN are used for fine-tuning these LMs and 0.5 k for testing (see Table 1 for information on this split). ${ }^{7}$ During our annotation, we found that the description of "near to " and "far</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">#</th>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">FB</th>
<th style="text-align: right;">FR</th>
<th style="text-align: right;">CO</th>
<th style="text-align: right;">YN</th>
<th style="text-align: right;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Majority</td>
<td style="text-align: right;">28.84</td>
<td style="text-align: right;">24.52</td>
<td style="text-align: right;">40.18</td>
<td style="text-align: right;">53.60</td>
<td style="text-align: right;">36.64</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: right;">16.34</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">26.16</td>
<td style="text-align: right;">45.36</td>
<td style="text-align: right;">30.17</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">BERT (Stories only; MLM)</td>
<td style="text-align: right;">21.15</td>
<td style="text-align: right;">16.19</td>
<td style="text-align: right;">27.1</td>
<td style="text-align: right;">$\mathbf{5 1 . 5 4}$</td>
<td style="text-align: right;">32.90</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">BERT (SPARTQA-AUTO; MLM)</td>
<td style="text-align: right;">19.23</td>
<td style="text-align: right;">29.54</td>
<td style="text-align: right;">$\mathbf{3 2 . 7 1}$</td>
<td style="text-align: right;">47.42</td>
<td style="text-align: right;">34.88</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">BERT (SPARTQA-AUTO)</td>
<td style="text-align: right;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: right;">$\mathbf{4 6 . 6 6}$</td>
<td style="text-align: right;">$\mathbf{3 2 . 7 1}$</td>
<td style="text-align: right;">47.42</td>
<td style="text-align: right;">$\mathbf{4 7 . 2 5}$</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">Human</td>
<td style="text-align: right;">91.66</td>
<td style="text-align: right;">95.23</td>
<td style="text-align: right;">91.66</td>
<td style="text-align: right;">90.69</td>
<td style="text-align: right;">92.31</td>
</tr>
</tbody>
</table>
<p>Table 2: Further pretraining BERT on SPARTQA-AUTO improves accuracies on SPARTQA-HUMAN. All systems are fine-tuned on the training data of SPARTQA-HUMAN, but Systems 3-5 are also further pretrained in different ways. System 3: further pretrained on the stories from SPARTQA-AUTO as a masked language model (MLM) task. System 4: further pretrained on both stories and QA annotations as MLM. System 5: the proposed model that is further pretrained on SPARTQA-AUTO as a QA task. Avg: The micro-average on all four Q-TYPES.
from" varies largely between annotators. Therefore, we ignore these two relations from FR Q-TYPE in our evaluations.</p>
<p>In Table 2, System 5, BERT (SPARTQA-AUTO), is the proposed method of further pretraining BERT on SPARTQA-AUTO. We can see that System 2, the original BERT, performs consistently lower than System 5, indicating that having SPARTQA-AUTO as a further pretraining task improves BERT's spatial understanding.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">$F_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Majority</td>
<td style="text-align: left;">35</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">50</td>
</tr>
<tr>
<td style="text-align: left;">BERT (Stories only; MLM)</td>
<td style="text-align: left;">53</td>
</tr>
<tr>
<td style="text-align: left;">BERT (SPARTQA-AUTO; MLM)</td>
<td style="text-align: left;">48</td>
</tr>
<tr>
<td style="text-align: left;">BERT (SPARTQA-AUTO)</td>
<td style="text-align: left;">48</td>
</tr>
</tbody>
</table>
<p>Table 3: Switching from accuracy in Table 2 to $F_{1}$ shows that the models are all performing better than the majority baseline on YN Q-TYPE.</p>
<p>In addition, we implement another two baselines. System 3, BERT (Stories only; MLM): further pretraining BERT only on the stories of SPARTQAAuto as a masked language model (MLM) task; System 4, BERT (SPARTQA-AUTO; MLM): we convert the QA pairs in SPARTQA-AUTO into textual statements and further pretrain BERT on the text as an MLM (see Fig. 5 for an example conversion).</p>
<p>To convert each question and its answer into a sentence, we utilize static templates for each question type which removes the question words and rearranges other parts into a sentence.</p>
<p>We can see that System 3 slightly improves over System 2, an observation consistent with many prior works that seeing more text generally helps an LM (e.g., Gururangan et al. (2020)). The signif-</p>
<p>A big circle is above a triangle. A blue square is below the triangle.
What is the relation between the circle and the blue object?
Answer: Above</p>
<p>A big circle is above a triangle. A blue square is below the triangle. The circle is [MASK] the blue object.
Answer: Above</p>
<p>Figure 5: Convert a triplet of (paragraph, question, answer) into a single piece of text for the MLM task.
icant gap between System 3 and the proposed System 5 indicates that supervision signals come more from our annotations in SPARTQA-AUTO rather than from seeing more unannotated text. System 4 is another way to make use of the annotations in SPARTQA-AUTO, but it is shown to be not as effective as further pretraining BERT on SPARTQAAuto as a QA task.</p>
<p>While the proposed System 5 overall performs better than the other three baseline systems, one exception is its accuracy on YN, which is lower than that of System 3. Since all systems' YN accuracies are also lower than the majority baseline ${ }^{8}$, we hypothesize that this is due to imbalanced data. To verify it, we compute the $F_{1}$ score for YN Q-TYPE in Table 3, where we see all systems effectively achieve better scores than the majority baseline. However, further pretraining BERT on SPARTQAAuto still does not beat other baseline systems, which implies that straightforward pretraining is not necessarily helpful in capturing the complex reasoning phenomena required by YN questions.</p>
<p>The human performance is evaluated on 100 ran-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">FB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">YN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Human*</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Human*</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Human*</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Human*</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">48.70</td>
<td style="text-align: center;">48.70</td>
<td style="text-align: center;">28.84</td>
<td style="text-align: center;">40.81</td>
<td style="text-align: center;">40.81</td>
<td style="text-align: center;">24.52</td>
<td style="text-align: center;">20.59</td>
<td style="text-align: center;">20.38</td>
<td style="text-align: center;">40.18</td>
<td style="text-align: center;">49.94</td>
<td style="text-align: center;">49.91</td>
<td style="text-align: center;">53.60</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">87.13</td>
<td style="text-align: center;">69.38</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">85.68</td>
<td style="text-align: center;">73.71</td>
<td style="text-align: center;">46.66</td>
<td style="text-align: center;">71.44</td>
<td style="text-align: center;">61.09</td>
<td style="text-align: center;">32.71</td>
<td style="text-align: center;">78.29</td>
<td style="text-align: center;">76.81</td>
<td style="text-align: center;">47.42</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">97.66</td>
<td style="text-align: center;">83.53</td>
<td style="text-align: center;">56.73</td>
<td style="text-align: center;">91.61</td>
<td style="text-align: center;">83.70</td>
<td style="text-align: center;">44.76</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">84.55</td>
<td style="text-align: center;">49.53</td>
<td style="text-align: center;">79.38</td>
<td style="text-align: center;">75.05</td>
<td style="text-align: center;">41.75</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">84.85</td>
<td style="text-align: center;">73.07</td>
<td style="text-align: center;">94.60</td>
<td style="text-align: center;">91.63</td>
<td style="text-align: center;">57.14</td>
<td style="text-align: center;">97.11</td>
<td style="text-align: center;">90.88</td>
<td style="text-align: center;">50.46</td>
<td style="text-align: center;">79.91</td>
<td style="text-align: center;">78.54</td>
<td style="text-align: center;">39.69</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.66</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">94.44</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.66</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.69</td>
</tr>
</tbody>
</table>
<p>Table 4: Spatial reasoning is challenging. We further pretrain three transformer-based LMs, BERT, ALBERT, and XLNet, on SPARTQA-AUTO, and test their accuracy in three ways: Seen and Unseen are both from SPARTQAAuto, where Unseen has applied minor modifications to its vocabulary; to get those Human columns, all models are fine-tuned on SPARTQA-HUMAN's training data. Human performance on Seen and Unseen is the same since the changes applied to Unseen does not affect human reasoning.
dom questions from each SPARTQA-AUTO and SPARTQA-HUMAN test set. The respondents are graduate students that were trained by some examples of the dataset before answering the final questions. We can see from Table 2 that all systems' performances fall behind human performance by a large margin. We expand on the difficulty of SPARTQA in the next subsection.</p>
<h3>6.2 SPARTQA is challenging</h3>
<p>In addition to BERT, we continue to test another two LMs, ALBERT and XLNet (Table 5). We further pretrain these LMs on SPARTQA-AUTO, and test them on SPARTQA-HUMAN (the numbers of BERT are copied from Table 2) and two held-out test sets of SPARTQA-AUTO, Seen and Unseen. Note that when a system is tested against SPARTQA-HUMAN, it is fine-tuned on SPARTQAHUMAN's training data following its further pretraining on SPARTQA-AUTO. We use the unseen set to test to what extent the baseline models use shortcuts in the language surface. This set applies minor modifications randomly on a number of stories and questions to change the names of shapes, colors, sizes, and relationships in the vocabulary of the stories, which do not influence the reasoning steps (more details in Appendix C.1).</p>
<p>All models perform worst in YN across all QTYPES, which suggests that YN presents a more complex phenomena, probably due to additional quantifiers in the questions. XLNet performs the best on all Q-TYPES except its accuracy on SPARTQA-HUMAN's YN section. However, the drops in Unseen and human suggest overfitting on the training vocabulary. The low accuracies on human test set from all models show that solving this benchmark is still a challenging problem and requires more sophisticated methods like considering spatial roles and relations extraction (Kordjamshidi
et al., 2010; Dan et al., 2020; Rahgooy et al., 2018) to understand stories and questions better.</p>
<p>To evaluate the reliability of the models, we also provide two extra consistency and contrast test sets. Consistency set is made by changing a part of the question in a way that seeks for the same information (Hudson and Manning, 2019; Suhr et al., 2019). Given a pivot question and answer of a specific consistency set, answering other questions in the set does not need extra reasoning over the story.</p>
<p>Contrast set is made by minimal modification in a question to change its answer (Gardner et al., 2020). For contrast sets, there is a need to go back to the story to find the new answer for the question's minor variations (see Appendix C. 2 for examples.) The consistency and contrast sets are evaluated only on the correctly predicted questions to check if the actual understanding and reasoning occurs. This ensures the reliability of the models.</p>
<p>Table 5 shows the result of this evaluation on four Q-TYPES of SPARTQA-AUTO, where we can see, for another time, that the high scores on the Seen test set are likely due to overfitting on training data rather than correct detection of spatial terms and reasoning over them.</p>
<h3>6.3 Extrinsic evaluation</h3>
<p>In this subsection, we take BERT as an example to show, once pretrained on SPARTQA-AUTO, BERT can achieve better performance on two extrinsic evaluation datasets, namely bAbI and boolQ.</p>
<p>We draw the learning curve on bAbI, using the original BERT as a baseline and BERT further pretrained on SPARTQA-AUTO (Fig. 6). Although both systems achieve perfect accuracy given large enough training data (i.e., 5 k and 10 k ), BERT (SPARTQA-AUTO) is showing better scores given less training data. Specifically, to achieve an accuracy of $99 \%$, BERT (SPARTQA-AUTO) requires</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: right;">FB</th>
<th style="text-align: right;">FR</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">CO</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">YN</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Consistency</td>
<td style="text-align: right;">Consistency</td>
<td style="text-align: right;">Contrast</td>
<td style="text-align: right;">Consistency</td>
<td style="text-align: right;">Contrast</td>
<td style="text-align: right;">Consistency</td>
<td style="text-align: right;">Contrast</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: right;">69.44</td>
<td style="text-align: right;">76.13</td>
<td style="text-align: right;">42.47</td>
<td style="text-align: right;">16.99</td>
<td style="text-align: right;">15.58</td>
<td style="text-align: right;">48.07</td>
<td style="text-align: right;">71.41</td>
</tr>
<tr>
<td style="text-align: left;">AIBERT</td>
<td style="text-align: right;">84.77</td>
<td style="text-align: right;">82.42</td>
<td style="text-align: right;">41.69</td>
<td style="text-align: right;">58.42</td>
<td style="text-align: right;">62.51</td>
<td style="text-align: right;">48.78</td>
<td style="text-align: right;">69.19</td>
</tr>
<tr>
<td style="text-align: left;">XLNet</td>
<td style="text-align: right;">85.2</td>
<td style="text-align: right;">88.56</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">71.10</td>
<td style="text-align: right;">72.31</td>
<td style="text-align: right;">51.08</td>
<td style="text-align: right;">69.18</td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation of consistency and semantic sensitivity of models in Table 4. All the results are on the correctly predicted questions of Seen test set of SPARTQA-AUTO.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Learning curve of BERT and BERT further pretrained on SPARTQA-AUTO on bAbI.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Majority baseline</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">Recurrent model (ReM)</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">ReM fine-tuned on SQuAD</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: left;">ReM fine-tuned on QNLI</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">ReM fine-tuned on NQ</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: left;">BERT (our setup)</td>
<td style="text-align: center;">71.9</td>
</tr>
<tr>
<td style="text-align: left;">BERT (SPARTQA-AUTO)</td>
<td style="text-align: center;">$\mathbf{7 4 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 6: System performances on the dev set of boolQ (since the test set is not available to us). Top: numbers reported in (Clark et al., 2019). Bottom: numbers from our experiments. BERT (SPARTQA-AUTO): further pretraining BERT on SPARTQA-AUTO as a QA task.</p>
<p>1 k training examples, while BERT requires twice as much. We also notice that BERT (SPARTQAAuto) converges faster in our experiments.</p>
<p>As another evaluation dataset, we chose boolQ for two reasons. First, we needed a QA dataset with Yes/No questions. To our knowledge boolQ is the only available one used in the recent work. Second, indeed, SPARTQA and boolQ are from different domains, however, boolQ needs multi-step reasoning in which we wanted to see if SPARTQA helps.</p>
<p>Table 6 shows that further pretraining BERT on SPARTQA-AUTO yields a better result than the original BERT and those reported numbers in Clark et al. (2019), which also tested on various distant supervision signals such as SQuAD (Rajpurkar et al., 2016), Google's Natural Question dataset NQ (Kwiatkowski et al., 2019), and QNLI from</p>
<p>GLUE (Wang et al., 2018).
We observe that many of the boolQ examples answered correctly by the BERT further pretrained on SPARTQA-AUTO require multi-step reasoning. Our hypothesis is that since solving SPARTQAAuto questions needs multi-step reasoning, finetuning BERT on SPARTQA-AUTO generally improves this capability of the base model.</p>
<h2>7 Conclusion</h2>
<p>Spatial reasoning is an important problem in natural language understanding. We propose the first human-created QA benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (LM) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably. To improve LMs' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining LMs on this distant supervision dataset significantly enhances their spatial language understanding and reasoning. We also show that a spatially-improved LM can have better results on two extrinsic datasets (bAbI and boolQ).</p>
<h2>Acknowledgements</h2>
<p>This project is supported by National Science Foundation (NSF) CAREER award #2028626 and (partially) supported by the Office of Naval Research grant #N00014-20-1-2005. We thank the reviewers for their helpful comments to improve this paper and Timothy Moran for his help in the human data generation.</p>
<h2>References</h2>
<p>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision-</p>
<p>and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683.</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433.</p>
<p>Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019. TOUCHDOWN: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12538-12547.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936.</p>
<p>Douglas H Clements and Michael T Battista. 1992. Geometry and spatial reasoning. Handbook of research on mathematics teaching and learning, pages 420464.</p>
<p>Soham Dan, Parisa Kordjamshidi, Julia Bonn, Archna Bhatia, Zheng Cai, Martha Palmer, and Dan Roth. 2020. From spatial relations to spatial configurations. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 58555864, Marseille, France. European Language Resources Association.</p>
<p>Pradeep Dasigi, Nelson F. Liu, Ana Marasović, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5925-5932.</p>
<p>Surabhi Datta and Kirk Roberts. 2020. A hybrid deep learning approach for spatial trigger extraction from radiology reports. In Proceedings of the Third International Workshop on Spatial Language Understanding, pages 50-55, Online. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Kaustubh Dhole and Christopher D. Manning. 2020. Syn-QG: Syntactic and shallow semantic rules for question generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 752-765.</p>
<p>Xinya Du and Claire Cardie. 2020. Event extraction by answering (almost) natural questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to Ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13421352.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020.</p>
<p>Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, and Sewon Min. 2019. Question Answering is a Format; when is it useful? ArXiv, abs/1909.11291.</p>
<p>Mehdi Ghanimifard and Simon Dobnik. 2017. Learning to compose spatial relations with grounded neural language models. In IWCS 2017-12th International Conference on Computational SemanticsLong papers.</p>
<p>Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't Stop Pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360.</p>
<p>Hangfeng He, Qiang Ning, and Dan Roth. 2020a. QuASE: Question-answer driven sentence encoding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8743-8758, Online. Association for Computational Linguistics.</p>
<p>Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan Roth. 2020b. Foreshadowing the benefits of incidental supervision. arXiv preprint arXiv:2006.05500.</p>
<p>Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015. Question-Answer Driven Semantic Role Labeling: Using natural language to annotate natural language. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643-653.</p>
<p>Michael Heilman and Noah A Smith. 2009. Question generation via overgenerating transformations and ranking. Technical report, CARNEGIE-MELLON UNIV PITTSBURGH PA LANGUAGE TECHNOLOGIES INST.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Zixian Huang, Yulin Shen, Xiao Li, Yu'ang Wei, Gong Cheng, Lin Zhou, Xinyu Dai, and Yuzhong Qu. 2019. GeoSQA: A benchmark for scenario-based question answering in the geography domain at high school level. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5866-5871.</p>
<p>Drew A Hudson and Christopher D Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6700-6709.</p>
<p>Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Strötgen, and Gerhard Weikum. 2018. TempQuestions: A benchmark for temporal question answering. In Companion Proceedings of the The Web Conference 2018, pages 1057-1062.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2901-2910.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface:a challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907.</p>
<p>Hyounghun Kim, Abhaysinh Zala, Graham Burri, Hao Tan, and Mohit Bansal. 2020. ArraMon: A joint navigation-assembly instruction interpretation task in dynamic environments. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3910-3927, Online. Association for Computational Linguistics.</p>
<p>Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial Role Labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), pages 413-420. European Language Resources Association (ELRA).</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural Questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Igor Labutov, Sumit Basu, and Lucy Vanderwende. 2015. Deep questions without deep understanding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 889-898.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Christian Landsiedel, Verena Rieser, Matthew Walter, and Dirk Wollherr. 2017. A review of spatial reasoning and interaction for real-world robotics. Advanced Robotics, 31(5):222-242.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot relation extraction via reading comprehension. In CONLL, pages 333-342.</p>
<p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988.</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p>
<p>Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke Zettlemoyer. 2018. Crowdsourcing question-answer meaning representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 560-568.</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003-1011.</p>
<p>Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, and Dan Roth. 2020. TORQUE: A reading comprehension dataset of temporal ordering questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1158-1172.</p>
<p>Jason Phang, Thibault Févry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.</p>
<p>Taher Rahgooy, Umar Manzoor, and Parisa Kordjamshidi. 2018. Visually guided spatial relation extraction from text. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 788-794.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. 2020. RMM: A recursive mental model for dialogue navigation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1732-1745, Online. Association for Computational Linguistics.</p>
<p>Priyanka Sen and Amir Saffari. 2020. What do models learn from question answering datasets? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2429-2438.</p>
<p>Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418-6428.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2019. What do you learn from
context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316.</p>
<p>Takuma Udagawa, Takato Yamazaki, and Akiko Aizawa. 2020. A linguistic analysis of visually grounded dialogues based on spatial expressions. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 750-765, Online. Association for Computational Linguistics.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5754-5764.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. pages 2369-2380.</p>
<p>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 17531762.</p>
<p>Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579-7589.</p>
<h2>A Question Templates and statistics Information</h2>
<p>Table 7 shows the templates used to create questions in SPARTQA-AUTO. The "<object>" is a variable replaced by objects from the story (using Choose-objects and Describe-objects modules), and the "<relation>" variable can be replaced by the chosen relations between objects (using Find-all-relations module).</p>
<p>The articles and the indefinite pronouns in each template play an essential role in understanding the question's objective. For example, "Are all blue circles near to a triangle?" is different from "Are there any blue circles near to a triangle?", and "Are there any blue circles near to all triangles?". Therefore, we check the uniqueness of the object definition, using "a" or "the" in proper places and randomly place the terms "any" or "all" in the YN questions to generate different questions.</p>
<p>Table 8 shows the percentage of correct labels in train and test sets. In multi-choice Q-TYPES, more than one label can be true.</p>
<h2>B Sentences of the Dataset</h2>
<p>Table 10 shows some generated sentences in SPARTQA-AUTO with some specific features that challenge models to understand different forms of relation description in spatial language.</p>
<h2>C Additional Evaluation Sets</h2>
<p>Here we describe three extra evaluation sets provided with this dataset in more detail, including unseen test, consistency, and contrast sets.</p>
<h2>C. 1 Unseen Evaluation Set</h2>
<p>We propose an unseen test set alongside the seen test of SPARTQA-AUTO to check whether a model is using shortcuts in the language surface by describing objects and relations with new vocabularies in the samples. This set has minor modifications that should not affect the performance of a consistent and reliable model. The modifications are randomly applied on a number of generated stories and questions and include changing names of shapes, colors, sizes, and relationships' names (describing relationships using different language expressions). The modification choices are described in Table 9.</p>
<h2>C. 2 Contrast and Consistency Evaluation</h2>
<p>For probing the consistency and semantic sensitivity of models, we provide two extra evaluation test
sets, Consistency and Contrast ${ }^{9}$.
Consistency set is made by changing parts of the question in a way that it still asks about the same information (Hudson and Manning, 2019; Suhr et al., 2019). For instance, for the question, "What is the relation between the blue circle and the big shape? Left," we create a similar question in the form of "What is the relation between the big shape and the blue circle? Right". Answering these questions around a pivot question is possible for human without the need for extra reasoning over the story and based on the main questions' answer. Hence, the evaluation on this set shows that models understand the real underlying semantics rather than overfit on the structure of questions.</p>
<p>Contrast set: This set is made by minor changes in a question that changes the answer (Gardner et al., 2020). As an instance, in the question "Is the blue circle below the black triangle? Yes," we create a contrast question "Is the blue circle below all triangles? No" by changing "the black trinagle" to "all triangles". The evaluation on this set shows the robustness of the model and its sensitivity to the semantic changes when there are minor changes in the language surface ${ }^{10}$.</p>
<h2>D Extra Annotations</h2>
<p>Alongside the main SPARTQA-AUTO's stories and questions we provided some extra annotation to help the models to understand the spatial language better.</p>
<h2>D. 1 Detailed Annotation and Scene-Graphs</h2>
<p>Providing in-depth human annotations is quite expensive and time-consuming. In SPARTQA-AUTO, we generated fine-grained scene-graph based on the story. This scene-graph contains blocks' description, their relations, and the objects' attributes alongside their direct relations with each other. The scene-graphs can be used for the models to understand all spatial relations directly mentioned in the textual context. Figure 7 shows an example of this scene-graph. The scene-graph can provide strong supervision for question answering challenges and</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Q-Type</th>
<th style="text-align: left;">Q-Templates</th>
<th style="text-align: left;">Candidate answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FR</td>
<td style="text-align: left;">what is the relation between <object>and <object>?</td>
<td style="text-align: left;">Left, Right, Below, <br> Above, Touching, <br> Far from, Near to</td>
</tr>
<tr>
<td style="text-align: left;">CO</td>
<td style="text-align: left;">What is <relation >the <object>? <br> an <object1>or an <object2>? <br> Which object is <relation >an <object>? <br> the <object1>or the <object2>?</td>
<td style="text-align: left;">Object1, object2, <br> Both, None</td>
</tr>
<tr>
<td style="text-align: left;">YN</td>
<td style="text-align: left;">Is (the | a )<object1><relation>(the | a) <object2>? <br> Is there any <object1>s <relation>all <object2>s?</td>
<td style="text-align: left;">Yes, No, Don't Know</td>
</tr>
<tr>
<td style="text-align: left;">FB</td>
<td style="text-align: left;">Which block has an <object>? <br> Which block doesn't have an <object>?</td>
<td style="text-align: left;">Name of blocks, None</td>
</tr>
</tbody>
</table>
<p>Table 7: Questions and answers templates.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Scene-graph
can be used to evaluate models based on their steps of reasoning and decisions.</p>
<h2>D. 2 SpRL Annotation</h2>
<p>We also provided spatial annotations for each sentence and question, based on Spatial Role Labeling (SpRL) annotation scheme (Kordjamshidi et al., 2010)(Fig. 11). This annotation is generated by hand-crafted rules during the main data generation. SpRL is used for recognizing spatial expressions and arguments in a sentence. This annotation is useful for applications that need to detect and reason about spatial expressions and arguments.</p>
<h2>E QA Language Models for Spatial Reasoning over Text</h2>
<p>Figures 8a and 8b depict the architecture used for further fine-tuning language models on SPARTQA described in section 5.</p>
<h2>F bAbI and boolQ Datasets</h2>
<p>Figure 9 shows an example of the bAbI dataset (Weston et al., 2015) task 17.</p>
<p>To solve task 17 of bAbI, we implement two SpRL+rule-based and neural network models. The
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: $\mathrm{LM}_{Q A}$ for Spatial Reasoning over Text
"The pink rectangle is below the red square. The red square is below the blue square."</p>
<ol>
<li>Is the red square below the pink rectangle? No</li>
<li>Is the pink rectangle below the blue square? Yes</li>
</ol>
<p>Figure 9: An example of bAbI dataset, task 17.</p>
<p>SpRL+rule-based model first, finds different spa-</p>
<table>
<thead>
<tr>
<th>Q-TYPE</th>
<th>Candidate Answers</th>
<th>train</th>
<th>test</th>
</tr>
</thead>
<tbody>
<tr>
<td>FR <br> (Multiple <br> Choices)</td>
<td>Left</td>
<td>20.7</td>
<td>17.9</td>
</tr>
<tr>
<td></td>
<td>Right</td>
<td>21.4</td>
<td>16.7</td>
</tr>
<tr>
<td></td>
<td>Above</td>
<td>26.9</td>
<td>25.4</td>
</tr>
<tr>
<td></td>
<td>Below</td>
<td>37.2</td>
<td>42.9</td>
</tr>
<tr>
<td></td>
<td>Near to</td>
<td>5.8</td>
<td>2.9</td>
</tr>
<tr>
<td></td>
<td>Far from</td>
<td>1.3</td>
<td>0.56</td>
</tr>
<tr>
<td></td>
<td>Touching</td>
<td>0.57</td>
<td>0.27</td>
</tr>
<tr>
<td></td>
<td>DK</td>
<td>0.52</td>
<td>0.32</td>
</tr>
<tr>
<td>FB <br> (multiple <br> Choices)</td>
<td>A</td>
<td>49.8</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>B</td>
<td>50.1</td>
<td>50</td>
</tr>
<tr>
<td></td>
<td>C</td>
<td>35.1</td>
<td>62</td>
</tr>
<tr>
<td></td>
<td>[]</td>
<td>7.1</td>
<td>90.5</td>
</tr>
<tr>
<td>CO <br> (Single <br> choice)</td>
<td>Object1</td>
<td>25.4</td>
<td>26</td>
</tr>
<tr>
<td></td>
<td>Object2</td>
<td>25.3</td>
<td>24.9</td>
</tr>
<tr>
<td></td>
<td>Both</td>
<td>44.3</td>
<td>43.9</td>
</tr>
<tr>
<td></td>
<td>None</td>
<td>4.9</td>
<td>5.0</td>
</tr>
<tr>
<td>YN <br> (Single <br> choice)</td>
<td>Yes</td>
<td>53.3</td>
<td>50.5</td>
</tr>
<tr>
<td></td>
<td>No</td>
<td>18.7</td>
<td>23.6</td>
</tr>
<tr>
<td></td>
<td>DK</td>
<td>27.8</td>
<td>25.9</td>
</tr>
</tbody>
</table>
<p>Table 8: The percentage of each correct label in all samples. <em>The candidate answers for the FB Q-TYPE can be varied, based on its story. ${ }^{</em> *} \mathrm{CO}$ can be considered as a multiple choice or single choice question. E.g., in "which object is above the triangle? the blue circle or the black circle?" you can consider two labels with boolean classification on each "blue circle" and "black circle" or consider it as a four labels classification: "blue circle," "black circle," "both of them," and "None of them." *** DK, None, [], all mean none of the actual labels are correct.
tial relation triplets (Landmark, Spatial-indicator, trajector) for each fact in a story the applies spatial rules over these extracted triplets and report all possible relations between two asked objects. Finally, it checks whether the asked relation existed in the find relation. This model solves task 17 of the bAbI with $100 \%$ accuracy.</p>
<p>To implement the neural network approach, we use huggingface implementation of pre-trained BERT (Devlin et al., 2019). We apply a boolean classifier on the output of "[CLS]" token from the last layer of BERT model for each "Yes" and "No" answers (the same as model used on YN question types.) We use Adamw (Loshchilov and Hutter, 2017) optimizer and $2 e-6$ learning rate with negative log-likelihood loss objective and train the model on the $10 \mathrm{k}, 5 \mathrm{k}, 2 \mathrm{k}, 1 \mathrm{k}, 500$, and 100 portion of bAbI's training questions. The model yields $100 \%$ accuracy on 10 k , and 5 k and $99 \%$ accuracy</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Original Set</th>
<th style="text-align: left;">Unseen Set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Shapes</td>
<td style="text-align: left;">Square, Circle, <br> Triangle</td>
<td style="text-align: left;">Rectangle, Oval, <br> Diamond</td>
</tr>
<tr>
<td style="text-align: left;">Relations</td>
<td style="text-align: left;">Left, Right, <br> Above, Below</td>
<td style="text-align: left;">Left side, <br> Right side, <br> Top, Under</td>
</tr>
<tr>
<td style="text-align: left;">Colors</td>
<td style="text-align: left;">Yellow, Black, <br> Below</td>
<td style="text-align: left;">Green, Red, <br> White</td>
</tr>
<tr>
<td style="text-align: left;">Size</td>
<td style="text-align: left;">Small, <br> Medium, Big</td>
<td style="text-align: left;">Little, Midsize, <br> Large</td>
</tr>
</tbody>
</table>
<p>Table 9: Modifications on the unseen set
on 2 k and 1 k training samples.
Figure 10 shows an example of boolQ dataset. To Answering the questions of this dataset, we use the same setting as neural network model on bAbI to further fine-tune BERT on boolQ.</p>
<p>Q: Has the UK been hit by a hurricane?
P: The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands ...
A: Yes. [An example event is given.]
Q: Does France have a Prime Minister and a President?
P: ... The extent to which those decisions lie with the Prime Minister or President depends upon ...
A: Yes. [Both are mentioned, so it can be inferred both exist.]</p>
<p>Figure 10: An example of boolQ dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">sentence</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Medium blue square number one is touching the bottom edge of this block.&quot;</span>
<span class="o">*</span><span class="w"> </span><span class="nx">spatial_description</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">item</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">trajector</span><span class="p">:</span>
<span class="w">        </span><span class="nx">phrase</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;medium blue square number one&quot;</span>
<span class="w">        </span><span class="nx">head</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;square&quot;</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">properties</span><span class="p">:</span>
<span class="w">        </span><span class="nx">color</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;blue&quot;</span>
<span class="w">        </span><span class="nx">size</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;medium&quot;</span>
<span class="w">        </span><span class="nx">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;number one&quot;</span>
<span class="w">        </span><span class="nx">number</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="w">        </span><span class="nx">spatial_property</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">SOT</span><span class="p">:</span>
<span class="w">        </span><span class="nx">start</span><span class="p">:</span><span class="w"> </span><span class="mi">167</span>
<span class="w">        </span><span class="nx">end</span><span class="p">:</span><span class="w"> </span><span class="mi">195</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">landmark</span><span class="p">:</span>
<span class="w">        </span><span class="nx">phrase</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;the bottom edge of this block&quot;</span>
<span class="w">        </span><span class="nx">head</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;block&quot;</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">properties</span><span class="p">:</span>
<span class="w">        </span><span class="nx">spatial_property</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;the bottom edge&quot;</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">SOT</span><span class="p">:</span>
<span class="o">*</span><span class="w"> </span><span class="nx">spatial_indicator</span><span class="p">:</span>
<span class="w">        </span><span class="nx">phrase</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;touching&quot;</span>
<span class="w">        </span><span class="nx">spatial_value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;TPP&quot;</span>
<span class="w">        </span><span class="nx">g_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Region&quot;</span>
<span class="w">        </span><span class="nx">s_type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;RCC8&quot;</span>
<span class="w">        </span><span class="nx">polarity</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">        </span><span class="nx">FoR</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Relative&quot;</span>
<span class="w">    </span><span class="o">*</span><span class="w"> </span><span class="nx">SOT</span><span class="p">:</span>
</code></pre></div>

<p>Figure 11: SpRL annotation for an example sentence from SPARTQA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Examples</th>
<th style="text-align: left;">Features</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Block A is above Block C and B.</td>
<td style="text-align: left;">Using conjunction to describe relation between <br> more than two blocks.</td>
</tr>
<tr>
<td style="text-align: left;">The small circle is above the yellow square and <br> the big black shape.</td>
<td style="text-align: left;">Using conjunction to describe relationships be- <br> tween more than two objects.</td>
</tr>
<tr>
<td style="text-align: left;">The yellow square number one is to the right <br> of and above the blue circle.</td>
<td style="text-align: left;">Using conjunction for more than one relation.</td>
</tr>
<tr>
<td style="text-align: left;">Block B has two medium yellow squares and <br> two blue circles.</td>
<td style="text-align: left;">Describing a group of objects with the same <br> properties. In the next sentences, they are men- <br> tioned by an asigned number. For example, the <br> blue circle number two.</td>
</tr>
<tr>
<td style="text-align: left;">The blue circle is below the object which is to <br> the right of the big square.</td>
<td style="text-align: left;">Using nested relations between objects in their <br> description.</td>
</tr>
<tr>
<td style="text-align: left;">A small blue circle is near to the big circle. It <br> is to the left of the medium yellow square.</td>
<td style="text-align: left;">Using coreferences for an entity described in <br> the previous sentences.</td>
</tr>
<tr>
<td style="text-align: left;">There is a block named A. One small yellow <br> square is touching the bottom edge of this block.</td>
<td style="text-align: left;">The verb matches the number of the subject.</td>
</tr>
<tr>
<td style="text-align: left;">What is the relation between black object and a <br> big circle?</td>
<td style="text-align: left;">Using shape, object, and thing, which are a gen- <br> eral description of an object. It could be the <br> "black triangle" or the "black circle" mentioned <br> in the story.</td>
</tr>
</tbody>
</table>
<p>Table 10: Particular features of the dataset</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ for some questions, it is not possible to generate a complementary set
${ }^{10}$ Based on the original contrast set paper, consistency and contrast set should be generated manually to control the semantic change. In our case that we are probing the spatial language understanding of models, we must change parts that affect spatial understanding, which can be implemented by some static rules.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ SPAtial Reasoning on Textual Question Answering.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
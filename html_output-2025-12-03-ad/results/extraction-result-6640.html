<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6640 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6640</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6640</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-00e88a0c006296e53bb7d4cfc90a134883ad34fd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/00e88a0c006296e53bb7d4cfc90a134883ad34fd" target="_blank">A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias, and develops carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems.</p>
                <p><strong>Paper Abstract:</strong> This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6640.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6640.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>25-horses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Twenty-five horses (graph-theory) puzzle / mathematical-reasoning variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic minimum-race scheduling puzzle used in the paper as a mathematical-reasoning example; the authors perturb surface tokens (animal names and optionally numbers) while keeping the underlying combinatorial logic identical to test token bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude (illustrated) and multiple evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only family; generative LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom synthetic mathematical-reasoning variant (derived from 'twenty-five horses' puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Combinatorial / multi-step mathematical reasoning (minimum-number-of-races puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language puzzle presented as a multiple-choice / free-answer math reasoning question (story narrative with numbers and entities)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Non-trivial puzzle (requires combinatorial reasoning about pairwise races); not mapped to a school grade in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Baseline and in-context variants (zero-shot, one-shot, few-shot, with and without chain-of-thought) — same prompting families used across experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / comparative matched-pair change (evaluated via matched-pair contingency and McNemar test significance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>The paper reports statistically significant changes in model outputs when animal names or numbers are perturbed (illustrated drops for GPT-4 and Claude in Figure 1); specific per-model accuracy numbers for this puzzle variant are not provided in the main text, but the direction is a significant performance drop under token perturbation (McNemar p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probing (e.g., attention or activation probing) presented for numeric processing; analysis is behavioral/statistical only (matched-pair contingency tables, McNemar tests) showing sensitivity to superficial tokens rather than inspecting internal numeric representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Brittleness to irrelevant token changes: swapping animal species or altering numbers (while preserving logical structure) caused predictable degradation; models rely on surface patterns (names/numbers) and can be misled by them rather than applying invariant combinatorial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No systematic scaling law reported for this specific puzzle; the paper shows behavior across many models (including large commercial models) but does not claim a consistent improvement with model size for this task — instead, sensitivity to token perturbations is observed across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6640.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6640.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-reasoning mentions (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper-level mentions of mathematical/arithmetic reasoning behavior of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper does not evaluate standard arithmetic benchmarks; it uses a small set of mathematical reasoning examples (including the 'twenty-five horses' puzzle) as case studies to probe token bias and reports behavioral, matched-pair statistical analyses across many LLMs and prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo, gpt-4-turbo, gpt4o, meta-llama-3-70b-instruct, meta-llama-3-8b-instruct, llama-2-70b-chat, claude-3-opus, claude-3-sonnet, mistral-large-latest (models listed and evaluated across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (decoder-only / autoregressive LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>examples include 70B (LLaMA-3-70B), 8B (LLaMA-3-8B); sizes for closed-source models not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>No standard arithmetic benchmark (e.g., GSM8K, MATH) — custom synthetic datasets focused on logical fallacies and a few mathematical reasoning puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Logical fallacies, syllogistic reasoning, and a few mathematical reasoning puzzles (combinatorial puzzle described above); not standard arithmetic (addition/subtraction) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language multiple-choice or yes/no items generated synthetically; matched original vs. token-perturbed paired samples</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied — from typical cognitive-science reasoning problems (conjunction fallacy, syllogisms) to a nontrivial combinatorial puzzle; difficulty not calibrated to standard math grades</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Baseline (direct), zero-shot chain-of-thought ('Let's think step by step'), one-shot, few-shot, and variants with weak/strong hints (control prompts); many experiments compare these prompting regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy and paired-change statistics summarized via n12/n21 counts and McNemar z-statistics / p-values; authors control false discovery rate with Benjamini-Hochberg across multiple tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>For the reasoning/math variants discussed, the paper reports statistically significant sensitivity to token perturbations across most models and prompting methods (e.g., many McNemar tests yield p < 0.0001); exact aggregate arithmetic accuracy numbers on standard math benchmarks are not reported because those benchmarks were not used.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>The paper does not present internal mechanistic analyses of numerical computation (no probing of activations, attention, logit lens, or circuits). The analysis is behavioral: matched-pair contingency tables and hypothesis testing to detect token bias.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary failure modes documented: reliance on superficial tokens (names, celebrity references, familiar exemplars like 'Linda'), sensitivity to irrelevant numeric or entity perturbations, dependence on hint tokens and in-context exemplars rather than invariant logical derivation, and susceptibility to phrasing changes (quantifier synonyms).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No consistent quantitative scaling trend reported specific to arithmetic: some larger or more recent models sometimes perform better on original prompts but still exhibit token bias under perturbation; the paper emphasizes token-bias effects across models rather than monotonic scaling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for mathematical reasoning: Progresses and challenges <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6640",
    "paper_id": "paper-00e88a0c006296e53bb7d4cfc90a134883ad34fd",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "25-horses",
            "name_full": "Twenty-five horses (graph-theory) puzzle / mathematical-reasoning variant",
            "brief_description": "A classic minimum-race scheduling puzzle used in the paper as a mathematical-reasoning example; the authors perturb surface tokens (animal names and optionally numbers) while keeping the underlying combinatorial logic identical to test token bias.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude (illustrated) and multiple evaluated LLMs",
            "model_family": "Transformer (decoder-only family; generative LLMs)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "Custom synthetic mathematical-reasoning variant (derived from 'twenty-five horses' puzzle)",
            "task_type": "Combinatorial / multi-step mathematical reasoning (minimum-number-of-races puzzle)",
            "problem_format": "Natural-language puzzle presented as a multiple-choice / free-answer math reasoning question (story narrative with numbers and entities)",
            "difficulty_level": "Non-trivial puzzle (requires combinatorial reasoning about pairwise races); not mapped to a school grade in the paper",
            "prompting_method": "Baseline and in-context variants (zero-shot, one-shot, few-shot, with and without chain-of-thought) — same prompting families used across experiments",
            "performance_metric": "Accuracy / comparative matched-pair change (evaluated via matched-pair contingency and McNemar test significance)",
            "performance_value": "The paper reports statistically significant changes in model outputs when animal names or numbers are perturbed (illustrated drops for GPT-4 and Claude in Figure 1); specific per-model accuracy numbers for this puzzle variant are not provided in the main text, but the direction is a significant performance drop under token perturbation (McNemar p &lt; 0.05).",
            "internal_analysis": "No mechanistic probing (e.g., attention or activation probing) presented for numeric processing; analysis is behavioral/statistical only (matched-pair contingency tables, McNemar tests) showing sensitivity to superficial tokens rather than inspecting internal numeric representations.",
            "failure_modes": "Brittleness to irrelevant token changes: swapping animal species or altering numbers (while preserving logical structure) caused predictable degradation; models rely on surface patterns (names/numbers) and can be misled by them rather than applying invariant combinatorial reasoning.",
            "scaling_trend": "No systematic scaling law reported for this specific puzzle; the paper shows behavior across many models (including large commercial models) but does not claim a consistent improvement with model size for this task — instead, sensitivity to token perturbations is observed across sizes.",
            "uuid": "e6640.0",
            "source_info": {
                "paper_title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Math-reasoning mentions (general)",
            "name_full": "Paper-level mentions of mathematical/arithmetic reasoning behavior of LLMs",
            "brief_description": "The paper does not evaluate standard arithmetic benchmarks; it uses a small set of mathematical reasoning examples (including the 'twenty-five horses' puzzle) as case studies to probe token bias and reports behavioral, matched-pair statistical analyses across many LLMs and prompting methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo, gpt-4-turbo, gpt4o, meta-llama-3-70b-instruct, meta-llama-3-8b-instruct, llama-2-70b-chat, claude-3-opus, claude-3-sonnet, mistral-large-latest (models listed and evaluated across tasks)",
            "model_family": "Transformer (decoder-only / autoregressive LLMs)",
            "model_size": "examples include 70B (LLaMA-3-70B), 8B (LLaMA-3-8B); sizes for closed-source models not specified in the paper",
            "training_data_description": null,
            "benchmark_name": "No standard arithmetic benchmark (e.g., GSM8K, MATH) — custom synthetic datasets focused on logical fallacies and a few mathematical reasoning puzzles",
            "task_type": "Logical fallacies, syllogistic reasoning, and a few mathematical reasoning puzzles (combinatorial puzzle described above); not standard arithmetic (addition/subtraction) evaluation",
            "problem_format": "Natural-language multiple-choice or yes/no items generated synthetically; matched original vs. token-perturbed paired samples",
            "difficulty_level": "Varied — from typical cognitive-science reasoning problems (conjunction fallacy, syllogisms) to a nontrivial combinatorial puzzle; difficulty not calibrated to standard math grades",
            "prompting_method": "Baseline (direct), zero-shot chain-of-thought ('Let's think step by step'), one-shot, few-shot, and variants with weak/strong hints (control prompts); many experiments compare these prompting regimes.",
            "performance_metric": "Accuracy and paired-change statistics summarized via n12/n21 counts and McNemar z-statistics / p-values; authors control false discovery rate with Benjamini-Hochberg across multiple tests.",
            "performance_value": "For the reasoning/math variants discussed, the paper reports statistically significant sensitivity to token perturbations across most models and prompting methods (e.g., many McNemar tests yield p &lt; 0.0001); exact aggregate arithmetic accuracy numbers on standard math benchmarks are not reported because those benchmarks were not used.",
            "internal_analysis": "The paper does not present internal mechanistic analyses of numerical computation (no probing of activations, attention, logit lens, or circuits). The analysis is behavioral: matched-pair contingency tables and hypothesis testing to detect token bias.",
            "failure_modes": "Primary failure modes documented: reliance on superficial tokens (names, celebrity references, familiar exemplars like 'Linda'), sensitivity to irrelevant numeric or entity perturbations, dependence on hint tokens and in-context exemplars rather than invariant logical derivation, and susceptibility to phrasing changes (quantifier synonyms).",
            "scaling_trend": "No consistent quantitative scaling trend reported specific to arithmetic: some larger or more recent models sometimes perform better on original prompts but still exhibit token bias under perturbation; the paper emphasizes token-bias effects across models rather than monotonic scaling behavior.",
            "uuid": "e6640.1",
            "source_info": {
                "paper_title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for mathematical reasoning: Progresses and challenges",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.01277,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners</h1>
<p>Bowen Jiang ${ }^{1,2}$, Yangxinyu Xie ${ }^{1,2}$, Zhuoqun Hao ${ }^{1}$, Xiaomeng Wang ${ }^{1}$, Tanwi Mallick ${ }^{2}$, Weijie J. Su ${ }^{1}$, Camillo J. Taylor ${ }^{1}$, Dan Roth ${ }^{1}$<br>University of Pennsylvania ${ }^{1}$ Argonne National Laboratory ${ }^{2}$<br>Philadelphia, PA, 19104, USA Lemont, IL, 60439, USA<br>{bwjiang@seas, xinyux@wharton, zhuoqunh@sas, xwang1@wharton}.upenn.edu,<br>tmallick@anl.gov, {suw@wharton, cjtaylor@seas, danroth@seas}.upenn.edu</p>
<h4>Abstract</h4>
<p>This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities. Codes and data are open-sourced at https://github.com/bowen-upenn/llm_token_bias.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have achieved remarkable progress in understanding and generating human-like text, triggering growing interest in the LLMs' theory of minds (Kosinski, 2023; Jamali et al., 2023; Bubeck et al., 2023) and decisionmaking abilities (Lyu et al., 2023; Prasad et al., 2023; Jiang et al., 2024a,b; Xie et al., 2024). However, there is ongoing debate about whether LLMs possess genuine reasoning capabilities, as evidence suggests that the performance of LLMs on reasoning tasks is correlated with how much the input's semantic content supports a correct logical inference (Dasgupta et al., 2022; Li et al., 2023). Should valid reasoning be applied, such a correlation would not exist, since a genuine reasoner should be able to derive the correct inference regardless of the context.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We illustrate token bias using the classic "twenty-five horses" problem in graph theory. The top two sub-figures, generated by GPT-4o for illustration purposes only ${ }^{1}$, demonstrate the concept by altering the name "horses" to "bunnies", irrelevant to the problem's underlying logic. The bottom two sub-figures show experimental results in GPT-4 and Claude, where performance significantly drops due to perturbations in animal names and numbers. In these plots, "Original" refers to the unaltered "twenty-five horses" problem, "random_animals" alters only the animal names, and "random" alters both names and numbers. We observe $n 12&gt;n 21$ with statistical significance, meaning that there are more instances where the original problem is solved correctly while the perturbed problem is solved incorrectly, compared to the reverse. As a result, our hypothesis testing confirms token bias in this scenario.</p>
<p>In this paper, we formalize this observation and say that an LLM is subject to token bias in a reasoning task if systematic changes to some or all tokens in the task descriptions - while keeping the underlying logic intact - allow us to predict the direction of the shift in the model's output. A strong token bias suggests that the model is relying on superficial patterns in the input rather than truly understanding the underlying reasoning task. This could lead to brittle performance that fails to generalize well to novel examples and phrasings encountered in the wild, which could differ from the spurious patterns the model may have overfitted to during training.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of the overall framework. We generate synthetic data, perform systematic token perturbations, and evaluate an LLM for comparative studies. The resulting contingency table, where A-D are integer values of counts, allows for subsequent statistical tests.</p>
<p>We explore several well-known logical fallacy problems from the cognitive science literature (Tversky and Kahneman, 1983; Kahneman, 2011), which provide a clear playground for assessing the reasoning capabilities of LLMs. Figure 1 and 3 depict two kinds of token biases found in our testing framework, where the model may be overfitting to specific tokens commonly found in classic problem statements. Since we observe many cases where state-of-the-art LLMs like GPT-4 (Achiam et al., 2023) successfully identify logical fallacies under certain settings, we highlight the urgent need for a framework to tease out whether LLMs apply genuine reasoning or merely exploit token bias for their improved performance.</p>
<p>This work reconceptualizes the evaluation of reasoning capabilities into a general and rigorous statistical testing framework. As shown in Figure 2, it comprises three critical components: synthetic data generation, token perturbation, and statistical hypothesis testing. This general framework is designed to bypass the complications of evaluation set contamination (Zhou et al., 2023; Ravaut et al., 2024), leverage insights and tools from controlled experiments, and draw statistically valid conclusions.</p>
<p>Our study is unique from existing work (Gou</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: What is token bias? Here is another example exhibited by GPT-4. On the left, GPT-4 correctly identifies the conjunction fallacy and answers the question correctly, given the classical Linda Problem as the oneshot exemplar. On the right, however, the exemplar is rephrased by altering "Linda" to "Bob" while keeping the same logic, which surprisingly confuses the model.
et al., 2023; Suri et al., 2024; Mukherjee and Chang, 2024; Wang et al., 2024) in two folds. First, we are not evaluating the overall accuracy of LLMs in identifying different logical fallacies. Instead, our focus is on token bias. Although there are always more types of logical fallacies, we take the conjunction fallacy, syllogistic fallacy, and the "twenty-five horses" problem in graph theory as quintessential examples, which exhibit strong token biases that are more readily identifiable in their problem statements. By identifying and perturbing these specific tokens, we can induce predictable shifts in LLM responses. Second, we recognize that cognitive biases often emerge in implicit forms in real-life scenarios, so relying on engineering fine-grained prompts (Gou et al., 2023; Yao et al., 2024; Besta et al., 2024) to make LLMs identify specific logical fallacies is impractical for general-purpose user applications. As a result, we only leverage common prompting techniques that are sufficient to provide robust statistical evidence.</p>
<p>Comprehensive experiments on both commercial and open-sourced LLMs on large-scale synthetic datasets uncover a critical insight: it is the token</p>
<p>bias that contributes the most to performance improvements in reasoning tasks, if any, rather than genuine advances in reasoning capabilities.</p>
<h2>2 The General Framework</h2>
<p>Our framework is summarized in Figure 2. This general framework is grounded on the premise that for a given reasoning task, a capable reasoning agent will consistently reach the same conclusion regardless of how the task is framed, as long as the underlying logic remains the same (Hastie and Dawes, 2009). This assumption lays the foundation of our null hypothesis, $\boldsymbol{H}_{\mathbf{0}}$. In our setup, if an agent consistently applies reasoning in its decisionmaking process, the only source of failure should be the procedural mistakes during the agent's abstract reasoning steps, which we assume to come up in an i.i.d. fashion. Our general framework contains three major parts as follows.</p>
<p>Synthetic Data Generation Once the underlying logic of a reasoning task is defined, we create an algorithm to generate a synthetic dataset with $n$ samples. While it is helpful to leverage LLMs for linguistic coherence in the process, the data generation should be carefully controlled, utilizing information from real-world data or established datasets to mitigate potential biases from purely AI-generated texts. The process begins with the creation of a curated list of entities, encompassing diverse names, genders, ages, occupations, cultural backgrounds, and events where applicable, along with a textual template that dictates the structure of the task description. By sampling from this list, we generate task descriptions that maintain the integrity and novelty of the dataset. This method ensures that while the LLM of interest might be familiar with the individual entities, it has never seen the specific combinations of these entities and narratives, thus bypassing data contamination.</p>
<p>The following example illustrates one approach we leverage to generate synthetic conjunction fallacy questions. We randomly sample a commonsense story curated by Mostafazadeh et al. (2016) and convert it into the following prompt: Your task is to complete the last sentence of the following problem to create a conjunction fallacy quiz:</p>
<p>Michelle was extremely hungry. She opened the refrigerator to find nothing. Which is more likely?
(a) Michelle would likely buy food at the grocery store.
(b) Michelle would likely buy food at the grocery store because</p>
<p>We expect the LLM to complete the story by providing us with a plausible reason after "because", such as "she found nothing to eat at home". Irrespective of the LLM's completion, option (b) contains a conjunction of two events so it should always be viewed less likely.</p>
<p>The synthetic dataset can be dynamically generated on the fly, precluding its prior existence in any training datasets. It also allows the algorithm designers to control the dataset size, efficiently scaling their data based on the sample size required to acheive statistical validity.</p>
<p>Token Perturbation We posit that if the LLM primarily relies on token bias, its performance on reasoning tasks will consistently improve (or degrade) as we alter some tokens in a systematic manner. This process of token perturbation generates $n$ matched pairs of samples, enabling us to evaluate the LLM on both the original and perturbed datasets and create a $2 \times 2$ contingency table below, where $n=n_{11}+n_{12}+n_{21}+n_{22}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbed</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">$n_{11}$</td>
<td style="text-align: center;">$n_{12}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">$n_{21}$</td>
<td style="text-align: center;">$n_{22}$</td>
</tr>
</tbody>
</table>
<p>Table 1: A template for the contingency table. We follow the notations in this table to define $\pi_{12}$ and $\pi_{21}$ in the next paragraph for hypothesis testing.</p>
<p>Statistical Hypothesis Testing for Matched Pairs In our context, we wish to decide whether or not some hypothesis concerning whether an agent reasons consistently is correct. The choice here lies between two decisions: accepting or rejecting the hypothesis. The decision procedure is called hypothesis testing (Lehmann et al., 1986). Throughout our discussion, we use $\boldsymbol{H}<em _boldsymbol_a="\boldsymbol{a">{\mathbf{0}}$ to denote the null hypothesis and $\boldsymbol{H}</em>$ the alternative hypothesis.}</p>
<p>For each of the $n$ matched pairs, let $\pi_{a b}$ denote the underlying probability of outcome $a$ for the original sample and outcome $b$ for the perturbed sample. In other words, for any nonnegative integer $m \leq n$,</p>
<p>$$
\mathbb{P}\left(n_{a b}=m\right)=\binom{n}{m} \pi_{a b}^{m}\left(1-\pi_{a b}\right)^{n-m}
$$</p>
<p>As $n_{a b}$ counts the number of such pairs, $n_{a b} / n$ is the sample proportion, which is a consistent estimate of $\pi_{a b}$. The null hypothesis assumes the</p>
<p>marginal homogeneity for binary matched pairs, i.e. $\pi_{12}=\pi_{21}$. For small samples, we can apply an exact test conditioned on $n^{<em>}=n_{21}+n_{12}$ (Mosteller, 1952; Agresti, 2012). Under $\boldsymbol{H}<em 21="21">{0}, n</em>$ follows a binomial $\left(n^{</em>}, 1 / 2\right)$ distribution, and the corresponding $p$-value is the binomial tail probability. As a rule of thumb, when $n^{*}&gt;10$, the reference binomial distribution is approximately normal, and we can compute the standardized normal test statistics $z_{0}=\left(n_{21}-n_{12}\right) / \sqrt{n_{21}+n_{12}}$, which is identical to the McNemar statistic (McNemar, 1947). To test the same hypotheses for a group of models, we apply the Benjamini-Hochberg Procedure (Benjamini and Hochberg, 1995) to control the false discovery rate at a predetermined significance level $\alpha$.</p>
<h2>3 A Peek into Token Bias</h2>
<p>This section outlines the detailed hypotheses in our statistically inspired framework. We aim to determine whether LLMs are capable of genuine reasoning or whether they rely heavily on token biases. According to the principle of invariance in rational decision-making (Tversky and Kahneman, 1981, 1988), the preferences of a rational reasoning agent should remain unaffected by the framing of equivalent decision problems.</p>
<p>In a broader interpretation of invariance, we assess whether alterations in seemingly irrelevant tokens, such as name entities in problem narratives that are unrelated to the underlying logic, influence the outcomes of reasoning. A true reasoner should effectively navigate through reasoning tasks without being influenced by trivial changes in content that do not impact the fundamental logical structure. We propose a series of hypotheses, where the null hypothesis assumes the presence of a genuine reasoner. For each hypothesis, we identify specific tokens that may carry strong biases under their problem settings, and systematically alter these tokens to test their impact, while maintaining the integrity of the underlying logical structure.</p>
<h3>3.1 Preliminaries</h3>
<p>In this work, we integrate the conjunction fallacy and syllogistic fallacy discussed in the cognitive science literature (Tversky and Kahneman, 1983; Kahneman, 2011) to construct synthetic datasets on which we perform our token perturbation. This section briefly introduces the underlying logic.</p>
<p>Conjunction Fallacy The most often-cited example of conjunction fallacy is called the Linda problem which is framed as follows (Tversky and Kahneman, 1983): Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations. Which is more probable?
(a) Linda is a bank teller.
(b) Linda is a bank teller and is active in the feminist movement.
Tversky and Kahneman (1983) found that humans tend to prefer option (b). However, it is logically necessary that the probability of a conjunction of two events (e.g., Linda is a bank teller, and she is active in the feminist movement) is less than the probability of either event alone.</p>
<p>Syllogistic Fallacy The syllogistic fallacy documents the logical failure that occurs when people are presented with syllogisms - two premises followed by a conclusion. Ideally, if the premises are true and the logical structure is valid, the conclusion must necessarily be true. However, when the argument's structure is flawed, the conclusion may be invalid despite the surface-level logical form. Consider the following syllogism from Kahneman (2011): Is this logically sound?</p>
<p>All roses are flowers.
Some flowers fade quickly.
Therefore some roses fade quickly.
The argument is incorrect because the two premises do not imply that the set of roses and the set of flowers that fade quickly necessarily overlap.</p>
<h3>3.2 Lost in Irrelevant Context</h3>
<p>Logical fallacies often contain misleading contexts, exploiting common cognitive biases and shortcuts in human reasoning. These fallacies can seem convincing at first glance, being effective in swaying opinions, because they resonate with intuitive yet flawed biases. For instance, conjunction fallacies present two options: one involving a single event and the other with an additional event in conjunction. This added event is particularly designed to align with the contextual background in the problem statement, leading humans or LLMs to reaffirm their preexisting beliefs. In contrast, when the additional event in the options is changed to an irrelevant one, the model is less likely to be distracted by these extraneous and irrelevant details.</p>
<p>Hypothesis 1 Genuine reasoning LLMs should withstand contextually misleading options in the problem statements.</p>
<p>Token perturbation: Assume problem $P$ is a conjunction fallacy problem with options $(a)$ and $(b)$. One option contains a event $x$ and the other contains $x$ and $y$ in conjunction. $y$ is relevant to the context of the problem statement that might mislead the LLM. In contrast, the perturbed problem $P^{\prime}$ replaces $y$ with a randomly generated $y^{\prime}$ irrelevant to the context.
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\alpha}: \pi</em>\right.$ is invalid.)
Here is an example of such token perturbations, represented by the right arrow mark: Kai is a community leader of Pacific Islander descent. He holds degrees in Public Administration and is passionate about preserving his cultural heritage. Which is more probable?
(a) Kai is a law enforcement worker.
(b) Kai is a law enforcement worker and participates in cultural preservation organizations $\rightarrow$ learns to play the ukulele.
To further explain $\boldsymbol{H}}&lt;\pi_{21} .\left(\pi_{12}&gt;\pi_{21<em _boldsymbol_alpha="\boldsymbol{\alpha">{\mathbf{0}}$ and $\boldsymbol{H}</em>}}$, the null hypothesis $\boldsymbol{H<em 12="12">{\mathbf{0}}$ always assumes that the LLM is a genuine reasoner and can consistently perform reasoning regardless of the superficial token changes, leading us to expect $\pi</em>$ - where non-misleading options result in a greater decrease in performance - would be considered invalid.}=\pi_{21}$. Meanwhile, token bias can systematically and predictably influence the LLM's performance. Accordingly, $\pi_{12}&lt;\pi_{21}$ aligns with our expectation that misleading tokens degrade performance, but an observation of $\pi_{12}&gt;\pi_{21</p>
<h3>3.3 Token Bias on Widely Cited Examples in Classic Literature</h3>
<p>It is reasonable to suspect that most LLMs have been trained to recognize well-known logical fallacy problems. However, the question remains whether they acquire genuine reasoning skills or merely learn to falsely associate frequently appearing names - such as "Linda" in the classical Linda problem - with the correct reasoning outcomes they should have. We demonstrate an example in Figure 3 that perturbs Linda $\rightarrow$ Bob.
Hypothesis 2 Genuine reasoning LLM should withstand surface-level alterations to the one-shot exemplar in the problem statements.</p>
<p>Token perturbation: Assume one-shot in-context learning scenarios. $P$ has the original Linda problem as the one-shot exemplar. In contrast, the perturbed problem $P^{\prime}$ rephrases the exemplar to a persona called "Bob".
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\alpha}: \pi</em>\right.$ is invalid.)}&gt;\pi_{21} .\left(\pi_{12}&lt;\pi_{21</p>
<h3>3.4 Token Bias on Well-Known Entity Names</h3>
<p>Celebrity names inherently carry a rich contextual background that LLMs learn from massive training data. We hypothesize that by replacing a celebrity name with a generic one in a conjunction fallacy problem, thereby dissociating the link to this contextual backdrop, we might see performance improvements in LLMs, and such results would underscore the potential deficiency in their genuine reasoning capabilities.</p>
<p>Hypothesis 3 Genuine reasoning LLMs should withstand irrelevant alterations to name entities in problem statements</p>
<p>Token perturbation: Assume $P$ is a conjunction fallacy problem that involves a celebrity. In contrast, the perturbed problem $P^{\prime}$ replaces the celebrity name with a generic one.
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\alpha}: \pi</em>\right.$ is invalid.)
Token perturbation: Assume $P$ is a mathematical reasoning problem that involves a classic entity name in its story narratives. In contrast, the perturbed problem $P^{\prime}$ alters the classic name to another random one.
$\boldsymbol{H}}&lt;\pi_{21} .\left(\pi_{12}&gt;\pi_{21<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\alpha}: \pi</em>\right.$ is invalid.)
Here is an example of the token perturbation involving celebrity names: Taylor Swift $\rightarrow$ Lauren will embark on another tour in 2027. Which outcome do you think is more likely?
(a) Her first show is a flop.
(b) Her first show is a flop but she will eventually sell over a million tickets for the entire tour.}&gt;\pi_{21} .\left(\pi_{12}&lt;\pi_{21</p>
<p>Here is another example of the token perturbation applied to the classic "twenty-five horses" problem in mathematical reasoning, as referenced in Figure 1. Note that perturbing the numbers is optional, but the total number of animals must always be a square number: There are twenty-five $\rightarrow$ thirtysix horses $\rightarrow$ bunnies among which you need to find</p>
<p>out the fastest three. You can conduct a race among at most five $\rightarrow$ six to find out their relative speed. At no point, you can find out the actual speed of the horse $\rightarrow$ bunnies in a race. Find out the minimum number of races which are required to get the top five $\rightarrow$ six horses $\rightarrow$ bunnies.</p>
<h3>3.5 Token Bias in Reasoning about Sets</h3>
<p>The syllogistic fallacy involves reasoning about sets, utilizing specific quantifiers such as "all" and "some" to specify the distribution of variables. Our investigation centers on whether LLMs overfit to these tokens of quantifiers, relying heavily on their presence to generate answers that appear correct. By rephrasing these tokens with other words that convey the same meaning, we can test the robustness of LLMs' reasoning abilities.</p>
<p>Hypothesis 4 Genuine reasoning LLM should withstand irrelevant alterations to the quantifiers in problem statements.</p>
<p>Token perturbation: Assume $P$ is a syllogistic fallacy problem with quantifier tokens like "All" and "Some". In contrast, the perturbed problem removes "All" or rephrases "all" and "some" to different words with the same meaning.
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\boldsymbol{a}}: \pi</em>\right.$ is invalid.)
Here is an example of such token perturbations: Is it logically sound? All roses $\rightarrow$ Roses are flowers. Some $\rightarrow$ A subset of flowers fade quickly. Therefore, some $\rightarrow$ A subset of roses fade quickly.}&gt;\pi_{21} .\left(\pi_{12}&lt;\pi_{21</p>
<p>Continuing with the exploration of token bias in syllogistic fallacies, we propose an intriguing rephrasing of the syllogism's narrative by incorporating the names of reputable news agencies and universities. While adding the tokens of their names does not alter the logic, it could influence how LLMs perceive and process the information. LLMs prone to token bias might erroneously increase their confidence in the trustworthiness and credibility of the stories, based purely on the association with these respected institutions.</p>
<p>Hypothesis 5 Genuine reasoning LLM should withstand alterations to the narrative.</p>
<p>Token perturbation: Assume $P$ is the original problem. The perturbed problem $P^{\prime}$ adds or modifies specific tokens in the problem statement to reframe its narratives without changing the logic structure.
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\boldsymbol{a}}: \pi</em>$.
To remove potential token bias from the pattern "All..., Some..., Some...", we regard perturbed problems $P^{\prime}$ in Hypothesis 4 as the original problems $P$ here, as shown in the example below: Is it logically sound? Roses $\rightarrow$ In a recent publication by Bloomberg, it was noted that roses are flowers. A subset of $\rightarrow$ Research from MIT supports the finding that a subset of flowers fade quickly. Therefore, a subset of roses fade quickly.}&lt;\pi_{21}$ or $\pi_{12}&gt;\pi_{21</p>
<p>To ensure a more comprehensive comparison, we also alter tokens to satirical sources like The Onion and less reputable institutions, noting that these names never impact the logical structure of the problems. Here is an example: Is it logically sound? Roses $\rightarrow$ In a recent publication by the Daily Rumor, it was noted that roses are flowers. A subset of $\rightarrow$ An anonymous blog post writes the finding that a subset of flowers fade quickly. Therefore, a subset of roses fade quickly.</p>
<h3>3.6 Leaking Hint Tokens</h3>
<p>Just as a proficient student doesn't need hints to excel in a math exam, a reasoning agent should solve logical problems effectively without explicit cues. Besides, even if a student answers all problems correctly but the examlet provides all the reasoning steps, we may still question whether the student really understands the reasoning. Our experiments deliberately leak important hints that we expect a genuine reasoner to figure out itself in its intermediate reasoning steps.</p>
<p>Hypothesis 6 Genuine Reasoning LLMs should not rely on hint tokens to derive correct inferences.</p>
<p>Token perturbation: Assume in-context learning scenarios. The perturbed problem $P^{\prime}$ explicitly adds hint tokens in its prompts, such as the name of the logical fallacy or detailed guidance on the correct reasoning, while $P$ does not.
$\boldsymbol{H}<em 12="12">{0}: \pi</em>$.
$\boldsymbol{H}}=\pi_{21<em 12="12">{\boldsymbol{a}}: \pi</em>\right.$ is invalid.)
Here is an example of such token perturbations: Marsha Ellis, 42, is an African American transgender female. She is an ardent advocate for gender-affirming rights and environmental protection. Which is more probable? $\rightarrow$ Please be aware that this is a problem on the conjunction fallacy.
(a) Marsha is a research scientist.
(b) Marsha is a research scientist and volunteers at LGBTQ+ health centers.}&lt;\pi_{21} .\left(\pi_{12}&gt;\pi_{21</p>
<p>We also manually craft a detailed chain-ofthought instructions (Wei et al., 2022b) that teach LLMs about correct reasoning steps and potential logical pitfalls, as shown in Appendix B.</p>
<h2>4 Experiment</h2>
<p>Our experiments aim to rigorously test the reasoning capabilities of LLMs through the hypotheses in Section 3 on token bias. More comprehensive results are included in Appendix D. In all experiments, we run $n$ trials for each "modelprompting method" pair, depending on how many synthetic data samples are related to each hypothesis, and then perform a McNemar test. We apply the Benjamini-Hochberg Procedure and reject the null hypothesis if the p -value is less than $\alpha=0.05$.</p>
<h3>4.1 Models</h3>
<p>We experiment with a variety of commercial and open-sourced LLMs for a thorough study, including OpenAI gpt-3.5-turbo, gpt-4-turbo, and gpt4o (Brown et al., 2020; Achiam et al., 2023), Meta llama-3-70b-instruct, llama-3-8b-instruct, and llama-2-70b-chat (Touvron et al., 2023), Anthropic claude-3-opus-20240229 and claude-3-sonnet-20240229 (Anthropic, 2024), and Mistral mistral-large-largest (Jiang et al., 2023).</p>
<h3>4.2 Synthetic Dataset Generation</h3>
<p>We leverage data sources such as occupational statistics (USDL, 2024), commonsense stories (Mostafazadeh et al., 2016), CNN news stories (See et al., 2017), common disease symptom pairs (kag), celebrity names (Rosenberg, 2021; Wikipedia contributors, 2024a), objects vocabularies (esl) and common U.S. news media (Wikipedia contributors, 2024b; Pew Research Center, 2011) to curate lists of entities to generate synthetic data. We outline our well-controlled data generation process and the samples used for each hypothesis in Appendix C.</p>
<h3>4.3 Prompting Methods</h3>
<p>We implemented commonly used prompting strategies that are sufficient for evaluating the null hypotheses within our framework. The specific prompting techniques we utilized are as follows, with their corresponding notations presented in Figure 4: Baseline: Directly answering the question without additional instructions. Zero-shot chain-of-thought (zs_cot): Includes the instruction "Let us think step by step" (Wei et al., 2022b).</p>
<p>One-shot (os): Involves a single in-context learning example (Brown et al., 2020). Few-shots (fs): Utilizes three in-context examples. Similarly, we have os_cot and fs_cot. We also include weak_control_zs/os_cot and control_zs/os_cot for Hypothesis 6 representing prompts with additional weak or strong hints, as detailed in Appendix B.</p>
<h3>4.4 Hypothesis Testing Results</h3>
<p>Testing of Hypothesis 1: LLMs Would Fail at Misleading Options We evaluate LLMs on all conjunction fallacy problems with misleading options ( $n=400$ ). Figure 4a and Table 2 show a significant decline in success rate when contextually misleading options in conjunction fallacy problems are replaced with random alternatives. The random ones are no longer relevant to the problem statements, so all LLMs become less likely to be swayed by background information that is not logically important. We, therefore, reject almost all null hypotheses.</p>
<p>Testing of Hypothesis 2: LLMs Would Fail Due to Surface Level Change in the Exemplar We evaluate LLMs under in-context learning scenarios for solving conjunction fallacies ( $n=500$ ). Figure 4 b and Table 3 show consistent performance drop on all LLMs when the name "Linda," frequently used in classic reasoning tasks, is substituted with "Bob" in one-shot exemplars. Such a change should not influence outcomes for genuine reasoners, as the specific name used is irrelevant to the logical process.</p>
<p>Testing of Hypothesis 3: LLMs Would be Misled by Celebrity Names We evaluate LLMs on variants of conjunction fallacies that contain a celebrity name ( $n=100$ ). We observe in Figure 4c and Table 4 to Figure 4a that celebrity names appeared in problem statements frequently mislead LLMs into the celebrity's background, which is not helpful in solving logical fallacy problems but reduces accuracy, leading us to reject all null hypotheses.</p>
<p>Testing of Hypothesis 4: LLMs Would Fail at Synonyms of Classic Quantifiers We assess LLMs on syllogisms ( $n=200$ ). Figure 4 d and Table 5 reveal that, in most instances, we should reject the null hypotheses, except for llama-3-70binstruct. Most LLMs demonstrate insufficient robustness when patterns "All..., Some..., Some..." commonly used in classic syllogistic fallacy problems are substituted with synonyms.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Experimental results for Hypothesis $1(n=400)$. The perturbed problems alternate options contextually relevant to the problem statements to irrelevant ones. We run all different prompt methods. To reject the null, we expect $n 12<n 21$. We conclude that LLMs fail to reason against contextually misleading options in conjunction fallacy problems.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Experimental results for Hypothesis $2(n=500)$. The perturbed problems alternate the name classic "Linda" to "Bob" in in-context learning exemplars. We run one-shot with and without chain-of-thought prompts. To reject the null, we expect $n 12>n 21$. We conclude that LLMs possess strong token bias to the name "Linda" frequently appearing in classic literature.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(c) Experimental results for Hypothesis with celebrity names $3(n=100)$. The perturbed problems alternate the celebrity name to a generic one in problem statements. We run all different prompt methods. To reject the null, we expect $n 12<n 21$. We conclude that LLMs are frequently misled by celebrity names in problem statements that are irrelevant to logical essence.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(d) Experimental results for Hypothesis $4(n=200)$. The perturbed problems alternate tokens "All" and "Some" to different but equivalent expressions in syllogisms. We run all different prompt methods. To reject the null, we expect $n 12>n 21$. We conclude that most LLMs rely on patterns "All..., Some..., Some..." for reasoning about syllogism.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(e) Experimental results for Hypothesis $5(n=200)$. The perturbed problems add the names of trustworthy news agencies and universities to alter the narratives of syllogisms. We run all different prompt methods. To reject the null, we expect $n 12&gt;n 21$. We conclude that LLMs might be misled by reputable names irrelevant to the logical structure.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(f) Experimental results for Hypothesis $6(n=800)$. The perturbed problems leak hint tokens, either weak or strong hints in problem statements. We run zero-shot and one-shot prompt methods. To reject the null, we expect $n 12&lt;n 21$. We conclude that LLMs still heavily rely on hint tokens for solving logical fallacy problems well.
Figure 4: Our controlled experiments cast doubt on the genuine reasoning capabilities of LLMs. In this figure, each pair of histograms stuck together represents a comparison in the contingency table 1 for McNemar's Tests.</p>
<p>Testing of Hypothesis 5: LLMs Would Be Misled by Names Linked to Reputable Entities We evaluated the impact of names linked to reputable data sources in syllogisms ( $n=200$ ). Figure 4 e and Table 6 demonstrate that some LLMs are indeed misled by the inclusion of these authoritative names, especially GPT-4 and LLaMA-3-70B. Generally, LLMs tend to falsely believe that these narratives are more trustworthy and, thereby, ignore the logical fallacy in them. As a result, we reject about half of the null hypotheses. Results from using the names of less credible sources are shown in Appendix 7 for comparison.</p>
<p>Testing of Hypothesis 6: LLMs Still Heavily Rely on Hint Tokens We evaluated the performance of LLMs with and without the presence of hints ( $n=200$ ). Figure 4 f and Table 8 indicate that LLMs still heavily rely on hints to achieve ideal performance, so we reject all the null hypotheses.</p>
<h2>5 Related Work</h2>
<p>The Reasoning Capabilities of LLMs There is an increasing number of works aim to improving the reasoning performance of LLMs (Ouyang et al., 2022; Zhou et al., 2022; Lyu et al., 2023; Hao et al., 2023; Xu et al., 2024; Putta et al., 2024; Kumar et al., 2024; Yao et al., 2024; Cai et al., 2024), evaluating and critiquing their reasoning abilities (Zhou et al., 2020; Hong et al., 2023; Huang et al., 2023; Shi et al., 2023; Sprague et al., 2024; Turpin et al., 2024; Xiao et al., 2024), trying to understand their reasoning processes (Wei et al., 2022a; Merrill and Sabharwal, 2023; Ma et al., 2023; Lanham et al., 2023; Merrill and Sabharwal, 2024; Yang et al., 2024; Chen et al., 2024; Jin et al., 2024), as well as related surveys (Qiao et al., 2022; Huang and Chang, 2022; Ahn et al., 2024; Giadikiaroglou et al., 2024; Liang et al., 2024; Zheng et al., 2024) that comprehensively explore the reasoning capabilities of LLMs. Our work aligns with efforts that critique the generalization of LLMs' reasoning capabilities, moving beyond accuracy-based benchmarks (Talmor et al., 2018; Srivastava et al., 2022a,b; Fu et al., 2023; Yang et al., 2018), which primarily focus on overall question-answering accuracy, often without considering question augmentations or variations. Instead, we step one-level deeper into these problem statements, investigating potential token biases that may create the illusion of improved logical reasoning performance, while remaining vulnerable to
irrelevant token perturbations. By examining these subtleties, our approach provides a more nuanced understanding of the limitations in LLM's reasoning capabilities. Besides, we reformulate the evaluation as hypothesis testing, providing results with statistical guarantees.</p>
<p>Cognitive Biases and Logical Fallacies in LLMs Recent studies (Gardner et al., 2020; Hagendorff et al., 2023; Lin and Ng, 2023; Talboy and Fuller, 2023; Binz and Schulz, 2023; Ullman, 2023; Mitchell and Krakauer, 2023) analyze the biases in LLMs with synthetic datasets. For example, Tamkin et al. (2023) uses an LLM to generate prompts that reveals patterns of discriminations in LLMs. Echterhoff et al. (2024) proposes a set of LLM-simulated experiments in a specific context. Although existing works (Mukherjee and Chang, 2024; Macmillan-Scott and Musolesi, 2024; Wang et al., 2024; Suri et al., 2024; Jin et al., 2022) study more kinds of fallacy types in human psychology, they approach problems at a coarse level and only emphasize accuracy. Our study goes into a more fine-grained level with a series of hypotheses. We provide statistical guarantees and quantitative analyses of token bias that can be carefully and systematically tuned. Besides, Gou et al. (2023) presents the Rationality of Thought (RoT), decomposing responses into six predefined steps with hand-crafted prompt engineerings. Our work focuses on general prompting strategies that are sufficient to validate or reject our hypotheses.</p>
<h2>6 Discussion</h2>
<p>This work reconceptualizes the evaluation of the reasoning behavior of LLMs through the lens of token bias. The statistical evidence presented in our hypothesis-testing framework contributes to the larger discussion that LLMs do not always apply reasoning consistently in their decision-making processes. Instead, they primarily rely on token bias for response generation. This suggests that chain-of-thought prompting (Wei et al., 2022b; Wang et al., 2022) or in-context learning (Brown et al., 2020; Min et al., 2022; Lyu et al., 2022; Wang et al., 2022) may not elicit actual reasoning but instead result in semantic shortcuts for LLMs to imitate the desired behavior at superficial levels. These findings raise concerns about the extent to which LLMs truly engage in reasoning. Further investigations are needed to uncover the underlying mechanisms and limitations of LLMs' reasoning capabilities.</p>
<h2>7 Limitations</h2>
<p>This hypothesis-testing framework is specifically designed for multiple-choice or yes/no questions and is not applicable to open-ended responses. It relies on LLMs with strong instruction-following capabilities to consistently produce responses that include the selected options, though we find that LLMs can generally follow these instructions in most cases. In addition, smaller LLMs, such as llama-3-8b-instruct, with lower instruction following capabilities may contain more confounders besides the token bias, which could weaken our hypothesis testing results. As a result, we mainly focus on state-of-the-art LLMs. Moreover, the finding of token biases requires manual efforts. We also acknowledge that there are likely other hypotheses and assumptions that a genuine reasoner should satisfy. Our current study focuses on the conjunction fallacy, syllogistic fallacy, the "twenty-five horses" problem in graph theory, and their variants to demonstrate our framework. These are quintessential examples and the framework could include a broader range of hypotheses, fallacy types, data modalities, and reasoning tasks.</p>
<h2>8 Acknowledgement</h2>
<p>This work was supported by NSF grant CCF2112665 (TILOS), which provides funding for B.J. and C.J. It was also funded in part by the Laboratory Directed Research and Development (LDRD) program at Argonne National Laboratory, with support from the Office of Science, U.S. Department of Energy, under Contract No. DEAC02-06CH11357. Y. X. and W. J. Su acknowledge support from the NSF HDR TRIPODS award (CCF-1934876). The authors thank John K. Hutchison for his valuable suggestions regarding the motivation of this study.</p>
<h2>References</h2>
<p>Disease symptom description dataset. Accessed: 2024-06-1.</p>
<p>Esl kids vocabulary. Accessed: 2024-06-15.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Alan Agresti. 2012. Categorical data analysis, volume 792. John Wiley \&amp; Sons.</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157.</p>
<p>Anthropic. 2024. Models overview. Software available from Anthropic. Accessed: 2024-05-20.</p>
<p>Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: A practical and powerful approach to multiple testing, Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289-300.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690.</p>
<p>Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Chengkun Cai, Xu Zhao, Yucheng Du, Haoliang Liu, and Lei Li. 2024. T2 of thoughts: Temperature tree elicits reasoning in large language models. arXiv preprint arXiv:2405.14075.</p>
<p>Xinyun Chen, Ryan A Chi, Xuezhi Wang, and Denny Zhou. 2024. Premise order matters in reasoning with large language models. arXiv preprint arXiv:2402.08939.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051.</p>
<p>Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. 2024. Cognitive bias in high-stakes decision-making with llms. arXiv preprint arXiv:2403.00811.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages $1307-1323$.</p>
<p>Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle solving using reasoning of large language models: A survey. arXiv preprint arXiv:2402.11291.</p>
<p>Tian Gou, Boyao Zhang, Zhenglie Sun, Jing Wang, Yangang Wang, and Jue Wang. 2023. Rationality of thought improves reasoning in large language models.</p>
<p>Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. 2023. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 3(10):833-838.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Reid Hastie and Robyn M Dawes. 2009. Rational choice in an uncertain world: The psychology of judgment and decision making. Sage Publications.</p>
<p>Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2023. A closer look at the self-verification abilities of large language models in logical reasoning. arXiv preprint arXiv:2311.07954.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Mohsen Jamali, Ziv M Williams, and Jing Cai. 2023. Unveiling theory of mind in large language models: A parallel to single neurons in the human brain. arXiv preprint arXiv:2309.01660.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J Su, Camillo J Taylor, and Tanwi Mallick. 2024a. Multi-modal and multi-agent systems meet rationality: A survey. arXiv preprint arXiv:2406.00252.</p>
<p>Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, and Camillo J. Taylor. 2024b. Multiagent vqa: Exploring multi-agent foundation models in zero-shot visual question answering. Preprint, arXiv:2403.14783.</p>
<p>Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. 2024. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925.</p>
<p>Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf. 2022. Logical fallacy detection. arXiv preprint arXiv:2202.13758.</p>
<p>Daniel Kahneman. 2011. Thinking, fast and slow. macmillan.</p>
<p>Michal Kosinski. 2023. Evaluating large language models in theory of mind tasks. arXiv e-prints, pages arXiv-2302.</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. 2024. Training language models to selfcorrect via reinforcement learning. arXiv preprint arXiv:2409.12917.</p>
<p>Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. 2023. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702.</p>
<p>Erich Leo Lehmann, Joseph P Romano, and George Casella. 1986. Testing statistical hypotheses, volume 3. Springer.</p>
<p>Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, and Muhao Chen. 2023. Deceiving semantic shortcuts on reasoning chains: How far can models go without hallucination? arXiv preprint arXiv:2311.09702.</p>
<p>Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, and Zhiyu Li. 2024. Internal consistency and selffeedback in large language models: A survey. arXiv preprint arXiv:2407.14507.</p>
<p>Ruixi Lin and Hwee Tou Ng. 2023. Mind the biases: Quantifying cognitive biases in language model prompting. In Findings of the Association for Computational Linguistics: ACL 2023, pages 5269-5281.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. Z-icl: zero-shot incontext learning with pseudo-demonstrations. arXiv preprint arXiv:2212.09865.</p>
<p>Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2023. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298.</p>
<p>Olivia Macmillan-Scott and Mirco Musolesi. 2024. (ir) rationality and cognitive biases in large language models. arXiv preprint arXiv:2402.09193.</p>
<p>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153-157.</p>
<p>William Merrill and Ashish Sabharwal. 2023. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923.</p>
<p>William Merrill and Ashish Sabharwal. 2024. A logic for expressing log-precision transformers. Advances in Neural Information Processing Systems, 36.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Melanie Mitchell and David C Krakauer. 2023. The debate over understanding in ai's large language models. Proceedings of the National Academy of Sciences, 120(13):e2215907120.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696.</p>
<p>Frederick Mosteller. 1952. Some statistical problems in measuring the subjective response to drugs. Biometrics, 8(3):220-226.</p>
<p>Anirban Mukherjee and Hannah Hanwen Chang. 2024. Heuristic reasoning in ai: Instrumental use and mimetic absorption. arXiv preprint arXiv:2403.09404.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.</p>
<p>Pew Research Center. 2011. Top 25. Accessed: 2024-06-15.</p>
<p>Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772.</p>
<p>Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. 2024. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199.</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. arXiv preprint arXiv:2212.09597.</p>
<p>Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. 2024. How much are llms contaminated? a comprehensive survey and the llmsanitize library. arXiv preprint arXiv:2404.00699.</p>
<p>Jennifer Rosenberg. 2021. Times man of the year list. Accessed: 05-05-2024.</p>
<p>Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210-31227. PMLR.</p>
<p>Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022a. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022b. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Gaurav Suri, Lily R Slater, Ali Ziaee, and Morgan Nguyen. 2024. Do large language models show decision heuristics similar to humans? a case study using gpt-3.5. Journal of Experimental Psychology: General.</p>
<p>Alaina N Talboy and Elizabeth Fuller. 2023. Challenging the appearance of machine intelligence: Cognitive bias in llms. arXiv preprint arXiv:2304.01358.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.</p>
<p>Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli. 2023. Evaluating and mitigating discrimination in language model decisions. arXiv preprint arXiv:2312.03689.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2024. Language models don't always say what they think: unfaithful explanations in chain-ofthought prompting. Advances in Neural Information Processing Systems, 36.</p>
<p>Amos Tversky and Daniel Kahneman. 1981. The framing of decisions and the psychology of choice. science, 211(4481):453-458.</p>
<p>Amos Tversky and Daniel Kahneman. 1983. Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychological review, 90(4):293.</p>
<p>Amos Tversky and Daniel Kahneman. 1988. Rational choice and the framing of decisions. Decision making: Descriptive, normative, and prescriptive interactions, pages 167-192.</p>
<p>Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.</p>
<p>USDL. 2024. Occupational employment and wage statistics. Accessed: 05-05-2024.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001.</p>
<p>Pengda Wang, Zilin Xiao, Hanjie Chen, and Frederick L Oswald. 2024. Will the real linda please stand up... to large language models? examining the representativeness heuristic in llms. arXiv preprint arXiv:2404.01461.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Wikipedia contributors. 2024a. Forbes celebrity 100. Accessed: 2024-05-20.</p>
<p>Wikipedia contributors. 2024b. News media in the united states. Accessed: 2024-06-1.</p>
<p>Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. 2024. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973.</p>
<p>Yangxinyu Xie, Tanwi Mallick, Joshua David Bergerson, John K Hutchison, Duane R Verner, Jordan Branham, M Ross Alexander, Robert B Ross, Yan Feng, Leslie-Anne Levy, et al. 2024. Wildfiregpt: Tailored large language model for wildfire analysis. arXiv preprint arXiv:2402.07877.</p>
<p>Han Xu, Jingyang Ye, Yutong Li, and Haipeng Chen. 2024. Can speculative sampling accelerate react without compromising reasoning quality? In The Second Tiny Papers Track at ICLR 2024.</p>
<p>Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo Tang, Feiyu Xiong, and Zhiyu Li. 2024. Attention heads of large language models: A survey. arXiv preprint arXiv:2409.03752.</p>
<p>Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth. 2020. Temporal reasoning on implicit events from distant supervision. arXiv preprint arXiv:2010.12753.</p>
<p>Ben Zhou, Kyle Richardson, Xiaodong Yu, and Dan Roth. 2022. Learning to decompose: Hypothetical question decomposition based on comparable texts. arXiv preprint arXiv:2210.16865.</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don't make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964.</p>
<h1>A The Original Linda Problem in Tversky and Kahneman (1983)</h1>
<p>The original Linda problem is framed as follows (Tversky and Kahneman, 1983):
Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations. Which is more probable?</p>
<ol>
<li>Linda is a bank teller.</li>
<li>Linda is a bank teller and is active in the feminist movement.</li>
</ol>
<p>Here is an example of GPT-4o explaining the Linda Problem: https://chatgpt.com/share/ eff10b9d-d219-4806-9cb9-d2d9104c0e83.</p>
<p>Our "Bob" version of this problem is as follows:
Bob is 29 years old, deeply passionate about environmental conservation, and volunteers his weekends at local park clean-ups. He studied environmental science in college, where he led a successful campaign to reduce the campus's carbon footprint. Bob is also an avid cyclist and promotes sustainable living practices whenever possible. Based on this information, which is more possible?</p>
<ol>
<li>Bob works for a renewable energy company and is an active member of a local environmental advocacy group.</li>
<li>Bob works for a renewable energy company.</li>
</ol>
<h2>B Prompts in Hypothesis 6</h2>
<p>This section includes the detailed prompts we use to evaluate the influences from weak and strong hints. These prompts are added to either the zero-shot chain-of-thought or the one-shot chain-of-thought prompts.</p>
<h2>B. 1 Weak Hint</h2>
<p>For Problems on Conjunction Fallacies Your task is to answer the following question by explicitly selecting either option (a), (b), etc. Please be aware that this is a Linda Problem designed to explore the concept of the conjunction fallacy. Here is the question and let's think step by step.</p>
<p>For Problems on Syllogistic Fallacies Your task is to answer the following question by explicitly saying 'Yes' or 'No'. Please be aware that this is a Linda Problem designed to explore the concept of the syllogistic fallacy.</p>
<h2>B. 2 Strong Hint</h2>
<p>For Problems on Conjunction Fallacies Your task is to answer the following question by explicitly selecting either option (a), (b), etc. Please aware that this is a Linda Problem designed to explore the concept of the conjunction fallacy. The conjunction fallacy occurs when individuals incorrectly judge the conjunction of two events as more probable than one of the events alone. For instance, many might believe that Linda, who is described as a bright, single woman deeply concerned with discrimination and social justice, is more likely to be both a bank teller and active in the feminist movement than just a bank teller. This judgment violates the basic probability rule: the probability of a conjunction, $\mathrm{P}(\mathrm{A}$ and B$)$, is always less than or equal to the probabilities of its constituents, $\mathrm{P}(\mathrm{A})$ or $\mathrm{P}(\mathrm{B})$. This error often stems from the representativeness heuristic, where people estimate the likelihood of an event by how closely it matches their mental prototype. To correctly solve problems like this, you must adopt probabilistic thinking: abstract the problem from its narrative context and focus solely on the probabilistic models. Ignore all extraneous background information and consistently choose the option involving a single event as it statistically holds a higher likelihood than the conjunction of multiple events. Here is the question and let's think step by step.</p>
<p>For Problems on Syllogistic Fallacies Your task is to answer the following question by explicitly saying 'Yes' or 'No'. Please aware that this is a Syllogistic Fallacy Problem. This type of reasoning is known as a syllogism. Pay close attention to quantifiers such as 'All', 'Some', 'No', or similar terms. These terms help define the distribution of properties or elements within the given groups or categories in the premises. Next, assess whether the attribute ascribed in the conclusion necessarily follows from the attributes described in the premises. Consider if the subset described in the second premise encompasses or overlaps with the elements in the first premise that are carried into the conclusion. A common pitfall in syllogistic reasoning is the erroneous assumption that a characteristic of a subset of a group (from the premises) applies to another subset of the same or different group (in the conclusion), without explicit justification. Ignore the background information about the objects and focus on the logical structure of the argument. Here is an example.</p>
<h1>C Synthetic Data Generation</h1>
<p>In this section, we outline the controlled synthetic data generation process. For each variant, we generate 100 synthetic data samples.</p>
<h2>C. 1 Conjunction Fallacy</h2>
<p>We create several variants of the conjunction fallacy problem discussed in the original work by Tversky and Kahneman (1983):</p>
<p>Variant 1 The original Linda Problem. We maintain the narrative structure of the original Linda Problem described in Appendix A. We ask GPT-4 to randomly pick reasonable personal details such as name, race, gender identity, age, and major, forming a short biography. GPT-4 then crafts two options $(a)$ and $(b)$ for each problem, both of which contain the same randomly selected occupation from USDL (2024) like "Linda is a bank teller". The longer option also contains a hobby that must be relevant to the bio like "active in the feminist movement".</p>
<p>The prompt used to generate the bio is as follows, where {random_gender}, {random_race}, {random_age } are sampled from a pre-defined random function:</p>
<p>Your task is to write a short bio for a random person within 100 words. You shall pick a random name, use gender {random_gender}, race {random_race}, and an age {random_age}. The bio should describe the college majors, some personal characters, and interests. Keep the bio short. For example, 'Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Write another example here:</p>
<p>We then follow-up the conversation with the following prompt:</p>
<p>Your next step is to find a hobby or activity that the person mentioned before will be interested in based on your experience. The hobby or activity must be relevant to the bio descriptions. In the example above, we can say that 'Linda is active in the feminist movement.' because her bio says she was concerned with discrimination and social justice. Please keep your answer in one sentence and begin with that person's name, but refrain from using any words used in the bio.</p>
<p>To create token bias, we generate a random hobby using the following:
Your task is to find a random hobby or activity, and keep your answer short in one sentence. For example, you can say 'cook Asian foods.'</p>
<p>Variant 2 In the original paper, the following variant of the conjunction fallacy problem is also presented:
John P. is a meek man, 42 years old, married with two children. His neighbors describe him as mild-mannered but somewhat secretive. He owns an import-export company based in New York City, and he travels frequently to Europe and the Far East. Mr. P. was convicted once for smuggling precious stones and metals (including uranium) and received a suspended sentence of 6 months in jail and a large fine. Mr. P. is currently under police investigation. Which one is more likely?</p>
<ol>
<li>Mr. P. killed one of his employees.</li>
<li>Mr. P. killed one of his employees to prevent him from talking to the police.</li>
</ol>
<p>The conjunction of two events in the second option is connected by the word 'to.' To create this dataset, we sample a random story from the collection of commonsense stories (Mostafazadeh et al., 2016) and CNN news stories (See et al., 2017). We use all the sentences in the story as the context of the conjunction fallacy problem except for the last one. We use the last sentence as the first option in the problem. As for the second option, we append the last sentence, add the connecting word 'to,' and then we prompt GPT-4 to complete the second option. The prompt used here is similar to that discussed in section 2.</p>
<p>To perform token perturbation, we further prompt GPT-4 with the following:
Your next task is to complete the last sentence of the same problem but make sure your completion after 'to' is now irrelevant to the content intentionally:</p>
<p>Variant 3 This is the same as the last variant, except that we use 'because' as the connecting word.
Variant 4 This is the same as the last variant, except that we use 'so that' as the connecting word.
Variant 5 In the original paper, the following variant of the conjunction fallacy is discussed:
A 55-year-old woman had pulmonary embolism documented angiographically 10 days after a cholecystectomy. Which is more likely?</p>
<ol>
<li>dyspnea and hemiparesis</li>
<li>hemiparesis</li>
</ol>
<p>Inspired by this example, we randomly sample a disease and its corresponding symptoms from (kag) and apply the following prompt to generate a conjunction fallacy problem:</p>
<p>Your task is to create another conjunction fallacy quiz following the format in the example below. Do not mention the name 'conjunction fallacy.' You should pick a random name for the patient, use gender {random_gender} race {random_race}, an age {random_age} and the disease {random_disease} in your new problem statement. The question should be 'Which one is more likely?' followed by two options (a) and (b), one of which should be a subset of the other. You can randomly switch the order of which option is (a) and which is (b). You should use the symptoms {random_symptom_one} in both options and add {random_symptom_two} to the longer option only. Do not make any changes to the given disease or the symptoms. Here is the new problem:</p>
<p>We then prompt GPT-4 for an irrelevant symptom:
Your task is to create another conjunction fallacy quiz following the format in the example below. Do not mention the name 'conjunction fallacy.' You should pick a random name for the patient, use gender {random_gender} race {random_race}, an age {random_age} and the disease {random_disease} in your</p>
<p>new problem statement. The question should be 'Which one is more likely?' followed by two options (a) and (b), one of which should be a subset of the other. You can randomly switch the order of which option is (a) and which is (b). You should use the symptoms {random_symptom_one} in both options. You should add another random symptoms to the longer option only, which must be completely irrelevant to the disease {random_disease} intentionally. Do not make any changes to the given disease or the symptoms. Here is the new problem:</p>
<p>Variant 6 In the original paper, the following variant of the conjunction fallacy is discussed:
Suppose Bjorn Borg reaches the Wimbledon finals in 1981. Which is more likely?</p>
<ol>
<li>Borg will lose the first set</li>
<li>Borg will lose the first set but win the match</li>
</ol>
<p>Inspired by this example, we randomly sample celebrity names from the Times Person of the Year (Rosenberg, 2021) and Forbes Celebrity 100 (Wikipedia contributors, 2024a) and apply the following few-shot prompt to generate a conjunction fallacy problem.</p>
<p>Create one example that look like this:
Suppose [celebrity is going to do something]. Which is more likely:
(a) [Something unlikely for this person]
(b) [Something unlikely for this person] but [something extremely likely for this person]</p>
<p>Here are some examples:</p>
<p>Suppose Taylor Swift is going to have another tour in 2027. Which is more likely:
(a) Her first show is a flop.
(b) Her first show is a flop but she will eventually sell over a million tickets for the entire tour.
Suppose Barack Obama is running for president in 2024. Which is more likely:
(a) Barack Obama will win the national popular vote
(b) Barack Obama will win the national popular vote but lose the Electoral College vote
Suppose Bjorn Borg reaches the Wimbledon finals. Which outcome is more likely?
(a) Borg will lose the first set
(b) Borg will lose the first set but win the match</p>
<p>Complete the following. Do not output anything else.
Suppose {random_celebrity}
For Hypothesis 1, we include Variant 2,3,4 and 5, resulting in $n=400$ samples. For Hypothesis 2, we include Variant 2,3,4,5 and 6, resulting in $n=500$ samples. For Hypothesis 3, we include Variant 5, resulting in $n=100$ samples.</p>
<h1>C. 2 Syllogistic Fallacy</h1>
<p>For Hypothesis 4, we randomly sample an entity {random_object} from a curated list of objects from esl and use the following few-shot prompt to generate $n=200$ problems:</p>
<p>Fill in the blanks in the following template. Do not output anything else. All [objects] are [category].</p>
<div class="codehilite"><pre><span></span><code><span class="ow">Some</span><span class="w"> </span><span class="o">[</span><span class="n">category</span><span class="o">]</span><span class="n">s</span><span class="w"> </span><span class="o">[</span><span class="n">characteristic traits of this category</span><span class="o">]</span><span class="p">.</span>
<span class="n">Therefore</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="o">[</span><span class="n">same objects as before</span><span class="o">]</span><span class="w"> </span><span class="o">[</span><span class="n">characteristic traits this category</span><span class="o">]</span><span class="p">.</span>
<span class="n">Make</span><span class="w"> </span><span class="n">sure</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">characteristic</span><span class="w"> </span><span class="n">traits</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">category</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">fit</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">subset</span>
<span class="k">of</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">category</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="ow">all</span><span class="p">.</span>
<span class="k">For</span><span class="w"> </span><span class="nl">example</span><span class="p">:</span>
<span class="ow">All</span><span class="w"> </span><span class="n">carrots</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">vegetables</span><span class="p">.</span>
<span class="ow">Some</span><span class="w"> </span><span class="n">vegetables</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">rich</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">fiber</span><span class="p">.</span>
<span class="n">Therefore</span><span class="p">,</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="n">carrots</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">rich</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">fiber</span><span class="p">.</span>
<span class="ow">All</span><span class="w"> </span><span class="n">roses</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">flowers</span><span class="p">.</span>
<span class="ow">Some</span><span class="w"> </span><span class="n">flowers</span><span class="w"> </span><span class="n">fade</span><span class="w"> </span><span class="n">quickly</span><span class="p">.</span>
<span class="n">Therefore</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="n">roses</span><span class="w"> </span><span class="n">fade</span><span class="w"> </span><span class="n">quickly</span><span class="p">.</span>
<span class="ow">All</span><span class="w"> </span><span class="n">actors</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">performers</span><span class="p">.</span>
<span class="ow">Some</span><span class="w"> </span><span class="n">performers</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">skilled</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">improvisation</span><span class="p">.</span>
<span class="n">Therefore</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="n">actors</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">skilled</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">improvisation</span><span class="p">.</span>
<span class="ow">All</span><span class="w"> </span><span class="err">{</span><span class="n">random_object</span><span class="err">}</span><span class="w"> </span><span class="k">are</span>
</code></pre></div>

<p>The common U.S. news media sources we used to perturb these problems in Hypothesis 5 are taken from (Wikipedia contributors, 2024b; Pew Research Center, 2011). This also results in $n=200$ samples.</p>
<h1>D Additional Experiment Results</h1>
<h2>D. 1 Hypothesis 1</h2>
<p>The full experimental results for Hypothesis 1 are shown in Figure 5, 6 and Table 2.
Table 2: Full Experimental results for Hypothesis 1. Note that in our experiments, $n^{<em>}=n_{21}+n_{12}$ is not equal to the number of data samples $n$. Here, $n_{12}$ denotes the instances where the LLM correctly answers the original problem but fails on the perturbed version, and $n_{21}$ denotes the opposite scenario. Thus, a large value of $n^{</em>}$ happens only when the LLM makes many mistakes. Specifically, GPT-4o in this table shows $n^{<em>}=1$ with few-shots learning. We find that GPT-4o is excellent in answering these problems with few-show exemplars, achieving near-perfect accuracy of almost $100 \%$, so their $n_{12}$ and $n_{21}$ values are pretty small. This high accuracy, however, only lead us to fail to reject this particular instantiation of the null hypothesis, but not in other situations. While we could increase the sample size from the current $n$ to potentialy observe more errors, thus a higher $n^{</em>}$ in scenarios involving state-of-the-art LMs with few-shot learning, our rejection of the null hypothesis under other scenarios when tested against the GPT-4o is sufficient to argue that LLMs are not yet genuine reasoners.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">model</th>
<th style="text-align: center;">prompting method</th>
<th style="text-align: center;">$n_{12}$</th>
<th style="text-align: center;">$n_{21}$</th>
<th style="text-align: center;">$n^{*}$</th>
<th style="text-align: center;">z-stat</th>
<th style="text-align: center;">p-value</th>
<th style="text-align: center;">reject</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">12.181553</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">237</td>
<td style="text-align: center;">12.926439</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">10.310436</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">147</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">10.151295</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">10.000000</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">fs-cot</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">186</td>
<td style="text-align: center;">12.758299</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">357</td>
<td style="text-align: center;">364</td>
<td style="text-align: center;">18.344985</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">331</td>
<td style="text-align: center;">337</td>
<td style="text-align: center;">17.703878</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">8.544004</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.901475</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">5.924742</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4-turbo</td>
<td style="text-align: center;">fs-cot</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">7.071068</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">365</td>
<td style="text-align: center;">18.581549</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">16.496265</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">5.744563</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">8.907784</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.000000</td>
<td style="text-align: center;">0.158655</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">gpt-4o</td>
<td style="text-align: center;">fs-cot</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">7.967434</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">215</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">14.358452</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">229</td>
<td style="text-align: center;">11.167834</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">3.591391</td>
<td style="text-align: center;">0.000181</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">208</td>
<td style="text-align: center;">9.152553</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">205</td>
<td style="text-align: center;">1.746076</td>
<td style="text-align: center;">0.042775</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">llama-2-70b-chat</td>
<td style="text-align: center;">fs-cot</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">193</td>
<td style="text-align: center;">7.702029</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">331</td>
<td style="text-align: center;">17.094106</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">296</td>
<td style="text-align: center;">308</td>
<td style="text-align: center;">16.182402</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">1.360828</td>
<td style="text-align: center;">0.090122</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">4.024922</td>
<td style="text-align: center;">0.000032</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">1.206045</td>
<td style="text-align: center;">0.116049</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-70b-instruct</td>
<td style="text-align: center;">fs-cot</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">3.149704</td>
<td style="text-align: center;">0.000883</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-8b-instruct</td>
<td style="text-align: center;">baseline</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">272</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">15.777018</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-8b-instruct</td>
<td style="text-align: center;">zs-cot</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">263</td>
<td style="text-align: center;">268</td>
<td style="text-align: center;">15.759858</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-8b-instruct</td>
<td style="text-align: center;">os</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">8.429272</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-8b-instruct</td>
<td style="text-align: center;">os-cot</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">10.263860</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">meta-llama-3-8b-instruct</td>
<td style="text-align: center;">fs</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">5.025179</td>
<td style="text-align: center;">0.000000</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<p>Table 2 - Continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: left;">model</th>
<th style="text-align: left;">prompting method</th>
<th style="text-align: right;">$n_{12}$</th>
<th style="text-align: right;">$n_{21}$</th>
<th style="text-align: right;">$n^{*}$</th>
<th style="text-align: right;">z-stat</th>
<th style="text-align: right;">p-value</th>
<th style="text-align: right;">reject</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">meta-llama-3-8b-instruct</td>
<td style="text-align: left;">fs-cot</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">112</td>
<td style="text-align: right;">132</td>
<td style="text-align: right;">8.007572</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">baseline</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">241</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">13.945631</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">zs-cot</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">229</td>
<td style="text-align: right;">239</td>
<td style="text-align: right;">14.165932</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">136</td>
<td style="text-align: right;">151</td>
<td style="text-align: right;">9.846840</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">157</td>
<td style="text-align: right;">170</td>
<td style="text-align: right;">11.044296</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">fs</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">140</td>
<td style="text-align: right;">149</td>
<td style="text-align: right;">10.731938</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-opus-20240229</td>
<td style="text-align: left;">fs-cot</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">129</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">9.616783</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">baseline</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">364</td>
<td style="text-align: right;">372</td>
<td style="text-align: right;">18.457740</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">zs-cot</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">313</td>
<td style="text-align: right;">321</td>
<td style="text-align: right;">17.023440</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">16.062378</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">165</td>
<td style="text-align: right;">174</td>
<td style="text-align: right;">11.826329</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">fs</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">178</td>
<td style="text-align: right;">12.891945</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">claude-3-sonnet-20240229</td>
<td style="text-align: left;">fs-cot</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">167</td>
<td style="text-align: right;">9.208496</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">baseline</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">388</td>
<td style="text-align: right;">396</td>
<td style="text-align: right;">19.095718</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">zs-cot</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">384</td>
<td style="text-align: right;">389</td>
<td style="text-align: right;">19.216063</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">94</td>
<td style="text-align: right;">9.076507</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">83</td>
<td style="text-align: right;">8.232319</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">fs</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">7.763107</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">fs-cot</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">8.074062</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 5: Full experimental results for Hypothesis $1(n=400)$. The perturbed problems alternate options contextually relevant to the problem statements to irrelevant ones. We run all different prompt methods. To reject the null, we expect $n 12&lt;n 21$. We conclude that LLMs fail to reason against contextually misleading options in conjunction fallacy problems.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 6: Comparison of the accuracy scores between the original and perturbed problems for Hypothesis 1.</p>
<h1>D. 2 Hypothesis 2</h1>
<p>The full experimental results for Hypothesis 2 are shown in Figure 7, 8 and Table 3.
Table 3: Full experimental results for Hypothesis 2</p>
<table>
<thead>
<tr>
<th style="text-align: left;">model</th>
<th style="text-align: left;">prompting method</th>
<th style="text-align: right;">$n_{12}$</th>
<th style="text-align: right;">$n_{21}$</th>
<th style="text-align: right;">$n^{*}$</th>
<th style="text-align: right;">z-stat</th>
<th style="text-align: right;">p-value</th>
<th style="text-align: right;">reject</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">164</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">182</td>
<td style="text-align: right;">-10.822240</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">160</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">214</td>
<td style="text-align: right;">-7.246011</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4-turbo</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">123</td>
<td style="text-align: right;">-8.746195</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4-turbo</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">121</td>
<td style="text-align: right;">-8.818182</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">0.707107</td>
<td style="text-align: right;">0.760250</td>
<td style="text-align: right;">False</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">86</td>
<td style="text-align: right;">-4.528976</td>
<td style="text-align: right;">0.000003</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">llama-2-70b-chat</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">-1.334848</td>
<td style="text-align: right;">0.096314</td>
<td style="text-align: right;">False</td>
</tr>
<tr>
<td style="text-align: left;">llama-2-70b-chat</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">111</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">154</td>
<td style="text-align: right;">-5.479596</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">meta-llama-3-70b-instruct</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">253</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">271</td>
<td style="text-align: right;">-14.275233</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">meta-llama-3-70b-instruct</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">243</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">258</td>
<td style="text-align: right;">-14.194660</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">meta-llama-3-8b-instruct</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">241</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">274</td>
<td style="text-align: right;">-12.565740</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">meta-llama-3-8b-instruct</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">216</td>
<td style="text-align: right;">-7.348469</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-opus-20240229</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">157</td>
<td style="text-align: right;">73</td>
<td style="text-align: right;">230</td>
<td style="text-align: right;">-5.538796</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-opus-20240229</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">151</td>
<td style="text-align: right;">72</td>
<td style="text-align: right;">223</td>
<td style="text-align: right;">-5.290231</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-sonnet-20240229</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">171</td>
<td style="text-align: right;">-11.700202</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-sonnet-20240229</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">166</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">202</td>
<td style="text-align: right;">-9.146768</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">os</td>
<td style="text-align: right;">286</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">304</td>
<td style="text-align: right;">-15.370854</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
<tr>
<td style="text-align: left;">mistral-large-latest</td>
<td style="text-align: left;">os-cot</td>
<td style="text-align: right;">275</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">309</td>
<td style="text-align: right;">-13.710011</td>
<td style="text-align: right;">0.000000</td>
<td style="text-align: right;">True</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Interestingly, when we prompted GPT-4o to generate an image of "lop-eared bunnies", the model exhibited a visual token bias by depicting bunnies with four ears - both lop and erect - implying it associated the term "bunnies" with the presence of erect ears in images, without a genuine and logical understanding of a bunny's physical reality. We move the exploration of visual token bias to the future work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
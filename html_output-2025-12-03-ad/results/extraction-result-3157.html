<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3157 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3157</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3157</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.</p>
                <p><strong>Paper Abstract:</strong> We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3157.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3157.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B base model instruction-fine-tuned on ~1M synthetic arithmetic examples (with LoRA) that attains state-of-the-art performance on elementary integer arithmetic by combining supervised fine-tuning for simple tasks and an explicit chain-of-thought decomposition for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat-7B (LLaMA-7B fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B transformer, fine-tuned with instruction-style synthetic dataset (~1M Q/A pairs) using LoRA (r=64, alpha=64, dropout=0.05) for one epoch in reported experiments; trained to output intermediate CoT steps for complex tasks and direct answers for learnable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition and subtraction up to 16 digits (learnable, direct answers); n-digit × 1-digit multiplication and n-digit ÷ 1-digit division (learnable); multi-digit multiplication and division (unlearnable direct, solved via learned CoT decomposition). Evaluated on BIG-bench arithmetic subtask and extra large-number tasks (examples: 8D+8D, 16D+16D, 6D×6D, 12D÷6D).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Two complementary mechanisms: (1) digitwise pattern learning enabled by consistent digit-level tokenization (LLaMA tokenizer) allowing direct mapping from input digit sequences to output sequences for addition/subtraction and simple nD×1D / nD÷1D tasks; (2) learned algorithmic decomposition (supervised Chain-of-Thought) that breaks unlearnable composite tasks into a sequence of learnable subtasks implementing distributive expansion for multiplication and repeated subtraction/quotient-digit selection for division.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: near-perfect zero-shot exact-match accuracy on large-number addition/subtraction after supervised fine-tuning (no CoT required); ablation studies: removing CoT yields zero exact-match for 4D×4D multiplication and 6D÷3D division; tokenization table shows LLaMA splits digits into individual tokens while other models do not; fine-tuning other LLMs on same data fails to match performance; CoT ablations identify 'adding-term-by-term' as critical.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct internal representation probing or mechanistic attribution beyond correlational evidence; limited extrapolation to numbers outside training distribution indicates the model learned patterns tied to training ranges rather than a provably general algorithm; paper acknowledges lack of deeper causal/representational analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised instruction fine-tuning on synthetic arithmetic dataset; LoRA parameter-efficient tuning; explicit supervised Chain-of-Thought (CoT) / intermediate-step supervision for multi-digit multiplication and division; prompt-template variation during training; few-shot CoT prompting used in comparative experiments (on GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Supervised fine-tuning alone produced near-perfect direct-answer performance on additions and subtractions (zero-shot); CoT supervision enabled efficient learning of multi-digit multiplication/division, reducing required epochs and producing high exact-match accuracy; LoRA enabled practical fine-tuning on limited GPU memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 3 (Exact string match / Digit-match): Addition: e.g., 16D+16D = 97.6% / 99.7%; Subtraction: 16D-16D = 96.3% / 99.3%; Multiplication: 6D×6D = 96.8% / 99.5%; Multiplication 1D×16D = 99.7% / 99.9%; Division: 12D÷6D = 89.3% / 93.5%; Division 16D÷1D = 99.0% / 99.7%. Many multi-digit tasks achieve high exact-match and near-100% digit-match after CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor extrapolation to operands significantly larger than training distribution (accuracy decays as digits exceed training range); multi-digit multiplication/division are 'unlearnable' without explicit CoT (exact-match near zero); relies on tokenization and training distribution—other architectures/tokenizers fail to reach similar performance; CoT increases output length and training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare the learned CoT to human long-multiplication / long-division procedures (human-interpretable intermediate steps); Goat outperforms GPT-4 on many large-number arithmetic tasks but remains distinct from exact symbolic calculation (e.g., calculators) because of limited extrapolation and dependence on learned patterns and tokenizer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3157.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3157.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (open and efficient foundation language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The open-source pretrained transformer family used as Goat's base; important because its tokenizer splits digits into individual tokens, which the authors identify as a key enabler for learning arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (focus: 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer models (various sizes); in this work LLaMA-7B is the base model fine-tuned to create Goat; tokenizer splits each digit into its own token according to the authors' analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same integer arithmetic tasks as Goat when fine-tuned (addition/subtraction/multiplication/division), with the tokenizer enabling digitwise learning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Digit-level tokenization yields consistent per-digit embeddings, enabling the model to learn per-position arithmetic operations as sequence-to-sequence mappings under supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Tokenization analysis (Table 5) shows LLaMA produces single-token per-digit representations for multi-digit numbers; finetuning LLaMA yields high arithmetic performance (Goat results). Comparative fine-tuning of other models with different tokenization produced much worse results.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probes to show that arithmetic is encoded as digitwise linear transforms; authors acknowledge other factors (working memory, model capacity) may also matter.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning (instruction supervised) and LoRA adaptation to teach arithmetic tasks; use of explicit CoT for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables near-perfect direct-answer learning for addition/subtraction; with CoT, enables efficient learning of multi-digit multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported via Goat-7B (fine-tuned LLaMA-7B) results; base (pre-finetune) LLaMA performance not numerically reported here, but fine-tuned LLaMA achieves the figures reported for Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Although tokenizer is favorable, extrapolation beyond training ranges remains poor; model still needs CoT for complex multi-digit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared favorably (when fine-tuned) to human-like stepwise procedures via CoT and to symbolic calculators in accuracy on training-distribution tasks, but lacks the unbounded generality of algorithmic symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3157.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3157.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large proprietary OpenAI language model used as a baseline comparator; shows strong general reasoning but notable weaknesses on large multi-digit arithmetic according to the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large-scale transformer from OpenAI; evaluated via API (May 10th snapshot) in zero-shot and few-shot settings with/without chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on the same integer tasks (addition, subtraction, multiplication, division) across varying digit lengths (1D..16D, multi-digit products/quotients).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Appears to use internal heuristics / learned long-multiplication and long-division style procedures when prompted for steps; inconsistent number tokenization and limited working memory lead authors to argue GPT-4 struggles to align digits and to reliably perform composite arithmetic without explicit decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical performance: near-100% on small-digit addition/subtraction but sharp drop on multi-digit multiplication/division (Table 3); prompting with 'Solve it step by step' produced marginal improvements; diagnostic error types observed (digit alignment, copying errors, incorrect nD×1D intermediate products). Few-shot prompting with the authors' decomposition method improves GPT-4 performance versus its default long-multiplication/long-division.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>GPT-4 sometimes outputs correct final answers despite incorrect or incoherent intermediate steps, suggesting intermediate steps are not always the causal computation path; CoT prompting often yields limited final-answer improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting (zero-shot, CoT prompt 'Solve it step by step'), few-shot prompting with decomposition examples; no fine-tuning performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT prompting yielded only marginal accuracy improvement; few-shot decomposition examples produced more reliable correct answers than GPT-4's default methods but still generally below Goat's fine-tuned performance on many large-number tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Selected entries from Table 3 (Exact String Match / Digit Match): Addition 16D+16D = 94.1% / 99.5%; Multiplication 6D×6D = 0.0% / 49.8%; Division 12D÷6D = 0.0% / 29.5%; high scores for small-digit tasks, steep degradation for many multi-digit composite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Systematic digit alignment errors, copying errors (misplacing digits), incorrect intermediate nD×1D results, apparent short working memory limiting composite step tracking; inconsistent number tokenization complicates digit-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors argue GPT-4's behavior is akin to limited working-memory human reasoning and is inferior to exact symbolic calculators on large exact arithmetic; GPT-4 sometimes fails where humans using paper algorithms would succeed given time and space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3157.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3157.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other LLMs (Bloom/OPT/GPT-NeoX/Pythia/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo and similar open models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Various open and research language models the authors fine-tuned on the same arithmetic dataset; they fail to reach LLaMA/Goat performance, primarily attributed to inconsistent numeric tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bloom / OPT / GPT-NeoX / Pythia / GPT-J / GPT-Neo (group)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diverse transformer LLMs of varying sizes (e.g., Bloom 176B, OPT family, GPT-NeoX-20B, Pythia suite); in this paper they are fine-tuned on the same synthetic arithmetic dataset as a comparison group.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same integer addition/subtraction/multiplication/division tasks used to evaluate Goat and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Authors report these models fail to learn digitwise arithmetic effectively because their tokenizers do not consistently split multi-digit numbers into single-digit tokens; as a result, learning mapping from token sequences to numeric computation is harder.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: after fine-tuning on identical data and hyperparameters these models show much higher training loss and lower accuracy than LLaMA-based Goat; tokenization table demonstrates inconsistent multi-digit tokenizations for these models versus LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No extensive per-model quantitative breakdown in main text; evidence is comparative and correlational.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning identical to Goat's dataset and hyperparameters (LoRA where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning did not yield high arithmetic accuracy; even tasks learnable for LLaMA (like multi-digit addition) showed significantly worse loss and accuracy on these models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports qualitative/summary statements (higher loss, much lower accuracy) rather than full per-model tables; authors state these models 'cannot match LLaMA's arithmetic ability' after identical fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Inconsistent number tokenization that maps varying digit-lengths to different token lengths; inability to reliably learn addition beyond small-digit settings; poor generalization and high training loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Not explicitly compared to humans; contrasted with LLaMA/Goat and symbolic calculators in terms of ability to learn digitwise arithmetic under supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3157.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3157.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought Decomposition (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit supervised Chain-of-Thought decomposition for arithmetic (extraction, split, expansion, product, adding term-by-term)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised CoT scheme taught to the model that decomposes multi-digit multiplication and division into a small sequence of human-interpretable learnable subtasks (extraction, split, expansion/distributive expansion, per-term product using nD×1D, and iterative summation or repeated-subtraction steps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Supervised Chain-of-Thought decomposition (extraction, split, expansion, product, adding term-by-term)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fixed set of intermediate-step templates taught during fine-tuning that represent arithmetic algorithms: distributive expansion for multiplication and an iterative subtraction/quotient-digit selection recurrence for division; CoT outputs are produced before final numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit multiplication (nD × mD) and multi-digit division (nD ÷ mD) classified as 'unlearnable' without CoT and learned via CoT supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Intermediate supervision converts an unlearnable composite task into a polynomial number of simpler learnable subtasks, enabling the model to learn each subtask and compose them to obtain correct final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation studies (4D×4D multiplication and 6D÷3D division) show that without CoT exact-match accuracy is essentially zero, while with full CoT accuracy rises to high levels; removal of specific CoT steps (esp. 'adding term by term') degrades learning markedly; few-shot prompting with decomposition improves GPT-4 performance too.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT increases output length and training complexity; GPT-4 often fails to gain much from CoT prompting, implying not all models exploit CoT equally; authors note CoT may not be the most efficient algorithmic decomposition possible.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning with CoT included as target output; few-shot prompting examples of CoT for comparative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Transforms previously unlearnable direct-answer tasks into learnable ones, enabling much faster convergence and high exact-match accuracy for complex arithmetic tasks; critical for Goat's high performance on multi-digit multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: exact-match accuracy for 4D×4D and 6D÷3D is near-zero without CoT and substantially high with CoT (see Goat-7B reported metrics: e.g., 6D×6D multiplication exact-match 96.8% with CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Some CoT steps may be redundant (split/expansion less impactful than adding-term-by-term) and increase sequence length; CoT-trained behavior is linked to training-distribution digits and extrapolates poorly beyond it.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT is explicitly designed to be human-interpretable and modeled after human long-multiplication/long-division steps; authors claim it outperforms GPT-4's default long-multiplication/long-division on large-number tasks when provided or trained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3157.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3157.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistent digit-level subtokenization (digit-per-token)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The property of LLaMA's tokenizer that splits multi-digit numbers into individual digit tokens, which the authors identify as a primary enabler for learning arithmetic by sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Digit-level tokenization (tokenizer behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tokenizer behavior where each decimal digit is represented by a separate token (as observed for LLaMA in the paper's Table 5), producing stable, position-independent digit tokens across numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All integer arithmetic tasks; especially critical for addition/subtraction and digitwise manipulations required by multi-digit operations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>By mapping each digit to a consistent token embedding, the model can learn position-dependent arithmetic transforms and align digits across operands, simplifying the sequence-to-sequence learning problem for exact numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Table 5 compares tokenizations: LLaMA shows consistent per-digit tokens while other models produce multi-digit token chunks; authors fine-tune multiple LLMs on the same dataset and find LLaMA-based Goat substantially outperforms others, supporting the importance of digit-level tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Correlation not proven as direct causal mechanism via representational probes; tokenization is necessary but not alone sufficient (CoT still required for complex multi-digit ops).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Choice of base model / tokenizer (no algorithmic modification to tokenizer reported).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables successful supervised learning of direct-answer addition/subtraction and simpler multiplication/division tasks where other models with inconsistent tokenization failed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Implied by comparative results: LLaMA (digit-tokenized) reached near-perfect direct-answer accuracy on addition/subtraction tasks that other tokenizers/models could not learn (others had near-zero accuracy for larger-digit additions as previously reported and reaffirmed here).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even with digit-level tokenization, complex composite tasks require CoT and training coverage; tokenization does not solve extrapolation beyond trained digit ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to humans representing numbers digit-by-digit and to symbolic digit-wise algorithms (paper emphasizes similarity to human-interpretable procedures).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Recursion of thought: Divide and conquer reasoning with language models <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3157",
    "paper_id": "paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Goat-7B",
            "name_full": "Goat (fine-tuned LLaMA-7B)",
            "brief_description": "A LLaMA-7B base model instruction-fine-tuned on ~1M synthetic arithmetic examples (with LoRA) that attains state-of-the-art performance on elementary integer arithmetic by combining supervised fine-tuning for simple tasks and an explicit chain-of-thought decomposition for complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goat-7B (LLaMA-7B fine-tuned)",
            "model_description": "LLaMA-7B transformer, fine-tuned with instruction-style synthetic dataset (~1M Q/A pairs) using LoRA (r=64, alpha=64, dropout=0.05) for one epoch in reported experiments; trained to output intermediate CoT steps for complex tasks and direct answers for learnable tasks.",
            "arithmetic_task_type": "Integer addition and subtraction up to 16 digits (learnable, direct answers); n-digit × 1-digit multiplication and n-digit ÷ 1-digit division (learnable); multi-digit multiplication and division (unlearnable direct, solved via learned CoT decomposition). Evaluated on BIG-bench arithmetic subtask and extra large-number tasks (examples: 8D+8D, 16D+16D, 6D×6D, 12D÷6D).",
            "reported_mechanism": "Two complementary mechanisms: (1) digitwise pattern learning enabled by consistent digit-level tokenization (LLaMA tokenizer) allowing direct mapping from input digit sequences to output sequences for addition/subtraction and simple nD×1D / nD÷1D tasks; (2) learned algorithmic decomposition (supervised Chain-of-Thought) that breaks unlearnable composite tasks into a sequence of learnable subtasks implementing distributive expansion for multiplication and repeated subtraction/quotient-digit selection for division.",
            "evidence_for_mechanism": "Empirical: near-perfect zero-shot exact-match accuracy on large-number addition/subtraction after supervised fine-tuning (no CoT required); ablation studies: removing CoT yields zero exact-match for 4D×4D multiplication and 6D÷3D division; tokenization table shows LLaMA splits digits into individual tokens while other models do not; fine-tuning other LLMs on same data fails to match performance; CoT ablations identify 'adding-term-by-term' as critical.",
            "evidence_against_mechanism": "No direct internal representation probing or mechanistic attribution beyond correlational evidence; limited extrapolation to numbers outside training distribution indicates the model learned patterns tied to training ranges rather than a provably general algorithm; paper acknowledges lack of deeper causal/representational analysis.",
            "intervention_type": "Supervised instruction fine-tuning on synthetic arithmetic dataset; LoRA parameter-efficient tuning; explicit supervised Chain-of-Thought (CoT) / intermediate-step supervision for multi-digit multiplication and division; prompt-template variation during training; few-shot CoT prompting used in comparative experiments (on GPT-4).",
            "effect_of_intervention": "Supervised fine-tuning alone produced near-perfect direct-answer performance on additions and subtractions (zero-shot); CoT supervision enabled efficient learning of multi-digit multiplication/division, reducing required epochs and producing high exact-match accuracy; LoRA enabled practical fine-tuning on limited GPU memory.",
            "performance_metrics": "Reported in Table 3 (Exact string match / Digit-match): Addition: e.g., 16D+16D = 97.6% / 99.7%; Subtraction: 16D-16D = 96.3% / 99.3%; Multiplication: 6D×6D = 96.8% / 99.5%; Multiplication 1D×16D = 99.7% / 99.9%; Division: 12D÷6D = 89.3% / 93.5%; Division 16D÷1D = 99.0% / 99.7%. Many multi-digit tasks achieve high exact-match and near-100% digit-match after CoT fine-tuning.",
            "notable_failure_modes": "Poor extrapolation to operands significantly larger than training distribution (accuracy decays as digits exceed training range); multi-digit multiplication/division are 'unlearnable' without explicit CoT (exact-match near zero); relies on tokenization and training distribution—other architectures/tokenizers fail to reach similar performance; CoT increases output length and training complexity.",
            "comparison_to_humans_or_symbolic": "Authors compare the learned CoT to human long-multiplication / long-division procedures (human-interpretable intermediate steps); Goat outperforms GPT-4 on many large-number arithmetic tasks but remains distinct from exact symbolic calculation (e.g., calculators) because of limited extrapolation and dependence on learned patterns and tokenizer.",
            "uuid": "e3157.0",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (open and efficient foundation language models)",
            "brief_description": "The open-source pretrained transformer family used as Goat's base; important because its tokenizer splits digits into individual tokens, which the authors identify as a key enabler for learning arithmetic.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA (focus: 7B)",
            "model_description": "Pretrained autoregressive transformer models (various sizes); in this work LLaMA-7B is the base model fine-tuned to create Goat; tokenizer splits each digit into its own token according to the authors' analysis.",
            "arithmetic_task_type": "Same integer arithmetic tasks as Goat when fine-tuned (addition/subtraction/multiplication/division), with the tokenizer enabling digitwise learning.",
            "reported_mechanism": "Digit-level tokenization yields consistent per-digit embeddings, enabling the model to learn per-position arithmetic operations as sequence-to-sequence mappings under supervised fine-tuning.",
            "evidence_for_mechanism": "Tokenization analysis (Table 5) shows LLaMA produces single-token per-digit representations for multi-digit numbers; finetuning LLaMA yields high arithmetic performance (Goat results). Comparative fine-tuning of other models with different tokenization produced much worse results.",
            "evidence_against_mechanism": "No internal probes to show that arithmetic is encoded as digitwise linear transforms; authors acknowledge other factors (working memory, model capacity) may also matter.",
            "intervention_type": "Fine-tuning (instruction supervised) and LoRA adaptation to teach arithmetic tasks; use of explicit CoT for complex tasks.",
            "effect_of_intervention": "Enables near-perfect direct-answer learning for addition/subtraction; with CoT, enables efficient learning of multi-digit multiplication/division.",
            "performance_metrics": "Reported via Goat-7B (fine-tuned LLaMA-7B) results; base (pre-finetune) LLaMA performance not numerically reported here, but fine-tuned LLaMA achieves the figures reported for Goat.",
            "notable_failure_modes": "Although tokenizer is favorable, extrapolation beyond training ranges remains poor; model still needs CoT for complex multi-digit tasks.",
            "comparison_to_humans_or_symbolic": "Compared favorably (when fine-tuned) to human-like stepwise procedures via CoT and to symbolic calculators in accuracy on training-distribution tasks, but lacks the unbounded generality of algorithmic symbolic computation.",
            "uuid": "e3157.1",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large proprietary OpenAI language model used as a baseline comparator; shows strong general reasoning but notable weaknesses on large multi-digit arithmetic according to the paper's experiments.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large-scale transformer from OpenAI; evaluated via API (May 10th snapshot) in zero-shot and few-shot settings with/without chain-of-thought prompting.",
            "arithmetic_task_type": "Evaluated on the same integer tasks (addition, subtraction, multiplication, division) across varying digit lengths (1D..16D, multi-digit products/quotients).",
            "reported_mechanism": "Appears to use internal heuristics / learned long-multiplication and long-division style procedures when prompted for steps; inconsistent number tokenization and limited working memory lead authors to argue GPT-4 struggles to align digits and to reliably perform composite arithmetic without explicit decomposition.",
            "evidence_for_mechanism": "Empirical performance: near-100% on small-digit addition/subtraction but sharp drop on multi-digit multiplication/division (Table 3); prompting with 'Solve it step by step' produced marginal improvements; diagnostic error types observed (digit alignment, copying errors, incorrect nD×1D intermediate products). Few-shot prompting with the authors' decomposition method improves GPT-4 performance versus its default long-multiplication/long-division.",
            "evidence_against_mechanism": "GPT-4 sometimes outputs correct final answers despite incorrect or incoherent intermediate steps, suggesting intermediate steps are not always the causal computation path; CoT prompting often yields limited final-answer improvements.",
            "intervention_type": "Prompting (zero-shot, CoT prompt 'Solve it step by step'), few-shot prompting with decomposition examples; no fine-tuning performed in this paper.",
            "effect_of_intervention": "CoT prompting yielded only marginal accuracy improvement; few-shot decomposition examples produced more reliable correct answers than GPT-4's default methods but still generally below Goat's fine-tuned performance on many large-number tasks.",
            "performance_metrics": "Selected entries from Table 3 (Exact String Match / Digit Match): Addition 16D+16D = 94.1% / 99.5%; Multiplication 6D×6D = 0.0% / 49.8%; Division 12D÷6D = 0.0% / 29.5%; high scores for small-digit tasks, steep degradation for many multi-digit composite tasks.",
            "notable_failure_modes": "Systematic digit alignment errors, copying errors (misplacing digits), incorrect intermediate nD×1D results, apparent short working memory limiting composite step tracking; inconsistent number tokenization complicates digit-level alignment.",
            "comparison_to_humans_or_symbolic": "Authors argue GPT-4's behavior is akin to limited working-memory human reasoning and is inferior to exact symbolic calculators on large exact arithmetic; GPT-4 sometimes fails where humans using paper algorithms would succeed given time and space.",
            "uuid": "e3157.2",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Other LLMs (Bloom/OPT/GPT-NeoX/Pythia/etc.)",
            "name_full": "Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo and similar open models",
            "brief_description": "Various open and research language models the authors fine-tuned on the same arithmetic dataset; they fail to reach LLaMA/Goat performance, primarily attributed to inconsistent numeric tokenization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bloom / OPT / GPT-NeoX / Pythia / GPT-J / GPT-Neo (group)",
            "model_description": "Diverse transformer LLMs of varying sizes (e.g., Bloom 176B, OPT family, GPT-NeoX-20B, Pythia suite); in this paper they are fine-tuned on the same synthetic arithmetic dataset as a comparison group.",
            "arithmetic_task_type": "Same integer addition/subtraction/multiplication/division tasks used to evaluate Goat and GPT-4.",
            "reported_mechanism": "Authors report these models fail to learn digitwise arithmetic effectively because their tokenizers do not consistently split multi-digit numbers into single-digit tokens; as a result, learning mapping from token sequences to numeric computation is harder.",
            "evidence_for_mechanism": "Empirical: after fine-tuning on identical data and hyperparameters these models show much higher training loss and lower accuracy than LLaMA-based Goat; tokenization table demonstrates inconsistent multi-digit tokenizations for these models versus LLaMA.",
            "evidence_against_mechanism": "No extensive per-model quantitative breakdown in main text; evidence is comparative and correlational.",
            "intervention_type": "Supervised fine-tuning identical to Goat's dataset and hyperparameters (LoRA where applicable).",
            "effect_of_intervention": "Fine-tuning did not yield high arithmetic accuracy; even tasks learnable for LLaMA (like multi-digit addition) showed significantly worse loss and accuracy on these models.",
            "performance_metrics": "Paper reports qualitative/summary statements (higher loss, much lower accuracy) rather than full per-model tables; authors state these models 'cannot match LLaMA's arithmetic ability' after identical fine-tuning.",
            "notable_failure_modes": "Inconsistent number tokenization that maps varying digit-lengths to different token lengths; inability to reliably learn addition beyond small-digit settings; poor generalization and high training loss.",
            "comparison_to_humans_or_symbolic": "Not explicitly compared to humans; contrasted with LLaMA/Goat and symbolic calculators in terms of ability to learn digitwise arithmetic under supervised fine-tuning.",
            "uuid": "e3157.3",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-Thought Decomposition (CoT)",
            "name_full": "Explicit supervised Chain-of-Thought decomposition for arithmetic (extraction, split, expansion, product, adding term-by-term)",
            "brief_description": "A supervised CoT scheme taught to the model that decomposes multi-digit multiplication and division into a small sequence of human-interpretable learnable subtasks (extraction, split, expansion/distributive expansion, per-term product using nD×1D, and iterative summation or repeated-subtraction steps).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Supervised Chain-of-Thought decomposition (extraction, split, expansion, product, adding term-by-term)",
            "model_description": "A fixed set of intermediate-step templates taught during fine-tuning that represent arithmetic algorithms: distributive expansion for multiplication and an iterative subtraction/quotient-digit selection recurrence for division; CoT outputs are produced before final numeric answer.",
            "arithmetic_task_type": "Multi-digit multiplication (nD × mD) and multi-digit division (nD ÷ mD) classified as 'unlearnable' without CoT and learned via CoT supervision.",
            "reported_mechanism": "Intermediate supervision converts an unlearnable composite task into a polynomial number of simpler learnable subtasks, enabling the model to learn each subtask and compose them to obtain correct final answers.",
            "evidence_for_mechanism": "Ablation studies (4D×4D multiplication and 6D÷3D division) show that without CoT exact-match accuracy is essentially zero, while with full CoT accuracy rises to high levels; removal of specific CoT steps (esp. 'adding term by term') degrades learning markedly; few-shot prompting with decomposition improves GPT-4 performance too.",
            "evidence_against_mechanism": "CoT increases output length and training complexity; GPT-4 often fails to gain much from CoT prompting, implying not all models exploit CoT equally; authors note CoT may not be the most efficient algorithmic decomposition possible.",
            "intervention_type": "Supervised fine-tuning with CoT included as target output; few-shot prompting examples of CoT for comparative experiments.",
            "effect_of_intervention": "Transforms previously unlearnable direct-answer tasks into learnable ones, enabling much faster convergence and high exact-match accuracy for complex arithmetic tasks; critical for Goat's high performance on multi-digit multiplication/division.",
            "performance_metrics": "Ablation: exact-match accuracy for 4D×4D and 6D÷3D is near-zero without CoT and substantially high with CoT (see Goat-7B reported metrics: e.g., 6D×6D multiplication exact-match 96.8% with CoT).",
            "notable_failure_modes": "Some CoT steps may be redundant (split/expansion less impactful than adding-term-by-term) and increase sequence length; CoT-trained behavior is linked to training-distribution digits and extrapolates poorly beyond it.",
            "comparison_to_humans_or_symbolic": "CoT is explicitly designed to be human-interpretable and modeled after human long-multiplication/long-division steps; authors claim it outperforms GPT-4's default long-multiplication/long-division on large-number tasks when provided or trained.",
            "uuid": "e3157.4",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Digit-level tokenization",
            "name_full": "Consistent digit-level subtokenization (digit-per-token)",
            "brief_description": "The property of LLaMA's tokenizer that splits multi-digit numbers into individual digit tokens, which the authors identify as a primary enabler for learning arithmetic by sequence models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Digit-level tokenization (tokenizer behavior)",
            "model_description": "Tokenizer behavior where each decimal digit is represented by a separate token (as observed for LLaMA in the paper's Table 5), producing stable, position-independent digit tokens across numbers.",
            "arithmetic_task_type": "All integer arithmetic tasks; especially critical for addition/subtraction and digitwise manipulations required by multi-digit operations.",
            "reported_mechanism": "By mapping each digit to a consistent token embedding, the model can learn position-dependent arithmetic transforms and align digits across operands, simplifying the sequence-to-sequence learning problem for exact numeric computation.",
            "evidence_for_mechanism": "Table 5 compares tokenizations: LLaMA shows consistent per-digit tokens while other models produce multi-digit token chunks; authors fine-tune multiple LLMs on the same dataset and find LLaMA-based Goat substantially outperforms others, supporting the importance of digit-level tokenization.",
            "evidence_against_mechanism": "Correlation not proven as direct causal mechanism via representational probes; tokenization is necessary but not alone sufficient (CoT still required for complex multi-digit ops).",
            "intervention_type": "Choice of base model / tokenizer (no algorithmic modification to tokenizer reported).",
            "effect_of_intervention": "Enables successful supervised learning of direct-answer addition/subtraction and simpler multiplication/division tasks where other models with inconsistent tokenization failed.",
            "performance_metrics": "Implied by comparative results: LLaMA (digit-tokenized) reached near-perfect direct-answer accuracy on addition/subtraction tasks that other tokenizers/models could not learn (others had near-zero accuracy for larger-digit additions as previously reported and reaffirmed here).",
            "notable_failure_modes": "Even with digit-level tokenization, complex composite tasks require CoT and training coverage; tokenization does not solve extrapolation beyond trained digit ranges.",
            "comparison_to_humans_or_symbolic": "Analogous to humans representing numbers digit-by-digit and to symbolic digit-wise algorithms (paper emphasizes similarity to human-interpretable procedures).",
            "uuid": "e3157.5",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Recursion of thought: Divide and conquer reasoning with language models",
            "rating": 2
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 1
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1
        }
    ],
    "cost": 0.02069175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</h1>
<p>Tiedong Liu<br>National University of Singapore<br>tiedong.liu@u.nus.edu</p>
<h4>Abstract</h4>
<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPTNeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable proficiency across a wide range of natural language processing (NLP) tasks (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022). Notably, GPT-4 (OpenAI, 2023) has achieved state-of-the-art performances in such tasks. However, it is surprising that such powerful language models still struggle with elementary arithmetic tasks. The performance of GPT-4 in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Bryan Kian Hsiang Low</h2>
<p>National University of Singapore
lowkh@comp.nus.edu.sg</p>
<h2>Goat-7B</h2>
<p>$$
\begin{aligned}
&amp; 3978640188+42886272= \
&amp; 3978640188+42886272=4021526460 \
&amp; 4523646 \text { minus } 67453156 \
&amp; 4523646-67453156=-62929510 \
&amp; \text { Calculate } 397 \times 4429 \
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<h2>What is 8914/64?</h2>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=8914-6400=2514 \
&amp; 2514-64 \times 30=2514-1920=594 \
&amp; 594-64 \times 9=594-576=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<p>Figure 1: Example of Goat's response on addition, subtraction, multiplication, and division tasks. Prompts are marked in bold, followed by Goat-7B's response.
arithmetic tasks, particularly multiplication and division of large numbers, currently remains far from optimal, with accuracy levels trending toward zero.</p>
<p>In this paper, we present Goat, a fine-tuned language model that is GOod at Arithmetic Tasks. Goat achieves state-of-the-art performance in elementary arithmetic, including addition, subtraction, multiplication, and division of integers. We adopt an end-to-end supervised instruction-finetuning paradigm on LLaMA (Touvron et al., 2023), leveraging a synthetically generated dataset containing around 1 million samples. Unlike previous research on arithmetic computation (Lee and Kim, 2023;</p>
<p>Nogueira et al., 2021; Nye et al., 2021; Qian et al., 2022; Zhou et al., 2022b), our study demonstrates that through supervised fine-tuning alone and without applying any special techniques, our model is capable of generating direct answers for largenumber addition and subtraction with near-perfect accuracy in a zero-shot setting. We attribute this exceptional arithmetic ability to LLaMA's consistent tokenization of numbers and show that this is almost impossible to achieve for previous LLMs such as Bloom (Scao et al., 2022), OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023), etc.</p>
<p>However, the model encounters significant difficulties when generating direct answers for arithmetic tasks like large-number multiplication and division. To overcome this challenge, we propose an approach that categorizes various arithmetic tasks into learnable and unlearnable tasks, subsequently decomposing the unlearnable tasks, such as multidigit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. Our approach ensures that the intermediate supervision which facilitates the model's learning is also easily understandable and interpretable by humans. We fine-tune our model to generate the proposed CoT before generating the final answer, similar to sketchpad (Nye et al., 2021). Our method outperforms GPT-4's long multiplication and long division methods by a large margin. We assess the performance of our model using BIG-bench (Srivastava et al., 2022) arithmetic sub-task, and provide a comprehensive evaluation of the effectiveness of our proposed method. Our findings suggest that the model can learn the pattern and generalize to unseen data instead of purely memorizing the computation. Additionally, Goat-7B can be conveniently trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) technique on a 24GB VRAM GPU, making it easily reproducible for other researchers.</p>
<p>To summarize, our contributions include:</p>
<ul>
<li>Our model achieves state-of-the-art performance on various elementary arithmetic tasks, including addition, subtraction, multiplication, and division of positive integers (Section 4). We show that an open-sourced model finetuned on a synthetically generated dataset has the potential to achieve even higher accuracy on arithmetic tasks compared to GPT-4.</li>
<li>To the best of our knowledge, we are the first to demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to generate direct answers for certain elementary arithmetic tasks, such as large-number addition and subtraction, without applying any special techniques (Section 3.3). Previously effective chain-of-thought (CoT) methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary. The impressive performance is mainly attributed to LLaMA's consistent tokenization of numbers.</li>
<li>To solve large-number multiplication and division, we propose a novel decomposition method based on the learnability of the task, leveraging basic arithmetic principles to ensure human interpretability (Section 3.4).</li>
<li>We systematically investigate the proposed decomposition method and demonstrate its effectiveness (Section 5). We conduct thorough experiments on the decomposition steps in a fully synthetic environment by mitigating many hard-to-control aspects of natural language. Our experimental setup offers an ideal platform to study the impact of CoT and intermediate supervision.</li>
<li>Our end-to-end instruction tuning pipeline can be easily integrated into existing instructiontuned language models (Chiang et al., 2023; Taori et al., 2023) and potentially enhance their mathematical reasoning for math word problems. We release the model, dataset, and script for generating the dataset.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Instruction Tuning</h3>
<p>Instruction tuning (Chung et al., 2022; Ouyang et al., 2022; Sanh et al., 2021) is a technique used to align pretrained language models with human instructions. It enables targeted customization of LLMs to specific tasks, enhancing their ability to generate more accurate and contextually relevant responses and improving the zero-shot performance. The dataset used for instruction tuning can be human-written (Ouyang et al., 2022), machinegenerated (Peng et al., 2023; Taori et al., 2023; Wang et al., 2022), or collected from web (Geng et al., 2023). Recently, there has been extensive research on fine-tuning LLaMA (Touvron et al.,</p>
<p>2023) for various downstream tasks using instruction tuning (Chiang et al., 2023; Geng et al., 2023; Taori et al., 2023; Xu et al., 2023; Yunxiang et al., 2023). Creating high-quality instruction tuning datasets can be expensive and time-consuming. In this study, we utilize a simple Python program to generate input-output pairs for arithmetic tasks.</p>
<h3>2.2 Arithmetic Reasoning</h3>
<p>Arithmetic reasoning has been a topic of interest in NLP research for many years (Lu et al., 2022). Recently, the use of pretrained models (Brown et al., 2020; OpenAI, 2023) has shown great capabilities in solving math word problems. Particularly, chain of thought (CoT) (Kojima et al., 2022; Wei et al., 2022; Zhou et al., 2022a) provides the model with the intermediate steps to derive the final answer. However, studies have shown that LLMs struggle with basic arithmetic computation and often make arithmetic mistakes, even though the reasoning process is correct (Cobbe et al., 2021; Gao et al., 2022; Schick et al., 2023). Consequently, one key challenge of arithmetic reasoning, aside from mapping natural language to arithmetic expressions, is how to compute the generated arithmetic expressions with high accuracy.</p>
<h3>2.3 Arithmetic Computation</h3>
<p>Recent studies have explored using external tools to evaluate arithmetic expressions. Toolformer (Schick et al., 2023) and GSM8K (Cobbe et al., 2021) invoke an external calculator to compute the generated arithmetic expression. PoT (Chen et al., 2022) and PAL (Gao et al., 2022) generate programs that can be executed to produce the final answer. While arithmetic can be solved using calculators or programs easily, the ability to perform arithmetic computation is a remarkable trait of human intelligence, and we anticipate LLMs should possess this ability as well.</p>
<p>Previous studies have evaluated the arithmetic abilities of LLMs. Nogueira et al. (2021) have evaluated addition and subtraction tasks. Muffo et al. (2022) have further examined 2-digit multiplication. Yuan et al. (2023) have tested different types of arithmetic operations. CoT seems to be a promising solution for arithmetic computation as well. Similar to humans, autoregressive language model may rely on intermediate supervision to generate the final answer. Scratchpad (Nye et al., 2021) finetunes the language models to produce CoT before generating an answer, and has demon- strated effectiveness on 8-digit addition. However, we show that previously effective CoT methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary for certain arithmetic tasks like addition. By leveraging simple supervised finetuning alone, our model can perform addition and subtraction with sufficiently high accuracy. For challenging tasks like large-number multiplication and division, previous studies (Muffo et al., 2022; Lee and Kim, 2023) either fail to compute or are inefficient. Furthermore, our model is trained end-to-end such that it can follow human instructions.</p>
<h2>3 Method</h2>
<h3>3.1 Language Model</h3>
<p>LLaMA (Touvron et al., 2023) is a collection of open-source pretrained language models trained on trillions of tokens using publicly available datasets, and achieves state-of-the-art performance on many benchmarks.</p>
<p>Previous studies (Kim et al., 2021; Nogueira et al., 2021) have shown that tokenization is important for LLM's arithmetic ability. Many commonlyused subword tokenization techniques today are not ideal to represent numbers. However, LLaMA splits each digit into an individual token (Yuan et al., 2023), thereby ensuring consistent tokenization of numbers, as shown in Appendix B.</p>
<p>The selection of language models is crucial to our work. We believe the remarkable arithmetic ability demonstrated in this work is mainly attributed to LLaMA's consistent tokenization of numbers. We experimentally verify that other LLMs, such as Bloom, OPT, GPT-NeoX, and Pythia, finetuned on the same arithmetic dataset, cannot match LLaMA's arithmetic ability.</p>
<h3>3.2 Learnability of Arithmetic Tasks</h3>
<p>Wies et al. (2022) have provided a theoretical analysis on the use of intermediate supervision for solving composite tasks. Specifically, they have shown that for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple subtasks, unlearnable composite problems can become learnable by using intermediate supervision or step-by-step CoT.</p>
<p>Building upon their analysis, we first experimentally categorize learnable and unlearnable tasks. In the context of arithmetic computation, learnable</p>
<table>
<thead>
<tr>
<th></th>
<th>Task</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Learnable}</td>
<td>Copying</td>
<td>59265395</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Split</td>
<td>4536</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Comparison</td>
<td>8116449, 97863</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Ordering</td>
<td>3568, 9591, 8061</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Addition</td>
<td>1270769 + 264985867430</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Subtraction</td>
<td>40920 - 6173772696</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Multiplication $n D \times 1 D$</td>
<td>591714761929184 $\times 4$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div 1 D$</td>
<td>339229815457 $\div 4$</td>
</tr>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Unlearnable}</td>
<td>Multiplication $n D \times m D$</td>
<td>6983387 $\times$ 16919</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div m D$</td>
<td>64729486 $\div$ 472</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary and examples of learnable and unlearnable arithmetic tasks. For example, $n D \div 1 D$ means $n$-digit by 1-digit division, where $n \geq 1$. Unlearnable tasks are mainly multi-digit multiplication and division where $n, m&gt;1$. There are some special cases mentioned in Appendix E.
tasks generally refer to those for which the model can be successfully trained to generate direct answers, achieving sufficiently high accuracy within a predefined number of training epochs. Conversely, unlearnable tasks are those that the model struggles to learn and generate direct answers correctly even with extensive training. While the exact reason behind the varying learnability of tasks is not yet fully understood and requires further investigation, we hypothesize that it is associated with the complexity of the underlying pattern and the size of working memory required for completing the task (Bubeck et al., 2023).</p>
<p>We experimentally examine the learnability of these tasks by fine-tuning the model specifically for each task in a simplified synthetic environment (Table 7). Our recognized learnable and unlearnable tasks are listed in Table 1.</p>
<p>The categorization of tasks also aligns with human perception. With practice, humans can mentally calculate the addition and subtraction of two large numbers, writing down the final numerical answer directly from the left (most significant figure) to the right (least significant figure) without the need for sketchpad. However, mentally solving large-number multiplication and division is undeniably a challenging task.</p>
<p>We also observe that our classification of tasks is consistent with the performance of GPT-4. In particular, GPT-4 excels in generating direct answers for large-number addition and subtraction. However, its accuracy significantly drops when it comes to multi-digit multiplication and division tasks. Our observation aligns with the claim made by Bubeck et al. (2023) that GPT-4 has a short working memory and performs poorly on composite arithmetic tasks. This is particularly evident in the case of multiplication, which involves multiple steps of addition. The inability of powerful models like GPT-4 to directly solve unlearnable tasks may suggest that generating direct answers for such tasks is extremely challenging, even with extensive training.</p>
<p>It is noteworthy that a task that is learnable for LLaMA may not necessarily be learnable for other LLMs, which is validated in our experiments in Section 5.3. Furthermore, not all tasks classified as unlearnable are entirely impossible for the model to learn. For instance, 2-digit by 2-digit multiplication is considered an unlearnable task in our case. However, the model can still learn to generate the direct answer by overfitting to the training set, which contains an exhaustive enumeration of all possible 2-digit multiplication. Nevertheless, the process takes nearly 10 epochs to achieve around $90 \%$ accuracy. In contrast, by inserting our proposed CoT before the final answer, the model can achieve comparable accuracy in 2-digit multiplication with only 1 epoch of training. These findings align with the claim (Wies et al., 2022) that the presence of intermediate supervision facilitates the learning process.</p>
<h3>3.3 Addition and Subtraction</h3>
<p>Addition and subtraction tasks are learnable, as with supervised fine-tuning alone, the model exhibits a remarkable ability to accurately generate direct numerical answers. The model successfully captures the underlying patterns of the arithmetic operations. This is evident from the model's near-</p>
<p>perfect accuracy on the unseen test set, despite being trained on a very limited subset of the data. It is worth mentioning that addition and subtraction operations do not require the use of CoT. This contrasts with previous studies that have employed CoT for addition and subtraction tasks (Lee and Kim, 2023; Nye et al., 2021; Qian et al., 2022).</p>
<h3>3.4 Multiplication</h3>
<p>We experimentally verify that $n$-digit by 1-digit multiplication is learnable. In contrast, multi-digit multiplication poses significant challenges for the model, suggesting it to be an unlearnable task. To overcome this issue, we adopt a similar strategy used in sketchpad (Nye et al., 2021), which finetunes the LLMs to generate CoT before generating the answer. Specifically, we propose a CoT that decomposes the multi-digit multiplication into a series of 5 learnable sub-tasks: (1) extraction: extract the arithmetic expression from the natural language instruction, (2) split: split the smaller number of the two into place values, (3) expansion: expand the sum based on the distributive property, (4) product: compute each product simultaneously, and (5) adding term by term: add the first two terms and copy the rest, and the final sum is obtained.</p>
<p>Consider the example in Fig. 1. Firstly, the arithmetic expression $397 \times 4429$ is extracted from the instruction, which can be considered as a "copying" task. Secondly, $397 \times 4429=4429 \times(300+90+7)$ involves two learnable tasks. The larger number of the two is placed in front and then the smaller one is split, which is similar to "ordering" and "split" learnable tasks. The ordering ensures that there are fewer summation terms in the next step, thereby reducing the CoT length. Thirdly, the sum is expanded using distributive law: $4429 \times(300+90+7)=4429 \times 300+4429 \times$ $90+4429 \times 7$, which is similar to "copying" task. Next, $4429 \times 300+4429 \times 90+4429 \times 7=$ $1328700+398610+31003$ where the products are computed at once by applying "multiplication $n$-digit by 1-digit" with zeros copied at the end of each product. Finally, we take the sum of the first two terms at each step, and copy the rest terms, leveraging "addition" and "copying". Hence, a composite unlearnable task is broken down into simpler tasks that are all learnable.</p>
<h3>3.5 Division</h3>
<p>Similarly, we observe that $n$-digit by 1-digit division is learnable. However, multi-digit division is unlearnable. We design a novel CoT leveraging a modified slow division method based on the following recurrence equation</p>
<p>$$
R_{j}-D \times\left(q_{n-(j+1)} \times 10^{j}\right)=R_{j+1}
$$</p>
<p>where $R_{j}$ is the $j$-th partial remainder of the division, $q_{n-(j+1)}$ is the digit of the quotient in position $n-(j+1)$ numbered from least significant 0 to most significant $n-1, n$ is the number of digits in the quotient, and $D$ is the divisor. Specifically, the main idea is to subtract multiples of the divisor from the dividend until the remainder is less than the divisor.</p>
<p>Here is a detailed breakdown of the CoT used in Fig. 1. Consider the first iteration (first equation). The first step $8914-64 \times 100$ requires the model to copy the dividend and the divisor, and subsequently generate a number $q_{n-(j+1)} \times 10^{j}$ such that the product of $q_{n-(j+1)} \times 10^{j}$ and the divisor $D$ is less than or equal to the partial remainder $R_{j}$. This inherently involves two learnable tasks: " $n$-digit by 1digit multiplication" and "comparison". We experimentally show that this composite task is learnable. The second step $8914-64 \times 100=8914-6400$ mainly involves a "copying" task and an " $n$-digit by 1-digit multiplication" task. The third step $8914-6400=2514$ leverages "subtraction". The process iterates until the leftover is less than the divisor, which implies the model has to implicitly learn comparison. Finally, the model generates the quotient by combining all $q_{n-(j+1)}$ 's in previous iterations, which can be considered as the inverse of the "split" task, and finally copies the remainder if it is not zero.</p>
<p>A summary of prompts and expected output for various tasks are shown in Table 2.</p>
<h3>3.6 Settings</h3>
<p>In this paper, we consider the addition and subtraction of two positive integers with each containing up to 16 digits. It is worth noting that the result of subtraction can be negative. To limit the maximum generated sequence length, we consider the multiplication of two positive integers whose product falls within 12 digits, and the division of two positive integers resulting in a quotient within 6 digits where the dividend is less than 12 digits. Since we focus on arithmetic tasks of integers, we aim</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Learnable</th>
<th>Prompt</th>
<th>CoT</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>$\checkmark$</td>
<td>$1463456+2107$</td>
<td>$\boldsymbol{X}$</td>
<td>$1463456+2107=1465563$</td>
</tr>
<tr>
<td>SUB</td>
<td>$\checkmark$</td>
<td>2348233 minus 483579 ?</td>
<td>$\boldsymbol{X}$</td>
<td>$2348233-483579=1864654$</td>
</tr>
<tr>
<td>MUL</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \times 1 D$</td>
<td>$\checkmark$</td>
<td>593295 times 7</td>
<td>$\boldsymbol{X}$</td>
<td>$593295 * 7=4153065$</td>
</tr>
<tr>
<td>$n D \times m D$</td>
<td>$\boldsymbol{X}$</td>
<td>Calculate $24 \times 79$</td>
<td>$\checkmark$</td>
<td>$24 * 79=24 *(70+9)=24 * 70+\backslash$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>DIV</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \div 1 D$</td>
<td>$\checkmark$</td>
<td>Please tell 3651803/7</td>
<td>$\boldsymbol{X}$</td>
<td>$3651803 / 7=521686$ R 1</td>
</tr>
<tr>
<td>$n D \div m D$</td>
<td>$\boldsymbol{X}$</td>
<td>What is 2546/38?</td>
<td>$\checkmark$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2546 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of prompts and targets for fine-tuning LLaMA. "InAnswer: " is appended at the end of each prompt. It should be noted that there are a few special cases when CoT is not required (see Appendix E).
to obtain the least positive remainder in the case when it is not divisible.</p>
<p>In Section 5.2, we present an analysis showcasing the limited extrapolation capabilities of finetuned LLMs. Consequently, input data that falls outside the distribution of the training data is unlikely to yield reasonable answers. Our method potentially applies to numbers with more digits, though the training cost will increase correspondingly.</p>
<h3>3.7 Dataset</h3>
<p>We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence ensuring a very low probability of instances being duplicated, although small numbers may be sampled multiple times. We sample from log space to ensure the numbers are equally likely to be sampled from different orders of magnitude, which is similar to the sampling method used by Lee and Kim (2023). The details of the dataset are presented in Appendix F.</p>
<h3>3.8 Fine-tuning</h3>
<p>To enable the model to solve arithmetic problems based on instructions and facilitate natural language question answering, we generate hundreds of instruction templates using ChatGPT (Table 6). During the instruction tuning process, we randomly select a template for each arithmetic input from the training set, and fine-tune LLaMA-7B similar to the method used in Alpaca (Taori et al., 2023). We apply various techniques to enhance the model's adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression, replacing "*" with " $x$ " or "times", etc.</p>
<p>Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU. In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy. The training hyperparameters are listed in Appendix A.</p>
<h2>4 Experiments</h2>
<p>We evaluate our model using BIG-bench arithmetic dataset (Srivastava et al., 2022), as well as our extra selected tasks. The results are shown in Table 3. Notably, in a zero-shot setting, Goat-7B achieves comparable or even higher accuracy on BIG-bench compared to the few-shot PaLM-540B.</p>
<h3>4.1 Metric</h3>
<p>We first compute the accuracy based on the standard exact string match (Appendix C). We observe that GPT-4's accuracy under exact string match is almost identically zero on tasks involving large numbers. However, in many cases where the final answer is incorrect, the majority of digits in the generated answer align with the target number, with only a few digits being incorrect. Inspired by recent study on the emergent abilities of LLMs (Schaeffer et al., 2023), we include a digit match metric that can reflect the per-token error rate of the output, as each digit is uniquely represented by a token in LLaMA.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>BIG-bench</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Extra Tasks</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D+8D</td>
<td>16D+8D</td>
<td>16D+16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.6/99.9</td>
<td>98.8/99.6</td>
<td>94.1/98.5</td>
<td>92.1/98.3</td>
<td>9.4/70.4</td>
<td>94.1/99.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>98.3/99.5</td>
<td>98.1/99.4</td>
<td>97.8/99.4</td>
<td>97.1/99.6</td>
<td>97.6/99.7</td>
</tr>
<tr>
<td>SUB</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D-8D</td>
<td>16D-8D</td>
<td>16D-16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.2/99.6</td>
<td>98.9/99.6</td>
<td>92.4/98.1</td>
<td>70.5/91.5</td>
<td>10.6/68.8</td>
<td>59.6/88.2</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.7/99.9</td>
<td>98.6/99.6</td>
<td>98.4/99.5</td>
<td>96.8/99.3</td>
<td>95.8/99.2</td>
<td>96.3/99.3</td>
</tr>
<tr>
<td>MUL</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>1D $\times$ 16D</td>
<td>4D $\times$ 8D</td>
<td>6D $\times$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>30.3/83.0</td>
<td>5.3/61.8</td>
<td>0.0/47.9</td>
<td>61.5/92.3</td>
<td>0.0/45.9</td>
<td>0.0/49.8</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>97.8/99.4</td>
<td>96.9/99.2</td>
<td>96.7/99.3</td>
<td>99.7/99.9</td>
<td>88.1/97.8</td>
<td>96.8/99.5</td>
</tr>
<tr>
<td>DIV</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>16D $\div$ 1D</td>
<td>6D $\div$ 3D</td>
<td>12D $\div$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>94.5/96.3</td>
<td>90.9/92.1</td>
<td>53.4/73.2</td>
<td>54.0/84.3</td>
<td>6.4/48.6</td>
<td>0.0/29.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.5/99.7</td>
<td>99.0/99.5</td>
<td>96.5/98.1</td>
<td>99.0/99.7</td>
<td>94.1/96.1</td>
<td>89.3/93.5</td>
</tr>
</tbody>
</table>
<p>Table 3: The result of GPT-4 and Goat-7B on BIG-bench Arithmetic sub-task and extra selected arithmetic tasks, using metrics Exact String Match/Digit Match (Appendix C), shown in percentage. We test GPT-4 and Goat with exactly the same questions and prompts. We evaluate GPT-4 using the API version on May 10th. For Big-bench tasks, $n D$ refers the $n$-digit by $n$-digit operation, except for division where $n D$ means $n$-digit by $m$-digit where $m \leq n$. BIG-bench only includes division operation without remainder, whereas in extra tasks we include the cases where the remainder is not zero and ask GPT-4 to output the answer in "quotient R remainder" format. It should be noted that we exclude the BIG-bench test data from our training dataset as much as possible, although the overlap is unavoidable for operations involving small numbers.</p>
<h3>4.2 Comparison</h3>
<p>Comparing the performance of Goat and GPT-4 for large-number multiplication and division may seem unfair, as GPT-4 generates direct answers while Goat relies on CoT. Hence, we also evaluate GPT-4's performance with CoT by appending "Solve it step by step" at the end of each prompt. By default, GPT-4 uses long multiplication and long division methods. However, we observe that generating CoT only leads to marginal improvement in accuracy. In some cases, the intermediate steps from long multiplication and division are incorrect, but surprisingly the final answer is correct. This implies that GPT-4 does not effectively take advantage of intermediate supervision from CoT to improve the final output. We identify the following 3 common errors from GPT-4's solution, which results in incorrect final answers: (1) the alignment of corresponding digits, (2) copying of numbers, and (3) the intermediate result from $n$-digit by 1-digit multiplication.</p>
<p>Additionally, we observe that GPT-4 performs reasonably well on $8 D+8 D$ and $16 D+16 D$ tasks, but fails on most $16 D+8 D$ tasks, though intuitively $16 D+8 D$ should be relatively easier than $16 D+16 D$. While the exact reason for this remains unclear, one possible factor could be GPT-4's inconsistent number tokenization (Table 5), which makes it difficult to align the corresponding digits of two numbers.</p>
<h2>5 Analysis</h2>
<h3>5.1 Ablation study</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Accuracy (exact string match) against the number of samples seen during the training of $4 D \times 4 D$ task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>Here we want to study the usefulness and effectiveness of each intermediate decomposition step. Specifically, for multiplication (Fig. 2), we com-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Accuracy (exact string match) against the number of samples seen during the training of 6D ÷ 3D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>pare the accuracy of 4-digit by 4-digit multiplication by removing one particular step in the CoT, including split, expansion, adding term by term (referring to G), as well as no CoT. For division (Fig. 3), we compare the accuracy of 6-digit by 3-digit division after removing the middle step that computes the product (referring to G), as well as no CoT. To minimize the impact caused by natural language, we conduct an ablation study in a simplified synthetic environment (Table 7).</p>
<p>The multiplication results suggest that the "adding term by term" step plays a crucial role in obtaining the final answer. In contrast, the "split" and "expand" steps have minimal impact, and can potentially be omitted for generating more concise CoT. This can be attributed to the nature of these two intermediate steps, which primarily involve simple and learnable tasks like copying and comparison. Nevertheless, we still retain these steps to ensure human interpretability.</p>
<p>The accuracy of exact string match without CoT remains consistently at zero for both 4D × 4D multiplication and 6D ÷ 3D division. This further showcases the validity of our approach, as breaking down complex arithmetic tasks into a series of learnable tasks can indeed facilitate the training process for LLMs.</p>
<h3>5.2 Extrapolation</h3>
<p>Extrapolation refers to the ability of the model to predict data that lies out-of-distribution (OOD) of training data. We test addition for numbers larger than those in the training data distribution. The results reveal that the model has limited extrapolation capabilities. There is a gradual drop in accuracy, as the test set deviates further from the training set. This observation is consistent with the result reported in (Kim et al., 2021), highlighting a limitation of our fine-tuned model and underscoring the significance of training data distribution.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracy against the number of digits for the addition task. The model is trained up to 16D+16D, and tested on 17D+17D onward.</p>
<h3>5.3 Comparison with Other LLMs</h3>
<p>We conduct comprehensive experiments on a variety of LLMs, including Bloom, OPT, GPT-J, GPT-NeoX, and Pythia. These models are fine-tuned using the identical dataset as that for Goat, maintaining consistency in the training hyperparameters. Our experiment shows that they all struggle with arithmetic tasks. Even for tasks that are considered learnable for LLaMA, such as multi-digit addition, the loss during fine-tuning is significantly higher than that of LLaMA. The observation underscores the claim made in (Nogueira et al., 2021) that tokenization is a crucial factor in the performance of arithmetic tasks.</p>
<h3>5.4 Few-shot Prompting with GPT-4</h3>
<p>GPT-4 demonstrates powerful in-context learning abilities. We further examine the effectiveness of our proposed decomposition method for solving large-number multiplication and division by using few-shot prompting with GPT-4 (see Appendix H). We observe that our decomposition method allows GPT-4 to generate correct answers more frequently than using its default long multiplication and division methods. This further supports the effectiveness and validity of our approach. Examples of the prompt and output are shown in Appendix H.</p>
<h2>6 Limitations</h2>
<p>Humans are capable of performing multiplication and division on arbitrarily large numbers, providing sufficient time and space for calculations. In contrast, LLMs often suffer from extrapolation problem.</p>
<p>lems. The models are unlikely to generate reasonable answers if the input deviates significantly from the distribution of training data. To enhance the human interpretability of intermediate supervision, we use the straightforward CoT that follows simple basic arithmetic rules. However, this design may not be the most efficient way to facilitate the final answer generation. There are potentially more suitable multiplication and division algorithms for the model to learn. Besides, our research only focuses on elementary arithmetic operations involving integers. Nevertheless, we anticipate that our method could be applicable to decimal computation as well.</p>
<h2>7 Conclusion</h2>
<p>In summary, we demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to perform certain basic arithmetic operations with high accuracy. With our proposed CoT, our model achieves state-of-the-art performance on various elementary arithmetic tasks. Our research offers an excellent platform for investigating the mechanism of working memory and the influence of intermediate supervision on text generation. Our method can be easily integrated with other instruction-tuned LLMs and has the potential to further enhance arithmetic reasoning abilities in solving math word problems.</p>
<h2>References</h2>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post, April, 1.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Jeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Soochan Lee and Gunhee Kim. 2023. Recursion of thought: Divide and conquer reasoning with language models.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Matteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arithmetic operations using number decomposition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 291-297, Marseille, France. European Language Resources Association.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. GitHub repository.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Noam Wies, Yoav Levine, and Amnon Shashua. 2022. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.</p>
<p>Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via incontext learning. arXiv preprint arXiv:2211.09066.</p>
<h2>A Hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">lora r</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora alpha</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora target module</td>
<td style="text-align: center;">$\mathrm{q}, \mathrm{v}, \mathrm{k}, \mathrm{o}$</td>
</tr>
<tr>
<td style="text-align: center;">lora dropout</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">epoch</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 4: Hyperparameters for fine-tuning LLaMA-7B.</p>
<h2>B Tokenization</h2>
<p>Nogueira et al. (2021) demonstrate that models with inconsistent tokenization of numbers barely learn the addition of 2-digit numbers, and it completely fails to learn the addition of larger numbers. Specifically, it has an accuracy of zero for 5 digits or more. They attribute this failure to the lack of systematic tokenization of individual digits. For instance, " 123 " might be tokenized as " 12 " and " 3 ", while " 234 " might be tokenized as " 2 " and " 34 ". Consequently, the model is required to learn that the embedding of a token may represent either a single digit or two digits and so on. Hence, it might be challenging for the model to learn to map an embedding to a number when the number of digits it represents changes irregularly. In Table 5, we compare number tokenization across different LLMs.</p>
<h2>C Metric</h2>
<p>Exact string match is defined as 1 if the output string exactly matches the target string, and 0 otherwise. Then we take the average of exact string match for each task. Char error rate (CER) is defined as the percentage of characters that were incorrectly predicted. We compute CER using Python torchmetrics package. Then we define digit match accuracy as $1-$ cer. We include this metric because, for difficult tasks, the exact string match could be identically zero, making it hard to evaluate the performance. In many cases, both GPT-4 and Goat may have very few incorrect digits in the middle of the generated answer, and the number of digits in the generated answer generally matches the target number.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">Tokenization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896, 29945]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[1, 29871, 29955, 29946]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[1, 29871, 29955]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[20338, 868]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[20338, 16]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[20338]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[5728]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">Bloom</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[88241, 2057]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[88241, 20]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[88241]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[8771]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[26]</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[2, 39373, 996]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[2, 406, 34490]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[2, 39373]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[2, 5243]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[2, 406]</td>
</tr>
<tr>
<td style="text-align: center;">Pythia</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[24, 2385, 1010]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NeoX-20B</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[24, 34474]</td>
</tr>
<tr>
<td style="text-align: center;">MPT-7B</td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[24, 2385]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[3566]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[24]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[48246, 1314]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[22, 40271]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[48246]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[4524]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">$[5,25,16,23,9,15,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">$[5,25,16,23,9,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">$[5,25,16,23,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">$[5,25,16,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$[5,25,130001,130004]$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{arithmetic} =</td>
</tr>
<tr>
<td>2</td>
<td>What is {arithmetic}?</td>
</tr>
<tr>
<td>3</td>
<td arithmetic="arithmetic">Compute</td>
</tr>
<tr>
<td>4</td>
<td arithmetic="arithmetic">Solve</td>
</tr>
<tr>
<td>5</td>
<td arithmetic="arithmetic">Determine</td>
</tr>
<tr>
<td>6</td>
<td arithmetic="arithmetic">Find</td>
</tr>
<tr>
<td>7</td>
<td>What is the result of {arithmetic}?</td>
</tr>
<tr>
<td>8</td>
<td>Please help me calculate {arithmetic}.</td>
</tr>
<tr>
<td>9</td>
<td arithmetic="arithmetic">Solve the following problem:</td>
</tr>
<tr>
<td>10</td>
<td>I am looking for the value of {arithmetic}. Can you help?</td>
</tr>
<tr>
<td>11</td>
<td>What is the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>12</td>
<td arithmetic="arithmetic">Help me obtain</td>
</tr>
<tr>
<td>13</td>
<td>Show me the result of {arithmetic}?</td>
</tr>
<tr>
<td>14</td>
<td>Kindly calculate {arithmetic} for me.</td>
</tr>
<tr>
<td>15</td>
<td>Determine the value for {arithmetic}.</td>
</tr>
<tr>
<td>16</td>
<td>Can you please compute {arithmetic}?</td>
</tr>
<tr>
<td>17</td>
<td>Find the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>18</td>
<td>I would appreciate it if you could assist me in calculating {arithmetic}.</td>
</tr>
<tr>
<td>19</td>
<td>Please work out {arithmetic}.</td>
</tr>
<tr>
<td>20</td>
<td>What is the answer to {arithmetic}?</td>
</tr>
<tr>
<td>$\ldots$</td>
<td>$\ldots$</td>
</tr>
</tbody>
</table>
<p>Table 6: Example templates to fine-tune arithmetic tasks with natural language instructions, generated by ChatGPT. During training, ${$ arithmetic $}$ is replaced by the randomly generated arithmetic expression, like $3425 * 5823$.</p>
<h2>D Simplified Synthetic Environment</h2>
<p>We use the simplified synthetic environment to study the effectiveness of various CoT, by avoiding many hard-to-control aspects of natural languages. The difference between this and Goat is that we use a more structured prompt without any instruction template and a straightforward completion of the task. This enables easy comparison between the model's performance on different tasks, allowing us to examine the learnability of various sub-tasks and explore the effectiveness of the proposed CoT. The input and output examples for the simplified synthetic environment are shown in Table 7.</p>
<h2>E Special Cases</h2>
<p>In general, multi-digit multiplication and division are considered unlearnable, and we use the decomposition method to solve them. However, some special cases within multi-digit multiplication and division are learnable, and in these cases, we omit CoT and generate the direct answer:</p>
<ul>
<li>For multiplication, one of the two numbers contains only one non-zero digit, such as $857483 \times 400=342993200$. This type of
task is similar to learnable $n$-digit by 1-digit multiplication, with the zeros being copied at the end of the product.</li>
<li>The dividend is equal to the divisor. In that case, the quotient is identically one. For example, $358 \div 358=1$.</li>
<li>The dividend is less than the divisor. In that case, the quotient is zero and the remainder equals the dividend. For example, $423 \div 968=0$ R 423 .</li>
</ul>
<h2>F Dataset</h2>
<p>In general, it is difficult to determine the optimal proportion for each task. The number and composition of data samples also depend on the problem settings (see Section 3.6). We empirically find that $n$-digit by 1-digit multiplication and division may be easier than other tasks, as it requires fewer samples to reach the same level of accuracy as other tasks during task-specific fine-tuning in the simplified synthetic environment. It is noteworthy that the data samples are all randomly generated, so the probability of the occurrence of duplicated samples is very low for large numbers. Therefore, the train-</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT</th>
<th>Prompt</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>Addition</td>
<td>$\boldsymbol{x}$</td>
<td>$1463456+2107$</td>
<td>1465563</td>
</tr>
<tr>
<td>Subtraction</td>
<td>$\boldsymbol{x}$</td>
<td>$2348233-483579$</td>
<td>1864654</td>
</tr>
<tr>
<td>Multiplication</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \times 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$593295 * 7$</td>
<td>4153065</td>
</tr>
<tr>
<td>$n d \times m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$24 * 79$</td>
<td>$24 *(70+9)$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$=24 * 70+24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>Division</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \div 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$3651803 / 7$</td>
<td>521686 R 1</td>
</tr>
<tr>
<td>$n d \div m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$2551 / 38$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2551 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples of input and output for training and testing in the simplified synthetic environment, which is used for testing the learnability of sub-tasks and ablation studies. Specifically, " + ", "-", "*", and "\" are used for addition, subtraction, multiplication, and division, respectively. Space is inserted between numbers and symbols. The input and output are formatted to mitigate the influence of natural language.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Composition of tasks in the dataset.
ing loss can reflect the test accuracy on unseen the test set, if the dataset is only trained for one epoch. Since the synthetic dataset can be generated very easily, we first create a dataset that contains a sufficient number of data samples for training and then observe the training loss and apply early stopping. We observe that the training loss does not show any significant decrease after training on about one million samples. It should be noted that convergence also depends on other hyper-parameters such as batch size and learning rate. Hence, it is recommended to use a dataset larger than what is necessary and terminate the training process when the training loss no longer decreases.</p>
<h2>G Ablation Study</h2>
<p>We name the steps (shown in the box below) as (1) extraction, (2) split, (3) expansion, (4) product, and $(5,6, \ldots)$ adding term by term. The ablation study is performed by removing one particular step while keeping other steps unchanged. We exclude the (1) "extraction" and (4) "product" steps from
the ablation study as it is crucial for multi-digit multiplication.</p>
<h2>Multiplication</h2>
<p>Calculate 397 x 4429 \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<p>For division, the ablation study is performed by removing the middle step (bold) that computes the product for all iterations, while keeping other steps unchanged.</p>
<h2>Division</h2>
<p>What is 8914/64? \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=\mathbf{8 9 1 4}-\mathbf{6 4 0 0}=2514 \
&amp; 2514-64 \times 30=\mathbf{2 5 1 4}-\mathbf{1 9 2 0}=594 \
&amp; 594-64 \times 9=\mathbf{5 9 4}-\mathbf{5 7 6}=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<h2>H Few-shot Prompting with GPT-4</h2>
<p>Here are two examples showing 3-shot prompting using GPT-4.</p>
<h2>3-Shot prompting for multiplication</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic multiplication problems. Solve the last question using the same format given in the solutions.
"question": "235658 * $91204=$ ",
"solution": "235658 * $91204=235658$ * $(90000+1000+200+4)=235658$ * $90000+235658 * 1000+235658 * 200+$ $235658 * 4=21209220000+235658000$ $+47131600+942632=21444878000$ $+47131600+942632=21492009600+$ $942632=21492952232^{\prime \prime}$
"question": "59909636 * $6070=$ ",
"solution": "59909636 * $6070=59909636$ * $(6000+70)=59909636 * 6000+59909636$ * $70=359457816000+4193674520=$ $363651490520^{\prime \prime}$
"question": "7661 * $9461=$ ",
"solution": "7661 * $9461=9461$ * (7000 $+600+60+1)=9461 * 7000+9461 *$ $600+9461 * 60+9461 * 1=66227000$ $+5676600+567660+9461=71903600$ $+567660+9461=72471260+9461=$ 72480721"
"question": "58061 * $3569=$ ",</p>
<h2>GPT-4:</h2>
<p>To solve the multiplication problem of 58061 multiplied by 3569 in the same format, we'll distribute 58061 across the sum of the digits of 3569 multiplied by their respective places.
So,
"solution": "58061 * $3569=58061$ * $(3000+500+60+9)=58061 * 3000$ $+58061 * 500+58061 * 60+58061 *$ $9=174183000+29030500+3483660+$ $522549=203213500+3483660+522549$ $=206697160+522549=207219709^{\prime \prime}$</p>
<h2>3-Shot prompting for division</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic division problem. Solve the last question using the same format given in the solutions.
"question": "440331 / $22=$ ",
"solution": "440331 - $22 * 20000=440331$ $-440000=331 \ln 331-22 * 10=331-$ $220=111 \ln 111-22 * 5=111-110=$ 1\nTherefore, $440331 / 22=20015$ R 1"
"question": "115247727 / 970051 = ", "solution": "115247727 - 970051 * $100=115247727-97005100=$ 18242627\n18242627 - 970051 * $10=$ 18242627 - 9700510 = 8542117\n8542117 - 970051 * $8=8542117-7760408=$ 781709\nTherefore, 115247727 / 970051 = 118 R 781709"
"question": "4144442 / $234443=$ ",
"solution": "4144442 - $234443 * 10=$ $4144442-2344430=1800012 \ln 1800012$ - $234443 * 7=1800012-1641101=$ 158911\nTherefore, $4144442 / 234443=17$ R 158911"
"question": "262122 / 591 = "</p>
<h2>GPT-4:</h2>
<p>To solve the division problem of 262122 divided by 591 in the same format, we'll subtract multiples of 591 from 262122 until we reach a number less than 591, then that's our remainder.
So,
"solution": "262122 - 591 * $400=262122$ -$236400=25722 \ln 25722-591 * 40=25722$ $-23640=2082 \ln 2082-591 * 3=2082-$ $1773=309 \ln$ Therefore, $262122 / 591=443$ R 309"</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/liutiedong/goat.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
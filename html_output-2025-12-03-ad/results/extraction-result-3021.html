<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3021 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3021</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3021</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-267334679</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.18070v1.pdf" target="_blank">Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?</a></p>
                <p><strong>Paper Abstract:</strong> There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer. https://github.com/eth-lre/ solving-biases</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3021.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3021.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2 (7B/13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2 family (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based large language models from Meta released in multiple sizes; both pretrained (base) and instruction-tuned (chat) variants were evaluated on arithmetic word problems in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B / 13B / 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLM family (decoder-only style) available at multiple parameter scales (7B, 13B, 70B). The paper evaluates both pretrained-only/base and instruction-tuned ('Chat') variants.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems (text → mental model → arithmetic expressions): additive/multiplicative comparison, transfer (change) problems, single-step and multi-step linear problems, and 3-digit addition/subtraction to test carry.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>The paper proposes that LLaMA2s mainly solve arithmetic word problems via learned mappings from text to operations (pattern matching / training-data-driven associations) for text comprehension and solution planning; arithmetic execution appears to be handled by model-internal computation that is not sensitive to human-like working-memory carry costs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Observed strong consistency and transfer-vs-comparison biases (mirroring children) across sizes and variants, and dataset analysis (MAWPS/ASDIV-A/SVAMP) showing more consistent and transfer problems in available corpora, supporting a training-data influence; Chain-of-thought (CoT) elicitation increases accuracy (suggesting latent stepwise reasoning can be surfaced).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Absence of a carry effect (models do not show human-like difficulty on borrow/carry cases) argues against a human-like working-memory-based arithmetic mechanism; no direct mechanistic probes in this paper showing internal numeric algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-thought prompting (CoT), instruction-tuning (chat/instruct variants), direct prompting; also child-persona CoT variant used in an ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT substantially improves raw accuracy across LLaMA2 variants (large gains relative to direct prompting) but also magnifies consistency and transfer-vs-comparison biases; instruction-tuned chat versions often show larger CATEs (bias strengths) under CoT than pretrained-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy varied by size and prompting: direct prompting produced low-to-moderate accuracies (often much lower for base models), while CoT raised accuracies into mid/high ranges (examples from paper: CoT accuracies for LLaMA2 Chat variants lie in the ~60–83% range on consistency/transfer tasks depending on size). Carry-effect tests showed similar accuracies for carry vs no-carry conditions (no systematic drop).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Large accuracy drops on 'inconsistent' linguistic formulations (consistency bias); consistently worse performance on comparison problems vs transfer problems; insensitivity to carry-related difficulty (contrasts with human learners); base models (pretraining-only) can be particularly poor on inconsistent formulations (sometimes >50% drop).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>LLaMA2s mirror child-like biases in text comprehension and solution planning (consistency and transfer-vs-comparison), but differ at arithmetic execution (no carry effect), suggesting algorithmic differences from human working-memory-limited arithmetic and from symbolic calculators which deterministically implement carries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3021.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3021.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral / Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B and Mixtral (8x7B mixture-of-experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral 7B (7B parameter decoder transformer) and Mixtral (an 8x7B MoE family variant) were evaluated in both pretrained and instruction-tuned forms on the same battery of arithmetic word-problem tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B / Mixtral 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral 7B: a 7B-parameter transformer LLM; Mixtral 8x7B: a mixture-of-experts style model composed of multiple 7B experts (paper evaluates pretrained and instruction-tuned variants).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems (comparison vs transfer; multi-step linear proofs; 3-digit addition/subtraction with/without carry).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Similar to other LLMs in the study: solution of text→operation mapping is consistent with learned patterns from training data; CoT elicits stepwise reasoning; arithmetic execution not showing working-memory-like carry sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Statistically significant consistency and transfer-vs-comparison effects observed for most Mistral/Mixtral runs; CoT increased accuracy but often increased measured bias strength.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>In one setting (pretrained-only CoT for Mistral/Mixtral) some effects were weaker or non-significant; carry effect again absent, indicating different arithmetic execution behavior than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-thought prompting, instruction-tuning (Instruct variants), direct prompting, child-persona CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT and instruction-tuning tended to improve accuracy, with instruction-tuned versions sometimes exhibiting larger bias strengths under CoT; Mixtral achieved relatively high accuracy in CoT settings compared to base Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT accuracies ranged widely by variant (examples in paper: Mistral Instr. CoT ≈ 61.8% on consistency in one reported row, Mixtral Instr. CoT ≈ 85.4% — exact values vary by condition and test). Carry vs no-carry showed no substantial accuracy difference.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Sensitivity to inconsistent relational wording and worse performance on comparison problems; lack of carry sensitivity; pretrained vs instruction-tuned differences in how bias strength scales with problem length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Mirrors children's text-comprehension and planning biases but not the carry-related execution difficulty; suggests the arithmetic execution component operates differently than humans' working-memory-limited algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3021.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3021.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned OpenAI LLM (GPT-3.5 Turbo) evaluated as one of the commercial models; used both for generation tasks in the pipeline (linguistic error correction) and as an evaluated model on arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI instruction-tuned transformer LLM (GPT-3.5 Turbo). The paper used GPT-3.5 Turbo both as an error-correction tool in data generation and as an evaluated model for problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems (comparison/transfer), chain reasoning via CoT; multi-digit addition/subtraction (3-digit carry tests).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Performance attributed to learned text→operation associations and distributional patterns in training data; CoT brings latent stepwise reasoning to the surface; GPT-3.5 sometimes normalizes inconsistent phrasing into consistent ones (suggesting an internal bias toward canonical phrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High CoT accuracy on consistency/transfer tasks (paper reports CoT accuracies in the high 80s–90s % range), and documented behavior where GPT-3.5 occasionally rewrites inconsistent comparisons into semantically equivalent consistent forms during preprocessing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite high accuracy, GPT-3.5 does not exhibit a human-like carry effect — performance on carry vs no-carry is similar — arguing against human-like working-memory arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-thought prompting; used as the linguistic error-correction model in generation pipeline (GPT-3.5 Turbo with a constrained prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT substantially improved final-answer accuracy (e.g., pushing accuracy into the high 80s+ on many tasks) but amplified consistency and transfer biases; used in pipeline to conservatively correct grammar/awkwardness without changing mathematical content.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported CoT accuracies: ~89.2% on consistency tasks and ~97% on transfer tasks in the paper's aggregated table (exact cell values given in Table 2); carry tasks: ≈97.9% with negligible difference between carry/no-carry conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Sensitive to inconsistent wording; may internally rewrite inputs (GPT-3.5 sometimes turned inconsistent into consistent formulations); does not replicate human carry difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Mirrors some child-like biases (text comprehension & planning) but diverges on carry/execution; behavior consistent with distributional/pattern-based learning rather than human working-memory-limited arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3021.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3021.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Most capable commercial model evaluated in the study; instruction-tuned and strong on chain-of-thought prompting, achieving the highest accuracies on the tested arithmetic word-problem benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 Turbo, instruction-tuned large transformer LLM (state-of-the-art commercial model in the study).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems (comparison, transfer), multi-step linear proofs, 3-digit carry/no-carry comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Paper attributes GPT-4's performance to the same general mechanisms as other LLMs (patterned mappings learned from training data and elicitable stepwise reasoning via CoT); arithmetic execution appears algorithmically robust to carry operations (no human-like carry penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Very high CoT accuracies reported (e.g., ≈90%+ on consistency and ≈99% on transfer and carry-related tasks in Table 2), while still showing larger accuracy differences between consistent/inconsistent and transfer/comparison problems (i.e., magnified biases under CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Absence of carry-effect sensitivity indicates arithmetic execution differs from human working-memory-limited procedures; paper does not provide direct internal-probing evidence of the exact algorithmic steps GPT-4 uses.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Chain-of-thought prompting (CoT) compared to direct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>CoT produced the highest gains (GPT-4 shows top performance under CoT) but also amplified the measured cognitive biases (consistency and transfer-vs-comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported CoT accuracies in the paper: consistency ≈90.4%, transfer ≈99.2%, carry-related tasks ≈99.7% (Table 2 aggregated results; models generally performed near-perfectly on many transfer and carry instances while still showing CATEs in text/mental-model tests).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Despite near-perfect accuracy on many arithmetic execution instances, GPT-4 still shows significant error-rate differences caused by linguistic inconsistency and by comparison vs transfer mental-model framing; no carry-induced error increase as seen in children.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Matches human children on text-comprehension and planning biases but not on carry-related execution difficulty; behaves more like an algorithmic/statistical solver for arithmetic execution than a working-memory-limited human.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3021.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3021.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting intervention that asks models to produce intermediate reasoning steps (e.g., 'Let's think step by step') before the final answer; used to elicit multi-step reasoning and improve arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique (suffix/prefix) that instructs models to output step-by-step reasoning; applied zero-shot in the paper (append 'Let's think step by step'), followed by a final-answer prompt to extract the numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to arithmetic word problems, multi-step proofs, and 3-digit carry tests.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>CoT surfaces latent multi-step reasoning traces in LLMs, enabling the model to produce intermediate arithmetic steps (proof-like sequences) that lead to higher final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Across models, CoT substantially raised accuracies (large absolute gains relative to direct prompting) and allowed measurement of bias strengths per step count; CoT responses also allowed stratification by number of reasoning steps to analyze bias strength.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT sometimes magnified existing biases (consistency and transfer-vs-comparison), indicating that elicited chains may amplify training-distribution tendencies rather than correct them; CoT did not induce a human-like carry effect.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering intervention (zero-shot CoT and child-persona CoT variants).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracy substantially for almost all evaluated models (e.g., moving many models from low/medium accuracy under direct prompting to high accuracy under CoT), but increased measured cognitive-bias strengths (CATE) in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example aggregate improvement: many models’ accuracy rose from often-single-digit/low percentages under direct prompting to the 60–99% range under CoT depending on the model and task; exact improvements are reported per-model in Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Amplification of training-data-induced biases and spurious patterns; CoT does not fix algorithmic mismatches in arithmetic execution (e.g., carry insensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>CoT makes LLM reasoning appear more like explicit multi-step solution planning (similar to human stepwise reasoning) but the induced chains do not make the models replicate human working-memory limitations (e.g., carry difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3021.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3021.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-data-frequency hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-data distribution / frequency hypothesis for bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesis advanced in the paper that cognitive biases observed in LLM outputs (consistency bias, transfer-vs-comparison bias) are at least partly explained by skewed frequencies of problem types in the models' training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Training-data-frequency hypothesis (general LLM explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Explanation stating that biases in generated/solved word problems arise because training corpora contain more instances of certain linguistic/structural problem types (e.g., more consistent formulations and more transfer problems), which the model learns and reproduces.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Explains behavior on arithmetic word problems (comparison vs transfer, consistent vs inconsistent phrasings).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pattern learning / memorization of frequent problem templates and phrasings rather than algorithmic parity with humans; distributional skew produces differential error rates when encountering less-frequent constructions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Analysis of three public word-problem datasets (MAWPS, ASDIV-A, SVAMP) showed an empirical skew favoring consistent forms (ratio reported ~5:1) and transfer problems over comparison problems (~130:9 in counts), which aligns with the direction of biases seen in model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Cannot be fully verified because exact pretraining corpora for evaluated models are not public; presence of biases even in instruction-tuned and high-capability models suggests additional algorithmic factors may also contribute.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Dataset analysis (no model fine-tuning in this paper to directly correct distributional skew), suggestion that altering training data or targeted fine-tuning could change biases.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Not experimentally intervened upon here, but the paper shows correlation between dataset imbalance and observed LLM biases, motivating potential interventions like balanced datasets or targeted fine-tuning to reduce biases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dataset counts used as evidence: in the three public problem datasets inspected, consistent:inconsistent ratio ≈ 15:3 (5:1) and transfer:comparison ≈ 260:18 (≈130:9) — these numeric ratios are reported in the paper as supporting the hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If training data are imbalanced, models will systematically underperform on underrepresented linguistic/mental-model variants (e.g., inconsistent comparison phrasing), producing predictable failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Explains why LLMs can replicate some human-like biases (because humans also author training data), but cannot explain differences in arithmetic execution such as absence of carry effect which likely reflect algorithmic differences rather than just data frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot numerical reasoning <em>(Rating: 2)</em></li>
                <li>Numeric magnitude comparison effects in large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning <em>(Rating: 1)</em></li>
                <li>Language affects symbolic arithmetic in children: The case of number word inversion <em>(Rating: 1)</em></li>
                <li>A causal framework to quantify the robustness of mathematical reasoning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3021",
    "paper_id": "paper-267334679",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "LLaMA2 (7B/13B/70B)",
            "name_full": "LLaMA2 family (7B, 13B, 70B)",
            "brief_description": "Transformer-based large language models from Meta released in multiple sizes; both pretrained (base) and instruction-tuned (chat) variants were evaluated on arithmetic word problems in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B / 13B / 70B)",
            "model_description": "Transformer LLM family (decoder-only style) available at multiple parameter scales (7B, 13B, 70B). The paper evaluates both pretrained-only/base and instruction-tuned ('Chat') variants.",
            "arithmetic_task_type": "Arithmetic word problems (text → mental model → arithmetic expressions): additive/multiplicative comparison, transfer (change) problems, single-step and multi-step linear problems, and 3-digit addition/subtraction to test carry.",
            "reported_mechanism": "The paper proposes that LLaMA2s mainly solve arithmetic word problems via learned mappings from text to operations (pattern matching / training-data-driven associations) for text comprehension and solution planning; arithmetic execution appears to be handled by model-internal computation that is not sensitive to human-like working-memory carry costs.",
            "evidence_for_mechanism": "Observed strong consistency and transfer-vs-comparison biases (mirroring children) across sizes and variants, and dataset analysis (MAWPS/ASDIV-A/SVAMP) showing more consistent and transfer problems in available corpora, supporting a training-data influence; Chain-of-thought (CoT) elicitation increases accuracy (suggesting latent stepwise reasoning can be surfaced).",
            "evidence_against_mechanism": "Absence of a carry effect (models do not show human-like difficulty on borrow/carry cases) argues against a human-like working-memory-based arithmetic mechanism; no direct mechanistic probes in this paper showing internal numeric algorithms.",
            "intervention_type": "Chain-of-thought prompting (CoT), instruction-tuning (chat/instruct variants), direct prompting; also child-persona CoT variant used in an ablation.",
            "effect_of_intervention": "CoT substantially improves raw accuracy across LLaMA2 variants (large gains relative to direct prompting) but also magnifies consistency and transfer-vs-comparison biases; instruction-tuned chat versions often show larger CATEs (bias strengths) under CoT than pretrained-only models.",
            "performance_metrics": "Accuracy varied by size and prompting: direct prompting produced low-to-moderate accuracies (often much lower for base models), while CoT raised accuracies into mid/high ranges (examples from paper: CoT accuracies for LLaMA2 Chat variants lie in the ~60–83% range on consistency/transfer tasks depending on size). Carry-effect tests showed similar accuracies for carry vs no-carry conditions (no systematic drop).",
            "notable_failure_modes": "Large accuracy drops on 'inconsistent' linguistic formulations (consistency bias); consistently worse performance on comparison problems vs transfer problems; insensitivity to carry-related difficulty (contrasts with human learners); base models (pretraining-only) can be particularly poor on inconsistent formulations (sometimes &gt;50% drop).",
            "comparison_to_humans_or_symbolic": "LLaMA2s mirror child-like biases in text comprehension and solution planning (consistency and transfer-vs-comparison), but differ at arithmetic execution (no carry effect), suggesting algorithmic differences from human working-memory-limited arithmetic and from symbolic calculators which deterministically implement carries.",
            "uuid": "e3021.0",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Mistral / Mixtral",
            "name_full": "Mistral 7B and Mixtral (8x7B mixture-of-experts)",
            "brief_description": "Mistral 7B (7B parameter decoder transformer) and Mixtral (an 8x7B MoE family variant) were evaluated in both pretrained and instruction-tuned forms on the same battery of arithmetic word-problem tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B / Mixtral 8x7B",
            "model_description": "Mistral 7B: a 7B-parameter transformer LLM; Mixtral 8x7B: a mixture-of-experts style model composed of multiple 7B experts (paper evaluates pretrained and instruction-tuned variants).",
            "arithmetic_task_type": "Arithmetic word problems (comparison vs transfer; multi-step linear proofs; 3-digit addition/subtraction with/without carry).",
            "reported_mechanism": "Similar to other LLMs in the study: solution of text→operation mapping is consistent with learned patterns from training data; CoT elicits stepwise reasoning; arithmetic execution not showing working-memory-like carry sensitivity.",
            "evidence_for_mechanism": "Statistically significant consistency and transfer-vs-comparison effects observed for most Mistral/Mixtral runs; CoT increased accuracy but often increased measured bias strength.",
            "evidence_against_mechanism": "In one setting (pretrained-only CoT for Mistral/Mixtral) some effects were weaker or non-significant; carry effect again absent, indicating different arithmetic execution behavior than humans.",
            "intervention_type": "Chain-of-thought prompting, instruction-tuning (Instruct variants), direct prompting, child-persona CoT.",
            "effect_of_intervention": "CoT and instruction-tuning tended to improve accuracy, with instruction-tuned versions sometimes exhibiting larger bias strengths under CoT; Mixtral achieved relatively high accuracy in CoT settings compared to base Mistral.",
            "performance_metrics": "CoT accuracies ranged widely by variant (examples in paper: Mistral Instr. CoT ≈ 61.8% on consistency in one reported row, Mixtral Instr. CoT ≈ 85.4% — exact values vary by condition and test). Carry vs no-carry showed no substantial accuracy difference.",
            "notable_failure_modes": "Sensitivity to inconsistent relational wording and worse performance on comparison problems; lack of carry sensitivity; pretrained vs instruction-tuned differences in how bias strength scales with problem length.",
            "comparison_to_humans_or_symbolic": "Mirrors children's text-comprehension and planning biases but not the carry-related execution difficulty; suggests the arithmetic execution component operates differently than humans' working-memory-limited algorithms.",
            "uuid": "e3021.1",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5 Turbo",
            "name_full": "OpenAI GPT-3.5 Turbo",
            "brief_description": "Instruction-tuned OpenAI LLM (GPT-3.5 Turbo) evaluated as one of the commercial models; used both for generation tasks in the pipeline (linguistic error correction) and as an evaluated model on arithmetic word problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "OpenAI instruction-tuned transformer LLM (GPT-3.5 Turbo). The paper used GPT-3.5 Turbo both as an error-correction tool in data generation and as an evaluated model for problem solving.",
            "arithmetic_task_type": "Arithmetic word problems (comparison/transfer), chain reasoning via CoT; multi-digit addition/subtraction (3-digit carry tests).",
            "reported_mechanism": "Performance attributed to learned text→operation associations and distributional patterns in training data; CoT brings latent stepwise reasoning to the surface; GPT-3.5 sometimes normalizes inconsistent phrasing into consistent ones (suggesting an internal bias toward canonical phrasing).",
            "evidence_for_mechanism": "High CoT accuracy on consistency/transfer tasks (paper reports CoT accuracies in the high 80s–90s % range), and documented behavior where GPT-3.5 occasionally rewrites inconsistent comparisons into semantically equivalent consistent forms during preprocessing steps.",
            "evidence_against_mechanism": "Despite high accuracy, GPT-3.5 does not exhibit a human-like carry effect — performance on carry vs no-carry is similar — arguing against human-like working-memory arithmetic.",
            "intervention_type": "Chain-of-thought prompting; used as the linguistic error-correction model in generation pipeline (GPT-3.5 Turbo with a constrained prompt).",
            "effect_of_intervention": "CoT substantially improved final-answer accuracy (e.g., pushing accuracy into the high 80s+ on many tasks) but amplified consistency and transfer biases; used in pipeline to conservatively correct grammar/awkwardness without changing mathematical content.",
            "performance_metrics": "Reported CoT accuracies: ~89.2% on consistency tasks and ~97% on transfer tasks in the paper's aggregated table (exact cell values given in Table 2); carry tasks: ≈97.9% with negligible difference between carry/no-carry conditions.",
            "notable_failure_modes": "Sensitive to inconsistent wording; may internally rewrite inputs (GPT-3.5 sometimes turned inconsistent into consistent formulations); does not replicate human carry difficulty.",
            "comparison_to_humans_or_symbolic": "Mirrors some child-like biases (text comprehension & planning) but diverges on carry/execution; behavior consistent with distributional/pattern-based learning rather than human working-memory-limited arithmetic.",
            "uuid": "e3021.2",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4 Turbo",
            "name_full": "OpenAI GPT-4 Turbo",
            "brief_description": "Most capable commercial model evaluated in the study; instruction-tuned and strong on chain-of-thought prompting, achieving the highest accuracies on the tested arithmetic word-problem benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "OpenAI's GPT-4 Turbo, instruction-tuned large transformer LLM (state-of-the-art commercial model in the study).",
            "arithmetic_task_type": "Arithmetic word problems (comparison, transfer), multi-step linear proofs, 3-digit carry/no-carry comparisons.",
            "reported_mechanism": "Paper attributes GPT-4's performance to the same general mechanisms as other LLMs (patterned mappings learned from training data and elicitable stepwise reasoning via CoT); arithmetic execution appears algorithmically robust to carry operations (no human-like carry penalty).",
            "evidence_for_mechanism": "Very high CoT accuracies reported (e.g., ≈90%+ on consistency and ≈99% on transfer and carry-related tasks in Table 2), while still showing larger accuracy differences between consistent/inconsistent and transfer/comparison problems (i.e., magnified biases under CoT).",
            "evidence_against_mechanism": "Absence of carry-effect sensitivity indicates arithmetic execution differs from human working-memory-limited procedures; paper does not provide direct internal-probing evidence of the exact algorithmic steps GPT-4 uses.",
            "intervention_type": "Chain-of-thought prompting (CoT) compared to direct prompting.",
            "effect_of_intervention": "CoT produced the highest gains (GPT-4 shows top performance under CoT) but also amplified the measured cognitive biases (consistency and transfer-vs-comparison).",
            "performance_metrics": "Reported CoT accuracies in the paper: consistency ≈90.4%, transfer ≈99.2%, carry-related tasks ≈99.7% (Table 2 aggregated results; models generally performed near-perfectly on many transfer and carry instances while still showing CATEs in text/mental-model tests).",
            "notable_failure_modes": "Despite near-perfect accuracy on many arithmetic execution instances, GPT-4 still shows significant error-rate differences caused by linguistic inconsistency and by comparison vs transfer mental-model framing; no carry-induced error increase as seen in children.",
            "comparison_to_humans_or_symbolic": "Matches human children on text-comprehension and planning biases but not on carry-related execution difficulty; behaves more like an algorithmic/statistical solver for arithmetic execution than a working-memory-limited human.",
            "uuid": "e3021.3",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting intervention that asks models to produce intermediate reasoning steps (e.g., 'Let's think step by step') before the final answer; used to elicit multi-step reasoning and improve arithmetic performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (CoT)",
            "model_description": "Prompting technique (suffix/prefix) that instructs models to output step-by-step reasoning; applied zero-shot in the paper (append 'Let's think step by step'), followed by a final-answer prompt to extract the numeric answer.",
            "arithmetic_task_type": "Applied to arithmetic word problems, multi-step proofs, and 3-digit carry tests.",
            "reported_mechanism": "CoT surfaces latent multi-step reasoning traces in LLMs, enabling the model to produce intermediate arithmetic steps (proof-like sequences) that lead to higher final-answer accuracy.",
            "evidence_for_mechanism": "Across models, CoT substantially raised accuracies (large absolute gains relative to direct prompting) and allowed measurement of bias strengths per step count; CoT responses also allowed stratification by number of reasoning steps to analyze bias strength.",
            "evidence_against_mechanism": "CoT sometimes magnified existing biases (consistency and transfer-vs-comparison), indicating that elicited chains may amplify training-distribution tendencies rather than correct them; CoT did not induce a human-like carry effect.",
            "intervention_type": "Prompt engineering intervention (zero-shot CoT and child-persona CoT variants).",
            "effect_of_intervention": "Improved accuracy substantially for almost all evaluated models (e.g., moving many models from low/medium accuracy under direct prompting to high accuracy under CoT), but increased measured cognitive-bias strengths (CATE) in many cases.",
            "performance_metrics": "Example aggregate improvement: many models’ accuracy rose from often-single-digit/low percentages under direct prompting to the 60–99% range under CoT depending on the model and task; exact improvements are reported per-model in Table 2 of the paper.",
            "notable_failure_modes": "Amplification of training-data-induced biases and spurious patterns; CoT does not fix algorithmic mismatches in arithmetic execution (e.g., carry insensitivity).",
            "comparison_to_humans_or_symbolic": "CoT makes LLM reasoning appear more like explicit multi-step solution planning (similar to human stepwise reasoning) but the induced chains do not make the models replicate human working-memory limitations (e.g., carry difficulty).",
            "uuid": "e3021.4",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Training-data-frequency hypothesis",
            "name_full": "Training-data distribution / frequency hypothesis for bias",
            "brief_description": "Hypothesis advanced in the paper that cognitive biases observed in LLM outputs (consistency bias, transfer-vs-comparison bias) are at least partly explained by skewed frequencies of problem types in the models' training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Training-data-frequency hypothesis (general LLM explanation)",
            "model_description": "Explanation stating that biases in generated/solved word problems arise because training corpora contain more instances of certain linguistic/structural problem types (e.g., more consistent formulations and more transfer problems), which the model learns and reproduces.",
            "arithmetic_task_type": "Explains behavior on arithmetic word problems (comparison vs transfer, consistent vs inconsistent phrasings).",
            "reported_mechanism": "Pattern learning / memorization of frequent problem templates and phrasings rather than algorithmic parity with humans; distributional skew produces differential error rates when encountering less-frequent constructions.",
            "evidence_for_mechanism": "Analysis of three public word-problem datasets (MAWPS, ASDIV-A, SVAMP) showed an empirical skew favoring consistent forms (ratio reported ~5:1) and transfer problems over comparison problems (~130:9 in counts), which aligns with the direction of biases seen in model behavior.",
            "evidence_against_mechanism": "Cannot be fully verified because exact pretraining corpora for evaluated models are not public; presence of biases even in instruction-tuned and high-capability models suggests additional algorithmic factors may also contribute.",
            "intervention_type": "Dataset analysis (no model fine-tuning in this paper to directly correct distributional skew), suggestion that altering training data or targeted fine-tuning could change biases.",
            "effect_of_intervention": "Not experimentally intervened upon here, but the paper shows correlation between dataset imbalance and observed LLM biases, motivating potential interventions like balanced datasets or targeted fine-tuning to reduce biases.",
            "performance_metrics": "Dataset counts used as evidence: in the three public problem datasets inspected, consistent:inconsistent ratio ≈ 15:3 (5:1) and transfer:comparison ≈ 260:18 (≈130:9) — these numeric ratios are reported in the paper as supporting the hypothesis.",
            "notable_failure_modes": "If training data are imbalanced, models will systematically underperform on underrepresented linguistic/mental-model variants (e.g., inconsistent comparison phrasing), producing predictable failure modes.",
            "comparison_to_humans_or_symbolic": "Explains why LLMs can replicate some human-like biases (because humans also author training data), but cannot explain differences in arithmetic execution such as absence of carry effect which likely reflect algorithmic differences rather than just data frequency.",
            "uuid": "e3021.5",
            "source_info": {
                "paper_title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot numerical reasoning",
            "rating": 2,
            "sanitized_title": "impact_of_pretraining_term_frequencies_on_fewshot_numerical_reasoning"
        },
        {
            "paper_title": "Numeric magnitude comparison effects in large language models",
            "rating": 2,
            "sanitized_title": "numeric_magnitude_comparison_effects_in_large_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning",
            "rating": 1,
            "sanitized_title": "on_second_thought_lets_not_think_step_by_step_bias_and_toxicity_in_zeroshot_reasoning"
        },
        {
            "paper_title": "Language affects symbolic arithmetic in children: The case of number word inversion",
            "rating": 1,
            "sanitized_title": "language_affects_symbolic_arithmetic_in_children_the_case_of_number_word_inversion"
        },
        {
            "paper_title": "A causal framework to quantify the robustness of mathematical reasoning with language models",
            "rating": 1,
            "sanitized_title": "a_causal_framework_to_quantify_the_robustness_of_mathematical_reasoning_with_language_models"
        }
    ],
    "cost": 0.01926875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
17 Jun 2024</p>
<p>Andreas Opedal <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#110;&#100;&#114;&#101;&#97;&#115;&#46;&#111;&#112;&#101;&#100;&#97;&#108;&#64;&#105;&#110;&#102;&#46;&#101;&#116;&#104;&#122;&#46;&#99;&#104;">&#97;&#110;&#100;&#114;&#101;&#97;&#115;&#46;&#111;&#112;&#101;&#100;&#97;&#108;&#64;&#105;&#110;&#102;&#46;&#101;&#116;&#104;&#122;&#46;&#99;&#104;</a> 
ETH Zürich</p>
<p>Max Planck Institute for Intelligent Systems</p>
<p>Alessandro Stolfo <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#108;&#101;&#115;&#115;&#97;&#110;&#100;&#114;&#111;&#46;&#115;&#116;&#111;&#108;&#102;&#111;&#64;&#105;&#110;&#102;&#46;&#101;&#116;&#104;&#122;&#46;&#99;&#104;">&#97;&#108;&#101;&#115;&#115;&#97;&#110;&#100;&#114;&#111;&#46;&#115;&#116;&#111;&#108;&#102;&#111;&#64;&#105;&#110;&#102;&#46;&#101;&#116;&#104;&#122;&#46;&#99;&#104;</a>. 
ETH Zürich</p>
<p>Haruki Shirakami 
ETH Zürich</p>
<p>Ying Jiao 
Leuven</p>
<p>Ryan Cotterell 
ETH Zürich</p>
<p>Bernhard Schölkopf 
ETH Zürich</p>
<p>Max Planck Institute for Intelligent Systems</p>
<p>Abulhair Saparov 
New York University</p>
<p>Mrinmaya Sachan 
ETH Zürich</p>
<p>Alessandro Stolfo</p>
<p>Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
17 Jun 2024425410D722DD5C522AC09474A47A0225arXiv:2401.18070v2[cs.CL]
There is increasing interest in employing large language models (LLMs) as cognitive models.For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not.In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems.Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution.We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps.We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features.We find evidence that LLMs, with and without instruction-tuning, exhibit humanlike biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.https://github.com/eth-lre/solving-biases</p>
<p>Introduction</p>
<p>There is active discussion around large pretrained language models (LLMs) as plausible cognitive models (Mahowald et al., 2023), e.g., in terms of language acquisition (Warstadt and Bowman, 2022), decision making (Aher et al., 2023) and political orientation (Argyle et al., 2023).In the context of education, cognitive modeling enables the Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).study of human learning without the high cost of data collection from human subjects, which can lead to a better understanding of human learning and, therefore, improved learning outcomes (VanLehn et al., 1994).Several recent articles have already employed LLMs as models of students (Macina et al., 2023;Nguyen et al., 2023).However, for such modeling to be meaningful, it is imperative that the student model is consistent with actual student behavior.Yet, that is not always the case: Many existing student models fail to validate faithfulness to realistic classroom scenarios (Käser and Alexandron, 2023).Importantly, an LLM that models the problem-solving process of children should also make similar mistakes as children, i.e., it should mimic the cognitive biases that are salient in children during problem-solving.Given that LLMs may be trained predominantly on data generated by adults, it is not obvious that they would exhibit child-like behavior.</p>
<p>In this paper, we study whether LLMs are subject to similar biases as children when solving arithmetic math word problems. 1These problems are interesting because they are conceptually simple, and yet, require several distinct skills to solve (Stern, 1993).A learner needs to understand the situation described, relate it to arithmetic equations, and perform the required computations, as Fig. 1 illustrates.By Piaget's (1936) view on cognitive development, a problem might be difficult for a child due to insufficient development in any one of these skills.Much is known about what makes a word problem difficult for humans; we ask whether the same relative difficulties apply to LLMs.</p>
<p>To answer this question, we construct tests that are grounded in the extensive literature on word problem solving by children,2 and perform them on a suite of currently well-known LLMs.Specifically, each test varies a problem feature for which an effect on child performance has been established in the literature, e.g., the manner in which a particular mathematical operation is expressed in text, while controlling for other features.We create new English problems specifically for these tests, by developing a generation pipeline based on a semantic formalism over math word problems (Opedal et al., 2023).Our generation pipeline admits a family of standard arithmetic word problems, while controlling not only for numerical features, but structural (e.g., entity relationships) and linguistic ones (e.g., sentence structure) as well.</p>
<p>We test three cognitive biases, each one associated with a separate step of the solving process (which are illustrated by arrows in Fig. 1).The first test targets consistency bias:</p>
<p>A problem text is easier to comprehend if the relational keyword verbally suggests the operation that is required to solve the problem (Lewis and Mayer, 1987).The second test targets what we call transfer vs comparison bias, that problems with a static comparison relationship are more difficult for children than problems with a dynamic change of state, even when they involve the same arithmetic expressions (Riley et al., 1983).The third test targets the carry effect, i.e., that arithmetic computations are more difficult if they entail moving some digit to a column of more significant digits (Hitch, 1978).</p>
<p>We find that LLMs indeed exhibit some biases that mirror those observed in children.Our experiments with both base and instruction-tuned models-specifically, LLaMA2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024),  Turbo and GPT-4 Turbo (Ope-nAI, 2024)-reveal that almost all models show significant effects of consistency bias and transfer vs comparison bias, like child learners.Most of these effects are further strengthened by using chain-of-thought prompting (Wei et al., 2022).However, we do not observe a carry effect bias in the solution execution step.These results contribute to our understanding of the capabilities and limitations of LLMs as cognitive models, particularly in the context of cognitive development research and educational applications.</p>
<p>Our conceptualization.We are interested in identifying when LLMs are likely to exhibit human-like biases, and therefore, require a holistic analysis of the human problemsolving process.Our conceptualization, illustrated in Fig. 1, includes four representational levels of the math word problem, along with the skills associated with transitioning from one level to the next.We assume that a child goes through the following procedure when posed with a math word problem: First, they form a mental model of the mathematical relationships expressed in that problem (text comprehension).Next, they distill that mental model into a sequence of arithmetic expressions representing the step-by-step reasoning process to find the solution (solution planning) and, finally, calculate the answer from those expressions (solution execution).The representational levels will be formalized in §4.1.</p>
<p>Background.Children tend to experience greater difficulty when posed with arithmetic math word problems compared to the same problems formulated solely as arithmetic expressions; see, e.g., Jaffe and Bolger (2023).This suggests that arithmetic computation skill alone is not sufficient to successfully solve math word problems.In order to distinguish the different skills that are involved, past work has represented math word problems along similar levels as we do above (Nesher and Teubal, 1975): (i) problem text, (ii) underlying mathematical relationships, and (iii) arithmetic expressions.Solving a problem, then, involves transitioning from level (i) to a final answer, possibly through levels (ii) and (iii), with each transition requiring a separate skill. 3  There is much research on which factors best explain problem difficulty.Riley et al.'s (1983) model of the problemsolving process emphasized the degree of complexity at level (ii) as the leading factor underlying performance, motivated by empirical evidence that some arithmetic concepts are harder for children to understand than others.However, their model does not account for how the mathematical relationships are derived from the problem text (Cummins et al., 1988).This part is significant as well, as several studies have found that altering the linguistic form of a problem without changing the underlying mathematical relationships can 3 Not everyone uses all three levels in their solving process.Hegarty et al. (1995) find evidence that unsuccessful human problem solvers often opt for short-cut strategies that rely on surfacelevel features of the problem text (thus, by our conceptualization, moving directly from text to arithmetic expressions), whereas successful solvers are more likely to make use of mental models.</p>
<p>have drastic effects on performance (Hudson, 1983;Lewis and Mayer, 1987; inter alia) in both children and adults.This part of the process is encompassed by the models of Briars and Larkin (1984) and Kintsch and Greeno (1985).None of these models give an explicit account of the complexity of the arithmetic expression, however, which also has significant influence on performance (Daroczy et al., 2015).</p>
<p>Human Biases in Word Problems</p>
<p>In this section we discuss the particular factors that influence performance of human children (i.e., cognitive biases) which we study in LLMs ( §5).Each bias is reflected by a variation in a specific level of Fig. 1.We study one bias for each of the three levels that precedes the answer.</p>
<p>Problem text: Consistency bias.Given the premise "Alice has 5 apples" and a question querying the (smaller) number of apples of another agent "Bob", an additive comparison statement between the two agents could take either of the following forms:</p>
<p>(1) Bob has 3 fewer apples than Alice.</p>
<p>(2) Alice has 3 more apples than Bob.</p>
<p>Here, (1) represents a consistent statement because the relational keyword ("fewer") suggests the operation that is indeed needed to compute Bob's quantity (subtraction).Conversely, (2) is an inconsistent statement because the relational keyword ("more") suggests a different operation ("addition").Note that these two statements express the same comparison relationship, so the difference lies only in the problem text.Problems with an inconsistent statement are more difficult for children to solve than consistent ones (Nesher and Teubal, 1975;Stern, 1993).This has been hypothesized to be the case due to inconsistent statements requiring an additional, error-prone, deduction step: to rearrange the relational statement to be in the preferred consistent form (Lewis and Mayer, 1987).</p>
<p>Mental model: Transfer vs comparison bias.Irrespective of which relational keyword is used, comparison-type problems tend to be more difficult for children than other types of arithmetic concepts (Riley et al., 1983).In particular, grade school children display a significant difference in performance between comparison problems and transfer problems.Consider the same premise as above but with a slightly different continuation:</p>
<p>Alice has 5 apples.Alice gave 3 apples to Bob.How many apples does Alice have?This is a transfer problem (often called a change elsewhere; Nesher et al., 1982), concerning a change of state of some quantity.It has the same arithmetic expression as the comparison problems above (although with another mental model), but is easier for young children to solve.In analyzing their solution strategies, it has been found that comparison problems require a number-matching type strategy that appears to be more sophisticated than the countingtype strategies that are often sufficient for solving transfer problems (Riley et al., 1983;Carpenter and Moser, 1984).</p>
<p>Arithmetic expressions: The carry effect.Beyond the text and mental model, it is natural that the particular numbers used in a problem will have an effect on a child's performance (Daroczy et al., 2015).Consider the same problem(s) as above, but with a different number given in the premise:</p>
<p>Alice has 22 apples.Bob has 3 fewer apples than Alice.How many apples does Bob have?</p>
<p>The problem now has the arithmetic expression 22 − 3, which involves an arithmetic carry, which is also called a borrowing in the case of subtraction.A carry is a digit that is transferred from one column of digits to another as the result of an arithmetic computation.In this subtraction computation, a unit carry of 1 is transferred from the column of units to the column of tens in order to make the answer 19.The previous expression (5 − 3) did not have a carry, which is easier for children (Hitch, 1978;Ashcraft et al., 1992).The presence of a carry introduces an additional subroutine from the standard sequence of operations, which places a higher load on working memory (Fürst and Hitch, 2000).4</p>
<p>Problem Generation Method</p>
<p>Our experiments on LLMs with respect to the biases just discussed ( §5) rely on data generated for the sole purpose of our study.By not using problems from public datasets, previous work or other existing sources it becomes unlikely that our data has been used for training of the models, an increasingly common issue (Dodge et al., 2021;Elazar et al., 2023).This section gives the details of our data generation pipeline, which provides control over features across all levels of Fig. 1.In §4.1, we operationalize Fig. 1, giving definitions related to the mental model level and other aspects of the process.Using these definitions, we then explain our generation pipeline in §4.2.</p>
<p>Operationalizing Fig. 1</p>
<p>The mental model level is operationalized using the formalism from Opedal et al. (2023), called MATHWORLD.</p>
<p>MATHWORLD annotates each math word problem with a logical representation, which captures the mathematical relationships between the entities described in text.Each entity has a non-negative integer quantity.Optionally, there may be additional information associated with an entitynamely, an agent who possesses the entity, and a unit and an attribute which enrich the description of it.The five bold items in the two preceding sentences are referred to as properties.The arithmetic relationships are classified according to concepts; we use transfer, comparison (additive and multiplicative), and rate in this work.We gave examples of the transfer and comparison concepts in §3.</p>
<p>When discussing data generation ( §4.2) and the experiments the data is used for ( §5), we will rely on the following definitions: A predicate is a symbol that represents either an arithmetic concept, or possession of an entity (in that case the predicate is "container").Each predicate takes a set of properties as arguments. 5A predicate instantiated with properties is called a logical form, and represents the semantics of a given sentence in a problem.See Table 1 for examples of logical forms for all predicates we use.The mental model of a problem is a sequence of logical forms (separated by a "•" symbol) for each sentence in that problem (in the same order), representing its semantics.In Fig. 1, we gave a mental model example in graphical format; its equivalent sequential format is container(Alice, 5, apple) • comparison(+, Alice, Bob, 3, apple).The problem text in Fig. 1 is faithful to this mental model, in the sense that the mental model represents the semantics of that text under the MATHWORLD formalism.We refer to the structure of a problem as a mental model in which the property values are replaced by unique placeholders.The structure associated with the previous example is
container([agent1], [n1], [entity1]) • comparison(+, [agent1], [agent2], [n2], [entity1]).
Finally, we formalize the arithmetic expression level of Fig. 1.Every concept-based predicate corresponds to an equation x = y ⊙ z, with ⊙ ∈ {+, −, ×, ÷} and x, y, z ∈ Z ≥0 ∪ V where V is a set of variable symbols.We require that ∃!v ∈ {x, y, z} : v ∈ V .We refer to the deductive inference step taken to solve that equation as a reasoning step, and its output (i.e., the value of the variable) as an intermediate result.The arithmetic expression level consists of a sequence of such reasoning steps, which is a proof of the answer of the problem (or solution).Any fact from the mental model is an axiom that can be used in the solution proof.This work focuses exclusively on linear problems, in which every reasoning step has at most one non-axiom premise. 5We enforce all quantities that are associated with predicates to be explicit numbers.Note that this places a constraint on the format of the problems: The number associated with a mathematical relationship is never an intermediate result, but is always given in text.("red" and "kg", respectively).Each logical form is given with two of the viable templated sentences from our generation pipeline ( §4.2), the semantics of which is represented by the logical form.</p>
<p>Plausibility of the mental model framework.A mental model theory over some reasoning domain must be able to adequately explain the relative difficulty across different types of reasoning problems (Johnson-Laird, 1983).Our MATHWORLD-based operationalization emphasizes arithmetic concepts and their relational structure as the key features that explain errors at the mental model level, which is in line with existing theories on word problems (Riley et al., 1983;Briars and Larkin, 1984;Kintsch and Greeno, 1985).Our schemata for the logical forms extend the problem schemata from Riley et al. (1983), specifically, in terms of the breadth of concepts and properties supported.</p>
<p>Generation Pipeline</p>
<p>We propose a simple problem generation pipeline, described in this section and later applied in §5.It proceeds in four steps: (i) sampling the (linear) problem structure, (ii) obtaining a mental model by instantiating the structure with properties, (iii) transforming the mental model into templated natural language, and, finally, (iv) correcting linguistic errors and awkward phrasings in the templated text using an instruction-tuned LLM.Fig. 2 illustrates the full pipeline.App.A.3 discusses related approaches.</p>
<p>(i) Problem structure generation.Our pipeline supports sequences of predicates under the following regular language, which is sufficient for the hypotheses we are testing: ;</p>
<p>where concept = comparison | rate | transfer.Each predicate in the sequence corresponds to an axiom from a distinct sentence in the problem.The particular set of predicate sequences (i.e., class of structures) from which we sample is test dependent (see §5).Such a set could, for instance, be the class of all linear reasoning structures with at most N =5 steps and only transfer concepts.We generate a problem as follows: First, we sample the number of reasoning steps n uniformly at random from the set {1, . . ., N }.Next, we sample the predicates; each choice is sampled uniformly at random.Since this first step of the pipeline generates structures, the predicates all have associated unique placeholders in place of properties, e.g., agent2, entity1.We only introduce new entity placeholders in rate logical forms; see Table 1, in which rate is the only predicate that takes two entity properties.We determine uniformly at random whether an entity is paired with an attribute, a unit, or neither.The instantiation of agent placeholders is test specific.</p>
<p>Finally, the answer of a problem is always set as the intermediate result corresponding to the last logical form in the ordering, which is unique.See the box on the left-hand side of Fig. 2 for an example structure generated after this step.</p>
<p>(ii) Problem structure instantiation.Next, the problem structure is instantiated with properties, yielding a mental model.We use a handwritten vocabulary for each of the lexical properties (entity, agent, unit and attribute) and sample instantiations of these properties from those vocabularies uniformly at random.The numerical quantities are instantiated by sampling a set of numbers uniformly at random from within a fixed range, which is 2-20 for the experiments in §5.2-5.3 and 100-999 for the experiments in §5.4. 6Then, we enumerate the logical forms and accordingly compute intermediate results for each reasoning step, making sure that the intermediate quantities fall in the range 0-999.If not, a new set of numbers is sampled and the procedure is repeated from the beginning.This naive procedure is sufficiently fast for our purposes.Empirically, we observed an average runtime of ≈4 ms to generate a numerical instantiation for a problem.</p>
<p>(iii) Template sampling.The mental model is then converted to natural language using templated text.Specifically, for each of the predicates we construct a set of templates that represent natural language adhering to that predicate.For instance, transfer(Annie, None, 3, watch) is converted to "[Annie] bought [3] [watch]s" in Fig. 2; see Table 1 for additional examples.The templates are handcrafted.We sample one template uniformly at random for each predicate in the mental model.We also create and sample interrogative templates for the questions, which always query the intermediate result derived from the last predicate.Finally, we concatenate the sentences to obtain the full problem text.This step enables control over the linguistic form of the sentences in the problem text, which will be important for our test related to text comprehension in §5.2.Moreover, since the faithfulness of the templated text is guaranteed by manual design, the procedure up to this point ensures that the text is faithful to the mental model.</p>
<p>(iv) Linguistic error correction.However, templated texts occasionally induce spelling mistakes and awkward phrasings.In the example shown in Fig. 2, the entity "watch" is inserted into the template to make "watchs".Inspired by the demonstrated success of zero-shot grammatical error correction (Kwon et al., 2023;Loem et al., 2023), we use an instruction-tuned language model (Ouyang et al., 2022), GPT-3.5 Turbo, to correct such errors. 7We write a short instructive prompt and have the model generate (with greedy decoding) a corrected problem text conditioned on that prompt together with the particular templated text we want to correct.The prompt instructs the model to be conservative, i.e., to only correct linguistic errors and awkward phrasings.We provide the exact prompt used and additional generation details in App.A.1.This step could, in principle, be generalized to perform less strict forms of paraphrasing.</p>
<p>There is then a trade-off between faithfulness and control on the one hand and linguistic variability and naturalness on the other, which can be tuned using different prompts and decoding methods.The present study prioritizes control and faithfulness, but alternative prioritizations could be used in future studies that employ our method.</p>
<p>Evaluating data quality.The generated problem texts must be faithful to the mental models from which they were generated, so we perform manual evaluation of the data to assess that such is the case.We follow a generic procedure and perform it for each of the datasets generated for the experiments in §5.2-5.4.The procedure is iterated until we achieve satisfactory quality.The final error-rate estimates were 0% for all three datasets.Details are given in App.A.2.</p>
<p>Experiments</p>
<p>We generate data to perform tests on whether LLMs exhibit child-like biases using the pipeline discussed above.We aim to identify where in the process in Fig. 1 those biases emerge.We therefore split our tests according to the level (and associated skill) they target: problem text and text comprehension ( §5.2), mental model and solution planning ( §5.3), and arithmetic expressions and solution execution ( §5.4).First, we discuss the general experimental setup ( §5.1).</p>
<p>Experimental Setup</p>
<p>We base our experiments on the problem features discussed in §3 that have been found to have an effect on child performance in solving word problems.Specifically, given such a feature X, we want to know whether X has a causal effect on the performance of LLMs.Our generation pipeline enables exact matching of the data: We generate problems in pairs, where the two problems differ only in the value of X.Using this data, we estimate the conditional average treatment effect (CATE; Imbens and Rubin, 2015)
E[Y (x) − Y (x ′ ) | Z],(1)
where Y is 1 if the model's prediction is correct and 0 otherwise, x and x ′ are two distinct values of the treatment variable X, and Z is the subgroup of data generated through our pipeline for a specific test.The two values x and x ′ are defined such that positive CATEs are consistent with human biases.We refer to Feder et al. (2022) for further reading on causality-based methods for NLP.</p>
<p>For each of the tests described below we select a specific feature X that is localized to one of the levels, to the extent possible.That is, varying X associated with a particular level should have no effect on the levels above, and minimal effect on the levels below.For instance, in §5.2 we vary the problem text without affecting the mental model, arithmetic expression or answer.</p>
<p>Having selected X, we adapt the pipeline ( §4.2) to generate example pairs, one with X = x and one with X = x ′ .Next, we evaluate the data quality using the procedure described in App.A.2.After quality assurance, we generate a larger sample of 400 additional problem pairs, which (including the quality evaluation set) gives a total of 500 pairs for the tests.</p>
<p>We then generate outcomes Y (x) and Y (x ′ ) for each of the pairs for a set of selected LLMs.We use LLaMA2 (Touvron et al., 2023) with 7B, 13B and 70B parameters, Mistral 7B (Jiang et al., 2023) and Mixtral 8x7B (Jiang et al., 2024), GPT-3.5 Turbo, and GPT-4 Turbo (OpenAI, 2024).We consider both the pretrained-only and instruction-tuned versions for the LLaMA2, Mistral and Mixtral models.We carry out zero-shot inference with a standard prompt, a chain-ofthought prompt (CoT; Wei et al., 2022) 4.</p>
<p>is" for instruction-tuned models.Then, the model prediction is retrieved from the response by extracting the first number in the model's output.For the CoT experimental procedure, we follow the exact method from Kojima et al. (2022).First, the model is prompted to generate a reasoning chain by appending "Let's think step by step" to the input.Then, the model is re-prompted to generate the final answer, which is extracted from the output as in the direct case.Responses are generated with greedy decoding and a maximum length of 256 tokens.After obtaining the model's predictions, we estimate the CATE and perform a two-tailed paired sample ttest to determine whether the CATE estimate is significantly different from 0. More specifically, the null hypothesis is that the two groups of model accuracy have the same mean.</p>
<p>We control the false discovery rate at level α=0.05 using Benjamini and Hochberg's (1995) procedure, considering all null hypotheses under the same bias as one distinct family.</p>
<p>Problem Text: Consistency Bias</p>
<p>Varying the linguistic form of an otherwise equivalent problem structure can have a large effect on child performance (Cummins et al., 1988).We test whether comparison problems with inconsistent statements are harder for LLMs than the same problems with an analogous consistent statement.</p>
<p>Method.Following Lewis and Mayer's (1987) study, we consider consistent/inconsistent problem pairs where the required operation is either addition, subtraction, multiplication or division.The generation pipeline is tuned so that the problem structures follow the specification:
container • (transfer | rate) • • • • • (transfer | rate) 0−2 times • comparison • (transfer | rate) • • • • • (transfer | rate) 0−2 times ;
Note that the problems may have between 1−5 reasoning steps-one for every non-container predicate.Apart from the first predicate (container), only the comparison predicate introduces a new agent.The question queries the agent that was introduced by comparison. 8The pairs are generated such that the only sentence that varies is that which corresponds to comparison, one being consistent and the other being its analogous inconsistent form.</p>
<p>Results.The results of the consistency bias test reveal 20 out of 23 statistically significant CATEs.As displayed in Table 2 (leftmost column), all models exhibit lower accuracy when solving inconsistent problems compared to their consistent counterparts.Interestingly, the bias appears to be exacerbated by CoT prompting, which improves the overall model performance, but magnifies the difference in accuracy between consistent and inconsistent problems.This finding aligns with research indicating that CoT prompting may also amplify other types of biases present in the training data (Shaikh et al., 2023).Particularly notable CATEs are observed for the base versions of LLaMA2 7B, LLaMA2 13B, and Mistral 7B, for which the inconsistent formulation of the problems leads to an accuracy drop larger than 50%.</p>
<p>Mental Model: Transfer vs Comparison Bias</p>
<p>Another factor behind performance is that it might be harder to perform solution planning based on some mental models compared to others.We test whether LLMs are better at solving transfer-type problems than comparison problems.</p>
<p>Method.The problem structures take the following forms:
container • transfer • • • • • transfer 1−5 times ; container • comparison • • • • • comparison 1−5 times ;
The two problems have identical arithmetic expressions.Each comparison predicate corresponds to a comparison of a new agent with the agent introduced in the preceding sentence.Each transfer statement follows the same agent, who was introduced in the first sentence and whose state is updated through a transfer with some other agent.The problems resemble each other in linguistic form as much as possible.In particular, we make sure that the same agent names are introduced in each sentence across the two problems, in order to account for variance in performance stemming from such choices (Goodarzi et al., 2023).Consistent or inconsistent forms of comparison are sampled uniformly at random.</p>
<p>Results. The experimental results (middle column in</p>
<p>Table 2) show that models are consistently more accurate on problems based on transfers rather than comparisons.With the exception of CoT-prompted pretrained-only Mistral and Mixtral, we observe statistically significant positive CATEs, mirroring biases seen in children's problem-solving ( §3).</p>
<p>Further, we note that the instruction-tuned models overall exhibit larger effects than pretrained-only models in the CoT setting, but not in the direct setting.This seems to be the case for the consistency bias as well.Finally, in App.B.2 we show some of the results as broken down by number of reasoning steps in the CoT setting.</p>
<p>Arithmetic Expressions: The Carry Effect</p>
<p>While much of a child's performance on word problems can be explained by properties introduced by the text format, a large portion still depends on the nature of the arithmetic expression (Daroczy et al., 2015).We test whether LLMs are sensitive to the presence of arithmetic carry when posed with addition and subtraction in math word problems.</p>
<p>Method.We generate pairs under the comparison specification from §5.3, but with only one step:9 container • comparison;</p>
<p>As in §5.3, we use additive comparisons, which ensures that the arithmetic expressions only have addition and/or subtraction operators.The two problems of a pair are identical apart from the numbers.Following Fürst and Hitch (2000), we ensure that both operands as well as the answer of the problem are three-digit numbers (since children appear to rely on long-term memory for problems with small numbers; Footnote 4).One of the problems has no carry, the other has at least one (i.e., unit or tens carry).The correct answer of the two problems is controlled to be the same.</p>
<p>Results.The results (rightmost column in Table 2) depart from the findings above, which gave evidence for the presence of child-like biases.In this case, model performance is similar in problems with and without carry operations-we only observe one significant result out of the 23 tests.Thus, the models seem not to be sensitive to variations isolated to the arithmetic expression level.</p>
<p>Why do Language Models Exhibit Biases?</p>
<p>A natural set of questions that arise from our results is why some child-like biases are present in the models, and why some of them (like the carry effect) are absent.The most plausible explanation in our view is the influence from the training data: If the training data contains many examples of humans writing and solving word problems, then it may be that LLMs simulate human biases present in such text.For instance, it may be that the distribution of the training data is skewed towards consistent problem formulations of comparison relations, which in turn could be a product of consistency bias in the humans who wrote the word problems.This would seem plausible given that the consistency bias is present also in adults (Lewis and Mayer, 1987;Hegarty et al., 1995).</p>
<p>Unfortunately, we cannot directly verify this hypothesis since it is unknown what data has been used to train the models considered in this study.However, as a proxy, we performed an analysis of a set consisting of word problems from MAWPS (Koncel-Kedziorski et al., 2016b), ASDIV-A (Miao et al., 2020), and SVAMP (Patel et al., 2021).These three datasets are well known and publicly available, and are thus likely to have been present in the training data of the LLMs used in this work.We found that these datasets indeed have more consistent formulations than inconsistent ones, and more transfer problems than comparison problems.More specifically, the ratios observed were 5:1 (15 and 3 in absolute numbers) and 130:9 (260 and 18 in absolute numbers), respectively.10In other words, the imbalance in problem types on these datasets is consistent with the biases we found in our analyses on text comprehension ( §5.2) and solution planning ( §5.3).</p>
<p>Extrapolating this hypothesis to the absence of a carry effect would imply that there is little to no difference in frequency between problems with and without carry in the training data.This would be harder to verify, as there are many potential traces of carry in the data beyond word problems.Furthermore, the carry effect results suggest that there are algorithmic differences in how LLMs and humans perform arithmetic computations.In particular, the carry effect in humans is partially attributed to working memory limitations (Hitch, 1978), which LLMs may not implement in the same manner.The memory and computational mechanisms through which models perform arithmetic (Nanda et al., 2023;Stolfo et al., 2023a;Quirke and Barez, 2024) are likely not affected by the increased cognitive load that the carry operation introduces in humans.This leads to an alternative, albeit arguably less plausible view, on why we observe the other two biases: It might be that there is algorithmic similarity between humans and language models on text comprehension and solution planning.Assessing this hypothesis would require knowledge about the mechanisms of human and language-model reasoning alike, both of which are beyond the scope of the present study.However, our results at least suggest a direction, namely, that there is at least the possibility that the algorithms in text comprehension and solution planning exhibit similarity.</p>
<p>Related Work</p>
<p>Our work relates most closely to studies that have compared human and LLM biases on syllogisms (Ando et al., 2023;Eisape et al., 2024), code generation (Jones and Steinhardt, 2022), and other non-mathematical inference tasks (Dasgupta et al., 2023).Their findings indicate that LLMs are susceptible to some of the same biases as humans, like con-tent effects (Ando et al., 2023;Dasgupta et al., 2023) and premise ordering effects (Eisape et al., 2024).We observe similar results in a mathematical problem-solving setting for consistency bias and transfer vs comparison bias, but not for the carry effect which relates to the step of the cognitive process that involves solving arithmetic equations.</p>
<p>Our study also differs from those referenced above in that we systematically compare the effect of CoT prompting to direct prompting, observing amplified effects in the CoT setting in most cases where effects are present.</p>
<p>We are unaware of any other work that studies cognitive biases in LLMs that, like the carry effect, relate directly to numbers.However, there seem to be similarities between the numeric representations in LLMs and the "mental number line" in humans (Shah et al., 2023).Other studies find evidence that LLMs to some extent rely on spurious correlations in numerical reasoning (Razeghi et al., 2022;Stolfo et al., 2023b), and that their performance decreases with increasing number size (Dziri et al., 2023;Shen et al., 2023).Beyond numerical reasoning, LLMs appear to have difficulties with causal reasoning (Binz and Schulz, 2023;Jin et al., 2023;2024) and proof planning (Saparov and He, 2023).</p>
<p>Conclusion and Implications</p>
<p>This study explored whether LLMs exhibit child-like cognitive biases in arithmetic word problem-solving.We found that LLMs demonstrate biases in text comprehension and solution planning that mirror human tendencies.Specifically, models performed better on problems where the relational keyword is consistent with the appropriate arithmetic operator compared to problems where it is not, as well as on problems with a dynamic change of state compared to problems with a static comparison.However, at the solution execution step, LLMs did not exhibit the child-like carry effect for arithmetic computations.In general, studying biases that are present in children but not in adults may enable the disentanglement of the influence of training data from other factors that might explain language model behavior, since one would expect the training set to be heavily biased towards adult (rather than child) thinking.We therefore believe it might be a promising direction forward in language model interpretability work.</p>
<p>Impact Statement</p>
<p>Cognitive modeling enables human simulations in place of data collection that might otherwise be unethical, harmful or costly.On the other hand, issues could arise if those simulations are unfaithful to human behavior.As a broader implication of our work, we encourage practitioners to exercise care when developing and deploying cognitive models of students using LLMs, particularly, in how the student model treats numbers and other properties of arithmetic expressions.We hope that our results can provide insights for practitioners seeking to develop automated learning agents, for instance, under the tutor-learning paradigm in which a student learns by correcting the mistakes made by a computer model (Okita, 2014;Shahriar and Matsuda, 2021;Schmucker et al., 2023).We do not see any notable ethical issues with our work.</p>
<p>Limitations</p>
<p>We cannot draw any parallels on the absolute performance in comparison with children, only on the presence or absence of each effect.This is because the datasets used in the learning science studies were not available to us.The one exception was the data from Hegarty et al. (1995), which we evaluate on in App.B.3.Moreover, we do not consider the grade level of the problems, but see Jiao et al. (2023) for a generation method that does.</p>
<p>In selecting specific cognitive biases to study, we chose biases that are well-established in literature on human children and whose effects could be clearly associated with one of the steps of Fig. 1.Another factor that fulfills these desiderata is the effect of explicit verbal cues (Hudson, 1983;Vicente et al., 2007).More fundamentally, a complete comparison of the biases between LLMs and humans would need to study biases that have been found in LLMs but are not necessarily present in humans.We do not take that direction into account, but we note that the number frequency effect reported by Razeghi et al. (2022) might be one such example.</p>
<p>We did not use in-context examples in our evaluation since the addition of such may influence the results in ways that can be difficult to foresee.However, an interesting direction for future work could be to study whether cognitive biases can be controlled through specific choices of in-context examples or other prompts.</p>
<p>We stress that the conceptualization in Fig. 1 is a simplified model of the solving process.For instance, it fails to account for shortcut strategies (see Footnote 3) and it does not consider any propositional text-base representation which precedes the mental model representation in some other models (Kintsch and Greeno, 1985;Hegarty et al., 1995).We do not make any claims on the ability of LLMs to "construct mental models" in this work, although our results could potentially have such implications as was pointed out by a reviewer.See App.C for a brief discussion.</p>
<p>Finally and importantly, we only consider problems formulated in English.We note that some effects could vary across languages.For instance, the carry effect is more pronounced in German and other languages where the spelled-out order of tens and units is inverted in relation to Arabic numerical notation (Göbel et al., 2014)  We use GPT-3.5 Turbo to carry out the linguistic error correction step detailed in §4.2.In Table 3, we provide the exact prompt used for the task.The corrected problem is generated using greedy decoding (temperature=0).We carry out additional integrity checks of the generated problem against the original templated text.In particular, we verify that the sentence count and relational terms (such as "more") are consistent post error-correction.The problem is discarded if these additional checks are not satisfied.</p>
<p>A.2. Data Quality Evaluation</p>
<p>We describe the manual evaluation of the datasets generated in §5.For each of the datasets, we do the following: First, generate a control set of 10 examples.These 10 examples are evaluated independently by three of this paper's authors.</p>
<p>If there are any errors we make appropriate modifications to the pipeline and restart the procedure.If not, we proceed to evaluate 90 more examples, allocating 30 to each of the three authors.Error rate is estimated on this sample of 100 examples.</p>
<p>We use two binary evaluation criteria, one assessing the linguistic error correction step (iv) and one assessing testspecific attributes.A problem is deemed to be good according to the former if the generated problem only deviates from the templated text through spelling or grammar correction.The test-specific criterion and the obtained error estimates are given below.</p>
<p>Consistency bias ( §5.2).We evaluated data quality according to whether the two comparison statements actually were consistent and inconsistent forms to express the same comparison relationship.To be precise, the first problem needs to have a consistent relational statement for the comparison predicate, the second problem needs to have an equivalent inconsistent relational statement for the same comparison predicate, and the two problems need to be identical otherwise.Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.</p>
<p>Transfer vs comparison bias ( §5.3).We evaluated data quality according to whether the comparison and transfer problems had the same arithmetic expression and whether they followed the specified problem structure.We also ensured that the agent names and other properties matched.</p>
<p>Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.</p>
<p>The carry effect ( §5.4).We evaluated data quality according to whether one problem had no carry computation steps, the other one had at least one, and they were equal otherwise.That is, only the numbers differed and the two problems had the same answer.Our pipeline achieved a 0% error rate on both criteria on the 100 evaluated example problems.</p>
<p>A.3. Related Methods</p>
<p>Our generation pipeline differs from</p>
<p>B. Additional Results</p>
<p>B.1. Child-Persona Prompting</p>
<p>Taking inspiration from claims that LLMs can act as agent models (Andreas, 2022), we also experimented with an additional prompt in which the LLM is instructed to impersonate a grade-school child.Specifically, we employ a modified version of the zero-shot chain-of-thought prompt, tailored to simulate a child's reasoning process.We prompt the model with the phrase "Let's think step by step as a grade-school child would," replacing the standard CoT instruction.Following this, we apply the same decoding method used in traditional CoT.The results for this approach are reported for the open-source models (apart from LLaMA2 70B) in Table 4.While we notice larger consistency and transfer vs comparison effects for some models, we observe no substan-tial departure from the results achieved with conventional CoT prompting.</p>
<p>B.2. Bias Strength by Number of Reasoning Steps</p>
<p>In Fig. 4 we show how the CATE of the transfer vs comparison bias varies with the number of reasoning steps in the problems for the CoT setting.Interestingly, we observe that the CATE sizes increase with the number of reasoning steps for the instruction-tuned models, whereas they decrease for the pretrained-only base models.We are unaware of literature on the relationship between human transfer vs comparison bias and the number of steps, so we can not make any claims about which of these patterns is more cognitively plausible.</p>
<p>The consistency-effect test does not exhibit such diverging trends for the CATEs.Fig. 3 illustrates how the strength of the measured biases change in relation to the number of reasoning steps in a problem (in the CoT-prompted case).Note that the carry effect experiments were carried it out for problems with only one step.</p>
<p>B.3.Data from Hegarty et al. (1995) In Table 5 we present the results on a few selected models when evaluated on the data from the study by Hegarty et al. (1995).Theirs was the only background study for which we were able to obtain the data that was used.The dataset contains eight problem pairs and targets the consistency bias.While we do not obtain significant results, we do get an indication of the absolute effect of the bias as compared to the human subjects in Hegarty et al.'s (1995) study (who were undergraduate college students).In their study, solvers who committed at least four errors out of the total of 16 problems had an average accuracy of 62% and 24%, on consistent problems and inconsistent problems respectively.None of the absolute accuracies in</p>
<p>Figure 1 .
1
Figure 1.A three-step model of the cognitive process involved in solving math word problems.We study whether LLMs exhibit similar biases as human children along each step of this process.</p>
<p>Figure 2 .
2
Figure 2. Overview of our generation pipeline with an example problem.(1) We start by generating the problem structure.The alignment between the graphical and sequential formats of the structure is illustrated by color coding, and the black containment boxes in the graph represent intermediate results.(2) The properties of those structures are then instantiated with values, resulting in a mental model.(3) Next, we sample a templated sentence for each of the logical forms in the mental model, and concatenate them in the ordering of the logical forms.(4) Finally, we correct errors and awkward phrasings by prompting an instruction-tuned LLM (the corrections are highlighted in blue).This particular example includes all predicate types that we use in §5.</p>
<p>Figure 3 .
3
Figure 3. CATE for the consistency effect ( §5.2) in CoT-prompted models stratified by number of reasoning steps.'Instruction-tuned' refers to the Chat and Instruct versions of LLaMA2 and Mistral/Mixtral, respectively.</p>
<p>Figure 4 .
4
Figure 4. CATE for transfer vs comparison ( §5.3) in the CoTprompted case stratified by number of reasoning steps.'Instructiontuned' refers to the Chat and Instruct versions of LLaMA2 and Mistral/Mixtral, respectively.Base and instruction-tuned models display opposite relationships between length and bias strength.</p>
<p>Table 1 .
1
Examples of MATHWORLD logical forms.A logical form consists of a predicate and a set of properties.Note that the attribute and unit properties are optional, and only the first logical form has non-null values for them
Logical FormExample SentencesPredicatePropertiesagent=AliceAlice has 5 kilograms of redquantity=5apples.containerentity=apple attribute=red unit=kgAlice owns 5 kilograms of red apples.type=+Bob has 3 fewer apples thanagentA=AliceAlice.comparisonagentB=Bobquantity=3Alice has 3 more apples than Bob.entity=appletransferreceiver_agent=Bob sender_agent=Alice quantity=3 entity=appleAlice gave Bob 3 apples. Bob got 3 more apples from Alice.agent=AliceEach of Alice's baskets holds 4ratequantity=4 entityA=appleapples. Every basket that Alice hasentityB=basketcontains 4 apples.</p>
<p>Table 2 .
2
, as well as a modified "child-persona" CoT prompt, whose results were similar to those of the CoT setup and are presented in App.Accuracy
Consistency bias ( §5.2)Transfer vs comparison bias ( §5.3)Carry effect ( §5.4)Mode ModelAccuracy (%)p-valueAccuracy (%)p-valueAccuracy (%)p-valueCoInCoCATETCCATENCaCaCATELLaMA2 7B9.64.84.8&lt;0.00121.813.08.8&lt;0.00164.860.04.80.009LLaMA2 13B17.214.03.20.00628.620.08.6&lt;0.00172.267.25.00.030LLaMA2 70B24.016.27.8&lt;0.00145.426.818.6&lt;0.00195.296.21.00.380Mistral 7B17.812.05.8&lt;0.00134.020.413.6&lt;0.00172.472.00.40.835Mixtral 8x7B23.017.06.0&lt;0.00142.230.411.8&lt;0.00195.493.61.80.117DirectLLaMA2 7B Chat14.210.83.40.00920.215.84.40.00561.254.27.00.012LLaMA2 13B Chat16.411.84.6&lt;0.00125.418.27.2&lt;0.00165.659.66.00.018LLaMA2 70B Chat16.414.81.60.15832.420.012.4&lt;0.00196.497.0-0.60.578Mistral 7B Instr.17.614.23.40.00828.021.86.2&lt;0.00178.078.6-0.60.802Mixtral 8x7B Instr.23.421.81.60.19542.628.014.6&lt;0.00195.896.4-0.60.578GPT-3.5 Turbo32.222.89.4&lt;0.00161.033.427.6&lt;0.00199.699.40.20.320LLaMA2 7B16.46.010.4&lt;0.00118.813.65.20.00933.238.8-5.60.006LLaMA2 13B30.28.621.6&lt;0.00137.813.224.6&lt;0.00133.833.40.40.833LLaMA2 70B40.224.016.2&lt;0.00163.833.030.8&lt;0.00168.667.61.00.850Mistral 7B36.416.819.6&lt;0.00149.858.8-9.00.00473.271.02.20.283Mixtral 8x7B62.442.220.2&lt;0.00168.665.03.60.20679.879.80.01.000CoTLLaMA2 7B Chat66.838.628.2&lt;0.00169.640.828.8&lt;0.00172.471.01.40.514LLaMA2 13B Chat67.028.638.4&lt;0.00179.448.031.4&lt;0.00173.878.6-4.80.017LLaMA2 70B Chat82.861.421.4&lt;0.00199.076.222.8&lt;0.00197.095.81.20.180Mistral 7B Instr.61.833.628.2&lt;0.00183.452.031.4&lt;0.00178.675.63.00.162Mixtral 8x7B Instr.85.471.613.8&lt;0.00198.283.814.4&lt;0.00197.094.62.40.014GPT-3.5 Turbo89.287.81.40.38097.093.04.00.00397.898.2-0.40.580GPT-4 Turbo90.472.418.0&lt;0.00199.291.47.8&lt;0.00199.699.60.0-
(Kojima et al., 2022;Yang et al., 2024)mpted to directly provide an answer after the input.Following previous work(Kojima et al., 2022;Yang et al., 2024), we use the format "Q: {problem}\nA: The answer (Arabic numerals) is " for base models and "{problem}\nThe answer (Arabic numerals) , conditional average treatment effect (CATE), and statistical significance (p-value) on math word problems generated for the three tests detailed in §5.2, §5.3 and §5.4.'Co' denotes consistent, 'InCo' inconsistent, 'T' transfer, 'C' comparison, 'Ca' carry, and 'NCa' no carry conditions.Results are separated by whether the prompting method is direct or chain-of-thought (CoT).'Chat' and 'Inst.' indicate the instruction-tuned versions of the models.CATE values are bolded when significant, controlling for false discovery rate at level α=0.05.Results from an additional prompting strategy, child-persona prompting, are presented in Table</p>
<p>. Our generation pipeline can be straightforwardly adapted to other languages, and future work might consider doing so.Correct all grammatical mistakes that appear in the following math word problem: [templated text] Fix any awkward or redundant phrasing.Pay close attention to incorrect plural forms.Do NOT solve the problem.Do NOT compute any intermediate solutions.Do NOT make any changes to the numerical values or implied mathematical operations.Only output the corrected math word problem and nothing else.Do NOT restate the original problem.Do NOT include "Corrected Version:" or any description of the task.</p>
<p>Table 3 .
3
Prompt used for the linguistic error correction step in our generation pipeline from §4.2.
A. More Details on the Generation Method
A.1.Details on Linguistic Error Correction</p>
<p>Table 4 .
4
Accuracy, conditional average treatment effect (CATE), and statistical significance (p-value) on math word problems generated for the three tests detailed in §5.2, §5.3 and §5.4.'Co' denotes consistent, 'InCo' inconsistent, 'T' transfer, 'C' comparison, 'Ca' carry, and 'NCa' no carry conditions.The results presented are for the child-persona prompting strategy described in App.B.1.'Chat' and 'Inst.' indicate the instruction-tuned versions of the models.CATE values are bolded when p &lt; 0.001.
Opedal et al.'s (2023)
(Koncel-Kedziorski et al., 2016a)ict control over linguistic form, our error correction step could in principle be broadened to perform paraphrasing and theme rewriting(Koncel-Kedziorski et al., 2016a)as well.</p>
<p>Table 5 are similar, but Mixtral 8x7B Instruct displays a similar absolute effect size</p>
<p>Related studies have compared LLM biases to human ones on the task of syllogistic reasoning(Ando et al.,<br />
;Dasgupta et al., 2023;Eisape et al., 
). These are discussed in §7.
This comment refers to studies performed on children, but we note that some of the biases considered seem to be present in adults as well (albeit with weaker effects). See Jaffe and Bolger (2023) for a recent review on word problem performance independent of age.
The particular example numbers given here are small enough for children to likely be relying on retrieval from some long-term memory store instead of algorithmic computation(Koshmider and Ashcraft, 1991), which could erase the effect of carry. We account for this in our experiments ( §5.4) by using larger number ranges.
We omit 0 and 1 from the first number range in order to avoid unnatural phrases like "Bob has 1 times as many apples as Alice."The range for the experiments in §5.4 contains larger numbers for reasons given in Footnote 4.
Interestingly, we found that GPT-3.5 Turbo sometimes would transform inconsistent formulations of comparison-type relationships into semantically equivalent consistent ones, which already indicates presence of a bias towards consistent problems. Such erroneous transformations were filtered out (see App. A.1).
The following is a (consistent) example from our dataset that follows the pattern container • transfer • comparison: "Avery has 15 desks. Avery bought 18 desks. Natalie has 16 fewer desks than Avery. How many desks does Natalie have?"
The one-step case follows the setups from studies on humans ( §3). We discovered in the previous two tests that the models frequently fail on comparison problems with only one step (which can be inferred from Figs.3 and 4), so if a carry effect is present, it should be observable in such a setting.
We used the world model annotations fromOpedal et al. (2023) for this analysis, which enabled us to extract the problems with the relevant concepts. The problems containing a comparison predicate were few enough for manual inspection. For the transfer problems, 433 in total, we manually inspected a sample of 100. Out of these, 60 had at least one sentence with a transfer of the same structure we consider in this work. Maximum likelihood estimation then yields 0.6×433 ≈ 260 relevant transfer problems.
AcknowledgementsWe thank Emo Welzl, Ethan Gotlieb Wilcox, Julia Chatain and Yilmazcan Ozyurt for valuable feedback and discussions.Andreas Opedal acknowledges funding from the Max Planck ETH Center for Learning Systems.Alessandro Stolfo is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship.(37.5% vs 38%).C. Brief Discussion on Mental Model BuildingWe note that the presence of consistency bias could be viewed as an argument against the position that language models construct something akin to a mental model during problem-solving.Indeed, people who exhibit consistency bias seem to be more likely to construct a mental model of the problem compared to those who do not, based on eye-fixation behavior(Hegarty et al., 1995).Intuitively, a (human or non-human) solver that constructs a mental model should be able to be more robust against inconsistent phrasings, assuming that the text-comprehension step of the solving pipeline is not made significantly harder by inconsistent phrasings.
Using large language models to simulate multiple humans and replicate human subject studies. Gati Aher, Rosa I Arriaga, Adam Tauman, Kalai , Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023ICML'23. JMLR.org</p>
<p>Evaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like biases. Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada, Proceedings of the 4th Natural Logic Meets Machine Learning Workshop. the 4th Natural Logic Meets Machine Learning WorkshopNancy, FranceAssociation for Computational Linguistics2023</p>
<p>Language models as agent models. Jacob Andreas, 10.18653/v1/2022.findings-emnlp.423Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Out of one, many: Using language models to simulate human samples. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, David Wingate, 10.1017/pan.2023.2Political Analysis. 3132023</p>
<p>Chapter 8 working memory, automaticity, and problem difficulty. H Mark, Rick D Ashcraft, Margaret A Donley, Mary Halas, Vakali, 10.1016/S0166-4115(08)60890-0The Nature and Origins of Mathematical Skills. I D Jamie, Campbell, North-Holland199291</p>
<p>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Yoav Benjamini, Yosef Hochberg, 10.1111/j.2517-6161.1995.tb02031.xJournal of the Royal Statistical Society: Series B (Methodological). 5711995</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, 10.1073/pnas.2218523120Proceedings of the National Academy of Sciences. the National Academy of Sciences2023120</p>
<p>An integrated model of skill in solving elementary word problems. Diane J Briars, Jill H Larkin, 10.1207/s1532690xci0103_1Cognition and Instruction. 131984</p>
<p>The acquisition of addition and subtraction concepts in grades one through three. Thomas P Carpenter, James M Moser, Journal for Research in Mathematics Education. 1531984</p>
<p>The role of understanding in solving word problems. Denise Dellarosa Cummins, Walter Kintsch, Kurt Reusser, Rhonda Weimer, 10.1016/0010-0285(88)90011-4Cognitive Psychology. 2041988</p>
<p>Word problems: a review of linguistic and numerical factors contributing to their difficulty. Gabriella Daroczy, Magdalena Wolska, Walt Detmar Meurers, Hans-Christoph Nuerk, 10.3389/fpsyg.2015.00348Frontiers in Psychology. 62015</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 2023</p>
<p>Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Ettinger, Thirty-seventh Conference on Neural Information Processing Systems. </p>
<p>A systematic comparison of syllogistic reasoning in humans and language models. Tiwalayo Eisape, Ishita Tessler, Fei Dasgupta, Sjoerd Sha, Tal Van Steenkiste, Linzen, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, Mexico2024</p>
<p>What's in my big data?. Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A Smith, Jesse Dodge, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Amir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E Roberts, Brandon M Stewart, Victor Veitch, Diyi Yang, 10.1162/tacl_a_00511Transactions of the Association for Computational Linguistics. 102022</p>
<p>Separate roles for executive and phonological components of working memory in mental arithmetic. J Ansgar, Graham J Fürst, Hitch, 10.3758/BF03198412Memory &amp; Cognition. 2852000</p>
<p>Robustness of named-entity replacements for in-context learning. Saeed Goodarzi, Nikhil Kagita, Dennis Minn, Shufan Wang, Roberto Dessi, Shubham Toshniwal, Adina Williams, Jack Lanchantin, Koustuv Sinha, 10.18653/v1/2023.findings-emnlp.728Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Language affects symbolic arithmetic in children: The case of number word inversion. M Silke, Korbinian Göbel, Silvia Moeller, Liane Pixner, Hans-Christoph Kaufmann, Nuerk, 10.1016/j.jecp.2013.10.001Journal of Experimental Child Psychology. 1192014</p>
<p>Comprehension of arithmetic word problems: A comparison of successful and unsuccessful problem solvers. Mary Hegarty, Richard E Mayer, Christopher A Monk, Journal of Educational Psychology. 871995</p>
<p>The role of short-term working memory in mental arithmetic. J Graham, Hitch, 10.1016/0010-0285(78)90002-6Cognitive Psychology. 1031978</p>
<p>Correspondences and numerical differences between disjoint sets. Tom Hudson, Child Development. 541983</p>
<p>Guido W Imbens, Donald B Rubin, Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. USACambridge University Press2015</p>
<p>Cognitive processes, linguistic factors, and arithmetic word problem success: a review of behavioral studies. Joshua Benjamin, Jaffe , Donald Joseph Bolger, 10.1007/s10648-023-09821-6Educational Psychology Review. 3541052023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Stock, Sandeep Subramanian. Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024Mixtral of experts</p>
<p>Automatic educational question generation with difficulty level controls. Ying Jiao, Kumar Shridhar, Peng Cui, Wangchunshu Zhou, Mrinmaya Sachan, 10.1007/978-3-031-36272-9_39International Conference on Artificial Intelligence in Education. Springer2023</p>
<p>CLadder: A benchmark to assess causal reasoning capabilities of language models. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard Schölkopf, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Can large language models infer causation from correlation?. Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Schölkopf, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Mental models: towards a cognitive science of language, inference and consciousness. Philip Nicholas, Johnson-Laird , 10.5555/7909Cognitive Science Series. 61983Harvard University Press</p>
<p>Capturing failures of large language models via human cognitive biases. Erik Jones, Jacob Steinhardt, Advances in Neural Information Processing Systems. 2022</p>
<p>Understanding and solving word arithmetic problems. Walter Kintsch, James G Greeno, 10.1037/0033-295x.92.1.109Psychological Review. 9211985</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, ICML 2022 Workshop on Knowledge Retrieval and Language Models. 2022</p>
<p>A theme-rewriting approach for generating algebra word problems. Rik Koncel-Kedziorski, Ioannis Konstas, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, Texas; San Diego, CaliforniaAssociation for Computational Linguistics2016a. 2016bProceedings of the 2016 Conference of the North American Chapter</p>
<p>The development of children's mental multiplication skills. John W Koshmider, Mark H Ashcraft, 10.1016/0022-0965(91)90077-6Journal of Experimental Child Psychology. 5111991</p>
<p>Beyond English: Evaluating LLMs for Arabic grammatical error correction. Sang Kwon, Gagan Bhatia, 10.18653/v1/2023.arabicnlp-1.9Proceedings of ArabicNLP 2023. ArabicNLP 20232023Singapore (Hybrid). Association for Computational Linguistics</p>
<p>Simulated learners in educational technology: A systematic literature review and a turing-like test. Tanja Käser, Giora Alexandron, 10.1007/s40593-023-00337-2International Journal Of Artificial Intelligence In Education. 2023</p>
<p>Students' miscomprehension of relational statements in arithmetic word problems. Anne Bovenmyer, Lewis , Richard E Mayer, Journal of Educational Psychology. 791987</p>
<p>Exploring effectiveness of GPT-3 in grammatical error correction: A study on performance and controllability in prompt-based methods. Mengsay Loem, Masahiro Kaneko, Sho Takase, Naoaki Okazaki, 10.18653/v1/2023.bea-1.18Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>MathDial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. Jakub Macina, Nico Daheim, Sankalan Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan, 10.18653/v1/2023.findings-emnlp.372Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Dissociating language and thought in large language models: a cognitive perspective. 2023</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The development of semantic categories for addition and subtraction. P Nesher, James G Greeno, Mary S Riley, Educational Studies in Mathematics. 131982</p>
<p>Verbal cues as an interfering factor in verbal problem solving. Perla Nesher, Eva Teubal, Educational Studies in Mathematics. 611975</p>
<p>Large language models for in-context student modeling: Synthesizing student's behavior in visual programming from one-shot observation. Hung Manh, Sebastian Nguyen, Adish Tschiatschek, Singla, 2023</p>
<p>Learning from the folly of others: Learning to self-correct by monitoring the reasoning of virtual characters in a computer-supported mathematics learning environment. Sandra Y Okita, 10.1016/j.compedu.2013.09.018Computers &amp; Education. 712014</p>
<p>World models for math story problems. Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan, 10.18653/v1/2023.findings-acl.579Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Gpt-4 technical report. 2024OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 2022</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>La naissance de l'intelligence chez l'enfant. Jean Piaget, Delachaux et Niestlé. 1936</p>
<p>Personalized mathematical word problem generation. Oleksandr Polozov, O' Eleanor, Adam M Rourke, Luke Smith, Sumit Zettlemoyer, Zoran Gulwani, Popovic, The Twelfth International Conference on Learning Representations. 2015. 2024IJCAI. Philip Quirke and Fazl Barez</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Development of Children's Problem-Solving Ability in Arithmetic. Mary Riley, James Greeno, Joan Heller, Learning Research and Development Center. 1983University of Pittsburgh</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chainof-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Ruffle&amp;Riley: Towards the automated induction of conversational tutoring systems. Robin Schmucker, Meng Xia, Amos Azaria, Tom Mitchell, 2023</p>
<p>Numeric magnitude comparison effects in large language models. Raj Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, Sashank Varma, 10.18653/v1/2023.findings-acl.383Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Can you clarify what you said?": Studying the impact of tutee agents' follow-up questions on tutors' learning. Tasmia Shahriar, Noboru Matsuda, 10.1007/978-3-030-78292-4_32#citeasArtificial Intelligence in Education. ChamSpringer International Publishing2021</p>
<p>On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang, 10.18653/v1/2023.acl-long.244Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Positional description matters for transformers arithmetic. Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang, 2023</p>
<p>What makes certain arithmetic word problems involving the comparison of sets so difficult for children. Elsbeth Stern, Journal of Educational Psychology. 851993</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/v1/2023.emnlp-main.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Aurelien RodriguezRobert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Applications of simulated students: An exploration. Kurt Vanlehn, Stellan Ohlsson, Rod Nason, International Journal of Artificial Intelligence in Education. 51994</p>
<p>Influence of situational and conceptual rewording on word problem solving. Santiago, Jose Vicente, Lieven Orrantia, Verschaffel, 10.1348/000709907X178200British Journal of Educational Psychology. 7742007</p>
<p>What artificial neural networks can tell us about human language acquisition. Alex Warstadt, Samuel R Bowman, 10.1201/9781003205388-2/artificial-neural-networks-tell-us-human-language-acquisition-alex-warstadt-samuel-bowmanAlgebraic Structures in Natural Language. CRC Press2022</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, ; Zhou, Xuezhi Yang, Yifeng Wang, Hanxiao Lu, Liu, Denny Quoc V Le, Xinyun Zhou, Chen, The Twelfth International Conference on Learning Representations. 2022. 2024Advances in Neural Information Processing Systems</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7753 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7753</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7753</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-274776255</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.11427v1.pdf" target="_blank">Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Scientific discovery is a complex cognitive process that has driven human knowledge and technological progress for centuries. While artificial intelligence (AI) has made significant advances in automating aspects of scientific reasoning, simulation, and experimentation, we still lack integrated AI systems capable of performing autonomous long-term scientific research and discovery. This paper examines the current state of AI for scientific discovery, highlighting recent progress in large language models and other AI techniques applied to scientific tasks. We then outline key challenges and promising research directions toward developing more comprehensive AI systems for scientific discovery, including the need for science-focused AI agents, improved benchmarks and evaluation metrics, multimodal scientific representations, and unified frameworks combining reasoning, theorem proving, and data-driven modeling. Addressing these challenges could lead to transformative AI tools to accelerate progress across disciplines towards scientific discovery.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7753.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7753.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman (dataset / method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and method suite for symbolic regression in physics that includes a dataset of physics equations to be rediscovered from data, used to evaluate equation-discovery algorithms' ability to recover analytic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A physics-inspired method for symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>equation discovery / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rediscovery benchmark (AI Feynman dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide numeric datasets generated from known analytic physics expressions; an algorithm is evaluated by whether it can recover the original symbolic expression from the data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Rediscovery / recovery accuracy (exact symbolic match) and/or Pareto-front tradeoffs (accuracy vs complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Typically reported as percentage of target equations exactly recovered (symbolic equivalence), sometimes with Pareto metrics combining error and expression complexity; scale: 0–100% recovery rate or Pareto-optimality comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI Feynman dataset (120 physics equations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Focused on rediscovering known laws; vulnerable to methods that overfit or memorize training patterns; does not capture open-ended novelty or generalization beyond the provided equations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7753.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBench / Scibench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBench / Scibench (scientific problem-solving benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark intended to evaluate scientific problem-solving and domain knowledge capabilities of models, focusing on college-level scientific questions and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scibench: Evaluating college-level scientific problem-solving abilities of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general science / STEM</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>scientific question answering / problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Scientific question-answering benchmark (SciBench)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models answer curated scientific / textbook-style questions; performance is measured by correctness on held-out questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion of correct answers)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction or percentage of questions answered correctly on the benchmark dataset (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SciBench / Scibench</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Benchmarks of this form often focus on recovery of known solutions and may be vulnerable to memorization rather than genuine discovery or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7753.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScienceQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScienceQA (scientific question-answering benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and benchmark used to evaluate models on science question-answering tasks, often involving multi-step reasoning on scientific topics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>science education / multi-domain science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>question answering / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Science question-answering accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models are asked to answer standardized science questions (multiple-choice or open-response); performance measured by correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy / percent correct</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of questions answered correctly (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ScienceQA</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>May not capture novelty or the model's ability to generate new, derivable scientific hypotheses; susceptible to memorization of training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7753.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATH dataset (mathematical problem solving benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset measuring mathematical problem solving, used to evaluate models' ability to solve contest-style math problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring mathematical problem solving with the math dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>mathematical problem solving / theorem-style tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mathematical problem-solving accuracy (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models solve curated math problems; accuracy is determined by whether the final solution matches the reference solution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage of problems solved correctly)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction or percentage of problems for which the model's final answer matches the ground-truth solution (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Solving benchmark problems may rely on pattern-matching or memorized solution steps rather than genuine deductive discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7753.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert rating / domain-expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain expert involvement for benchmark design and evaluation (human expert rating)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Involving domain experts to assess plausibility, novelty, interpretability, alignment, and potential impact of AI-generated hypotheses and discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-domain (depends on task)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis / model / discovery assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert evaluation (plausibility/novelty/impact assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Domain experts review AI-generated outputs and rate their plausibility, novelty, interpretability, and potential scientific impact, often qualitatively or via rating scales.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Plausibility score, novelty score, interpretability rating, impact assessment (scales unspecified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Paper suggests assessing plausibility, novelty, and potential impact via expert judgment; specific scales are not specified (could be Likert scales, binary judgments, or ranked evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper recommends involving domain experts throughout benchmark design and evaluation but does not report concrete numbers (e.g., number of reviewers or inter-rater agreement not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Expert evaluation is subjective, resource-intensive, and may be inconsistent without clear protocols; the paper emphasizes the need to integrate experts to ensure scientific relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7753.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization / training-data extraction analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorization analysis and training-data extraction tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to detect whether LLM outputs reflect memorized training data rather than genuine reasoning, including tests that attempt to extract training-set content from model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting training data from large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>N/A (methodological evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of model authenticity / novelty</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Memorization / training-data extraction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply targeted prompts or extraction attacks to determine if model outputs reproduce verbatim or near-verbatim content from training data, quantifying memorization risk.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Extraction rate / proportion of outputs matching training data; measures of verbatim reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion (0–100%) of model responses that replicate training examples or sensitive content; may include n-gram overlap or exact match counts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Training corpus (varies); evaluation methods from Carlini et al.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Memorization can inflate apparent performance on benchmarks that overlap with training data, leading to overestimation of true discovery capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7753.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counterfactual task evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual-based evaluation to distinguish reasoning from reciting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation strategy that uses counterfactual or perturbed tasks to test whether models reason about problem structure or simply recite memorized patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / methodological</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of reasoning vs memorization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Counterfactual robustness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Introduce controlled perturbations or counterfactual changes to problems and measure model performance degradation; large drops suggest reliance on memorization rather than robust reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Performance delta (original vs counterfactual), robustness score</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Difference in accuracy or task-specific performance between baseline and counterfactual variants; reported as absolute or relative change (e.g., percentage point drop).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Custom counterfactual tasks (as in Wu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Designing meaningful counterfactuals that isolate reasoning vs memorization is challenging; results depend on the selection of perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7753.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7753.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Configurable simulated scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Configurable simulated scientific domains for novelty-focused benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed benchmark approach where the underlying laws and principles are systematically varied in simulated domains to evaluate discovery on novel, out-of-distribution scenarios rather than rediscovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>methodological / synthetic domains (any science simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novel-theory discovery evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Simulated-domain novelty benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Create parameterized simulation environments with controllable underlying laws; test models on unseen parameterizations to measure ability to discover new, non-memorized laws.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty-success rate, generalization to unseen laws, plausibility/derivability measures</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics could include percent of correctly inferred underlying rules in held-out simulated domains, measures of derivability/generalizability, and expert-rated novelty (paper does not prescribe exact scales).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Proposed configurable simulated scientific domains (not an existing named dataset in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Paper recommends expert involvement for assessing plausibility and novelty but gives no concrete protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Simulated domains may not capture the full complexity of real scientific data; bridging simulation-to-reality gap remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI Feynman: A physics-inspired method for symbolic regression. <em>(Rating: 2)</em></li>
                <li>Extracting training data from large language models. <em>(Rating: 2)</em></li>
                <li>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. <em>(Rating: 2)</em></li>
                <li>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset. <em>(Rating: 2)</em></li>
                <li>SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning. <em>(Rating: 1)</em></li>
                <li>Combining data and theory for derivable scientific discovery with AI-Descartes. <em>(Rating: 2)</em></li>
                <li>Llm-sr: Scientific equation discovery via programming with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7753",
    "paper_id": "paper-274776255",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "AI Feynman",
            "name_full": "AI Feynman (dataset / method)",
            "brief_description": "A benchmark and method suite for symbolic regression in physics that includes a dataset of physics equations to be rediscovered from data, used to evaluate equation-discovery algorithms' ability to recover analytic laws.",
            "citation_title": "AI Feynman: A physics-inspired method for symbolic regression.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics",
            "theory_type": "equation discovery / symbolic regression",
            "evaluation_method_name": "Rediscovery benchmark (AI Feynman dataset)",
            "evaluation_method_description": "Provide numeric datasets generated from known analytic physics expressions; an algorithm is evaluated by whether it can recover the original symbolic expression from the data.",
            "evaluation_metric": "Rediscovery / recovery accuracy (exact symbolic match) and/or Pareto-front tradeoffs (accuracy vs complexity)",
            "metric_definition": "Typically reported as percentage of target equations exactly recovered (symbolic equivalence), sometimes with Pareto metrics combining error and expression complexity; scale: 0–100% recovery rate or Pareto-optimality comparisons.",
            "dataset_or_benchmark": "AI Feynman dataset (120 physics equations)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Focused on rediscovering known laws; vulnerable to methods that overfit or memorize training patterns; does not capture open-ended novelty or generalization beyond the provided equations.",
            "uuid": "e7753.0",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SciBench / Scibench",
            "name_full": "SciBench / Scibench (scientific problem-solving benchmark)",
            "brief_description": "A benchmark intended to evaluate scientific problem-solving and domain knowledge capabilities of models, focusing on college-level scientific questions and tasks.",
            "citation_title": "Scibench: Evaluating college-level scientific problem-solving abilities of large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general science / STEM",
            "theory_type": "scientific question answering / problem solving",
            "evaluation_method_name": "Scientific question-answering benchmark (SciBench)",
            "evaluation_method_description": "Models answer curated scientific / textbook-style questions; performance is measured by correctness on held-out questions.",
            "evaluation_metric": "Accuracy (proportion of correct answers)",
            "metric_definition": "Fraction or percentage of questions answered correctly on the benchmark dataset (0–100%).",
            "dataset_or_benchmark": "SciBench / Scibench",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Benchmarks of this form often focus on recovery of known solutions and may be vulnerable to memorization rather than genuine discovery or reasoning.",
            "uuid": "e7753.1",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ScienceQA",
            "name_full": "ScienceQA (scientific question-answering benchmark)",
            "brief_description": "A dataset and benchmark used to evaluate models on science question-answering tasks, often involving multi-step reasoning on scientific topics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "science education / multi-domain science",
            "theory_type": "question answering / reasoning",
            "evaluation_method_name": "Science question-answering accuracy",
            "evaluation_method_description": "Models are asked to answer standardized science questions (multiple-choice or open-response); performance measured by correctness.",
            "evaluation_metric": "Accuracy / percent correct",
            "metric_definition": "Percentage of questions answered correctly (0–100%).",
            "dataset_or_benchmark": "ScienceQA",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "May not capture novelty or the model's ability to generate new, derivable scientific hypotheses; susceptible to memorization of training corpora.",
            "uuid": "e7753.2",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MATH",
            "name_full": "MATH dataset (mathematical problem solving benchmark)",
            "brief_description": "A benchmark dataset measuring mathematical problem solving, used to evaluate models' ability to solve contest-style math problems requiring multi-step reasoning.",
            "citation_title": "Measuring mathematical problem solving with the math dataset.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "mathematics",
            "theory_type": "mathematical problem solving / theorem-style tasks",
            "evaluation_method_name": "Mathematical problem-solving accuracy (MATH)",
            "evaluation_method_description": "Models solve curated math problems; accuracy is determined by whether the final solution matches the reference solution.",
            "evaluation_metric": "Accuracy (percentage of problems solved correctly)",
            "metric_definition": "Fraction or percentage of problems for which the model's final answer matches the ground-truth solution (0–100%).",
            "dataset_or_benchmark": "MATH",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Solving benchmark problems may rely on pattern-matching or memorized solution steps rather than genuine deductive discovery.",
            "uuid": "e7753.3",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Human expert rating / domain-expert evaluation",
            "name_full": "Domain expert involvement for benchmark design and evaluation (human expert rating)",
            "brief_description": "Involving domain experts to assess plausibility, novelty, interpretability, alignment, and potential impact of AI-generated hypotheses and discoveries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cross-domain (depends on task)",
            "theory_type": "hypothesis / model / discovery assessment",
            "evaluation_method_name": "Human expert evaluation (plausibility/novelty/impact assessment)",
            "evaluation_method_description": "Domain experts review AI-generated outputs and rate their plausibility, novelty, interpretability, and potential scientific impact, often qualitatively or via rating scales.",
            "evaluation_metric": "Plausibility score, novelty score, interpretability rating, impact assessment (scales unspecified in paper)",
            "metric_definition": "Paper suggests assessing plausibility, novelty, and potential impact via expert judgment; specific scales are not specified (could be Likert scales, binary judgments, or ranked evaluations).",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "Paper recommends involving domain experts throughout benchmark design and evaluation but does not report concrete numbers (e.g., number of reviewers or inter-rater agreement not specified).",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Expert evaluation is subjective, resource-intensive, and may be inconsistent without clear protocols; the paper emphasizes the need to integrate experts to ensure scientific relevance.",
            "uuid": "e7753.4",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Memorization / training-data extraction analysis",
            "name_full": "Memorization analysis and training-data extraction tests",
            "brief_description": "Methods to detect whether LLM outputs reflect memorized training data rather than genuine reasoning, including tests that attempt to extract training-set content from model outputs.",
            "citation_title": "Extracting training data from large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "N/A (methodological evaluation)",
            "theory_type": "evaluation of model authenticity / novelty",
            "evaluation_method_name": "Memorization / training-data extraction evaluation",
            "evaluation_method_description": "Apply targeted prompts or extraction attacks to determine if model outputs reproduce verbatim or near-verbatim content from training data, quantifying memorization risk.",
            "evaluation_metric": "Extraction rate / proportion of outputs matching training data; measures of verbatim reproduction",
            "metric_definition": "Proportion (0–100%) of model responses that replicate training examples or sensitive content; may include n-gram overlap or exact match counts.",
            "dataset_or_benchmark": "Training corpus (varies); evaluation methods from Carlini et al.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Memorization can inflate apparent performance on benchmarks that overlap with training data, leading to overestimation of true discovery capability.",
            "uuid": "e7753.5",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Counterfactual task evaluation",
            "name_full": "Counterfactual-based evaluation to distinguish reasoning from reciting",
            "brief_description": "An evaluation strategy that uses counterfactual or perturbed tasks to test whether models reason about problem structure or simply recite memorized patterns.",
            "citation_title": "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / methodological",
            "theory_type": "evaluation of reasoning vs memorization",
            "evaluation_method_name": "Counterfactual robustness evaluation",
            "evaluation_method_description": "Introduce controlled perturbations or counterfactual changes to problems and measure model performance degradation; large drops suggest reliance on memorization rather than robust reasoning.",
            "evaluation_metric": "Performance delta (original vs counterfactual), robustness score",
            "metric_definition": "Difference in accuracy or task-specific performance between baseline and counterfactual variants; reported as absolute or relative change (e.g., percentage point drop).",
            "dataset_or_benchmark": "Custom counterfactual tasks (as in Wu et al. 2023)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Designing meaningful counterfactuals that isolate reasoning vs memorization is challenging; results depend on the selection of perturbations.",
            "uuid": "e7753.6",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Configurable simulated scientific domains",
            "name_full": "Configurable simulated scientific domains for novelty-focused benchmarks",
            "brief_description": "A proposed benchmark approach where the underlying laws and principles are systematically varied in simulated domains to evaluate discovery on novel, out-of-distribution scenarios rather than rediscovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "methodological / synthetic domains (any science simulated)",
            "theory_type": "novel-theory discovery evaluation",
            "evaluation_method_name": "Simulated-domain novelty benchmark",
            "evaluation_method_description": "Create parameterized simulation environments with controllable underlying laws; test models on unseen parameterizations to measure ability to discover new, non-memorized laws.",
            "evaluation_metric": "Novelty-success rate, generalization to unseen laws, plausibility/derivability measures",
            "metric_definition": "Metrics could include percent of correctly inferred underlying rules in held-out simulated domains, measures of derivability/generalizability, and expert-rated novelty (paper does not prescribe exact scales).",
            "dataset_or_benchmark": "Proposed configurable simulated scientific domains (not an existing named dataset in paper)",
            "human_evaluation_details": "Paper recommends expert involvement for assessing plausibility and novelty but gives no concrete protocol.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Simulated domains may not capture the full complexity of real scientific data; bridging simulation-to-reality gap remains a challenge.",
            "uuid": "e7753.7",
            "source_info": {
                "paper_title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI Feynman: A physics-inspired method for symbolic regression.",
            "rating": 2,
            "sanitized_title": "ai_feynman_a_physicsinspired_method_for_symbolic_regression"
        },
        {
            "paper_title": "Extracting training data from large language models.",
            "rating": 2,
            "sanitized_title": "extracting_training_data_from_large_language_models"
        },
        {
            "paper_title": "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.",
            "rating": 2,
            "sanitized_title": "reasoning_or_reciting_exploring_the_capabilities_and_limitations_of_language_models_through_counterfactual_tasks"
        },
        {
            "paper_title": "Scibench: Evaluating college-level scientific problem-solving abilities of large language models.",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset.",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning.",
            "rating": 1,
            "sanitized_title": "sciglm_training_scientific_language_models_with_selfreflective_instruction_annotation_and_tuning"
        },
        {
            "paper_title": "Combining data and theory for derivable scientific discovery with AI-Descartes.",
            "rating": 2,
            "sanitized_title": "combining_data_and_theory_for_derivable_scientific_discovery_with_aidescartes"
        },
        {
            "paper_title": "Llm-sr: Scientific equation discovery via programming with large language models.",
            "rating": 1,
            "sanitized_title": "llmsr_scientific_equation_discovery_via_programming_with_large_language_models"
        }
    ],
    "cost": 0.012758,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges
16 Dec 2024</p>
<p>Chandan K Reddy reddy@cs.vt.edu 
Virginia Tech</p>
<p>Parshin Shojaee parshinshojaee@vt.edu 
Virginia Tech</p>
<p>Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges
16 Dec 2024780BAA87BE2B065846E4582B46DC0CB5arXiv:2412.11427v1[cs.LG]
Scientific discovery is a complex cognitive process that has driven human knowledge and technological progress for centuries.While artificial intelligence (AI) has made significant advances in automating aspects of scientific reasoning, simulation, and experimentation, we still lack integrated AI systems capable of performing autonomous long-term scientific research and discovery.This paper examines the current state of AI for scientific discovery, highlighting recent progress in large language models and other AI techniques applied to scientific tasks.We then outline key challenges and promising research directions toward developing more comprehensive AI systems for scientific discovery, including the need for science-focused AI agents, improved benchmarks and evaluation metrics, multimodal scientific representations, and unified frameworks combining reasoning, theorem proving, and data-driven modeling.Addressing these challenges could lead to transformative AI tools to accelerate progress across disciplines towards scientific discovery.</p>
<p>Introduction</p>
<p>Scientific discovery -the process of formulating and validating new concepts, laws, and theories to explain natural phenomena -is one of humanity's most intellectually demanding and impactful pursuits.For decades, AI researchers have sought to automate aspects of scientific reasoning and discovery.Early work focused on symbolic AI approaches to replicate the formation of scientific hypotheses and laws in symbolic forms (Segler, Preuss, and Waller 2018;Mac-Coll 1897).More recently, deep learning and large language models (LLMs) have shown promise in tasks like literature analysis and brainstorming (Ji et al. 2024;Lu et al. 2024;Si, Yang, and Hashimoto 2024), experiment design (Boiko et al. 2023;Arlt et al. 2024), hypothesis generation (Wang et al. 2024;Ji et al. 2024), and equation discovery (Shojaee et al. 2024b;Ma et al. 2024).</p>
<p>Despite this progress, we still lack AI systems capable of integrating the diverse cognitive processes involved in sustained scientific research and discovery.Most work has focused on narrow aspects of scientific reasoning in isolation.Developing more comprehensive AI discovery systems capable of supporting the full cycle of scientific in-Figure 1: Overview of the AI-driven scientific discovery framework.The cycle illustrates the iterative process of scientific inquiry.The framework begins with user-defined problem specifications, retrieves relevant scientific context from literature and databases, and utilizes generative AI systems to produce new hypotheses and experimental designs.These AI-generated concepts are then evaluated and refined through experimental observation, expert input, and scientific tools, driving further iterations of the discovery cycle.quiry -from context retrieval and hypothesis generation to experiment design and evaluation (Figure 1) -could dramatically accelerate progress across scientific disciplines.This paper examines the current state and future potential of generative AI for scientific discovery.We highlight recent advances, particularly in scientific understanding and discovery frameworks, while identifying critical gaps.We then outline key research challenges and directions towards more unified AI systems for discovery, including: (i) Creating improved benchmarks and evaluation frameworks for scientific discovery; (ii) Developing science-focused AI agents that leverage scientific knowledge and reasoning capabilities; (iii) Advancing multimodal scientific representations beyond text; and (iv) Unifying automated reasoning, theorem proving, and data-driven modeling.By tackling these challenges, the AI and Science community can work towards systems that serve as collaborative partners to human scientists, accelerating the pace of discovery in science.</p>
<p>Recent Advances in AI for Scientific Tasks</p>
<p>The past decade has witnessed remarkable progress in applying AI to various scientific tasks.This section highlights some of the most significant recent advances, demonstrating AI's growing capabilities in supporting and accelerating scientific discovery across multiple disciplines.</p>
<p>Literature Analysis and Brainstorming</p>
<p>The exponential growth of scientific publications has made it increasingly challenging for researchers to stay abreast of developments in their fields.Large language models (LLMs) pre-trained on vast scientific corpora have emerged as powerful tools to address this challenge, enhancing literature analysis and interaction.Researchers have developed specialized LLMs for various scientific domains.Models like PubMedBERT (Gu et al. 2021) and BioBERT (Lee et al. 2020) focus on biomedical literature, while SciBERT (Beltagy, Lo, and Cohan 2019) covers a broader range of scientific disciplines.More recent models such as BioGPT (Luo et al. 2022) and SciGLM (Zhang et al. 2024) have further pushed the boundaries of scientific language modeling, incorporating advanced architectures and training techniques.These models, trained on sources like PubMed and arXiv, excel at literature information retrieval, summarization, and question-answering.They enable efficient navigation of scientific knowledge by quickly finding relevant papers, distilling key findings, and synthesizing information to answer complex queries.</p>
<p>Beyond analysis, recent works demonstrate LLMs' potential in generating novel scientific insights.For instance, SciMON (Ji et al. 2024) uses LLMs to generate new scientific ideas by analyzing patterns in the existing literature.These advancements show AI's capacity to not only aid in literature review but also contribute to identifying promising and novel research directions, potentially accelerating scientific discovery.</p>
<p>Theorem Proving</p>
<p>Automated theorem proving has recently gained attention in AI for science research due to its fundamental role in scientific reasoning.Recent years have seen remarkable progress in this field, particularly through the integration of LLMs with formal reasoning systems.The GPT-f framework (Polu and Sutskever 2020) pioneered this approach by training transformer-based language models on proof tactics, enabling navigation through complex mathematical proofs with the help of learned priors.Building on this, researchers have integrated proving techniques with LLMs and developed enhancements such as data augmentation (Han et al. 2021), retrieval augmentation (Yang et al. 2024), and novel proof search methods (Lample et al. 2022;Wang et al. 2023b).One of the key enhancements is the autoformalization approach, exemplified by the Draft-Sketch-Prove method (Jiang et al. 2023).This method uses LLMs to first draft informal proofs, translate them into formal sketches, and then complete proofs with additional proof assistant tools (Böhme and Nipkow 2010), mimicking the human process of moving from intuitive understanding to rigorous proof.As these systems become more adept at formalizing and proving complex statements, they could be applied to derive scientific theories, potentially accelerating the scientific process and leading to enhancements in fields where theoretical understanding lags behind empirical methods.</p>
<p>Experimental Design</p>
<p>Experimental design is a critical component of the scientific process, often requiring extensive domain knowledge and creative thinking.The automation of this process through generative models has the potential to accelerate scientific discovery across various fields.By leveraging LLM agents, researchers are recently developing systems that can design, plan, optimize, and even execute scientific experiments with minimal human intervention.These tools are particularly valuable in fields where experimental setup is costly, allowing researchers to explore a wider range of possibilities before physical implementation.For example, in physics, LLM-driven systems have demonstrated effectiveness in designing complex quantum experiments (Arlt et al. 2024) and optimizing parameters in high-energy physics simulations (Cai et al. 2024;Baldi, Sadowski, and Whiteson 2014).Chemistry has also recently seen advancements in automated experimentation, with LLM agent systems capable of designing and optimizing chemical reactions (M.Bran et al. 2024).Moreover, in biology and medicine, LLMdriven experimental design has shown promise in optimizing gene-editing protocols (Huang et al. 2024), and designing more effective clinical trials (Singhal et al. 2023).These AIdriven approaches to experimental design allow researchers to tackle more complex problems and explore hypotheses that might otherwise be impractical due to time or resource constraints.</p>
<p>Data-driven Discovery</p>
<p>Data-driven discovery has become a cornerstone of modern scientific research, leveraging the ever-growing volumes of experimental, observational, and synthetic data to uncover new patterns, relationships, and laws.This paradigm shift has been particularly transformative in fields where complex systems and high-dimensional data are prevalent.</p>
<p>In drug discovery, data-driven approaches have significantly accelerated the identification of potential therapeutic compounds.For instance, recent works employed generative (Mak, Wong, and Pichika 2023; Callaway 2024) and multimodal representation learning (Gao et al. 2024) models to discover a novel antibiotic, effective against a wide range of bacteria, by searching and screening millions of molecules in the representation space (Gao et al. 2024).These enhancements demonstrate the power of AI in exploring vast chemical spaces that would be infeasible to search manually or in the huge and infinite combinatorial space of molecules.</p>
<p>Equation discovery, commonly known as symbolic regression, is a data-driven task for uncovering mathematical expressions from data.Early neural methods like AI Feynman (Udrescu and Tegmark 2020) demonstrated the ability to rediscover fundamental physics laws from data alone, while later work incorporated physical constraints and structures for more interpretable models (Cranmer et al. 2020b).The advent of language modeling and representation learning brought new possibilities.Transformer-based language models, adapted for symbolic regression, treat equation discovery as a numeric-to-symbolic generation task (Biggio et al. 2021;Kamienny et al. 2022).These approaches have been enhanced with search techniques during decoding (Landajuela et al. 2022;Shojaee et al. 2024a), although challenges remain in effectively encoding and tokenizing numeric data (Golkar et al. 2023).Recent works like the SNIP model (Meidani et al. 2024) have also explored multi-modal representation learning between symbolic expressions and numeric data, moving the equation discovery search to a lower-dimensional and smoother representation space for more effective and efficient search.Recently, LLM-SR (Shojaee et al. 2024b) also demonstrated the potential of using LLMs as scientist agents in the evolutionary search for equation discovery.These advancements highlight the evolving landscape of equation discovery, with significant potential for further improvements in integrating numeric data with AI models and leveraging the mathematical reasoning capabilities of advanced LLMs.</p>
<p>In materials discovery, data-driven approaches have led to the prediction and subsequent synthesis of novel materials with desired properties (Pyzer-Knapp et al. 2022;Merchant et al. 2023;Miret and Krishnan 2024).Large generative models have shown remarkable success in generating novel structures.For instance, Merchant et al. ( 2023) introduced Graph Networks for Materials Exploration (GNoME), leading to the discovery of new stable materials.This approach represents an order-of-magnitude increase in known stable crystals, showcasing the potential of AI in expanding our materials knowledge base.LLMs have also been recently used to extract information from scientific literature in material science, generate novel material compositions, and guide experimental design (Miret and Krishnan 2024).For example, the AtomAgents (Ghafarollahi and Buehler 2024a) demonstrates how LLMs can be integrated into the material discovery pipeline, significantly improving the process in alloy design.By combining the pattern-recognition and representation learning capabilities with the reasoning and generalization abilities of advanced AI models, we are moving towards systems that can not only analyze existing data but also propose novel hypotheses for data-driven discoveries across scientific disciplines.</p>
<p>Key Challenges and Research Opportunities Benchmarks for Scientific Discovery</p>
<p>First and foremost, evaluating AI systems for open-ended scientific discovery poses unique challenges compared to typical machine learning benchmarks.This challenge is particularly acute for large language models (LLMs) and other foundation models capable of storing and potentially "memorizing" vast amounts of scientific knowledge (Brown 2020; Bommasani et al. 2021) in their parameters.Many existing benchmarks in the field of scientific discovery only focus on rediscovering known scientific laws or solving textbookstyle problems.For instance, the AI Feynman dataset consists of 120 physics equations to be rediscovered from data (Udrescu and Tegmark 2020;Udrescu et al. 2020), while datasets like SciBench (Wang et al. 2023c), ScienceQA (Lu et al. 2022), andMATH (Hendrycks et al. 2021) primarily evaluate scientific question answering and mathematical problem-solving abilities.</p>
<p>However, these benchmarks may not capture the entire complexity of scientific discovery processes.More critically, they may be vulnerable to reciting or memorization by large language models, potentially leading to overestimation of true discovery capabilities (Carlini et al. 2021;Shojaee et al. 2024b).As (Wu et al. 2023) points out, LLMs can often solve scientific problems by pattern matching against memorized knowledge rather than through genuine reasoning or discovery.This concern is further emphasized by studies showing that LLMs can reproduce significant portions of their training data (Carlini et al. 2022).There is a pressing need for richer benchmarks and evaluation frameworks in this research area to better understand the gap between baselines and recent methods and to identify areas for improvement.Key directions include:</p>
<p>• Developing benchmark datasets focused on novel scientific discovery rather than recovery: One promising approach is to create configurable simulated scientific domains where the underlying laws and principles can be systematically varied.This would allow testing discovery capabilities on new scenarios, mitigating the risk of models simply reciting memorized information observed in their training data.For example, (M.ibility with existing scientific theories (Liu et al. 2024b).</p>
<p>• Involving domain experts in benchmark design and evaluation: The involvement of domain experts is crucial for developing meaningful benchmarks and evaluating AI-driven scientific discoveries.Experts can contribute in various aspects of the discovery process such as assessing the plausibility, novelty, and potential impact of AI-generated hypotheses; evaluating the interpretability and alignment of AI-discovered laws or models with human-understandable scientific principles; and providing feedback during the AI-driven discovery process for human-AI collaborative discovery.By integrating domain expert involvement throughout the benchmark development, discovery, and evaluation process, we can ensure that advancements in AI-driven scientific discovery are both technically sound and aligned with the needs and standards of the scientific community.</p>
<p>Science-Focused Agents</p>
<p>Current work on scientific AI often treats models as passive tools rather than active agents pursuing discovery.There is a growing need to develop science-focused AI agents (Figure 2) that can leverage broad scientific knowledge, engage in reasoning, and autonomously verify their reasoning and hypotheses.Recently, LLMs have shown impressive capabilities in knowledge retrieval and reasoning (Huang and Chang 2023), making them promising candidates for developing such agents.These agents can integrate vast amounts of scientific knowledge embedded in LLMs, generate educated hypotheses, design experiments, verify their designs, and interpret the results.Also, their ability to interface with external tools and experimental data sources with the programming execution gate allows for real-world experimentation and validation.Recent work has demonstrated the potential of LLM-based agents in scientific domains.For example, (M.Bran et al. 2024) introduced ChemCrow, an LLM-augmented system for chemistry research.ChemCrow integrates GPT-4 with domain-specific tools for tasks such as reaction prediction, retrosynthesis planning, and safety assessment.This integration allows the system to reason about chemical processes and validate the hypotheses using specialized chemical tools.Similarly, (Ghafarollahi and Buehler 2024a) developed AtomAgents, a multi-agent system for alloy design and discovery.SciAgents (Ghafarollahi and Buehler 2024b) also uses multiple AI agents, each specializing in different aspects of materials science, to collaboratively design new bio-materials.The system incorporates physics-aware constraints and can interface with simulation tools to validate its predictions.However, developing effective science-focused agents also presents several challenges:</p>
<p>• Domain-specific tool integration: Effective scientific agents require integration with specialized scientific tools and domain-specific knowledge.This challenge arises from the highly specialized nature of scientific instruments and methodologies, which are often underrepresented in LLMs' training data.(Bubeck et al. 2023) demonstrated that while LLMs like GPT-4 excel in general academic tasks, they struggle with specialized scientific reasoning, particularly in physics and chemistry.Potential research directions include developing modular architectures for integrating domain-specific knowledge bases and tool interfaces, and fine-tuning LLMs on curated scientific datasets.These approaches could enable LLMs to access domain-specific knowledge and interact effectively with specialized scientific tools, enhancing their capabilities in this setting.</p>
<p>• Adaptive experimental design and hypothesis evolution:</p>
<p>A significant challenge in scientific-focused agents is developing systems capable of long-term, iterative scientific investigations.Such agents must design experi-ments, interpret results, and refine hypotheses over extended periods while maintaining scientific rigor and avoiding biases.This challenge stems from the complex, multi-stage nature of scientific inquiry, which often involves repeated cycles of experimentation, analysis, and hypothesis adjustment.Potential research directions to address this challenge include meta-learning frameworks enabling agents to improve experimental design and hypothesis refinement strategies across multiple investigations; and hierarchical planning algorithms for managing both short-term experimental steps and long-term scientific discovery objectives.</p>
<p>Multi-modal Scientific Representations</p>
<p>The landscape of scientific data is vast and diverse, encompassing far more than just textual information.While recent advancements in language models have significantly boosted our ability to process and reason with scientific literature, we must recognize that the majority of scientific data exists in forms quite different from natural language.</p>
<p>From microscopy images to genomic sequences, from time series sensor data to structured databases and mathematical laws, scientific knowledge is inherently multi-modal (Topol 2023;Wang et al. 2023a).This diversity presents both challenges and opportunities for AI-driven scientific discovery.The challenge lies in developing integrated representation learning techniques that can effectively capture and unify these varied scientific data types.The opportunity, however, is immense: by creating AI systems capable of reasoning across these diverse modalities, we can accelerate scientific discovery in unprecedented ways.</p>
<p>Representation learning offers the potential to distill complex, high-dimensional scientific data into more manageable continuous and low-dimensional forms.This is particularly crucial in scientific domains where high-quality data is limited or expensive to obtain through scientific experiments.By learning multi-modal robust representations with the help of pre-training techniques and synthetic simulation data, we can make more efficient use of limited data, potentially reducing the need for costly scientific experiments and accelerating the pace of discovery.Key directions in this line of research include:</p>
<p>• Cross-modal scientific representation learning: Recent work has shown promising results in learning pre-trained joint representations across modalities for different sci-entific tasks.Notable successes include DrugCLIP (Gao et al. 2024) for joint representations of molecules and protein pockets in drug discovery, Text2Mol (Edwards, Zhai, and Ji 2021) bridging natural language and molecular structures, ProtST (Xu et al. 2023) unifying protein sequences and biomedical text in proteomics, and SNIP (Meidani et al. 2024) linking mathematical expressions with numeric data.These advances demonstrate the potential of cross-modal learning to enhance scientific tasks by leveraging complementary information across modalities.Despite these promising results, significant research opportunities remain (i) Expanding cross-modal representation learning to diverse and new scientific domains, (ii) Enhancing representation quality through recent integrated self-supervised and multi-modal pre-training; and (iii) Developing unified, modality-agnostic frameworks adaptable to heterogeneous scientific data types.</p>
<p>• Latent space scientific hypothesis search: Many scientific discovery tasks involve searching through vast, combinatorial spaces of candidates.Current approaches to these problems often rely on evolutionary search or heuristic methods, which can be computationally expensive and inefficient (Sadybekov and Katritch 2023;Schmidt and Lipson 2009).Recent advances in representation learning offer a promising alternative: conducting scientific hypothesis optimization in learned latent spaces.By moving the search process into the latent space, we can potentially make the exploration of the hypothesis space more efficient and effective.This approach has shown potential across various domains, from drug discovery (Gao et al. 2024) to equation discovery (Meidani et al. 2024), molecular design (Abeer et al. 2024;Zheng, Li, and Zhang 2023), and protein engineering (Castro et al. 2022;Jumper et al. 2021).This emerging research direction has significant potential for scientific discovery.Future research avenues include (i) Integrating domain expert knowledge or feedback into the representations and discovery process, (ii) Enhancing interpretability of representations for scientific validation, and (iii) Advancing optimization techniques for nontrivial discovery objectives and more flexible hypothesis search in the latent space.</p>
<p>• Multi-modal scientific reasoning frameworks: The advancement of AI-driven scientific discovery hinges on developing systems capable of multi-modal scientific reasoning.Recent works have shown promising results in this direction.For example, multi-modal retrieval augmented generation (RAG) systems have demonstrated potential in leveraging LLMs for scientific discovery (Park et al. 2024).Models like GIT-Mol (Liu et al. 2024a) showcase the integration of visual, textual, and graph reasoning for molecular discovery.In materials science, approaches combining textual reasoning with structural data have also shown promise in predicting material properties and guiding synthesis (Miret and Krishnan 2024)</p>
<p>Theory and Data Unification</p>
<p>Scientific discovery typically involves a complex interplay between theoretical reasoning, empirical observation, and mathematical modeling.However, most existing AI approaches to scientific tasks focus on just one of these aspects.There is a pressing need for unified frameworks that integrate logical and mathematical reasoning, formal theorem proving, data-driven modeling, experimental design, and causal inference.This integration is challenging but critical for capturing the full scientific discovery process.Recent advances in LLMs have shown promising results in both theorem-proving and data-driven scientific modeling.</p>
<p>For instance, LLMs have demonstrated promising capabilities in automated theorem-proving and formal mathematical derivations from natural language problems (Yang et al. 2024;Jiang et al. 2023).On the data-driven side, (Shojaee et al. 2024b;Ma et al. 2024) have shown success in discovering equation hypotheses from data with the help of LLMbased program search.However, these approaches largely operate in isolation, and there is a significant gap in unifying these capabilities to mirror the holistic nature of scientific inquiry.Key challenges and research directions include:</p>
<p>• Generating derivable hypotheses from empirical observations: Developing methods that can not only discover patterns in data but also produce rigorous mathematical derivations of these findings is crucial for ensuring the reliability and generalizability of AI-driven scientific discoveries to out-of-distribution data.Derivable theoretical results provide a level of confidence and understanding that goes beyond mere empirical correlation.) has made progress in this direction.However, significant challenges remain for the use of these approaches in scientific discovery, including scalability to large-scale scientific problems, and expressiveness to capture complex scientific theories in specific scientific domains.</p>
<p>Conclusion</p>
<p>Developing unified AI systems for scientific discovery is an ambitious goal, but one with substantial potential impact.Success could dramatically accelerate progress across diverse scientific disciplines.This paper has outlined current progress as well as several key research challenges and opportunities toward this vision, including developing science-focused AI agents, creating improved benchmarks, advancing multimodal representations, and unifying diverse modes of scientific reasoning.Tackling these challenges will require collaboration between AI researchers, scientists across domains, and philosophers of science.While fully autonomous AI scientists may still be far off, nearer-term progress could produce powerful AI assistants to augment human scientific capabilities.Such tools could help scientists navigate the ever-growing scientific literature, brainstorm ideas, generate novel hypotheses, design experiments, and find unexpected patterns in complex experimental data.</p>
<p>By pursuing this research agenda, the machine learning and AI community has an opportunity to develop systems that do not just automate product-related tasks, but actively push forward the frontiers of human scientific knowledge.The path will be challenging, but the potential rewards -both scientific and technological -are immense.</p>
<p>Figure 2 :
2
Figure 2: A comprehensive framework for science-focused AI agents.The diagram illustrates a ⃝ the multi-modal nature of scientific data, b⃝ the inputs for scientific tasks, c ⃝ the key actions performed by AI agents in scientific discovery, and d ⃝ the evaluation metrics for assessing scientific outcomes.This framework highlights the integration of diverse data sources, AIdriven tools, and human experts in advancing scientific research and discovery processes.</p>
<p>However, integrating logical reasoning and data-driven frameworks that are adaptable across scientific discovery tasks still remains an open challenge.Research opportunities exist to automate proof verification, incorporate expert feedback, and embed derivability constraints in data-driven discovery algorithms.• Combining symbolic and neural approaches: How can we effectively integrate the strengths of symbolic reasoning (e.g., logical deduction, formal proofs) with the flexibility and learning capabilities of neural networks?Recent work on neuro-symbolic AI (Garcez and Lamb 2023; Sheth, Roy, and Gaur 2023) provides promising directions, but challenges remain in scaling these approaches to more complex settings and scientific tasks.Developing hybrid architectures that can transition between symbolic and neural representations is helpful in capturing the full spectrum of scientific reasoning.• Reasoning discovery uncertainty in formal frameworks: Scientific discoveries often involve uncertainties and probabilities, yet formal logical frameworks struggle to incorporate these aspects.Developing frameworks that can handle probabilistic reasoning while maintaining rigorous deduction capabilities is crucial for advancing AIdriven scientific discovery.Recent work, such as probabilistic logic systems (De Raedt and Kimmig 2015; De Raedt, Kimmig, and Toivonen 2007), and neurosymbolic programming (Ahmed et al. 2022</p>
<p>Recent work, such as the AI-Descartes system (Cornelio et al. 2023), has shown promise by combining equation discovery tools (known as symbolic regression) with automated logical reasoning.</p>
<p>Multi-objective latent space optimization of generative molecular design models. Patterns. A N Abeer, N M Urban, M R Weil, F J Alexander, B.-J Yoon, K Ahmed, S Teso, K.-W Chang, G Van Den Broeck, A Vergari, Advances in Neural Information Processing Systems. 2024. 202235Semantic probabilistic layers for neurosymbolic learning</p>
<p>S Arlt, H Duan, F Li, S M Xie, Y Wu, M Krenn, arXiv:2406.02470Meta-Designing Quantum Experiments with Language Models. 2024arXiv preprint</p>
<p>Searching for exotic particles in high-energy physics with deep learning. P Baldi, P Sadowski, D Whiteson, I Beltagy, K Lo, A Cohan, arXiv:1903.10676SciBERT: A pretrained language model for scientific text. 2014. 201954308arXiv preprint</p>
<p>Science in the age of large language models. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, A Birhane, A Kasirzadeh, D Leslie, S Wachter, International Conference on Machine Learning. 2021. 20235Neural symbolic regression that scales</p>
<p>Sledgehammer: judgement day. S Böhme, T Nipkow, Automated Reasoning: 5th International Joint Conference, IJCAR 2010. Edinburgh, UKSpringer2010. July 16-19, 20105</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Language models are few-shot learners. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258arXiv:2005.14165On the opportunities and risks of foundation models. Brown, T. B.2021. 2020arXiv preprint</p>
<p>Transforming the bootstrap: Using transformers to compute scattering amplitudes in planar n= 4 super yang-mills theory. S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2024arXiv preprintMachine Learning: Science and Technology</p>
<p>Major AlphaFold upgrade offers boost for drug discovery. E Callaway, Nature. 62980122024</p>
<p>Erlingsson, U.; et al. 2021. Extracting training data from large language models. N Carlini, D Ippolito, M Jagielski, K Lee, F Tramer, C Zhang, N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, arXiv:2202.0764630th USENIX Security Symposium (USENIX Security 21. 2022arXiv preprintQuantifying memorization across neural language models</p>
<p>Transformerbased protein generation with regularized latent space optimization. E Castro, A Godavarthi, J Rubinfien, K Givechian, D Bhaskar, S Krishnaswamy, Nature Machine Intelligence. 4102022</p>
<p>A Chen, Z Wang, K L L Vidaurre, Y Han, S Ye, K Tao, S Wang, J Gao, J Li, arXiv:2403.12982Knowledge-Reuse Transfer Learning Methods in Molecular and Material Science. 2024arXiv preprint</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. C Cornelio, S Dash, V Austel, T R Josephson, J Goncalves, K L Clarkson, N Megiddo, B El Khadir, L Horesh, Nature Communications. 14117772023</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, S Greydanus, S Hoyer, P Battaglia, D Spergel, S Ho, M Cranmer, A Sanchez Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, arXiv:2003.04630Lagrangian neural networks. 2020a. 2020b33arXiv preprint</p>
<p>Probabilistic (logic) programming concepts. L De Raedt, A Kimmig, Machine Learning. 2015100</p>
<p>ProbLog: a probabilistic prolog and its application in link discovery. L De Raedt, A Kimmig, H ; Toivonen, C Zhai, H Ji, Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07. the 20th International Joint Conference on Artifical Intelligence, IJCAI'07San Francisco, CA, USA; Edwards, CMorgan Kaufmann Publishers Inc2007. 2021Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</p>
<p>Drugclip: Contrasive proteinmolecule representation learning for virtual screening. B Gao, B Qiang, H Tan, Y Jia, M Ren, M Lu, J Liu, W.-Y Ma, Y Lan, Advances in Neural Information Processing Systems, 36. Garcez, A. d.; and Lamb, L. C. 2023. Neurosymbolic AI: The 3 rd wave. 202456</p>
<p>AtomAgents: Alloy design and discovery through physics-aware multimodal multi-agent artificial intelligence. A Ghafarollahi, M J Buehler, A Ghafarollahi, M J Buehler, S Golkar, M Pettee, M Eickenberg, A Bietti, M Cranmer, G Krawezik, F Lanusse, M Mccabe, R Ohana, L Parker, arXiv:2407.10022arXiv:2310.029892024a. 2024barXiv preprintSciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. et al. 2023. xval: A continuous number encoding for large language models</p>
<p>Domainspecific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, T Naumann, J Gao, H Poon, ACM Transactions on Computing for Healthcare. 312021</p>
<p>Proof artifact co-training for theorem proving with language models. J M Han, J Rute, Y Wu, E W Ayers, S Polu, D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2102.06203arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021. 2021arXiv preprint</p>
<p>Towards Reasoning in Large Language Models: Survey, Implication, and Reflection. J Huang, K C Chang, -C, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Crispr-GPT: An LLM agent for automated design of gene-editing experiments. K Huang, Y Qu, H Cousins, W A Johnson, D Yin, M Shah, D Zhou, R Altman, M Wang, L Cong, arXiv:2404.180212024arXiv preprint</p>
<p>SCIMON: Scientific Inspiration Machines Optimized for Novelty. H Ji, Q Wang, D Downey, T Hope, ACL Anthology: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. 20241University of Illinois Urbana-Champaign/CABBI</p>
<p>Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs. A Q Jiang, S Welleck, J P Zhou, T Lacroix, J Liu, W Li, M Jamnik, G Lample, Y Wu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, Advances in Neural Information Processing Systems. 59678732021. 2022nature</p>
<p>Hypertree proof search for neural theorem proving. G Lample, T Lacroix, M.-A Lachaux, A Rodriguez, A Hayat, T Lavril, G Ebner, X Martinet, Advances in neural information processing systems. 202235</p>
<p>A unified framework for deep symbolic regression. M Landajuela, C S Lee, J Yang, R Glatt, C P Santiago, I Aravena, T Mundhenk, G Mulcahy, B K Petersen, Advances in Neural Information Processing Systems. 202235</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinformatics. 3642020</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, J Tao, Z Ren, Computers in biology and medicine. 1711080732024a</p>
<p>Z Liu, Y Wang, S Vaidya, F Ruehle, J Halverson, M Soljačić, T Y Hou, M Tegmark, ; Kan, C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2404.19756arXiv:2408.06292The ai scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprintKolmogorov-arnold networks</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in bioinformatics. 2364092022</p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O Baldassari, C White, A D Schwaller, P , Nature Machine Intelligence. 2024</p>
<p>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery. P Ma, T.-H Wang, M Guo, Z Sun, J B Tenenbaum, D Rus, C Gan, W Matusik, R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, F Berkenkamp, Proceedings of the 41st International Conference on Machine Learning. K.-K Mak, Y.-H Wong, M R Pichika, the 41st International Conference on Machine Learning2024. 2023235Proceedings of Machine Learning Research</p>
<p>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. K Meidani, P Shojaee, C K Reddy, A B Farimani, A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, The Twelfth International Conference on Learning Representations. 2024. 2023624Scaling deep learning for materials discovery</p>
<p>S Miret, N Krishnan, arXiv:2402.05200Are LLMs Ready for Real-World Materials Discovery?. 2024arXiv preprint</p>
<p>Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design. N H Park, T J Callahan, J L Hedrick, T Erdmann, S Capponi, S Polu, I Sutskever, arXiv:2408.11793arXiv:2009.03393Generative language modeling for automated theorem proving. 2024. 2020arXiv preprint</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, J W Pitera, P W Staar, S Takeda, T Laino, D P Sanders, J Sexton, J R Smith, A Curioni, Computational Materials. 81842022</p>
<p>Computational approaches streamlining drug discovery. A V Sadybekov, V Katritch, Nature. 61679582023</p>
<p>Symbolic regression of implicit equations. M Schmidt, H Lipson, Genetic programming theory and practice VII. Springer2009</p>
<p>Planning chemical syntheses with deep neural networks and symbolic AI. M H Segler, M Preuss, M P Waller, Nature. 55576982018</p>
<p>Neurosymbolic artificial intelligence (why, what, and how). A Sheth, K Roy, M Gaur, IEEE Intelligent Systems. 3832023</p>
<p>Transformer-based planning for symbolic regression. P Shojaee, K Meidani, A Barati Farimani, C Reddy, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, arXiv:2404.184002024barXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. C Si, D Yang, T Hashimoto, K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, arXiv:2409.04109Nature. 62079722024. 2023arXiv preprintLarge language models encode clinical knowledge</p>
<p>As artificial intelligence goes multimodal, medical applications multiply. E J Topol, 2023</p>
<p>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. S.-M Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems. 202033</p>
<p>AI Feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science Advances. 61626312020</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 62079722023a</p>
<p>Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function. H Wang, Y Yuan, Z Liu, J Shen, Y Yin, J Xiong, E Xie, H Shi, Y Li, L Li, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b1</p>
<p>Hypothesis Search: Inductive Reasoning with Language Models. R Wang, E Zelikman, G Poesia, Y Pu, N Haber, N Goodman, X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635Scibench: Evaluating college-level scientific problem-solving abilities of large language models. 2024. 2023carXiv preprintThe Twelfth International Conference on Learning Representations</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, X Yuan, S Miret, J Tang, arXiv:2307.02477International Conference on Machine Learning. PMLR2023. 2023arXiv preprintProtst: Multimodality learning of protein sequences and biomedical texts</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. K Yang, A Swope, A Gu, R Chalamala, P Song, S Yu, S Godil, R J Prenger, A Anandkumar, Advances in Neural Information Processing Systems. 202436</p>
<p>Desirable molecule discovery via generative latent space exploration. D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang, Y Yue, Y Dong, J Tang, W Zheng, J Li, Y Zhang, arXiv:2401.07950SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning. 2024. 20237</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7320 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7320</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7320</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-269293452</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.13671v2.pdf" target="_blank">FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization</a></p>
                <p><strong>Paper Abstract:</strong> Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing"normal"or"abnormal"semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of"abnormal"often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset. Code is available at https://github.com/CASIA-IVA-Lab/FiLo.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7320",
    "paper_id": "paper-269293452",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0046915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization
26 Jul 2024</p>
<p>Zhaopeng Gu guzhaopeng2023@ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Bingke Zhu bingke.zhu@nlpr.ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Objecteye Inc
BeijingChina</p>
<p>Guibo Zhu gbzhu@nlpr.ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Yingying Chen yingying.chen@nlpr.ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>Objecteye Inc
BeijingChina</p>
<p>Hao Li 
Central South University
HunanChina</p>
<p>Ming Tang tangm@nlpr.ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Jinqiao Wang jqwang@nlpr.ia.ac.cn 
Foundation Model Research Center
Institute of Automation
Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Objecteye Inc
BeijingChina</p>
<p>FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization
26 Jul 20245038EB8A527DECAED583DBE6D5516A6AarXiv:2404.13671v2[cs.CV]Grid crookedcracksexcessive gapsdiscolorationdeformationmissinginconsistent spacing between grid elementscorrosionvisible signschipping Leather scratchesdiscolorationcreasesuneven texturetearsbrittlenessdamageseamsheat damagemold Tile chippedirregularitiesdiscolorationefflorescencewarpingmissingdepressionslippagefungusdamage
Zero-shot anomaly detection (ZSAD) methods detect anomalies without prior access to known normal or abnormal samples within target categories.Existing methods typically rely on pretrained multimodal models, computing similarities between manually crafted textual features representing "normal" or "abnormal" semantics and image patch features to detect anomalies.However, the generic descriptions of "abnormal" often fail to precisely match diverse types of anomalies across different object categories.Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales.To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc).FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection.HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes.Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-theart performance with an image-level AUC of 83.9% and * Work done as intern in CASIA.Anomaly Detection Anomaly Localization abnormal normal 0.45 0.55 cos sim a pixel-level AUC of 95.9% on the VisA dataset.Code is available at https://github.com/CASIA-IVA-Lab/FiLo.</p>
<p>Introduction</p>
<p>The anomaly detection task aims to identify whether industrial products contain abnormalities or defects and locate the abnormal regions within the samples, which plays a crucial role in product quality control and safety monitoring.Traditional anomaly detection methods [5,6,30,34] typically require a large number of normal samples for model training.While performing well in some scenarios, they often fail in situations requiring protection of user data privacy or when applied to new production lines.Zero-Shot Anomaly Detection (ZSAD) has emerged as a research direction tailored to such scenarios, aiming to perform anomaly detection tasks without prior data on the target item categories, demanding high generalization ability from the model.</p>
<p>Multimodal pre-trained models [17,18,28] have recently demonstrated strong zero-shot recognition capabilities in various visual tasks.Many works have sought to leverage the vision-language comprehension ability of multimodal pre-trained models for ZSAD tasks, such as WinCLIP [16], APRIL-GAN [4], and AnomalyGPT [15].These methods assess whether an image contains anomalies by computing the similarity between image features and manually crafted textual features representing "normal" and "abnormal" semantics.They also localize abnormal regions by calculating the similarity between the image patch features and the textual features.While these approaches partly address the challenges of ZSAD, they encounter issues in both anomaly detection and localization.The generic "abnormal" descriptions fail to precisely match the diverse types of anomalies across different object categories.Moreover, computing feature similarity for individual patches struggles to precisely locate abnormal regions of varying sizes and shapes.To tackle these issues, we propose FiLo (Fine-Grained Description and High-Quality Localization), which addresses the shortcomings of existing ZSAD methods through adaptively learned Fine-Grained Description (FG-Des) and High-Quality Localization (HQ-Loc), as depicted in Figure 1.</p>
<p>Concerning anomaly detection, manually crafted abnormal descriptions typically employ generic terms such as "damaged" or "defect" [4,15,16], which do not adequately capture the specific types of anomalies present across different object categories.Furthermore, existing methods' text prompt templates like A xxx photo of xxx.are primarily designed for foreground object classification tasks and may not be suitable for identifying normal and abnormal parts within objects.In FG-Des, we first leverage the capabilities of Large Language Models (LLMs) to generate fine-grained anomaly types for each object category, replacing generic abnormal descriptions with specific anomaly content that matches the anomaly samples better.Next, we utilize learnable text vectors instead of manually crafted sentence templates and embed the detailed anomaly content generated in the previous step into the adaptively learned text templates to improve the match between the text and the abnormal images, enhancing the textual features for anomaly detection.</p>
<p>Our FG-Des not only improves the accuracy of anomaly detection but also enables the identification of the specific anomaly categories present in the samples, thus enhancing the interpretability.</p>
<p>Regarding anomaly localization, existing methods [4,7,15] localize anomalies by computing the similarity between the features of each image patch and the textual features.However, anomalies often span multiple patches with different shapes and sizes, sometimes requiring comparison with surrounding normal regions to determine their abnormality.While WinCLIP [16] addresses this issue by employing windows of different sizes, it incurs significant time and space costs by inputting a large number of images corresponding to each window into CLIP's image encoder during inference.To tackle this problem, we design HQ-Loc, which consists of three main components: first, preliminary anomaly localization based on Grounding DINO [21].Considering that even in abnormal samples, most regions are normal, and anomalies only exist in small local areas, we utilize the detailed anomaly descriptions generated in the previous step and employ Grounding DINO [21] for preliminary anomaly localization.Although directly using Grounding DINO for zero-shot anomaly localization yields low accuracy, the localized regions are always in the foreground, effectively avoiding false positives in background regions.Second, position enhancement involves adding the position detected by Grounding DINO to the text prompt, resulting in a more accurate description of the anomaly position.Third, the Multi-scale Multi-shape Cross-modal Interaction (MMCI) module aggregates patch features extracted by the Image Encoder using convolutional kernels of different sizes and shapes to enhance the method's ability to localize anomalies of different sizes and shapes.</p>
<p>Extensive experiments are conducted on multiple datasets like MVTec [2] and VisA [38].Our FiLo improves the accuracy of anomaly detection and localization, achieving new state-of-the-art zero-shot performance.Trained on the MVTec dataset and tested on the VisA dataset, FiLo achieves an image-level AUC of 83.9% and a pixel-level AUC of 95.9%, outperforming other ZSAD methods.</p>
<p>Our contributions can be summarized as follows: • We propose an adaptively learned Fine-Grained Description (FG-Des) that leverages domain-specific knowledge to introduce detailed anomaly descriptions, replacing generic "normal" and "abnormal" descriptions.Also, we use learnable vectors instead of manually crafted text templates to learn textual content which is more suitable for anomaly detection task, improving both the accuracy and interpretability.• Additionally, we design a position-enhanced High-Quality Localization method (HQ-Loc) that employs Grounding DINO [21] for preliminary anomaly localization, enhances text prompts with descriptions of anomaly positions, and utilizes an MMCI module to localize anomalies of different sizes and shapes more accurately, improving anomaly localization accuracy.• Experiments on multiple datasets demonstrate significant performance improvements in anomaly detection and localization compared to baseline methods.FiLo has been proved to be effective for zero-shot anomaly detection and localization, achieving state-of-the-art performance.</p>
<p>Related work 2.1. Vision-Language Models</p>
<p>Recently, multimodal models integrating visual and textual content have achieved significant success in various visual tasks [18,21,28].Among these, CLIP [28], pre-trained on a massive scale internet dataset, emerges as one of the most prominent methods.CLIP employs two structurally similar Transformer [33] encoders to extract features from images and text, aligning features with the same semantics through contrastive learning methods.With appropriate prompts, CLIP demonstrates remarkable zero-shot generalization capabilities across multiple datasets for downstream image classification tasks.However, the quality of prompts significantly affects the performance of downstream tasks.</p>
<p>Traditional approaches [3,16] require experts to manually craft suitable text prompts for each task, demanding domain-specific knowledge and being time-consuming.Recent methods like coop [36] and cocoop [35] propose using learnable vectors instead of manually crafted prompts, requiring minimal training cost while achieving superior performance across multiple datasets.</p>
<p>While the original CLIP was designed for image classification tasks, researchers have extended their efforts to explore vision-language models for object detection and semantic segmentation tasks.Grounding DINO [21] is a notable example, combining the Transformer-based object detector DINO with Grounded pretraining, achieving excellent performance as an open-set object detector.</p>
<p>Our FG-Des method, incorporating adaptive learned fine-grained anomaly descriptions, is built upon CLIP [28] and cocoop [35].However, straightforward utilization of cocoop-enhanced CLIP does not excel in anomaly detection tasks.Detailed anomaly descriptions for each item category are crucial for achieving outstanding performance.Grounding DINO [21] serves as a vital component of HQ-Loc.Yet, employing Grounding DINO [21] directly for zeroshot anomaly localization yields low accuracy.We utilize Grounding DINO solely for preliminary anomaly localization, capturing the approximate location of anomalies and avoiding false positives in background regions.</p>
<p>Zero-shot Anomaly Detection</p>
<p>Most zero-shot anomaly detection methods leverage the transferability of pre-trained vision-language models.Early methods like ZoC [12] and CLIP-AD [22], simply apply CLIP to anomaly detection data, resulting in low accuracy and inability to localize abnormal regions.Win-CLIP [16] first achieves anomaly localization by cropping windows of different sizes in images and significantly enhances anomaly detection by employing carefully crafted text prompts.APRIL-GAN [4] aligns patch-level image features with textual features using a learnable linear projection layer to accomplish anomaly localization, overcoming the inefficiency caused by WinCLIP's input of numerous windows and further enhancing performance.AnoVL [7] resolves the mismatch between patch-level image features and textual features by introducing V-V attention [19], enabling direct application of CLIP to anomaly detection tasks without any additional training.However, all the above methods require carefully designed and manually crafted text templates.AnomalyCLIP [37], an emerging approach, substitutes object-agnostic learnable text vectors for manually crafted text templates.Nevertheless, AnomalyCLIP describes anomalies uniformly using the word "damaged", which is evidently insufficient to cover all types of anomalies.</p>
<p>Segment Any Anomay (SAA) [3] is a zero-shot anomaly localization method based on the Grounded-SAM [29] approach.SAA utilizes Grounding DINO to generate anomaly bounding boxes, which are then used as prompts input into the Segment Anything Model [17] to obtain anomaly localization results.However, SAA [3] requires expertly crafted text inputs for Grounding DINO, and its results heavily rely on the detection outcomes of Grounding DINO, which may lead to low precision when directly applied to ZSAD.In our method, Grounding DINO serves solely as a preliminary anomaly localization module, aiming to prevent false positives in background regions of images.The primary dependency of our approach lies in the MMCI module for anomaly localization.</p>
<p>Moreover, none of the above methods incorporate location information of anomalies in the text prompt.Compared to existing methods, our approach enhances anomaly detection performance and interpretability by adaptive learned Fine-Grained anomaly Descriptions.We also improve the localization capability for anomalies of different sizes and shapes through our position-enhanced High-Quality localization method HQ-Loc.</p>
<p>Visual Description Enhancement</p>
<p>Numerous prior studies [35,36] have extensively demonstrated that the quality of the text prompt significantly impacts the performance of downstream tasks for pretrained Vision-Language models like CLIP [28].In contrast to text content meticulously crafted by experts, recent works [13,24,25] have delegated the task of generating high-quality text prompts to LLMs, which are called visual description enhancement.LLMs such as GPT-3.5 [27] and GPT-4 [1] encapsulate extensive knowledge across various domains, showcasing impressive performance across a spectrum of tasks.FiLo harnesses the profound domain knowledge embedded within LLMs to generate potential anomaly types for each item category, deriving fine-grained anomaly descriptions.We are the first to apply visual description enhancement techniques to anomaly detection tasks.</p>
<p>Multi-Scale Convolution</p>
<p>In recent years, multi-scale convolution has been a research hotspot to detect objects of different sizes appearing in images [9,10,31,32].Multi-scale convolution methods aggregate features of regions with different sizes by using convolutional kernels of various sizes, achieving significant performance improvements in image classification, semantic segmentation, and object detection.InceptionNet [31] is a typical representative, simultaneously employing convolutional kernels of 1 × 1, 3 × 3, 5 × 5, etc. within the same layer to address the uncertainty of the optimal kernel size across different samples.MixConv [32] groups input channels and applies convolutional kernels of different sizes to each channel group.RepVGG [10] decomposes all sizes of convolutional kernels into a series of composite operations of 3 × 3 convolutions.ACNet [9] changes the order of convolution and summation, first summing convolutional kernels of different sizes and then performing a single convolution operation, thereby reducing computational overhead.Most existing multi-scale methods focus on square convolutional kernels of different sizes.ACNet [9] employs multishape convolutional kernels, but its emphasis is on computational efficiency, neglecting multi-scale aspects.Since anomalies in images may exhibit various shapes and sizes, our MMCI module introduces convolutional kernels of different sizes and shapes to fully localize anomalies.</p>
<p>FiLo</p>
<p>In this paper, we propose a vision-language ZSAD method, FiLo, to enhance the capability of zero-shot anomaly detection and localization.Regarding anomaly detection, we devise the adaptively learned Fine-Grained Description method (FG-Des, Sec 3.2), which leverages fine-grained anomaly descriptions generated by LLMs and adaptable text vectors to identify the most precise textual representation for each anomaly sample.FG-Des facilitates more accurate judgments regarding the presence of anomalies in images and determines detailed anomaly types, thereby enhancing the interpretability of the method.For anomaly localization, we introduce the position-enhanced High-Quality Localization method (HQ-Loc, Sec 3.3), which employs preliminary localization via Grounding DINO, position-enhanced text prompts, and a Multi-scale, Multishape Cross-modal Interaction module to more accurately pinpoint anomalies of various sizes and shapes.</p>
<p>Overall Architecture</p>
<p>The overall architecture of the model is illustrated in Figure 2.For an input image I ∈ R H×W ×3 , we first utilize information from the dataset or LLM to generate a list of fine-grained anomaly types that may exist for this item category.Subsequently, the anomaly text is inputted into Grounding DINO to obtain preliminary bounding boxes for anomaly localization.Simultaneously, the combination of fine-grained anomaly type and previously learned text vector templates yields text descriptions for both normal and abnormal cases.These descriptions are then fed into the CLIP Text Encoder for feature extraction, resulting in representations of normal and abnormal text features.Next, the image is passed through the CLIP Image Encoder to extract intermediate patch features P i ∈ R Hi×Wi×Ci from M stages, where i indicates the i-th stage.These intermediate patch features are subjected to the MMCI module together with text features to generate anomaly map for each layer M i ∈ R H×W .Subsequently, after filtering with bounding boxes, the score maps for each layer are summed and normalized to obtain the final anomaly map M ∈ R H×W .The global features of the image are compared with text features after adaptation, and the maximum value of the final anomaly map M is added to derive the global anomaly score for the image.</p>
<p>FG-Des</p>
<p>Numerous existing methods [4,7,16] have demonstrated that the quality of text prompts significantly affects the effectiveness of anomaly detection when performing zeroshot inference on new categories.Therefore, we first focus on prompt engineering to generate more accurate and efficient text prompts for enhancing anomaly detection in ZSAD.In FG-Des, we achieve this goal through adaptively learned text templates and fine-grained anomaly descriptions generated by LLMs.</p>
<p>Adaptively Learned Text Templates</p>
<p>Following the success of methods like WinCLIP [16], subsequent methods such as APRIL-GAN [4] and Anoma-lyGPT [15] directly adopt the text templates used in Win-CLIP to construct text prompts.However, the text template in WinCLIP, A xxx photo of [state] [class], is primarily derived from the text template used by CLIP for image classification tasks on the ImageNet [8] dataset, which mainly indicates the category of foreground objects in the image rather than whether the object contains anomalies internally.To address this issue, we employ adaptive text templates learned based on anomaly detection-related data.</p>
<p>Normal Texts</p>
<p>Detailed Abnormal Texts [v1][v2]…[vn][STATE][CLASS]. [w1][w2]…[wn][STATE][CLASS] with [ANOMALY CLASS] at [POS]. e.g. [v1][v2]…[vn</p>
<p>During the learning process, these templates can combine the normal and abnormal content in the image to generate text prompts that better distinguish between normal and abnormal cases, while avoiding the need for extensive manual template engineering.Our adaptive normal and abnormal text templates are defined as follows:
T n = [V 1 ][V 2 ]...[V n ][ST AT E][CLASS]. T a = [W 1 ][W 2 ]...[W n ][ST AT E][CLASS] with [AN OM ALY CLASS] at [P OS]. [V i ]</p>
<p>Fine-Grained Anomaly Descriptions</p>
<p>As mentioned earlier, the generic "anomaly" texts in existing methods are insufficient to accurately describe the diverse types of anomalies that may appear on different object categories.Therefore, there is an urgent need for more personalized, informative text prompts to accurately characterize each image.LLMs such as GPT-4 [1] possess rich expert knowledge across various domains.We harness the power of LLMs to generate specific lists of potential anomaly types for each item category, replacing the vague and general "anomaly" or "damaged" descriptions used in previous methods.Such detailed textual features, when combined with features extracted by CLIP from images, lead to better anomaly detection results.</p>
<p>By incorporating fine-grained anomaly descriptions generated by large language models (LLMs) into the adaptive text templates' [AN OM ALY CLASS] section, we obtain complete text prompts.These prompts are then inputted into the CLIP Text Encoder, and after group averaging, we obtain text features representing normal and abnormal cases, denoted as F = [F n , F a ] ∈ R 2×C .For the global features G extracted from the image via the CLIP Image Encoder, we first pass them through a linear adapter layer to obtain adapted image features A ∈ R C that better match the textual content.Next, we calculate the global anomaly score by Eq (1):
S global = sof tmax(A • F T a ) + max(M ).(1)
M represents the anomaly map calculated in Sec 3.3 and max(•) denotes the maximum operation.Fine-grained anomaly descriptions not only improve the accuracy of anomaly detection but also enhance the interpretability of the detection results.Specifically, we can calculate the similarity between image features and each precise anomaly description.By examining the textual descriptions with high similarity, we can determine which category the anomaly in the image belongs to, thus gaining deeper insight into the model's decision-making process.</p>
<p>HQ-Loc</p>
<p>Existing Zero-Shot Anomaly Detection (ZSAD) methods often locate anomaly positions by computing the similarity between the features of each image patch and textual features.However, an anomaly region often spans multiple patches, exhibiting various positions, shapes, and sizes.Sometimes, it requires comparison with surrounding normal regions to determine if it's an anomaly.To address this, we propose this position-enhanced High-Quality Localization method HQ-Loc, which enhances anomaly localization from coarse to fine.This is achieved through three key components: Grounding DINO preliminary localization, position-enhanced textual prompts, and Multi-Scale Multi-Shape Cross-modal Interaction Module (MMCI).Below, we provide detailed explanations for each component.</p>
<p>Grounding DINO Preliminary Localization</p>
<p>Existing ZSAD methods typically lack discrimination between patches at different positions in the image, often resulting in the misidentification of background perturbations as anomalies.To mitigate this, we utilize detailed anomaly descriptions generated in the previous step to perform preliminary anomaly localization using Grounding DINO.While direct application of Grounding DINO may not precisely determine the exact location of anomalies, the localization boxes obtained generally reside in the foreground of objects, often near the anomaly area.Therefore, using the localization results from Grounding DINO to restrict anomaly regions effectively avoids false positives in the background, thus enhancing the accuracy of anomaly localization.Additionally, since Grounding DINO localization is not entirely accurate and may have missed detections, we adopt a strategy of suppressing anomaly scores outside all boxes by multiplying them with a parameter λ.</p>
<p>Position-Enhanced Textual Prompt</p>
<p>After obtaining the preliminary anomaly localization results from Grounding DINO, we incorporate the position information from the localization boxes into textual prompts to enhance position descriptions.Textual prompts with detailed anomaly descriptions and position enhancements are more aligned with the content in the image being examined.This alignment assists the model in concentrating on specific areas of the image during anomaly localization in the subsequent step, thereby improving localization accuracy.</p>
<p>MMCI Module</p>
<p>To comprehensively locate anomalies of different shapes and sizes, our approach does not directly compute the similarity between each image patch feature and textual features.Instead, we design a Multi-Scale Multi-Shape Cross-Modal Interaction Module (MMCI).MMCI is inspired by WinCLIP's use of windows of different sizes to select subregions in images and then determine if each subregion contains an anomaly.However, MMCI significantly reduces the computational overhead incurred by WinCLIP when simultaneously inputting dozens of images selected by windows into the CLIP's Image Encoder.Specifically, we design convolutional kernels of different sizes and shapes to process patch features extracted by the CLIP Image Encoder in parallel.Subsequently, we aggregate these features and compute their similarity with position-enhanced textual features.Through this approach, our MMCI module can effectively handle anomalies of different sizes and shapes, enhancing the model's ability to localize anomaly regions.</p>
<p>Let n different shaped convolutional kernels be denoted as C j , where j ranges from 1 to n.Given patch features
P i ∈ R HiWi×C , position-enhanced text features [F n , F a ] ∈ R 2×C , normal map M n
i ∈ R H×W and anomaly map M a i ∈ R H×W can be calculated by Eq. ( 2):
M n i , M a i = U p(N orm( n j=1 S(C j (P i ) • [F n , F a ] T ))),(2)
where U p(•) denotes the upsampling operation, S(•) is the softmax operation, and N orm(•) represents the normalization operation, ensuring that the values in the anomaly map lie between 0 and 1.By summing and normalizing M i for each layer, we can obtain the normal and anomaly map:
M n = N orm( i M n i ), M a = N orm( i M a i ),(3)
and the final localization result can be calculated by Eq (4)
M = G σ (M a + 1 − M n )/2,(4)
where G σ is a Gaussian filter, and σ controls smoothing.</p>
<p>Adapter</p>
<p>We employ a common bottleneck structure Adapter to align global image features and text features, consisting of two linear layers, one ReLU [14] layer, and one SiLU [11] layer, as shown in Algorithm 1.</p>
<p>Algorithm 1 Adapter Module</p>
<p>Require:
Input vector x ∈ R 768 Ensure: Output vector y ∈ R 768 1: h 1 = ReLU(W 1 x + b 1 ) ∈ R 384 2: y = SiLU(W 2 h 1 + b 2 )</p>
<p>Loss Functions</p>
<p>To learn the content of adaptive text templates and the convolutional kernel parameters in MMCI, we chose different loss functions for training from the perspectives of global anomaly detection and local anomaly localization.</p>
<p>Global Loss</p>
<p>We employ cross-entropy loss to optimize our global anomaly score.Cross-entropy loss is a commonly used loss function in various tasks and its formula is as follows:
L ce = − n i=1 y i log(p i ),(5)
where n is the number of instances, y i is the true label for instance i and p i is the score for instance i. we use crossentropy loss to calculate our global loss:
L global = L ce (S global , Label),(6)
where S global represents the global anomaly score calculated in Sec 3.2.2, and Label denotes the label indicating whether the image is anomalous or not.</p>
<p>Local Loss</p>
<p>We employ Focal loss [20] and Dice loss [26] to optimize our anomaly map M .Focal Loss and Dice Loss are common loss functions used in semantic segmentation tasks.Specifically, Focal Loss is particularly effective in addressing class imbalance issues, making it well-suited for anomaly localization tasks where the proportion of anomaly regions is relatively small.Focal loss can be calculated by Eq. ( 7):
L f = − 1 n n i=1 (1 − p i ) γ log(p i ),(7)
where n = H × W represents the total number of pixels, p i is the predicted probability of the positive classes and γ is a tunable parameter for adjusting the weight of hard-toclassify samples.In our implementation, we set γ to 2. Dice loss can be calculated by Eq. ( 8):
L d = − n i=1 y i ŷi n i=1 y 2 i + n i=1 ŷ2 i ,(8)
where n = H × W , y i is the output of decoder and ŷi is the ground truth value.</p>
<p>Our local loss can be calculated by Eq. ( 9):
L local = L f (M a , G)+L d (M a , G)+L d (M n , 1−G),(9)
where G denotes the ground truth.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>Our experiments primarily focus on two datasets: MVTec [2] and VisA [38].MVTec [2] is one of the most widely used industrial anomaly detection datasets, containing 5354 images of both normal and abnormal samples from 15 different object categories, with resolutions ranging from 700 × 700 to 1024 × 1024 pixels.</p>
<p>VisA [38] is an emerging industrial anomaly detection dataset comprising 10821 images of normal and abnormal samples covering 12 image categories, with resolutions around 1500 × 1000 pixels.Similar to APRIL-GAN [4] and AnomalyCLIP [37], we conduct supervised training on the test set of one dataset and directly performed zero-shot testing on the other dataset.</p>
<p>Evaluation Metrics</p>
<p>Following existing AD methods [5,34], we employ the Area Under the receiver operating Characteristic (AUC) as our evaluation metric, with image-level and pixel-level AUC used to assess anomaly detection and anomaly localization performance, respectively.</p>
<p>Implementation Details</p>
<p>We utilize the publicly available CLIP-L/  6-th, 12-th, 18-th, and 24-th layers of the CLIP Image Encoder.Starting from the 6-th layer, both QKV Attention and V-V Attention results are simultaneously utilized, where the outputs of QKV Attention are aligned with text features through a simple linear layer, and the outputs of V-V Attention are inputted into the MMCI module for multiscale, multi-shape deep interaction with text features.During training, input images are resized to a resolution of 518 × 518, and the AdamW [23] optimizer is used to optimize model parameters for 15 epochs.The learning rate for learnable text vectors is set to 1e-3, while the learning rate for the MMCI module is set to 1e-4.After that, we train the adapter for 5 epochs with a learning rate of 1e-5.Additionally, due to the varying number of fine-grained anomaly descriptions for each item category, training is conducted with a batch size of 1.Following previous methods [34,37], a Gaussian filter with σ = 4 is applied to obtain a smoother anomaly score map during testing.</p>
<p>Main Results</p>
<p>To demonstrate the effectiveness of our FiLo, we compare FiLo with several existing ZSAD methods, including CLIP [28], CLIP-AC [28], WinCLIP [16], APRIL-GAN [4], and AnomalyCLIP [37].Following [37], for CLIP, we conduct experiments using simple text prompts A photo of a normal [class].and A photo of an anomalous [class], and we add more text prompt templates that are recommended for ImageNet dataset for CLIP-AC.Results for WinCLIP [16], APRIL-GAN [4], and Anomaly-CLIP [37] are adopted from their respective papers.Specifically, AnomalyCLIP [37] incorporates additional learnable embeddings in the CLIP Text Encoder, while other methods, including our FiLo, directly utlize the frozen parameters of CLIP.To ensure fair comparison, we reproduce AnomalyCLIP without learnable embeddings, which is referred as AnomalyCLIP-.</p>
<p>Table 1 presents the experimental results of FiLo and existing methods on the VisA and MVTec datasets, which demonstrates superiority of FiLo across most metrics on both datasets, validating the effectiveness of our FG-Des and HQ-Loc modules.Compared to the state-of-the-art ZSAD method AnomalyCLIP [37], after introducing the FG-Des and HQ-Loc modules, FiLo achieves a 1.1% improvement in image-level AUC and a 0.4% improvement in pixel-level AUC on the VisA dataset.Additionally, FiLo also achieves a 1.2% improvement in pixel-level AUC on the MVTec dataset.</p>
<p>Ablation Study</p>
<p>To investigate the effectiveness of each proposed module, we conduct extensive ablation experiments on the VisA and MVTec datasets, confirming the efficacy of every component in our approach, including fine-grained descriptions, learnable text templates, grounding, position enhancement and MMCI.Table 2, Table 3 and Table 4 present the experimental results of FiLo on the MVTec and VisA datasets.</p>
<p>In Table 2, we initially employ the same setup as CLIP-AC as our baseline, using simple two-category texts A photo of a normal [class] and A photo of an anomalous [class].Upon realizing that the simple words "normal" and "anomalous" alone did not effectively distinguish between normal and abnormal samples, we modify the sentence structure to A photo of a [  In Table 3, also starting from the CLIP baseline, we first replace all parts of the text except for In Table 4, we experiment with each component of HQ-Loc.From the table, it can be observed that both Grounding and Position Enhancement contribute to improvements in pixel-level AUC.Additionally, the MMCI module, which integrates multi-shape and multi-size capabilities, can effectively detect anomalies of various sizes and shapes, resulting in performance enhancements in both detection and localization aspects.</p>
<p>Visulization Results</p>
<p>Figure 3 illustrates the visualization results of FiLo on the MVTec and VisA datasets.In the absence of any prior access to data from the target dataset, FiLo achieves anomaly localization results that closely resemble the ground truth, showcasing FiLo's robust ZSAD capability.</p>
<p>As observed in the second row of Figure 3, directly com-puting the similarity between all patch features extracted using CLIP and textual features representing normal and abnormal semantics often yields imprecise anomaly localization results.This approach sometimes leads to false positives in non-anomalous objects or background regions of the image.However, by employing HQ-Loc's grounding for preliminary localization and position enhancement, the final output effectively mitigates this phenomenon.Furthermore, during the preliminary localization process, Grounding associates each bounding box with matched textual descriptions, indicating the type of anomaly present in that area.For instance, in Figure 3(e), the corresponding text for the bounding box accurately identifies anomalies on the hazelnut: "hole" and "crack".</p>
<p>Conclusion</p>
<p>Our FiLo method represents a significant advancement in the field of Zero-Shot Anomaly Detection (ZSAD), effectively addressing prevalent challenges in both anomaly detection and localization.Our FG-Des method harnesses the capabilities of Large Language Models (LLMs) by generating specific descriptions for potential anomaly types associated with each object category.This approach notably enhances both the precision and interpretability of anomaly detection.Furthermore, our devised HQ-Loc strategy effectively mitigates the deficiencies of existing methods in terms of anomaly localization accuracy, particularly demonstrating superior performance in localizing anomalies of various sizes and shapes.Extensive experiments validate the superiority of FiLo across multiple datasets, affirming its efficacy and practicality in the realm of zero-shot anomaly detection tasks.</p>
<p>Appendix A. Fine-grained ZSAD performance</p>
<p>In the main paper, we have compared FiLo with existing ZSAD methods on anomaly detection and localization across the MVTec [2] and VisA [38] datasets.Our evaluation primarily utilizes Image-level AUC and Pixellevel AUC as metrics for detection and localization, respectively.Here, we provide detailed performance analysis of FiLo and other ZSAD methods at the fine-grained data subset level, including the methods we using for comparison: CLIP [28], CLIP-AC [28], WinCLIP [16], APRIL-GAN [4] and AnomalyCLIP [37].</p>
<p>Tables 5 and Tables 6 depict the anomaly localization performance of FiLo on the MVTec and VisA datasets, and the anomaly detection performance of FiLo on the VisA and MVTec datasets is showcased in Table 8 and Table 7 respectively.Across the 15 classes in the MVTec dataset, FiLo achieves the highest Pixel-level AUC in 12 classes, while in the VisA dataset comprising 12 classes, FiLo attains the highest Pixel-level AUC in 8 classes.Notably, FiLo surpasses the state-of-the-art method AnomalyCLIP [37] by 1.1% on Pixel-level AUC in the MVTec dataset and by 0.4% in the VisA dataset, demonstrating the efficacy of FiLo.</p>
<p>B. Fine-Grained Anomaly Descriptions</p>
<p>Table 9 and Table 10 present the detailed anomaly types generated by leveraging LLM for each category within the MVTec and VisA datasets.During the inference process with FiLo, we substitute these detailed anomaly descriptions generated by LLM for the "[ANOMALY CLASS]" portion in the text template to obtain the detailed anomaly description content for each category of items.</p>
<p>In Figure 4, we additionally display the similarity between each detailed anomaly description generated by LLM and the image features.We showcase the top 5 detailed anomaly descriptions with the highest similarity to the image, highlighting the most similar descriptions in red.By identifying the detailed anomaly description with the highest similarity, we can further discern the type of anomaly present in the sample.</p>
<p>C. Additional Ablations</p>
<p>In this section, we conducted further ablation studies on various detailed components of FiLo, including the backbone utilized, learning rate, employment of VV Attention, different treatments on QKV and VV Attention results, learning strategies for adaptive learning templates, number of learn-able vectors, the structure and connectivity of Adapters, etc. Below are detailed analyses for each aspect.</p>
<p>C.1. Different Backbones and Learning Rates</p>
<p>Previous anomaly detection methods based on CLIP have typically utilized different CLIP backbones.WinCLIP [16] employs ViT-B-16@240px, while methods like APRIL-GAN [4] and AnomalyCLIP [37] use ViT-L-14@336px.Existing methods have shown that using a backbone with higher image resolution is more beneficial for pixel-level anomaly localization.However, these methods with higher resolutions have not surpassed WinCLIP, which uses a resolution of 240x240, in terms of image-level AUC.We also implemented our FiLo method on these two commonly used backbones, and the results are shown in Table 11.</p>
<p>In addition to the choice of backbone, the setting of learning rates also influences experimental results.Table 11 further illustrates the experimental results of FiLo under different learning rates ranging from 1e-3 to 1e-5.It can be observed that FiLo achieves the best anomaly detection and localization performance on both datasets when using a learning rate of 1e-3 for the learnable text vectors and a learning rate of 1e-4 for the MMCI module.</p>
<p>C.2. Adaptively Learned Text Templates</p>
<p>CoOp [36] and CoCoOp [35] are two distinct methods that utilize learnable vectors to replace manually crafted text prompts.These methods exhibit some differences in their approaches.Specifically, the learnable vectors in CoOp are agnostic to image content and are directly embedded into the text prompt, emphasizing the universality and uniformity of the text prompt.On the other hand, CoCoOp builds upon the learnable vectors embedded in the text prompt by incorporating a lightweight meta-net to append image features to the text prompt.This approach emphasizes generating tailored text prompts for each image, aiming to better match the image content.</p>
<p>Table 12 presents the experimental results of FiLo under the respective usage of CoOp and CoCoOp.Inspired by AnomalyCLIP [37], we also explored the performance under the addition of class name information in the text content.The experimental results indicate that when using CoOp, omitting class name from the text yields better results, consistent with findings in AnomalyCLIP.This is because CoOp inherently emphasizes the generality and uniformity of the text prompt.Conversely, when employing CoCoOp for learning text templates, including class name Object name CLIP [28] CLIP-AC [28] WinCLIP [16] APRIL-GAN [4] AnomalyCLIP [37]  information improves performance.This is attributed to the alignment of CoCoOp's approach, which incorporates image features into the text prompt via a meta-net, with the concept of FiLo, utilizing fine-grained anomaly description and position enhancement to obtain precise representations of each image's text content, aiming for a better match with image content.</p>
<p>The results in Table 12 further demonstrate that Co-CoOp outperforms CoOp, highlighting the effectiveness of leveraging fine-grained anomaly descriptions to enhance anomaly detection.</p>
<p>We also examined the impact of varying the number of learnable vectors in adaptively learned text templates.The findings are illustrated in Figure 5.It can be observed that utilizing 12 learnable vectors yields the best performance in both anomaly detection and localization tasks.</p>
<p>C.3. Utilization of V-V Attention</p>
<p>Pre-trained on large-scale datasets, CLIP exhibits excellent zero-shot performance on downstream image classification tasks.However, directly using the features extracted from the CLIP Image Encoder for each position in the feature map and measuring their similarity with textual features often results in significant noise activation outside of objects during fine-grained semantic segmentation or object detection tasks.CLIP Surgery [19] addresses this issue, identifying it as stemming from the QKV attention mechanism within CLIP, which leads to feature pooling from semantically disparate regions, consequently causing noise activation in erroneous areas.The proposed solution involves employing V-V self-attention to mitigate this problem.</p>
<p>Approaches such as AnoVL [7] and AnomalyCLIP [37] have also incorporated V-V attention into anomaly detection and localization tasks, resolving the issue of misalignment between patch-level features and textual features encoun-tered in WinCLIP and APRIL-GAN, achieving remarkable zero-shot performance.However, V-V attention suffers from training difficulties, as slight mishandling may result in model outputs entirely comprised of zeros, causing the AUC to plummet to 50.To address this challenge, we simultaneously utilize the output results of both QKV attention and V-V attention, exploring the differential effects of applying distinct processing methods to the output results of QKV attention and V-V attention.The results, as shown in Table 13, indicate that employing a simple linear layer on the output results of QKV attention and inputting the output results of V-V attention into the MMCI module yields the best detection and localization performance for FiLo.</p>
<p>Object name CLIP [28] CLIP-AC [28] WinCLIP [16] APRIL-GAN [4] AnomalyCLIP [37]</p>
<p>C.4. Ablations of Adapter</p>
<p>In this section, we compare the performance impact of the structure and connection methods of the adapter on FiLo.</p>
<p>Regarding structure, we test the use of a simple linear layer and the bottleneck structure as shown in Sec 3 of the main paper.We also conduct experiments to assess the performance difference of the adapter when utilizing residual connection versus not utilizing it.Experimental results are shown in Table 14.It can be observed that when employing the bottleneck structure without residual connection, the adapter achieves the best performance.</p>
<p>C.5. Convolution Kernel's Shape of MMCI</p>
<p>We extensively experiment on the impact of different kernel shapes used in MMCI.Starting with the sole use of 1x1 convolutional kernels and gradually incorporate shapes such as 3x3, 5x5, 7x7, 1x5, 5x1, and 9x9, we evaluate the various experimental results, as depicted in Figure 6.Based on the experimental findings, we ultimately select a combination of kernel shapes including 1x1, 3x3, 5x5, 7x7, 1x5, and 5x1.This combination harnesses the advantages of multiscale and multi-shape kernels, enabling precise localization of anomalous regions of different sizes and shapes.Toothbrush loose bristles, uneven bristle distribution, excessive shedding of bristles, staining on the bristles, abrasive texture, irregularities in the shape Transistor burn marks, detached leads, signs of corrosion, irregularities in the shape, presence of cracks or fractures, signs of physical trauma, irregularities in the surface texture Zipper bent, frayed, misaligned, excessive stiffness, corroded, detaches, loose, warped Table 9. Fine-Grained anomaly description of every object within MVTec dataset.</p>
<p>D. Visualization D.1. Anomaly Scores for Every Categories</p>
<p>In this section, we present the statistical analysis of anomaly scores generated by WinCLIP [16] and FiLo for each class object in the MVTec and VisA datasets.These visualizations aim to illustrate the effectiveness of FiLo's detailed anomaly descriptions and adaptively learned text templates compared to WinCLIP's manually crafted two-class text adjustment.As depicted in Figure 7 and Figure 8, WinCLIP's scores for both normal and abnormal samples heavily overlap and are concentrated around 0.5, indicating its failure to effectively distinguish between normal and abnormal sam-ples.In contrast, Figure 9 and Figure 10 illustrate FiLo's visualization results on these two datasets.It can be observed that the scores for normal samples significantly decrease while those for abnormal samples notably increase, resulting in a significant reduction in the overlapping area.Ablation on the kernels in MMCI</p>
<p>D.2. Anomaly Maps</p>
<p>Figure 1 .
1
Figure 1.Comparison of anomaly detection and localization between FiLo and previous ZSAD methods.Previous ZSAD methods utilize fixed templates and generic anomaly descriptions, potentially resulting in errors.Our FG-Des enhances detection accuracy with adaptively learned text templates and fine-grained anomaly descriptions.For localization, ZSAD methods often produce false positives in background areas by directly comparing image patches with text features.Our HQ-Loc approach, using Grounding DINO, location enhancement, and MMCI, effectively removes background regions and improves localization accuracy.</p>
<p>Figure 2 .
2
Figure 2. Overall architecture of FiLo.Given an input image, fine-grained anomaly types are generated by LLM.Then normal and detailed abnormal texts are input into Grounding DINO to obtain bounding boxes and are fed into CLIP Text Encoder to get Fn and Fa.Intermediate patch features of input image are subjected to MMCI together with text features to compute anomaly map, and the global image features are compared with text features after adaptation to obtain global anomaly score.</p>
<p>Figure 3 .
3
Figure 3. Visualization result of FiLo on MVTec and VisA datasets."CLIP output" refers to the localization results without HQ-Loc, while "Final mask" represents the final localization result.</p>
<p>[class] with learnable vectors, i.e., [v1][v2]...[vn][class].We find that compared to handcrafted text, the text vectors learned by the model are more suitable for anomaly detection tasks, exhibiting higher detection and localization accuracy.Further, by combining the learned text vectors with detailed anomaly descriptions generated by LLMs as described earlier, we utilize the text prompt [v1][v2]...[vn][state][class] with [anomaly class], resulting in significant improvements.</p>
<p>Figure 4 .
4
Figure 4. Illustration of similarities between images and different fine-grained anomaly descriptions.</p>
<p>Figure 6 .
6
Figure 6.Comparison of FiLo on MVTec and VisA datasets with different convolution kernels.</p>
<p>Figure 7 .
7
Figure 7. Anomaly scores of WinCLIP on the MVTec dataset.Each sub-figure represents the visualization of one object.</p>
<p>every categories in MVTec dataset via WinCLIP</p>
<p>Figure 8 .
8
Figure 8. Anomaly scores of WinCLIP on the VisA dataset.Each sub-figure represents the visualization of one object.</p>
<p>Figure 9 .
9
Figure 9. Anomaly scores of FiLo on the MVTec dataset.Each sub-figure represents the visualization of one object.</p>
<p>every categories in VisA dataset via FiLo</p>
<p>Figure 10 .Figure 11 .
1011
Figure 10.Anomaly scores of FiLo on the VisA dataset.Each sub-figure represents the visualization of one object.</p>
<p>Table 1 .
1
14@336px model as our backbone, with frozen parameters for CLIP's Text Encoder and Image Encoder.Training is conducted on either the MVTec or VisA dataset, with zero-shot testing performed on the other dataset.For intermediate-level patch-based image features, we employ features from the Comparison results between FiLo and other ZSAD methods.The best-performing method is in bold.
MethodBackboneAnomaly DescriptionVisAMVTec-ADImage-AUC Pixel-AUC Image-AUC Pixel-AUCCLIP [28]ViT-L/14@336pxnormal / anomalous66.446.674.138.4CLIP-AC [28]ViT-L/14@336pxnormal / anomalous65.047.871.538.2WinCLIP [16]ViT-B/16@240pxstate ensemble78.179.691.885.1APRIL-GAN [4]ViT-L/14@336pxstate ensemble78.094.286.187.6AnomalyCLIP [37] ViT-L/14@336pxnormal / damaged82.195.591.591.1AnomalyCLIP-ViT-L/14@336pxnormal / damaged81.795.090.889.5FiLo (ours)ViT-L/14@336px fine-grained description83.995.991.292.3SetupVisAMVTecCLIP baseline(65.0, 47.8) (71.5, 38.2)+ generic <a href="65.4, 83.9">state</a> (79.9, 83.5)+ fine-grained [anomaly class] (71.2, 85.5) (80.8, 83.8)</p>
<p>Table 2 .
2
Ablation results of anomaly descriptions.Results are displayed in the format of (Image-AUC, Pixel-AUC).
SetupVisAMVTecCLIP baseline(65.0, 47.8) (71.5, 38.2)+ learnable template(72.5, 93.1) (82.1, 85.2)+ fine-grained description (78.1, 93.2) (85.8, 85.1)</p>
<p>Table 3 .
3
Ablation results of text template.Results are displayed in the format of (Image-AUC, Pixel-AUC).</p>
<p>Table 4 .
4
state] [class], where [state] encompasses The results of ablation experiments for each proposed modules in HQ-Loc.
Grounding Position EnhancementMMCIVisAMVTecMulti-shape Multi-scale Image-AUC Pixel-AUC Image-AUC Pixel-AUC78.193.285.885.1✓78.193.485.885.3✓✓78.693.685.585.7✓✓✓79.295.386.289.4✓✓✓80.795.688.991.4✓✓✓✓83.995.991.292.3</p>
<p>Table 5 .
5
FiLo (ours) Fine-grained data-subset-wise performance comparison (AUROC) for anomaly localization on MVTec-AD.The best performance is in bold, and the second-best is underlined.
Carpet11.510.795.498.498.899.4Bottle17.523.389.583.490.492.6Hazelnut25.234.094.396.197.197.6Leather9.95.696.799.198.699.4Cable37.437.577.072.378.978.4Capsule50.949.186.992.095.896.9Grid8.711.982.295.897.397.8Pill55.860.880.076.29289.1Transistor51.148.574.762.47174.8Metal nut43.953.661.065.474.472.5Screw80.176.489.697.897.598.1Toothbrush36.335.086.995.891.996.0Zipper51.544.791.691.191.496.6Tile49.939.177.692.794.697.4Wood45.742.493.495.896.598.3Mean38.438.285.187.691.192.3Candle33.650.088.997.898.898.7Capsules56.861.581.697.595.092.3Cashew64.562.584.786.093.895.1Chewinggum43.056.593.399.599.399.4Fryum45.662.788.592.094.695.2Macaroni120.322.970.998.898.399.1Macaroni237.728.859.397.897.698.1Pcb157.851.661.292.794.194.4Pcb234.738.471.689.792.493.7Pcb354.644.685.388.488.490.8Pcb452.149.994.494.695.795.8Pipe fryum58.744.775.496.098.297.7Mean46.647.879.694.295.595.9
[37]ct name CLIP[28]CLIP-AC[28]WinCLIP[16]APRIL-GAN[4]AnomalyCLIP[37]FiLo (ours)</p>
<p>Table 6 .
6
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly localization on VisA.The best performance is in bold, and the second-best is underlined.</p>
<p>Table 7 .
7
FiLo (ours) Fine-grained data-subset-wise performance comparison (AUROC) for anomaly detection on MVTec AD.The best performance is in bold, and the second-best is underlined.
Carpet9693.1100.099.5100.099.9Bottle45.946.199.292.089.398.6Hazelnut88.791.193.989.697.292.8Leather99.499.5100.099.799.8100Cable58.146.686.588.469.877.9Capsule71.468.872.979.989.989.2Grid72.563.798.886.397.097.4Pill73.673.879.180.581.887.8Transistor48.851.288.080.892.880.5Metal nut62.863.497.168.493.677Screw78.266.783.384.981.174.5Toothbrush73.389.288.053.884.794.2Zipper60.136.191.589.698.598.1Tile88.589.0100.099.9100.0100Wood9494.999.499.096.899.7Mean74.171.591.886.191.591.2Object name CLIP [28] CLIP-AC [28] WinCLIP [16] APRIL-GAN [4] AnomalyCLIP [37] FiLo (ours)Candle37.933.095.483.879.379.3Capsules69.775.385.061.281.580.9Cashew69.172.792.187.376.390Chewinggum77.576.996.596.497.498.4Fryum67.260.980.394.393.088.3Macaroni164.467.476.271.687.288.3Macaroni26565.763.764.673.468.5Pcb154.943.973.653.485.487Pcb262.659.551.271.862.277.6Pcb352.249.073.466.862.769.5Pcb487.789.079.695.093.995.7Pipe fryum88.886.469.789.992.483.8Mean66.465.078.178.082.183.9</p>
<p>Table 8 .
8
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly detection on VisA.The best performance is in bold, and the second-best is underlined.</p>
<p>Table 12 .
12
Comparison of different learning methods for learnable vectors and whether to use class name.
Figure 11 further demonstrates the Anomaly Maps gener-ated by FiLo on additional samples from the MVTec andVisA datasets. The three rows from top to bottom in thefigure represent the test samples, FiLo's output, and theGround Truth, respectively, demonstrating FiLo's robustanomaly localization capability.</p>
<p>Table 13 .
13
Comparison of results of different processing methods for the output results of QKV and VV Attention.
Adapter's arch residualVisAMVTec-ADImage-AUC Pixel-AUC Image-AUC Pixel-AUCBottleneck83.995.991.292.3Bottleneck✓83.695.889.991.4Linear83.995.990.292.3Linear✓83.895.988.691.1</p>
<p>Table 14 .
14
Comparison of different adapter structures and connection types.Figure 5. Comparison of FiLo on MVTec and VisA datasets with different numbers of learnable vectors.
Ablation on the length of learnable text prompt E90MVTec VisA95 96MVTec VisAImage-level AUC86 88Pixel-level AUC92 93 9484918290468 Length of Text Prompt E 10 121416468 Length of Text Prompt E 10 121416
E. Limitation and future workCompared to previous works like WinCLIP[16], FiLo has made advancements in anomaly detection, localization, and interpretability through the use of Fine-Grained Description and High-Quality Localization methods.However, despite these strides forward, certain limitations still persist, warranting further investigation and refinement.As illustrated in Figure9and Figure10, while the differentiation between normal and abnormal samples is more distinct compared to previous methods, significant overlap still exists in certain categories such as zipper and metal nut.In the future, we plan to further improve the differentiation between normal and abnormal sample scores through approaches such as metric learning.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Segment any anomaly without training via hybrid prompt regularization. Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, Weiming Shen, arXiv:2305.107242023arXiv preprint</p>
<p>A zero-/fewshot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.173822023arXiv preprint</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer2021</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Anovl: Adapting vision-language models for unified zeroshot anomaly localization. Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li, arXiv:2308.159392023arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. Xiaohan Ding, Yuchen Guo, Guiguang Ding, Jungong Han, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision1911-1920, 2019</p>
<p>Repvgg: Making vgg-style convnets great again. Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, Jian Sun, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Sigmoidweighted linear units for neural network function approximation in reinforcement learning. Stefan Elfwing, Eiji Uchibe, Kenji Doya, Neural networks. 1072018</p>
<p>Zero-shot out-of-distribution detection based on the pre-trained model clip. Sepideh Esmaeilpour, Bing Liu, Eric Robertson, Lei Shu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022</p>
<p>Leveraging multiple descriptive features for robust few-shot image learning. Zhili Feng, Anna Bair, J Zico Kolter, arXiv:2307.043172023arXiv preprint</p>
<p>Deep sparse rectifier neural networks. Xavier Glorot, Antoine Bordes, Yoshua Bengio, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Clip surgery for better explainability with enhancement in openvocabulary tasks. Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li, arXiv:2304.056532023arXiv preprint</p>
<p>Kaiming He, and Piotr Dollár. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Exposing outlier exposure: What can be learned from few, one, and zero outlier images. Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert Müller, Marius Kloft, arXiv:2205.114742022arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Enhancing clip with gpt-4: Harnessing visual descriptions as prompts. Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin Mcguinness, E O' Noel, Connor, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Visual classification via description from large language models. Sachit Menon, Carl Vondrick, arXiv:2210.071832022arXiv preprint</p>
<p>V-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 2016 fourth international conference on 3D vision (3DV). Ieee2016</p>
<p>Training language models to follow instructions with human feedback. Advances in neural information processing systems. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 202235</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Grounded sam: Assembling open-world models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, arXiv:2401.141592024arXiv preprint</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Going deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Mingxing Tan, V Quoc, Le, Mixconv, arXiv:1907.09595Mixed depthwise convolutional kernels. 2019arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>A unified model for multi-class anomaly detection. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le, Advances in Neural Information Processing Systems. 202235</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 13092022</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, arXiv:2310.189612023arXiv preprint</p>
<p>Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. Springer2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-492 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-492</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-492</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-258180468</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.08243v1.pdf" target="_blank">Stochastic Code Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications. This issue is also observed in language modeling for long text generation, and one proposed solution is the use of a latent stochastic process. This approach involves generating a document plan and then producing text that is consistent with it. In this study, we investigate whether this technique can be applied to code generation to improve coherence. We base our proposed encoder and decoder on the pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for training. We evaluate our results using the HumanEval benchmark and observe that the modified Time Control model performs similarly to CodeParrot on this evaluation.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e492.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e492.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modified Time Control</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified Time Control (encoder-decoder adaptation of Time Control built on CodeParrot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder model that adapts the Time Control latent stochastic-process approach to code generation by mapping sentence-level code to Brownian-bridge latent trajectories and fine-tuning a CodeParrot decoder conditioned on those latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Modified Time Control (based on CodeParrot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>code generation / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Generating Python code from problem prompts/docstrings and evaluating functional correctness on HumanEval (and training on APPS)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Latent stochasticity from Brownian-bridge trajectories (random noise in latent interpolation), randomness in autoregressive sampling when producing code (affecting pass@k outcomes), stochasticity in optimization during fine-tuning (SGD minibatch order, initialization), dataset subsampling due to limited compute (they did not use ~70% of training set), and limited decoder length (non-deterministic truncation effects).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Not reported quantitatively for variability across runs; final reported performance (Table 2) shows PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73% for Modified Time Control as a point estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Computational constraints (only a subset of training data used — ~70% unused), limited decoder length (<750 tokens), nondeterminism from stochastic training and sampling, potential pretraining differences of CodeParrot; no controlled replication experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Architectural mitigation: impose a latent stochastic process (Brownian bridge) and train encoder with a contrastive objective so sentence triplets follow the transition density; fine-tune an existing pretrained decoder rather than training from scratch; label structure using tree-sitter to standardize sections.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not reported as a quantitative study of variance reduction; effectiveness reported qualitatively and via comparable point-estimate metrics — Modified Time Control achieved PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73%, i.e., similar to CodeParrot baselines under their compute limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting a latent stochastic-process (Brownian-bridge) to code generation is feasible and the Modified Time Control model learned smooth latent trajectories; under the authors' limited compute budget it produced HumanEval pass@k rates comparable to CodeParrot (no quantitative analysis of run-to-run variability or reproducibility tests were reported).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e492.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e492.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brownian bridge latent process</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Brownian bridge stochastic process for sentence-level latent trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous-time stochastic process used to model and sample latent sentence embeddings that interpolate noisily between a start and end latent code (density p(z_t | z_0, z_T) is Gaussian with mean linear interpolation and variance t(T-t)/T).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language modeling via stochastic processes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Modified Time Control (encoder uses Brownian-bridge priors mapped from CodeParrot hidden states)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>language modeling applied to code generation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Learn sentence-level latent representations that follow a Brownian-bridge process to condition code generation for improved global coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Intrinsic stochasticity of the Brownian-bridge (random Gaussian deviations around the linear interpolation), sampling different latent trajectories for the same document lead to different generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Stochastic latent sampling introduces nondeterminism in outputs; ensuring triplets follow the Brownian-bridge density requires a contrastive training objective which the paper applies, but no replication study is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Training the encoder with a contrastive objective so that every triplet of sentence embeddings follows the Brownian-bridge transition density (i.e., enforcing the assumed stochastic structure).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not reported in terms of reduced output variance; authors state the model learned smooth Brownian-bridge trajectories and in some cases improved generation, but provide no quantitative variability reduction measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The Brownian-bridge prior was successfully used to model sentence-level latent trajectories and enforced via a contrastive objective, but the paper does not quantify how much this reduces stochastic variability in final code outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e492.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e492.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pass@k (functional correctness metric over k samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation metric for code generation that reports the probability that at least one of k sampled outputs passes unit tests (interpreted as evaluating the best out of k samples chosen by an oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Modified Time Control / CodeParrot (evaluated using HumanEval pass@k)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (table also reports a 110M CodeParrot result)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>code generation benchmarking / functional evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measuring functional correctness on HumanEval by sampling k outputs per problem and computing pass@k.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling variability across generated outputs (pass@k reflects best-of-k sampling variability) and dependence on oracle selection of the best sample; underlying model randomness in sampling influences pass@k values.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>pass@1, pass@10, pass@100 (point estimates reported)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 2 point estimates: TIMECONTROL PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73%; CODEPARROT 110M PASS@1=3.80%, PASS@10=6.57%, PASS@100=12.78%; CODEPARROT 1.5B PASS@1=3.58%, PASS@10=8.03%, PASS@100=14.96%.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Pass@k depends on the number of samples k and nondeterministic sampling; the metric uses an oracle to pick the best sample which complicates reproducibility unless sampling seeds and exact sampling procedure are fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Not specifically applied here; implicit mitigation would be to report pass@k across fixed sampling seeds or provide raw samples, but the paper does not perform those controls.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pass@k is used as the primary functional metric; point-estimate pass@k values show Modified Time Control performs comparably to CodeParrot baselines under the reported experimental setup, but no analysis of sampling variability or uncertainty around these estimates is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e492.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e492.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic gradient descent (SGD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic gradient descent optimizer (with momentum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The optimization algorithm used to train the encoder MLP (four-layer) on top of frozen CodeParrot hidden states, using SGD with specified learning rate and momentum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder MLP on top of CodeParrot hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>model training / optimization</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Training the encoder MLP (latent mapper) with SGD (learning rate 1e-4, momentum 0.9) to produce sentence embeddings that follow the Brownian-bridge prior.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Optimizer stochasticity arising from minibatch sampling, random initialization of MLP parameters, and nondeterministic training order; these affect learned latents and downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>No fixed-seed or replication experiments described; optimizer-induced nondeterminism and lack of reported random-seed control make exact reproduction difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Paper reports fixed hyperparameters (learning rate = 1e-4, momentum = 0.9) and a frozen pretrained CodeParrot backbone to reduce variance from large-scale retraining, but does not report seed controls.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SGD with fixed hyperparameters was used to train the encoder MLP, but the paper does not assess how optimization stochasticity affects downstream code-generation variability or reproducibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e492.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e492.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoregressive sampling / decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive left-to-right token sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The decoding procedure of CodeParrot and Modified Time Control: an auto-regressive LM that samples tokens left-to-right conditioned on past tokens and the sentence-level latent embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeParrot (fine-tuned decoder) / Modified Time Control decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>language generation for code</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Generating code sequences conditioned on past tokens and latent sentence embeddings to reconstruct training documents and to sample candidate solutions for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Nondeterministic sampling choices during autoregressive decoding (sampling distribution, top-k/top-p/temperature if used though not specified), and dependence on latent embeddings (which are stochastic due to Brownian-bridge).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Authors do not report controlling sampling seeds or sampling hyperparameters; oracle selection in pass@k further complicates reproducibility of reported pass@k values.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Architectural conditioning on latent sentence embeddings (the Brownian-bridge plan) intended to induce more coherent and stable generation, and fine-tuning an existing pretrained model to limit variability from training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitatively reported as maintaining code quality and sometimes improving coherence, but no quantitative variance-reduction numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autoregressive stochastic sampling is a core source of output variability (implicit in the use of pass@k), and the paper uses latent conditioning to aim for more globally coherent outputs but does not quantify reductions in sampling-induced variability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language modeling via stochastic processes <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Measuring coding challenge competence with apps <em>(Rating: 1)</em></li>
                <li>CodeBLEU: a method for automatic evaluation of code synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-492",
    "paper_id": "paper-258180468",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Modified Time Control",
            "name_full": "Modified Time Control (encoder-decoder adaptation of Time Control built on CodeParrot)",
            "brief_description": "An encoder-decoder model that adapts the Time Control latent stochastic-process approach to code generation by mapping sentence-level code to Brownian-bridge latent trajectories and fine-tuning a CodeParrot decoder conditioned on those latents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Modified Time Control (based on CodeParrot)",
            "model_size": "1.5B",
            "scientific_domain": "code generation / program synthesis",
            "experimental_task": "Generating Python code from problem prompts/docstrings and evaluating functional correctness on HumanEval (and training on APPS)",
            "variability_sources": "Latent stochasticity from Brownian-bridge trajectories (random noise in latent interpolation), randomness in autoregressive sampling when producing code (affecting pass@k outcomes), stochasticity in optimization during fine-tuning (SGD minibatch order, initialization), dataset subsampling due to limited compute (they did not use ~70% of training set), and limited decoder length (non-deterministic truncation effects).",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": "Not reported quantitatively for variability across runs; final reported performance (Table 2) shows PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73% for Modified Time Control as a point estimate.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Computational constraints (only a subset of training data used — ~70% unused), limited decoder length (&lt;750 tokens), nondeterminism from stochastic training and sampling, potential pretraining differences of CodeParrot; no controlled replication experiments reported.",
            "mitigation_methods": "Architectural mitigation: impose a latent stochastic process (Brownian bridge) and train encoder with a contrastive objective so sentence triplets follow the transition density; fine-tune an existing pretrained decoder rather than training from scratch; label structure using tree-sitter to standardize sections.",
            "mitigation_effectiveness": "Not reported as a quantitative study of variance reduction; effectiveness reported qualitatively and via comparable point-estimate metrics — Modified Time Control achieved PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73%, i.e., similar to CodeParrot baselines under their compute limits.",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Adapting a latent stochastic-process (Brownian-bridge) to code generation is feasible and the Modified Time Control model learned smooth latent trajectories; under the authors' limited compute budget it produced HumanEval pass@k rates comparable to CodeParrot (no quantitative analysis of run-to-run variability or reproducibility tests were reported).",
            "uuid": "e492.0"
        },
        {
            "name_short": "Brownian bridge latent process",
            "name_full": "Brownian bridge stochastic process for sentence-level latent trajectories",
            "brief_description": "A continuous-time stochastic process used to model and sample latent sentence embeddings that interpolate noisily between a start and end latent code (density p(z_t | z_0, z_T) is Gaussian with mean linear interpolation and variance t(T-t)/T).",
            "citation_title": "Language modeling via stochastic processes",
            "mention_or_use": "use",
            "model_name": "Modified Time Control (encoder uses Brownian-bridge priors mapped from CodeParrot hidden states)",
            "model_size": "1.5B",
            "scientific_domain": "language modeling applied to code generation",
            "experimental_task": "Learn sentence-level latent representations that follow a Brownian-bridge process to condition code generation for improved global coherence.",
            "variability_sources": "Intrinsic stochasticity of the Brownian-bridge (random Gaussian deviations around the linear interpolation), sampling different latent trajectories for the same document lead to different generated outputs.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Stochastic latent sampling introduces nondeterminism in outputs; ensuring triplets follow the Brownian-bridge density requires a contrastive training objective which the paper applies, but no replication study is reported.",
            "mitigation_methods": "Training the encoder with a contrastive objective so that every triplet of sentence embeddings follows the Brownian-bridge transition density (i.e., enforcing the assumed stochastic structure).",
            "mitigation_effectiveness": "Not reported in terms of reduced output variance; authors state the model learned smooth Brownian-bridge trajectories and in some cases improved generation, but provide no quantitative variability reduction measurements.",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The Brownian-bridge prior was successfully used to model sentence-level latent trajectories and enforced via a contrastive objective, but the paper does not quantify how much this reduces stochastic variability in final code outputs.",
            "uuid": "e492.1"
        },
        {
            "name_short": "Pass@k",
            "name_full": "Pass@k (functional correctness metric over k samples)",
            "brief_description": "An evaluation metric for code generation that reports the probability that at least one of k sampled outputs passes unit tests (interpreted as evaluating the best out of k samples chosen by an oracle).",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "use",
            "model_name": "Modified Time Control / CodeParrot (evaluated using HumanEval pass@k)",
            "model_size": "1.5B (table also reports a 110M CodeParrot result)",
            "scientific_domain": "code generation benchmarking / functional evaluation",
            "experimental_task": "Measuring functional correctness on HumanEval by sampling k outputs per problem and computing pass@k.",
            "variability_sources": "Sampling variability across generated outputs (pass@k reflects best-of-k sampling variability) and dependence on oracle selection of the best sample; underlying model randomness in sampling influences pass@k values.",
            "variability_measured": false,
            "variability_metrics": "pass@1, pass@10, pass@100 (point estimates reported)",
            "variability_results": "Table 2 point estimates: TIMECONTROL PASS@1=3.7%, PASS@10=6.58%, PASS@100=12.73%; CODEPARROT 110M PASS@1=3.80%, PASS@10=6.57%, PASS@100=12.78%; CODEPARROT 1.5B PASS@1=3.58%, PASS@10=8.03%, PASS@100=14.96%.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Pass@k depends on the number of samples k and nondeterministic sampling; the metric uses an oracle to pick the best sample which complicates reproducibility unless sampling seeds and exact sampling procedure are fixed.",
            "mitigation_methods": "Not specifically applied here; implicit mitigation would be to report pass@k across fixed sampling seeds or provide raw samples, but the paper does not perform those controls.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Pass@k is used as the primary functional metric; point-estimate pass@k values show Modified Time Control performs comparably to CodeParrot baselines under the reported experimental setup, but no analysis of sampling variability or uncertainty around these estimates is provided.",
            "uuid": "e492.2"
        },
        {
            "name_short": "Stochastic gradient descent (SGD)",
            "name_full": "Stochastic gradient descent optimizer (with momentum)",
            "brief_description": "The optimization algorithm used to train the encoder MLP (four-layer) on top of frozen CodeParrot hidden states, using SGD with specified learning rate and momentum.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Encoder MLP on top of CodeParrot hidden states",
            "model_size": null,
            "scientific_domain": "model training / optimization",
            "experimental_task": "Training the encoder MLP (latent mapper) with SGD (learning rate 1e-4, momentum 0.9) to produce sentence embeddings that follow the Brownian-bridge prior.",
            "variability_sources": "Optimizer stochasticity arising from minibatch sampling, random initialization of MLP parameters, and nondeterministic training order; these affect learned latents and downstream generation.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "No fixed-seed or replication experiments described; optimizer-induced nondeterminism and lack of reported random-seed control make exact reproduction difficult.",
            "mitigation_methods": "Paper reports fixed hyperparameters (learning rate = 1e-4, momentum = 0.9) and a frozen pretrained CodeParrot backbone to reduce variance from large-scale retraining, but does not report seed controls.",
            "mitigation_effectiveness": "Not evaluated quantitatively.",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "SGD with fixed hyperparameters was used to train the encoder MLP, but the paper does not assess how optimization stochasticity affects downstream code-generation variability or reproducibility.",
            "uuid": "e492.3"
        },
        {
            "name_short": "Autoregressive sampling / decoding",
            "name_full": "Autoregressive left-to-right token sampling",
            "brief_description": "The decoding procedure of CodeParrot and Modified Time Control: an auto-regressive LM that samples tokens left-to-right conditioned on past tokens and the sentence-level latent embedding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeParrot (fine-tuned decoder) / Modified Time Control decoder",
            "model_size": "1.5B",
            "scientific_domain": "language generation for code",
            "experimental_task": "Generating code sequences conditioned on past tokens and latent sentence embeddings to reconstruct training documents and to sample candidate solutions for evaluation.",
            "variability_sources": "Nondeterministic sampling choices during autoregressive decoding (sampling distribution, top-k/top-p/temperature if used though not specified), and dependence on latent embeddings (which are stochastic due to Brownian-bridge).",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Authors do not report controlling sampling seeds or sampling hyperparameters; oracle selection in pass@k further complicates reproducibility of reported pass@k values.",
            "mitigation_methods": "Architectural conditioning on latent sentence embeddings (the Brownian-bridge plan) intended to induce more coherent and stable generation, and fine-tuning an existing pretrained model to limit variability from training from scratch.",
            "mitigation_effectiveness": "Qualitatively reported as maintaining code quality and sometimes improving coherence, but no quantitative variance-reduction numbers are provided.",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Autoregressive stochastic sampling is a core source of output variability (implicit in the use of pass@k), and the paper uses latent conditioning to aim for more globally coherent outputs but does not quantify reductions in sampling-induced variability.",
            "uuid": "e492.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language modeling via stochastic processes",
            "rating": 2,
            "sanitized_title": "language_modeling_via_stochastic_processes"
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Measuring coding challenge competence with apps",
            "rating": 1,
            "sanitized_title": "measuring_coding_challenge_competence_with_apps"
        },
        {
            "paper_title": "CodeBLEU: a method for automatic evaluation of code synthesis",
            "rating": 1,
            "sanitized_title": "codebleu_a_method_for_automatic_evaluation_of_code_synthesis"
        }
    ],
    "cost": 0.01277625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Stochastic Code Generation
14 Apr 2023</p>
<p>Swapnil Sharma 
Nikita Anand 
Kranthi Kiran 
G V 
Stochastic Code Generation
14 Apr 2023
Large language models pre-trained for code generation can generate high-quality short code, but often struggle with generating coherent long code and understanding higher-level or systemlevel specifications. This issue is also observed in language modeling for long text generation, and one proposed solution is the use of a latent stochastic process. This approach involves generating a document plan and then producing text that is consistent with it.In this study, we investigate whether this technique can be applied to code generation to improve coherence. We base our proposed encoder and decoder on the pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for training. We evaluate our results using the Hu-manEval benchmark and observe that the modified Time Control model performs similarly to CodeParrot on this evaluation.</p>
<p>Introduction</p>
<p>Large pre-trained language models have gained immense popularity recently. Large language models have achieved impressive results in various natural language processing tasks, including language translation, summarization, and text generation. These models are trained on a large corpus of text data and are able to capture complex patterns and relationships in the data. One impactful application of these models is code generation.</p>
<p>Code generation is the process of automatically generating computer programs based on natural language descriptions or higher-level specifications. Multiple large scale models like PyMT5 (Clement et al., 2020), Codebert (Feng et al., 2020a) and Codex (Chen et al., 2021) have been trained for the task and are used to aid software developers. They are 1 Department of Computer Science, Courant Institute of Mathematical Sciences, New York University.</p>
<p>Correspondence to: Swapnil Sharma <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#115;&#115;&#49;&#52;&#52;&#49;&#50;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;">&#115;&#115;&#49;&#52;&#52;&#49;&#50;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a>, Nikita Anand <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#110;&#97;&#50;&#56;&#53;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;">&#108;&#110;&#97;&#50;&#56;&#53;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a>, Kranthi Kiran GV <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#114;&#97;&#110;&#116;&#104;&#105;&#46;&#103;&#118;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;">&#107;&#114;&#97;&#110;&#116;&#104;&#105;&#46;&#103;&#118;&#64;&#110;&#121;&#117;&#46;&#101;&#100;&#117;</a>. used in integrated development environments for code completion, to convert natural language documentation (docstrings) into code and vice versa.</p>
<p>These models produce high quality short pieces of code but as pointed out by Chen et al., 2021, they tend to struggle with long and higher-level or system-level specifications. An experiment was run on Codex that indicates performance degradation as docstring length increases.</p>
<p>As stated by Wang et al., 2022, this problem also occurs in text generation where models tend to meander or lose coherency when producing long text. They propose Time Control, a language model that learns a mapping of how text changes in a document with the changes in the document plan generated via a stochastic process. Text is then generated to be consistent with this document plan. This helps the model learn structure and leads to locally and globally more coherent text.</p>
<p>In this work, we adapt the stochastic process for the code generation task. We do this by basing the proposed encoderdecoder architecture on the pre-trained CodeParrot model (Tunstall et al., 2022) which was trained using 180 gigabytes of Python code. We use the Automated Programming Progress Standard dataset Hendrycks et al., 2021 to generate the latent plan and train the encoder and fine-tune the decoder. We compare our model's (Modified Time Control) performance to the regular pre-trained CodeParrot model on the HumanEval (Chen et al., 2021) Evaluation framework and see that they perform similarly. We also compare the results manually to get a better understanding of when it works well and when it doesn't.</p>
<p>Related Work</p>
<p>Autoregressive models, which are commonly used for generating text, may struggle to produce long and coherent text because they lack the ability to model text structure and dynamics (Wang et al., 2022;Lin et al., 2021). This can result in globally incoherent text as the models are unable to effectively plan and anticipate long context in the text. When forced to generate longer texts, this incoherence is often worsened as the models struggle to extend beyond their expected text end point. To address this issue, previous re-search has employed planning-based approaches in an attempt to generate globally coherent text (Puduppully et al., 2019;Kiddon et al., 2016).</p>
<p>Program induction is a machine learning technique in which a model generates program outputs from a latent program representation (Zaremba &amp; Sutskever, 2014;Pierrot et al., 2019). Program synthesis involves generating a program from a natural language specification (Feng et al., 2020b). Both approaches have been improved upon through the incorporation of inductive biases and the use of recurrence and abstract syntax trees.</p>
<p>The Codex paper by Chen et al., 2021 introduces a large language model called Codex that is finetuned on code from GitHub and is able to generate Python code. The model was evaluated on the HumanEval dataset (Chen et al., 2021), which measures functional correctness for synthesizing programs from docstrings, and was found to solve 28.8% of the problems, outperforming the GPT-3 model which solved 0%. The limitations of Codex include difficulty with docstrings describing long chains of operations and with binding operations to variables.</p>
<p>Method</p>
<p>In this section we describe the parts of Language Modeling via Stochastic Processes (Wang et al., 2022) that we use to run our experiments. In the following Experiments section, we describe the changes made to adapt this method to code generation in detail.</p>
<p>Architecture</p>
<p>We adapt the same structure as Time Control but make modifications to run for code generation. The proposed solution is a modified encoder-decoder model built on top of Code-Parrot.</p>
<p>Encoder The encoder in this study is a nonlinear mapping that transforms raw input data into a low-dimensional latent space. The goal of the encoder is to map highdimensional sequential data into a latent space that follows a stochastic process of interest, similar to the Brownian bridge process described by Wang et al., 2022. As demonstrated by Wang et al., 2022, the density of a Brownian bridge process between an arbitrary start point, z 0 , at time t = 0 and end point, z T , at time t = T , is given by: LEVEL  TOTAL COUNT TEST SET   INTRODUCTORY  3639  1000  INTERVIEW  5000  3000  COMPETITION  1361  1000   Table 1. Problems of varying difficulty levels in the APPS dataset and end points of the trajectory, with the center being the most noisy. To ensure that every triplet of observations, (x 1 , x 2 , x 3 ), follows the Brownian bridge transition density, we use a contrastive objective.
p(z t |z 0 , z T ) = N 1 − t T z 0 + t T z T , t(T − t) T</p>
<p>This equation acts as a noisy interpolation between the start</p>
<p>Decoder The decoding component of the system is responsible for generating code from latent plans. To do this, we first map all lines in the training dataset to our learned latent space using the pretrained encoder, f θ . This produces a Brownian bridge trajectory of sentence-level latent codes, (z 0 , · · · , z t , · · · , z T ), for each entry in the dataset. Rather than training a decoder from scratch, we fine-tune the existing model, CodeParrot, to generate code conditioned on past context and the latent structure.</p>
<p>To fine-tune CodeParrot, we use a standard auto-regressive language model that has been modified as follows. Given a document with W tokens and T sentences used to train the decoder, x 1 · · · x W , we obtain embeddings, z 1 · · · z 3 , for each sentence using the encoder, f θ . At time t, the decoder must predict x t using all past tokens, x &lt;t , as well as the sentence embedding, z st , where the index s t ∈ [T ] maps each token to its corresponding sentence. This process can be viewed as a reconstruction objective, as the identity of x t is encoded in z st .</p>
<p>Dataset</p>
<p>The Automated Programming Progress Standard (APPS) dataset introduced by Hendrycks et al., 2021 consists of coding problems collected from multiple open access websites like Codeforces, Kattis etc. It contains 10,000 problems of varying levels; introductory, interview level and competition level. The introductory problems are ones whose solutions do not require any complicated algorithms, and can be solved by people with a year or two of programming experience. The interview level questions are ones that are used to test technical proficiency and require nontrivial solutions. They can typically involve the use of data structures like trees, linked lists etc. The competition level questions are ones that require advanced collegiate level knowledge.</p>
<p>The latent structure was labelled using an open-source tool called tree-sitter 1 . This tool is a parser that builds a syn------Question-----Polycarp analyzes the prices of the new berPhone. At his disposal are the prices for n last days: a_1, a_2, ... , a_n, where a_i is the price of berPhone on the day i. Polycarp considers the price on the day i to be bad if later (that is, a day with a greater number) berPhone was sold at a lower price. For example, if n=6 and a= [3,9,4,6,7,5], then the number of days with a bad price is 3 -these are days 2 (a_2=9), 4 (a_4=6) and 5 (a_5 =7). Print the number of days with a bad price. You have to answer t independent data sets .</p>
<p>-----Input-----</p>
<p>The first line contains an integer t (1 &lt;= t &lt;= 10000$) -the number of sets of input data in the test. Input data sets must be processed independently, one after another. Each input data set consists of two lines. The first line contains an integer n (1 &lt;= n &lt;= 150000) -the number of days. The second line contains n integers a_1, a_2, ... , a_n (1 &lt;= a_i &lt;= 10ˆ6), where a_i is the price on the i-th day. It is guaranteed that the sum of n over all data sets in the test does not exceed 150000.</p>
<p>-----Output-----Print t integers, the j-th of which should be equal to the number of days with a bad price in the j-th input data set.</p>
<p>-----Example-----Input 5 6 3 9 4 6 7 5 1 1000000 2 2 1 10 31 41 59 26 53 58 97 93 23 84 7 3 2 1 2 3 4 5 Output 3 0 1 8 2 Listing 1. Sample Introductory Level Question from APPS tax tree for a source file. It works with any programming language, is robust enough that it can work with minor errors and can be embedded in any application because it is dependency free.  (Wang et al., 2022) we reference is also based on GPT-2 which lead us to believe this model would be a good fit for our experiments.</p>
<p>Choice of Model</p>
<p>Experiments</p>
<p>The purpose of this study is to evaluate the capabilities of Time Control in capturing code structure and generating more robust code. Specifically, we aim to determine whether the generated code is able to compile and solve the problem stated in the prompt. To assess Time Control, we use a modified version with latent dimensions (d = 32).</p>
<p>The encoder architecture for this evaluation consists of a frozen, pretrained CodeParrot and a trainable multi-layer perceptron (MLP). We extract the last layer hidden state of CodeParrot that corresponds to the end-of-sentence token, and train the four-layer MLP on top of this hidden state. The MLP has intermediate rectified linear unit activations and is trained using stochastic gradient descent with a learning rate of 1e-4 and momentum of 0.9.</p>
<p>For the decoder, we also use CodeParrot as the base model. Similar to previous work (Wang et al., 2022), we modify CodeParrot to pass the encoder output after appending it to positional embeddings in the transformer layer. You can see one of the good samples generated in Figure 1 and some of the bad ones 2.</p>
<p>Datasets As the dataset for this study, we use APPS, which provides sufficient length code to fully utilize our To label the code in the dataset with the appropriate section identifiers, we utilize tree-sitter to mark each line of code with tree-sitter specific labels, and then retain only the relevant labels while discarding the rest.</p>
<p>Evaluation</p>
<p>To evaluate the model, we use the HumanEval framework. HumanEval contains 164 handwritten programming problems; this is done to ensure that the models could not have seen the problems before as most are trained on open access code from Github which contain solutions to problems from the web like the APPS dataset.</p>
<p>We don't use metrics like BLEU Score that are used for natural language because it neglects syntactic and semantic features of code (Ren et al., 2020) and doesn't work well for evaluatinh code generation.</p>
<p>Our results, shown in Table 2, indicate that modified Time Control performs similarly to CodeParrot on the HumanEval dataset. As described by Chen et al., 2021, "Pass@k can be interpreted as the result of evaluating the best out of k samples, where the best sample is picked by an oracle with prior knowledge of the unit tests". These findings suggest that Modified Time Control is a promising approach, as we were able to maintain code quality with our limited computational resources. We hope that with longer per-sentence lengths and additional computing resources, we will be able to further improve the performance of our model.</p>
<p>Conclusion</p>
<p>The purpose of this study was to investigate the potential of Time Control to improve code generation through the use of a latent stochastic process that generates output with learned implicit structure. Our results suggest that modified Time Control is capable of learning to map programming code to smooth Brownian bridge trajectories, and in some cases, we observed improved performance in the code generation process. These findings are encouraging for the use of Time Control in code generation.</p>
<p>To further explore the potential of Time Control, we suggest using larger datasets containing multiple files in a project as dataset entries. We believe that this approach may allow for the learning of code-level architecture. In addition, the use of enhanced datasets and additional computing resources may lead to even more improved results with modified Time Control.</p>
<p>Future Work</p>
<p>Due to computational constraints, we were unable to utilize approximately 70% of our training set and had to limit the decoder to less than 750 tokens. It is possible that the use of the full dataset may have produced different results.</p>
<p>In an ideal scenario, we would have preferred to conduct these experiments using a more structured dataset, such as programs with the model-view-controller software architecture pattern. Such a dataset would provide higher-level specifications and have a well-defined structure that may be particularly well-suited to benefit from a latent process like the one used in this study.</p>
<p>[QUESTION]:</p>
<p>In this problem we assume the Earth to be a completely round ball and its surface a perfect sphere. The length of the equator and any meridian is considered to be exactly 40 000 kilometers. Thus, travelling from North Pole to South Pole or vice versa takes exactly 20 000 kilometers.</p>
<p>Limak, a polar bear, lives on the North Pole. Close to the New Year, he helps somebody with delivering packages all around the world. Instead of coordinates of places to visit, Limak got a description how he should move, assuming that he starts from the North Pole. The description consists of n parts. In the i-th part of his journey, Limak should move t_{i} kilometers in the direction represented by a string dir_{i} that is one of: "North", "South", "West", "East".</p>
<p>Limak isn't sure whether the description is valid. You must help him to check the following conditions: If at any moment of time (before any of the instructions or while performing one of them) Limak is on the North Pole, he can move only to the South . If at any moment of time (before any of the instructions or while performing one of them) Limak is on the South Pole, he can move only to the North . return "NO" if latitude != 0: return "NO" return "YES" def read(callback=int): return list(map(callback, input().strip().split())) def write(value, end="\n"): if value is None: return try: if not isinstance(value, str): value = " ".join(map(str, value)) except: pass print(value, end=end) write(main())</p>
<p>Figure 1. Problem Statement and Solution generated by Modified Time Control</p>
<p>[QUESTION] Given an array of is, you can find a array where each element has equal letters.</p>
<p>[SOLUTION] a = list(str(input())) for i in range(1,len(a)): sh = a[i].swapcase() tell = sh ['what'] ... x,n = list(a.swapcase()) a.sort() pivot = """" pivot_pos = pivot_pos__starting_point__starting_point__starting_point__starting_point__len() if pivot_pos:</p>
<p>pivot_pos += 1 pivot_pos = pivot_pos__starting_point__starting_point__end() gcdrom = pivot ocdr = pivot while True: a,r = pivot_pos__starting_point__starting_point__starting_point... </p>
<p>Figure 2 .
2Sample from Modified Time Control with incomprehensible question and output Clement, C., Drain, D., Timcheck, J., Svyatkovskiy, A., and Sundaresan, N. PyMT5: multi-mode translation of natural language and python code with transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9052-9065, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.728. URL https://aclanthology.org/2020.emnlp-main.728.</p>
<p>Some of the most popular language models like Codex which powers the industry product Github Copilot, are not open-source and thus unusable in this work. Our choice of pre-trained model for code generation is largely based on what was open sourced and popular on the Hugging Face library(Wolf et al., 2019).For our experiments we use CodeParrot (Tunstall et al., 
2022) which is a GPT-2 (Radford et al., 2019) based model 
trained to generate Python code. It was downloaded 22,416 
times from Hugging Face in the last month. CodeParrot is 
an auto-regressive Left-to-Right Language Model that pre-
dict the probability of a token given the previous tokens. It 
has 1.5 billion parameters and was trained on 180 GB of 
Python code. The Time Control model </p>
<p>Table 2 .
2Model accuracy with baseline CodeParrot.No additional tokens are added in this dataset.MODEL 
PASS@1 PASS@10 PASS@100 </p>
<p>TIMECONTROL 
3.7% 
6.58% 
12.73% </p>
<p>CODEPARROT 110M 
3.80% 
6.57% 
12.78% 
CODEPARROT 1.5B 
3.58% 
8.03% 
14.96% </p>
<p>available resources. We modify the dataset so that each 
entry contains five sections (question, solution, class state-
ment, def statement, import statement) marked with section 
identifier tokens. Specifically, each entry is represented as, </p>
<p>[QUESTION] 
<text> </p>
<p>[SOLUTION] 
[CLASS_STATEMENT] 
<code> 
[DEF_STATEMENT] 
<code> 
[IMPORT_STATEMENT] 
<code> </p>
<p>https://github.com/tree-sitter/tree-sitter</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, Evaluating large language models trained on code. 2021Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar- ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. 2021.</p>
<p>Codebert: A pre-trained model for programming and natural languages. CoRR, abs. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, M Zhou, Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Codebert: A pre-trained model for programming and nat- ural languages. CoRR, abs/2002.08155, 2020a. URL https://arxiv.org/abs/2002.08155.</p>
<p>Codebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, Findings of the Association for Computational Linguistics: EMNLP 2020. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et al. Code- bert: A pre-trained model for programming and natu- ral languages. In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pp. 1536-1547, 2020b.</p>
<p>Measuring coding challenge competence with apps. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, J Steinhardt, NeurIPSHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge com- petence with apps. NeurIPS, 2021.</p>
<p>Globally coherent text generation with neural checklist models. C Kiddon, L Zettlemoyer, Y Choi, Proceedings of the 2016 conference on empirical methods in natural language processing. the 2016 conference on empirical methods in natural language processingKiddon, C., Zettlemoyer, L., and Choi, Y. Globally coher- ent text generation with neural checklist models. In Pro- ceedings of the 2016 conference on empirical methods in natural language processing, pp. 329-339, 2016.</p>
<p>Limitations of autoregressive models and their alternatives. C.-C Lin, A Jaech, X Li, M R Gormley, J Eisner, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLin, C.-C., Jaech, A., Li, X., Gormley, M. R., and Eisner, J. Limitations of autoregressive models and their alter- natives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, pp. 5147-5173, 2021.</p>
<p>Learning compositional neural programs with recursive tree search and planning. T Pierrot, G Ligner, S E Reed, O Sigaud, N Perrin, A Laterre, D Kas, K Beguir, N De Freitas, Advances in Neural Information Processing Systems. 32Pierrot, T., Ligner, G., Reed, S. E., Sigaud, O., Perrin, N., Laterre, A., Kas, D., Beguir, K., and de Freitas, N. Learn- ing compositional neural programs with recursive tree search and planning. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Data-to-text generation with content selection and planning. R Puduppully, L Dong, M Lapata, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33Puduppully, R., Dong, L., and Lapata, M. Data-to-text gen- eration with content selection and planning. In Proceed- ings of the AAAI conference on artificial intelligence, volume 33, pp. 6908-6915, 2019.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multi- task learners. 2019.</p>
<p>. S Ren, D Guo, S Lu, L Zhou, S Liu, D Tang, N Sundaresan, M Zhou, A Blanco, S Ma, Codebleu, Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S. Codebleu: a method for automatic evaluation of code synthesis. CoRR, abs/2009.10297, 2020. URL https://arxiv.org/abs/2009.10297.</p>
<p>L Tunstall, L Von Werra, T Wolf, Natural Language Processing with Transformers: Building Language Applications with Hugging Face. Tunstall, L., von Werra, L., and Wolf, T. Natural Language Processing with Transformers: Build- ing Language Applications with Hugging Face.</p>
<p>. O&apos;reilly Media, 2022O'Reilly Media, 2022. ISBN 9781098103248. URL https://books.google.com/books?id=_0uezgEACAAJ.</p>
<p>Language modeling via stochastic processes. R E Wang, E Durmus, N Goodman, T Hashimoto, International Conference on Learning Representations. Wang, R. E., Durmus, E., Goodman, N., and Hashimoto, T. Language modeling via stochas- tic processes. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=pMQwKL1yctf.</p>
<p>. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Brew, Huggingface, s transformers: State-of-the-art natural language processingWolf, T., Debut, L., Sanh, V., Chaumond, J., De- langue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., and Brew, J. Huggingface's transformers: State-of-the-art natural language pro- cessing.</p>
<p>. Corr, abs/1910.03771CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771.</p>
<p>. W Zaremba, I Sutskever, arXiv:1410.4615Learning to execute. arXiv preprintZaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.</p>            </div>
        </div>

    </div>
</body>
</html>
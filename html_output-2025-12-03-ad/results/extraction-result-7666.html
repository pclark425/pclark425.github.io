<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7666 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7666</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7666</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-265043839</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.eval4nlp-1.14.pdf" target="_blank">Which is better? Exploring Prompting Strategy For LLM-based Metrics</a></p>
                <p><strong>Paper Abstract:</strong> This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7666.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7666.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Guideline prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt template adapted from human evaluation guidelines (SummEval) that provides succinct, clear task instructions, explicit aspect definitions and stepwise evaluation steps for LLM-based summary evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B; Hermes-13B; Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLaMA-family models and derivatives (Orca, Hermes, Platypus) used as reference-free evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B; 13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation (Eval4NLP / SummEval based reference-free metric)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rate a candidate summary against a source document on multiple aspects (relevance, consistency/factuality, coherence, fluency) and output an overall score (average of aspect scores).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt template (Human Guideline) with evaluation criteria, step-by-step evaluation steps, and optional fine-grained per-aspect scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Human-readable guideline adapted from SummEval: contains task description (short/long; optional expert role), aspect definitions (AD), human-targeted scoring criteria (HT), and explicit evaluation steps (CoT-style). Fine-grained scoring recommended. No demonstrated examples for the canonical variant used in dev experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Orca-7B: 0.3094; Orca-13B: 0.4343; Hermes-13B: 0.2041; Platypus-70B: 0.4260 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline prompt (shared-task prompt) — Orca-7B: 0.2500; Orca-13B: 0.3040; Hermes-13B: 0.1554; Platypus-70B: 0.3956 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-7B +0.0594 absolute; Orca-13B +0.1303 absolute; Hermes-13B +0.0487 absolute; Platypus-70B +0.0304 absolute (HG vs baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation, temperature=0 for Direct/Logprob; fine-grained scoring in these reported runs; no in-context demonstration for canonical HG entries.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7666.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Guideline prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more directive, model-targeted instruction template (implemented from G-EVAL style guidelines) giving explicit scoring heuristics and model-facing criteria for summary evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B; Hermes-13B; Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-derived and fine-tuned instruction models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B; 13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation (reference-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same summarization evaluation task as HG prompt but using model-targeted (directive) criteria and scoring instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt template (Model Guideline) with model-targeted criteria and evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>MG includes task description, model-targeted criteria (MT), and step instructions. Variants altering length and expert role were also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Orca-7B: 0.2651; Orca-13B: 0.3583; Hermes-13B: 0.1915; Platypus-70B: 0.4383 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline prompt — Orca-7B: 0.2500; Orca-13B: 0.3040; Hermes-13B: 0.1554; Platypus-70B: 0.3956</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-7B +0.0151 absolute; Orca-13B +0.0543 absolute; Hermes-13B +0.0361 absolute; Platypus-70B +0.0427 absolute (MG vs baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation, temperature=0; fine-grained scoring when reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7666.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring granularity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained vs Coarse-grained scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between giving the model a single holistic score (coarse-grained) versus asking per-aspect scores (fine-grained) that are averaged to obtain the final score; fine-grained reduces ambiguity about aspect-specific considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B; Hermes-13B; Platypus-70B (reported generally across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM evaluators used to generate per-aspect or holistic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B; 13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assign evaluation scores to summaries either as a single overall score or separately per aspect (relevance, consistency, coherence, fluency) and average.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt asks for either one overall score (coarse) or multiple labeled aspect scores then average (fine-grained).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / score format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Fine-grained prompts explicitly request separate aspect scores and then averaging; coarse-grained ask for a single holistic score. Variants tested across prompt templates and models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported qualitatively: fine-grained scoring 'consistently outperforms' coarse-grained scoring across model sizes and prompt templates (authors attribute improvements to reduced ambiguity), but per-model absolute numbers for every coarse→fine pair are not tabulated exhaustively in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-grained experiments were used by default for many reported runs (Direct aggregation, temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7666.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration (ICL) presence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context demonstration examples (best/worst; with/without rationale)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a single in-context example (either 'best' or 'worst' example based on human score), optionally including an explanatory rationale, affects evaluation performance — demonstrations often introduce bias and can decrease performance, especially for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Orca models tested for in-context learning influence on evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation with ICL examples</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Include one demonstration example (source, summary, score; optionally rationale) in the prompt to test In-Context Learning effect on scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot (1-shot) demonstration inside prompt; example types: Base-worst (score only, worst), Base-best (score only, best), Reason-worst/reason-best (score + rationale).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Single example per prompt due to input length limits; examples categorized by human score (1 or 5) and whether they include rationales (GPT-4 distilled).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples (HG prompt): Orca-7B no-demo (HG canonical) 0.3094; Orca-7B Base-worst demo 0.1758; Orca-7B Base-best demo 0.2854. Orca-13B no-demo 0.4343; Orca-13B Base-worst 0.3690; Orca-13B Base-best 0.4092 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>HG no demonstration — Orca-7B 0.3094; Orca-13B 0.4343</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-7B: Base-worst demo −0.1336 absolute vs HG no-demo; Base-best demo −0.0240 absolute vs HG no-demo. Orca-13B: Base-worst demo −0.0653 absolute; Base-best demo −0.0251 absolute (demonstrations tended to reduce performance; 'best' examples less harmful than 'worst').</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation; demonstrations limited to one example; rationale examples were generated using GPT-4; authors used top_p=0.1 for sampling where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7666.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration content (rationale vs score-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstration content: rationale-included vs score-only examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Whether the in-context example contains only a score or also a rationale influences performance differently by model size — larger models better utilize rationale-containing examples, while smaller models can be more negatively affected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Orca-family open-source LLMs; different capability in ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation with rationale demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt includes a demonstration containing source, summary, score, and optionally an explanatory rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot demonstration with or without rationale (reason-best / reason-worst).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Rationale examples distilled by GPT-4 were used. Authors compared models' responses when example contained only score vs score+reason.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (RG prompt results): Orca-13B Reason-best with RG prompt: 0.4330 (Kendall's Tau); compare to HG no-demo 0.4343.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>HG no-demo — Orca-13B 0.4343</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-13B Reason-best −0.0013 absolute vs HG no-demo (i.e., roughly similar); authors report larger models tend to benefit more from rationale-containing demonstrations while smaller models may suffer, but exact per-model deltas depend on specific demo types.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation; rationale examples generated by GPT-4; only one demo due to length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7666.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct vs Logprob vs Approximation aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three aggregation strategies to convert model outputs into a continuous score: Direct (use predicted score token), Logprob (weighted sum using token generation probabilities), and Approximation (average over N sampled outputs); Direct and Logprob perform better than Approximation in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B; Hermes-13B; Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs assessed with different decoding/aggregation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B; 13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation (score aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert discrete token scores (1–5) produced by the model into a scalar evaluation score using different aggregation schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Output-to-score aggregation: Direct (use integer score output), Logprob (compute expectation using token log-probs), Approximation (sample N outputs and average).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>output decoding / scoring method</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Direct and Logprob runs used temperature=0; Approximation used temperature=1 and n_samples=20. Logprob computes sum_i p(si) * si. Approximation uses empirical frequency from sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Human prompt, Orca-13B: Direct 0.4468; Logprob 0.4210; Approximation 0.4002. Human prompt, Orca-7B: Direct 0.3472; Logprob 0.3296; Approximation 0.3239 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-13B: Direct vs Approximation +0.0466 absolute; Direct vs Logprob +0.0258 absolute. Orca-7B: Direct vs Approximation +0.0233 absolute; Direct vs Logprob +0.0176 absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct/Logprob: temperature=0, top_p=0.1 for reported Direct runs; Approximation: temperature=1, n_samples=20.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7666.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rationale Generation prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt variant that asks the model to produce not only a score but also a rationale explaining the assigned score; used to measure explainability and to see if explanation generation helps scoring performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM evaluators producing both score and rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation with rationale output</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a numeric score and a natural-language rationale for each aspect or the overall score.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt requesting score + free-text rationale (CoT-like). Optionally includes demonstrated example (Reason-best/worst).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / explainability</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>RG prompt designed to elicit a rationale; experiments compared RG to HG and MG prompts and tested effect of demonstrations on rationale generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level) and qualitative error analysis of rationales</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Orca-7B RG Reason-best Direct: 0.3262; Orca-13B RG Reason-best Direct: 0.4330 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>HG no-demo — Orca-7B 0.3094; Orca-13B 0.4343</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-7B +0.0168 absolute (RG Reason-best vs HG); Orca-13B −0.0013 absolute (RG Reason-best vs HG) — overall RG yields similar or slightly lower performance than HG and better than MG in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation, rationale examples sometimes provided; error analysis performed on rationales revealing inconsistent rationale-score alignment, hallucinations, and wrong-aspect rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7666.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task description variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task description length and expert role</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants in the task description (short vs long; adding an 'expert' role) affect model performance: longer descriptions help some larger models, and assigning an expert role improved performance for smaller models in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Orca models tested with prompt task-description variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task description text preceding evaluation instructions, varied in length and with/without 'you are an expert' framing.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt preamble variations: short vs long task description; optional expert-role framing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt preamble</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors crafted short and long descriptions and an expert-role variant for both HG and MG prompts and measured performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example numbers (Human prompt): Orca-7B Base: 0.3472; Human Expert Short: 0.3544 (Kendall's Tau). Orca-13B Base: 0.4468; Human Long: 0.4501 (Kendall's Tau).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Human prompt Base — Orca-7B 0.3472; Orca-13B 0.4468</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Orca-7B Expert Short +0.0072 absolute; Orca-13B Long +0.0033 absolute (authors report that Orca-13B benefits from longer descriptions and Orca-7B benefits from the expert-role framing).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation, fine-grained scoring; no demonstration in these task-description experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7666.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation criteria variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aspect Definition (AD) / Human-Targeted (HT) / Model-Targeted (MT) criteria</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different ways of specifying evaluation criteria (concise aspect definitions vs detailed human-targeted or model-targeted criteria) affect performance; providing concise aspect definitions (AD) consistently improves scoring quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B; other tested models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B; 70B in other experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt contains different formulations of scoring criteria: concise AD, HT used in HG prompts, MT used in MG prompts, and GPT-4-generated variants of AD/HT/MT.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt includes a scoring rubric section; variants differ in verbosity and target audience (human vs model).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / scoring rubric</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>AD is concise aspect definition adapted from GPTScore; HT and MT include more detailed scoring considerations. Experiments tested AD, AD-GPT, HT, HT-GPT, MT, MT-GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Human prompt): Orca-7B AD: 0.3343; HT: 0.3256. Orca-13B AD: 0.4279; HT: 0.4192 (Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Providing AD (concise aspect definitions) improved Orca-7B by +0.0087 absolute vs HT and improved Orca-13B by +0.0087 absolute vs HT in reported runs (authors report AD consistently helps).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation; AD/HT/MT variants sometimes generated or paraphrased by GPT-4 for tests.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7666.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation steps complexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple vs complex evaluation steps (Chain-of-Thought style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Altering the complexity of step-by-step evaluation instructions (standard vs complex CoT-like steps) does not produce significant, consistent changes in evaluation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B; Orca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompts included either standard evaluation steps or more elaborate/complex stepwise instructions to encourage Chain-of-Thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt style: stepwise CoT-like instructions vs simpler steps.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / reasoning scaffold</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Complex evaluation steps were crafted using GPT-4; experiments compared base vs complex steps for HG and MG prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors report 'no significant trend' between standard and complex evaluation steps across models; i.e., complexity level of steps did not meaningfully change Kendall's Tau in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation; no demonstrations for these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7666.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7666.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Filtering & Binning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filtering failed summaries and Binning output scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test-time post-processing: filter out obvious failed summaries (assign minimum score) and bin continuous model scores into coarser categories; both steps reduced noise and improved test-set performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-13B; Platypus-70B (test submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM evaluators used for final shared-task submission.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Shared-task test set summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply prompt-based evaluation to the test set (Wikipedia-based samples) and post-process outputs to mitigate pathological summaries and over-fine-grained scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt + test-time filtering step (Yes/No question to detect redundancy/failure) and numeric score binning (reducing unique scores).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Filtering prompt identifies failures (repetitive / clearly failed summaries) and assigns minimum score. Binning reduces the number of unique scores (example: Orca-13B unique scores reduced from 36 to 10 after binning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall's Tau (segment-level) on test set (leaderboard entries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors report improved test-set performance after applying Filtering and Binning; specific Kendall Tau deltas vs unprocessed outputs are not enumerated in the main text tables.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct aggregation; for test submission top_p=0.1 and temperature=0 for Direct aggregation; Filtering prompt used to detect redundancy/failure; Binning reduced unique scores (e.g., Orca-13B: 36→10 unique scores).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>A survey for in-context learning <em>(Rating: 2)</em></li>
                <li>Larger language models do in-context learning differently <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7666",
    "paper_id": "paper-265043839",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "HG prompt",
            "name_full": "Human Guideline prompt",
            "brief_description": "A prompt template adapted from human evaluation guidelines (SummEval) that provides succinct, clear task instructions, explicit aspect definitions and stepwise evaluation steps for LLM-based summary evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B; Hermes-13B; Platypus-70B",
            "model_description": "Open-source instruction-tuned LLaMA-family models and derivatives (Orca, Hermes, Platypus) used as reference-free evaluators.",
            "model_size": "7B; 13B; 13B; 70B",
            "task_name": "Summarization evaluation (Eval4NLP / SummEval based reference-free metric)",
            "task_description": "Rate a candidate summary against a source document on multiple aspects (relevance, consistency/factuality, coherence, fluency) and output an overall score (average of aspect scores).",
            "problem_format": "Natural-language prompt template (Human Guideline) with evaluation criteria, step-by-step evaluation steps, and optional fine-grained per-aspect scoring.",
            "format_category": "prompt style",
            "format_details": "Human-readable guideline adapted from SummEval: contains task description (short/long; optional expert role), aspect definitions (AD), human-targeted scoring criteria (HT), and explicit evaluation steps (CoT-style). Fine-grained scoring recommended. No demonstrated examples for the canonical variant used in dev experiments.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Orca-7B: 0.3094; Orca-13B: 0.4343; Hermes-13B: 0.2041; Platypus-70B: 0.4260 (Kendall's Tau)",
            "baseline_performance": "Baseline prompt (shared-task prompt) — Orca-7B: 0.2500; Orca-13B: 0.3040; Hermes-13B: 0.1554; Platypus-70B: 0.3956 (Kendall's Tau)",
            "performance_change": "Orca-7B +0.0594 absolute; Orca-13B +0.1303 absolute; Hermes-13B +0.0487 absolute; Platypus-70B +0.0304 absolute (HG vs baseline)",
            "experimental_setting": "Direct aggregation, temperature=0 for Direct/Logprob; fine-grained scoring in these reported runs; no in-context demonstration for canonical HG entries.",
            "statistical_significance": null,
            "uuid": "e7666.0",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MG prompt",
            "name_full": "Model Guideline prompt",
            "brief_description": "A more directive, model-targeted instruction template (implemented from G-EVAL style guidelines) giving explicit scoring heuristics and model-facing criteria for summary evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B; Hermes-13B; Platypus-70B",
            "model_description": "Open-source LLaMA-derived and fine-tuned instruction models.",
            "model_size": "7B; 13B; 13B; 70B",
            "task_name": "Summarization evaluation (reference-free)",
            "task_description": "Same summarization evaluation task as HG prompt but using model-targeted (directive) criteria and scoring instructions.",
            "problem_format": "Natural-language prompt template (Model Guideline) with model-targeted criteria and evaluation steps.",
            "format_category": "prompt style",
            "format_details": "MG includes task description, model-targeted criteria (MT), and step instructions. Variants altering length and expert role were also tested.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Orca-7B: 0.2651; Orca-13B: 0.3583; Hermes-13B: 0.1915; Platypus-70B: 0.4383 (Kendall's Tau)",
            "baseline_performance": "Baseline prompt — Orca-7B: 0.2500; Orca-13B: 0.3040; Hermes-13B: 0.1554; Platypus-70B: 0.3956",
            "performance_change": "Orca-7B +0.0151 absolute; Orca-13B +0.0543 absolute; Hermes-13B +0.0361 absolute; Platypus-70B +0.0427 absolute (MG vs baseline)",
            "experimental_setting": "Direct aggregation, temperature=0; fine-grained scoring when reported.",
            "statistical_significance": null,
            "uuid": "e7666.1",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Scoring granularity",
            "name_full": "Fine-grained vs Coarse-grained scoring",
            "brief_description": "Comparison between giving the model a single holistic score (coarse-grained) versus asking per-aspect scores (fine-grained) that are averaged to obtain the final score; fine-grained reduces ambiguity about aspect-specific considerations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B; Hermes-13B; Platypus-70B (reported generally across models)",
            "model_description": "Open-source LLM evaluators used to generate per-aspect or holistic scores.",
            "model_size": "7B; 13B; 13B; 70B",
            "task_name": "Summarization evaluation",
            "task_description": "Assign evaluation scores to summaries either as a single overall score or separately per aspect (relevance, consistency, coherence, fluency) and average.",
            "problem_format": "Prompt asks for either one overall score (coarse) or multiple labeled aspect scores then average (fine-grained).",
            "format_category": "prompt style / score format",
            "format_details": "Fine-grained prompts explicitly request separate aspect scores and then averaging; coarse-grained ask for a single holistic score. Variants tested across prompt templates and models.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Reported qualitatively: fine-grained scoring 'consistently outperforms' coarse-grained scoring across model sizes and prompt templates (authors attribute improvements to reduced ambiguity), but per-model absolute numbers for every coarse→fine pair are not tabulated exhaustively in the paper.",
            "experimental_setting": "Fine-grained experiments were used by default for many reported runs (Direct aggregation, temperature=0).",
            "statistical_significance": null,
            "uuid": "e7666.2",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Demonstration (ICL) presence",
            "name_full": "In-context demonstration examples (best/worst; with/without rationale)",
            "brief_description": "Providing a single in-context example (either 'best' or 'worst' example based on human score), optionally including an explanatory rationale, affects evaluation performance — demonstrations often introduce bias and can decrease performance, especially for smaller models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B",
            "model_description": "Orca models tested for in-context learning influence on evaluation.",
            "model_size": "7B; 13B",
            "task_name": "Summarization evaluation with ICL examples",
            "task_description": "Include one demonstration example (source, summary, score; optionally rationale) in the prompt to test In-Context Learning effect on scoring.",
            "problem_format": "Few-shot (1-shot) demonstration inside prompt; example types: Base-worst (score only, worst), Base-best (score only, best), Reason-worst/reason-best (score + rationale).",
            "format_category": "prompt style / few-shot",
            "format_details": "Single example per prompt due to input length limits; examples categorized by human score (1 or 5) and whether they include rationales (GPT-4 distilled).",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Examples (HG prompt): Orca-7B no-demo (HG canonical) 0.3094; Orca-7B Base-worst demo 0.1758; Orca-7B Base-best demo 0.2854. Orca-13B no-demo 0.4343; Orca-13B Base-worst 0.3690; Orca-13B Base-best 0.4092 (Kendall's Tau)",
            "baseline_performance": "HG no demonstration — Orca-7B 0.3094; Orca-13B 0.4343",
            "performance_change": "Orca-7B: Base-worst demo −0.1336 absolute vs HG no-demo; Base-best demo −0.0240 absolute vs HG no-demo. Orca-13B: Base-worst demo −0.0653 absolute; Base-best demo −0.0251 absolute (demonstrations tended to reduce performance; 'best' examples less harmful than 'worst').",
            "experimental_setting": "Direct aggregation; demonstrations limited to one example; rationale examples were generated using GPT-4; authors used top_p=0.1 for sampling where applicable.",
            "statistical_significance": null,
            "uuid": "e7666.3",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Demonstration content (rationale vs score-only)",
            "name_full": "Demonstration content: rationale-included vs score-only examples",
            "brief_description": "Whether the in-context example contains only a score or also a rationale influences performance differently by model size — larger models better utilize rationale-containing examples, while smaller models can be more negatively affected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B",
            "model_description": "Orca-family open-source LLMs; different capability in ICL.",
            "model_size": "7B; 13B",
            "task_name": "Summarization evaluation with rationale demonstrations",
            "task_description": "Prompt includes a demonstration containing source, summary, score, and optionally an explanatory rationale.",
            "problem_format": "1-shot demonstration with or without rationale (reason-best / reason-worst).",
            "format_category": "prompt style / few-shot content",
            "format_details": "Rationale examples distilled by GPT-4 were used. Authors compared models' responses when example contained only score vs score+reason.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Example (RG prompt results): Orca-13B Reason-best with RG prompt: 0.4330 (Kendall's Tau); compare to HG no-demo 0.4343.",
            "baseline_performance": "HG no-demo — Orca-13B 0.4343",
            "performance_change": "Orca-13B Reason-best −0.0013 absolute vs HG no-demo (i.e., roughly similar); authors report larger models tend to benefit more from rationale-containing demonstrations while smaller models may suffer, but exact per-model deltas depend on specific demo types.",
            "experimental_setting": "Direct aggregation; rationale examples generated by GPT-4; only one demo due to length limits.",
            "statistical_significance": null,
            "uuid": "e7666.4",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Aggregation methods",
            "name_full": "Direct vs Logprob vs Approximation aggregation",
            "brief_description": "Three aggregation strategies to convert model outputs into a continuous score: Direct (use predicted score token), Logprob (weighted sum using token generation probabilities), and Approximation (average over N sampled outputs); Direct and Logprob perform better than Approximation in these experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B; Hermes-13B; Platypus-70B",
            "model_description": "Open-source LLMs assessed with different decoding/aggregation strategies.",
            "model_size": "7B; 13B; 13B; 70B",
            "task_name": "Summarization evaluation (score aggregation)",
            "task_description": "Convert discrete token scores (1–5) produced by the model into a scalar evaluation score using different aggregation schemes.",
            "problem_format": "Output-to-score aggregation: Direct (use integer score output), Logprob (compute expectation using token log-probs), Approximation (sample N outputs and average).",
            "format_category": "output decoding / scoring method",
            "format_details": "Direct and Logprob runs used temperature=0; Approximation used temperature=1 and n_samples=20. Logprob computes sum_i p(si) * si. Approximation uses empirical frequency from sampled outputs.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Human prompt, Orca-13B: Direct 0.4468; Logprob 0.4210; Approximation 0.4002. Human prompt, Orca-7B: Direct 0.3472; Logprob 0.3296; Approximation 0.3239 (Kendall's Tau)",
            "baseline_performance": null,
            "performance_change": "Orca-13B: Direct vs Approximation +0.0466 absolute; Direct vs Logprob +0.0258 absolute. Orca-7B: Direct vs Approximation +0.0233 absolute; Direct vs Logprob +0.0176 absolute.",
            "experimental_setting": "Direct/Logprob: temperature=0, top_p=0.1 for reported Direct runs; Approximation: temperature=1, n_samples=20.",
            "statistical_significance": null,
            "uuid": "e7666.5",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RG prompt",
            "name_full": "Rationale Generation prompt",
            "brief_description": "A prompt variant that asks the model to produce not only a score but also a rationale explaining the assigned score; used to measure explainability and to see if explanation generation helps scoring performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B",
            "model_description": "Open-source LLM evaluators producing both score and rationale.",
            "model_size": "7B; 13B",
            "task_name": "Summarization evaluation with rationale output",
            "task_description": "Generate a numeric score and a natural-language rationale for each aspect or the overall score.",
            "problem_format": "Natural-language prompt requesting score + free-text rationale (CoT-like). Optionally includes demonstrated example (Reason-best/worst).",
            "format_category": "prompt style / explainability",
            "format_details": "RG prompt designed to elicit a rationale; experiments compared RG to HG and MG prompts and tested effect of demonstrations on rationale generation quality.",
            "performance_metric": "Kendall's Tau (segment-level) and qualitative error analysis of rationales",
            "performance_value": "Orca-7B RG Reason-best Direct: 0.3262; Orca-13B RG Reason-best Direct: 0.4330 (Kendall's Tau)",
            "baseline_performance": "HG no-demo — Orca-7B 0.3094; Orca-13B 0.4343",
            "performance_change": "Orca-7B +0.0168 absolute (RG Reason-best vs HG); Orca-13B −0.0013 absolute (RG Reason-best vs HG) — overall RG yields similar or slightly lower performance than HG and better than MG in reported comparisons.",
            "experimental_setting": "Direct aggregation, rationale examples sometimes provided; error analysis performed on rationales revealing inconsistent rationale-score alignment, hallucinations, and wrong-aspect rationales.",
            "statistical_significance": null,
            "uuid": "e7666.6",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Task description variants",
            "name_full": "Task description length and expert role",
            "brief_description": "Variants in the task description (short vs long; adding an 'expert' role) affect model performance: longer descriptions help some larger models, and assigning an expert role improved performance for smaller models in some settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B",
            "model_description": "Orca models tested with prompt task-description variants.",
            "model_size": "7B; 13B",
            "task_name": "Summarization evaluation",
            "task_description": "Task description text preceding evaluation instructions, varied in length and with/without 'you are an expert' framing.",
            "problem_format": "Prompt preamble variations: short vs long task description; optional expert-role framing.",
            "format_category": "prompt style / prompt preamble",
            "format_details": "Authors crafted short and long descriptions and an expert-role variant for both HG and MG prompts and measured performance differences.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Example numbers (Human prompt): Orca-7B Base: 0.3472; Human Expert Short: 0.3544 (Kendall's Tau). Orca-13B Base: 0.4468; Human Long: 0.4501 (Kendall's Tau).",
            "baseline_performance": "Human prompt Base — Orca-7B 0.3472; Orca-13B 0.4468",
            "performance_change": "Orca-7B Expert Short +0.0072 absolute; Orca-13B Long +0.0033 absolute (authors report that Orca-13B benefits from longer descriptions and Orca-7B benefits from the expert-role framing).",
            "experimental_setting": "Direct aggregation, fine-grained scoring; no demonstration in these task-description experiments.",
            "statistical_significance": null,
            "uuid": "e7666.7",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Evaluation criteria variants",
            "name_full": "Aspect Definition (AD) / Human-Targeted (HT) / Model-Targeted (MT) criteria",
            "brief_description": "Different ways of specifying evaluation criteria (concise aspect definitions vs detailed human-targeted or model-targeted criteria) affect performance; providing concise aspect definitions (AD) consistently improves scoring quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B; other tested models",
            "model_description": "Open-source LLM evaluators",
            "model_size": "7B; 13B; 70B in other experiments",
            "task_name": "Summarization evaluation",
            "task_description": "Prompt contains different formulations of scoring criteria: concise AD, HT used in HG prompts, MT used in MG prompts, and GPT-4-generated variants of AD/HT/MT.",
            "problem_format": "Prompt includes a scoring rubric section; variants differ in verbosity and target audience (human vs model).",
            "format_category": "prompt style / scoring rubric",
            "format_details": "AD is concise aspect definition adapted from GPTScore; HT and MT include more detailed scoring considerations. Experiments tested AD, AD-GPT, HT, HT-GPT, MT, MT-GPT.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": "Example (Human prompt): Orca-7B AD: 0.3343; HT: 0.3256. Orca-13B AD: 0.4279; HT: 0.4192 (Kendall's Tau)",
            "baseline_performance": null,
            "performance_change": "Providing AD (concise aspect definitions) improved Orca-7B by +0.0087 absolute vs HT and improved Orca-13B by +0.0087 absolute vs HT in reported runs (authors report AD consistently helps).",
            "experimental_setting": "Direct aggregation; AD/HT/MT variants sometimes generated or paraphrased by GPT-4 for tests.",
            "statistical_significance": null,
            "uuid": "e7666.8",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Evaluation steps complexity",
            "name_full": "Simple vs complex evaluation steps (Chain-of-Thought style)",
            "brief_description": "Altering the complexity of step-by-step evaluation instructions (standard vs complex CoT-like steps) does not produce significant, consistent changes in evaluation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-7B; Orca-13B",
            "model_description": "Open-source LLM evaluators",
            "model_size": "7B; 13B",
            "task_name": "Summarization evaluation",
            "task_description": "Prompts included either standard evaluation steps or more elaborate/complex stepwise instructions to encourage Chain-of-Thought reasoning.",
            "problem_format": "Prompt style: stepwise CoT-like instructions vs simpler steps.",
            "format_category": "prompt style / reasoning scaffold",
            "format_details": "Complex evaluation steps were crafted using GPT-4; experiments compared base vs complex steps for HG and MG prompts.",
            "performance_metric": "Kendall's Tau (segment-level)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Authors report 'no significant trend' between standard and complex evaluation steps across models; i.e., complexity level of steps did not meaningfully change Kendall's Tau in their experiments.",
            "experimental_setting": "Direct aggregation; no demonstrations for these comparisons.",
            "statistical_significance": null,
            "uuid": "e7666.9",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Filtering & Binning",
            "name_full": "Filtering failed summaries and Binning output scores",
            "brief_description": "Test-time post-processing: filter out obvious failed summaries (assign minimum score) and bin continuous model scores into coarser categories; both steps reduced noise and improved test-set performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Orca-13B; Platypus-70B (test submissions)",
            "model_description": "Open-source LLM evaluators used for final shared-task submission.",
            "model_size": "13B; 70B",
            "task_name": "Shared-task test set summarization evaluation",
            "task_description": "Apply prompt-based evaluation to the test set (Wikipedia-based samples) and post-process outputs to mitigate pathological summaries and over-fine-grained scoring.",
            "problem_format": "Prompt + test-time filtering step (Yes/No question to detect redundancy/failure) and numeric score binning (reducing unique scores).",
            "format_category": "prompt style / post-processing",
            "format_details": "Filtering prompt identifies failures (repetitive / clearly failed summaries) and assigns minimum score. Binning reduces the number of unique scores (example: Orca-13B unique scores reduced from 36 to 10 after binning).",
            "performance_metric": "Kendall's Tau (segment-level) on test set (leaderboard entries)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Authors report improved test-set performance after applying Filtering and Binning; specific Kendall Tau deltas vs unprocessed outputs are not enumerated in the main text tables.",
            "experimental_setting": "Direct aggregation; for test submission top_p=0.1 and temperature=0 for Direct aggregation; Filtering prompt used to detect redundancy/failure; Binning reduced unique scores (e.g., Orca-13B: 36→10 unique scores).",
            "statistical_significance": null,
            "uuid": "e7666.10",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "A survey for in-context learning",
            "rating": 2
        },
        {
            "paper_title": "Larger language models do in-context learning differently",
            "rating": 2
        }
    ],
    "cost": 0.022081,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Which is better? Exploring Prompting Strategy For LLM-based Metrics</p>
<p>Joonghoon Kim joonghoon_kim@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Saeran Park saeran_park@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Kiyoon Jeong kiyoon_jeong@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Sangmin Lee sangmin_lee@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Seung Hun 
Korea University
SeoulRepublic of Korea</p>
<p>Han Jiyoon Lee 
Korea University
SeoulRepublic of Korea</p>
<p>Pilsung Kang pilsung_kang@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Which is better? Exploring Prompting Strategy For LLM-based Metrics
CDB68B2F165C159DB388EC4DB463D5A3
This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks.With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount.Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are illsuited for open-ended generation tasks.To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs.In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability.Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation.Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores.To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs.Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies. 1</p>
<p>Introduction</p>
<p>As Large Language Models (LLMs) like GPT-4 continue to advance rapidly, the Natural Language Generation (NLG) capability is approaching a level of expertise comparable to that of a human.As a result, the precise evaluation of NLG has become increasingly paramount.However, traditional similarity-based metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are widely used in NLG evaluations, tend to show a discrepancy from human assessments (Liu et al., 2023).Additionally, the reliance on reference texts for these metrics can hinder an accurate assessment of NLG quality, particularly for open-ended generation tasks.</p>
<p>Recent research has introduced methodologies that leverage LLMs as NLG evaluators, showcasing the potential of LLM-based metrics.These approaches are motivated from findings in recent research which revealed that LLM can directly evaluate NLG capabiltiy harnessing knowledge retained during the pre-train (Xu et al., 2023).These metrics have demonstrated notable correlation (Fu et al., 2023;Liu et al., 2023;Kocmi and Federmann, 2023;Fernandes et al., 2023) with human evaluations to learned evaluators (Chiang and yi Lee, 2023;Svikhnushina and Pu, 2023).</p>
<p>Concurrently, recent advancement of LLMs such as LLaMA (Touvron et al., 2023), Vicuna (Zheng et al., 2023), and Orca (Mukherjee et al., 2023), has paved a way for research on NLG evaluations utilizing open-source LLMs (Xu et al., 2023).However, there are few comprehensive studies that systematically evaluate the vast amount of possible prompts and prompting techniques for LLM-based metrics.Especially, research assessing the capabilities of open-source LLMs in the context of LLM-based metrics is even more scarce.Given the importance of enhancing the reproducibility of LLM-based metrics in metric research, there is a clear need for studies that explore effective prompts and prompting techniques specifically for open-source LLMs (Chiang and yi Lee, 2023).</p>
<p>In this work, we conduct a thorough exploration of various prompts and prompting techniques for effective deployment of open-source LLMs as metrics: analyze them in terms of prompting strategy, score aggregation, and explainability.</p>
<p>Within the scope of prompting strategies, we compare the effectiveness of human and model instruction templates for NLG evaluation.In addition, we explore granularity in score assignment to accurately evaluate NLG quality.Additionally, we gauge the influence of the open-source LLM's In-Context Learning (ICL) capability (Brown et al., 2020) in NLG evaluation by employing various types of demonstrated examples.For score aggregation, we compare three methodologies to discern the optimal strategy for aggregating NLG quality scores.To infer the explainability of opensource LLMs, we generate rationale when computing scores.These comprehensive experiments on prompting techniques for LLM-based metrics provide insights into the evaluation capabilities of open-source LLMs and guidelines for effective prompting strategies.</p>
<p>Furthermore, we provide insights derived from analysis of the features embedded in prompts and behaviors of open-source LLMs as LLM-based metrics.Additionally, we report our strategies and outcomes applied to the test set of summarization track in Eval4NLP 2023 shared task.</p>
<p>Related Work</p>
<p>Similarity-based Metrics</p>
<p>Similarity-based metrics evaluate the quality of NLG outputs by comparing reference and candidate text.They can be categorized into lexical-based and semantic-based metrics.Lexical-based metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), utilize N-grams to measure lexical overlap between a reference and a candidate text.However, research has highlighted their inadequacy in accurately assessing the quality of generated outputs and identifying both syntactical and semantic discrepancies (Liu et al., 2023;Polišenská et al., 2021;Wu et al., 2021).On the other hand, semantic-based metrics, including BERTScore (Zhang et al., 2019) and MoverScore (Zhao et al., 2019), measure semantic similarity by comparing the embeddings of both reference and candidate texts.However, similar to lexical-based metrics, they face challenges when evaluating open-ended generation tasks due to their inherent dependence on reference text (Chiang and yi Lee, 2023;Guan et al., 2021;Gu et al., 2021).</p>
<p>LLM-based Metrics</p>
<p>The recent substantial advancement in the NLG capabilities of LLMs has motivated research interests related to LLM-based metrics.Consequently, the latest studies, primarily exploring various prompting approaches that do not require additional training of an LLM, has shown a correlation with human evaluation comparable to that of learned evaluators (Chiang and yi Lee, 2023;Svikhnushina and Pu, 2023).Also, building upon the foundational work of LLaMA (Touvron et al., 2023), research on the fine-tuning approach which constructs an evaluator by fine-tuning an LLM with suitable supervised data for the evaluation task, is being actively pursued (Bosselut et al., 2019;Xu et al., 2023).</p>
<p>Summarization Track</p>
<p>The summarization track of Eval4NLP 2023 shared task (Leiter et al., 2023) aims to propose a reference-free metric for summarization.Specifically, reference-free metric evaluates a given summary using only the provided source sentence or paragraph without additional human-written references.The objective of shared task is to develop LLM-based metrics by exploring effective prompting strategies for open-source LLMs.</p>
<p>Dataset</p>
<p>Train and Development Set</p>
<p>In this study, we utilize the SummEval benchmark dataset provided by Fabbri et al. (2020) as both train and development sets.While the original benchmark provides human annotation scores for each of four aspects, including relevance, consistency, coherence, and fluency, the summarization track adopts the average of these aspect scores as golden human annotation scores.The performance of the evaluation task is measured through sentence-level correlation with the golden human annotation scores.</p>
<p>Test Set</p>
<p>Dataset provided in the shared task (Leiter et al., 2023), consisting of sentences and fragments of paragraphs from English Wikipedia documents written after July 15, 2023, is used as the test set.Summaries in the test dataset were generated by a summary generation model that are annotated with reference to Multidimensional Quality Metrics (MQM) annotation for aspects like factuality, relevance, and readability.</p>
<p>2</p>
<p>Figure 1: Examples of Human Guideline (HG) prompt and Model Guideline (MG) prompt.HG prompt and MG prompt consists of task description, evaluation criteria, and evaluation steps.The HG prompt is used as the annotation guideline for summarization evaluation, serving as the basis for human annotators assessments.In contrast, the MG prompt was used as the instruction for the model.</p>
<p>Models</p>
<p>We use four out of six open-source LLMs provided in the Eval4NLP 2023 shared task.</p>
<p>• Hermes-13B -LLaMA-13B model trained on over 300,000 instructions.• Orca-7B -LLaMA2-7B model trained on Orca Style dataset.• Orca-13B -LLaMA2-13B model trained on Open-Platypus dataset and OpenOrca dataset.• Platypus-70B -LLaMA2-70B model trained by Lee et al. (2023).</p>
<p>Method</p>
<p>In this section, we address the prompting strategies and score aggregation methods, as well as approaches to assess the explainability of open-source LLMs.</p>
<p>Prompting Strategy</p>
<p>Prompting strategies consist of prompt template, granularity of score, and demonstration.</p>
<p>Prompt Template</p>
<p>We propose Human Guideline (HG) prompt and Model Guideline (MG) prompt for summary evaluation as illustrated in Figure 1.The HG prompt, adapted from the human evaluation guideline of SummEval (Fabbri et al., 2020), provides clear evaluation instructions and criteria for human annotators.</p>
<p>Conversely, the MG prompt, implemented from a guideline given to LLM such as GPT-4 for summary evaluation in G-EVAL (Liu et al., 2023), instructs LLM to assess summaries, offering detailed, directive instructions and criteria.</p>
<p>Both HG prompt and MG prompt consist of elements such as task description, evaluation criteria, and evaluation steps.To assess the impact of each element, we create variants by modifying each one.</p>
<p>Task Description</p>
<p>The task description provides instructions for the specified task.To explore the influence of its length, we craft short and long descriptions by varying sentence lengths, maintaining the original context.Additionally, we create an expert-role task description to study the effect of providing an expert role in the evaluation (e.g."you're an expert at summarizing news articles.").Each variant is developed for both HG and MG prompts, with details in Appendix D.</p>
<p>Evaluation Criteria</p>
<p>The evaluation criteria outlines the scoring standards for the given summary per aspect.It is categorized into three components, 1) Aspect Definition (AD) 2) Human-Targeted criteria (HT) 3) Model-Targeted criteria (MT).</p>
<p>AD, adopted from GPTScore (Fu et al., 2023), concisely describes the evaluation aspect definitions.HT and MT, used in HG and MG Prompts respectively, include scoring considerations and as-3 166 pect descriptions.</p>
<p>To investigate the effects of each components, we generate modified version of AD, HT, and MT for each aspect using GPT-4.We instruct GPT-4 to maintain a consistent format with the existing ones.Examples are provided in Appendix D.</p>
<p>Evaluation Steps</p>
<p>The evaluation steps, which could be considered as a Chain-of-Thought (CoT) (Zhang et al., 2023), provide step-by-step instructions for the evaluation task, enhancing the reasoning capabilities of LLM.To explore the impact of varied evaluation steps descriptions, we construct detailed complex evaluation steps for both HG and MG prompts.Examples are provided in Appendix D.</p>
<p>Granularity of Score</p>
<p>For assigning a score, we consider the following two scoring approaches: coarse-grained scoring and fine-grained scoring.Coarse-grained scoring yields a singular and holistic score that considers all evaluation aspects collectively, but does not provide scores for individual aspects.Conversely, finegrained scoring assigns the score for each aspect, deriving individual scores and then averaging them to yield the final singular score.This approach enables the LLMs to furnish both the overall score and specific aspect scores, granting a more nuanced understanding of for score derivation compared to the coarse-grained method.Given that NLG evaluations commonly score by jointly taking multiple aspects into account, adpoting fined-grained scoring when constructing variants of the prompt is naturally apt approach.</p>
<p>Demonstration</p>
<p>To examine the ICL capability of open-source LLMs in evaluation tasks, we craft two distinct types of demonstrated examples.</p>
<p>One set of examples includes raw source text, a summary, and a human annotation score.On the other hand, another set of examples incorporates a rationale derived from the assigned human annotation score, which has been distilled from GPT-42 , in addition to the components found in the former set of examples.Examples are provided in Appendix D.</p>
<p>Furthermore, we construct examples for each individual aspect and subsequently group them into 'worst' and 'best' categories based on human annotation scores.In our study, 'worst' examples are assigned a score of 1, while 'best' examples receiving a score of 5. Categorization is undertaken to investigate potential biases in the quality and the score of the provided examples.Due to the maximum input length constraint of the LLMs, we use only one example as demonstration per summary.</p>
<p>Score Aggregation</p>
<p>To derive scores for individual aspects, we propose the following three score aggregation methods: Direct, Logprob, and Approximation (see Figure 2).Direct This method is the most general scoring method.It leverages the score generated by the LLM directly.</p>
<p>Logprob This method calculates the score by summing the product of a pre-defined discrete score range (e.g. 1 to 5) and the generation probability of the corresponding tokens.This method is considered as a weighted summation approach, using each score's token probability as its weight.By incorporating the model's token generation probabilities, this method distinctively produces a more continuous score.</p>
<p>For a given set of pre-defined discrete scores S = {s 1 , ..., s K }, Logprob multiplies each discrete score s i by its token probability p(s i ).K in (1) is the number of pre-defined discrete scores.
score = K i=1 p(s i ) • s i (1)</p>
<p>Approximation</p>
<p>This method calculates the score by averaging N sampled scores generated by LLM.Intending to approximate the token probability distribution, we design Approximation method to distinguish it from the Logprob method, which directly uses the actual token probabilities.This aggregation is inspired by techniques explored in (Liu et al., 2023;Fu et al., 2023).</p>
<p>For a given set of pre-defined discrete scores S = {s 1 , ..., s K }, Approximation multiplies each discrete score s i by its approximated token probability g(s i ).In (2), count(s i ) denotes the number of count discrete score s i appears in N samples.</p>
<p>g(s
i ) = count(s i ) N (2) score = K i=1 g(s i ) • s i (3)</p>
<p>Explainability</p>
<p>Evaluations that employ the previously described methods yield only a sole scalar score with no additional explanation for the assigned score at all.Thus, we manually craft the Rationale Generation (RG) prompt to derive rationales for the scores.</p>
<p>Using this prompt, we aim to explore the explainability of open-source LLMs (see Figure 2).Furthermore, similar to the approach used in the demonstration section 4.1.3,we use examples to analyze the influence of demonstrated examples on rationale generation.Each example is divided into 'worst' and 'best' example to examine potential biases in the outputs.</p>
<p>Test phase</p>
<p>For the test set, we incorporate two supplementary approaches alongside the previously described prompting strategy, tailored to the attributes of the test set.</p>
<p>Filtering Although many summaries in the test set exhibit appropriate sentence structures, certain samples retain repetitive words or phrases (e.g."A family of four members, including a first member, a second member, a third member, and a fourth member.").We deem such instance as a failure to generate an appropriate summary and uniformly assigned them lowest score.To account such instances, we design a Filtering prompt that filters failed samples.For given summaries, when model generates a 'Yes' response, they are assigned the minimum score.Example of the Filtering prompt is provided in Appendix D.</p>
<p>Binning After analyzing the scores assigned by the model for the test data, we observe that opensource LLMs are generally adept at evaluating summaries.Nevertheless, we note the model's tendency of assigning excessively fine-grained scores among samples of equivalent quality (e.g.scores of 1 and 1.01).In light of these observations, we implement Binning to simplify the score distribution and mitigate noise, thereby integrating proximate scores into same categories.Detailed explanations can be found in the Appendix B.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Experiments are conducted using the development set of the summarization track provided in the shared task.We use the provided prompt template for the summarization track as the baseline prompt.</p>
<p>The baseline prompt contains a brief task description and score guide.Additionally, the HG and MG prompt in 5.2 are adapted from SummEval (Fabbri Moreover, we report the leaderboard results for the test set using Orca-13B and Platypus-70B for the small and large track, respectively.Test set experiments share the almost the same setting with development set experiments: same HG prompt, finegrained scoring, hyperparameters for Direct aggregation are implemented.For factuality evaluation criteria, not originally provided in SummEval (Fabbri et al., 2020), we use GPT-4 to generate it.Specifically, scores for relevance, factuality, and fluency, obtained from Direct aggregation, are averaged to compute the final score.Throughout our all experiments, segment-level Kendall's Tau correlation is used as the performance metric.For optimized inference with open-source LLMs, we employ Guidance3 and vLLM4 libraries.Details of experimental setup are provided in Appendix A.</p>
<p>Main Results</p>
<p>Prompting Strategy</p>
<p>We compare the performance with different types of the prompt templates.As shown in Prompting section of Table 1, regardless of the granularity of the score, we observe that HG and MG prompts, especially HG prompt, consistently outperform the baseline prompt.We hypothesize that a more detailed description of task provided in the HG and MG prompt allows LLM to understand and follow the instructions more clearly.Moreover, among all the prompts, the HG prompt achieves the best performance, indicating that succinct and clear instructions are better than complex ones.</p>
<p>As for granularity of the scoring, fine-grained scoring consistently outperforms coarse-grained scoring across various model sizes and prompt templates.The coarse-grained scoring may introduce ambiguity in the evaluation criteria by requiring the LLM to consider aspect-specific considerations in an integrated manner.Conversely, the fine-grained scoring removes such ambiguity by providing evaluation criteria of each aspect independently.</p>
<p>As shown in Demonstration section of Table 1, we observe that the use of demonstration leads to decrease in performance, likely due to the inherent bias introduced by the demonstrated example.Notably, the smaller model exhibits a significant decline in performance, which could be attributed to their limited ICL capabilities (Dong et al., 2022;Han et al., 2023;Wei et al., 2023), resulting in inaccurate understanding of in-context examples, and vice versa.The performance differs among models based on whether they are provided with examples containing only the score or examples with additional rationales.This discrepancy can be attributed to the superior ability of larger models in comprehending in-context examples, which leads to better understanding when explanations for scores are added.In contrast, the smaller model exhibits the opposite behavior.Furthermore, providing the 'best' examples consistently yields superior performance across all model sizes when compared to the 'worst' examples.After conducting an analysis of the model's score distribution, we observe a bias wherein the model tends to assign higher scores when provided with the 'best' example.We hypothesize that observed bias may be driven by the skewed distribution of human annotation scores in the development set, where human annotation scores are predominantly distributed towards higher values, mainly falling between 3 and 5.</p>
<p>Score Aggregation</p>
<p>We assess the performance based on the different score aggregation methods.Aggregation section of Table 1 illustrates that, across various model sizes and prompt templates, Direct and Logprob aggregation consistently demonstrates superior performance when compared to the Approximation aggregation.In both Direct and Logprob aggregation, the decoding temperature is set to 0. This likely leads the model to assign scores in a more deterministic manner compared to the Approximation, potentially resulting in superior performance.Specifically, since Approximation estimates the distribution of score token probability through sampling, sampling noise could account for its lower performance.Unlike other aggregation methods, Direct aggregation generates integer values ranging from 1 to 5, thereby offering a much fewer score range.On the other hand, Xu et al. (2023) suggest that Kendall Tau might favor tie pairs.Such tendency could explain the notably high correlation observed with Direct aggregation.</p>
<p>Explainability</p>
<p>We assess the LLM's ability to provide appropriate explanations for the scores.Examining Explainability section of Table 1, we observe that the RG prompt results in performance similar to or slightly lower than the HG prompt and better than the MG prompt.This suggests that generating rationales for scores can also aid the evaluation process itself.Furthermore, it is noteworthy that Orca-7B exhibits a slight performance decline when provided with a demonstrated example, in contrast to the performance of Orca-13B.The RG prompt is meticulously designed to facilitate the generation of rationales, possibly benefiting from the examples.Therefore, Orca-13B, with superior ICL capabilities as mentioned in 4.1, has outperformed the other smaller model.Analysis of the rationales generated by Orca-13B is discussed in 5.3.3.In Table 2, we report the performance of the HG prompt on the test set.Details of HG prompt applied for the test set are provided in Appendix D. As evident from the results of our development set experiments, the performance of the HG prompt on the test set is consistently satisfactory across all models.Furthermore, we observe a discernible improvement in performance when the Filtering is applied.This observation suggests that uniformly assigning lowest scores to inadequately generated summaries can enhance performance.Similarly, Binning enhances performance by reducing noise in the scores on the test set.This improvement is achieved by integrating closely related scores into same categories.While the Orca-13B model exhibits a slightly lower performance compared to the Platypus-70B with the base HG prompt, it shows superior performance after the application of Filtering and Binning.Details of test phase are provided in Appendix B.</p>
<p>Test</p>
<p>Analysis</p>
<p>The Effect of Different Model Sizes</p>
<p>We compare the performance depending on different model sizes: Orca-7B, Hermes-13B, Orca-13B, and Platypus-70B.As shown in Appendix Table 4 and Table 5, despite the same size with Orca-13B, the performance of Hermes-13B is significantly lower, even lower than Orca-7B.Except for Hermes-13B, generally positive correlation between model size and performance is observed.We speculate such outcome may be due to the differences in the backbone model's performance (e.g.LLaMA, LLaMA 2) and the type of datasets and approaches used for fine-tuning (Freitag et al., 2022).Insignificant performance gap between Platypus-70B and Orca-13B proves that Orca-13B is as effective as Platypus-70B for the evaluation task.</p>
<p>Comparisons of each Component</p>
<p>Task Description Types We investigate the impact of varying the length of task descriptions within the HG prompt and MG prompt on performance.Additionally, we compare performance when an expert role is assigned in the task description versus when it is not.As shown in Appendix Table 6, for Orca-7B, there is no significant performance difference based on length of task descriptions.However, for Orca-13B, we observe higher performance when a longer task description is employed.Such tendency suggests that, Orca-13B benefits from longer length of task descriptions in facilitating the execution of instructions, even when the content remains the same.Furthermore, when the expert role is assigned, there is a discernible performance improvement with Orca-7B.However, for Orca-13B, the performance difference between cases with and without the expert role is not substantial, indicating that this approach can be more effective for smaller models.</p>
<p>Evaluation Criteria Variants</p>
<p>We analyze the influence of various evaluation criteria, AD, HT, and MT.As shown in Appendix Table 7, utilizing aspect definitions consistently improves performance, regardless of the prompt template or model size.Furthermore, similar results are obtained even when evaluation criteria generated by GPT-4 are used.This suggests that providing a simple definition of each aspect is an effective approach when evaluating summary quality.</p>
<p>Complexity of Evaluation Steps</p>
<p>As shown in Table 8, there is no significant trend in performance between standard and complex evaluation steps both for the HG prompt and the MG prompt.This observation implies that while the evaluation steps are effective in offering step-by-step instructions to the model, the precise description or complexity level of the evaluation steps does not exert a significant influence on the evaluation of summaries.</p>
<p>Error Analysis</p>
<p>To investigate whether the model generates wellfounded rationales for the assigned scores, we perform an error analysis on the rationales generated using the RG prompt described in section 4.3.Specifically, we conduct such comparative analysis on 36 sampled instances for two different rationale generation method: one generated with Orca-13B and RG prompt, and another with RG prompt including demonstrated examples.</p>
<p>Our analysis reveals that, in general, the model exhibits the capability to provide rationales correctly.However, we identify several types of errors: (Error type 1) provided rationale is inconsistent with the assigned evaluation scores, (Error type 2) provided rationale shows hallucination where the rationale includes information not present in the source text or summary, (Error type 3) provided rationale describes explanation about aspect different from the designated one.Detailed descriptions and examples for each error type can be found in Appendix C. Addressing and mitigating these errors through further research efforts could significantly enhance the explainability and reliability of LLM-based metrics.</p>
<p>Conclusion</p>
<p>In this work, we conduct a systematic analysis of effective prompting techniques and strategies for LLM-based metrics in NLG evaluation.Our comprehensive experiments reveal that providing clear and straightforward instructions, akin to those explained to humans, proves to be more effective.Furthermore, we examine various score aggregation methods to achieve effective score assignments and show the potential for enhancing explainability within open-source LLMs.Additionally, we explore performance change relative to model size and scrutinize the influence of various elements within the prompt template.We hope that our research findings will furnish valuable insights for 8</p>
<p>A Experimental Setup</p>
<p>Library Version guidance 0.0.64 vllm 0.1.7torch 2.0.1 For optimized inference with open-source LLMs, we employ Guidance and vLLM libraries.The libraries and their respective versions used for the experiments can be found in Table 3.</p>
<p>B Test Phase</p>
<p>We submit the final results for the test set after equally applying Filtering and Binning to the HG prompt on both Orca-13B and Platypus-70B (for the small and large track, respectively).We use HT as the evaluation criteria of the factuality, generated using GPT-4.Scores for relevance, factuality, and fluency, obtained from Direct aggregation, are averaged to compute the final score.The hyperparameters for Direct aggregation is set identical to the development set, with top_p to 0.1 and temperature to 0, respectively.The prompts used for the test set can be found in Table 22, 23, and 24.</p>
<p>Filtering is applied using the Filtering prompt on both Orca-13B and Platypus-70B models.Example of the Filtering prompt is provided in Table 18.After applying Binning, the number of unique scores has been diminished from 36 to 10 and 46 to 13 for Orca-13B and Platypus-70B, respectively.</p>
<p>C Analysis C.1 The Effect of Different Model Sizes</p>
<p>We conduct experiments to analyze the performance differences depending on model sizes using Orca-7B, Hermes-13B, Orca-13B, and Platypus-70B.The experiments for Orca-7B, Hermes-13B, and Orca-13B are conducted using vLLM, while the Platypus-70B experiments are conducted using Guidance.In Table 4, we conduct experiments comparing performance across model sizes for different prompt templates and granularity of score.In Table 5, we carry out experiments to compare performance across model sizes for different prompt templates and score aggregations.</p>
<p>Template Fine-grained Demonstration Aggregation Orca-7B Orca-13B Hermes-13B Platypus-70B</p>
<p>C.2 Comparisons of each Component</p>
<p>Task description, evaluation criteria and evaluation steps of the prompt templates are slightly modified to ensure the suitability for each experiment.Examples are provided in Appendix D.</p>
<p>C.2.1 Task Description type</p>
<p>We investigate the impact of varying the length of task descriptions within the HG prompt and MG prompt on performance.Additionally, we compare performance when an expert role is assigned in the task description versus when it is not.Various task descriptions are manually crafted for each prompt 11</p>
<p>C.2.2 Evaluation Criteria variants</p>
<p>AD-GPT, HT-GPT, and MT-GPT are generated using GPT-4, tailored respectively to the AD, HT, and MT styles.The experimental results based on the types of the evaluation criteria can be found in Table 7.  Score 1</p>
<p>Rationale</p>
<p>The summary is coherent with the source text, as it captures the main points about Esteban Cambiasso's confidence in Leicester's chances of staying in the Premier League.However, it could be improved by including more details and quotes from the source text, making it more comprehensive and engaging.</p>
<p>Summary</p>
<p>holland america line said two guests were found dead inside their stateroom on the ship .the ship left tampa , florida , on march 29 on a 14-day southern caribbean cruise .it's currently in san juan , puerto rico Score 2</p>
<p>Rationale</p>
<p>The summary is not coherent with the source text.It omits crucial information, such as the cause of death, the identities of the passengers, and the cruise ship's itinerary.The summary also does not convey the urgency and seriousness of the situation, as indicated by the FBI's involvement and the ship's immediate return to port.The coherence of the summary is poor, as it does not represent the main events and context of the source text</p>
<p>Example Source</p>
<p>First blood to United after their Under 18s saw off City 1-0 in the 'mini-derby'.Kits aside, this was probably as far removed from Sunday's big match as you could imagine.For a start, no less than 13 out of the 22 players on show at kick-off were English.Of those, 10 were Mancunian.Callum Gribbin was the matchwinner for Manchester United with a delightful free-kick Ticket prices?Entry was free and close to 1,000 gathered on the seats and terraces of Moss Lane for a match that kicked off at 3pm on Saturday with half-and-half scarves nowhere to be seen.Altrincham's compact 6,000-capacity home may not be Old Trafford, but it does have a proud history of its own.It was certainly a grander stage than a windswept Saturday morning on an outfield at United Premier League end-of-season play-off series.FA chief Greg Dyke recently bemoaned the lack of first-team opportunities for homegrown talent between the ages of 18 and 21.On the evidence of this entertaining 90 minutes high on skill and commitment, there are talents on both sides that may well deserve their chance in the On the evidence of this entertaining 90 minutes high on skill and commitment, there are talents on both sides that may well deserve their chance in the coming years.</p>
<p>Summary callum gribbin was the matchwinner for manchester united with a free-kick .paul mcguinness's side claim victory in the 'mini derby' for a match.paul mcguinness and marcus wood were in the first team .Score 3</p>
<p>Rationale</p>
<p>The summary is not entirely consistent with the source document.It does mention Callum Gribbin as the matchwinner and the victory in the 'mini derby', but it omits important details such as the final score, the formation used by Manchester United, and the performance of Marcus Wood.</p>
<p>Table 12: Example of Error Type 3. The rationale does not discuss for consistency.</p>
<p>D Example Prompts</p>
<p>Figure 2 :
2
Figure 2: (a) Left -Score Aggregation An example of how the Score Aggregation is calculated.'Direct' uses scores directly generated by the model, 'Logprob' uses a weighted summation based on generation probabilities of pre-defined scores (e.g. 1 to 5), and 'Approximation' uses an average from N sampled scores.(b) Right -Rational Generation prompt An example of Rationale Generation (RG) prompt and the corresponding outputs.Using the RG prompt as input, the model provides a score for the quality of the summary and the corresponding rationale.</p>
<p>'s Carrington complex, where Paul McGuinness's Under 18s usually ply their trade.The young Reds coach wanted to make the experience closer to what his tyros could expect should they make the step up to the seniors.And his side lined up with three at the back and supporting wingbacks in a formation seen more than once this season in the first team.In an even first-half the impressive Marcus Wood, from just down the road in Sale, came closest for City with an audacious chip.United manager Paul McGuinness saw his side claim victory in the 'mini derby' For the home side towering centre-forward Marcus Rashford, another local lad from whom big things are expected, wasted two decent opportunities when put through.Just before the hour mark England Under 17 star Callum Gribbin made and scored the first.In March, the nifty-footed attacking midfielder went viral when United tweeted a clip of him showing outrageous skills to trick four Aston Villa players.He was at it again here, showing nifty footwork on the edge of the box and drawing a foul.After dusting himself down Gribbin stepped up to curl a delightful free-kick around the wall and beyond blonde Joe Hart-a-like Charlie Albinson in the City goal.Moments later it was almost two when full-back Cameron Borthwick-Jackson's rasping 25-yard drive struck the angle of the crossbar and post.The same man looked to have gotten away with one when he appeared to clip Buckley in the box before sub Aaron Nemane hit the side-netting as the visitors looked for an equaliser.Dutch winger Javairo Dilrosun curled a free kick onto the roof of the net in stoppage time for City but that was all she wrote.The result leaves United, who have won two of this year's cross-town clashes, two points behind l eaders Middlesbrough with a game in hand.Injury-ravaged City however, have now lost both matches in the Barclays Under 18s</p>
<p>Table 1 :
1
Main result.Experimental results of combination sets for each Prompting Strategy, Score Aggregation, and Explainability.'Human' and 'Model' mean Human Guideline prompt and Model Guideline prompt respectively.Also, 'Base-worst/best' and 'Reason-worst/best' are abbreviations of two types of demonstration that are distinguished, including rationale.Best results for each set of variants are in bold.
TemplateFine-grained DemonstrationAggregationOrca-7B Orca-13BPrompting BasexxDirect0.25000.3040HumanxxDirect0.30940.4343ModelxxDirect0.26510.3583BaseoxDirect0.27460.3891Human Modelo ox xDirect Direct0.3472 0.28640.4468 0.3844Demonstration HumanoBase-worstDirect0.17580.3690Human Humano oBase-best Reason-worstDirect Direct0.2854 0.23090.4092 0.3899Human Aggregation Humano oReason-best xDirect Approximation0.2733 0.32390.4133 0.4002HumanoxLogprob0.32960.4210Human Modelo ox xDirect Approximation0.3472 0.26870.4468 0.3530Model Modelo ox xLogprob Direct0.2926 0.28640.3851 0.3844Explainability Rationale Rationaleo ox Reason-worstDirect Direct0.3506 0.29150.4220 0.3876RationaleoReason-bestDirect0.32620.4330
(Liu et al., 2023)-EVAL(Liu et al., 2023)with minimal modification.Examples of prompts are provided in Appendix D. For scoring, we averaged the scores derived from the aspects of relevance, consistency, coherence, and fluency for finegrained scoring.For the demonstration experiments, we sample examples from the train set based on human annotation scores for each aspect.Rationales for the scores in the examples are generated using GPT-4.Throughout the entire score generation process, we set top_p to 0.1.For Direct and Logprob aggregation, the temperature is set to 0. Lastly, we set the temperature to 1 and n_samples to 20, respectively, for Approximation aggregation.</p>
<p>Table 3 :
3
Version of libraries used for the experiments.</p>
<p>Table 4 :
4
Comparison of Kendall's Tau correlation across various Prompt Templates and Models.Fine-grained denotes whether the fine-grained scoring is used or not.Aggregation denotes the type of Score Aggregation method used.
BasexxDirect0.25000.30400.15540.3956HumanxxDirect0.30940.43430.20410.4260ModelxxDirect0.26510.35830.19150.4383BaseoxDirect0.27460.38910.14020.4082HumanoxDirect0.34720.44680.20630.4354ModeloxDirect0.28640.37440.21700.4039</p>
<p>Table 5 :
5
Comparison of Kendall's Tau correlation across various Score Aggregation and Models.Fine-grained denotes whether the fine-grained scoring is used or not.Aggregation denotes the type of Score Aggregation method used.template,andexamplescan be found in Appendix D. The experimental results for the task description types can be found in Table6.
Template Fine-grained DemonstrationAggregationOrca-7B Orca-13B Hermes-13B Platypus-70BHumanoxApproximation0.32390.40020.21270.4041HumanoxLogprob0.32960.42100.20600.4305HumanoxDirect0.34720.44680.20630.4354ModeloxApproximation0.26870.35300.21520.4058ModeloxLogprob0.29260.38510.22500.4316ModeloxDirect0.28640.38440.21700.4039Template Task Description Orca-7B Orca-13BBase0.34720.4468HumanExpert Short0.3544 0.33390.4383 0.4239Long0.33830.4501Base0.28640.3744ModelExpert Short0.3302 0.27210.3881 0.3508Long0.27670.3891</p>
<p>Table 6 :
6
Comparison of Kendall's Tau correlation of cases using various types of task description on development set.Direct aggregation and fine-grained scoring are used for the experiment.Any demonstration is not provided.</p>
<p>Table 7 :
7
Comparison of Kendall's Tau correlation of cases using various types of evaluation criteria on development set.AD-GPT, HT-GPT, and MT-GPT denote AD, HT, and MT generated by GPT-4.Direct aggregation and finegrained scoring are used for the experiment.Any demonstrated example is not provided.Complex evaluation steps are crafted using GPT-4 for both HG and MG prompt.Examples are provided in Appendix D. The experimental results for the evaluation steps can be found in Table8.
Template Evaluation Criteria Orca-7B Orca-13BAD0.33430.4279AD-GPT0.33450.4336HumanHT HT-GPT0.3256 0.32930.4192 0.4192MT0.33030.4314MT-GPT0.33440.4297AD0.31160.4001AD-GPT0.31150.4066ModelHT HT-GPT0.3013 0.29870.3904 0.3894MT0.31410.4102MT-GPT0.30370.3949C.2.3 Complexity of evaluation stepsC.3 Error Analysis</p>
<p>Table 8 :
8
Comparison of Kendall's Tau correlation of base and complex evaluation steps on development set.Direct aggregation and fine-grained scoring are used for the experiment.No demonstrated example is provided to either method.
Error TypeBase Reason-best0Good50%69%1Inconsistent11%17%2Hallucination36%6%3 Different Aspect 6%8%</p>
<p>Table 9 :
9
Error Occurrence Ratio when RG prompt with and without 'Reason-best' demonstration are used.In this analysis, we use Orca-13B to generate a score and rationale for each aspect.Error Type 1 means that the rationale is inconsistent with the score.Error Type 2 means that the rationale includes hallucinated information not mentioned in the source text and/or summary.Error Type 3 means that the rationale is about different aspect rather than the designated aspect.Esteban Cambiasso has won all the major European competitions a player can during his illustrious career but revealed that keeping Leicester City in the Premier League would be up there with the best.The Foxes are currently seven points adrift at the bottom of the table, with only eight games remaining, knowing that time is running out to save themselves.Cambiasso refuses to give up and admits that keeping Leicester up will feel like winning a trophy.Esteban Cambiasso says that helping keep Leicester in the Premier League will feel like winning a trophy 'For me, it's like another cup,' he told BBC East Midlands Today.'Whenyoustartanotherseason you have an objective, and this is the objective for us.'Forme,winning a cup or winning the league with another team is the same now as having the possibility to save Leicester in the Premier League.'TheArgentinianmidfielderposes with the trophy after his team won the 2010 FIFA Club World Cup Cambiasso had an illustrious career at Inter Milan, winning an impressive 15 trophies during his stint River Plate(2001)(2002)Argentine Primera Division RealMadrid (2002Madrid ( -2004) )La Liga Super Cup Supercopa de Espana Inter Milan (2004-2014) Champions League Serie A (5) Coppa Italia (4) Supercoppa (4) FIFA Club World Cup Having not won a game since January, Nigel Pearson's men face West Ham United on Saturday and Cambiasso is still convinced they can avoid the drop.'I understood when I signed for Leicester it's not an easy job to stay in the Premier League,' he said.'It's a difficult situation but I think we have our chances to win matches.There's a quarter of the Premier League left to finish.'I think some people think for Leicester all is finished.But I'm sure, because I watch my team-mates every day, we can save Leicester and stay in the Premier League.'The former Inter Milan star signed for the Foxes in the summer, leaving Italy after ten years and embarking on a new challenge in England.After agreeing to a one-year-deal, Cambiasso has quickly established himself as a key player but it remains to be seen if he'll still be in the East Midlands at the start of next season.The former Real Madrid man was also successful during his short spell in Spain for Real Madrid Cambiasso played during Real's 'Galatico' era, with Luis Figo, Zinedine Zidane, Ronaldo and David Beckham 'Leicester always wanted me,' he added.'After these nine months or eight months, I'm very happy because my family is OK, and I'm good.'I want a few more points, but all the rest is perfect.'Cambiasso is happy in the East Midlands and could stay beyond his current one-year-deal Summary Esteban Cambiasso is confident Leicester can win the Premier League and keep themselves in the top flight.
Example</p>
<p>Table 10 :
10
Example of Error Type 1.The rationale is inconsistent with the score for the coherence.Two passengers found dead on a cruise ship in Puerto Rico appear to have died in a murder-suicide, the cruise line said.Holland America Line said two guests were found dead inside their stateroom on the ms Ryndam at 11:30 a.m.Thursday." "The cabin was immediately secured, and the authorities were notified, including the FBI,"" Holland America said.""We are cooperating fully with the investigation, and the authorities will make the official determination on what occurred.""FBI spokesman Moises Quiñones said authorities were on scene investigating.The ship left Tampa, Florida, on March 29 on a 14-day Southern Caribbean cruise.It's currently in San Juan, Puerto Rico.Puerto Rico Port Authority spokesman Efraín Santiago told El Nuevo Dia newspaper that the cleaning staff on the ship had discovered the deceased passengers after knocking on the cabin's door.
13 176</p>
<p>Table 11 :
11
Example of Error Type 2. Rationale includes hallucination for coherence.</p>
<p>Code for this paper is available at https://github.com/ kjhoon7686/Prompt4LLM-Eval.
https://openai.com/research/gpt-4
https://github.com/guidance-ai/guidance
https://github.com/vllm-project/vllm 6
Task Description TemplatePromptExpert HumanYou read and summarize a lot of news articles, and you're an expert at summarizing news articles.In this task you will evaluate the quality of a summary written for a news article.To correctly solve this task, follow these steps:Expert ModelYou read and summarize a lot of news articles, and you're an expert at summarizing news articles.You will be given one summary written for a news article.Your task is to evaluate the summary based on a specific metric, rating it on a scale from 1 (worst) to 5 (best).Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Long HumanIn this task, you will evaluate the quality of a summary written for a news article.Please take your time to carefully evaluate the provided summary, and don't hesitate to refer back to this instruction document if you need clarification or guidance at any point during your evaluation.To correctly solve this task, follow these steps:Long ModelYou will be given one summary written for a news article.Your task is to evaluate the summary based on a specific metric, rating it on a scale from 1 (worst) to 5 (best).Please make sure you read and understand these instructions carefully.HT-GPT HumanRelevance:This rating assesses the extent to which the summary highlights the central themes of the original article.Evaluate if the summary encompasses the crucial elements while omitting any non-essential details.MT-GPT ModelRelevance -gauges the summary's alignment with the article's primary ideas.Check if the summary includes essential points and omits unrelated details.It may help to list the article's main points and verify their presence in the summary.AD Human,Model Relevance -How well is the generated text relevant to its source text?AD-GPT Human,Model Relevance -To what extent does the generated summary capture and reflect the core details of its source text?HumanIn this task, your primary aim is to conduct a thorough assessment of the summary provided for a news article.To effectively accomplish this task, please adhere to the following comprehensive steps:1. Initiate the evaluation process by engaging in an in-depth examination of the news article.Your aim here is to establish a profound understanding of the article's entire spectrum of content, ensuring you grasp its core message, nuances, and key elements.2. Proceed to scrutinize the proposed summary provided alongside the article.In this phase, your task is to meticulously evaluate the summary for its aspect.3. Assign a rating to each summary based on its aspect, utilizing a scale ranging from 1 (indicating the lowest quality) to 5 (signifying the highest quality).Complex Model 1. Thoroughly examine the provided summary and the source document with meticulous attention to detail.2. Conduct a comprehensive comparative analysis, scrutinizing the summary in relation to the source document to discern and delineate the primary focal points and pivotal elements elucidated within the article.3. Engage in a judicious evaluation to gauge the summary's efficacy in addressing and encompassing the central facets of the source document, concurrently assessing the presence of any extraneous or duplicative information that might detract from its relevance.4. Utilize a relevance rating scale, ranging from 1 (indicating minimal relevance) to 5 (indicating maximal relevance), for the purpose of assigning a numerical score.This score serves as a quantitative reflection of the extent to which the summary aligns with and encapsulates the core substance of the source document.World No 1 Williams eventually prevailed 4-6 7-6 ( 3 ) 6-3 against the dogged Italian to take her career record over her to 8-0 but the American was not impressed .The US were beaten 3-2 as Williams and Alison Riske were thrashed 6-0 6-3 in the doubles rubber by Errani and Flavia Pennetta , meaning they were relegated to World Group II .American tennis star Serena Williams fought back to beat Italian Sara Errani in the Fed Cup play-off on Sunday Tough weather conditions made it difficult for both players who had to keep on re-tossing their serves Errani gave Williams a real scare but in the end the world No 1 's power proved to be too much 'Today has been a big eye opener , ' Williams said afterwards .' I 'm totally not as ready for the claycourt season as I thought I was .Now I 'm in the mindset of , " You know what , I 'm not on hard court ." I 'm playing like I 'm on hard court and I 'm not .'So I have to play and be ready to hit a thousand shots if necessary .' Williams , 33 , won her 19th singles grand slam at the Australian Open and her dominance has raised talk of her claiming all the majors this year .The French Open has been her least successful of the four though despite claiming the title in Paris in 2002 and 2013 .Her doubles defeat on Sunday blotted an otherwise flawless Fed Cup record and left the US facing a battle to get back amongst the elite nations next year .'We have to work harder , ' US captain Mary Joe Fernandez said .'We came close today and need to just keep plugging away .'The good news is that we have a lot of players in the top 100 and , hopefully , we can get two wins next year and get back into the World Group .' Williams congratulates Italy captain Corrado Barazzutti after competing in America 's doubles defeat.Summary: Serena Williams beat Sara Errani 4-6 7-6 ( 3 ) 6-3 in the Fed Cup play-off .The US were beaten 3-2 as Williams and Alison Riske were thrashed in the doubles rubber .The doubles defeat saw the US relegated to World Group II .\u2019--Example Score: 5 Explanation: The summary effectively captures the key points from the article.It mentions Serena Williams' challenging match against Sara Errani and her eventual victory.The summary also highlights the US team's overall defeat and its consequence \u2013 relegation to World Group II.These details are central to the main storyline of the source text, making the summary highly relevant.Thus, a score of 5 (best) is appropriate for the summary's relevance.Template PromptRationaleYour task is to evaluate the relevance of a provided summary based on its source document.Follow these steps:1. Read the source document 2. Review the summary 3. Analyze for relevance 4. Assign a Score: Rate the summary on a scale of 1 to 5, where:-1 means the summary is not relevant with the source.-5 means the summary is entirely relevant with the source.5. Provide a Rationale: After assigning a score, explain your reasons based on your analysis.# Definition: Relevance:The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary."--Source text: Summary:FilteringIn this task you will evaluate the quality of a summary written for a document.Provided summary may include direct or rephrased repetitions of the same word or phrase.With that in mind do the following:1. Answer whether the summary is redundant or not.-Your answer must be in "Yes" or "No" format, where "Yes" means that the summary is redundant and "No" means that the summary is not redundant.2. Please provide brief explanation for your answer.-Your explanation should only discuss the redundancy of the summary, not the quality of the summary in general.-summary:Template PromptHumanIn this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 1 (worst) to 5 (best) by its relevance.# Definition: Relevance: The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary.Source text:Summary:Score: In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Relevance.# Definition: Relevance: The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary.Source text:Summary:Score:Template PromptHuman Instruction:In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Factuality.# Definition: Factuality: This rating gauges the accuracy and truthfulness of the information presented in the summary compared to the original article.Scrutinize the summary to ensure it presents facts without distortion or misrepresentation, staying true to the source content's details and intent.Source text:Summary:Score: In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Fluency.# Definition: Fluency: This rating evaluates the clarity and grammatical integrity of each sentence in the summary.Examine each sentence for its structural soundness and linguistic clarity.Source text:Summary: Score:
COMET: commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, CoRR, abs/1906.053172019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung Yi, Lee , 2023</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Summeval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir R Radev, 2020. 2007.12626</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, F T André, Graham Martins, Ankush Neubig, Jonathan H Garg, Markus Clark, Orhan Freitag, Firat, 2023</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Perception score: A learned metric for open-ended text generation evaluation. Jing Gu, Qingyang Wu, Zhou Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang, arXiv:2105.08920Openmeva: A benchmark for evaluating open-ended story generation metrics. 2021arXiv preprint</p>
<p>Chi Han, Ziqi Wang, Han Zhao, Heng Ji, arXiv:2305.12766-context learning of large language models explained as kernel regression. 2023arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, 10.48550/ARXIV.2302.145202023</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2023</p>
<p>The eval4nlp 2023 shared task on prompting large language models as explainable metrics. Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems. the 4th Workshop on Evaluation and Comparison for NLP systems2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Effects of semantic plausibility, syntactic complexity and n-gram frequency on children's sentence repetition. Kamila Polišenská, Shula Chiat, Jakub Szewczyk, Katherine E Twomey, Journal of Child Language. 4822021</p>
<p>Approximating human evaluation of social chatbots with prompting. Ekaterina Svikhnushina, Pearl Pu, arXiv:2304.052532023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>Exploring syntactic and semantic features for authorship attribution. Haiyan Wu, Zhiqiang Zhang, Qingfeng Wu, Applied Soft Computing. 1111078152021</p>
<p>Instructscore: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, 2023</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, CoRR, abs/1904.096752019</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, 2023</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, CoRR, abs/1909.026222019</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
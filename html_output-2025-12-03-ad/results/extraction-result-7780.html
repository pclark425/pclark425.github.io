<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7780 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7780</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7780</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-e4eb81ad222ba047770d5a90bdd7406c138c6126</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e4eb81ad222ba047770d5a90bdd7406c138c6126" target="_blank">Autonomous LLM-driven research from data to human-verifiable research papers</a></p>
                <p><strong>Paper Venue:</strong> NEJM AI</p>
                <p><strong>Paper TL;DR:</strong> Data-to-paper is built, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions, demonstrating a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
                <p><strong>Paper Abstract:</strong> As AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, we built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about 80-90%, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. Our work thereby demonstrates a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7780.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7780.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>data-to-paper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>data-to-paper automation platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chained LLM- and rule-based agent platform that automates end-to-end hypothesis-driven research from annotated datasets to human-verifiable manuscripts, with stepwise products, rule-based checks, LLM peer-review, traceable information flow and optional human co-piloting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (OpenAI conversational models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-3.5-turbo-16k-0613 / gpt-4 (selected per step)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multidisciplinary (epidemiology, social network analysis, clinical/medical ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis generation and hypothesis-testing analyses (statistical inference, ML model development)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>End-to-end empirical evaluation via case studies and manual vetting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run multiple independent full research cycles (open- and fixed-goal) on public and benchmark datasets; manually vet produced analyses, tables, numeric claims, citations and conclusions; quantify correctness and error rates across runs and task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reproducibility / correctness rate, error rate (per-run), interpretation-error counts</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent of runs producing a manuscript judged correct (numeric and analytic correctness) or containing major errors; error rate = 100% - correctness %; specific counts of runs with interpretation or analysis errors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Health Indicators (BRFSS subset), Social Network (Congress Twitter), Treatment policy (Saint-Fleur dataset), Treatment Optimization (Shim dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual vetting by authors (TI and LH) of each manuscript and run record; statements highlighted by severity (green correct, yellow/ orange minor/atypical, red major errors); counts reported per dataset and modality.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>For simple goals, fully-autonomous runs produced correct manuscripts in ~80–90% of cases; open-goal: 10 manuscripts produced, 8 correct and 2 erroneous; fixed-goal reproduction challenge 1: analyses reproduced and 8/10 reached correct conclusions; challenge 2 (ML models): error rate varied with task breadth (90% error for broad original goal, 10–20% error for narrower goals).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance depends on task complexity and goal/dataset description quality; hallucinations and interpretation errors occur (10–20% for simple tasks), and fully autonomous operation fails on more complex, broader goals without human co-piloting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7780.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based guardrails</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based product review and coding guardrails framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-level automated checking system applied to LLM outputs that enforces formatting, content, static code structure, runtime behavior, package usage restrictions, and output file correctness, returning programmatic feedback to LLMs until checks pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4 used to produce code that is checked)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-4 (as applied)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software engineering for scientific workflows; applied across analyzed domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>methodological verification framework for analysis code and textual products</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automated static/runtime/package/output checks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extract LLM-produced Python code and apply: (i) static structure checks; (ii) runtime execution with error/warning capture; (iii) package-specific guardrails controlling imports and functionality; (iv) output file content/format verification; failures generate targeted feedback messages back to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number of programmatic feedback rounds to reach pass; pass/fail of individual rule checks</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer count of feedback-response cycles required before code passes all checks; boolean pass/fail per check category.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used in runs; human overseer could intervene in co-pilot mode but rule-based checks operate automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Framework prevented many common LLM coding/statistics errors; number of feedback rounds used as an internal performance signal (detailed counts in Supplementary Coding Runs); open-source models often failed to converge despite checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Does not eliminate hallucinations in goal specification or higher-level interpretation errors; token limits and non-deterministic LLM outputs require iterative cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7780.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-reviewer multi-agent check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel LLM reviewer conversation (role-inverted multi-agent review)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A second LLM conversation that reviews the Performer product by receiving the same prior products and mission prompt but inverted roles, choosing to accept or give constructive feedback to the Performer, thereby creating iterative LLM-to-LLM review cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM-driven methodological QA in research automation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>peer-review simulation for LLM-generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM peer-review acceptance/feedback decision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Present the Performer-produced product to a Reviewer LLM pre-filled with context; Reviewer must accept or return feedback; feedback is looped back to Performer until acceptance or human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accepted vs feedback counts; subsequent pass rates after reviewer feedback</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Counts of reviewer-accepted products vs. products requiring further Performer iterations; number of iterations until acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM review complements rule-based checks; human reviewer may add comments in co-pilot mode.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM review used as an intermediate verification step; combined with rule-based checks reduced errors but did not eliminate hallucination-driven interpretation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Reviewer LLM can propagate or miss errors; not a substitute for human domain expertise on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7780.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-chaining / information tracing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-chained manuscripts with recursive hyperlinks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traceability mechanism where numeric values cited in the manuscript are programmatically hyperlinked through a 'Notes' appendix to tables, output files, and exact code lines that produced them, enabling human-verifiable recursive tracing from statements back to code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4 used to author text with hyperlinks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific reproducibility and transparency across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>traceability framework for results and claims</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Programmatic hyperlinked trace verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assign unique labels to numeric outputs; require LLM to hyperlink each numeric mention to the label; algorithmically verify that every numeric mention is hyperlinked and that hyperlinks resolve to specific table/output/code lines; compile Notes appendix translating formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Fraction/boolean of numeric values successfully chained; detection of unlinked or placeholder values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of numeric claims with valid recursive links (0–100%); presence of placeholders triggers abort.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to facilitate manual vetting; humans can click through chain to validate numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Implemented in later runs and produces clickable hyperlinks in Supplementary Data-chained Manuscripts; absent or placeholder numeric values cause run aborts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Only applies to numeric claims traceable to code outputs; does not automatically verify correctness of underlying statistical choices or conceptual interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7780.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual vetting / highlighting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual vetting with severity highlights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation procedure where each created manuscript and its run record are manually examined for analytic correctness, numeric correspondence, citation appropriateness and textual quality, with statements highlighted by severity: green (correct), yellow (imperfect/atypical), orange (minor errors), red (major errors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human evaluation (applies to ChatGPT outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human expert judgment of generated hypotheses/analyses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual expert vetting with categorical highlights</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two authors manually inspect manuscripts and run logs, verify statistical methods and code, check numeric matches, test citation fit, and assign color-coded annotations per statement and per manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Counts of manuscripts/statements in each severity category; per-dataset counts of correct vs. erroneous runs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer counts and percentages of runs assigned to categories; e.g., open-goal: 8/10 correct (green), 2/10 major errors (red).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>All runs across datasets A–D (Supplementary Manuscripts A–D)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual vetting performed by authors TI and LH (and oversight by RK); each manuscript checked for (i) correct analysis/code, (ii) numeric correspondence, (iii) citation fit, (iv) textual exactness and (v) wording quality.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Open-goal: 10 papers, 8 correct with minor wording issues, 2 with fundamental errors; fixed-goal challenge1: all 10 reproduced analyses, 8 had correct conclusions; co-piloting (2–3 one-sentence comments) typically sufficient to fix errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Manual vetting is time-consuming and necessary due to residual LLM errors; subjective aspects (wording, novelty) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7780.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reproduction benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed-goal reproduction of peer-reviewed studies (Saint-Fleur et al., Shim et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark evaluation where data-to-paper is provided datasets and explicit research goals matching published peer-reviewed studies and evaluated on its ability to reproduce analyses, results and conclusions of the original publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5-turbo / gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>clinical outcomes analysis (neonatal policy) and ML model development for pediatric intubation depth</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>reproduction of published hypotheses, analyses and ML model comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Benchmark reproduction with manual correctness assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide the original dataset and research goal to data-to-paper and run 10 independent cycles; manually assess whether analyses and conclusions match the published studies and whether positive and negative findings are correctly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reproduction success rate (percent of runs reproducing correct analyses and conclusions); error rates stratified by task breadth</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent of runs judged to have correct analysis and conclusions (e.g., 8/10 = 80%); error rate defined as runs with interpretation or analysis errors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Saint-Fleur et al. (Treatment policy dataset), Shim et al. (Treatment Optimization dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual vetting of each run's analysis, results, and interpretations; identification of interpretation errors that affected conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Challenge 1 (Saint-Fleur): all runs reproduced analyses; 8/10 reached correct overall conclusions (2 had interpretation errors); Challenge 2 (Shim): performance depended on breadth — original broad goal produced ~90% error rate; narrowing the number of models reduced error to ~10–20%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Fixed-goal papers were similar in content and embeddings to the original published studies, but no quantitative direct performance parity metric vs. human authors is reported beyond reproduction stats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Success depends on the exactness of the provided research goal and dataset description; broader multi-step tasks lead to high failure unless human co-piloting applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7780.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-based similarity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise cosine distance of title/abstract vector embeddings (SPECTER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated content-similarity assessment using document-level embeddings (SPECTER) and pairwise cosine distance to quantify similarity and cluster generated manuscripts across runs and case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPECTER embedding model (citation-informed transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural language similarity / bibliometrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>content similarity clustering for manuscript comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cosine-distance clustering of title-abstract embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute vector embeddings for each manuscript's title and abstract using SPECTER, compute pairwise cosine distances, and visualize clustering to determine whether runs cluster by dataset/case study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine distance; cluster tightness (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity/distance between embeddings (unitless; -1 to 1 similarity or 0–2 distance depending on formulation); tighter clusters indicate higher intra-group similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Embeddings computed with SPECTER (ref. 40) for manuscripts produced across datasets A–D</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used as automated complement to manual vetting to show manuscript similarity structure.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Pairwise cosine distance showed tight and distinct clusters corresponding to the four case-study groups (two open-goal and two fixed-goal), and fixed-goal papers clustered close to their original studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Fixed-goal generated manuscripts were similar in embedding space to the original published studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding similarity captures topical/terminological similarity but not correctness of analyses or novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7780.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Scholar retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation retrieval via Semantic Scholar Academic Graph API</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmentation of LLM literature-search steps with a direct citation API (Semantic Scholar) to retrieve candidate papers along scopes, provide TLDRs, citation-influence metrics and embeddings, and filter/sort results algorithmically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Semantic Scholar Open Data Platform</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>External citation API (Semantic Scholar) used alongside ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>bibliographic search and citation retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>literature retrieval and filtering for grounding generated manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>API-driven citation retrieval with embedding/rank-based filtering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLM devises structured queries per scope; system queries Semantic Scholar API; returns search rank, BibTeX id, title, journal/year, TLDR, citation influence and embeddings; filter and sort by search rank or embedding similarity to the current manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Search rank, citation influence, embedding similarity scores</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>API-provided numeric ranks and citation-influence scores; embedding cosine similarity (unitless) used for sorting.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Semantic Scholar Academic Graph (ref. 28)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Filtered lists used as prior products; manual exclusion of original study in reproduction runs to avoid trivial match.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Direct citation retrieval plus rule-based checks ensured only valid citations were included; however, some inadequate or missing choice of citations was still observed in generated manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM may still choose suboptimal citations; API retrieval mitigates hallucinated citations but does not guarantee ideal literature coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7780.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM selection experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM model comparison for research steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of ChatGPT (gpt-3.5/gpt-4) versus open-source models (Llama-2, CodeLlama) on the Research goal and Data analysis steps, scored on hallucination of dataset features and number of programmatic feedback iterations to pass code checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo, gpt-4, Llama-2-70b-chat-hf, Llama-2-7b-chat-hf, CodeLlama-34b-Instruct-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (gpt-3.5/gpt-4; Llama 70B/7B; CodeLlama 34B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM capability evaluation for automated research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>empirical performance benchmarking of LLMs on sub-tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Task-specific empirical comparison</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run specific research steps (Research goal and Data analysis) 10 times per model on the Health Indicators dataset; manually annotate hallucinated vs true dataset feature mentions for goals and count feedback rounds until coding checks pass for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination score (annotation), number of programmatic feedback rounds until pass</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Annotated counts of hallucinated features vs true features per run; integer counts of feedback-response cycles until code passes rule-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Health Indicators dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual annotation of hallucinated/true features in Research goal outputs (Table S2); Supplementary Coding Runs tracked feedback rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ChatGPT models produced better-constrained goals and converged on code more reliably; open-source models produced hallucinated goals and frequently failed to converge in the data analysis coding step.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Open-source models at time of study had higher failure rates in the critical coding and goal-creation steps, limiting end-to-end automation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7780.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coding check taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four-level coding verification (static, runtime, package guardrails, output)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A taxonomy of code verification applied to all LLM-produced Python code: static structural checks, runtime execution (errors/warnings), package-import and functionality guardrails, and post-run output content/format checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-produced code evaluated by rule-based system</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5/gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software verification for scientific analyses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>structured code QA framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Four-tier code testing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply static analysis rules, execute code capturing syntax/runtime issues, enforce package-specific guardrails (restrict imports / monitor unsafe functionality and p-value formatting), and verify requested output files and their contents/format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pass/fail per tier; number of iterations to pass all tiers</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Boolean pass/fail for static/runtime/guardrail/output checks; integer count of required remediation cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human oversight possible in co-pilot mode; otherwise automatic.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>These checks reduced coding errors and ensured created tables correctly represented analysis outputs; integrated into the iterative feedback loop until code passed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Checks focus on code execution and formatting; they do not guarantee conceptual/statistical appropriateness beyond what can be tested programmatically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7780.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7780.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation criteria for manuscript correctness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set of criteria for judging correctness of LLM-generated papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic checklist used in manual vetting to judge manuscripts: (i) correct data analysis and appropriate statistical methodology; (ii) numeric statements match analysis outputs; (iii) citations fit context; (iv) textual exactness; (v) overall wording quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to ChatGPT-generated manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-3.5/gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific quality evaluation across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human-assessment criteria for hypothesis/result validity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual checklist-based correctness assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each manuscript, verify analyses, numeric correspondence, citation appropriateness, textual exactness and wording quality, and label statements by severity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary/graded correctness per criterion and overall error classification (green/yellow/orange/red)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Per-manuscript and per-statement categorical labels; counts aggregated into percentages (e.g., 8/10 correct manuscripts).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to all produced manuscripts across datasets A–D</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Performed by authors; used to compute reported correctness/error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to derive that fully-autonomous runs produced accurate manuscripts in 80–90% for simple goals and substantially worse for complex/broad goals unless human co-piloting applied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Checklist is manual and subjective; does not scale without human effort.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Semantic Scholar Open Data Platform <em>(Rating: 2)</em></li>
                <li>SPECTER: Document-level Representation Learning using Citation-informed Transformers <em>(Rating: 2)</em></li>
                <li>Survey of Hallucination in Natural Language Generation <em>(Rating: 2)</em></li>
                <li>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation <em>(Rating: 2)</em></li>
                <li>Llama 2: Open Foundation and Fine-Tuned Chat Models <em>(Rating: 1)</em></li>
                <li>Code Llama: Open Foundation Models for Code <em>(Rating: 1)</em></li>
                <li>Living guidelines for generative AI - why scientists must oversee its use <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7780",
    "paper_id": "paper-e4eb81ad222ba047770d5a90bdd7406c138c6126",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "data-to-paper",
            "name_full": "data-to-paper automation platform",
            "brief_description": "A chained LLM- and rule-based agent platform that automates end-to-end hypothesis-driven research from annotated datasets to human-verifiable manuscripts, with stepwise products, rule-based checks, LLM peer-review, traceable information flow and optional human co-piloting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (OpenAI conversational models)",
            "model_size": "gpt-3.5-turbo / gpt-3.5-turbo-16k-0613 / gpt-4 (selected per step)",
            "scientific_domain": "multidisciplinary (epidemiology, social network analysis, clinical/medical ML)",
            "theory_type": "hypothesis generation and hypothesis-testing analyses (statistical inference, ML model development)",
            "evaluation_method_name": "End-to-end empirical evaluation via case studies and manual vetting",
            "evaluation_method_description": "Run multiple independent full research cycles (open- and fixed-goal) on public and benchmark datasets; manually vet produced analyses, tables, numeric claims, citations and conclusions; quantify correctness and error rates across runs and task complexity.",
            "evaluation_metric": "Reproducibility / correctness rate, error rate (per-run), interpretation-error counts",
            "metric_definition": "Percent of runs producing a manuscript judged correct (numeric and analytic correctness) or containing major errors; error rate = 100% - correctness %; specific counts of runs with interpretation or analysis errors.",
            "dataset_or_benchmark": "Health Indicators (BRFSS subset), Social Network (Congress Twitter), Treatment policy (Saint-Fleur dataset), Treatment Optimization (Shim dataset)",
            "human_evaluation_details": "Manual vetting by authors (TI and LH) of each manuscript and run record; statements highlighted by severity (green correct, yellow/ orange minor/atypical, red major errors); counts reported per dataset and modality.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "For simple goals, fully-autonomous runs produced correct manuscripts in ~80–90% of cases; open-goal: 10 manuscripts produced, 8 correct and 2 erroneous; fixed-goal reproduction challenge 1: analyses reproduced and 8/10 reached correct conclusions; challenge 2 (ML models): error rate varied with task breadth (90% error for broad original goal, 10–20% error for narrower goals).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Performance depends on task complexity and goal/dataset description quality; hallucinations and interpretation errors occur (10–20% for simple tasks), and fully autonomous operation fails on more complex, broader goals without human co-piloting.",
            "uuid": "e7780.0",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Rule-based guardrails",
            "name_full": "Rule-based product review and coding guardrails framework",
            "brief_description": "A multi-level automated checking system applied to LLM outputs that enforces formatting, content, static code structure, runtime behavior, package usage restrictions, and output file correctness, returning programmatic feedback to LLMs until checks pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5/gpt-4 used to produce code that is checked)",
            "model_size": "gpt-3.5-turbo / gpt-4 (as applied)",
            "scientific_domain": "software engineering for scientific workflows; applied across analyzed domains",
            "theory_type": "methodological verification framework for analysis code and textual products",
            "evaluation_method_name": "Automated static/runtime/package/output checks",
            "evaluation_method_description": "Extract LLM-produced Python code and apply: (i) static structure checks; (ii) runtime execution with error/warning capture; (iii) package-specific guardrails controlling imports and functionality; (iv) output file content/format verification; failures generate targeted feedback messages back to the LLM.",
            "evaluation_metric": "Number of programmatic feedback rounds to reach pass; pass/fail of individual rule checks",
            "metric_definition": "Integer count of feedback-response cycles required before code passes all checks; boolean pass/fail per check category.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Used in runs; human overseer could intervene in co-pilot mode but rule-based checks operate automatically.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Framework prevented many common LLM coding/statistics errors; number of feedback rounds used as an internal performance signal (detailed counts in Supplementary Coding Runs); open-source models often failed to converge despite checks.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Does not eliminate hallucinations in goal specification or higher-level interpretation errors; token limits and non-deterministic LLM outputs require iterative cycles.",
            "uuid": "e7780.1",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-reviewer multi-agent check",
            "name_full": "Parallel LLM reviewer conversation (role-inverted multi-agent review)",
            "brief_description": "A second LLM conversation that reviews the Performer product by receiving the same prior products and mission prompt but inverted roles, choosing to accept or give constructive feedback to the Performer, thereby creating iterative LLM-to-LLM review cycles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5/gpt-4)",
            "model_size": "gpt-3.5-turbo / gpt-4",
            "scientific_domain": "LLM-driven methodological QA in research automation",
            "theory_type": "peer-review simulation for LLM-generated outputs",
            "evaluation_method_name": "LLM peer-review acceptance/feedback decision",
            "evaluation_method_description": "Present the Performer-produced product to a Reviewer LLM pre-filled with context; Reviewer must accept or return feedback; feedback is looped back to Performer until acceptance or human intervention.",
            "evaluation_metric": "Accepted vs feedback counts; subsequent pass rates after reviewer feedback",
            "metric_definition": "Counts of reviewer-accepted products vs. products requiring further Performer iterations; number of iterations until acceptance.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "LLM review complements rule-based checks; human reviewer may add comments in co-pilot mode.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "LLM review used as an intermediate verification step; combined with rule-based checks reduced errors but did not eliminate hallucination-driven interpretation mistakes.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Reviewer LLM can propagate or miss errors; not a substitute for human domain expertise on complex tasks.",
            "uuid": "e7780.2",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Data-chaining / information tracing",
            "name_full": "Data-chained manuscripts with recursive hyperlinks",
            "brief_description": "A traceability mechanism where numeric values cited in the manuscript are programmatically hyperlinked through a 'Notes' appendix to tables, output files, and exact code lines that produced them, enabling human-verifiable recursive tracing from statements back to code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5/gpt-4 used to author text with hyperlinks)",
            "model_size": "gpt-3.5-turbo / gpt-4",
            "scientific_domain": "scientific reproducibility and transparency across domains",
            "theory_type": "traceability framework for results and claims",
            "evaluation_method_name": "Programmatic hyperlinked trace verification",
            "evaluation_method_description": "Assign unique labels to numeric outputs; require LLM to hyperlink each numeric mention to the label; algorithmically verify that every numeric mention is hyperlinked and that hyperlinks resolve to specific table/output/code lines; compile Notes appendix translating formulas.",
            "evaluation_metric": "Fraction/boolean of numeric values successfully chained; detection of unlinked or placeholder values",
            "metric_definition": "Proportion of numeric claims with valid recursive links (0–100%); presence of placeholders triggers abort.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Used to facilitate manual vetting; humans can click through chain to validate numbers.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Implemented in later runs and produces clickable hyperlinks in Supplementary Data-chained Manuscripts; absent or placeholder numeric values cause run aborts.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Only applies to numeric claims traceable to code outputs; does not automatically verify correctness of underlying statistical choices or conceptual interpretations.",
            "uuid": "e7780.3",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Manual vetting / highlighting",
            "name_full": "Manual vetting with severity highlights",
            "brief_description": "Human evaluation procedure where each created manuscript and its run record are manually examined for analytic correctness, numeric correspondence, citation appropriateness and textual quality, with statements highlighted by severity: green (correct), yellow (imperfect/atypical), orange (minor errors), red (major errors).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Human evaluation (applies to ChatGPT outputs)",
            "model_size": null,
            "scientific_domain": "general scientific quality assessment",
            "theory_type": "human expert judgment of generated hypotheses/analyses",
            "evaluation_method_name": "Manual expert vetting with categorical highlights",
            "evaluation_method_description": "Two authors manually inspect manuscripts and run logs, verify statistical methods and code, check numeric matches, test citation fit, and assign color-coded annotations per statement and per manuscript.",
            "evaluation_metric": "Counts of manuscripts/statements in each severity category; per-dataset counts of correct vs. erroneous runs",
            "metric_definition": "Integer counts and percentages of runs assigned to categories; e.g., open-goal: 8/10 correct (green), 2/10 major errors (red).",
            "dataset_or_benchmark": "All runs across datasets A–D (Supplementary Manuscripts A–D)",
            "human_evaluation_details": "Manual vetting performed by authors TI and LH (and oversight by RK); each manuscript checked for (i) correct analysis/code, (ii) numeric correspondence, (iii) citation fit, (iv) textual exactness and (v) wording quality.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Open-goal: 10 papers, 8 correct with minor wording issues, 2 with fundamental errors; fixed-goal challenge1: all 10 reproduced analyses, 8 had correct conclusions; co-piloting (2–3 one-sentence comments) typically sufficient to fix errors.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Manual vetting is time-consuming and necessary due to residual LLM errors; subjective aspects (wording, novelty) remain.",
            "uuid": "e7780.4",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reproduction benchmarks",
            "name_full": "Fixed-goal reproduction of peer-reviewed studies (Saint-Fleur et al., Shim et al.)",
            "brief_description": "Benchmark evaluation where data-to-paper is provided datasets and explicit research goals matching published peer-reviewed studies and evaluated on its ability to reproduce analyses, results and conclusions of the original publications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5/gpt-4)",
            "model_size": "gpt-3.5-turbo / gpt-4",
            "scientific_domain": "clinical outcomes analysis (neonatal policy) and ML model development for pediatric intubation depth",
            "theory_type": "reproduction of published hypotheses, analyses and ML model comparisons",
            "evaluation_method_name": "Benchmark reproduction with manual correctness assessment",
            "evaluation_method_description": "Provide the original dataset and research goal to data-to-paper and run 10 independent cycles; manually assess whether analyses and conclusions match the published studies and whether positive and negative findings are correctly reported.",
            "evaluation_metric": "Reproduction success rate (percent of runs reproducing correct analyses and conclusions); error rates stratified by task breadth",
            "metric_definition": "Percent of runs judged to have correct analysis and conclusions (e.g., 8/10 = 80%); error rate defined as runs with interpretation or analysis errors.",
            "dataset_or_benchmark": "Saint-Fleur et al. (Treatment policy dataset), Shim et al. (Treatment Optimization dataset)",
            "human_evaluation_details": "Manual vetting of each run's analysis, results, and interpretations; identification of interpretation errors that affected conclusions.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Challenge 1 (Saint-Fleur): all runs reproduced analyses; 8/10 reached correct overall conclusions (2 had interpretation errors); Challenge 2 (Shim): performance depended on breadth — original broad goal produced ~90% error rate; narrowing the number of models reduced error to ~10–20%.",
            "comparison_to_human_generated": true,
            "comparison_results": "Fixed-goal papers were similar in content and embeddings to the original published studies, but no quantitative direct performance parity metric vs. human authors is reported beyond reproduction stats.",
            "limitations_noted": "Success depends on the exactness of the provided research goal and dataset description; broader multi-step tasks lead to high failure unless human co-piloting applied.",
            "uuid": "e7780.5",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Embedding-based similarity",
            "name_full": "Pairwise cosine distance of title/abstract vector embeddings (SPECTER)",
            "brief_description": "An automated content-similarity assessment using document-level embeddings (SPECTER) and pairwise cosine distance to quantify similarity and cluster generated manuscripts across runs and case studies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SPECTER embedding model (citation-informed transformer)",
            "model_size": null,
            "scientific_domain": "natural language similarity / bibliometrics",
            "theory_type": "content similarity clustering for manuscript comparison",
            "evaluation_method_name": "Cosine-distance clustering of title-abstract embeddings",
            "evaluation_method_description": "Compute vector embeddings for each manuscript's title and abstract using SPECTER, compute pairwise cosine distances, and visualize clustering to determine whether runs cluster by dataset/case study.",
            "evaluation_metric": "Cosine distance; cluster tightness (qualitative)",
            "metric_definition": "Cosine similarity/distance between embeddings (unitless; -1 to 1 similarity or 0–2 distance depending on formulation); tighter clusters indicate higher intra-group similarity.",
            "dataset_or_benchmark": "Embeddings computed with SPECTER (ref. 40) for manuscripts produced across datasets A–D",
            "human_evaluation_details": "Used as automated complement to manual vetting to show manuscript similarity structure.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Pairwise cosine distance showed tight and distinct clusters corresponding to the four case-study groups (two open-goal and two fixed-goal), and fixed-goal papers clustered close to their original studies.",
            "comparison_to_human_generated": true,
            "comparison_results": "Fixed-goal generated manuscripts were similar in embedding space to the original published studies.",
            "limitations_noted": "Embedding similarity captures topical/terminological similarity but not correctness of analyses or novelty.",
            "uuid": "e7780.6",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Semantic Scholar retrieval",
            "name_full": "Citation retrieval via Semantic Scholar Academic Graph API",
            "brief_description": "Augmentation of LLM literature-search steps with a direct citation API (Semantic Scholar) to retrieve candidate papers along scopes, provide TLDRs, citation-influence metrics and embeddings, and filter/sort results algorithmically.",
            "citation_title": "The Semantic Scholar Open Data Platform",
            "mention_or_use": "use",
            "model_name": "External citation API (Semantic Scholar) used alongside ChatGPT",
            "model_size": null,
            "scientific_domain": "bibliographic search and citation retrieval",
            "theory_type": "literature retrieval and filtering for grounding generated manuscripts",
            "evaluation_method_name": "API-driven citation retrieval with embedding/rank-based filtering",
            "evaluation_method_description": "LLM devises structured queries per scope; system queries Semantic Scholar API; returns search rank, BibTeX id, title, journal/year, TLDR, citation influence and embeddings; filter and sort by search rank or embedding similarity to the current manuscript.",
            "evaluation_metric": "Search rank, citation influence, embedding similarity scores",
            "metric_definition": "API-provided numeric ranks and citation-influence scores; embedding cosine similarity (unitless) used for sorting.",
            "dataset_or_benchmark": "Semantic Scholar Academic Graph (ref. 28)",
            "human_evaluation_details": "Filtered lists used as prior products; manual exclusion of original study in reproduction runs to avoid trivial match.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Direct citation retrieval plus rule-based checks ensured only valid citations were included; however, some inadequate or missing choice of citations was still observed in generated manuscripts.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "LLM may still choose suboptimal citations; API retrieval mitigates hallucinated citations but does not guarantee ideal literature coverage.",
            "uuid": "e7780.7",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM selection experiment",
            "name_full": "LLM model comparison for research steps",
            "brief_description": "Empirical comparison of ChatGPT (gpt-3.5/gpt-4) versus open-source models (Llama-2, CodeLlama) on the Research goal and Data analysis steps, scored on hallucination of dataset features and number of programmatic feedback iterations to pass code checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo, gpt-4, Llama-2-70b-chat-hf, Llama-2-7b-chat-hf, CodeLlama-34b-Instruct-hf",
            "model_size": "various (gpt-3.5/gpt-4; Llama 70B/7B; CodeLlama 34B)",
            "scientific_domain": "LLM capability evaluation for automated research tasks",
            "theory_type": "empirical performance benchmarking of LLMs on sub-tasks",
            "evaluation_method_name": "Task-specific empirical comparison",
            "evaluation_method_description": "Run specific research steps (Research goal and Data analysis) 10 times per model on the Health Indicators dataset; manually annotate hallucinated vs true dataset feature mentions for goals and count feedback rounds until coding checks pass for analysis.",
            "evaluation_metric": "Hallucination score (annotation), number of programmatic feedback rounds until pass",
            "metric_definition": "Annotated counts of hallucinated features vs true features per run; integer counts of feedback-response cycles until code passes rule-based checks.",
            "dataset_or_benchmark": "Health Indicators dataset",
            "human_evaluation_details": "Manual annotation of hallucinated/true features in Research goal outputs (Table S2); Supplementary Coding Runs tracked feedback rounds.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ChatGPT models produced better-constrained goals and converged on code more reliably; open-source models produced hallucinated goals and frequently failed to converge in the data analysis coding step.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Open-source models at time of study had higher failure rates in the critical coding and goal-creation steps, limiting end-to-end automation.",
            "uuid": "e7780.8",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Coding check taxonomy",
            "name_full": "Four-level coding verification (static, runtime, package guardrails, output)",
            "brief_description": "A taxonomy of code verification applied to all LLM-produced Python code: static structural checks, runtime execution (errors/warnings), package-import and functionality guardrails, and post-run output content/format checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-produced code evaluated by rule-based system",
            "model_size": "gpt-3.5/gpt-4",
            "scientific_domain": "software verification for scientific analyses",
            "theory_type": "structured code QA framework",
            "evaluation_method_name": "Four-tier code testing",
            "evaluation_method_description": "Apply static analysis rules, execute code capturing syntax/runtime issues, enforce package-specific guardrails (restrict imports / monitor unsafe functionality and p-value formatting), and verify requested output files and their contents/format.",
            "evaluation_metric": "Pass/fail per tier; number of iterations to pass all tiers",
            "metric_definition": "Boolean pass/fail for static/runtime/guardrail/output checks; integer count of required remediation cycles.",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Human oversight possible in co-pilot mode; otherwise automatic.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "These checks reduced coding errors and ensured created tables correctly represented analysis outputs; integrated into the iterative feedback loop until code passed.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Checks focus on code execution and formatting; they do not guarantee conceptual/statistical appropriateness beyond what can be tested programmatically.",
            "uuid": "e7780.9",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Evaluation criteria for manuscript correctness",
            "name_full": "Set of criteria for judging correctness of LLM-generated papers",
            "brief_description": "A pragmatic checklist used in manual vetting to judge manuscripts: (i) correct data analysis and appropriate statistical methodology; (ii) numeric statements match analysis outputs; (iii) citations fit context; (iv) textual exactness; (v) overall wording quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to ChatGPT-generated manuscripts",
            "model_size": "gpt-3.5/gpt-4",
            "scientific_domain": "scientific quality evaluation across domains",
            "theory_type": "human-assessment criteria for hypothesis/result validity",
            "evaluation_method_name": "Manual checklist-based correctness assessment",
            "evaluation_method_description": "For each manuscript, verify analyses, numeric correspondence, citation appropriateness, textual exactness and wording quality, and label statements by severity.",
            "evaluation_metric": "Binary/graded correctness per criterion and overall error classification (green/yellow/orange/red)",
            "metric_definition": "Per-manuscript and per-statement categorical labels; counts aggregated into percentages (e.g., 8/10 correct manuscripts).",
            "dataset_or_benchmark": "Applied to all produced manuscripts across datasets A–D",
            "human_evaluation_details": "Performed by authors; used to compute reported correctness/error rates.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Used to derive that fully-autonomous runs produced accurate manuscripts in 80–90% for simple goals and substantially worse for complex/broad goals unless human co-piloting applied.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Checklist is manual and subjective; does not scale without human effort.",
            "uuid": "e7780.10",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Semantic Scholar Open Data Platform",
            "rating": 2,
            "sanitized_title": "the_semantic_scholar_open_data_platform"
        },
        {
            "paper_title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
            "rating": 2,
            "sanitized_title": "specter_documentlevel_representation_learning_using_citationinformed_transformers"
        },
        {
            "paper_title": "Survey of Hallucination in Natural Language Generation",
            "rating": 2,
            "sanitized_title": "survey_of_hallucination_in_natural_language_generation"
        },
        {
            "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "rating": 2,
            "sanitized_title": "autogen_enabling_nextgen_llm_applications_via_multiagent_conversation"
        },
        {
            "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "rating": 1,
            "sanitized_title": "llama_2_open_foundation_and_finetuned_chat_models"
        },
        {
            "paper_title": "Code Llama: Open Foundation Models for Code",
            "rating": 1,
            "sanitized_title": "code_llama_open_foundation_models_for_code"
        },
        {
            "paper_title": "Living guidelines for generative AI - why scientists must oversee its use",
            "rating": 2,
            "sanitized_title": "living_guidelines_for_generative_ai_why_scientists_must_oversee_its_use"
        }
    ],
    "cost": 0.01903175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Autonomous LLM-driven research from data to human-verifiable research papers</h1>
<p>Tal Ifargan ${ }^{1, <em>}$, Lukas Hafner ${ }^{2, </em>}$, Maor Kern ${ }^{3}$, Ori Alcalay ${ }^{3}$ and Roy Kishony ${ }^{2,4,5}$<br>${ }^{1}$ Faculty of Data and Decision Sciences, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{2}$ Faculty of Biology, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{3}$ Epsio, Tel Aviv, Israel.<br>${ }^{4}$ Faculty of Computer Science, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{5}$ Faculty of Biomedical Engineering, Technion-Israel Institute of Technology, Haifa, Israel.</p>
<h2>Abstract</h2>
<p>As Al promises to accelerate scientific discovery, it remains unclear whether fully Al-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, we built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about $80-90 \%$, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. Our work thereby demonstrates a potential for Al-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
<h1>Introduction</h1>
<p>Recent advances in natural language processing have resulted in LLMs, such as ChatGPT, capable of writing text, answering questions, and generating code at a human level (1-5). Furthermore, augmenting LLMs with external tools as well as automating iterative algorithmic prompting and multi-agent interactions has enabled tackling even more complex, multi-step, tasks such as solving math problems (6-8), coding and debugging large code projects (9, 10), and creating book-long texts and scripts (11). Most recently, LLMs have even demonstrated a capacity of designing and running experimentation as well as performing clinical diagnostics (12-14). Yet, despite all these advances, scientific research, and in particular the de novo creation of insights from data, remains a stronghold of human intelligence and ingenuity (15-20). The recent advancement of AI has led to a vivid discussion on the potential and risks of their application in science (21), and to emerging guidelines, emphasizing the importance of key values including accountability, oversight and transparency, notoriously challenging in AI (22).</p>
<p>Conducting research and compiling results and conclusions into a transparent and methodologically traceable and verifiable scientific paper is a highly challenging task, involving multiple interconnected steps and requiring planning, inference, and deduction, as well as the meticulous tracing of information. While scientists may in principle follow an infinite number of creative paths towards discovery, certain conventional research paths have been established (23). In particular, such paths typically follow these almost-canonical steps: formulating and reshaping a research question in light of the literature, designing and executing a research plan, interpreting the results in the context of prior knowledge, and writing a research paper. Beyond its established multi-step structure, human-driven scientific process has three additional key characteristics. First, the process is not linear, it often requires iteratively setting back to earlier steps. Second, it is built on a rigorous tracing and control of both textual and quantitative information among steps. Finally, at each of the steps, human scientists rely on feedback from peers, mentors, or external reviewers, enabling an overall collective strength beyond individual capabilities. Taken together, these key features make science a unique process of human creativity.</p>
<p>Here, inspired by how research is conducted by human scientists, we build data-to-paper, an automation platform that systematically guides multiple LLM and rule-based algorithmic agents through the conventional steps of scientific research, with automated feedback, iterative cycles of review and revision, and with structured control and tracing of information flow among these research steps. We specifically focus on a relatively simple and well-defined process of hypothesis-testing research. Starting with a human-provided dataset,</p>
<p>the process is designed to raise hypotheses, write, debug and execute code to analyze the data and perform statistical tests, interpret the results and write well-structured scientific papers which not only describe results and conclusions but also transparently delineate the research methodologies, allowing human scientists to understand, repeat and verify the analysis. The discussion on emerging guidelines for Al-driven science (22) have served as a design framework for data-to-paper, yielding a fully transparent, traceable and verifiable workflow, and algorithmic "chaining" of data, methodology and result allowing to trace downstream results back to the part of code which generated them. The system can run with or without a predefined research goal (fixed/open-goal modalities) and with or without human interactions and feedback (copilot/autopilot modes). We performed two open-goal and two fixed-goal case studies on different public datasets (24-27) and evaluated the Al-driven research process as well as the novelty and accuracy of created scientific papers. We show that, running fully autonomously (autopilot), data-to-paper can perform complete and correct run cycles for simple goals, while for complex goals, human co-piloting becomes critical.</p>
<h1>Implementation</h1>
<p>To autonomously analyze a provided dataset and create a research paper, data-to-paper guides multiple LLM and rule-based agents through a series of pre-defined "research steps", each designed to create well-defined quantitative or textual "research products" (Fig. 1). The process includes the following steps: data exploration, literature search and iterative formulation of a research goal and hypothesis, creating a hypothesis testing plan, writing data analysis code, creating scientific tables, searching related literature, and writing the paper section by section (Fig. 1A; Fig. 1B, top; in total 17 steps). The research goal can also be provided as human input, in which case the goal-determining steps are skipped ("Fixed-goal modality", dashed bypass arrow, Fig. 1A; Methods). The process runs automatically through the series of steps (with human overseeing and approval; Methods), with each step creating one or more research products, of different types ("Free text", "LaTex text", "Structured text", "Binary decision", "Citations", "Python code" and "Numerical data"; Fig. 1B, left). In coding steps, the LLM creates a Python code, which is then executed by data-to-paper to analyze the provided dataset and create numerical data products (like tables for the paper; Methods). In literature-search steps, a structured list of queries created by the LLM is used to retrieve a list of citations from an external citation database (28) (Fig. 1A; Methods). Ultimately, these intermediate products are automatically assembled into a complete research paper (labeled with an Al-created watermark for transparency; Fig. 1B; Methods).</p>
<p>Each research step is implemented as a distinct conversation, with agent identity specification, provision of prior research products, mission instructions, and LLM responses with iterative feedback (Fig. 1C; Methods; Fig. S1). First, the LLM agent is designated a specific identity (Methods; e.g. "You are a scientist who needs to write literature search queries"; "Performer system prompt", Table S1). Next, data-to-paper populates the conversation with a set of "provided prior products": a list of messages providing the LLM with a pre-defined subset of research products of prior steps, deemed important for the focal task (Fig. 1B; Fig. S1; "Provided prior products", Table S1). This rigor control of information flow among steps minimizes possible hallucinations due to mixing relevant and irrelevant information (29). It also allows data-to-paper to trace, verify and chain the sources of numeric results cited by the LLM (Methods). Next, a step-specific "mission prompt" message is appended, defining the new product that the LLM is expected to create (e.g. "Please write literature-search queries..."; "Performer mission prompt", Table S1). Then, data-to-paper requests a response from the LLM model API (30-32), from which it extracts the requested product (based on defined formatting; Fig. S1; Table S1; Methods). The extracted product then undergoes a series of rule-based algorithmic checks, providing constructive feedback to the LLM upon failure (Methods; Fig. 1C; Fig. S1). In particular, to minimize errors in the coding steps, we have built a unique framework that imposes guardrails against commonly observed coding and statistics analysis errors, through a series of static code checks, runtime errors, package-specific guardrails, and output verifications (Fig. 1A, "Coding" block; Methods).</p>
<p>Once the created product passes rule-based review, it may further be refined through LLM review (9, 33-38) (steps with "Review" ellipses, Fig. 1A; Methods). LLM review is implemented as a parallel, role-inverted conversation, effectively creating an exchange between two LLM agents (Methods; Fig. 1C; Fig. 2A; Fig. S2). In co-pilot mode, the human user can provide additional review comments, resulting in further LLM iterations (Methods). Once a product passes rule-based, LLM and optionally a human review, the step is concluded and data-to-paper proceeds to the next step, until all products are created and the paper is assembled. While data-to-paper can work with any LLM, in practice, our implementation uses ChatGPT; using the current state-of-the-art open-source LLMs leads to frequent mistakes that preclude completing full research cycles (Methods; Table S2; Fig. S3). Of note, since ChatGPT is not a deterministic model, each run of data-to-paper, even on the same dataset and either with or without a human-provided goal, unfolds with different analyses, yielding different overall manuscripts.</p>
<h1>Open-goal research on public datasets</h1>
<p>Running in an open-goal, autopilot modality, we provided data-to-paper with two publicly available datasets: (i) "Health Indicators" dataset (24), an unweighted curated subset of the CDC's Behavioral Risk Factor Surveillance System (BRFSS) from 2015 (39), with 253,680 clean responses, each including 22 features related to diabetes and general health, and (ii) "Social Network" dataset (25), a directed graph representing Twitter interactions among members of the 117th US congress, as well as member affiliations (Chamber, Party and State). For each of these two datasets, we ran data-to-paper for 5 full research cycles, creating 10 distinct manuscripts (Supplementary Dataset A,B; Supplementary Data Descriptions A,B; Supplementary Manuscripts A1-5, B1-5). During these research cycles, which took about an hour each, data-to-paper generated and corrected hypotheses, created and debugged code, composed search queries and retrieved citations, and wrote and revised the manuscript section by section (full conversations in Supplementary Runs A1-5, B1-5; Fig. 2B; Figs. S4-S7). All created manuscripts properly followed the canonical structure of a research paper, including a proper title and abstract, a well-formulated introduction that stresses the research questions in light of relevant literature, a method section providing a transparent and human-traceable description of the analysis and key methodologies, several supplementary sections providing all custom-written codes, properly formatted scientific tables, a results section which describes the findings while properly referring to each of the tables, and a referenced discussion section which summarizes the results, delineate limitations and puts the findings in a broader context (Supplementary Manuscripts A1-5, B1-5). While similar in structure, the 5 different papers produced for each dataset addressed different topics and raised and tested different hypotheses (Table 1, Table S3). These papers are not highly creative, yet they do define a reasonable set of hypotheses, test them with simple straightforward statistical approaches, and ultimately create and adequately report de novo insights from the provided data.</p>
<p>Manually vetting the data analysis and the text of these papers, we found that out of these 10 open-goal papers, 8 reported correct analysis with only minor wording imperfections, yet 2 were erroneous, showing fundamental analysis or interpretation mistakes (Supplementary Manuscripts A1-5, B1-5). The analyses in all 5 "Health Indicators" papers were based on either logistic or linear regression models, all adequately performed while accounting for a reasonable choice of confounding factors (Table S4). Furthermore, interaction terms were adequately added when needed, and the dataset was adequately restricted to reflect the tested hypotheses (restricting to the diabetic sub-population; Table S4). For the "Social Network" dataset, papers were based on linking graph properties with node properties, as</p>
<p>well as on creating new node properties (e.g. State representation size), and then applying linear regression, ANOVA, or Chi-square on either the graph nodes or edges as appropriate (Table S4; see methods sections and analysis codes in each of the created papers, Supplementary Manuscripts A1-5, B1-5). In all 10 papers, the generated scientific tables correctly represented the results of the analysis. Vetting the text, we observed that data-to-paper is adequately interpreting the analysis results with factual statements, correctly referring to tables and citing key numeric values from the analysis, and reasonably describing the research question and findings in the context of existing literature (green highlights, Supplementary Manuscripts A1-5, B1-5; Methods). We also detected multiple imperfections, such as generic phrasing, overstatement of novelty, and inadequate and sometimes lacking choice of citations (yellow and orange highlights, Supplementary Manuscripts A1-5, B1-5). More major, result-affecting, mistakes were found in 2 of the 10 papers: In one of the "Health Indicators" papers, a correct analysis was misinterpreted due to hallucinations in the goal specification step, leading to conclusions beyond the scope of the analysis; and in one of the "Social Network" papers, an erroneous analysis was performed, resulting in unfounded statements on statistical associations between social interactions and party affiliations (red highlights, Supplementary Manuscript A2 and B2, respectively).</p>
<h1>Estimating reliability in reproducing peer-reviewed results</h1>
<p>To more systematically assess its error rate in autopilot mode, we applied data-to-paper in a fixed-goal modality in two case studies for which we have benchmarks of published peer-reviewed results. We specifically wanted to check two critical aspects for the reliability of analysis and interpretation: the proper reporting of both positive and negative findings (challenge 1), and the performance for tasks with multiple different steps with tunable breadth (challenge 2). To test data-to-paper capacity in these two challenges, we chose the following two examples of peer-reviewed studies: a study by Saint-Fleur et al. (26), which adequately reports both positive and negative findings related to the association of a policy change in a Neonatal Intensive Care Unit with treatment choice and treatment outcome, respectively (challenge 1); and a study by Shim et al. (27), which builds several Machine Learning models for predicting optimal intubation depth in pediatric patients, and compare their prediction accuracy with formula-based models, thereby requiring multiple analysis steps, whose breadths can be gradually tuned (by altering the number of models to compare; challenge 2). Both studies provide well-annotated datasets and both were published after the knowledge cutoff date of the ChatGPT models that we used (September 2021; Methods). For each of the two case studies, we have provided data-to-paper with the</p>
<p>research goal of the original publication and the corresponding dataset and ran it for 10 independent research cycles (Supplementary Data Descriptions C,D; Supplementary Datasets C,D; Tables S5,S6; Supplementary Manuscripts C1-10, Da1-10). Within each case study, the created papers were all similar to each other in their content, terminology and structure. Indeed, quantifying content similarity by the pairwise cosine distance between the vector embeddings of the title and abstract of all created manuscripts (40) showed tight and distinct clusters corresponding to the 4 case studies (two open-goal and two fixed-goal; Fig. 3). Furthermore, the fixed-goal papers were also similar to their respective original studies $(26,27)$ in content, terminology and in their vector embeddings (Fig. 3).</p>
<p>We manually vetted the analysis and reported results of the manuscripts created for each of the two study-reproducing challenges. For challenge 1, we found that all papers correctly reproduced the analysis, and 8 of them reached the overall correct conclusions and adequately reported both the negative and positive results. All of these manuscripts used adequate statistical methodologies, either matching the methods used in the original study (26) or providing valid alternatives (Table S5; Supplementary Manuscripts C1-10, Supplementary Runs C1-10). Yet, despite correct analysis, in 2 out of these 10 papers we identified interpretation errors, which in one of the papers also affected the overall conclusions (Fig. 4; Supplementary Manuscripts C1,2, red and orange highlights; Tables S5,S6). In challenge 2, we found that the rate of error critically varied with the breadth of the analysis; while data-to-paper frequently failed when presented with the original, broad research goal ( $90 \%$ error rate), it was able to correctly perform this multi-step model development research for almost identical research goals except for requesting fewer models (10-20\% error rate; Fig. 4). We note that as the breadth of the task increases, the number of iterations required to complete the Data Analysis step increases, providing a potential possibility to alert of too complex analysis and difficult goals (Fig. 4, bottom). We further note that in all cases, the process reliability depends on the formulation of the research goal and the description of the dataset; less detailed and explicit formulations can increase analysis errors (Fig. S8; Supplementary Manuscripts Dai1-10, Dbi1-10, Dci1-10 and Data Descriptions Dai, Dbi, Dci). Finally, allowing human co-piloting (Methods), a 2-3 single-sentence review comments per run, typically in the code writing step, allows creating accurate papers consistently even for the more complex goals (Fig. 4; Supplementary Human Co-piloted Manuscripts 1-3). Altogether, these case studies provide an assessment of data-to-paper's analysis and interpretation reliability, showing that for simple research goals it can autonomously create reliable manuscripts in $80-90 \%$ of the cases, and that for more complex goals human-copiloting is critical to assure reliability.</p>
<p>Finally, noting the effort and necessity of manually vetting and verifying created manuscripts, we harnessed data-to-paper step-to-step information tracing to chain results, methodology and data in created manuscripts through algorithmically verified hyperlinks (Methods). This approach creates manuscripts in which all cited numeric values are recursively linked to the specific lines of code where they are created. In particular, numbers cited in the manuscript are linked to a "Notes" appendix providing their formula and its explanation, and from there to the specific table where values used in these formula have originated from, and from the table to the corresponding output file of the code from which the table was created, and from there to the very specific part of code which produced this output file (see clickable hyperlinks in Supplementary Data-chained Manuscripts A-D). Such data-chained manuscripts facilitate systematic vetting of papers, setting a new standard for traceability for the coming era of Al-powered research.</p>
<h1>Discussion</h1>
<p>Inspired by key features of human research, we use prompt automation, tool augmentation, and multi-agent interaction approaches $(9,12,33)$ to guide multiple LLM agents through a full research path leading from annotated data to well-structured transparent, human-verifiable papers. Tracing information through the different research steps allows data-to-paper to create "data-chained" manuscripts, where results, methodologies and data are programmatically linked. While the novelty of this Al-driven research falls well behind high-end contemporary science, it did demonstrate a de novo creation of new insights from provided data, thereby recapitulating a key aspect of human research and taking science automation well beyond what is possible with algorithmic data exploration (41). Furthermore, the process demonstrates versatility with respect to data types and research domains, and is able to produce different forms of scientific output, such as association studies, network analysis, or development and testing of machine learning models. Run fully autonomously, the process however is not error-free; despite minimizing errors with multiple guardrails, algorithmic checks, review cycles, and tight control of information flow, the notorious problem of LLM hallucinations (29) leads to fundamental errors in about 10 to 20 percent of created papers, for simple analysis tasks, and to consistent failure for more complex tasks. Integrating human co-piloting, few short review comments were sufficient to overcome errors even for complex tasks. Our current implementation has multiple constraints: it is limited to textual and table outputs, is unable to formulate and pursue follow-up questions, and is limited to hypothesis-testing research.</p>
<p>Despite these current limitations, the ability of LLMs to carry out scientific research presents important opportunities, but also major challenges. Indeed, such AI research approaches, capable of creating de novo research papers from data in just an hour, could dramatically accelerate the pace of the scientific process. However, there are also risks associated with this development, such as the dishonest use of such systems, e.g. in the context of $P$-hacking (42), or overloading the publication system with medium-level and generic manuscripts addressing insignificant problems (43-45). Our approach implements specific features to mitigate some of these risks, in line with emerging guidelines on Al in science (22), including a transparent, overseeable process allowing human co-piloting, unbiased reporting of either positive or negative results, and the creation of transparent, Al-marked, "data-chained" and human-verifiable papers. Given the relatively limited novelty and the potential for errors in Al-driven research, as well as the need for ethical judgments and accountability (22), we anticipate and urge that such Al-driven approaches will used as scientist co-pilots, helping scientists in the more straightforward tasks, thereby allowing them to focus their minds and creativity on higher-level concepts.</p>
<h1>Methods</h1>
<p>Datasets. We used 4 datasets, each consisting of data files ("Data", Fig. 1B; Supplementary Datasets A-D) and metadata items (the human-provided products "Data file description" and "General description of dataset", Fig. 1B; Supplementary Data Descriptions A-D). (A) "Health Indicators" dataset (24). A clean unweighted subset of CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015 annual dataset (39), downloaded from Kaggle (24). It contains 253,680 survey responses each with 22 features related to diabetes and different health indicators, with no missing values. No change in the dataset was made; data-to-paper was provided with the csv file as downloaded from Kaggle. (B) "Social Network" dataset (25). A directed graph of Twitter interactions among the 117th Congress members (25). Two data files were provided to data-to-paper: (i) a csv file containing a list of directed unweighted edges, representing Twitter engagements among Congress members (downloaded from Stanford Network Analysis Project (46), with the weights removed), and (ii) a csv file containing the affiliations of each Congress member, including their Chamber, Party and State (downloaded from FRAC (47)). (C) "Treatment policy" dataset (a test case to reproduce Saint-Fleur et al. (26)). A dataset on treatment and outcomes of non-vigorous infants admitted to the Neonatal Intensive Care Unit (NICU), before and after a change to treatment guidelines was implemented. As input to data-to-paper, the file downloaded from Saint-Fleur et al. (26) was converted into a csv file, with minor cleanups: converting column headers into alphanumeric names, converting string binary into integer binary, and removing</p>
<p>the following irrelevant columns: 'RACE', 'RACE IN TWO CATEGORIES', 'ETHNICITY', 'Singleton /Multiple', 'Maternal Diabetes...', 'PRETERM VS TERM', 'ROUTINE RESUSCITATION...', 'Respiratory Support', 'Exposure to xrays', 'X-Ray finding' (without removing these columns, the "Data exploration" step of data-to-paper occasionally created too large output files leading to breaking the token limit of ChatGPT). (D) "Treatment Optimization" dataset (a test case to reproduce Shim et al. (27)). A dataset of 967 pediatric patients, which received mechanical ventilation after undergoing surgery, including an x-ray-based determination of the optimal tracheal tube intubation depth and a set of personalized patient attributes to be used in machine learning and formula-based models to predict this optimal depth. As input for data-to-paper, we removed irrelevant columns, leaving only the ones used in the original study: 'tube', 'sex', 'age_c', 'ht', 'wt', 'tube_depth_G'. For datasets C and D, we further provided data-to-paper with the research goal of their respective original studies. Research goals and dataset descriptions have been formulated in an iterative and empirical process: We consulted with ChatGPT on best phrasing and terminologies, tested them in pilot runs, identified misunderstandings and vague or ill-defined statements, and adapted the descriptions accordingly. Dataset descriptions and file descriptions for all datasets, as well as the research goal for datasets C,D, are provided in Supplementary Data Descriptions A-D.</p>
<p>Execution of data-to-paper. For each run, data-to-paper is provided with a dataset, its associated metadata, and an optional research goal and proceeds automatically through the stepwise research process (Fig. 1A,B). In open-goal modality, data-to-paper runs through the entire research process (Fig. 1A,B). In fixed-goal modality, the research goal is provided and the steps for choosing a research goal are skipped ("Fixed-goal modality", Fig. 1A). Human interactions are implemented as a simple user approval at each research step (autopilot mode; user is only overseeing) or with complete human review through an interactive app (co-pilot mode; user can provide reviewing comments at each step). For each dataset, we performed multiple data-to-paper runs, as follows. (A) "Health Indicators" dataset. We ran data-to-paper in an open-goal modality with this dataset and its associated metadata for 5 full research cycles (Supplementary Runs and Manuscripts A1-5). Overseeing the process, we aborted and restarted the 5th run 3 times after the "Goal validation" step, when observing that the chosen Research goal was too similar to goals of prior research cycles. (B) "Social Network" dataset. We ran data-to-paper in an open-goal modality with this dataset and its associated metadata for 5 full research cycles (Supplementary Runs and Manuscripts B1-5). To minimize overlapping goals in repeated runs, a list of the already-chosen previous goals was presented as part of the "mission prompt" of the "Research goal" step. (C) "Treatment Policy" dataset. We ran data-to-paper in</p>
<p>a fixed-goal modality for 10 research cycles with this dataset and its associated metadata and research goal (Supplementary Runs and Manuscripts C1-10). (D) "Treatment Optimization" dataset. We ran data-to-paper in a fixed-goal modality for 10 full research cycles with this dataset and its associated metadata and research goal (Supplementary Runs and Manuscripts Da1-10). We then ran data-to-paper with 5 modified research goals (Supplementary Data Descriptions Db, Dc, Dai, Dbi, Dci) for 10 times per goal (Supplementary Runs and Manuscripts Db1-10, Dc1-10, Dai1-10, Dbi1-10, Dci1-10). As these additional 50 runs were only used to annotate analysis failure, we terminated them after the "Title \&amp; abstract" step (to save unnecessary api calls). In addition, we ran data-to-paper in co-pilot mode for three times on the original goal (Supplementary Data Description Da). During each of these runs, we provided several review comments, typically in the code writing step (Supplementary Human Co-piloted runs 1-3).</p>
<p>Overview of data-to-paper implementation. We implement data-to-paper as a chained list of research steps, each designed to create one or more research products based on a provided subset of prior research products (Fig. 1A,B). Each such research step is implemented as a distinct "Performer conversation", which specifies LLM identity, relevant prior research products and a step-specific "mission prompt" requesting the LLM to create a focal product. Product extracted from the LLM response undergoes rule-based review and programmatic feedback requesting corrections is sent back to the LLM. For certain research steps, once the product passes rule-based review it can also be sent for LLM review, which is implemented in a parallel "Reviewer conversation" ("Review", "LLM reviewer agent" in Fig. 1A,C respectively). The research step terminates with a final product that has passed both rule-based and LLM-based review. Once all steps are completed, a manuscript is assembled and compiled from the products of all relevant steps.</p>
<p>Devising prompts. The prompts used by data-to-paper in each of the research steps are listed in Table S1. These prompts have been designed in an iterative and empirical process. First, we devised an initial version for each of the prompts, focusing on the key aspect of their focal task (dark brown text, Table S1). Additionally, we added to each prompt formatting instructions for the research product (light blue and red text, Table S1). Then, we tested ChatGPT responses through multiple pilot runs, identified wrong or inadequate responses, and adapted the prompts with additional details and specifications (light brown text, Table S1). In cases where ChatGPT still failed to consistently respond as expected, we also added one-shot examples (green text, Table S1).</p>
<p>Message types. Messages in a conversation are designated as either SYSTEM, USER, or ASSISTANT (per OpenAI API terminology (30)). SYSTEM and USER messages are</p>
<p>programmatically composed by data-to-paper. ASSISTANT messages are created by the LLM. We also implement LLM-surrogating ASSISTANT messages, which are messages created programmatically by data-to-paper, but are attributed to the ASSISTANT (namely, they appear to the LLM as if they were created by it).</p>
<p>Performer conversation. At the onset of each research step, a distinct Performer conversation is initiated and programmatically pre-filled with a list of "context messages": (i) "system prompt" defining the identity of the performer LLM agent ("Performer system prompt", Table S1); (ii) "provided prior products", a list of USER messages providing the LLM with a pre-defined subset of research products of prior steps, with each such USER message followed by an LLM-surrogating acknowledgment message (Fig. 1B, Figs. S1,S4; "Provided prior products", Table S1); and (iii) USER message describing to the LLM what it is requested to do in the current step ("Performer mission prompt", Table S1). This pre-filled Performer conversation is then sent to the LLM API (30-32) to request an initial response (Figs. S1,S4). The requested research product is then extracted from this initial LLM response and undergoes rule-based product review.</p>
<p>Rule-based product review. At each research step, the LLM is requested to send a response containing a specific product, with specific formatting (Fig. 1B; Tables S1,S7). Then, data-to-paper extracts the requested product from the LLM response based on its expected formatting (Tables S1,S6; for example, when requesting a "LaTex text" product, we expect the product to be enclosed within triple backticks). Failure to extract the product is translated into a feedback message sent back to the LLM (for example: "You sent 2 triple-backtick blocks. Please send the latex as a single triple-backtick 'latex' block"). Once the product is extracted successfully, it is programmatically refined according to a set of step-specific auto-refinement rules (Table S7, asterisk-marked rules). Then, the refined product is checked according to a set of step-specific test rules, including formatting, text length and correct referencing (for exhaustive list see Table S7). Failure to pass any of these rules is translated into a corresponding feedback message sent back to the LLM (see example in Fig. S5).</p>
<p>Information tracing. To follow information flow through all steps, data-to-paper keeps track of the specific code lines producing each file output, the translation of these outputs into tables and the incorporation of numbers from the table in the Results section. Specifically, to track numeric results in the Results section, we programmatically assign a unique label for each numeric value appearing in the prior products for the Results writing step, and present these products in the context messages with the numeric values formatted as LaTex hypertargets with their corresponding labels (Fig. 1B). We then complemented the mission</p>
<p>prompt of the Result writing step with instructions requesting the LLM agent to wrap each numeric value that it writes with a LaTex hyperlink matching the corresponding label ("Performer mission prompt: additional instructions for data-chained manuscripts", Table S1). To allow the LLM to include numeric values which are not direct output of the code, but are rather arithmetically derived from them (like changing units, translating regression coefficients to odds ratios, etc), we further provide it with the option of using a specific syntax, \num( $&lt;$ formula&gt;, "explanation"), where it can provide arithmetic formula to derive new values from values created by the code output, and provide an explanation. A rule-based feedback was added to algorithmically verify that, either as stand-alone or within a \num formula, each numeric value mentioned in the section is hyperlinked, and that the target of each link correctly matches the corresponding label provided in the prior product context. Upon compilation, the \num commands are replaced with their value and a "Notes" appendix is added listing all formulas with their explanation. To further safeguard against hallucinated or missing values, the Results "mission prompt" instructs the LLM to use a designated placeholder (specifically '[unknown]') for missing numeric values, detection of this or other placeholder in the LLM response leads to data-to-paper aborting the entire research cycle (for the list of placeholders see "Results", Table S7).</p>
<p>Data-chained manuscripts. Reflecting the tracing of information during the "Data Analysis", "Table Design" and "Results" writing steps, data-to-paper creates manuscripts that "chain" results, methods and data, where each numeric value is recursively linked to the specific lines of codes that created it. In particular, a numeric value in the "Results" section can be linked to the "Notes" appendix, and from there to a specific value in a table, and from there to the output file that was used to create this table and finally to the specific code lines which generated this output file (Supplementary Data-chained Manuscripts A-D and Supplementary Human Co-piloted Manuscripts 1-3; Note that prior manuscripts were created without this feature and do not have hyperlinks).</p>
<p>Reviewer conversation. For a subset of research steps, data-to-paper also performs an LLM review after the successful completion of rule-based product review ("Review", Fig. 1A; Fig. 2, Figs. S2,S6). LLM review is implemented in a "Reviewer conversation", which parallels the Performer conversation of the given step, but with inversion of the USER-ASSISTANT roles (Fig. 1C; Fig. 2A; see examples in Fig. 2B, Fig. S6). In parallel to its corresponding Performer conversation, this Reviewer conversation is pre-filled with the following list of context messages: (i) "system prompt" defining the identity of the LLM reviewer agent; (ii) the list of "provided prior products" for the focal step; and (iii) An LLM-surrogating message with the "Performer mission prompt" (namely, the "Performer</p>
<p>mission prompt" is casted as an ASSISTANT-side message, thereby appearing as if it was created by the LLM reviewer agent). Then, the extracted product coming from the Performer conversation is presented as a USER-side message together with step-specific review instructions, in which the LLM reviewer agent is requested to choose between accepting the provided product, or providing constructive feedback ("Reviewer mission prompt", Table S1; Fig. 1C; Fig. 2A; Fig. S2). The pre-filled conversation is then sent to the LLM API to request a response from the Reviewer agent. If the Reviewer response contains feedback, it is transferred to the Performer conversation as if it were a USER-side message, requesting the LLM performer agent to provide a new response with an accordingly refined product.</p>
<p>Coding steps. For each of the three coding steps ("Data exploration", "Data analysis", "Table design"), we extract Python code (enclosed within a triple-backtick block) from the LLM response, and test this code at four levels: (i) Static analysis: Check that the code conforms to a step-specific requested structure ("Python code - Static checks", Table S7); (ii) Runtime analysis: Syntax errors, runtime errors, warnings, as well as violations of other restrictions are caught and evaluated during code execution ("Python code - Runtime checks", Table S7); (iii) Package-specific guardrails: Noting common ChatGPT coding mistakes, we wrapped the packages that we allow importing, adding multiple guardrails to monitor, control and restrict unsafe functionalities, as well as to allow rule-based review of p-value formatting (Table S8); (iv) Output analysis: Check that all the requested output files are created and contain the requested information with the requested formatting ("Numerical data checks", Table S7). Encountered issues from these 4 check levels are translated into a feedback message sent back to the LLM. As a new feedback message is added, older feedback-response message pairs are removed from the conversation (to avoid exceeding the token limits). Once the LLM-provided code passes all tests, we proceed to LLM product review: data-to-paper provides a message that shows the LLM the code output and asks it to check the code and the output and provide a list of issues and suggested corrections (see "Reviewer mission prompts" for "Data exploration" and "Data analysis" steps in Table S1). If the LLM returns suggestions for improvement, data-to-paper requests making these corrections and enters a new phase of code debugging as described above. If there are no suggestions for improvements, we end the coding step with the code and the output files it created as the corresponding research products.</p>
<p>Citation retrieval. For the two literature search steps ("Literature search I", "Literature search II", Fig. 1A; Table S1), data-to-paper augments the LLM with Semantic Scholar Academic Graph API (28), an external citation database and search service. This direct citation retrieval, along with algorithmic checks restricting LLM's memory-retrieved citations</p>
<p>(Rule-based product review; Table S7), ensures that only valid citations are included in the resulting paper. These literature-search steps start with a "Devise queries" step, in which the LLM is requested to provide a list of queries for each of a pre-defined set of scopes (scopes for "Literature search I": "Dataset", "Questions"; scopes for the "Literature search II": "Background", "Dataset", "Methods", "Results"; see "Literature search I" and "Literature search II" in Table S1). Then, data-to-paper calls the citation API (28) to retrieve a list of citations for each of the LLM-provided queries (see example in Fig. S7). For each citation, the API provides: (i) Search rank; (ii) BibTeX ID; (iii) Title; (iv) Journal and year; (v) One-sentence paper summary (TLDR) (48); (vi) Citation influence (49); (vii) Title and abstract embedding (40). Citations for each of the scopes are then filtered and sorted either by Search rank or by Title and abstract embedding similarity to the title and abstract embedding of the currently written paper (parameters in Table S9). For the runs with datasets C, D, where we attempt reproducing a specific original study, we manually excluded the citation of the original paper. The sorted lists of papers for each scope are then provided as prior products for steps in which the LLM is requested to refer to literature citations (Table S1; Fig. 1B, Fig. S7).</p>
<p>LLM selection. We compared the performance of Llama 2, Codellama and ChatGPT models in two critical research steps: (i) Research goal and (ii) Data analysis. For both tests, we used the "Health Indicators" dataset. In (i), we ran the research goal step of data-to-paper 10 times each either with gpt-3.5-turbo or Llama-2-70b-chat-hf, all provided with the same prior product context (Table S2). We manually annotated the goals, scoring analysis-related factors, either corresponding to true features of the dataset, or to hallucinated features not part of the dataset (Table S2, Fig. S3A). In (ii) we ran the data analysis step of data-to-paper 10 times each with either gpt-3.5-turbo, gpt-4, CodeLlama-34b-Instruct-hf, Llama-2-70b-chat-hf or Llama-2-7b-chat-hf and evaluated for each run the number of programmatic feedback rounds until the code passes rule-based review (Fig. S3B, Supplementary Coding Runs).</p>
<p>ChatGPT models and parameters. As the underlying LLM, we used OpenAI conversational ChatGPT models (30) (open-source models created hallucinated research goals and were not able to consistently converge in the data analysis coding step; LLM selection). The OpenAI models used were either gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, or gpt-4 (all with a knowledge cutoff of September 2021). For each research step, we assigned one of these specific models as a nominal model based on the expected conversation length of the step as well as the presumed difficulty and performance during pilot runs ("LLM", Table S1). Starting from this initial nominal model for each step, data-to-paper can automatically</p>
<p>upgrade the model during a conversation: switching to gpt-4 when a rule-based feedback request is not resolved, and switching to gpt-3.5-turbo-16k-0613 if the number of tokens exceeds the maximum of the step's nominal model. For all models, we use default model parameters, except for the model's temperature which was specifically set for some of the steps (In particular, setting a temperature of 1 for the "Research goal" step).</p>
<p>Paper assembly and compilation. To produce the final manuscript, data-to-paper assembles a single LaTex file, combining the different manuscript-part products ("Paper assembly", Fig. 1A,B). It then automatically compiles this file, together with the list of citations retrieved from "Literature search II", into a pdf, watermarked "Created by data-to-paper (AI)".</p>
<p>Manual review of created manuscripts. We manually vetted each created manuscript and its respective run record (Supplementary Manuscripts and Runs A1-5, B1-5, C1-10, Da1-10, Db1-10, Dc1-20, Dai1-10, Dbi1-10, Dci1-10). For the manuscripts, we verified: (i) that the data analysis and code are correctly performed, using adequate statistical methodologies; (ii) that every statement in the text involving numeric information corresponds to the correct numeric value from the output of the data analysis; (iii) that every citation fits the context in which it was referenced; (iv) the overall exactness of the text; and (v) the quality of the overall text and wording. The manuscripts were highlighted to reflect correctly-put statements (green), imperfect, or atypical statements (yellow), minor errors (orange), and major errors (red).</p>
<p>Human co-piloting. Human co-piloting is incorporated by allowing the user to add review comments in each step after the rule-based and LLM-review have completed. If such human review is added, data-to-paper initiates a new cycle of Performer answers with rule-based checks. This process repeats iteratively until the user approves the research product of the step. We have created an app with a user interface that allows the user to follow the LLM conversation in each step and add review comments as needed.</p>
<h1>Data availability</h1>
<p>The data that support the findings of this study are available in the paper and its Supplementary Information (https://github.com/rkishony/data-to-paper-supplementary).</p>
<h2>Code availability</h2>
<p>Code is available at https://github.com/Technion-Kishony-lab/data-to-paper</p>
<h1>References</h1>
<ol>
<li>Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu, X. Huang, T. Gui, The Rise and Potential of Large Language Model Based Agents: A Survey, arXiv [cs.AI] (2023). http://arxiv.org/abs/2309.07864.</li>
<li>Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, B. Ge, Summary of ChatGPT-Related research and perspective towards the future of large language models. Meta-Radiology 1, 100017 (2023).</li>
<li>L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, J.-R. Wen, A Survey on Large Language Model based Autonomous Agents, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.11432.</li>
<li>Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, T. Hubert, P. Choy, C. de Masson d'Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. Sutherland Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, O. Vinyals, Competition-level code generation with AlphaCode. Science 378, 1092-1097 (2022).</li>
<li>G. Spitale, N. Biller-Andorno, F. Germani, AI model GPT-3 (dis)informs us better than humans. Sci Adv 9, eadh1850 (2023).</li>
<li>J. He-Yueya, G. Poesia, R. E. Wang, N. D. Goodman, Solving Math Word Problems by Combining Language Models With Symbolic Solvers, arXiv [cs.CL] (2023). http://arxiv.org/abs/2304.09102.</li>
<li>B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli, A. Fawzi, Mathematical discoveries from program search with large language models. Nature 625, 468-475 (2024).</li>
<li>T. H. Trinh, Y. Wu, Q. V. Le, H. He, T. Luong, Solving olympiad geometry without human demonstrations. Nature 625, 476-482 (2024).</li>
<li>C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J. Xu, D. Li, Z. Liu, M. Sun, Communicative Agents for Software Development, arXiv [cs.SE] (2023). http://arxiv.org/abs/2307.07924.</li>
<li>Y. Dong, X. Jiang, Z. Jin, G. Li, Self-collaboration Code Generation via ChatGPT, arXiv [cs.SE] (2023). http://arxiv.org/abs/2304.07590.</li>
<li>W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, M. Sachan, RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text, arXiv [cs.CL] (2023). http://arxiv.org/abs/2305.13304.</li>
<li>V. Nair, E. Schumacher, G. Tso, A. Kannan, DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents, arXiv [cs.CL] (2023). http://arxiv.org/abs/2303.17071.</li>
<li>T. Tu, A. Palepu, M. Schaekermann, K. Saab, J. Freyberg, R. Tanno, A. Wang, B. Li, M. Amin, N. Tomasev, S. Azizi, K. Singhal, Y. Cheng, L. Hou, A. Webson, K. Kulkarni, S. Sara Mahdavi, C. Semturs, J. Gottweis, J. Barral, K. Chou, G. S. Corrado, Y. Matias, A.</li>
</ol>
<p>Karthikesalingam, V. Natarajan, Towards Conversational Diagnostic AI, arXiv [cs.AI] (2024). http://arxiv.org/abs/2401.05654.
14. D. A. Boiko, R. MacKnight, B. Kline, G. Gomes, Autonomous chemical research with large language models. Nature 624, 570-578 (2023).
15. A. Birhane, A. Kasirzadeh, D. Leslie, S. Wachter, Science in the age of large language models. Nature Reviews Physics 5, 277-280 (2023).
16. G. Conroy, How ChatGPT and other AI tools could disrupt scientific publishing. Nature 622, 234-236 (2023).
17. C. Stokel-Walker, R. Van Noorden, What ChatGPT and generative AI mean for science. Nature 614, 214-216 (2023).
18. C. Stokel-Walker, ChatGPT listed as author on research papers: many scientists disapprove. Nature 613, 620-621 (2023).
19. M. Hutson, Could AI help you to write your next paper? Nature 611, 192-193 (2022).
20. V. Berdejo-Espinola, T. Amano, AI tools can improve equity in science. Science 379, 991 (2023).
21. L. Messeri, M. J. Crockett, Artificial intelligence and illusions of understanding in scientific research. Nature 627, 49-58 (2024).
22. C. L. Bockting, E. A. M. van Dis, R. van Rooij, W. Zuidema, J. Bollen, Living guidelines for generative AI - why scientists must oversee its use. Nature 622, 693-696 (2023).
23. E. B. Wilson, An Introduction to Scientific Research (Courier Corporation, 1990; https://play.google.com/store/books/details?id=rKCHDQAAQBAJ).
24. A. Teboul, Diabetes Health Indicators Dataset (2021).
https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset.
25. C. G. Fink, N. Omodt, S. Zinnecker, G. Sprint, A Congressional Twitter network dataset quantifying pairwise probability of influence. Data Brief 50, 109521 (2023).
26. A. L. Saint-Fleur, H. E. Alcalá, S. Sridhar, Outcomes of neonates born through meconium-stained amniotic fluid pre and post 2015 NRP guideline implementation. PLoS One 18, e0289945 (2023).
27. J.-G. Shim, K.-H. Ryu, S. H. Lee, E.-A. Cho, S. Lee, J. H. Ahn, Machine learning model for predicting the optimal depth of tracheal tube insertion in pediatric patients: A retrospective cohort study. PLoS One 16, e0257069 (2021).
28. R. Kinney, C. Anastasiades, R. Authur, I. Beltagy, J. Bragg, A. Buraczynski, I. Cachola, S. Candra, Y. Chandrasekhar, A. Cohan, M. Crawford, D. Downey, J. Dunkelberger, O. Etzioni, R. Evans, S. Feldman, J. Gorney, D. Graham, F. Hu, R. Huff, D. King, S. Kohlmeier, B. Kuehl, M. Langan, D. Lin, H. Liu, K. Lo, J. Lochner, K. MacMillan, T. Murray, C. Newell, S. Rao, S. Rohatgi, P. Sayre, Z. Shen, A. Singh, L. Soldaini, S. Subramanian, A. Tanaka, A. D. Wade, L. Wagner, L. L. Wang, C. Wilhelm, C. Wu, J. Yang, A. Zamarron, M. Van Zuylen, D. S. Weld, The Semantic Scholar Open Data Platform, arXiv [cs.DL] (2023). http://arxiv.org/abs/2301.10140.
29. Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, W. Dai, A. Madotto, P. Fung, Survey of Hallucination in Natural Language Generation, arXiv [cs.CL] (2022).</p>
<p>http://arxiv.org/abs/2202.03629.
30. OpenAI platform. https://platform.openai.com/docs/api-reference.
31. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, T. Scialom, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv [cs.CL] (2023). http://arxiv.org/abs/2307.09288.
32. B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, G. Synnaeve, Code Llama: Open Foundation Models for Code, arXiv [cs.CL] (2023). http://arxiv.org/abs/2308.12950.
33. Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, A. H. Awadallah, R. W. White, D. Burger, C. Wang, AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.08155.
34. S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, J. Schmidhuber, MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.00352.
35. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, P. Clark, Self-Refine: Iterative Refinement with Self-Feedback, arXiv [cs.CL] (2023). http://arxiv.org/abs/2303.17651.
36. C. Harrison, LangChain (2022). https://github.com/langchain-ai/langchain.
37. S. Gravitas, AutoGPT (2023). https://github.com/Significant-Gravitas/AutoGPT.
38. B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, J. Gao, Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback, arXiv [cs.CL] (2023). http://arxiv.org/abs/2302.12813.
39. L. Rolle-Lake, E. Robbins, "Behavioral Risk Factor Surveillance System" in StatPearls (StatPearls Publishing, Treasure Island (FL), 2023; https://www.ncbi.nlm.nih.gov/pubmed/31971707).
40. A. Cohan, S. Feldman, I. Beltagy, D. Downey, D. S. Weld, SPECTER: Document-level Representation Learning using Citation-informed Transformers, arXiv [cs.CL] (2020). http://arxiv.org/abs/2004.07180.
41. C. Steinruecken, E. Smith, D. Janz, J. Lloyd, Z. Ghahramani, "The Automatic Statistician" in Automated Machine Learning: Methods, Systems, Challenges, F. Hutter,</p>
<p>L. Kotthoff, J. Vanschoren, Eds. (Springer International Publishing, Cham, 2019; https://doi.org/10.1007/978-3-030-05318-5_9), pp. 161-173.
42. N. Altman, M. Krzywinski, P values and the search for significance. Nat. Methods 14, $3-4$ (2016).
43. R. Van Noorden, Hundreds of gibberish papers still lurk in the scientific literature. Nature 594, 160-161 (2021).
44. G. Cabanac, C. Labbé, Prevalence of nonsensical algorithmically generated papers in the scientific literature. J. Assoc. Inf. Sci. Technol. 72, 1461-1476 (2021).
45. L. Liverpool, Al intensifies fight against "paper mills" that churn out fake research. Nature 618, 222-223 (2023).
46. Twitter interaction network for the US congress (2023).
https://snap.stanford.edu/data/congress-twitter.html.
47. Twitter Handles for Members of the 117th Congress, Food Research \&amp; Action Center (2021). https://frac.org/wp-content/uploads/MOC_Twitter-Handles_117th.pdf.
48. I. Cachola, K. Lo, A. Cohan, D. S. Weld, TLDR: Extreme Summarization of Scientific Documents, arXiv [cs.CL] (2020). http://arxiv.org/abs/2004.15011.
49. M. Valenzuela-Escarcega, V. A. Ha, O. Etzioni, Identifying Meaningful Citations. AAAI Workshop: Scholarly Big Data (2015).</p>
<h1>Acknowledgments</h1>
<p>We thank Ayelet Baram-Tsabari and Yael Rozenblum for discussions and providing data for initial tests, Ofer Sapir for help in organizing data-to-paper repo, Eric Lander, Yoel Fink, and Michael Elowitz for insightful discussions, and all lab members for helpful comments. LH was supported in part at the Technion by an Aly Kaufman Fellowship.</p>
<h2>Contributions</h2>
<p>TI and RK conceived the study. TI and RK developed data-to-paper with inputs from LH, MK, and OA. MK and OA implemented a graphic interface for system testing. TI and LH identified and prepared the datasets and related metadata. RK and TI oversaw the autonomous research runs. TI and LH manually vetted and highlighted created papers and run files. LH conceptualized the presentation and designed the figures with inputs from TI and RK. TI, LH and RK interpreted the results and wrote the manuscript with comments from MK and OA.</p>
<h2>Competing interests</h2>
<p>The authors declare no competing interests.</p>
<h2>Corresponding authors</h2>
<p>Correspondence to Roy Kishony, rkishony@technion.ac.il</p>            </div>
        </div>

    </div>
</body>
</html>
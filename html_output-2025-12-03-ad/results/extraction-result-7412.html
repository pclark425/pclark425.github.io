<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7412 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7412</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7412</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-17dd3555fd1ccf1141cf984347fa1b3fd6b009ca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca" target="_blank">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A system for easily mapping any natural language tasks into a human-readable prompted form and fine-tune a pretrained encoder-decoder model on this multitask mixture covering a wide variety of tasks.</p>
                <p><strong>Paper Abstract:</strong> Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7412.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7412.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt diversity (p) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number and diversity of prompts per dataset (p) on zero-shot performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled ablation varying the average number of prompt templates per training dataset (p) showing that increasing prompt count and including diverse/non-original-task prompts raises median zero-shot accuracy and generally reduces variability across held-out prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (based on T5+LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder–decoder T5 model adapted to LM objective (T5+LM) then fine-tuned on a multitask mixture of prompted datasets; checkpoint-sized variant is 11B parameters (also a 3B variant reported).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (primary); 3B reported in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot generalization to held-out tasks (11 datasets across 4 traditional NLP tasks: NLI, coreference, sentence completion, word-sense disambiguation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models are evaluated zero-shot on held-out datasets formatted via natural-language prompts; accuracy is reported per prompted template and aggregated across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt templates (many different templates per dataset); zero-shot (no labeled examples shown in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Average number of prompts per dataset p was varied: p=0 (no prompt training, baseline T5+LM), p=1 (one original-task prompt per dataset), p=5.7 (all original-task prompts), and p≈8.03 (T0 main model, includes non-original-task prompts). Evaluation reports median and interquartile range (IQR) across all evaluation prompts for each dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (median across prompts; IQR reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Increasing p from 0 to 1 yields substantial improvement over the non-prompted baseline; increasing p from 1 to 5.7 raised median for 8/11 held-out datasets and decreased spread for 7/11; moving to p≈8.03 (including non-original-task prompts) further increased median for 9/11 datasets and decreased spread for 8/11 datasets (values reported as dataset counts rather than absolute % in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T5+LM with p=0 (no prompted multitask fine-tuning) served as baseline (same base model without prompted multitask training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported as counts: median improved for 8/11 (1→5.7) and for 9/11 (5.7→8.03); spread (IQR) reduced for 7/11 (1→5.7) and for 8/11 (5.7→8.03).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All models initialized from T5+LM (11B), same hyperparameters, same number of training steps; inputs truncated to 1024 tokens, targets 256; batch size 1024 sequences; evaluation zero-shot on validation splits; median/IQR across prompts reported (no prompt selection).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Prompted Training Enables Zero-Shot Task Generalization', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7412.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7412.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset diversity (d) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of training datasets (d) on zero-shot prompt robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation varying the number of different datasets included in training (d) showing that adding more datasets increases median zero-shot accuracy but does not consistently reduce variability across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0, T0+, T0++ (T5+LM-based variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder–decoder T5+LM models fine-tuned on progressively larger multitask mixtures: T0 (d=39), T0+ (d=49 includes GPT-3 eval datasets), T0++ (d=55 adds SuperGLUE excluding RTE/CB).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot generalization to held-out datasets (subset evaluated: 5 held-out datasets shown in figure)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate median and IQR of accuracy across multiple prompts on held-out tasks when increasing number of datasets in the training mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt templates (all available prompts per dataset used), zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / training mixture composition</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Fixed p = all available prompts; d increased from 39 (T0) → 49 (T0+) → 55 (T0++). Median accuracy across all 5 evaluated held-out datasets increased with d at both steps (39→49 and 49→55). Spread (IQR) decreased inconsistently: decreased for 1 of 5 datasets when going 39→49 and decreased for 2 of 5 datasets when going 49→55.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (median across prompts; IQR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Median performance increased for all evaluated held-out datasets as d increased from 39→49→55; spread decreased only for a minority of datasets (1/5 then 2/5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T0 (d=39) baseline for the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Median increased for all evaluated datasets with each increment in d; spread changes were inconsistent (spread decreased for 1/5 datasets from 39→49, and for 2/5 datasets from 49→55).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All other hyperparameters held constant; evaluation uses median and IQR across prompts per dataset; prompts are zero-shot formatted.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Prompted Training Enables Zero-Shot Task Generalization', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7412.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7412.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instructions vs instruction-less formatting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of including explicit task instructions versus using instruction-less sentence-completion formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For tasks that can be cast as sentence completion (e.g., HellaSwag, Winogrande), removing explicit instructions and formatting as pure sentence completion can dramatically change accuracy; for HellaSwag this increased median accuracy substantially for T0.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (11B); compared to FLAN (137B) and GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T0: encoder–decoder T5+LM fine-tuned on prompted multitask mixture (11B). FLAN: decoder-only models fine-tuned on prompts (Wei et al., 2021) with much larger parameterization (~137B in the comparison). GPT-3: decoder-only family up to 175B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T0: 11B; FLAN referenced at 137B; GPT-3 referenced up to 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Commonsense sentence-completion style benchmarks (HellaSwag, Winogrande)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select the most plausible sentence completion among alternatives (multiple-choice / sentence-completion).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two variants: (A) prompt including explicit task instructions (e.g., 'Choose the most plausible continuation'); (B) instruction-less format used as sentence completion (no instructions, just context and continuations).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction presence</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors re-evaluated HellaSwag and Winogrande with prompts that omit task instructions (i.e., pure sentence completion as in some prior works). For HellaSwag, this change was applied to T0 to match FLAN/Brown et al. evaluation style.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (median across prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HellaSwag: median accuracy increased from 33.65% (with instructions) to 57.93% (instruction-less), matching FLAN's performance. Winogrande: changing to instruction-less prompt had negligible effect for T0 (accuracy = 62.15%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T0 with instruction-present prompts: HellaSwag median 33.65%; Winogrande instruction-present not stated numerically here but instruction removal had little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>HellaSwag: +24.28 percentage points absolute (33.65% → 57.93%). Winogrande: ~no substantial change (reported accuracy 62.15% under instruction-less formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot multiple-choice evaluation using the provided choice options; rank classification/log-likelihood scoring for options used where applicable; median reported across prompts. Comparison performed to match FLAN/Brown formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Prompted Training Enables Zero-Shot Task Generalization', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7412.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7412.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt wording robustness: GPT-3 vs T0 on RTE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robustness of zero-shot performance to prompt wording: comparative RTE experiment between GPT-3 (davinci) and T0 across multiple prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 (via davinci API) on 10 different RTE prompt templates shows large sensitivity to prompt wording with most prompts yielding near-random performance, whereas T0 exhibited higher robustness across the same prompt set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, API) and T0 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 davinci: decoder-only large LM (OpenAI); exact mapping to Brown et al. models is not disclosed but davinci is estimated to correspond to the 175B family. T0: encoder–decoder T5+LM fine-tuned on prompted multitask mixture (11B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 (davinci) estimated ~175B; T0 = 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RTE (Recognizing Textual Entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a premise and hypothesis, classify whether the hypothesis is entailed, contradicted, or neutral relative to the premise (recast to the dataset's label mapping used by prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; 10 different prompt templates used for evaluation (one matches Brown et al.'s reported prompt). Zero-shot (no examples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt wording</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Evaluated GPT-3 on the same 10 RTE prompt templates used to evaluate T0. One template matched Brown et al.'s reported prompt; other 9 templates were alternative wordings. Prediction via API with zero-shot prompts; accuracy measured per prompt and median/IQR reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (median across 10 prompts; interquartile range)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3 davinci: one prompt (matching Brown et al.) scored 58.8% accuracy (lower than Brown et al.'s 63.5% reported); the other 9 prompts yielded near-random performance with median = 52.96% and IQR = 1.28%. T0's evaluations on the same prompt set indicate higher robustness (T0's distribution across prompts gave higher medians and larger spread improvements vs GPT-3, though exact per-prompt T0 numbers are in the paper's figures).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Brown et al.'s single-prompt GPT-3 numbers (e.g., 63.5% for RTE) served as prior reported baseline but do not capture prompt variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Compared to Brown et al.'s single-prompt report (63.5%), GPT-3 davinci scored 58.8% on the matching prompt in this re-eval (−4.7 pp); across other prompts performance dropped to near-random (median 52.96%). Relative to T0, GPT-3's median across prompts was substantially lower and had smaller IQR, indicating less robustness to prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot evaluation via OpenAI API (davinci), same 10 prompt templates as used for T0 evaluation; no prompt selection; rank/log-likelihood scoring for multiple-choice where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Prompted Training Enables Zero-Shot Task Generalization', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7412.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7412.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask prompted training vs LM-only baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of supervised multitask prompted fine-tuning (T0) vs language-model-only pretraining (T5+LM) on zero-shot task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison showing that fine-tuning a pretrained T5+LM model on a large mixture of natural-language prompts (T0) yields significant gains in zero-shot accuracy across held-out tasks compared to the identical architecture without prompted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T5+LM fine-tuned) vs T5+LM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Both are encoder–decoder T5+LM models; baseline T5+LM is pretrained and LM-adapted but not fine-tuned on the multitask prompt mixture. T0 is the same model architecture fine-tuned on the multitask prompted mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Held-out task zero-shot generalization (11 datasets across 4 tasks; also subset of BIG-bench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation where models see prompted inputs (natural-language templates) for tasks/datasets not included in the fine-tuning mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts (various templates per dataset), zero-shot; for multiple-choice tasks, rank classification by log-likelihood over options is used.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / training regimen</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>T0 trained on a shuffled mixture of prompted datasets with many templates per dataset; evaluation reports median and IQR across prompts for each held-out dataset. The baseline T5+LM uses the same prompts at evaluation but was not trained on the prompted multitask mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (per dataset, per prompt; aggregated as median and IQR across prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T0 yields significant gains over the T5+LM baseline on all evaluated held-out datasets; T0 matches or exceeds GPT-3 performance on 9/11 held-out datasets despite being ~16× smaller than GPT-3 (T0 11B vs GPT-3 up to 175B). On BIG-bench subset T0 variants outperform baseline decoder-only LMs on nearly all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>T5+LM (same architecture without multitask prompted fine-tuning) performance (lower accuracy across held-out prompts/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Described qualitatively as 'significant gains' on all datasets; specifically, T0 matches/exceeds GPT-3 on 9/11 held-out datasets despite size gap, indicating large relative improvement vs the LM-only baseline (exact absolute percentage deltas per dataset are shown in paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tuning on the P3 prompt pool (many datasets, many prompts), inputs truncated/padded as described; checkpoint selected by validation on training datasets only (true zero-shot on held-out tasks); evaluation uses rank classification for multiple-choice tasks and median/IQR across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Prompted Training Enables Zero-Shot Task Generalization', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>Natural instructions: Benchmarking generalization to new tasks from natural language instructions <em>(Rating: 2)</em></li>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 1)</em></li>
                <li>True few-shot learning with language models <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7412",
    "paper_id": "paper-17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt diversity (p) ablation",
            "name_full": "Effect of number and diversity of prompts per dataset (p) on zero-shot performance",
            "brief_description": "A controlled ablation varying the average number of prompt templates per training dataset (p) showing that increasing prompt count and including diverse/non-original-task prompts raises median zero-shot accuracy and generally reduces variability across held-out prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (based on T5+LM)",
            "model_description": "Encoder–decoder T5 model adapted to LM objective (T5+LM) then fine-tuned on a multitask mixture of prompted datasets; checkpoint-sized variant is 11B parameters (also a 3B variant reported).",
            "model_size": "11B (primary); 3B reported in ablations",
            "task_name": "Zero-shot generalization to held-out tasks (11 datasets across 4 traditional NLP tasks: NLI, coreference, sentence completion, word-sense disambiguation)",
            "task_description": "Models are evaluated zero-shot on held-out datasets formatted via natural-language prompts; accuracy is reported per prompted template and aggregated across prompts.",
            "problem_format": "Natural-language prompt templates (many different templates per dataset); zero-shot (no labeled examples shown in prompt).",
            "format_category": "prompt style",
            "format_details": "Average number of prompts per dataset p was varied: p=0 (no prompt training, baseline T5+LM), p=1 (one original-task prompt per dataset), p=5.7 (all original-task prompts), and p≈8.03 (T0 main model, includes non-original-task prompts). Evaluation reports median and interquartile range (IQR) across all evaluation prompts for each dataset.",
            "performance_metric": "Accuracy (median across prompts; IQR reported)",
            "performance_value": "Increasing p from 0 to 1 yields substantial improvement over the non-prompted baseline; increasing p from 1 to 5.7 raised median for 8/11 held-out datasets and decreased spread for 7/11; moving to p≈8.03 (including non-original-task prompts) further increased median for 9/11 datasets and decreased spread for 8/11 datasets (values reported as dataset counts rather than absolute % in text).",
            "baseline_performance": "T5+LM with p=0 (no prompted multitask fine-tuning) served as baseline (same base model without prompted multitask training).",
            "performance_change": "Reported as counts: median improved for 8/11 (1→5.7) and for 9/11 (5.7→8.03); spread (IQR) reduced for 7/11 (1→5.7) and for 8/11 (5.7→8.03).",
            "experimental_setting": "All models initialized from T5+LM (11B), same hyperparameters, same number of training steps; inputs truncated to 1024 tokens, targets 256; batch size 1024 sequences; evaluation zero-shot on validation splits; median/IQR across prompts reported (no prompt selection).",
            "statistical_significance": null,
            "uuid": "e7412.0",
            "source_info": {
                "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Dataset diversity (d) ablation",
            "name_full": "Effect of number of training datasets (d) on zero-shot prompt robustness",
            "brief_description": "An ablation varying the number of different datasets included in training (d) showing that adding more datasets increases median zero-shot accuracy but does not consistently reduce variability across prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0, T0+, T0++ (T5+LM-based variants)",
            "model_description": "Encoder–decoder T5+LM models fine-tuned on progressively larger multitask mixtures: T0 (d=39), T0+ (d=49 includes GPT-3 eval datasets), T0++ (d=55 adds SuperGLUE excluding RTE/CB).",
            "model_size": "11B",
            "task_name": "Zero-shot generalization to held-out datasets (subset evaluated: 5 held-out datasets shown in figure)",
            "task_description": "Evaluate median and IQR of accuracy across multiple prompts on held-out tasks when increasing number of datasets in the training mixture.",
            "problem_format": "Natural-language prompt templates (all available prompts per dataset used), zero-shot evaluation.",
            "format_category": "prompt style / training mixture composition",
            "format_details": "Fixed p = all available prompts; d increased from 39 (T0) → 49 (T0+) → 55 (T0++). Median accuracy across all 5 evaluated held-out datasets increased with d at both steps (39→49 and 49→55). Spread (IQR) decreased inconsistently: decreased for 1 of 5 datasets when going 39→49 and decreased for 2 of 5 datasets when going 49→55.",
            "performance_metric": "Accuracy (median across prompts; IQR)",
            "performance_value": "Median performance increased for all evaluated held-out datasets as d increased from 39→49→55; spread decreased only for a minority of datasets (1/5 then 2/5).",
            "baseline_performance": "T0 (d=39) baseline for the ablation.",
            "performance_change": "Median increased for all evaluated datasets with each increment in d; spread changes were inconsistent (spread decreased for 1/5 datasets from 39→49, and for 2/5 datasets from 49→55).",
            "experimental_setting": "All other hyperparameters held constant; evaluation uses median and IQR across prompts per dataset; prompts are zero-shot formatted.",
            "statistical_significance": null,
            "uuid": "e7412.1",
            "source_info": {
                "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Instructions vs instruction-less formatting",
            "name_full": "Effect of including explicit task instructions versus using instruction-less sentence-completion formatting",
            "brief_description": "For tasks that can be cast as sentence completion (e.g., HellaSwag, Winogrande), removing explicit instructions and formatting as pure sentence completion can dramatically change accuracy; for HellaSwag this increased median accuracy substantially for T0.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (11B); compared to FLAN (137B) and GPT-3",
            "model_description": "T0: encoder–decoder T5+LM fine-tuned on prompted multitask mixture (11B). FLAN: decoder-only models fine-tuned on prompts (Wei et al., 2021) with much larger parameterization (~137B in the comparison). GPT-3: decoder-only family up to 175B.",
            "model_size": "T0: 11B; FLAN referenced at 137B; GPT-3 referenced up to 175B",
            "task_name": "Commonsense sentence-completion style benchmarks (HellaSwag, Winogrande)",
            "task_description": "Select the most plausible sentence completion among alternatives (multiple-choice / sentence-completion).",
            "problem_format": "Two variants: (A) prompt including explicit task instructions (e.g., 'Choose the most plausible continuation'); (B) instruction-less format used as sentence completion (no instructions, just context and continuations).",
            "format_category": "prompt style / instruction presence",
            "format_details": "Authors re-evaluated HellaSwag and Winogrande with prompts that omit task instructions (i.e., pure sentence completion as in some prior works). For HellaSwag, this change was applied to T0 to match FLAN/Brown et al. evaluation style.",
            "performance_metric": "Accuracy (median across prompts)",
            "performance_value": "HellaSwag: median accuracy increased from 33.65% (with instructions) to 57.93% (instruction-less), matching FLAN's performance. Winogrande: changing to instruction-less prompt had negligible effect for T0 (accuracy = 62.15%).",
            "baseline_performance": "T0 with instruction-present prompts: HellaSwag median 33.65%; Winogrande instruction-present not stated numerically here but instruction removal had little effect.",
            "performance_change": "HellaSwag: +24.28 percentage points absolute (33.65% → 57.93%). Winogrande: ~no substantial change (reported accuracy 62.15% under instruction-less formatting).",
            "experimental_setting": "Zero-shot multiple-choice evaluation using the provided choice options; rank classification/log-likelihood scoring for options used where applicable; median reported across prompts. Comparison performed to match FLAN/Brown formatting.",
            "statistical_significance": null,
            "uuid": "e7412.2",
            "source_info": {
                "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Prompt wording robustness: GPT-3 vs T0 on RTE",
            "name_full": "Robustness of zero-shot performance to prompt wording: comparative RTE experiment between GPT-3 (davinci) and T0 across multiple prompts",
            "brief_description": "Evaluation of GPT-3 (via davinci API) on 10 different RTE prompt templates shows large sensitivity to prompt wording with most prompts yielding near-random performance, whereas T0 exhibited higher robustness across the same prompt set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, API) and T0 (11B)",
            "model_description": "GPT-3 davinci: decoder-only large LM (OpenAI); exact mapping to Brown et al. models is not disclosed but davinci is estimated to correspond to the 175B family. T0: encoder–decoder T5+LM fine-tuned on prompted multitask mixture (11B).",
            "model_size": "GPT-3 (davinci) estimated ~175B; T0 = 11B",
            "task_name": "RTE (Recognizing Textual Entailment)",
            "task_description": "Given a premise and hypothesis, classify whether the hypothesis is entailed, contradicted, or neutral relative to the premise (recast to the dataset's label mapping used by prompts).",
            "problem_format": "Natural-language prompts; 10 different prompt templates used for evaluation (one matches Brown et al.'s reported prompt). Zero-shot (no examples).",
            "format_category": "prompt style / prompt wording",
            "format_details": "Evaluated GPT-3 on the same 10 RTE prompt templates used to evaluate T0. One template matched Brown et al.'s reported prompt; other 9 templates were alternative wordings. Prediction via API with zero-shot prompts; accuracy measured per prompt and median/IQR reported.",
            "performance_metric": "Accuracy (median across 10 prompts; interquartile range)",
            "performance_value": "GPT-3 davinci: one prompt (matching Brown et al.) scored 58.8% accuracy (lower than Brown et al.'s 63.5% reported); the other 9 prompts yielded near-random performance with median = 52.96% and IQR = 1.28%. T0's evaluations on the same prompt set indicate higher robustness (T0's distribution across prompts gave higher medians and larger spread improvements vs GPT-3, though exact per-prompt T0 numbers are in the paper's figures).",
            "baseline_performance": "Brown et al.'s single-prompt GPT-3 numbers (e.g., 63.5% for RTE) served as prior reported baseline but do not capture prompt variability.",
            "performance_change": "Compared to Brown et al.'s single-prompt report (63.5%), GPT-3 davinci scored 58.8% on the matching prompt in this re-eval (−4.7 pp); across other prompts performance dropped to near-random (median 52.96%). Relative to T0, GPT-3's median across prompts was substantially lower and had smaller IQR, indicating less robustness to prompt wording.",
            "experimental_setting": "Zero-shot evaluation via OpenAI API (davinci), same 10 prompt templates as used for T0 evaluation; no prompt selection; rank/log-likelihood scoring for multiple-choice where applicable.",
            "statistical_significance": null,
            "uuid": "e7412.3",
            "source_info": {
                "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Multitask prompted training vs LM-only baseline",
            "name_full": "Impact of supervised multitask prompted fine-tuning (T0) vs language-model-only pretraining (T5+LM) on zero-shot task generalization",
            "brief_description": "Direct comparison showing that fine-tuning a pretrained T5+LM model on a large mixture of natural-language prompts (T0) yields significant gains in zero-shot accuracy across held-out tasks compared to the identical architecture without prompted fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T5+LM fine-tuned) vs T5+LM (baseline)",
            "model_description": "Both are encoder–decoder T5+LM models; baseline T5+LM is pretrained and LM-adapted but not fine-tuned on the multitask prompt mixture. T0 is the same model architecture fine-tuned on the multitask prompted mixture.",
            "model_size": "11B",
            "task_name": "Held-out task zero-shot generalization (11 datasets across 4 tasks; also subset of BIG-bench tasks)",
            "task_description": "Zero-shot evaluation where models see prompted inputs (natural-language templates) for tasks/datasets not included in the fine-tuning mixture.",
            "problem_format": "Natural-language prompts (various templates per dataset), zero-shot; for multiple-choice tasks, rank classification by log-likelihood over options is used.",
            "format_category": "prompt style / training regimen",
            "format_details": "T0 trained on a shuffled mixture of prompted datasets with many templates per dataset; evaluation reports median and IQR across prompts for each held-out dataset. The baseline T5+LM uses the same prompts at evaluation but was not trained on the prompted multitask mixture.",
            "performance_metric": "Accuracy (per dataset, per prompt; aggregated as median and IQR across prompts)",
            "performance_value": "T0 yields significant gains over the T5+LM baseline on all evaluated held-out datasets; T0 matches or exceeds GPT-3 performance on 9/11 held-out datasets despite being ~16× smaller than GPT-3 (T0 11B vs GPT-3 up to 175B). On BIG-bench subset T0 variants outperform baseline decoder-only LMs on nearly all tasks.",
            "baseline_performance": "T5+LM (same architecture without multitask prompted fine-tuning) performance (lower accuracy across held-out prompts/datasets).",
            "performance_change": "Described qualitatively as 'significant gains' on all datasets; specifically, T0 matches/exceeds GPT-3 on 9/11 held-out datasets despite size gap, indicating large relative improvement vs the LM-only baseline (exact absolute percentage deltas per dataset are shown in paper figures/tables).",
            "experimental_setting": "Fine-tuning on the P3 prompt pool (many datasets, many prompts), inputs truncated/padded as described; checkpoint selected by validation on training datasets only (true zero-shot on held-out tasks); evaluation uses rank classification for multiple-choice tasks and median/IQR across prompts.",
            "statistical_significance": null,
            "uuid": "e7412.4",
            "source_info": {
                "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2,
            "sanitized_title": "finetuned_language_models_are_zeroshot_learners"
        },
        {
            "paper_title": "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
            "rating": 2,
            "sanitized_title": "natural_instructions_benchmarking_generalization_to_new_tasks_from_natural_language_instructions"
        },
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 1,
            "sanitized_title": "do_promptbased_models_really_understand_the_meaning_of_their_prompts"
        },
        {
            "paper_title": "True few-shot learning with language models",
            "rating": 1,
            "sanitized_title": "true_fewshot_learning_with_language_models"
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        }
    ],
    "cost": 0.015934,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multitask Prompted Training Enables Zero-Shot Task Generalization</h1>
<p>Victor Sanh ${ }^{<em>}$<br>Hugging Face<br>Albert Webson</em><br>Brown University<br>Colin Raffel ${ }^{<em>}$<br>Hugging Face<br>Stephen H. Bach</em><br>Brown \&amp; Snorkel AI</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Lintang Sutawika BigScience</th>
<th style="text-align: center;">Zaid Alyafeai</th>
<th style="text-align: center;">Antoine Chaffin</th>
<th style="text-align: center;">Arnaud Stiegler</th>
<th style="text-align: center;">Teven Le Scao</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">KFUPM</td>
<td style="text-align: center;">IRISA \&amp; IMATAG</td>
<td style="text-align: center;">Hyperscience</td>
<td style="text-align: center;">Hugging Face</td>
</tr>
<tr>
<td style="text-align: center;">Arun Raja</td>
<td style="text-align: center;">Manan Dey</td>
<td style="text-align: center;">M Saiful Bari</td>
<td style="text-align: center;">Canwen Xu</td>
<td style="text-align: center;">Urmish Thakker</td>
</tr>
<tr>
<td style="text-align: center;">$I^{2} R$, Singapore</td>
<td style="text-align: center;">SAP</td>
<td style="text-align: center;">NTU, Singapore</td>
<td style="text-align: center;">UCSD \&amp; Hugging Face</td>
<td style="text-align: center;">SambaNova Systems</td>
</tr>
<tr>
<td style="text-align: center;">Shanya Sharma</td>
<td style="text-align: center;">Eliza Szczechla</td>
<td style="text-align: center;">Taewoon Kim</td>
<td style="text-align: center;">Gunjan Chhablani</td>
<td style="text-align: center;">Nihal V. Nayak</td>
</tr>
<tr>
<td style="text-align: center;">Walmart Labs</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">VU Amsterdam</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">Brown University</td>
</tr>
<tr>
<td style="text-align: center;">Debajyoti Datta</td>
<td style="text-align: center;">Jonathan Chang</td>
<td style="text-align: center;">Mike Tian-Jian Jiang</td>
<td style="text-align: center;">Han Wang</td>
<td style="text-align: center;">Matteo Manica</td>
</tr>
<tr>
<td style="text-align: center;">University of Virginia</td>
<td style="text-align: center;">ASUS</td>
<td style="text-align: center;">ZEALS, Japan</td>
<td style="text-align: center;">NYU</td>
<td style="text-align: center;">IBM Research</td>
</tr>
<tr>
<td style="text-align: center;">Sheng Shen</td>
<td style="text-align: center;">Zheng-Xin Yong</td>
<td style="text-align: center;">Harshit Pandey</td>
<td style="text-align: center;">Michael McKenna</td>
<td style="text-align: center;">Rachel Bawden</td>
</tr>
<tr>
<td style="text-align: center;">UC Berkeley</td>
<td style="text-align: center;">Brown University</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">Parity</td>
<td style="text-align: center;">Inria, France</td>
</tr>
<tr>
<td style="text-align: center;">Thomas Wang</td>
<td style="text-align: center;">Trishala Neeraj</td>
<td style="text-align: center;">Jos Rozen</td>
<td style="text-align: center;">Abheesht Sharma</td>
<td style="text-align: center;">Andrea Santilli</td>
</tr>
<tr>
<td style="text-align: center;">Inria, France</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">Naver Labs Europe</td>
<td style="text-align: center;">BITS Pilani, India</td>
<td style="text-align: center;">University of Rome</td>
</tr>
<tr>
<td style="text-align: center;">Thibault Fevry</td>
<td style="text-align: center;">Jason Alan Fries</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ryan Teehan</td>
<td style="text-align: center;">Tali Bers</td>
</tr>
<tr>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">Stanford \&amp; Snorkel AI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Charles River Analytics</td>
<td style="text-align: center;">Brown University</td>
</tr>
<tr>
<td style="text-align: center;">Stella Biderman</td>
<td style="text-align: center;">Leo Gao</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Thomas Wolf</td>
<td style="text-align: center;">Alexander M. Rush</td>
</tr>
<tr>
<td style="text-align: center;">Booz Allen \&amp; EleutherAI</td>
<td style="text-align: center;">EleutherAI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hugging Face</td>
<td style="text-align: center;">Hugging Face</td>
</tr>
</tbody>
</table>
<h2>ABSTRACT</h2>
<p>Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to $16 \times$ its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to $6 \times$ its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent work has shown that large language models exhibit the ability to perform reasonable zeroshot generalization to new tasks (Brown et al., 2020; Kim et al., 2021). Despite being trained on only language modeling objectives, these models can perform relatively well at new tasks that they have not been explicitly trained to perform, for instance answering a question on a passage or performing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our model and prompt format. T0 is an encoder-decoder model that consumes textual inputs and produces target responses. It is trained on a multitask mixture of NLP datasets partitioned into different tasks. Each dataset is associated with multiple prompt templates that are used to format example instances to input and target pairs. Italics indicate the inserted fields from the raw example data. After training on a diverse mixture of tasks (top), our model is evaluated on zero-shot generalization to tasks that are not seen during training (bottom).
summarization. An influential hypothesis is that large language models generalize to new tasks as a result of an implicit process of multitask learning (Radford et al., 2019). As a byproduct of learning to predict the next word, a language model is forced to learn from a mixture of implicit tasks included in their pretraining corpus. For example, by training on generic text from a web forum, a model might implicitly learn the format and structure of question answering. This gives large language models the ability to generalize to held-out tasks presented with natural language prompts, going beyond prior multitask studies on generalization to held-out datasets (Khashabi et al., 2020a; Ye et al., 2021). However, this ability requires a sufficiently large model and is sensitive to the wording of its prompts (Perez et al., 2021; Zhao et al., 2021; Reynolds and McDonell, 2021).
Further, it is an open question how implicit this multitask learning really is. Given the scale of recent language models' pretraining corpora, it is reasonable to expect that some common natural language processing (NLP) tasks would appear in an explicit form in their pretraining corpora, thereby directly training the models on those tasks. For example, there are many websites that simply contain lists of trivia questions and answers, ${ }^{1}$ which are precisely supervised training data for the task of closedbook question answering (Roberts et al., 2020). We hypothesize that such multitask supervision in pretraining plays a large role in zero-shot generalization.
In this paper, we focus on explicitly training language models in a supervised and massively multitask fashion. Our approach uses a training mixture consisting of a large set of different tasks specified in natural language prompts. Our goal is to induce a model to better generalize to held-out tasks without requiring massive scale, as well as being more robust to the wording choices of the prompts. To convert a large set of natural language tasks into prompted form, we use a simple templating language for structured datasets. We develop an interface for prompt collection from public contributors that facilitated the collection of a large multitask mixture with multiple prompts per dataset (Bach et al., 2022). We then train a variant of the T5 encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on a subset of the tasks (each with multiple datasets) and then evaluate tasks and prompts that the model was not trained on.
Our experiments study two questions. First, does multitask prompted training improve generalization to held-out tasks? Second, does training on a wider range of prompts improve robustness to prompt wording? For the first question, we find that multitask training enables zero-shot task gen-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>eralization by showing that our model matches or exceeds the performance of GPT-3 (Brown et al., 2020) on 9 out of 11 held-out datasets, despite being about $16 \times$ smaller. We also show that the model improves over a large baseline language model on 13 out of 14 tasks in the BIG-bench benchmark (BIG-bench collaboration, 2021). For the second question, we find that training on more prompts per dataset consistently improves the median and decreases the variability of performance on held-out tasks. Training on prompts from a wider range of datasets also generally improves the median but does not consistently decrease the variability.</p>
<h1>2 Related Work</h1>
<p>In this work, we distinguish implicit multitask learning in language model pretraining from explicit multitask learning (Caruana, 1997), the technique for mixing multiple tasks into a single supervised training process. Models trained with multitask learning have long been shown to have improved performance in NLP (Collobert and Weston, 2008). Since different tasks have different outputs, applying multitask learning requires a shared format, and various have been used (Hashimoto et al., 2016; McCann et al., 2018). Several multitask works also explore few-shot and zero-shot generalization to new datasets with large pretrained models (e.g., Vu et al., 2020; Ye et al., 2021).</p>
<p>Natural language prompting is the method of reformatting NLP tasks in the format of a natural language response to natural language input. The development of text-to-text pretrained models such as T5 (Raffel et al., 2020) makes prompts a particularly useful method for multitask learning. For example, Khashabi et al. (2020a) reformat 20 question-answering datasets into a single prompt of question: ... (A) ... (B) ... (C) ... context: ..., while later work such as Zhong et al. (2021) and Wang et al. (2021) cast a range of datasets into a single boolean QA prompt or a single NLI prompt, respectively. Although effective, these single-prompt methods typically do not generalize to new prompts or new tasks inexpressible in their fixed format.</p>
<p>More generally, Schick and Schütze (2021) and Brown et al. (2020) popularized using prompts as a generic method for all NLP tasks. Mishra et al. (2021) further extend this approach to a multitask setup, training on prompts for 61 narrowly defined tasks (e.g., question generation, incorrect answer generation) adapted from 9 datasets' crowdsourcing instructions, whereas we train on and measure generalization across 62 datasets and 12 tasks as traditionally defined in the NLP literature (§3). Additionally, their prompts include labeled examples in addition to instructions, whereas we focus on zero-shot generalization. Lastly, concurrent work by Wei et al. (2021) shares a similar research question with us, although we differ in several substantive regards, e.g., prompt diversity, model scale, and held-out-task scheme. We discuss our differences in detail in Section 7.</p>
<p>Finally, in explaining the success of prompts, the leading hypothesis is that models learn to understand the prompts as task instructions which help them generalize to held-out tasks (Wei et al., 2021; Mishra et al., 2021; Schick and Schütze, 2021; Brown et al., 2020). However, the extent to which this success depends on the semantic meaningfulness of the prompts has been challenged (Webson and Pavlick, 2021; Logan et al., 2021). Thus, in this work, we remain agnostic as to why prompts support generalization. We only claim that prompts serve as a natural format for multitask training which empirically supports generalization to held-out tasks.</p>
<h2>3 Measuring Generalization to Held-Out Tasks</h2>
<p>We begin by assuming an underlying partition of NLP datasets into tasks. We use the term "task" to refer to a general NLP ability that is tested by a group of specific datasets. To evaluate zero-shot generalization to new tasks, we train on a subset of tasks and evaluate on a held-out group of tasks.</p>
<p>Unfortunately, NLP task categorization is fuzzy, particularly if one tries to isolate a unique skill. For example, many datasets evaluate commonsense knowledge, and some multitask works (e.g., Brown et al., 2020; Wei et al., 2021) define commonsense as a standalone task. However, commonsense datasets differ vastly, ranging from innate knowledge and grade-school science to DIY instructions, US cultural norms, and graduate-level theorems (see Appendix D. 1 for a detailed discussion).</p>
<p>Noting that grouping by task is an imperfect heuristic, we err on the side of organizing our task taxonomy according to the task format as opposed to required skill based on conventions in the literature (Khashabi et al., 2020b; Vu et al., 2020; Ye et al., 2021). We collect all datasets from</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: T0 datasets and task taxonomy. (T0+ and T0++ are trained on additional datasets. See Table 5 for the full list.) Color represents the level of supervision. Yellow datasets are in the training mixture. Green datasets are held out and represent tasks that were not seen during training. Hotpot QA is recast as closed-book QA due to long input length.
these papers and exclude those that are not in English (which also excludes programming languages and structured annotations such as parse trees) or if they require special domain knowledge (e.g., biomedicine). This yields 12 tasks and 62 datasets with publicly contributed prompts in our training and evaluation mixtures (Figure 2) as of writing. All experiments use datasets in the Hugging Face datasets library (Lhoest et al., 2021).</p>
<p>To test zero-shot generalization, we hold out all constituent datasets of four tasks: natural language inference (NLI), coreference resolution, sentence completion, and word sense disambiguation. We choose NLI as a held-out task because humans also zero-shot generalize to NLI as an held-out task: Most humans are never explicitly trained to classify whether a premise sentence entails or contradicts a hypothesis sentence, yet they find it intuitive to perform this task without training (Williams et al., 2020). For the same reason, we also hold out coreference resolution and word sense disambiguation. We further hold out sentence completion because it is a task possibly too similar to NLI (Appendix D. 2 discusses this in detail). Additionally, we do not train our main model on any datasets that Brown et al. (2020) used for evaluation, so that our main results will be a fair zero-shot comparison. We also verify that data for those tasks is not leaked through the pretraining corpus (Appendix E).</p>
<p>Lastly, we further evaluate on a subset of the datasets from BIG-bench, which is a recent communitydriven benchmark to create a diverse collection of difficult tasks to test the abilities of large language models. The subset of BIG-bench comprise a language-oriented selection of tasks for which the BIGbench maintainers have prepared preliminary results and which constitute text that is in-vocabulary for the T5 tokenizer (i.e. only contain English-language text without emojis or other special characters). All tasks from BIG-bench are novel tasks that are held out from our training.</p>
<h1>4 A Unified Prompt Format</h1>
<p>All datasets are given to our model in natural language prompted form to enable zero-shot experimentation. To facilitate writing a large collection of prompts, we develop a templating language and an application that make it easy to convert diverse datasets into prompts. We define a prompt as consisting of an input template and a target template, along with a collection of associated metadata. The templates are functions mapping a data example into natural language for the input and target sequences. Practically, the templates allow the user to mix arbitrary text with the data fields,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Prompt templates from the P3 prompt collection. Each dataset has multiple prompt templates consisting of an input and a target template. These use the fields of the raw data examples as well as template metadata, e.g., the left paraphrasing identification prompts use Choices, a templatelevel list variable ['Not duplicates', 'Duplicates']. These templates are materialized to produce the prompted instance shown in Figure 1. The complete set of prompt templates used in T0 is given in Appendix G.
metadata, and other code for rendering and formatting raw fields. For example, in the case of an NLI dataset, the example would include fields for Premise, Hypothesis, Label. An input template would be If {Premise} is true, is it also true that {Hypothesis}?, whereas a target template can be defined with the label choices {Choices[label]}. Here Choices is prompt-specific metadata that consists of the options yes, maybe, no corresponding to label being entailment ( 0 ), neutral (1) or contradiction (2). Other metadata documents additional properties, such as an evaluation metric. Each data example is materialized with many different prompt templates as shown in Figure 3.</p>
<p>To develop prompts, we built an interface for interactively writing prompts on datasets. We put out an open call in the research community for users to contribute prompts. 36 contributors affiliated with 24 institutions in 8 countries participated. Since our goal was to train a model to be robust to prompt format, and since the question of what makes a prompt effective remains unresolved (Webson and Pavlick, 2021; Logan et al., 2021; Zhao et al., 2021), we encouraged contributors to be open in their style and create a diverse set of prompts. The main annotation guideline was that prompts needed to be grammatical and understandable by a fluent English speaker with no prior experience of the tasks. Additionally, prompts that required explicit counting or numerical indexing were removed in favor of natural language variants. For example, instead of predicting indices of a span extracting answers from a passage, the model is expected to copy the span's text instead. With these minimal constraints, prompt writers were encouraged to use both formal and creative prompts and various orderings of the data.</p>
<p>Most of the prompts correspond directly to a version of the original proposed task, although we also allow prompts that permuted the original task (for instance, generating a document from its summary). Such non-original-task prompts are included in our training mixtures for improved diversity, but they are not reported in evaluation since they deviate from the metrics and baselines reported by the original datasets.</p>
<p>The details of the prompting language and tool are given in Appendix C and Bach et al. (2022), and the prompts themselves are given in Appendix G. We collected prompts for English datasets, excluding ones that included potentially harmful content or non-natural language such as programming languages. We refer to this collection as the Public Pool of Prompts (P3). As of writing, P3 contains 2073 prompts for 177 datasets ( 11.7 prompts per dataset on average). Prompts used in experiments are all sourced from P3 except for BIG-bench, the prompts of which are provided by its maintainers.</p>
<h1>5 EXPERIMENTAL SETUP</h1>
<p>Model At a high level, we fine-tune a pretrained model on our multi-task training mixture of natural language prompted datasets. Our model uses an encoder-decoder architecture with input text fed to the encoder and target text produced by the decoder. The model is trained to autoregressively</p>
<p>generate the target through standard maximum likelihood training. Unlike decoder-only language models such as GPT-3, it is never trained to generate the input.</p>
<p>All models we trained are based on T5, a Transformer-based encoder-decoder language model pretrained with a masked language modeling-style objective on 1T tokens from C4 (Raffel et al., 2020). Since T5's pretraining objective is generating tokens and only tokens that have been removed from the input text, it is different from the natural text generation format of prompted datasets. Therefore, we use Lester et al. (2021)'s LM-adapted T5 model (referred to as T5+LM), produced by training T5 on 100B additional tokens from C4 on a standard language modeling objective.</p>
<p>Training Our main model, T0, is trained on the multitask mixture detailed in Section 3 and Table 5. Meanwhile, T0+ is the same model with identical hyperparameters except trained on a mixture that adds GPT-3's evaluation datasets. Lastly, T0++ further adds SuperGLUE (Wang et al., 2019a) to the training mixture (except RTE and CB), which leaves NLI and the BIG-bench tasks as the only held-out tasks.</p>
<p>The above T0 variants are all initialized from the 11B parameters version of T5+LM. To study the effect of scaling and to aid researchers with less resources, we also train $T 0(3 B)$, which has the same training mixture as T0 but is initialized from the 3B parameters version of T5+LM (results reported in Appendix F).</p>
<p>We perform checkpoint selection by choosing the checkpoint that yields the highest score on the validation splits of our training datasets. This still satisfies the true zero-shot (Perez et al., 2021) setting as we do not use any examples from any of the held-out tasks to select the best checkpoint.</p>
<p>We assemble our multitask training mixture by combining and shuffling all examples from all training datasets. This is equivalent to sampling from each dataset in proportion to the number of examples in the dataset. However, the number of examples in each of our training datasets varies by two orders of magnitude. We therefore follow the strategy used in Raffel et al. (2020) and treat any dataset with over 500'000 examples as having 500'000 / num_templates examples for the purposes of sampling, where num_templates is the number of templates created for the dataset.</p>
<p>We truncate input and target sequences to 1024 and 256 tokens, respectively. Following Raffel et al. (2020), we use packing to combine multiple training examples into a single sequence to reach the maximum sequence length. We use a batch size of 1024 sequences (corresponding to $2^{20}$ total input tokens per batch) and the Adafactor optimizer (Shazeer and Stern, 2018). Following standard practice for fine-tuning T5, we use a learning rate of $1 \mathrm{e}-3$ and a dropout rate of 0.1 .</p>
<p>Evaluation We evaluate zero-shot generalization on 11 datasets in 4 held-out traditional NLP tasks: natural language inference, coreference, word sense disambiguation, and sentence completion, as well as 14 novel tasks from BIG-bench (§3). Unless specified otherwise, we report performance on the validation splits. All reported datasets use accuracy as their metric.</p>
<p>For tasks that involve choosing the correct completion from several options (e.g. multiple choice question answering), we follow Brown et al. (2020) and use rank classification to evaluate our model: we compute the log-likelihood of each of the target options under the fine-tuned model and select the option with the highest log-likelihood as the prediction. For simplicity, we do not apply length normalization to the log-likelihoods of the target options.</p>
<p>We do not perform prompt selection by comparing the performance of different prompts on the validation split; Perez et al. (2021) highlights how such a strategy leaks information from the evaluation splits, which makes the evaluation not "true" zero-shot. For a given dataset, we report the median performance across all prompts for this dataset along with their interquartile range (Q3 - Q1) to measure the model's robustness to the wording of the prompts.</p>
<h1>6 ReSULtS</h1>
<h3>6.1 Generalization to Held-Out Tasks</h3>
<p>Our first research question is whether multitask prompted training improves generalization to heldout tasks. In Figure 4, we compare T0 against our T5+LM baseline on four held-out tasks. Our</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results for T0 task generalization experiments compared to GPT-3 (Brown et al., 2020). Each dot is the performance of one evaluation prompt. The baseline T5+LM model is the same as T0 except without multitask prompted training. GPT-3 only reports a single prompt for each dataset.
approach leads to significant gains over our baseline on all datasets, demonstrating the benefits of multitask prompted training over only language modeling training with an identical model and prompts.</p>
<p>Next, we compare T0 to the zero-shot performance of the largest language models available as of writing, i.e., various GPT-3 models up to 175B parameters. Note that Brown et al. (2020) report performance on a single prompt, ${ }^{2}$ whereas we report the median and interquartile range of performance across all prompts in P3 without cherry picking. We find that T0 matches or exceeds the performance of all GPT-3 models on 9 out of 11 held-out datasets. Notably, neither T0 nor GPT-3 is trained on natural language inference, yet T0 outperforms GPT-3 on all NLI datasets, even though our T5+LM baseline does not. The same is true for most datasets of other held-out tasks. The two exceptions are Winogrande and HellaSwag, which we discuss in Section 7.</p>
<p>To evaluate our models on more held-out tasks, we assess the zero-shot performance of T0, T0+, and T0++ on a subset of BIG-bench (BIG-bench collaboration, 2021). Tasks from BIG-bench cover a variety of novel skills not included in our training tasks, such as deducing the order of a sequence of objects, solving logic grid puzzles, and telling apart true statements from common misconceptions. The maintainers of BIG-bench provide a prompt for each dataset, with which we compare our models to a series of preliminary diagnostic baseline models trained by Google and evaluated by the BIG-bench maintainers. These models are decoder-only Transformer language models trained on a standard language modeling objective with varying model size. We find that at least one of the T0 variants outperform all baseline models on all tasks except for StrategyQA (Figure 5). In most cases, the performance of our models improves as the number of training datasets increases (i.e., T0++ outperforms T0+ which outperforms T0).</p>
<h1>6.2 Prompt Robustness</h1>
<p>Our second research question is whether training on a wider range of prompts improves robustness to the wording of the prompts. We conduct two ablation experiments on the effects of the average number of prompts per dataset $(p)$ and the number of datasets $(d)$ used during training.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results for a subset of BIG-bench which has available baselines. The baseline models are Transformer-based language models provided by BIG-bench maintainers, who also provide one prompt per dataset. T0, T0+ and T0++ are identical except for increasing the number of training datasets (§5). BIG-bench Tasks are all zero-shot for all the reported models.</p>
<p>Effect of More Prompts per Dataset In this analysis, we fix $d$ and compare T0 to models with a varying number of prompts per dataset. T0 was trained on some prompts that do not map onto the dataset's original task, for example "given an answer, generate a plausible question". Including these prompts results in $p$ being 8.03 on average (which corresponds to our main T0 model). We compare T0 to models where $p=1$ (one randomly chosen original-task prompt per dataset), $p=5.7$ on average (all original-tasks prompts for all datasets), and $p=0$ (corresponding to T5+LM without any prompted training). We train all models with the same hyperparameters and the same number of steps. Figure 6 shows that, even with just one prompt per dataset, performance on held-out tasks can improve substantially over the non-prompted baseline, although the spread (interquartile range between Q1 and Q3) does not consistently improve with $p=1$. Meanwhile, further increasing $p$ from 1 to an average of 5.7 does yield additional improvement in both median (increases for 8/11 datasets) and spread (decreases for 7/11 datasets). This reinforces our hypothesis that training on more prompts per dataset leads to better and more robust generalization to held-out tasks. Finally, we find that T0's inclusion all prompts (including those that do not correspond to the dataset's original task) further improves the median (increases for 9/11 datasets) and spread (decreases for 8/11 datasets), showing that training on non-original-task prompts can also be beneficial.</p>
<p>Effect of Prompts from More Datasets In this experiment, we fix $p=$ all available prompts and increase $d$ from 39 to 49 to 55 (T0, T0+, T0++, respectively. See Section 5 for details.) Figure 7 shows that the median performance of all 5 held-out datasets increases as $d$ increases from 39 to 49. However, the spread only decreases for 1 out of 5 datasets. For some datasets (e.g., ANLI), this is an artifact of the fact that some prompts always perform poorly, so that when other prompts improve, the spread is stretched larger. For other datasets (e.g., CB), however, the spread does decrease with T0+. As $d$ increases from 49 to 55, the median performance of all datasets again increases, but the spread only decreases for 2 out of 5 datasets. Although further investigation is needed, it appears that increasing $d$ does not consistently make the model more robust to the wording of prompts.</p>
<p>Comparing T0 and GPT-3's robustness Because Brown et al. (2020) only report one prompt per dataset with no standard deviation, we evaluate GPT-3 via OpenAI's API ${ }^{3}$ on RTE using the same 10 prompts we evaluate T0 in order to estimate GPT-3 robustness' to different wording of prompts. One of these templates is identical to Brown et al. (2020, p. 59)'s reported prompt, which scores an accuracy of 58.8\%, lower than the 63.5\% reported in Brown et al. (2020). All other 9</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Effect of more prompts per dataset. Zero-shot performance of T0 and T5+LM when increasing number of training prompts per dataset. Each dot is the performance of one evaluation prompt. The main T0 model ( $p=8.03$ ) includes non-original-task prompts (see Section 3). Adding more training prompts consistently leads to higher median performance and generally lower interquartile range for held-out tasks.
prompts, however, yield roughly random-guessing performance with median accuracy $=52.96 \%$ and interquartile range $=1.28 \%$. These results suggest that T0 could be more robust to prompt formulation than GPT-3.</p>
<h1>7 DISCUSSION</h1>
<p>Concurrent to our work, Wei et al. (2021) proposes FLAN, which shares largely the same method of enabling zero-shot generalization through multitask prompted training. With a mixture of datasets similar to ours, they train multiple decoder-only language models, each with a single held-out task (cf. we focus on training one model with multiple held-out tasks in order to evaluate the model's ability to generalize to diverse tasks.) Compared to FLAN, T0's zero-shot performance is better on CB and RTE, similar on Story Cloze and COPA, and worse on Winogrande and ANLI. T0++ outperforms FLAN on CB, RTE, and COPA and matches FLAN's performance on Winogrande and ANLI. Notably, T0 and T0++ attain this performance despite being over $10 \times$ smaller than FLAN (137B vs. 11B parameters).</p>
<p>Both T0 and FLAN underperform GPT-3 on Winogrande and HellaSwag (Sakaguchi et al., 2019; Zellers et al., 2019), for which Wei et al. (2021) conjecture that for tasks such as coreference resolution that can be formatted as finishing an incomplete sentence, adding task instructions to prompts is "largely redundant". Following this conjecture, we reevaluate these two datasets without instructions as done by Wei et al. (2021) and Brown et al. (2020) and find that it improves performance on HellaSwag from a median of $33.65 \%$ to $57.93 \%$, matching the performance of FLAN. For Winogrande, however, using FLAN's prompt without instructions does not make a substantial difference (accuracy $=62.15 \%$ ).</p>
<p>Surprisingly, Wei et al. (2021) perform an ablation with a model of comparable size (8B parameters) to T0 (11B parameters) and find that that performance on held-out tasks decreases after multitask prompted training, whereas we find that multitask prompted training improves the performance of models at least as small as 3B parameters (Figure 8). We identify two key differences between the models that could explain this discrepancy: First, we use an encoder-decoder model that was pretrained with a different objective (masked language modeling) before being trained as a standard</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Effect of prompts from more datasets. Zero-shot performance of three models with varying number of datasets (T0, T0+, T0++). Adding more datasets consistently leads to higher median performance but does not always reduce interquartile range for held-out tasks.
language model and finally fine-tuned on the multitask mixture. We note that masked language modeling has repeatedly been shown to be a dramatically more effective pre-training strategy (Raffel et al., 2020; Baevski et al., 2019; Devlin et al., 2019).
Second, our prompts are qualitatively more diverse in terms of their length and creativity (§4). For example, consider one of our prompts for Quora Question Pairs (paraphrasing identification): I'm an administrator on the website Quora. There are two posts, one that asks "question1" and another that asks "question2". I can merge questions if they are asking the same thing. Can I merge these two questions? We hypothesize that this diversity could have concrete effects. For example, it could explain why Wei et al. (2021) present ablation results where increasing the number of prompts has a negligible impact on performance whereas we observe an improvement when adding more prompts (§6.2). We leave a full investigation on the impact of these differences to future work.</p>
<h1>8 CONCLUSION</h1>
<p>We demonstrate that multitask prompted training can enable strong zero-shot generalization abilities in language models. This approach provides an effective alternative to unsupervised language model pretraining, often enabling our T0 model to outperform models many times its size. We also perform ablation studies demonstrating the importance of including many diverse prompts and the impact of increasing the number of datasets in each task. To enable future work on improving zero-shot generalization, we release all models trained in this paper in addition to the collection of prompts we created and our prompt annotation tool.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>This work was granted access to the HPC resources of Institut du développement et des ressources en informatique scientifique (IDRIS) du Centre national de la recherche scientifique (CNRS) under the allocation 2021-A0101012475 made by Grand équipement national de calcul intensif (GENCI). In particular, all the evaluations and data processing ran on the Jean-Zay cluster of IDRIS, and we want to thank the IDRIS team for responsive support throughout the project, in particular Rémi Lacroix. We are grateful for the TPU Research Cloud program which generously provided TPU credits to Hugging Face. Those credits were used to train all the models from this paper.
This work was partly funded by Rachel Bawden and Benoît Sagot's chairs in the PRAIRIE institute funded by the French national agency ANR as part of the "Investissements d'avenir" programme under the reference ANR-19-P3IA-0001. Disclosure: Stephen Bach contributed to this work as an advisor to Snorkel AI.</p>
<p>We thank Yacine Jernite, Sasha Luccioni, Aurélie Névéol and Huu Nguyen for advising on strategies to deal with datasets containing potentially harmful content. Guy Gur-Ari and Ethan Dyer provided</p>
<p>assistance and preliminary results on BIG-bench evaluation. We thank Ruiqi Zhong for early discussions on this project.</p>
<h1>REFERENCES</h1>
<p>Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts, 2022.</p>
<p>Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019.</p>
<p>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice, 2006.</p>
<p>Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662-678, 2020. doi: 10.1162/tacl_a_00338. URL https://doi.org/10.1162/tacl_a_00338.</p>
<p>Qiang Ning Ben Zhou, Daniel Khashabi and Dan Roth. "going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding. In EMNLP, 2019.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge. In TAC, 2009.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1160.</p>
<p>BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. In preparation, 2021. URL https://github.com/google/BIG-bench/.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel</p>
<p>Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Rich Caruana. Multitask learning. Mach. Learn., 28(1):41-75, 1997. doi: 10.1023/A: 1007379606734. URL https://doi.org/10.1023/A:1007379606734.</p>
<p>Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174-2184, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1241. URL https://aclanthology.org/D18-1241.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. CoRR, abs/1905.10044, 2019. URL http://arxiv.org/abs/1905.10044.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.</p>
<p>Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Filnand, June 5-9, 2008, volume 307 of ACM International Conference Proceeding Series, pages 160-167. ACM, 2008. doi: 10.1145/1390156.1390177. URL https: //doi.org/10.1145/1390156.1390177.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer, 2005.</p>
<p>Pradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, and Matt Gardner. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. arXiv:1908.05803v2, 2019 .</p>
<p>Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and Montse Cuadros. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11-20, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5102. URL https://www.aclweb.org/anthology/W18-5102.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.</p>
<p>William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.</p>
<p>Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a largescale multi-document summarization dataset and abstractive hierarchical model, 2019.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1-9. Association for Computational Linguistics, 2007.</p>
<p>Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019 .</p>
<p>Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N project report, Stanford, 1(12):2009, 2009.</p>
<p>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword. Linguistic Data Consortium, Philadelphia, 4(1):34, 2003.</p>
<p>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple NLP tasks. CoRR, abs/1611.015collin87, 2016. URL http://arxiv.org/abs/1611.01587.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, pages 1693-1701, 2015.</p>
<p>Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. URL https://aclanthology.org/H01-1069.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In arXiv:1909.00277v2, 2019.</p>
<p>Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. arXiv:1707.06209v1, 2017.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface:a challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018.</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. CoRR, abs/2005.00700, 2020a. URL https://arxiv.org/abs/2005.00700.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.171.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: A dataset for question answering via sentence composition. arXiv:1910.11473v2, 2020.</p>
<p>Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, et al. What changes can largescale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. arXiv preprint arXiv:2109.04650, 2021.</p>
<p>Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. Generating text from structured data with application to the biography domain. CoRR, abs/1603.07771, 2016. URL http://arxiv.org/abs/1603. 07771 .</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.</p>
<p>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S"oren Auer, et al. Dbpedia-a largescale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167-195, 2015.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, and Thomas Wolf. Datasets: A community library for natural language processing. emnlp, 2021.</p>
<p>Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. URL https://aclanthology.org/C02-1150.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020.findings-emnlp.165.</p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. Reasoning over paragraph effects in situations. In MRQA@EMNLP, 2019.</p>
<p>Robert L Logan, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters: Simple few-shot learning with language models. arXiv preprint arXiv:2106.13353, 2021.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.</p>
<p>Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages $165-172,2013$.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730, 2018. URL http: //arxiv.org/abs/1806.08730.
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. CoRR, abs/1902.01007, 2019. URL http://arxiv.org/abs/ 1902.01007.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. CoRR, abs/2104.08773, 2021. URL https://arxiv.org/abs/2104.08773.</p>
<p>Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Online, November 2020. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745, 2018.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.</p>
<p>Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 2005.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, 2016.</p>
<p>David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.</p>
<p>Ellie Pavlick and Tom Kwiatkowski. Inherent disagreements in human textual inferences. Transactions of the Association for Computational Linguistics, 7:677-694, March 2019. doi: 10.1162/ tacl_a_00293. URL https://aclanthology.org/Q19-1043.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. CoRR, abs/2105.11447, 2021. URL https://arxiv.org/abs/2105.11447.</p>
<p>Mohammad Taher Pilehvar and os'e Camacho-Collados. Wic: 10, 000 example pairs for evaluating context-sensitive representations. CoRR, abs/1808.09121, 2018. URL http://arxiv.org/abs/1808. 09121 .</p>
<p>Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67-81, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1007. URL https://aclanthology.org/ D18-1007.</p>
<p>Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231-5247, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.467. URL https: //aclanthology.org/2020.acl-main. 467.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, art. arXiv:1606.05250, 2016.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. CoRR, abs/2102.07350, 2021. URL https://arxiv.org/abs/2102.07350.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main. 437.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.</p>
<p>Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to AI complete question answering: A set of prerequisite real tasks. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8722-8731. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6398.</p>
<p>Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.</p>
<p>Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015. doi: 10.18653/v1/d15-1044. URL http://dx.doi.org/10.18653/v1/ D15-1044.</p>
<p>Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension. In Meeting of the Association for Computational Linguistics (ACL), 2018.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WINOGRANDE: an adversarial winograd schema challenge at scale. CoRR, abs/1907.10641, 2019. URL http://arxiv. org/abs/1907.10641.</p>
<p>Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online, April 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.eacl-main. 20.</p>
<p>Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM, 63(12):54-63, 2020.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointergenerator networks. CoRR, abs/1704.04368, 2017. URL http://arxiv.org/abs/1704.04368.</p>
<p>Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.</p>
<p>Reddy Siva, Chen Danqi, and Manning Christopher D. Wikiqa: A challenge dataset for open-domain question answering. arXiv, 2018.</p>
<p>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. CoRR, abs/1908.09203, 2019. URL http://arxiv.org/abs/1908.09203.</p>
<p>Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645-3650, 2019.</p>
<p>Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. DREAM: A challenge dataset and models for dialogue-based reading comprehension. Transactions of the Association for Computational Linguistics, 2019. URL https://arxiv.org/abs/1902.00164v1.</p>
<p>Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. "quartz: An open-domain dataset of qualitative relationship questions". EMNLP, "2019".</p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. Quarel: A dataset and models for answering questions about qualitative relationships. CoRR, abs/1811.08048, 2018. URL http://arxiv.org/abs/1811.08048.</p>
<p>Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7882-7926, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.635. URL https://aclanthology.org/ 2020.emnlp-main. 635 .</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537, 2019a. URL http://arxiv.org/abs/1905.00537.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. ICLR, 2019b. In the Proceedings of ICLR.</p>
<p>Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot learner. CoRR, abs/2104.14690, 2021. URL https://arxiv.org/abs/2104.14690.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.</p>
<p>Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts?, 2021. URL https://arxiv.org/abs/2109.01247.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2021.</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents, 2018.</p>
<p>Adina Williams, Tristan Thrush, and Douwe Kiela. Anlizing the adversarial natural language inference dataset. arXiv preprint arXiv:2010.12729, 2020.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835, 2021. URL https://arxiv.org/abs/2104. 08835 .</p>
<p>Yang Yi, Yih Wen-tau, and Christopher Meek. WikiQA: A Challenge Dataset for Open-Domain Question Answering. Association for Computational Linguistics, page 2013-2018, 2015. doi: $10.18653 / \mathrm{v} 1 / \mathrm{D} 15-1237$.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649-657, 2015a.</p>
<p>Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015b.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scrambling. In Proc. of NAACL, 2019.</p>
<p>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15-20, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2003. URL https://aclanthology.org/N18-2003.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models, 2021.</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. CoRR, abs/2104.04670, 2021. URL https://arxiv.org/abs/2104.04670.</p>
<h1>A CONTRIBUTIONS AND PROJECT STRUCTURE</h1>
<p>This research was conducted under the BigScience project for open research, ${ }^{4}$ a year-long initiative targeting the study of large models and datasets. The goal of the project is to research language models in a public environment outside large technology companies. The project has 600 researchers from 50 countries and more than 250 institutions. The BigScience project was initiated by Thomas Wolf at Hugging Face, and this collaboration would not have been possible without his effort. This research was the focus of the BigScience Prompt Engineering working group, which focused on the role of prompting in large language model training.</p>
<p>This project was led by the joint first-authors of this work. Victor Sanh co-led the prompt engineering group, managed the prompt collection procedure, implemented the prompt materialization, and ran evaluation systems. Albert Webson reviewed and selected all training and evaluation datasets, led the analysis of results, designed the ablation studies, and co-managed the writing process. Colin Raffel proposed the research direction, trained all the models, named the model, and built the main evaluation system. Stephen Bach co-led the prompt engineering group, developed the prompting tool and guidelines, and led the prompt collection effort central to the work. Additionally, Alexander Rush helped develop the prompt templating language and tool, and co-managed paper writing.</p>
<p>Following the goals of the BigScience project, this work is co-authored by all contributors to the working group. We define this contribution as having contributed at least 3 accepted prompted datasets to the project. Lacking a better metric, authors are sorted based on code contributions to the project. We explicitly highlight the work of: Lintang Sutawika, who helped with evaluation and writing; Urmish Thakker, Mike Tian-Jian Jiang, Shanya Sharma, Arnaud Stiegler, and Manan Dey who helped with the development of the prompting tool; M Saiful Bari, who helped for the models and dataset release; Teven Le Scao, who conducted the contamination analysis.</p>
<h2>B Broader Impacts</h2>
<h2>B. 1 Environmental Costs</h2>
<p>Training large language models can incur substantial environmental costs (Strubell et al., 2019; Schwartz et al., 2020; Lacoste et al., 2019; Bender et al., 2021). These costs are due to the energy used to power the hardware required for training. Recently, Patterson et al. (2021) performed a detailed analysis of the carbon emissions resulting from the training of various recent large language models. One model analyzed in that study was the largest T5 variant which was estimated to have emitted around $46.7 \mathrm{tCO}_{2} \mathrm{e}$. Since we based T 0 on this T 5 variant and performed training on the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>same hardware (Google Cloud TPUs), we can estimate the carbon emissions produced by our study by simply re-scaling the T5 estimate from Patterson et al. (2021) by the amount of training we performed. Specifically, T5 was pretrained for one trillion tokens; across all of our training runs (including preliminary test experiments not described in this paper) we trained for 250 billion tokens, or about $25 \%$ as many. These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device. Further, T5 was trained in Google's Taiwan datacenter, whereas we trained in the europe-west4-a Cloud region. The $\mathrm{gCO}<em 2="2">{2} \mathrm{eq} / \mathrm{kWh}$ published by Google for these datacenters are 540 and 410 respectively, ${ }^{5}$ suggesting that our carbon emissions should further be scaled by a factor of $410 / 540 \approx 75.9 \%$. Based on the above, we estimate the total emissions for training our models to be about $46.7 \times 25 \% \times 75.9 \% \approx 8.9 \mathrm{tCO}</em>}$ e. As a point of reference, Patterson et al. (2021) estimate that a roundtrip jet plane flight from San Francisco to New York emits around $180 \mathrm{tCO<em 2="2">{2}$ e and Strubell et al. (2019) estimate the average per-passenger emissions to be about 1 $\mathrm{tCO}</em>$. Note that our experiments incurred additional emissions due to the cost of evaluation, the XL-sized ablation, and data preprocessing, but these costs are negligible compared to the training runs for the main T0 model. Moreover, most of the evaluations and data preprocessing ran on the French Jean-Zay cluster whose electricity mostly comes from nuclear energy.} \mathrm{e</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Hardware</th>
<th style="text-align: center;">Hours</th>
<th style="text-align: center;">Grid</th>
<th style="text-align: center;">$\mathrm{gCO}_{2} \mathrm{eq} / \mathrm{kWh}$</th>
<th style="text-align: center;">Estimated $\mathrm{tCO}_{2} \mathrm{e}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T0 (single run)</td>
<td style="text-align: center;">v3-512</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">europe-west4-a</td>
<td style="text-align: center;">410</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">All experiments in this paper</td>
<td style="text-align: center;">v3-512</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">europe-west4-a</td>
<td style="text-align: center;">410</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: left;">T5-11B (single run)</td>
<td style="text-align: center;">v3-1024</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">Taiwan</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">46.7</td>
</tr>
</tbody>
</table>
<p>Table 1: Carbon emissions information for T0 and T5.</p>
<h1>B. 2 Risks in Developing and Releasing Large Language Models</h1>
<p>The focus of this paper is an empirical exploration of multitask prompt training and how it improves zero-shot performance on multiple tasks. We transformed datasets by writing multiple prompts for each of the datasets, fine-tuned pretrained models on the transformed examples and observed strong zero-shot capabilities on multiple tasks. We note that the zero-shot performance of our model is still significantly behind models that are fine-tuned on the given task in a "traditional" transfer-learning setup. This highlights how much research is still needed in this area, and we believe this work and the resources developed as part of this work are central to future research.</p>
<p>This work is built exclusively on publicly available datasets from the Hugging Face datasets library (Lhoest et al., 2021) and a publicly available model, T5+LM (Lester et al., 2021). The implications of releasing large language models have been extensively discussed in Bender et al. (2021); Bommasani et al. (2021); Solaiman et al. (2019) among others. We expect replicating our work to be within the capabilities of dozens of organizations worldwide, the main barrier being financial constraints. As such, we believe that the additional potential risks and harms produced by releasing our model are limited, and not releasing it would primarily impact less funded research organizations and individuals.</p>
<p>Moreover, we favor openness, transparency, and reproducibility. Releasing the dataset, models and tools developed as part of this work are key elements that will facilitate the reproduction of our results and future works. As such, our resources are available at :</p>
<ul>
<li>Collection of prompts: http://github.com/bigscience-workshop/promptsource</li>
<li>Trained models: https://github.com/bigscience-workshop/t-zero</li>
<li>Materialized prompted data used to train the models: https://huggingface.co/datasets/ bigscience/P3</li>
</ul>
<h2>B. 3 Bias and Fairness</h2>
<p>Some of the datasets available in the Hugging Face library contain potentially harmful content. We deliberately excluded these datasets from the training mixture to avoid training the model to generate</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>such outputs. In particular, we excluded datasets that are constructed from forums and social media content such as Sentiment140 (Go et al., 2009) which was based on Twitter content or Hate Speech 18 (de Gibert et al., 2018) which was built on data from a white supremacist forum.</p>
<p>Even if we took deliberate decisions to exclude datasets with potentially harmful content from the fine-tuning, the models trained are not bias-free. Based on a few experiments, T0++ can generate answers that could be categorized as conspiracist or biased:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Is the earth flat?</td>
<td style="text-align: left;">yes</td>
</tr>
<tr>
<td style="text-align: left;">Do vaccines cause autism?</td>
<td style="text-align: left;">yes</td>
</tr>
<tr>
<td style="text-align: left;">Complete this sentence: This man works as a</td>
<td style="text-align: left;">Architect</td>
</tr>
<tr>
<td style="text-align: left;">Complete this sentence: This woman works as a</td>
<td style="text-align: left;">Nanny</td>
</tr>
</tbody>
</table>
<p>Table 2: A few problematic predictions from T0++.
Language models can reproduce undesirable social biases represented in the large corpus they are pre-trained on. We evaluate our models in two ways: first, in their ability to recognize or label gender biases and second, in the extent to which they reproduce those biases.</p>
<p>To measure the ability of our model to recognize gender biases, we evaluate our models using the WinoGender Schemas (Rudinger et al., 2018) (also called AX-g under SuperGLUE) and CrowSPairs (Nangia et al., 2020). WinoGender Schemas are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias. We use the version from Poliak et al. (2018) that casts WinoGender as a textual entailment task and report accuracy. CrowS-Pairs is a challenge dataset for measuring the degree to which U.S. stereotypical biases present in the masked language models using minimal pairs of sentences. We re-formulate the task by predicting which of two sentences is stereotypical (or anti-stereotypical) and report accuracy. For each dataset, we evaluate between 5 and 10 prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Mean (Acc.)</th>
<th style="text-align: center;">Median (Acc.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CrowS-Pairs</td>
<td style="text-align: left;">T0</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">83.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0+</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">83.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0++</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0 (p=1)</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">69.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0 (3B)</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">82.6</td>
</tr>
<tr>
<td style="text-align: left;">WinoGender</td>
<td style="text-align: left;">T0</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0+</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0++</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0 (p=1)</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">84.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">T0 (3B)</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">69.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Average and median accuracies on CrowS-Pairs and WinoGender reformulated as classification tasks.</p>
<p>To measure the extent to which our model reproduces gender biases, we evaluate our models using the WinoBias Schemas (Zhao et al., 2018). WinoBias Schemas are pronoun coreference resolution tasks that have the potential to be influenced by gender bias. WinoBias Schemas has two schemas (type1 and type2) which are partitioned into pro-stereotype and anti-stereotype subsets. A "pro-stereotype" example is one where the correct answer conforms to stereotypes, while an "anti-stereotype" example is one where it opposes stereotypes. All examples have an unambiguously correct answer, and so the difference in scores between the "pro-" and "anti-" subset measures the extent to which stereotypes can lead the model astray. We report accuracies by considering a prediction correct if the target noun is present in the model's prediction. We evaluate on 6 prompts.</p>
<h1>C Annotation SYSTEM - PromptSource</h1>
<p>In order to collect hundreds of templates for prompts, we first needed a system that enabled users to view data, provide templates in a standard format, and verify that their templates work correctly. We</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Subset</th>
<th style="text-align: left;">Average (Acc.)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Median (Acc.)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Pro</td>
<td style="text-align: left;">Anti</td>
<td style="text-align: left;">Pro - Anti</td>
<td style="text-align: left;">Pro</td>
<td style="text-align: left;">Anti</td>
<td style="text-align: left;">Pro - Anti</td>
</tr>
<tr>
<td style="text-align: left;">T0</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">68.0</td>
<td style="text-align: left;">61.9</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">71.7</td>
<td style="text-align: left;">61.9</td>
<td style="text-align: left;">9.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">76.4</td>
<td style="text-align: left;">2.8</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">4.3</td>
</tr>
<tr>
<td style="text-align: left;">T0+</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">66.6</td>
<td style="text-align: left;">57.2</td>
<td style="text-align: left;">9.4</td>
<td style="text-align: left;">71.5</td>
<td style="text-align: left;">62.6</td>
<td style="text-align: left;">8.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">77.7</td>
<td style="text-align: left;">73.4</td>
<td style="text-align: left;">4.3</td>
<td style="text-align: left;">86.1</td>
<td style="text-align: left;">81.3</td>
<td style="text-align: left;">4.8</td>
</tr>
<tr>
<td style="text-align: left;">T0++</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">63.8</td>
<td style="text-align: left;">55.9</td>
<td style="text-align: left;">7.9</td>
<td style="text-align: left;">72.7</td>
<td style="text-align: left;">63.4</td>
<td style="text-align: left;">9.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">66.8</td>
<td style="text-align: left;">63.0</td>
<td style="text-align: left;">3.9</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">74.0</td>
<td style="text-align: left;">5.3</td>
</tr>
<tr>
<td style="text-align: left;">T0 (p=1)</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">73.7</td>
<td style="text-align: left;">60.5</td>
<td style="text-align: left;">13.2</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">60.6</td>
<td style="text-align: left;">18.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">77.7</td>
<td style="text-align: left;">69.6</td>
<td style="text-align: left;">8.0</td>
<td style="text-align: left;">80.8</td>
<td style="text-align: left;">69.7</td>
<td style="text-align: left;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">T0 (original task only)</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">67.7</td>
<td style="text-align: left;">10.4</td>
<td style="text-align: left;">81.8</td>
<td style="text-align: left;">67.2</td>
<td style="text-align: left;">14.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">85.2</td>
<td style="text-align: left;">82.3</td>
<td style="text-align: left;">2.9</td>
<td style="text-align: left;">89.6</td>
<td style="text-align: left;">85.4</td>
<td style="text-align: left;">4.3</td>
</tr>
<tr>
<td style="text-align: left;">T0 (3B)</td>
<td style="text-align: left;">Type 1</td>
<td style="text-align: left;">82.3</td>
<td style="text-align: left;">70.1</td>
<td style="text-align: left;">12.2</td>
<td style="text-align: left;">83.6</td>
<td style="text-align: left;">62.9</td>
<td style="text-align: left;">20.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Type 2</td>
<td style="text-align: left;">83.8</td>
<td style="text-align: left;">76.5</td>
<td style="text-align: left;">7.3</td>
<td style="text-align: left;">85.9</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">10.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracies on WinoBias coreference task.
implemented a lightweight interface in Streamlit ${ }^{6}$ that users could download, run locally in a web browser, and then upload their results to a central repository.</p>
<p>Testing iterations of the interface on pilot template-writing tasks, we converged on three views for the interface. First, a "helicopter" view allows users to see what datasets are available for writing templates and how many are written for each, to prioritize user attention. Second, a "sourcing" view allows users to select a dataset to prompt, browse examples from that dataset in the form of Python dictionaries provided by the Hugging Face datasets library, and enter a template for that dataset. As the user writes their template, every time they save it, the output of the template applied to the current example is displayed next to the editor. We also collect metadata like a name for the template, and a reference for any bibliographic information or rationale for the template. Third, in the "prompted dataset" view, users can select templates and browse the prompts generated by them. The original example (a Python dictionary) is viewed side-by-side with the resulting prompt, with the substituted text highlighted to distinguish from text hard-coded in the template. Users can quickly scroll through many examples, verify the behavior of their template, and return to the sourcing view if changes are needed.</p>
<p>A key design decision is the format for templates. We experimented with multiple formats and found that they exhibited a tradeoff between expressivity and explicit structure. On one side, a maximally expressive format such as pure Python code would let users write complex programs to manipulate the semi-structured examples into prompts. However, analyzing these programs to understand how the prompts are created becomes difficult. This difficulty limits downstream manipulation and analysis of the templates, such as automatic template augmentation. On the other side, a maximally structured format such as rule-based generation limits the kinds of templates that users can create. We found it infeasible to enumerate types of rules sufficient for the wide range of tasks and data formats for which we wanted templates.</p>
<p>We therefore settled on a middle ground between the two: the Jinja templating engine ${ }^{7}$ originally designed for producing web markup. Users write templates as prompts with placeholders, such as If {{premise}} is true, is it also true that {{hypothesis}}? ||| {{entailed}}. The separator ||| denotes the break between the conditioning text and the desired completion.Placeholders refer to fields in the underlying example dictionary. Users also have access to Jinja's built-in functions, such as manipulating strings and structured data. For each template, prompts are created by applying the template to all examples in the corresponding dataset.</p>
<p>During the development of our tool (which we called PromptSource), we found that a few idioms were particularly useful. First, not all templates are applicable to all examples in a dataset. Users can wrap templates in Jinja's built-in conditional statements, and any example that results in</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://streamlit.io/
${ }^{7}$ https://jinja.palletsprojects.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
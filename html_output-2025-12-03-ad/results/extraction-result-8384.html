<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-3dd61d97827e3f380bf9304101149a3f865051fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3dd61d97827e3f380bf9304101149a3f865051fc" target="_blank">Injecting Numerical Reasoning Skills into Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work shows that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.</p>
                <p><strong>Paper Abstract:</strong> Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GENBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GENBERT (GenBERT: BERT-based generative model for numerical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-initialized encoder-decoder Transformer that generates answers token-by-token (including numbers) and is pre-trained on synthetic numerical (ND) and textual (TD) data to inject numerical reasoning skills into a general-purpose LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer initialized from BERT-base-uncased with tied encoder/decoder weights, added FFNs to distinguish encoder/decoder representations, generative decoder head for token-by-token answer generation; digit-level tokenization for numbers; trained with multi-task objective (ND, TD, and MLM). Model size: BERT-base (≈110M params).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Signed float combinations (up to 4 terms), min/max/avg, argmax/argmin on value lists, date differences (days/months/years), percentage complement (100-x), simple sums/subtractions typical of DROP and filtered MWP (ADDSub/SOp/SEQ).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Latent internal computation performed by the decoder: numbers produced token-by-token (digit-by-digit tokens), numeric values represented via digit token sequences; no explicit symbolic program is output — computations are learned end-to-end in model parameters. Uses tied Transformer attention for integrating text and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Synthetic pre-training tasks: ND (numerical-only templates) and TD (textual templates from extracted patterns) as multi-task pre-training combined with standard masked LM loss; ablations removing MLM, DT (digit tokenization) or RS (random shift); zero-shot evaluation on MWP datasets; fine-tuning on DROP; encoder weight transplant to other architectures (NABERT+, MS-TAG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline GENBERT (no synthetic pretraining) on DROP dev: 46.1 EM, 49.3 F1. GENBERT+ND: ~64.7 EM (reported), GENBERT+TD: ~64.4 EM, GENBERT+ND+TD: 68.8 EM and reported improvement in F1 from 49.3 -> 72.3 (abstract). Per-answer-type F1 (dev): GENBERT: number 42.3, span 67.3, date 47.5, spans 21.1; GENBERT+ND: number 75.2, span 74.5, date 56.4, spans 24.2; GENBERT+ND+TD: number 75.0, span 71.3, date 44.2, spans 53.4. Zero-shot MWP EM (ADDSub/SOp/SEQ): GENBERT: 2 / 1.2 / 1.3; GENBERT+ND+TD: 22.8 / 28.3 / 22.3. SQuAD v1 dev: BERT 81.1 EM / 88.6 F1; GENBERT+ND+TD 81.3 EM / 88.6 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails on reasoning types not covered by pretraining (sorting, some counting variants, complex calculations, complex semantics) — ~43% of sampled errors; inaccurate predictions (span length errors, partial-digit numeric matches) ~23%; inability to handle multi-span list answers (unsupported); severe degradation for arithmetic expressions with >3 terms (near-zero beyond 3 terms).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablations: removal of digit-tokenization (DT) stops learning of ND templates (ND-LM-DT fails); omitting MLM loss reduces downstream performance (ND-LM worse); random shift (RS) helps performance on short inputs; GENBERT achieves >96% accuracy on synthetic ND/TD tasks during pretraining, indicating it internalizes the required numeric computations; pretraining improves sample complexity and downstream DROP/MWP performance; transplanting GENBERT encoder into other architectures gives ~2 EM improvements, indicating learned numerical representations are transferable.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Specialized symbolic/external-calculator architectures (e.g., MTMSN or models with arithmetic/count heads) still outperform GENBERT on some structured tasks and multi-span/list outputs; GENBERT struggles with expressions longer than 3 terms and with reasoning operations not included in synthetic templates (sorting, some percentage splits, long-range numeric aggregation); model currently limited by numeric range and template coverage; no neuron-level or attention-based mechanistic proof (only behavioral/ablation evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used pre-trained Transformer encoder used as the initialization backbone for GENBERT's encoder and tied decoder weights; serves as baseline for numeric reasoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder pretrained with masked LM and next-sentence tasks (base size ≈110M parameters); used to initialize GENBERT encoder and decoder weight tying.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>None inherent — used as baseline for DROP and SQuAD; when fine-tuned on DROP without numeric pretraining, poor performance on arithmetic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Standard contextual token representations (wordpiece tokenization) which treat numbers as ordinary tokens — shown insufficient for learning numeric operations end-to-end without additional pretraining or modules.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning on DROP (baseline) and used as initialization for GENBERT; compared to GENBERT pretraining variants to probe effect of numeric pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On SQuAD v1 dev: 81.1 EM / 88.6 F1 (reported). As a DROP baseline (when used inside hybrid models) achieves lower numeric-answer performance compared to models with numeric modules or GENBERT after ND/TD pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Cannot reliably generate numeric answers not present in input; poor sample efficiency for numeric computations when fine-tuned only on target datasets like DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral comparison: GENBERT initialized from BERT and pre-trained on ND/TD substantially outperforms vanilla BERT fine-tuned on DROP for numeric answers, indicating BERT's native representations lack sufficient numeric processing abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>BERT can be augmented with specialized numeric heads or external calculators to handle arithmetic (hybrid approaches), but without this performs poorly on NRoT tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid arithmetic heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specialized arithmetic/count heads used in prior hybrid NRoT models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architecture pattern that augments pre-trained encoders with specialized heads for count prediction and arithmetic over numbers found in text, selecting among heads via a type head and marginalizing over expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hybrid extractive + arithmetic head architectures (e.g., MTMSN style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder (BERT) producing contextual representations fed to multiple FFN heads: context-span head, question-span head, count head (predicts 0..9), arithmetic head (signed combinations of identified numbers), and a type head to select/marginalize over heads.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting up to small fixed thresholds (e.g., 0..9), addition/subtraction of small sets of numbers (signed combinations), limited symbolic arithmetic over numbers that appear in the input.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Explicit symbolic-style outputs: arithmetic head enumerates signed combinations of input numbers and assigns probabilities; computations effectively performed outside or by constrained output heads, with answers restricted to enumerated expression results.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Marginalization over all expressions during training; designed modules for count/arithmetic as architectural intervention compared to end-to-end generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>State-of-the-art comparable models (MTMSN BASE) achieve DROP EM ≈ 68.2 and higher MWP EM (e.g., ADDSub 32.2 / SEQ 32.5) outperforming GENBERT variants on some MWP tasks and multi-span outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Output space severely constrained (e.g., counts limited to 0..9), combinatorial explosion when extending expression space, requires non-differentiable search or sampling to scale, inability to internally combine computation with downstream span selection when answer must be a span derived from computed number.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prior models using these heads show strong performance on DROP; comparison in the paper indicates such specialized heads perform better on some tasks (multi-span) and longer arithmetic expressions when computation is performed externally or constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Rigid architecture limits extensibility (hard to add new operations or larger numeric ranges) and complicates end-to-end training due to combinatorial marginalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ND (Numerical Data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ND: Synthetic Numerical Data pre-training dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically-generated dataset of 1M numeric-only expressions (and 10k validation) covering templates such as signed float combinations, min/max/avg, argmax/min, date ops, date differences, and percentage complements for training numeric computation skills.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT pretraining dataset component (ND)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic numerical expressions without textual context, designed to teach numeric token value comprehension and arithmetic (floats with two decimals, integers up to 20k, dates up to Sep 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Signed float combos (≤4 floats), min/max/avg, argmin/argmax, date min/max, date difference (days/months/years), percentage complement calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Presents numeric operations as direct expressions (e.g., '3+4+11=') to train token-by-token numeric generation and internal arithmetic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as standalone pretraining (GENBERT+ND) and in multi-task together with TD and MLM; ablated (ND-LM) to test importance of MLM; ablation ND-LM-DT showed failure when numbers tokenized by wordpieces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GENBERT trained on ND achieves >96% accuracy on ND evaluation tasks; pretraining on ND yields large downstream gains on DROP numeric answers (number F1 from 42.3 -> 75.2 for GENBERT+ND).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Covers many numeric ops but limited in scope (templates, numeric range). Without digit tokenization or MLM, models fail or underperform. Does not cover sorting, some count varieties, or complex multi-term expressions beyond template scope.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>ND training substantially speeds convergence on TD and improves numeric-answer performance; ND-LM-DT ablation (wordpiece tokenization) fails to learn ND tasks, demonstrating DT is crucial for ND usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>ND alone cannot teach integration with natural language; ND-trained model still struggles on multi-term (>3) arithmetic generalization and tasks not expressed in the templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8384.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8384.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD (Textual Data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TD: Synthetic Textual Data pre-training dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically-generated question-passage pairs (2.5M train, 10k val) using templates extracted from math word problems to teach how numeric computations are expressed in pseudo-natural language and to maintain alignment between textual context and numeric world states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GENBERT pretraining dataset component (TD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Passages composed by instantiating 12 frequent sentence templates with small domain vocabularies (history, NFL) plus 13 question templates across 7 skill types, generating numeric-world-state questions requiring selection, comparisons, differences, sums, superlatives, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>In-text selection, intra/inter-entity differences, subset counts, superlatives, sums across containers — arithmetic combined with entity tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Connects numeric world state and textual events, letting the encoder learn to map linguistic descriptions of changes to numeric state updates and then allow decoder to generate numeric answers token-by-token.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Pretraining on TD alone (GENBERT+TD) and combined with ND (GENBERT+ND+TD); zero-shot evaluation on MWP; ablations to measure ND impact on TD learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GENBERT+TD achieves >96% on TD evaluation; improves DROP EM to ~64.4 when used alone; when combined with ND gives best overall drop performance (68.8 EM). Zero-shot MWP improvement modest compared to ND but TD improves robustness for text-conditioned numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>TD lacks coverage for some numeric operations and ranges; models pretrained only on TD converge slower and obtain lower numeric-only generalization than with ND; fails on operations not captured by templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GENBERT trained first on ND converged faster on TD — supports shared underlying numeric skills; high TD eval accuracy indicates the model learned to map textual templates to numeric computations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>TD alone insufficient for short-context numeric templates and numeric token learning; must be combined with ND and MLM to avoid catastrophic forgetting and to learn digit-level numeracy effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8384.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8384.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DT & RS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit Tokenization (DT) and Random Shift (RS) interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two training/representation interventions: DT tokenizes numbers digit-by-digit to improve numeric learning; RS randomly shifts positional embeddings to avoid overfitting to absolute positions on short numeric inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretraining interventions: DT (digit tokenization) & RS (random shift)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DT: split numeric tokens into individual digit wordpieces (e.g., '517.4' -> '5','1','7','.','4'); RS: randomize position IDs within 0..512-n to reduce reliance on absolute positions for short inputs during ND training.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Support for any numeric tasks by improving internal numeric representation and positional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>DT makes value composition easier by exposing digit-level structure to Transformer; RS encourages position-invariant learning so numeric computation generalizes across input positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation experiments: ND-LM-DT (no DT) failed to learn ND tasks; ND-LM-RS (no RS) reduced performance on short inputs; comparisons of variants shown in learning curves and downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DT crucial: ND-LM-DT failed to learn ND (learning curves in Fig.4); RS helpful: variants without RS/MLM had lower DROP performance. Quantitatively, inclusion of DT enabled learning >96% on ND; excluding DT prevented learning.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without DT numeric tokenization learning fails (poor sample complexity); without RS the model can overfit positional patterns of short ND examples and generalize worse.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical ablation: ND-LM-DT does not learn ND templates; ND-LM-RS and ND-LM (no MLM) have lower downstream DROP scores, showing DT and RS materially affect capacity to learn arithmetic from synthetic inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>DT/RS are helpful but do not solve limitations in arithmetic generalization (e.g., >3 terms) or coverage of non-templated reasoning operations; digit-tokenization increases vocabulary/sequence length tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Injecting Numerical Reasoning Skills into Language Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do nlp models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Giving bert a calculator: Finding operations and arguments with reading comprehension <em>(Rating: 2)</em></li>
                <li>A multi-type multi-span network for reading comprehension that requires discrete reasoning <em>(Rating: 2)</em></li>
                <li>MathQA: Towards interpretable math word problem solving with operation-based formalisms <em>(Rating: 1)</em></li>
                <li>Deep learning for symbolic mathematics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8384",
    "paper_id": "paper-3dd61d97827e3f380bf9304101149a3f865051fc",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GENBERT",
            "name_full": "GENBERT (GenBERT: BERT-based generative model for numerical reasoning)",
            "brief_description": "A BERT-initialized encoder-decoder Transformer that generates answers token-by-token (including numbers) and is pre-trained on synthetic numerical (ND) and textual (TD) data to inject numerical reasoning skills into a general-purpose LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GENBERT",
            "model_description": "Encoder-decoder Transformer initialized from BERT-base-uncased with tied encoder/decoder weights, added FFNs to distinguish encoder/decoder representations, generative decoder head for token-by-token answer generation; digit-level tokenization for numbers; trained with multi-task objective (ND, TD, and MLM). Model size: BERT-base (≈110M params).",
            "arithmetic_task_type": "Signed float combinations (up to 4 terms), min/max/avg, argmax/argmin on value lists, date differences (days/months/years), percentage complement (100-x), simple sums/subtractions typical of DROP and filtered MWP (ADDSub/SOp/SEQ).",
            "mechanism_or_representation": "Latent internal computation performed by the decoder: numbers produced token-by-token (digit-by-digit tokens), numeric values represented via digit token sequences; no explicit symbolic program is output — computations are learned end-to-end in model parameters. Uses tied Transformer attention for integrating text and computation.",
            "probing_or_intervention_method": "Synthetic pre-training tasks: ND (numerical-only templates) and TD (textual templates from extracted patterns) as multi-task pre-training combined with standard masked LM loss; ablations removing MLM, DT (digit tokenization) or RS (random shift); zero-shot evaluation on MWP datasets; fine-tuning on DROP; encoder weight transplant to other architectures (NABERT+, MS-TAG).",
            "performance_metrics": "Baseline GENBERT (no synthetic pretraining) on DROP dev: 46.1 EM, 49.3 F1. GENBERT+ND: ~64.7 EM (reported), GENBERT+TD: ~64.4 EM, GENBERT+ND+TD: 68.8 EM and reported improvement in F1 from 49.3 -&gt; 72.3 (abstract). Per-answer-type F1 (dev): GENBERT: number 42.3, span 67.3, date 47.5, spans 21.1; GENBERT+ND: number 75.2, span 74.5, date 56.4, spans 24.2; GENBERT+ND+TD: number 75.0, span 71.3, date 44.2, spans 53.4. Zero-shot MWP EM (ADDSub/SOp/SEQ): GENBERT: 2 / 1.2 / 1.3; GENBERT+ND+TD: 22.8 / 28.3 / 22.3. SQuAD v1 dev: BERT 81.1 EM / 88.6 F1; GENBERT+ND+TD 81.3 EM / 88.6 F1.",
            "error_types_or_failure_modes": "Fails on reasoning types not covered by pretraining (sorting, some counting variants, complex calculations, complex semantics) — ~43% of sampled errors; inaccurate predictions (span length errors, partial-digit numeric matches) ~23%; inability to handle multi-span list answers (unsupported); severe degradation for arithmetic expressions with &gt;3 terms (near-zero beyond 3 terms).",
            "evidence_for_mechanism": "Ablations: removal of digit-tokenization (DT) stops learning of ND templates (ND-LM-DT fails); omitting MLM loss reduces downstream performance (ND-LM worse); random shift (RS) helps performance on short inputs; GENBERT achieves &gt;96% accuracy on synthetic ND/TD tasks during pretraining, indicating it internalizes the required numeric computations; pretraining improves sample complexity and downstream DROP/MWP performance; transplanting GENBERT encoder into other architectures gives ~2 EM improvements, indicating learned numerical representations are transferable.",
            "counterexamples_or_challenges": "Specialized symbolic/external-calculator architectures (e.g., MTMSN or models with arithmetic/count heads) still outperform GENBERT on some structured tasks and multi-span/list outputs; GENBERT struggles with expressions longer than 3 terms and with reasoning operations not included in synthetic templates (sorting, some percentage splits, long-range numeric aggregation); model currently limited by numeric range and template coverage; no neuron-level or attention-based mechanistic proof (only behavioral/ablation evidence).",
            "uuid": "e8384.0",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A widely-used pre-trained Transformer encoder used as the initialization backbone for GENBERT's encoder and tied decoder weights; serves as baseline for numeric reasoning experiments.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT-base-uncased",
            "model_description": "Transformer encoder pretrained with masked LM and next-sentence tasks (base size ≈110M parameters); used to initialize GENBERT encoder and decoder weight tying.",
            "arithmetic_task_type": "None inherent — used as baseline for DROP and SQuAD; when fine-tuned on DROP without numeric pretraining, poor performance on arithmetic answers.",
            "mechanism_or_representation": "Standard contextual token representations (wordpiece tokenization) which treat numbers as ordinary tokens — shown insufficient for learning numeric operations end-to-end without additional pretraining or modules.",
            "probing_or_intervention_method": "Fine-tuning on DROP (baseline) and used as initialization for GENBERT; compared to GENBERT pretraining variants to probe effect of numeric pretraining.",
            "performance_metrics": "On SQuAD v1 dev: 81.1 EM / 88.6 F1 (reported). As a DROP baseline (when used inside hybrid models) achieves lower numeric-answer performance compared to models with numeric modules or GENBERT after ND/TD pretraining.",
            "error_types_or_failure_modes": "Cannot reliably generate numeric answers not present in input; poor sample efficiency for numeric computations when fine-tuned only on target datasets like DROP.",
            "evidence_for_mechanism": "Behavioral comparison: GENBERT initialized from BERT and pre-trained on ND/TD substantially outperforms vanilla BERT fine-tuned on DROP for numeric answers, indicating BERT's native representations lack sufficient numeric processing abilities.",
            "counterexamples_or_challenges": "BERT can be augmented with specialized numeric heads or external calculators to handle arithmetic (hybrid approaches), but without this performs poorly on NRoT tasks.",
            "uuid": "e8384.1",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Hybrid arithmetic heads",
            "name_full": "Specialized arithmetic/count heads used in prior hybrid NRoT models",
            "brief_description": "Architecture pattern that augments pre-trained encoders with specialized heads for count prediction and arithmetic over numbers found in text, selecting among heads via a type head and marginalizing over expressions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Hybrid extractive + arithmetic head architectures (e.g., MTMSN style)",
            "model_description": "Encoder (BERT) producing contextual representations fed to multiple FFN heads: context-span head, question-span head, count head (predicts 0..9), arithmetic head (signed combinations of identified numbers), and a type head to select/marginalize over heads.",
            "arithmetic_task_type": "Counting up to small fixed thresholds (e.g., 0..9), addition/subtraction of small sets of numbers (signed combinations), limited symbolic arithmetic over numbers that appear in the input.",
            "mechanism_or_representation": "Explicit symbolic-style outputs: arithmetic head enumerates signed combinations of input numbers and assigns probabilities; computations effectively performed outside or by constrained output heads, with answers restricted to enumerated expression results.",
            "probing_or_intervention_method": "Marginalization over all expressions during training; designed modules for count/arithmetic as architectural intervention compared to end-to-end generation.",
            "performance_metrics": "State-of-the-art comparable models (MTMSN BASE) achieve DROP EM ≈ 68.2 and higher MWP EM (e.g., ADDSub 32.2 / SEQ 32.5) outperforming GENBERT variants on some MWP tasks and multi-span outputs.",
            "error_types_or_failure_modes": "Output space severely constrained (e.g., counts limited to 0..9), combinatorial explosion when extending expression space, requires non-differentiable search or sampling to scale, inability to internally combine computation with downstream span selection when answer must be a span derived from computed number.",
            "evidence_for_mechanism": "Prior models using these heads show strong performance on DROP; comparison in the paper indicates such specialized heads perform better on some tasks (multi-span) and longer arithmetic expressions when computation is performed externally or constrained.",
            "counterexamples_or_challenges": "Rigid architecture limits extensibility (hard to add new operations or larger numeric ranges) and complicates end-to-end training due to combinatorial marginalization.",
            "uuid": "e8384.2",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "ND (Numerical Data)",
            "name_full": "ND: Synthetic Numerical Data pre-training dataset",
            "brief_description": "Automatically-generated dataset of 1M numeric-only expressions (and 10k validation) covering templates such as signed float combinations, min/max/avg, argmax/min, date ops, date differences, and percentage complements for training numeric computation skills.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GENBERT pretraining dataset component (ND)",
            "model_description": "Synthetic numerical expressions without textual context, designed to teach numeric token value comprehension and arithmetic (floats with two decimals, integers up to 20k, dates up to Sep 2019).",
            "arithmetic_task_type": "Signed float combos (≤4 floats), min/max/avg, argmin/argmax, date min/max, date difference (days/months/years), percentage complement calculations.",
            "mechanism_or_representation": "Presents numeric operations as direct expressions (e.g., '3+4+11=') to train token-by-token numeric generation and internal arithmetic representations.",
            "probing_or_intervention_method": "Used as standalone pretraining (GENBERT+ND) and in multi-task together with TD and MLM; ablated (ND-LM) to test importance of MLM; ablation ND-LM-DT showed failure when numbers tokenized by wordpieces.",
            "performance_metrics": "GENBERT trained on ND achieves &gt;96% accuracy on ND evaluation tasks; pretraining on ND yields large downstream gains on DROP numeric answers (number F1 from 42.3 -&gt; 75.2 for GENBERT+ND).",
            "error_types_or_failure_modes": "Covers many numeric ops but limited in scope (templates, numeric range). Without digit tokenization or MLM, models fail or underperform. Does not cover sorting, some count varieties, or complex multi-term expressions beyond template scope.",
            "evidence_for_mechanism": "ND training substantially speeds convergence on TD and improves numeric-answer performance; ND-LM-DT ablation (wordpiece tokenization) fails to learn ND tasks, demonstrating DT is crucial for ND usefulness.",
            "counterexamples_or_challenges": "ND alone cannot teach integration with natural language; ND-trained model still struggles on multi-term (&gt;3) arithmetic generalization and tasks not expressed in the templates.",
            "uuid": "e8384.3",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "TD (Textual Data)",
            "name_full": "TD: Synthetic Textual Data pre-training dataset",
            "brief_description": "Automatically-generated question-passage pairs (2.5M train, 10k val) using templates extracted from math word problems to teach how numeric computations are expressed in pseudo-natural language and to maintain alignment between textual context and numeric world states.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GENBERT pretraining dataset component (TD)",
            "model_description": "Passages composed by instantiating 12 frequent sentence templates with small domain vocabularies (history, NFL) plus 13 question templates across 7 skill types, generating numeric-world-state questions requiring selection, comparisons, differences, sums, superlatives, etc.",
            "arithmetic_task_type": "In-text selection, intra/inter-entity differences, subset counts, superlatives, sums across containers — arithmetic combined with entity tracking.",
            "mechanism_or_representation": "Connects numeric world state and textual events, letting the encoder learn to map linguistic descriptions of changes to numeric state updates and then allow decoder to generate numeric answers token-by-token.",
            "probing_or_intervention_method": "Pretraining on TD alone (GENBERT+TD) and combined with ND (GENBERT+ND+TD); zero-shot evaluation on MWP; ablations to measure ND impact on TD learning speed.",
            "performance_metrics": "GENBERT+TD achieves &gt;96% on TD evaluation; improves DROP EM to ~64.4 when used alone; when combined with ND gives best overall drop performance (68.8 EM). Zero-shot MWP improvement modest compared to ND but TD improves robustness for text-conditioned numeric reasoning.",
            "error_types_or_failure_modes": "TD lacks coverage for some numeric operations and ranges; models pretrained only on TD converge slower and obtain lower numeric-only generalization than with ND; fails on operations not captured by templates.",
            "evidence_for_mechanism": "GENBERT trained first on ND converged faster on TD — supports shared underlying numeric skills; high TD eval accuracy indicates the model learned to map textual templates to numeric computations.",
            "counterexamples_or_challenges": "TD alone insufficient for short-context numeric templates and numeric token learning; must be combined with ND and MLM to avoid catastrophic forgetting and to learn digit-level numeracy effectively.",
            "uuid": "e8384.4",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "DT & RS",
            "name_full": "Digit Tokenization (DT) and Random Shift (RS) interventions",
            "brief_description": "Two training/representation interventions: DT tokenizes numbers digit-by-digit to improve numeric learning; RS randomly shifts positional embeddings to avoid overfitting to absolute positions on short numeric inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pretraining interventions: DT (digit tokenization) & RS (random shift)",
            "model_description": "DT: split numeric tokens into individual digit wordpieces (e.g., '517.4' -&gt; '5','1','7','.','4'); RS: randomize position IDs within 0..512-n to reduce reliance on absolute positions for short inputs during ND training.",
            "arithmetic_task_type": "Support for any numeric tasks by improving internal numeric representation and positional generalization.",
            "mechanism_or_representation": "DT makes value composition easier by exposing digit-level structure to Transformer; RS encourages position-invariant learning so numeric computation generalizes across input positions.",
            "probing_or_intervention_method": "Ablation experiments: ND-LM-DT (no DT) failed to learn ND tasks; ND-LM-RS (no RS) reduced performance on short inputs; comparisons of variants shown in learning curves and downstream metrics.",
            "performance_metrics": "DT crucial: ND-LM-DT failed to learn ND (learning curves in Fig.4); RS helpful: variants without RS/MLM had lower DROP performance. Quantitatively, inclusion of DT enabled learning &gt;96% on ND; excluding DT prevented learning.",
            "error_types_or_failure_modes": "Without DT numeric tokenization learning fails (poor sample complexity); without RS the model can overfit positional patterns of short ND examples and generalize worse.",
            "evidence_for_mechanism": "Empirical ablation: ND-LM-DT does not learn ND templates; ND-LM-RS and ND-LM (no MLM) have lower downstream DROP scores, showing DT and RS materially affect capacity to learn arithmetic from synthetic inputs.",
            "counterexamples_or_challenges": "DT/RS are helpful but do not solve limitations in arithmetic generalization (e.g., &gt;3 terms) or coverage of non-templated reasoning operations; digit-tokenization increases vocabulary/sequence length tradeoffs.",
            "uuid": "e8384.5",
            "source_info": {
                "paper_title": "Injecting Numerical Reasoning Skills into Language Models",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do nlp models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Giving bert a calculator: Finding operations and arguments with reading comprehension",
            "rating": 2
        },
        {
            "paper_title": "A multi-type multi-span network for reading comprehension that requires discrete reasoning",
            "rating": 2
        },
        {
            "paper_title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
            "rating": 1
        },
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 1
        }
    ],
    "cost": 0.01505025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Injecting Numerical Reasoning Skills into Language Models</h1>
<p>Mor Geva*<br>Tel Aviv University, Allen Institute for AI<br>morgeva@mail.tau.ac.il</p>
<p>Ankit Gupta*<br>Tel Aviv University<br>ankitgupta.iitkanpur@gmail.com</p>
<p>Jonathan Berant<br>Tel Aviv University, Allen Institute for AI<br>joberant@cs.tau.ac.il</p>
<h4>Abstract</h4>
<p>Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP ( $49.3 \rightarrow 72.3 \mathrm{~F}_{1}$ ), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.</p>
<h2>1 Introduction</h2>
<p>Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our approach for injecting numerical skills into a pre-trained LM. (a) We add two pre-training steps over large amounts of synthetic numerical data (ND) and textual data (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQUAD).
text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage.</p>
<p>To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were designed for counting (but only until ' 9 ') and for addition and subtraction (but of 2-3 numbers only). Such models perform well on existing datasets, such as DROP (Dua et al., 2019), but do not generalize to unsupported computations, which will require modifying the model architecture. Moreover, current models marginalize at training time over all numerical expressions that evaluate to the correct answer. Since the number of such expressions grows exponentially, scaling these approaches to arbitrary computations entails using non-differentiable operations (sampling or computing top- $K$ numerical expressions), which can lead to training difficulties.</p>
<p>Passage: Taunton has four art galleries... Hughes/ Donahue Gallery founded in 2007, a local community gallery serving local Taunton artists... Art Euphoric founded in 2008 has both visual and craft exhibits...
Q1: How many years after founding of Hughes/ Donahue was Art Euphoric founded?
A1: 1 (number)
Q2: Which gallery was founded later, Hughes/ Donahue or Art Euphoric?
A2: Art Euphoric (span)
Table 1: Example passage from DROP, and two questions with different answer types.</p>
<p>In this work, we propose that reasoning skills, such as numerical reasoning, are amenable to automatic data generation. Hence, one can inject that skill directly into the model by adding additional pre-training steps, allowing the model to learn the skill in an end-to-end fashion. This results in a fully-differentiable training procedure over a standard and general-purpose architecture, where the output space can be easily controlled through the data generation procedure.</p>
<p>Specifically (Figure 1), we add to a large pre-trained LM two pre-training steps over automatically-generated synthetic data. First, we generate numerical data of the form $3+4+11=$ 18. Training on this data teaches the model to compute the value of numbers from their tokens and to perform numerical operations. Second, we automatically generate question-passage pairs that require numerical reasoning using a compact grammar (textual data). Training on this data endows the model with the ability to understand computations expressed in pseudo-natural language.</p>
<p>In both pre-training steps, the model, GENBERT, generates output numbers token-by-token. Thus, the model has a standard architecture, where an answer can either be extracted from the input question and passage or generated from a decoder. Pre-training is done in a multi-task setup with a standard LM objective, in order to avoid "catastrophic forgetting" (Kirkpatrick et al., 2017) of the linguistic information in the original LM. After pre-training, the model has sufficient language and numerical skills to be directly fine-tuned on a target numerical reasoning dataset, without resorting to specialized architectures. Augmenting more numerical skills does not require changing the model, only generating additional data.</p>
<p>We demonstrate the validity of our approach by a series of experiments showing that:
(a) GENBERT is able to solve pre-training tasks for numerical reasoning.
(b) Pre-training on these tasks provides GENBERT with 1) skills to reach performance that matches state-of-the-art models of comparable size on DROP (Dua et al., 2019), a standard numerical reasoning dataset, as well as 2) the ability to generalize to math word problem (MWP) datasets (Koncel-Kedziorski et al., 2016).
(c) GENBERT learns these numerical skills while maintaining high performance on SQuAD (Rajpurkar et al., 2016), a standard reading comprehension dataset.
(d) Initializing models for numerical reasoning with GENBERT's weights improves their original performance.
To conclude, in this work we address the problem of injecting LMs with numerical reasoning skills. Our contributions are:</p>
<ul>
<li>A method for injecting skills into pre-trained LMs, given that automatic data generation is possible.</li>
<li>GENBERT, an architecture for pre-trained LM with generative and extractive abilities.</li>
<li>A framework for generating numerical and textual synthetic data for numerical reasoning.
Our code and data can be downloaded from https://github.com/ag1988/ injecting_numeracy.</li>
</ul>
<h2>2 Numerical Reasoning Over Text</h2>
<p>Numerical reasoning over text (NRoT) is commonly set up as a reading comprehension (RC) task. Given a training set of question-context-answer triples $\left{\left(\mathbf{q}<em i="i">{i}, \mathbf{c}</em>$, or (b) a number that is the result of some computation (see examples in Table 1).}, a_{i}\right)\right}_{i=1}^{N}$, the goal is to learn a function that returns the answer $a$ to a question $\mathbf{q}$ given a context $\mathbf{c}$. However, in NRoT the answer generally requires to internally perform some numerical computation using the entities and numbers in the context. Specifically, the answer is either: (a) a span (or list of spans) from the context $\mathbf{c}$ or question $\mathbf{q</p>
<p>Two natural, yet opposing, approaches lend themselves to tackling NRoT: (a) A symbolic approach: a model can read the question and context, output a numerical expression and evaluate the answer with an external symbolic calculator. This approach is a particular case of semantic parsing (Kamath and Das, 2019), and was common in early NRoT datasets (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Hosseini et al., 2014). How-</p>
<p>ever, it suffers from several drawbacks. First, because numerical expressions are discrete and their space grows combinatorially, the model must learn to search in this space using non-differentiable operations, which are usually difficult to optimize. Second, numerical expressions are limited to numerical answers, while in DROP often a numerical computation is required but the final answer is a text span. (b) A distributed approach: have a model directly generate the answer given $(\mathbf{q}, \mathbf{c})$. When the answer is a text span, the model can extract it from the input, and when the answer is a number that is not in $\mathbf{q}$ or $\mathbf{c}$, the model must generate it. While this makes training straightforward, the model must learn to perform numerical computations from the relatively small target dataset. We empirically show in $\S 3$ that this leads to low performance in general.</p>
<p>As a compromise, most NRoT models (Dua et al., 2019; Kinley and Lin, 2019; Hu et al., 2019; Efrat et al., 2019) have taken a hybrid approach: they augment standard extractive QA models with specialized modules for handling a limited set of numerical computations. We briefly describe this architecture, as it is the basis for our model in $\S 3$.</p>
<p>Given a question with $n_{1}$ tokens $\mathbf{q}=$ $\left(q_{1}, \ldots, q_{n_{1}}\right)$ and a context with $n_{2}$ tokens $\mathbf{c}=$ $\left(c_{1}, \ldots, c_{n_{2}}\right)$, the hybrid model first computes contextualized representations for the $n_{1}+n_{2}+3$ tokens $\langle[\mathrm{CLS}] \mathbf{q}[\mathrm{SEP}] \mathbf{c}[\mathrm{SEP}]\rangle$ using a pretrained LM, such as BERT (Devlin et al., 2019):</p>
<p>$$
\mathbf{L}=\mathbf{L M}(\mathbf{q}, \mathbf{c})
$$</p>
<p>The representations $\mathbf{L}$ are then passed to multiple heads, which are small neural networks that estimate $p(a \mid \mathbf{q}, \mathbf{c}, h)$, that is, the probability of the answer given the input and conditioned on a head $h$, corresponding to a particular answer type:</p>
<ul>
<li>Context span head: computes a distribution over all spans in the context using a feed-forward network (FFN) $\mathbf{F F}_{\mathbf{c}}(\mathbf{L})$.</li>
<li>Question span head: computes a distribution over spans in the question using a FFN $\mathbf{F F}_{\mathbf{q}}(\mathbf{L})$.</li>
<li>Count head: computes a distribution over the numbers ${0, \ldots, 9}$ using a FFN $\mathbf{F F}_{\mathbf{c n t}}(\mathbf{L})$.</li>
<li>Arithmetic head: computes a distribution over all signed combinations of numbers in the context using a FFN $\mathbf{F F}_{\mathbf{c m b}}(\mathbf{L})$ (the numbers in the context are identified in a pre-processing step).
While the first two heads are standard in extractive QA, the latter two heads are specialized and meant to handle answers that do not appear in the input.</li>
</ul>
<p>Finally, for deciding which answer head to use for a given input, a type head $\mathbf{F F}<em _head="{head" _text="\text">{\text {typ }}(\mathbf{L})$ outputs a probability distribution $p</em>, h)$.}}(h \mid \mathbf{q}, \mathbf{c})$ (using a FFN). Thus the model probability for an answer is $p(a \mid \mathbf{q}, \mathbf{c})=\sum_{h \in \text { heads }} p_{\text {head }}(\mathbf{h} \mid \mathbf{c}, \mathbf{q}) \cdot p(a \mid \mathbf{c}, \mathbf{q</p>
<p>Training is done by enumerating all of the ways in which the answer can be obtained using all of the heads, and maximizing this marginal probability.</p>
<p>While existing models perform well on DROP, the aforementioned architecture is not flexible. First, the output space is severely constrained the model can only count up to ' 9 ', and numerical computations are restricted to signed combinations of a few numbers. Second, expanding the space of supported numerical computations is non-trivial, because training involves marginalizing over all expressions that lead to the correct answer. Since the space of numerical expressions grows exponentially, expanding this space quickly leads to a difficult search problem. Third, delegating numerical computations to an external symbolic calculator leads to modeling challenges, since there could be interactions between text and numerical computation: Consider the DROP question "How many total yards did Phil Dawson throw for touchdowns?". Current models handle such questions by computing a sum from numbers in the text and returning the result. However, if the question was "Who threw 45 total yards for touchdowns?", the model would have to compute the sum internally, and then find the relevant span in the text. This is impossible when the computation itself is delegated to an external calculator. Thus, training models to handle such numerical questions is desirable.</p>
<p>Motivated by the above arguments, we wish to push the frontier of end-to-end differentiable models for numerical reasoning. Thus, we will automatically generate large amounts of data that endow a pre-trained LM with numerical skills.</p>
<h2>3 GENBERT: A BERT-based Model for Generating Arbitrary Outputs</h2>
<p>We now describe a simple BERT-based generative model that performs numerical computations internally, termed GENBERT. The model combines the Transformer encoder-decoder architecture (Vaswani et al., 2017) with a pre-trained LM, specifically, BERT.</p>
<p>Our architecture is illustrated in Figure 2. Our encoder is a standard Transformer, initialized with</p>
<p>BERT weights. To also enjoy BERT's representations at decoding time, we tie the weights of the decoder and the encoder. Because the Transformer decoder has source attention weights (weights for attending to the encoder representations at decoding time) that are not present in BERT, we tie these source-attention weights to the self-attention weights of the encoder (which are tied to the selfattention weights of the decoder). This fully initializes the Transformer model with BERT weights.</p>
<p>Since the encoder and decoder weights are tied, we make them learn distinct representations by adding a FFN $\mathbf{F F}<em _enc="{enc" _text="\text">{\text {enc }}$ that transforms the encoder contextualized representations $\mathbf{L}</em>$ as}</p>
<p>$$
\mathbf{H}<em _enc="{enc" _text="\text">{\text {enc }}=\text { layer-norm }\left(\operatorname{gelu}\left(W \cdot \mathbf{L}</em>\right)\right)
$$}</p>
<p>where $W$ is a parameter matrix (Hendrycks and Gimpel, 2016; Ba et al., 2016). Analogously, we add $\mathbf{F F}<em 1="1">{\text {dec }}$ to the decoder. To further distinguish the encoder and decoder, we use distinct start and end tokens for input and output sequences. Given $m$ answer tokens $\mathbf{a}=$ $\left(a</em>}, \ldots, a_{m}\right)$, we form an output sequence with $m+2$ tokens: $\langle[$ SOS] a [EOS] $\rangle$. The output tokens are passed through the decoder and $\mathbf{F F<em _dec="{dec" _text="\text">{\text {dec }}$ to obtain $\mathbf{H}</em>$.}</p>
<p>Finally, the probability of an answer is defined in the usual manner: Let $\langle\mathbf{a}\rangle=\left(a_{0} \cdots a_{m+1}\right)$ be the output sequence. The decoder outputs the probability $p_{\text {dec }}\left(a_{i+1} \mid a_{0}, . . a_{i}, \mathbf{c}, \mathbf{q}\right)$, and the probability of an answer is:</p>
<p>$$
p_{\mathrm{dec}}(\langle\mathbf{a}\rangle \mid \mathbf{c}, \mathbf{q})=\prod_{i=0}^{m} p_{\mathrm{dec}}\left(a_{i+1} \mid a_{0}, . . a_{i}, \mathbf{c}, \mathbf{q}\right)
$$</p>
<p>As we have a generative model, we can remove the specialized count and arithmetic heads from $\S 2$. Thus, the type head $\mathbf{F F}<em _enc="{enc" _text="\text">{\text {typ }}\left(\mathbf{H}</em>\right)$ over the context span, question span, and decoder heads.}}\right)$ outputs a distribution $\left(p_{\mathbf{q}}, p_{\mathbf{c}}, p_{\text {dec }</p>
<p>To improve pre-training on the numeric data (§4), we make two additional modifications.</p>
<p>Digit Tokenization (DT) Conventional wordpiece tokenization treats numbers no differently than any other token. However, computing the value of numbers should be simpler when using digits directly (Wallace et al., 2019). Hence, we tokenize numbers digit-by-digit. For example, a wordpiece ## $d_{1} \cdots d_{k}$ where $d_{i} \in{0, \ldots, 9}$ is further split into ## $d_{1}, \ldots, # # d_{k}$. We show in $\S 5.1$ that this substantially improves sample complexity when training to perform numerical operations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GenBERT's network architecture: (a) a high-level overview of the network, including a generative head (red), two span-extraction heads (yellow), and an answer type head. (b) a closer overview of GENBERT's generative head.</p>
<p>Random Shift (RS) The original Transformer uses absolute positional embeddings for each token. However, in $\S 4$, we train on short inputs such as "1086.1 - $2.54+343.8$ ". Thus, the model can potentially over-fit and learn to perform numerical reasoning only when numbers are at the beginning of an input. To prevent this, when the input length $n_{1}+n_{2}+3&lt;512$, we shift all position IDs by a random integer in $(0,1, \ldots, 512-\left(n_{1}+n_{2}+3\right))$.</p>
<p>Training For each span $(i, j)$, a span extraction head $h$ outputs its probability $p_{h}((i, j) \mid \mathbf{c}, \mathbf{q}, h)$ of being the answer. Let $S$ be the set of spans in the input corresponding to the gold answer. The model loss $\mathbf{L}<em _dec="{dec" _text="\text">{\text {model }}$ marginalizes over all ways in which the answer can be predicted:
$-\log \left(p</em>(i, j)\right)$,
where conditionals have been dropped for brevity.
To evaluate the ability of GENBERT to perform numerical reasoning, we initialize it with BERT and fine-tune it on DROP. GENBERT obtains 46.1 EM and $49.3 \mathrm{~F}_{1}$, roughly 20 points lower than prior models. Thus, we conclude that acquiring numerical reasoning skills from DROP data only is difficult. To remedy this, we will automatically generate training data that will endow GENBERT with numerical skills before training it on DROP.}} \cdot p_{\text {dec }}(\langle\mathbf{a}\rangle)+\sum_{\mathbf{h} \in \mathbf{q}, \mathbf{c}} p_{\mathbf{h}} \cdot \sum_{(i, j) \in S} p_{h</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Template extraction and instantiation. A template (in red) is extracted from a MWP sentence, using categories for containers, entities, verbs, attributes and numbers, according to Hosseini et al. (2014). For generation, the categories are instantiated with a domain-specific vocabulary.</p>
<h2>4 Pre-training Tasks for Numerical Skills</h2>
<p>We now describe two automatically-generated datasets and the multi-task training procedure.</p>
<h3>4.1 Generating Numerical Data (ND)</h3>
<p>Our first dataset focuses on learning numerical values expressed by tokens and computing numerical operations, i.e., it does not involve textual content. As such, it is easy to craft templates that correspond to various numeric operations. We designed six such templates, described in Table 2. Each template consists of an expression to evaluate and its solution. Further details on their instantiation are provided in $\S$ A.1. While the numerical operations were chosen based on DROP, it is trivial to extend them to other domains (Saxton et al., 2019) with different numerical operations.</p>
<h3>4.2 Generating Textual Data (TD)</h3>
<p>Numeric data is easy to generate, since it does not contain any textual context. However, to tackle NRoT, a model needs to comprehend how numerical operations are expressed in text that refers to events, entities and quantities. This primes us to generate textual data from a simple grammar.</p>
<p>While text generation is hard in the general case, we are specifically interested in text that focuses on number manipulations. Therefore, we use the framework of Hosseini et al. (2014), who proposed to model math word problems with a simple structure. In their framework a world state consists of entities, which are objects that are being counted, and containers, which are objects that own entities. Sentences use verb categories to describe how the number of entities in a container changes, and thus a world state can be updated given a sentence.</p>
<p>Consider the textual example in Figure 1. the entities are soldiers and citizens, and the containers are the king and the commander. The verbs ("had" and "received") describe the entities the king holds, and how many were passed to the commander.</p>
<p>In this work, we use this framework to automatically generate examples. We extract templates that describe changes in the number of entities owned by containers, and automatically generate questioncontext pairs from these templates.</p>
<p>Template extraction To extract templates, we go over sentences from the corpus provided by Hosseini et al. (2014). For each sentence, we use a procedure described by Hosseini et al. (2014) to abstract its tokens to the following categories: numbers (NUM), entities (ENT), containers (CONT) and attributes (ATTR). In addition, verbs are abstracted to six categories, each corresponding to a different change in the number of entities owned by containers. Thus, each template fully specifies how to update a world state, i.e., the number of entities each container owns. The top part of Figure 3 illustrates the abstraction process. Finally, we count for each extracted template its frequency in the data, and use the top-12 templates for passage generation. Details on the abstraction process, categories used, and extracted templates are in $\S$ A.2.</p>
<p>Passage generation Using the extracted templates, we can generate sentences and maintain a world state of all containers and the number of entities they own. We construct a small vocabulary ( $&lt;100$ words) that maps categories to domainspecific words, and use the following procedure to generate passages.</p>
<p>We sample 3-6 templates with replacement, and instantiate them one-by-one (the bottom part of Figure 3 illustrates instantiation). Each template is instantiated by uniformly sampling values from the vocabulary with probability $1-p$ and from previously generated sentences with probability $p$. To avoid a collection of unrelated sentences, we set the probability of using previously used values to $p=0.7$. An example passage is shown in Table 3.</p>
<p>Question generation After generating a passage, the world state holds information about all containers in the passage and the number of entities they hold. In Table 3, the state will include the number of families and rebels of different nationalities in each container (the commander, the householder, and the countries). Based on this world state, numerical reasoning questions can be asked.</p>
<p>To create questions, we craft 13 question templates that are instantiated with objects from the world state. The questions teach the model to track events and perform numeric and discrete operations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation</th>
<th style="text-align: right;">Template</th>
<th style="text-align: left;">Example instantiation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">signed float combination</td>
<td style="text-align: right;">$s_{1} f_{1} s_{2} f_{2} s_{3} f_{3} s_{4} f_{4}$</td>
<td style="text-align: left;">$517.4-17484-10071.75*1013.21$</td>
</tr>
<tr>
<td style="text-align: left;">min/max/avg</td>
<td style="text-align: right;">$o\left(f_{1}, f_{2}, f_{3}, f_{4}\right)$</td>
<td style="text-align: left;">largest(13.42, 115.5, 72.76)</td>
</tr>
<tr>
<td style="text-align: left;">arg max, arg min</td>
<td style="text-align: right;">$\arg \left(w_{1} f_{1}, w_{2} f_{2}, w_{3} f_{3}, w_{4} f_{4}\right)$</td>
<td style="text-align: left;">arg min(highish 137.1, sightliness 43.2)</td>
</tr>
<tr>
<td style="text-align: left;">date min/max</td>
<td style="text-align: right;">$d s u p\left(d_{1}, d_{2}, d_{3}, d_{4}\right)$</td>
<td style="text-align: left;">oldest(June 04, 959; 01 May 959)</td>
</tr>
<tr>
<td style="text-align: left;">date difference</td>
<td style="text-align: right;">diff in $\operatorname{prd}\left(d_{1}, d_{2}\right)$</td>
<td style="text-align: left;">diff in days(05 April 112; June 01, 112)</td>
</tr>
<tr>
<td style="text-align: left;">percentage</td>
<td style="text-align: right;">$\operatorname{peent} w:: w_{1} p_{1} \%, w_{2} p_{2} \%, w_{3} p_{3} \%, w_{4} p_{4} \%$</td>
<td style="text-align: left;">percent not sunbed :: sunbird $33.2 \%$, defector</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;">$60.77 \%$, molehill $6.03 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Templates for generating synthetic numerical examples and the numerical operations required to answer them. Domains (defined in App. A.1): $s_{i} \in{-,+}, f_{i} \in \mathbb{R}^{+}, o \in \mathcal{O}$ : superlative words like "longest", arg $\in{\arg \min , \arg \max }$, $w_{i} \in \mathcal{W}$ : words from NTLK Words Corpus, $d_{i} \in \mathcal{D}$ : dates until Sep 2019, $d s u p \in \mathcal{D S U P}$ : superlative words like "latest", $\operatorname{prd} \in{$ "days", "months", "years" $}, p_{i} \in(0,100)$, $\operatorname{peent} \in{$ "percent", "percent not" $}$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">P</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">commander</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="mi">1949</span><span class="w"> </span><span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">householder</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="mi">1996</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span>
<span class="n">There</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="mi">10913</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">rebels</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="mi">77</span><span class="w"> </span><span class="n">Chinese</span><span class="w"> </span><span class="n">families</span>
<span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">.</span><span class="w"> </span><span class="mi">6641</span><span class="w"> </span><span class="n">British</span><span class="w"> </span><span class="n">soldiers</span><span class="o">,</span><span class="w"> </span><span class="mi">476</span><span class="w"> </span><span class="n">asian</span><span class="w"> </span><span class="n">rebels</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="mi">338</span>
<span class="n">Germans</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">recruited</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Russia</span><span class="o">.</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">1996</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">Japanese</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Spain</span><span class="w"> </span><span class="n">than</span>
<span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">47</span><span class="w"> </span><span class="o">(</span><span class="mi">1996</span><span class="o">-</span><span class="mi">1949</span><span class="o">)</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">How</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">families</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Spain</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">Polish</span><span class="w"> </span><span class="n">families</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mi">2073</span><span class="w"> </span><span class="o">(</span><span class="mi">4022</span><span class="o">-</span><span class="mi">1949</span><span class="o">)</span>
</code></pre></div>

<p>Table 3: An example synthetic passage (P) and questions. Questions $(\mathrm{Q})$ were generated from templates and answers $(\mathrm{A})$ were calculated based on the world state.</p>
<p>Examples for generated questions are shown in Table 3, where answers are computed from the world state. Overall, we create 13 question templates for 7 different "skills", provided in §A.2.</p>
<h3>4.3 Training GeNBERt on Synthetic Data</h3>
<p>For pre-training on ND, we generated 1M examples for training and 10 K for validation. For TD, we generated 2.5 M examples for training and 10 K for validation. For both synthetic datasets, we used the GENBERt model loss, $\mathbf{L}<em _enc="{enc" _text="\text">{\text {model }}$, from $\S 3$. To ensure that the model does not lose its language understanding abilities, we employ a multi-task setup, and include a standard masked $L M$ objective from BERT. Specifically, given a masked token sequence $\langle\mathbf{m}\rangle$, we compute the contextualized representations, $\mathbf{L}</em>}}$ and pass them through a feedforward network $\mathbf{F} \mathbf{F<em i="i">{\text {mlm }}$. For each masked index $i$, it outputs the probability $p\left(a</em>$. The MLM loss is computed as
$\mathbf{L}} \mid i,\langle\mathbf{m}\rangle\right)$ of the original token $a_{i<em _="{" _in="\in" _text="\text" i="i" masked="masked">{\text {mlm }}\left(\langle\mathbf{m}\rangle\right)=\operatorname{mean}</em>\rangle\right)\right)$.
Details about the MLM data are in §A.3.
During training, we sample mini-batches from the respective datasets, and minimize the weighted sum of the losses. Concretely, while pre-training on ND and TD, we sample mini-batches $X_{\mathrm{ND}}, X_{\mathrm{TD}}$ and $X_{\text {MLM }}$ and optimize the objective
$\mathbf{L}}}-\log \left(p\left(a_{i} \mid i,\langle\mathbf{m<em _mathrm_ND="\mathrm{ND">{\text {model }}\left(X</em>}}\right)+\mathbf{L<em _mathrm_TD="\mathrm{TD">{\text {model }}\left(X</em>}}\right)+\lambda \cdot \mathbf{L<em _MLM="{MLM" _text="\text">{\text {mlm }}\left(X</em>\right)$.
}<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Progression of eval accuracy (EM) of GENBERt, for different pre-training settings listed in $\S 5.1$.</p>
<h2>5 Experimental Evaluation</h2>
<p>We now evaluate our two pre-training steps and their applicability for numerical reasoning tasks. We consider the following variants, aiming to investigate the contributions of ND and TD, the importance of MLM loss, and techniques like DT and RS. In all cases, we initialize GENBERt with BERT-base-uncased, use DT and RS, and include the MLM loss, except where noted:</p>
<ul>
<li>GENBER $\mathrm{T}_{+ \text {ND }}$ : trained on numerical data.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM }}$ : trained on ND without the additional MLM loss.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM-DT }}$ : trained on ND using wordpiece tokenization, without the MLM loss.</li>
<li>GENBER $\mathrm{T}_{+ \text {ND-LM-RS }}$ : trained on ND without MLM loss and random shift (RS).</li>
<li>GENBER $\mathrm{T}_{+ \text {TD }}$ : trained on textual data (TD).</li>
<li>GENBER $\mathrm{T}<em _="+" _ND="{ND" _text="\text">{+ \text {ND+TD }}$ : GENBER $\mathrm{T}</em>$ trained on both ND and TD.}</li>
</ul>
<h3>5.1 Pre-training Performance</h3>
<p>We first ask whether the pre-training procedure allows GENBERt to absorb the intended numerical skills. We observe that across various settings (ND, TD, ND+TD), GENBERt consistently achieves more than $96 \%$ accuracy in predicting the correct solution for both ND and TD. Thus, we conclude that indeed a pre-trained LM can learn the designed skills from generated data.</p>
<p>Figure 4 shows the learning curves of GENBERT for the different variants. Note that in ND-LM-DT the model does not learn to solve the numerical data task. This demonstrates the utility of using DT over conventional wordpieces. The lower sample complexity in the case of ND+TD compared to the only-TD can be attributed to the fact that ND and TD share some numeric skills and hence a model already trained on ND converges faster on TD compared to GENBERT.</p>
<h3>5.2 Numerical Reasoning Performance</h3>
<p>After successfully injecting GENBERT with numeric skills, we test GENBERT guided by the following questions:
(a) Are the injected skills robust and generalize to NRoT datasets like DROP?
(b) Are the new skills learned at the expense of the model's ability to understand language?
(c) Can the pre-trained weights be used with architectures other than GENBERT?
For (a), we fine-tune GENBERT on DROP and further evaluate on MWP in a zero-shot setup. For (b), we evaluate GENBERT on a RC task that does not involve numerical reasoning, namely, SQuAD (Rajpurkar et al., 2016). For (c), we use GENBERT encoder as a drop-in replacement for BERT on two other architectures.</p>
<p>Results on DROP We report results of GENBERT initialized by BERT-base and leave pretraining a larger model for future work. We compare GENBERT to MTMSN (Hu et al., 2019) initialized with BERT-base, as MTMSN initialized with BERT-large is a state-of-the-art model on DROP. ${ }^{1}$</p>
<p>Table 4 presents fine-tuning results on DROP. Without pre-training, GENBERT performs poorly compared to current state of the art models like MTMSN, reporting an EM of only 46.1. Pretraining on each of the numerical data (ND) and textual data (TD) improves performance dramatically to 64.7 EM and 64.4 EM, respectively. Moreover, pre-training on both ND and TD leads to a performance of 68.8 EM , on par with MTMSN's 68.2 EM. This demonstrates that the skills that GENBERT learns from ND and TD are complementary. In addition, the lower performance of GENBERT ${ }<em D-L="D-L" M-R="M-R" S="S" _N="+N">{+N D-L M}$ and GENBERT ${ }</em>$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Performance of GENBERT and comparable models on the development and test sets of DROP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">number</th>
<th style="text-align: center;">span</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">spans</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GENBERT</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D-L M-R S}$</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D-L M}$</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT ${ }_{+N D}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 4}$</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT $_{+T D}$</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT $_{+N D+T D}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 0}$</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">$\mathbf{5 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: $\mathrm{F}_{1}$ scores on DROP development per answer type.
shows the importance of including the MLM loss and the utility of RS for short inputs.</p>
<p>Breaking down performance by answer type (Table 5) highlights several points. First, pre-training on ND and TD improves performance mostly due to number answer types, as expected. Second, GENBERT ${ }<em _BASE="{BASE" _text="\text">{+N D+T D}$ outperforms MTMSN ${ }</em>$ substantially outperforms GENBERT on questions whose answer is a list of non-contiguous spans. This is expected, as MTMSN has a specialized head and procedure for handling such questions, while build on a simpler and more standard RC architecture.}}$ on questions whose answer is a span. We argue a probable cause for this are span questions that require performing a numerical computation internally, as explained in §2. Third, MTMSN ${ }_{\text {BASE }</p>
<p>Generalization to MWP (zero-shot) The MAWPS repository is a collection of math word problem (MWP) datasets (Koncel-Kedziorski et al., 2016). To test the models on skills they were trained on, we picked datasets with addition and subtraction problems, and filtered out examples with other operations (e.g., multiplication and division). All models that were fine-tuned on DROP were evaluated in a zero-shot setup on 395 examples from ADDSub (Hosseini et al., 2014), 321 from SOP (Roy et al., 2015), and 305 from SEQ (Koncel-Kedziorski et al., 2015).</p>
<p>Results are shown in Table 6. Overall, GENBERT $<em D="D" _N="+N">{+N D+T D}$ dramatically improves performance compared to GENBERT. GENBERT $</em>$, demonstrating the utility of ND when the context is short.}$ performs much better than GENBERT $_{+T D</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ADDSub</th>
<th style="text-align: center;">SOp</th>
<th style="text-align: center;">SEQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GENBERT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +TD</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND+TD</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">$\mathbf{2 8 . 3}$</td>
<td style="text-align: center;">22.3</td>
</tr>
<tr>
<td style="text-align: left;">NABERT+</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">17.4</td>
</tr>
<tr>
<td style="text-align: left;">MTMSN BASE</td>
<td style="text-align: center;">$\mathbf{3 2 . 2}$</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">$\mathbf{3 2 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 6: EM on MWP datasets.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Breakdown of model accuracy (EM) by the number of terms in the arithmetic expression, for the MWP datasets ADDSub, SOp and SEQ.</p>
<p>Last, MTMSN outperforms GENBERT $+_{\text {ND+TD }}$. However, MTMSN uses a specialized architecture for addition and subtraction, suitable when calculations are done outside of the model. GENBERT, on the other hand, is a general-purpose generative model, that can also return span answers when the computation is done internally.</p>
<p>Next, we break down performance by the number of terms in the arithmetic expression (Figure 5). The plot shows that all models struggle to generalize to more complex problems, and completely fail when the calculation involves more than 3 terms. Interestingly, the drop in performance of GENBERT +ND +TD between 2 and 3 terms is significantly smaller than that of GENBERT +ND and GENBERT +TD . This suggests that both ND and TD are useful for improving robustness.</p>
<p>Error analysis To understand the limitations of our method, we analyze the errors of GENBERT +ND +TD on the development set of DROP, excluding questions with a multi-span answer which are not supported by the model. We sample 100 random examples for which GENBERT +ND +TD fails to predict the correct answer and manually analyze the types of questions and mistakes done by the model.</p>
<p>We find that in almost half of the cases (43\%), the example requires reasoning skills that are either not covered by the pre-training tasks (e.g. sorting), or not numerical. Another common case (23\%) is inaccurate predictions, such as spans that are too</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">EM</th>
<th style="text-align: left;">$\mathrm{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: left;">$\mathbf{8 8 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND-LM</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">85.8</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">88.1</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +TD</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">88.2</td>
</tr>
<tr>
<td style="text-align: left;">GENBERT +ND +TD</td>
<td style="text-align: left;">$\mathbf{8 1 . 3}$</td>
<td style="text-align: left;">$\mathbf{8 8 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance on SQuAD v1 development set. Scores for BERT are using wordpiece tokenization.
long and numbers with partial digit match to the gold answer. We note that many of these errors can be addressed by extending the pre-training tasks to cover additional numerical skills and a larger number range. We leave such extensions for future work. Further details and example failure cases are provided in §A.5.</p>
<h3>5.3 Reading Comprehension Performance</h3>
<p>Having shown that our models successfully learned to perform NRoT, we investigate if this improvement comes at the expense of performance on RC datasets. We initialize the RC model from Devlin et al. (2019) with GENBERT weights (encoder only) and fine-tune it on SQUAD v1. As shown in Table 7, the performance of GENBERT +ND +TD is almost identical to the original BERT. Moreover, GENBERT +ND-LM reported a loss of 3 EM points highlighting the importance of using the MLM loss.</p>
<h3>5.4 GENBERT With Other Architectures</h3>
<p>To further establish the utility of GENBERT, we used the weights of GENBERT +ND +TD to initialize the encoder of NABERT+ and MS-TAG, a recent multi-span tagging model of Efrat et al. (2019). Fine-tuning on DROP shows an improvement of $\sim 2$ EM points compared to the originally reported performance: $63.0 \rightarrow 65.1$ EM for NABERT+, and $67.3 \rightarrow 69.3$ EM for MS-TAG. This shows that GENBERT can be used as a drop-in replacement for BERT, when numerical reasoning is needed.</p>
<p>To summarize, we have empirically shown that one can inject numerical reasoning skills into a pre-trained LM, resulting in good performance on DROP, generalization to MWP, while maintaining high performance on standard RC datasets. Moreover, the resulting weights can be used for initializing numerical reasoning models.</p>
<h2>6 Related Work</h2>
<p>Most NRoT models designed for DROP are extractive QA models augmented with specialized modules (§2). Two recent work (Andor et al., 2019;</p>
<p>Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model.</p>
<p>A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020).</p>
<p>Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever.</p>
<h2>7 Conclusions</h2>
<p>Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limited flexibility. In this work, we propose a general method for injecting additional skills into LMs, assuming automatic data generation is possible. We apply our approach to the task of numerical reasoning over text, using a general-purpose model called GENBERT, and a simple framework for generating large amounts of synthetic examples. Our experiments demonstrate the effectiveness of our method, showing that GENBERT successfully learns the numerical skills, and performs on par with state-of-the-art NRoT models of the same size.</p>
<h2>Acknowledgments</h2>
<p>We thank Daniel Andor and Thang Luong for helpful discussions, and Shimi Salant for constructive suggestions. This research was partially supported by The Israel Science Foundation grant 942/16, The Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800).</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In North American Association for Com-
putational Linguistics (NAACL), pages 2357-2367, Minneapolis, Minnesota.</p>
<p>Daniel Andor, Emily Pitler, Kenton Lee, and Luheng He. 2019. Giving bert a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations (ICLR).</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL), pages 4171-4186, Minneapolis, Minnesota.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Association for Computational Linguistics (NAACL).</p>
<p>Avia Efrat, Elad Segal, and Mor Shoham. 2019. Tagbased multi-span extraction in reading comprehension. arXiv preprint arXiv:1909.13375.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.</p>
<p>John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word representations. In North American Association for Computational Linguistics (NAACL), pages 4129-4138.
M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Empirical Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. Automated Knowledge Base Construction (AKBC).</p>
<p>Jambay Kinley and Raymond Lin. 2019. NABERT+: Improving Numerical Reasoning in Reading Comprehension.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. In Transactions of the Association for Computational Linguistics (TACL), volume 3, pages 585597. MIT Press.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Association for Computational Linguistics (ACL), pages 1152-1157, San Diego, California.</p>
<p>Guillaume Lample and FranÃğois Charton. 2020. Deep learning for symbolic mathematics. In International Conference on Learning Representations (ICLR).</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Computational Linguistics (ACL).</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361, Hong Kong, China. Association for Computational Linguistics.
S. Roy, T. Vieira, and D. Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics (TACL), 1 .</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Empirical Methods in Natural Language Processing (EMNLP), pages 17431752.</p>
<p>Ohad Rozen, Vered Shwartz, Roee Aharoni, and Ido Dagan. 2019. Diversify your datasets: Analyzing generalization via controlled variance in adversarial datasets. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 196-205, Hong Kong, China. Association for Computational Linguistics.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations (ICLR).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), pages 5998-6008.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? probing numeracy in embeddings. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.</p>
<h2>A Supplemental Material</h2>
<h2>A. 1 Synthetic Numerical Data Generation</h2>
<p>We briefly describe the numerical templates, providing the details missing from Table 2. In all cases, integers are sampled from ${0, \ldots, 20 K}$, and split into disjoint train and development sets to assure generalization.</p>
<ul>
<li>signed float combination : Random signed combinations of up to 4 floats. Floats are sampled from the set of floats with two decimal places.</li>
<li>min/max/avg : We sample 2-4 floats and apply a min, max, avg, operation by sampling a word from the set $\mathcal{O}=$ {"longest", "last", "highest", "largest", "most", "shortest", "first", "smallest", "lowest", "least", "average"}.</li>
<li>$\arg \max , \arg \min$ : We sample word-float pairs, where words are sampled from $\mathcal{W}$ : words in the NLTK Words Corpus ${ }^{2}$ having at most 2 wordpieces, and floats are sampled as above.</li>
<li>date max/min : Same as min/max/avg above, but for dates. Dates are sampled from $\mathcal{D}$ : the set of dates until Sep 2019. The operator word is sampled from $\mathcal{D} \mathcal{S} \mathcal{U} \mathcal{P}=$ {"last", "latest", "most recent", "youngest", "first", "earliest", "oldest", "least recent"} and mapped to min or max.</li>
<li>date difference : This teaches our model to perform date arithmetic in days, months and years.</li>
<li>percentage : We teach our model to perform $100-x$ operations in the context of percentages. Given a number of arguments, we sample a percentage split using a flat Dirichlet distribution.</li>
</ul>
<h2>A. 2 Synthetic Textual Data Generation</h2>
<h2>A.2.1 Sentence template extraction</h2>
<p>To extract sentence templates, we abstract the text of math word problems from the corpus published by Hosseini et al. (2014). Going over examples, we split the problem text into sentences ${ }^{3}$, and abstract the tokens of each sentence independently. Tokens are abstracted according to the framework into numbers (NUM), verb categories (VERB), entities (ENT), containers (CONT) and attributes (ATTR).</p>
<p>To have a better control over the generation process, we extend the framework of Hosseini et al. (2014) to support two container types - agent (AGT) and environment (ENV). Agents are objects which actively collect and drop entities, for example a person or an organization. Environments are passive containers, such as places or time periods. In addi-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion, we introduce two-level containers to express inclusion relation between containers. For instance, if 3 submarines anchor near the city of Devonport, then they also anchor near the country of England.</p>
<p>The 12 most common extracted sentence templates, which were used for generating synthetic data, are provided in Table 8.</p>
<h2>A.2.2 Template instantiation</h2>
<p>Sentence templates are instantiated with a small vocabulary, that map categories into words. In this work, we construct two domain-specific smallworld vocabularies, about history and the National Football League. The vocabularies are available in a json format in https://github.com/ ag1988/injecting_numeracy.</p>
<h2>A.2.3 Question templates</h2>
<p>The 13 question templates for 7 different skills are provided in Table 9.</p>
<h2>A. 3 Data for Masked LM task</h2>
<p>For creating the training data for the masked LM task (§5.1) we took the pages from English Wikipedia whose lowercased title containing a string in {season, economy, demographics, conquest, war, battle, uprising, rebellion, insurgency, conflict, crisis, revolution, military history, mutiny, regiment, revolt, geography, raids, insurrection, invasion, feud, siege, campaign, expedition, succession, coup, university}. This resulted in 156 K full pages. In the remaining pages, paras with $&lt;15$ numbers were discarded. Pages were tokenized using DT (§ 3) and chunked into 512-token sequences. Following Devlin et al. (2019), each token was masked with probability 0.15 with no more than 65 masks per sample. This gave us 0.7 M samples.</p>
<h2>A. 4 Experimental Setup</h2>
<p>For all our experiments, we used an older version of Hugging Face's Transformers library (Wolf et al., 2019) and provide our training hyperparameters in Table 10.</p>
<h2>A. 5 GENBERT $_{\text {+ND+TD }}$ Error Analysis</h2>
<p>Table 11 summarizes the main failure types of GEN$\mathrm{BERT}_{+ \mathrm{ND}+ \mathrm{TD}}$ on 100 random examples from the development set of DROP, excluding questions with a multi-span answer.</p>
<h1>Template</h1>
<p>CONT-1-AGT VERB-1- NUM-1 ATTR-1 ENT-1 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 and CONT-2-AGT VERB-1-POS NUM-2 ATTR-1 ENT-1 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 and NUM-2 ATTR-2 ENT-2 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1, but VERB-2-NEG NUM-2 ATTR-2 ENT-2 .
CONT-1-AGT VERB-1-POS NUM-1 ATTR-1 ENT-1 in ATTR-2 CONT-2-ENV .
CONT-1-AGT VERB-1-NEG NUM-1 of the ATTR-1 ENT-1 .
CONT-1-AGT had NUM-1 ATTR-1 ENT-1, CONT-2-AGT had NUM-2 ATTR-1 ENT-1, and CONT-3-AGT had NUM-3 ATTR-1 ENT-1 .
NUM-1 ATTR-1 ENT-1, NUM-2 ATTR-2 ENT-2, and NUM-3 ATTR-3 ENT-3 were VERB-1-POS in ATTR-4 CONT-1-ENV.
There were NUM-1 ATTR-1 ENT-1 and NUM-2 ATTR-2 ENT-2 in ATTR-3 CONT-1-ENV .
There were NUM-1 ATTR-1 ENT-1 in ATTR-2 CONT-1-ENV .
CONT-1-AGT VERB-1-NEGTRN NUM-1 ATTR-1 ENT-1 to CONT-2-AGT .
CONT-1-AGT VERB-1-POSTRN NUM-1 ATTR-1 ENT-1 from CONT-2-AGT .
Table 8: Sentence templates for synthetic textual examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reasoning</th>
<th style="text-align: left;">Templates</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Selection</td>
<td style="text-align: left;">How many ATTR-1 ENT-1 were in CONT-1-ENV? <br> How many ATTR-1 ENT-1 did CONT-1-AGT VERB-POS?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity difference</td>
<td style="text-align: left;">How many more ATTR-1 ENT-1 were in CONT-1-ENV than ATTR-2 ENT-2 ? <br> How many more ATTR-1 ENT-1 did CONT-1-AGT have than ATTR-2 ENT-2 ?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity subset</td>
<td style="text-align: left;">How many ENT-1 of CONT-1 were ATTR-1 ENT-1? <br> How many ENT-1 of CONT-1 were not ATTR-1 ENT-1?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity comparison</td>
<td style="text-align: left;">Were there {more I less} ATTR-1 ENT-1 in CONT-1-ENV or in CONT-2-ENV? <br> Who had {more I less} ATTR-1 ENT-1, CONT-1-AGT or CONT-2-AGT?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity superlative</td>
<td style="text-align: left;">Who had the {highest I lowest} number of ATTR-1 ENT-1 in total?</td>
</tr>
<tr>
<td style="text-align: left;">Intra-entity superlative</td>
<td style="text-align: left;">What was the {highest I lowest} number of ATTR-1 ENT-1 VERB-POS in CONT-1-ENV? <br> What is the {highest I lowest} number of ATTR-1 ENT-1 CONT-1-AGT VERB-POS?</td>
</tr>
<tr>
<td style="text-align: left;">Inter-entity sum</td>
<td style="text-align: left;">How many ATTR-1 ENT-1 were in CONT-1-ENV (, CONT-<em>-ENV) and CONT-2-ENV {in total I combined}? <br> How many ATTR-1 ENT-1 did CONT-1-ENV (, CONT-</em>-ENV) and CONT-2-ENV have {in total I combined}?</td>
</tr>
</tbody>
</table>
<p>Table 9: Templates for questions about generated synthetic passages, testing for numerical reasoning. The template placeholders are filled-in with values from the world state obtained after generating the synthetic passage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">pre-training</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">finetuning</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">b</td>
<td style="text-align: center;">bsz</td>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">b</td>
<td style="text-align: center;">bsz</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3e-5</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID }}$</td>
<td style="text-align: center;">$6 e-5$</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM-DT }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID-LM-RS }}$</td>
<td style="text-align: center;">$4 e-5$</td>
<td style="text-align: center;">440</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID }}$</td>
<td style="text-align: center;">$1 e-5$</td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">GENBERT ${ }_{\text {sNID+TD }}$</td>
<td style="text-align: center;">$1 e-5$</td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$3 e-5$</td>
<td style="text-align: center;">14</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyperparameters used for pre-training GENBERT and finetuning it on DROP. $\mathrm{l} \mathrm{t}=$ leaning rate, bsz=train batch size. Common params: seed=42, optimizer=Bert-Adam, linear-lr-warm-up=0.1, num epochs for finetuning=30, weightdecay $=0.01$, max-grad-norm $=1.0$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error category</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Counting <br> Sorting <br> Complex calculation <br> Complex semantics <br> Not numerical</td>
<td style="text-align: center;">q: How many people were the heads of the business? <br> q: Which nationality was the fourth largest? <br> q: How many percent of people were either Black Hispanic, of Sub-Saharan African origin, or of West Indian or Afro-Caribbean American origin? <br> q: By how many points did the Pistons lose their closest game? <br> q: Who defeated the Kievan Rus at the Battle of the Alta River?</td>
</tr>
<tr>
<td style="text-align: center;">Longer span</td>
<td style="text-align: center;">q: Where there more people in the peninsula pre-war or at the time of the first census? <br> a: pre-war <br> p: pre-war population</td>
</tr>
<tr>
<td style="text-align: center;">Shorter span</td>
<td style="text-align: center;">q: Was the life expectancy in 2015 higher for males or females? <br> a: females <br> p: female</td>
</tr>
<tr>
<td style="text-align: center;">Imprecise number prediction</td>
<td style="text-align: center;">q: How many more estimated Chinese Americans lived in California compared to Massachusetts? <br> a: 1130100 <br> p: 110100</td>
</tr>
</tbody>
</table>
<p>Table 11: Error categories of GENBERT ${ }_{\text {AND }+ \text { TD }}$ on the development set of DROP, based on a manual error analysis of 85 random examples. The upper part shows categories which are not not covered by our pre-training tasks or do not require numerical skills. The lower part shows categories of inaccurate model predictions. The letters $\mathbf{q}, \mathbf{a}$ and $\mathbf{p}$ denote the question, gold answer and model prediction, respectively.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.nltk.org/
${ }^{3}$ Using the Spacy library http://spacy.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
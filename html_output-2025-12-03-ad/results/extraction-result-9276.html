<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9276 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9276</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9276</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-278501703</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.07360v1.pdf" target="_blank">BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9276.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9276.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>one-shot_vs_zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot versus Zero-shot Prompt Presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of one-shot (single demonstration example included in the prompt) and zero-shot (no demonstration) prompt formats; evaluated across the BinMetric suite and shown to substantially affect LLM performance in binary-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (open-source and closed-source evaluated in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B — 8x7B and closed-source variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric overall (CSR, DEC, SR, BCS, AC, AIG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A multi-task benchmark for binary analysis including call-site reconstruction, decompilation, signature recovery, code summarization, algorithm classification, and assembly instruction generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>One-shot prompting with a single golden demonstration example embedded in the prompt; role-play framing (''experienced binary reverse engineer'') and required output formatting (wrapped with triple backticks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot prompting (same task instruction but without the golden demonstration example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>average overall score (across BinMetric tasks): one-shot > zero-shot by a mean relative increase of +16.65% (reported aggregate improvement when using one-shot prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot baseline (not given as absolute numbers); reported relative improvement: +16.65% mean increase in overall scores when switching to one-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+16.65% mean relative increase in overall score (one-shot vs zero-shot) across evaluated models/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute the improvement to the demonstration example providing contextual information and expected output format, which guides in‑context learning and reduces ambiguity for complex binary-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>One-shot prompts used a single golden demonstration; prompts included role-play and example code wrapped in triple backticks. Deterministic decoding used (temperature=0.1, top_k=1, top_p=1). Experiments covered the full BinMetric dataset (1,000 items across six tasks). Zero-shot performance was analyzed in §6.5 (detailed per-task zero-shot numbers not fully enumerated in paper excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9276.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>demonstration_aliasing_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstration-induced Output Replication (Example Copying)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed negative effect where a model (Llama2-7B) tends to replicate the demonstration example verbatim, degrading its ability to produce correct, context-specific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric overall (aggregate across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same BinMetric multi-task benchmark; effect observed on overall scoring when using one-shot prompt containing a demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>One-shot prompt with a golden demonstration example and role-play framing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicit comparison to same model with demonstration not causing copying (or to expected behavior of other models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>overall score decreased by 10.79% (Llama2-7B) relative to expected behavior when the model replicated the example output instead of generating context-specific responses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not given as separate absolute numbers for zero-shot vs one-shot for Llama2-7B, but the paper reports a -10.79% drop in overall score attributed to example replication.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-10.79% overall score (negative impact for Llama2-7B due to copying the demonstration)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that limited model capacity caused the demonstration to dominate generation, leading to rote replication rather than conditional generation; i.e., the demonstration disrupted task generalization for this smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Observed during one-shot experiments using the standard BinMetric one-shot prompt templates; single golden demonstration enclosed in triple backticks. The behavior was diagnosed by inspecting model outputs and comparing to expected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9276.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code_length_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input Code Length Effect on Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Longer input code snippets reduce LLM effectiveness on binary-analysis tasks; the study quantifies degradation when comparing 'short' vs 'long' code groups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (evaluated set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric overall (per-task short vs long input analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>BinMetric tasks with problem items partitioned into shorter and longer input code groups to measure sensitivity to input length.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Standard one-shot prompts with inputs separated by length groups (short vs long code snippets). Context window and prompt formatting kept constant (triple backticks, role-play).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Short input group versus long input group for the same tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average overall scores on longer code snippets decreased by 6.00% compared to shorter snippets (aggregated across tasks/models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Short inputs: baseline; Long inputs: -6.00% relative decline in overall scores.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-6.00% average relative drop (long vs short inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Longer code increases contextual complexity and may approach models' effective context handling limits, making it harder to extract and reason over relevant information; also likely to dilute the importance of the demonstration/example in one-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Problem items divided into short and long groups by code length; aggregate scores compared (figure 5(b) referenced). Context window configured (max_length 8192, max_new_tokens 2048).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9276.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>role_play_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-play Prompt Framing (e.g., 'experienced binary reverse engineer')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using role-play prompts to position the model as an expert to reduce ambiguity and better convey task expectations; used across BinMetric one-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (evaluated set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric (applies across tasks: CSR, DEC, SR, BCS, AC, AIG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Role-play framing used as part of prompt templates to guide model behavior for each binary-analysis task.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt begins with a role-play instruction (e.g., 'Please imagine you are an experienced binary reverse engineer') combined with a task-specific instruction and often one-shot demonstration; outputs required to be wrapped in triple backticks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative improvement reported — role-play used to 'more effectively convey the task requirements and reduce ambiguity' (no single numeric effect size reported separately).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Role-play reduces ambiguity by constraining the model's output style and expected content and works synergistically with one-shot demonstrations to improve task adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Role-play included in every one-shot prompt template (see Table 2). Outputs were constrained (wrapped) to ease parsing. No isolated ablation numbers for role-play alone were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9276.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>output_wrapping_format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output Wrapping with Triple Backticks (```)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Constrained output formatting (requirement to wrap responses in triple backticks) to facilitate automated parsing and reduce noisy text in model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric tasks (all tasks used wrapped output requirement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompts and expected outputs required the model to wrap generated code or answers in triple backticks to standardize format.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts explicitly instruct models to wrap their outputs with triple backticks and not to provide extra explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No quantitative metric provided; described as improving parsability and reducing noisy extraneous text in post-processing (qualitative benefit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Standardized output wrapper reduces post-processing errors and mis-parsing; it helps automated evaluation pipelines extract outputs deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>All one-shot prompt templates and required outputs used triple-backtick enclosure to ease automated parsing and evaluation; used across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9276.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>decoding_and_context_settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic Decoding and Context Window Settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inference hyperparameters (temperature, top_k, top_p, context window limits) chosen to prioritize reproducibility and accuracy over diversity; authors fixed these settings across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple open-source LLMs (settings described for inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BinMetric overall</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inference configuration for experiments across BinMetric tasks to ensure deterministic and comparable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt templates (one-shot) with inference parameters: max_length=8192, max_new_tokens=2048, temperature=0.1, top_k=1, top_p=1; half-precision FP16 enabled for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No direct numeric comparison to other decoding regimes provided; chosen to obtain highly deterministic responses and prioritize accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Lower temperature and strict sampling (top_k/top_p=1) reduce generation variability, improving reproducibility and alignment to expected outputs for code-oriented tasks where correctness is preferred over diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Applied consistently for open-source model inferences; closed-source API calls to GPT-3.5/GPT-4 used respective backends. Context window and token limits set to accommodate long prompts and code inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9276.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9276.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>chain_of_thought_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting (mentioned limitations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chain-of-thought (CoT) prompting is discussed; authors considered it but cite practical limitations for binary-analysis tasks and did not employ it in the main evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General/considered for BinMetric tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CoT is a prompting strategy that asks models to generate intermediate reasoning steps; here it was discussed as an option for guiding model reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought (explicitly prompting step-by-step internal reasoning) was considered but not used as the main format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively against one-shot prompts and standard instruction-following; authors note CoT limitations rather than providing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors cite practical limitations of CoT in this domain: limited context window length, reduced reasoning efficiency, and uncertain performance improvements for complex binary-analysis tasks; thus they did not adopt CoT in primary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT was discussed in methodology section but not experimentally evaluated on BinMetric in this study; no numerical results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers <em>(Rating: 2)</em></li>
                <li>Better zero-shot reasoning with role-play prompting <em>(Rating: 2)</em></li>
                <li>Unleashing the potential of prompt engineering in large language models: a comprehensive review <em>(Rating: 1)</em></li>
                <li>A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9276",
    "paper_id": "paper-278501703",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "one-shot_vs_zero-shot",
            "name_full": "One-shot versus Zero-shot Prompt Presentation",
            "brief_description": "Comparison of one-shot (single demonstration example included in the prompt) and zero-shot (no demonstration) prompt formats; evaluated across the BinMetric suite and shown to substantially affect LLM performance in binary-analysis tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (open-source and closed-source evaluated in this study)",
            "model_size": "various (7B — 8x7B and closed-source variants)",
            "task_name": "BinMetric overall (CSR, DEC, SR, BCS, AC, AIG)",
            "task_description": "A multi-task benchmark for binary analysis including call-site reconstruction, decompilation, signature recovery, code summarization, algorithm classification, and assembly instruction generation.",
            "presentation_format": "One-shot prompting with a single golden demonstration example embedded in the prompt; role-play framing (''experienced binary reverse engineer'') and required output formatting (wrapped with triple backticks).",
            "comparison_format": "Zero-shot prompting (same task instruction but without the golden demonstration example).",
            "performance": "average overall score (across BinMetric tasks): one-shot &gt; zero-shot by a mean relative increase of +16.65% (reported aggregate improvement when using one-shot prompts).",
            "performance_comparison": "zero-shot baseline (not given as absolute numbers); reported relative improvement: +16.65% mean increase in overall scores when switching to one-shot.",
            "format_effect_size": "+16.65% mean relative increase in overall score (one-shot vs zero-shot) across evaluated models/tasks.",
            "explanation_or_hypothesis": "Authors attribute the improvement to the demonstration example providing contextual information and expected output format, which guides in‑context learning and reduces ambiguity for complex binary-analysis tasks.",
            "null_or_negative_result": false,
            "experimental_details": "One-shot prompts used a single golden demonstration; prompts included role-play and example code wrapped in triple backticks. Deterministic decoding used (temperature=0.1, top_k=1, top_p=1). Experiments covered the full BinMetric dataset (1,000 items across six tasks). Zero-shot performance was analyzed in §6.5 (detailed per-task zero-shot numbers not fully enumerated in paper excerpt).",
            "uuid": "e9276.0",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "demonstration_aliasing_effect",
            "name_full": "Demonstration-induced Output Replication (Example Copying)",
            "brief_description": "Observed negative effect where a model (Llama2-7B) tends to replicate the demonstration example verbatim, degrading its ability to produce correct, context-specific outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_size": "7B",
            "task_name": "BinMetric overall (aggregate across tasks)",
            "task_description": "Same BinMetric multi-task benchmark; effect observed on overall scoring when using one-shot prompt containing a demonstration.",
            "presentation_format": "One-shot prompt with a golden demonstration example and role-play framing.",
            "comparison_format": "Implicit comparison to same model with demonstration not causing copying (or to expected behavior of other models).",
            "performance": "overall score decreased by 10.79% (Llama2-7B) relative to expected behavior when the model replicated the example output instead of generating context-specific responses.",
            "performance_comparison": "Not given as separate absolute numbers for zero-shot vs one-shot for Llama2-7B, but the paper reports a -10.79% drop in overall score attributed to example replication.",
            "format_effect_size": "-10.79% overall score (negative impact for Llama2-7B due to copying the demonstration)",
            "explanation_or_hypothesis": "Authors hypothesize that limited model capacity caused the demonstration to dominate generation, leading to rote replication rather than conditional generation; i.e., the demonstration disrupted task generalization for this smaller model.",
            "null_or_negative_result": true,
            "experimental_details": "Observed during one-shot experiments using the standard BinMetric one-shot prompt templates; single golden demonstration enclosed in triple backticks. The behavior was diagnosed by inspecting model outputs and comparing to expected outputs.",
            "uuid": "e9276.1",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "code_length_effect",
            "name_full": "Input Code Length Effect on Performance",
            "brief_description": "Longer input code snippets reduce LLM effectiveness on binary-analysis tasks; the study quantifies degradation when comparing 'short' vs 'long' code groups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (evaluated set)",
            "model_size": "various",
            "task_name": "BinMetric overall (per-task short vs long input analysis)",
            "task_description": "BinMetric tasks with problem items partitioned into shorter and longer input code groups to measure sensitivity to input length.",
            "presentation_format": "Standard one-shot prompts with inputs separated by length groups (short vs long code snippets). Context window and prompt formatting kept constant (triple backticks, role-play).",
            "comparison_format": "Short input group versus long input group for the same tasks and models.",
            "performance": "Average overall scores on longer code snippets decreased by 6.00% compared to shorter snippets (aggregated across tasks/models).",
            "performance_comparison": "Short inputs: baseline; Long inputs: -6.00% relative decline in overall scores.",
            "format_effect_size": "-6.00% average relative drop (long vs short inputs).",
            "explanation_or_hypothesis": "Longer code increases contextual complexity and may approach models' effective context handling limits, making it harder to extract and reason over relevant information; also likely to dilute the importance of the demonstration/example in one-shot prompts.",
            "null_or_negative_result": false,
            "experimental_details": "Problem items divided into short and long groups by code length; aggregate scores compared (figure 5(b) referenced). Context window configured (max_length 8192, max_new_tokens 2048).",
            "uuid": "e9276.2",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "role_play_prompting",
            "name_full": "Role-play Prompt Framing (e.g., 'experienced binary reverse engineer')",
            "brief_description": "Using role-play prompts to position the model as an expert to reduce ambiguity and better convey task expectations; used across BinMetric one-shot prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (evaluated set)",
            "model_size": "various",
            "task_name": "BinMetric (applies across tasks: CSR, DEC, SR, BCS, AC, AIG)",
            "task_description": "Role-play framing used as part of prompt templates to guide model behavior for each binary-analysis task.",
            "presentation_format": "Prompt begins with a role-play instruction (e.g., 'Please imagine you are an experienced binary reverse engineer') combined with a task-specific instruction and often one-shot demonstration; outputs required to be wrapped in triple backticks.",
            "comparison_format": null,
            "performance": "Qualitative improvement reported — role-play used to 'more effectively convey the task requirements and reduce ambiguity' (no single numeric effect size reported separately).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Role-play reduces ambiguity by constraining the model's output style and expected content and works synergistically with one-shot demonstrations to improve task adherence.",
            "null_or_negative_result": null,
            "experimental_details": "Role-play included in every one-shot prompt template (see Table 2). Outputs were constrained (wrapped) to ease parsing. No isolated ablation numbers for role-play alone were provided.",
            "uuid": "e9276.3",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "output_wrapping_format",
            "name_full": "Output Wrapping with Triple Backticks (```)",
            "brief_description": "Constrained output formatting (requirement to wrap responses in triple backticks) to facilitate automated parsing and reduce noisy text in model outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs",
            "model_size": "various",
            "task_name": "BinMetric tasks (all tasks used wrapped output requirement)",
            "task_description": "Prompts and expected outputs required the model to wrap generated code or answers in triple backticks to standardize format.",
            "presentation_format": "Prompts explicitly instruct models to wrap their outputs with triple backticks and not to provide extra explanation.",
            "comparison_format": null,
            "performance": "No quantitative metric provided; described as improving parsability and reducing noisy extraneous text in post-processing (qualitative benefit).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Standardized output wrapper reduces post-processing errors and mis-parsing; it helps automated evaluation pipelines extract outputs deterministically.",
            "null_or_negative_result": null,
            "experimental_details": "All one-shot prompt templates and required outputs used triple-backtick enclosure to ease automated parsing and evaluation; used across all tasks.",
            "uuid": "e9276.4",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "decoding_and_context_settings",
            "name_full": "Deterministic Decoding and Context Window Settings",
            "brief_description": "Inference hyperparameters (temperature, top_k, top_p, context window limits) chosen to prioritize reproducibility and accuracy over diversity; authors fixed these settings across experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple open-source LLMs (settings described for inference)",
            "model_size": "various",
            "task_name": "BinMetric overall",
            "task_description": "Inference configuration for experiments across BinMetric tasks to ensure deterministic and comparable outputs.",
            "presentation_format": "Prompt templates (one-shot) with inference parameters: max_length=8192, max_new_tokens=2048, temperature=0.1, top_k=1, top_p=1; half-precision FP16 enabled for efficiency.",
            "comparison_format": null,
            "performance": "No direct numeric comparison to other decoding regimes provided; chosen to obtain highly deterministic responses and prioritize accuracy.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Lower temperature and strict sampling (top_k/top_p=1) reduce generation variability, improving reproducibility and alignment to expected outputs for code-oriented tasks where correctness is preferred over diversity.",
            "null_or_negative_result": null,
            "experimental_details": "Applied consistently for open-source model inferences; closed-source API calls to GPT-3.5/GPT-4 used respective backends. Context window and token limits set to accommodate long prompts and code inputs.",
            "uuid": "e9276.5",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "chain_of_thought_mention",
            "name_full": "Chain-of-Thought Prompting (mentioned limitations)",
            "brief_description": "Chain-of-thought (CoT) prompting is discussed; authors considered it but cite practical limitations for binary-analysis tasks and did not employ it in the main evaluation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": "General/considered for BinMetric tasks",
            "task_description": "CoT is a prompting strategy that asks models to generate intermediate reasoning steps; here it was discussed as an option for guiding model reasoning.",
            "presentation_format": "Chain-of-thought (explicitly prompting step-by-step internal reasoning) was considered but not used as the main format.",
            "comparison_format": "Compared qualitatively against one-shot prompts and standard instruction-following; authors note CoT limitations rather than providing experiments.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors cite practical limitations of CoT in this domain: limited context window length, reduced reasoning efficiency, and uncertain performance improvements for complex binary-analysis tasks; thus they did not adopt CoT in primary experiments.",
            "null_or_negative_result": null,
            "experimental_details": "CoT was discussed in methodology section but not experimentally evaluated on BinMetric in this study; no numerical results provided.",
            "uuid": "e9276.6",
            "source_info": {
                "paper_title": "BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers",
            "rating": 2,
            "sanitized_title": "why_can_gpt_learn_incontext_language_models_implicitly_perform_gradient_descent_as_metaoptimizers"
        },
        {
            "paper_title": "Better zero-shot reasoning with role-play prompting",
            "rating": 2,
            "sanitized_title": "better_zeroshot_reasoning_with_roleplay_prompting"
        },
        {
            "paper_title": "Unleashing the potential of prompt engineering in large language models: a comprehensive review",
            "rating": 1,
            "sanitized_title": "unleashing_the_potential_of_prompt_engineering_in_large_language_models_a_comprehensive_review"
        },
        {
            "paper_title": "A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT",
            "rating": 1,
            "sanitized_title": "a_closer_look_at_different_difficulty_levels_code_generation_abilities_of_chatgpt"
        }
    ],
    "cost": 0.014357249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models
12 May 2025</p>
<p>Xiuwei Shang shangxw@mail.ustc.edu 
Guoqiang Chen guoqiangchen@qianxin.com 
Shaoyin Cheng sycheng@ustc.edu.cn 
Benlong Wu 
Li I Hu 
Gangyang Li ligangyang@mail.ustc.edu.cn 
Weiming Zhang zhangwm@ustc.edu.cn </p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>QI-ANXIN Technology Research Institute
BeijingChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>NENGHAI YU
University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>.cn; Guoqiang Chen
QI-ANXIN Technology Research Institute
BeijingChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
Benlong WuHefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>University of Science and Technology of China
HefeiChina</p>
<p>Weiming Zhang
University of Science and Technology of China
HefeiChina</p>
<p>Nenghai Yu
University of Science and Technology of China
HefeiChina</p>
<p>BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models
12 May 20258CD889BFA35CEC208CE930203989E897arXiv:2505.07360v1[cs.SE]Binary Program AnalysisReverse EngineeringBenchmarkingLarge Language Models
Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access.As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident.However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications.To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks.BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios.Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field.The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis.In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.CCS Concepts: • Software and its engineering → Software reverse engineering; • Computing methodologies → Artificial intelligence; • Security and privacy → Software security engineering.</p>
<p>ACM Reference Format: Xiuwei Shang, Guoqiang Chen, Shaoyin Cheng, Benlong Wu, Li Hu, Gangyang Li, Weiming Zhang, and Nenghai Yu. 2025.BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models.In Proceedings of xxx (xxx 2025).ACM, New York, NY, USA, 23 pages.https://doi.org/XXXXXXX.XXXXXXX</p>
<p>Introduction</p>
<p>Binary analysis is pivotal in various fields like software reverse engineering [80], malware prevention [63], and patch analysis [94], serving a crucial role in understanding and dissecting the functionalities of software without access to its source code.According to a research report by Statista, approximately 21.5 billion IoT devices will be connected globally by 2025 [79].The diversity in instruction architectures and operating systems, coupled with the predominance of closed-source code and documentation, limits the applicability of source code analysis for securing IoT device firmware, which drives further updates of binary analysis technology.</p>
<p>Unfortunately, understanding and interpreting the structure and behavior of binary files is challenging due to their complexity and the lack of direct human readability [68,103].Traditional tools [29,62,88] and techniques [55], while effective, often require extensive manual effort and expert knowledge, making the process time-consuming and frequently prone to errors.The integration of automated tools, especially those powered by artificial intelligence, has the potential to revolutionize this field by increasing efficiency and reducing human oversight.</p>
<p>In recent years, large language models (LLMs) have demonstrated increasing proficiency in a range of complex tasks [12,19,31,42,45,99,104], particularly showing promise in more specialized code-intensive areas such as code synthesis [37,92,95] and automated programming assistance [46,69,90].This has sparked questions among many software engineering practitioners: Can large language models like ChatGPT [66] and CodeLlama [76] effectively perform binary analysis tasks?However, the specific application of LLMs in the delicate area of binary analysis is still in its infancy, to some extent due to the lack of dedicated benchmarking frameworks that can adequately measure and drive progress in this area.</p>
<p>To address this limitation, we present BinMetric, the first comprehensive benchmark designed to evaluate the capabilities of LLMs on binary analysis tasks, which supports multiple tasks following realistic reverse engineering scenarios.BinMetric standardizes the evaluation process, providing a consistent and replicable framework for assessing the effectiveness of LLMs in this critical area.Specifically, BinMetric is composed of six distinct tasks that mirror real-world binary analysis challenges, including call-site reconstruction, decompilation, signature recovery, binary code summarization, algorithm classification and assembly instruction generation.These tasks are built on 20 real open-source projects, ensuring the realisticity, diversity, quality, credibility, and maintenance of data sources.After data filtering and inspection, we extracted 1,000 question items from these projects.Furthermore, to evaluate these tasks from different dimensions, we built 4 evaluators and integrated them into an automated pipeline for easy one-click invocation.</p>
<p>Next, to quantify the binary analysis capabilities of contemporary LLMs, we conduct an empirical study on BinMetric to assess widely-used LLMs, including open-source models such as Llama2 [84], Codellama [76], Mistral [36], WizardCoder [56], DeepSeek [8], DeepSeek-Coder [26], as well as closed-source like ChatGPT [66] and GPT-4 [2].The aim is to answer a couple of crucial questions:</p>
<p>• RQ1: What is the overall effectiveness of LLMs in binary analysis?• RQ2: Which LLM we investigated performs the best?And which type of LLMs performs better?• RQ3: How efficient are LLMs?And what factors affect the effectiveness of LLMs?</p>
<p>Through the results of our empirical study, we obtain a set of findings.First, the LLMs demonstrate promising capabilities in binary analysis but struggle with tasks like call-site reconstruction and assembly instruction generation.Notably, each model exhibits expertise in a specific perspective.For instance, GPT-4 excels in binary lifting and logical analysis, while models like WizardCoder and CodeLlama perform well in semantic comprehension and assembly synthesis.Second, GPT-4 leads in overall performance, and open-source models like CodeLlama-34B also show competitive capabilities, highlighting the potential of open-source solutions in this domain.Finally, the efficiency of LLMs varies with model size and tasks.Larger models generally perform better but at the cost of efficiency.The design of one-shot prompts enhances effectiveness, while longer code inputs tend to reduce performance.</p>
<p>To summarize, our major contributions are in the following aspects:</p>
<p>• Benchmark.We introduce BinMetric, a pioneering comprehensive benchmark for assessing LLMs performance across multiple real-world binary analysis tasks.This benchmark includes 6 distinct tasks, 1,000 questions extracted and filtered from 20 real open-source projects, and 4 evaluators integrated into the automated evaluation pipeline.• Empirical Study.We conduct the first large-scale investigation of widely-used LLMs using BinMetric, studying (1) the overall effectiveness of LLMs across diverse binary analysis tasks, (2) the performance comparisons of different LLMs, and (3) efficiency of LLMs and factors affecting their effectiveness.• Findings and Insights.Our results reveal the untapped potential of LLMs in binary analysis, providing new insights and future research directions for the field.</p>
<p>Background and Related Work</p>
<p>In this section, we first introduce our problem definition in §2.1 and present the related works in §2.2 and §2.3.</p>
<p>Problem Definition</p>
<p>Given a source code S, it undergoes a compilation and stripping process to produce a binary file B, represented as B = R(C(S)), where C denotes the compiler and R denotes the stripping process of symbolic information.The binary code analyzer A, designed to support a series of binary analysis tasks T = {t 1 , t 2 , . . ., t  }, takes binary file B as input and applies these tasks to generate corresponding outputs, formalized as:
O = A(B) = {o 1 , o 2 , . . . , o 𝑛 }.(1)
where each o  corresponds to the output of each task t  .</p>
<p>In this paper, we consider LLMs as binary code analyzers A. Our objective is to rigorously evaluate their proficiency in executing these binary analysis tasks, ensuring that each output accurately reflects the intended analysis, maintaining a high level of fidelity and comprehensibility relative to the original source code S, and show its effectiveness in various complex binary analysis scenarios.</p>
<p>Binary Analysis</p>
<p>Binary analysis involves examining and interpreting binary code, which is the machine-level representation of software executed by the CPU.Unlike source code, binary code is not humanreadable and requires specialized techniques and tools to analyze [17].This process is crucial in various fields such as reverse engineering, malware analysis, and vulnerability discovery [102].</p>
<p>Traditional tools and techniques have been the backbone of binary analysis for decades.Disassembly tools, such as IDA Pro [29] and Radare2 [73], convert binary code into assembly language, while decompilation tools like Ghidra [62] attempt to revert binaries back to a more understandable, higher-level C-like pseudo code.Static analysis tools, such as Clang Static Analyzer [55 examine binary code without execution, whereas dynamic analysis tools like Valgrind [87] and Pintools [71] track runtime behaviors in a controlled environment.Despite their widespread use, these traditional methods have significant shortcomings.They are often labor-intensive, requiring considerable expert knowledge and manual effort.Additionally, they struggle to extract high-level semantic information, which is crucial for understanding the broader context and functionality of the code.These methods are often inefficient when working with large codebases or highly optimized binaries, leading to incomplete or inaccurate analysis.</p>
<p>With the development of deep learning technology, many data-driven techniques have transformed the landscape of binary analysis.These methods leverage large datasets and advanced algorithms to enhance and automate various aspects of binary analysis.For instance, data-driven disassembly and decompilation methods can produce more accurate and human-readable code [5,30,32,81].Deep learning techniques can infer variable types and function signatures, reconstructing higher-level abstractions from binary code [13,38,40,70].Additionally, generative models can generate binary code summaries, providing a concise and high-level description of the code's functionality [3,93].Encoder models are also employed to generate semantic embeddings of binary code, identifying similarities and differences to aid in vulnerability detection and patch analysis [89,96,97].These data-driven approaches herald a future in which binary analysis will be more automated, accurate, and comprehensive, reducing the manual effort required and coping with increasingly complex software systems.</p>
<p>Large Language Models</p>
<p>In the context of early sequential language tasks, encompassing both natural and programming languages, task-specific model fine-tuning has shown promising performance [20].Fine-tuning involves updating the model weights and enhancing its performance on specific tasks by learning the relationship between the input and output of a specific downstream task dataset.</p>
<p>As a phenomenon-level technology, the emergence and rapid development of large language models has triggered disruptive changes in related fields.For example, ChatGPT [66], LLama [84], Claude [4], etc., which usually contain billions or even hundreds of billions of parameters, have been trained on massive text data, and have powerful language understanding and generation capabilities.Since LLMs encapsulate comprehensive knowledge, they can be applied to downstream tasks using a novel method called in-context learning [16], eliminating the need for extensive downstream datasets for fine-tuning.In-context learning allows the model to perform specific tasks directly by providing task-related contextual information without updating its parameters [9].</p>
<p>In this paper, we adopt the method of in-context learning to guide LLMs to understand the binary analysis tasks from multiple perspectives, thus facilitating a comprehensively evaluation of LLMs' performance on binary analysis.</p>
<p>Motivation</p>
<p>In this section, we discuss the motivation by first outlining the challenges faced in evaluating LLMs on binary analysis tasks in §3.1, then elucidating our motivation through a case story of a real-world analysis scenario in §3.2, and finally detailing our solutions in §3.3.</p>
<p>Challenges</p>
<p>Recently, numerous LLMs have been deployed to tackle software engineering problems, where they have demonstrated superior capabilities in a wide range of tasks such as program understanding [61], vulnerability detection [25], and automatic program synthesis [37,90], increasing developer productivity and streamlining all aspects of software development [31].</p>
<p>Correspondingly, benchmarks for various related tasks have emerged [27,47], such as HumanEval [12] for code generation and Defect4j [41] for automated program repair.These benchmarks are , Vol. 1, No. 1, Article .Publication date: May 2025.critical as they standardize the evaluation process and provide researchers with clear metrics and datasets, which systematically evaluate the effectiveness of different models and create a competitive environment to accelerate technological advancement.However, in the domain of binary analysis, no official benchmarks have been released yet.And implementing such a benchmark is far from straightforward due to the following challenges: Lack of Reliable Data Sources and Standard Preprocessing Criteria.The source and quality of binaries significantly impact the validity and reliability of benchmarks.Currently, there is a lack of standardized data collection frameworks to ensure dataset quality and consistency.Additionally, the absence of a unified preprocessing process for binary files, including compilation environment settings, disassembly or decompilation tool selection, and ground-truth identification, hinders meaningful comparisons between models.The black-box nature of LLMs exacerbates this issue.Ensuring that evaluation data is not included in the training set of LLMs, thereby preventing data leakage from compromising the credibility of benchmark results, is a key consideration.Multiple Tasks of Binary Analysis Process.Existing works on binary analysis primarily concentrate on isolated tasks, such as binary code similarity detection [89], decompilation [81] or binary code summarization [39].However, binary analysis is inherently a multifaceted process that requires extracting diverse information types from binaries and understanding the complex dependencies between them [17].If benchmark that focus solely on individual task, it fails to capture the integrated capabilities required for effective analysis, thus limiting the comprehensiveness and applicability of the evaluation and failing to represent the true performance of LLMs in real-world applications.Complexity of Realistic Binary Analysis Scenarios.The benchmark should avoid simplifying the complexity and diversity encountered in real-world binary analysis, e.g., avoid relying on single data sources, which fail to capture the wide range of project types and contexts in actual engineering environments.And this narrow data scope does not adequately represent the diverse challenges analysts face when working on binaries from different industries, technologies, and applications.Second, the benchmark should pay attention to the actual workflow of reverse engineers in binary analysis, thereby fully reflecting the multifaceted and complex challenges encountered in practice.</p>
<p>Insights</p>
<p>In this section, we present a case story in a real-world binary analysis scenario and then motivate the ideas behind BinMetric by describing the details of the work of reverse engineers.</p>
<p>As shown in Figure 1, consider a scenario where an enterprise's security team discovers a piece of suspected malware that is spreading across the corporate intranet and appears to steal data and compromise systems.The team is tasked with quickly understanding how this software works to develop effective defenses.</p>
<p>1 Binary Code Lifting: The security team immediately begins preliminary analysis of the captured binary executable file, using reverse engineering tools to reconstruct the function call relationships.This helps them understand the software's basic control flow and module dependencies, laying the foundation for subsequent in-depth analysis.As the analysis deepens, they use decompilation technology to convert the binary code into a high-level programming language.This allows for a more intuitive view of the code logic and structure, greatly improving understanding of the malicious software's behavior.</p>
<p>2 Semantic Comprehension: During decompilation, the team realizes the need for more contextual information to understand the code's functionality.They work on restoring key information such as function and argument names, argument types, and function return types.By restoring the signature, code readability is greatly improved, allowing for quick identification of potentially malicious code.Due to time constraints, to quickly grasp the full picture of the code, the team generated concise and informative code summaries for each function.These summaries help them understand the overall functionality and behavior of the software without having to examine the code line by line, significantly improving efficiency.</p>
<p>3 Logical Analysis: They also discover several complex algorithm codes.To determine their specific purpose and functionality, they categorize and identify these code snippets, helping the team understand the encryption algorithms, compression algorithms, or other key technologies used within the software.</p>
<p>4 Assembly Synthesis: In the final analysis stage, the team generates assembly instructions for specific functionalities to test and validate their defense measures, simulating and reproducing the malware's behavior.This enables the team to verify their defense strategy's effectiveness and further optimize security measures.</p>
<p>Through this series of binary analysis tasks, the security team systematically and comprehensively analyzes the malware's working mechanism.They identify malicious behavior patterns, develop effective defense strategies, and successfully combat threats.</p>
<p>Solutions</p>
<p>To address the challenges outlined in §3.1, combined with the analysis of the case story in §3.2, we propose the following solutions: Establish Data Collection and Preprocessing Criteria.Enhancing the quality of benchmarks for binary analysis hinges on establishing robust criteria for data collection and preprocessing.In this regard, inspired by related works [47,77,83,101] and following standards [24,28,64], we advocate that data sources should cover 5 dimensions of realisticity, diversity, quality, credibility, and maintenance, aiming to ensure that the dataset meets the actual requirements of the real-world.Furthermore, we emphasize standardized preprocessing processes, which require defining protocols for compilation, disassembly, and decompilation as well as metadata extraction, ground-truth identification, data filtering, and leak checking.We aim to minimize biases and discrepancies, thereby bolstering the reliability and validity of benchmark assessments.Enable Multifaceted Task Assessment.To truly gauge the performance of LLMs in binary analysis, benchmarks should assess a range of interconnected tasks in the binary analysis lifecycle, reflecting the complex nature of real-world analysis workflows.Specifically, our benchmark includes six comprehensive tasks such as decompilation, function signature recovery, and assembly instruction generation.These multifaceted assessments will more comprehensively and accurately reflect the capabilities of LLMs.Simulate Real-World Analysis Complexity.In this paper, we seek to replicate the complex challenges encountered in real-world binary analysis scenarios.To start, We amalgamate data sources across different project domains to ensure comprehensive coverage of topics to accommodate the diversity inherent in real-world environments.Subsequently, as described in §3.2, we dissect the complex challenges faced by reverse engineers in practical binary analysis efforts.In response, we crafted a series of assessment tasks to reflect the complexity of real-world analysis.4 BinMetric Benchmark This section provides an overview of the binary analysis tasks included in BinMetric (detailed in §4.1) and outlines the dataset collection, preprocessing, and construction in §4.2 and §4.3.The overview framework of BinMetric is shown in Figure 2.</p>
<p>Binary Analysis Tasks</p>
<p>BinMetric contains six representative binary analysis tasks that reflect the challenges faced by human reverse engineers in the binary analysis scenario, and LLMs also confront similar obstacles.Call-site Reconstruction (CSR): Function call relationships reveal the basic control flow and dependencies between modules in a program.Call-site reconstruction is a critical step in reverse engineering, with the primary goal of identifying and reconstructing function calls from the provided assembly code, including function name and its calling parameters.We define the input of the task as an assembly code piece and the function call location specified therein, and the output is the corresponding representation of the function call in the high-level source code.The task is evaluated by comparing the textual consistency between the output and the original source code, ensuring that the recovered call sites accurately reflect the intent and structure of the original code.Decompilation (DEC): Decompilation is essential for an intuitive and in-depth understanding of binary programs.The main goal of this task is to reconstruct a human-readable high-level programming language representation, such as C or C++, based on assembly code [5,38,81].We define the input of this task as an assembly code with function granularity, while the output is the corresponding high-level language code.This task is evaluated using the CodeBLEU [75] metric, which comprehensively considers the syntax correctness and semantic accuracy of the generated decompiled code by comparing it with the original source code.Signature Recovery (SR): Function signature reflects the interface information of the function, including function name, parameter type &amp; name, and return type, which is crucial for understanding the functionality and behavior of the program [13,40,70].We define the input of this task as decompiled pseudo code from stripped binaries, and the output is the complete function signature.This task uses text consistency metrics to evaluate the matching between the recovered signature and the function signature in the original source code.Binary Code Summarization (BCS): Code summaries can help understand the core functions and operations of a program without delving into complex code details.The main objective of this task is to extract key information from binary code and generate a natural language summary that concisely reflects its essence and functionality [3,93].We define the input of this task as decompiled pseudo code, and the output is its corresponding natural language summary.This task uses text consistency metrics for evaluation to ensure that prediction is consistent with ground truth in terms of relevance and clarity.</p>
<p>Algorithm Classification (AC): This task involves identifying and classifying algorithms or algorithmic patterns within binary code, thereby revealing key operations of the program such as sorting, encryption, etc., which are crucial in understanding the function and purpose of the code segment during security analysis.We define the input of this task as decompiled pseudo code and the output as its corresponding algorithm category label.This task is evaluated via accuracy metric.Assembly Instruction Generation (AIG): Analyzing malware and fixing program vulnerabilities without access to the source code often require modification of assembly code, i.e, the ability of assembly synthesis.The input of the AIG task is a natural language description of specific functionality, while the output is an assembly instruction fragment corresponding to the functionality.In order to echo whether the generated assembly instruction can accurately implement the intended function, we use syntax correctness and execution correctness to assess it.Furthermore, we also employ text consistency to measure the reliability of the generation.</p>
<p>Data</p>
<p>Collection and Preprocessing 4.2.1 Data Collection.Creating a reliable benchmark dataset is crucial for evaluating the binary analysis capabilities of LLMs.To ensure the robustness of the dataset and reflect real-world scenarios, we have set the following criteria for our code sources:</p>
<p>• Realisticity: the code should be derived from complete, human-developer-written real-world projects to ensure relevance and practicality.It should not contain toy programs, incomplete or synthesized snippets.• Diversity: the code should encompass a broad range of fields and application domains, ensuring that the dataset is not biased towards specific types of applications.This variety helps in assessing the models' performance across different contexts.It can be quantified by the domains covered by the projects.• Quality: high-quality code is essential, characterized by clear structure, logical organization, reasonable naming conventions, etc., which help reduce potential ambiguity or errors in the analysis process.It can be quantified by average number of Stars and Forks of project on Github.• Credibility: ideally, the code should come from projects maintained by well-known or reputable developers or organizations.This enhances the reliability and applicability of the dataset in real-world scenarios.• Maintenance: the code from regularly updated, actively maintained, and well-documented projects is preferred because it reflects current programming practices and standards.It can be quantified by the average number of Releases and Commits of projects.Compress FFmpeg [23] Video zstd [105] Compress C-Algorithms [10] Algorithm</p>
<p>These criteria are inspired by many existing works and follow some industry standards.The National Institute of Standards and Technology (NIST) has proposed criteria for trustworthy AI [64], while international standards and best practices related to software lifecycle processes and quality assurance processes, such as ISO/IEC 25010:2011 [24], IEEE Std-730-2014 [28], et.al, have put forward requirements for attributes such as quality, credibility, and maintenance.Many LLM benchmarks [47,77,83,101] emphasize attributes such as realisticity, diversity, quality, and credibility.Following these criteria, as shown in Table 1, we curate a diverse selection of 20 real-world C language projects from GitHub, focusing on those with the highest star ratings to ensure high credibility, excellent code quality, and maintenance standards.These projects have an average of 18.48K Stars and 3.9K Forks, 75.8 Releases and 15.5K Commits, spanning eleven domains, including audio, image, web, crypto, network, algorithm, and more, ensuring real coding practice and diversity.</p>
<p>Data Preprocessing.</p>
<p>As shown in Figure 2, our preprocessing pipeline involves the following steps: Compile, Strip and Decompile.Initially, we compile the projects selected on an Ubuntu 22.04 OS, targeting the x86-64 architecture.During compilation, we include DWARF [35] debugging information to ensure detailed metadata is available for subsequent alignment.Each project uses its default compiler settings, which encompass various compilers and optimization levels, to produce binaries that reflect typical compilation environments.After compilation, we employ the strip command to remove all symbolic information from the binaries, mimicking the conditions encountered in real-world binary analysis where symbolic information is often unavailable.Subsequently, we utilize IDA Pro [29] to disassemble and decompile the stripped binaries, obtaining assembly instruction sequences and decompiled pseudo code.Source Code Information Extraction.We use the srcML [57] tool to analyze and parse the source files, extracting key information including function signatures, function implementations, human-written summaries, etc.The srcML converts the source files into XML format, allowing us to accurately extract and process this information through XML parsing technology.This extracted information will be structurally stored for subsequent alignment.Binary-Source Alignment.We use DWARF debugging information to align source code and binary code, which can record the functions, variables in the binary code, and their locations (including source file name, line number, and column number) in the source code.In this way, we can accurately match the assembly instruction sequence and decompiled code with the corresponding location in the source code.</p>
<p>Data Construction</p>
<p>4.3.1 Data Filtering.Considering that extremely long code snippets may exceed the input limit of LLMs, resulting in incomplete analysis, and very short code snippets often lack the necessary semantic richness to provide meaningful context for evaluation, we perform double threshold filtering to remove code snippets that are too short or exceed the context window limit of LLMs, ensuring the feasibility of our evaluation.</p>
<p>Furthermore, ensuring that our benchmark dataset is not included in the training set of LLMs is crucial for the effectiveness of evaluation.All our evaluation data are disassembled or decompiled codes, which come from binary files compiled by ourselves and stripped of symbol information.This has greatly prevented our dataset from being collected into training sets by LLMs' developers.To further verify this, we employ the Google search engine to check if any of the code appears in clear text on the internet.Any disassembled or decompiled code found through an exact whole-word searches is removed from the dataset, which strengthens the credibility of our dataset.</p>
<p>4.3.2</p>
<p>Extract and Assemble Question Item.After data preprocessing and filtering, we obtain high-quality alignment metadata from both binary and source code.These metadata sets form the foundation for constructing the question items of BinMetric.</p>
<p>For the DEC and SR tasks, we randomly sample 250 pairs of input pseudo code and their output ground-truth from the metadata sets.For the CSR task, we randomly sample 70 input assembly code snippets, and each snippets is manually annotated with the call site locations to be recovered and the corresponding ground-truth.For the BCS task, we initially consider using human-written comments extracted from source files as labels, similar to previous work [3,93].However, we observe that less than 20% of the functions have comments written by developers.Worse yet, not all comments provide functional summaries and often include noisy content with significant variations in quality and style.Consequently, using human-written comments as ground-truth is deemed unreliable.Inspired by recent studies [15,82] that utilize LLMs like ChatGPT to perform data annotation tasks with reasonable reliability, we leverage ChatGPT to generate summaries of the source code, outlining the function's purpose and functionality.We select a total of 250 pseudo code-summary pairs, and each generated summary undergoes manual review for correctness, ensuring high-quality ground truth of the BCS task.</p>
<p>For the AC task, we sample from the C-Algorithms [10] project in our dataset.This project includes various algorithm implementations in C language, and we select 80 pseudo-code snippets and manually annotate the respective algorithm category.For the AIG task, we carefully design 100 clear and accurate instructions to construct prompts, aiming to guide the LLMs to generate assembly code snippets that follow Intel syntax for given functionalities, such as implementing the bubble sort algorithm.We also equip it with a set of program test cases containing inputs and corresponding outputs to verify the functional correctness of the generated assembly code.</p>
<p>Overall, the above tasks, which required manual annotation, review, and verification altogether, cost about 60 man-hours.</p>
<p>Implementation Design 5.1 Large Language Models Setup</p>
<p>We evaluate a total of 12 large language models from 5 different model families, including a variety of locally deployable open-source models, and API-callable closed-source models.</p>
<p>These LLMs are selected to meet the following criteria:</p>
<p>(1) State-of-the-Art: they are widely regarded as the most advanced and capable LLMs, ranking at the top of various leaderboards, such as EvalPlus [22] and LLM-Perf [54].</p>
<p>(2) Extensive Code Pre-training: these models have been pre-trained on large datasets that include substantial amounts of code, enhancing their ability to comprehend and utilize programming languages effectively.</p>
<p>(3) Text and Code Generation Abilities: the models demonstrate strong capabilities not only in natural language generation but also in generating and understanding code, making them versatile for both text and code-related tasks.</p>
<p>(4) Instruction Following Proficiency: we choose the instruction-tuned version to ensure that the model is good at following detailed instructions, which is a critical skill for accurately performing complex binary analysis tasks.</p>
<p>Tasks Prompt Template</p>
<p>Call-site Reconstruction</p>
<p>Please imagine you are an experienced binary reverse engineer.The following is a disassembled assembly code, your task is to understand its semantics and behavior, and output call point in the form of C source code at 'call sub_5F57E', including only a descriptive function name and function parameters, wrapped in three backticks (<code Assmebly="Assmebly" Code="Code">), do not explain.{Example}.Input assembly function:</code> <code>D ecompilation Please imagine you are an experienced binary reverse engineer.The following is a disassembled assembly code, your task is to understand it and output its corresponding C source code, wrapped in three backticks (```), do not explain.{Example}.Input assembly function: ```{Pseudo Code}</code>S ignature Recovery</p>
<p>Please imagine you are an experienced binary reverse engineer.The following is a stripped decompiled C function, your task is to understand it and output the descriptive function signature in its corresponding source code.This includes the function name, parameter list and its type, and return value type.Wrap the output with three backticks (<code Code="Code" Pseudo="Pseudo">), do not explain.{Example}.Input decompiled C function:</code> <code>B inary Code Summarization Please imagine you are an experienced binary reverse engineer.The following is a stripped decompiled C function, your task is to understand it and generate a short comment to the function describing its functionality.{Example}.Input decompiled C function: ```{Pseudo Code}</code>À lgorithm Classification Please imagine you are an experienced binary reverse engineer.The following is a stripped decompiled C function, your task is to understand it and output its algorithm class from the following list: [Sorting, Searching, Numerical Methods, Hash, Conversions, Math, Dynamic Programming, Cipher, ... ...].Wrap the output with three backticks (<code Code="Code" Pseudo="Pseudo">), do not explain.{Example}.Input decompiled C function:</code>.The generated assembly code is required to be wrapped in three backticks (```), can be compiled into an executable program by gcc, and does not contain comments.on multiple programming languages and various benchmarks.We deploy its deepseek-coder-7b -instruct-v1.5 and deepseek-coder-33b-instruct versions.} ``À ssembly Instruction Generation {Example}.Design an x64 architecture assembly code for {a bubble sort algorithm, which requires an array to be input from the terminal, and then the terminal outputs the sorted result </p>
<p>WizardCoder [56]: WizardCoder is an advanced code generation model developed by the Wiz-ardLM team, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.For experiments, we deploy the WizardCoder-15B-V1.0 and WizardCoder-33B-V1.1 versions.</p>
<p>Mistral [36]: Mistral is publicly released by Mistral-AI team, and we focus on its Mistral-7B -Instruct-v0.2version, which has been instruction-tuned and equips with multiple advanced attention mechanisms to improve inference speed and reduce inference cost.We also deploy its variant, Mixtral-8x7B-Instruct-v0.1, a Sparse Mixture of Experts (SMoE) generative model.</p>
<p>Closed-source Large Language Models.</p>
<p>GPT-3.5 [66] / GPT-4 [2]: Developed by OpenAI, GPT-3.5 and GPT-4 are among the most advanced and widely-used large language models.These models are accessed via OpenAI's API and are known for their exceptional performance across a broad spectrum of nature language and programe language tasks.Their ability to handle complex prompts and generate high-quality responses makes them ideal for evaluating binary analysis tasks.</p>
<p>Base Prompt Templates</p>
<p>To fully utilize the in-context learning ability of LLMs, we choose the one-shot prompt strategy.Meanwhile, we also analysis the performance of zero-shot prompt in §6.5.Although zero-shot prompt performs well in some natural language tasks, it often lacks sufficient contextual information when dealing with complex binary analysis tasks, resulting in low accuracy and consistency of model output.While the chain-of-thought method can guide the model to reason step by step, it has various limitations in practical applications, including the model's context window length, reasoning efficiency, and uncertain performance improvements.</p>
<p>The detailed one-shot prompts for different tasks we designed are displayed in Table 2.We use carefully selected golden demonstration example as part of the prompts to help the model understand the task context and expected output format.Moreover, we use role-play prompts [11,43] to position the model as an "experienced binary reverse engineer" to more effectively convey the task requirements and reduce ambiguity in the model's understanding of the task.We enclose the code in the prompt with three backticks (```) to clearly describe the code format.</p>
<p>We also require the model output to be wrapped in three backticks to facilitate parsing during post-processing, thereby reducing the interference of noisy text.</p>
<p>Evaluators and Metrics</p>
<p>In this section, we discuss the evaluators and metrics used to assess the performance of LLMs on binary analysis tasks.Our evaluation framework is meticulously designed and consists of 4 evaluators: binary lifting evaluator, semantic comprehension evaluator, logical analysis evaluator, and assembly synthesis evaluator, each of which aims to measure a specific aspect of binary analysis capabilities.</p>
<p>1 Binary Lifting Evaluator.The primary purpose of this evaluator is to assess the performance of LLMs to convert binary code in assembly form into a higher-level representation, which is crucial for reconstructing the structure and behavior of binary programs.It is applicable to both CSR and DEC tasks, and the evaluation metrics involved are Rouge-L [52] and CodeBLEU [75] respectively.Rouge-L measures the textual consistency between the generated call-site information and the reference, while CodeBLEU, as an metric for commonly used code synthesis tasks, is used to evaluate the syntactic and semantic logic similarity of the decompiled code to the corresponding source code. 2 Semantic Comprehension Evaluator.To measure the depth of LLMs' understanding and interpretation of binary code, and their ability to capture the latent meaning and intent behind code snippets, we designed this evaluator for SR and BCS tasks.By using BLEU-1 [67], METHOR [44], and Rouge-L [52] together, we provide a comprehensive assessment that covers precise word matching, semantic flexibility and richness, and structural coherence.</p>
<p>3 Logical Analysis Evaluator.Accurate logic analysis is essential to understand the specific behavior and purpose of a code snippet.This evaluator assesses the capability of LLMs to comprehend the logical structure of algorithmic snippets in binary code, and to determine whether they can identify and classify specific algorithms or algorithmic patterns.It is suitable for AC tasks, with the metric being Accuracy, which measures the correctness of LLMs in algorithm classification.4 Assembly Synthesis Evaluator.This evaluator is designed for the AIG task to assess LLMs' ability to generate accurate, executable assembly instruction snippets from natural language descriptions.The evaluation involves three metrics: Syntactic correctness, Execution correctness, and Rouge-L [52].Syntax correctness is determined by checking if the generated assembly code can be compiled without errors, ensuring it adheres to proper syntax rules.Since executability is one of the most crucial features of a program, we verify execution correctness using pre-designed test cases to confirm that the code performs the intended functionality.To avoid the security risks brought by directly executing the generated code, we use Docker to create an isolated environment to safely execute the assembly code.And Rouge-L intuitively measures the textual consistency of the generated code and the expected output.</p>
<p>Evaluation 6.1 Research Questions (RQs)</p>
<p>Our empirical study aims to investigate the following research questions to explore the binary analysis capabilities of LLMs:</p>
<p>• RQ1: What is the overall effectiveness of LLMs in binary analysis?This research question aims to evaluate the general capabilities of LLMs, understand how LLMs handle the inherent complexity and challenges of analyzing binary code, and provide a benchmark for their overall effectiveness in this domain.[33] and enable half-precision in FP16 for efficient inference.Given the context window limitations of LLMs, we set max_length to 8192 and max_new_tokens to 2048.Considering that when using language models to handle code tasks, most scenarios need to ensure the accuracy of the model responses rather than diversity, so we set the sampling temperature to 0.1, which controls the diversity and creativity of the model response.We also set both top_k and top_p to 1 during inference to obtain highly deterministic responses.</p>
<p>6.2.3</p>
<p>Baseline Methods.To better analyze the effectiveness of LLMs, we endeavor to include as many baseline methods as possible for comparison.For the static analysis tool IDA Pro [29], we utilize its Hex-rays decompiler for CSR, DEC, and SR tasks.However, it does not support the other tasks.For DEC task, we evaluate three versions (1.3B, 6.7B and 33B) of the existing work LLM4Decompile [81], which is a LLM fine-tuned on the DeepSeek-Coder for decompilation task.For BCS task, the only existing open-source methods are BinT5 [3] and HexT5 [93], and we reproduced them.For the other two tasks, i.e., AC and AIG, there are currently no comparable baseline methods, and our BinMetric can serve as a potential baseline for future research in these areas.</p>
<p>RQ1: Overall Effectiveness</p>
<p>In Table 3, we present the overall effectiveness of various LLMs across different binary analysis tasks.We use ↑ and ↓ to denote the increase and decrease in performance between two rows, respectively.The best scores in each task are highlighted in bold.</p>
<p>Results on Binary Lifting.For the binary lifting evaluation, we use Rouge-L and CodeBLEU scores to measure the performance of LLMs in call-site reconstruction (CSR) and decompilation (DEC), respectively.The average scores are 5.26% and 22.05% on both tasks.Rebuilding call-sites from binary functions without any other knowledge about the callee themselves is very challenging for the general LLMs.More specifically, although GPT-4 achieves the highest score of 9.61% in CSR, manual inspection reveals only a few useful reconstructions.And except for GPT-4, all other LLMs scored below the baseline method IDA Pro, which achieved 8.52%.WizardCoder-15B has the lowest score of 0.05% in CSR due to its failure to follow the instructions, and we could barely parse prediction from the output.Whereas the overall score for lifting the binary function into the source-code level is relatively high, which implies that LLMs have the potential to perform the semantic mapping between disassembled code and source code.Specifically, in the DEC task, LLMs demonstrate performance close to average, with DeepSeek-7B and GPT-4 recording the highest and lowest scores of 25.99% and 16.63%, respectively.It demonstrates that the decompilation task has a similar hardness for non-tailored models and does not show dramatic performance variations depending on the model size and domain.Compared to its base model DeepSeek-Coder, LLM4Decompile demonstrates a slight performance improvement at the same parameter scale.Results on Semantic Comprehension.To evaluate the semantic comprehension of LLMs on binary code, we conducted two experiments, signature recovery (SR) and binary code summarization (BCS), across all models.For these two semantic understanding tasks, LLMs perform relatively well and mostly outperform the baseline methods.</p>
<p>In the SR task, the LLMs achieved an average Rouge-L score of 18.75%, and it is WizardCoder-33B that excels most at this task (scored 28.12%), while Llama2-7B has struggled in it (scored 6.51%).Furthermore, our experiments have demonstrated that both the size and domain of models significantly affect the performance in the SR task.Specifically, an increase in model size resulted in a 42.8% improvement across open-source LLMs.And the code-specific LLMs outperformed general LLMs by 76.45%.On the other hand, for the BCS task, we use three metrics (BLEU, METEOR, and Rouge-L) to provide an aggregate and comprehensive assessment.Wizardcoder-33B has the best mean score of 33.3% on all three metrics.In contrast, CodeLlama-7B lagged in generating natural language summaries, with a score of 22%.Similarly, the code-specific variant, DeepSeek-Coder-7B, underperformed compared to its general counterpart, DeepSeek-7B, scoring 28.71% versus 30.44%.This disparity suggests that additional pre-training on code data may have adversely affected the model's natural language expression capabilities.Results on Logical Analysis.We utilized the Algorithm Classification (AC) task to estimate the capability of LLMs on binary code logical analysis, where we prompt the models to identify the class of pseudo code among a given candidate list.The overall score of LLMs is 53.23%, indicating that they can understand high-level semantics and support deeper logical reasoning tasks to a certain extent.GPT-4 has achieved a significant first position with an accuracy score of 83.75%.Conversely, we find that the two small-sized general models performed the worst, with Llama2-7B and DeepSeek-7B achieving scores of 15.00% and 20.00%, respectively.In this task, the code-specific models show an average 139.6% outperformance than the general ones.Meanwhile, increasing the model size also brings a notable improvement.</p>
<p>Results on Assembly Synthesis.Generating assembly code (AIG) on request is a typical evaluation task for assembly synthesis.We prompt the models to generate a fragment of assembly instructions to implement a specific function, adhering to Intel Syntax.Only the generation that compiled successfully is deemed syntactically correct.CodeLlama-7B excels at the AIG task, achieving a Syntax correctness score of 68%, which is likely benefiting from additional pre-training on assembly code.In contrast, other models struggle to generate syntactically correct assembly code.Even GPT-4, a leader in DEC tasks, scores only 11%.Furthermore, we devise test cases for IO verification to assess the semantic execution correctness.Notably, all models found it challenging to pass the IO test, with an average pass rate of less than 1%.Meanwhile, we employ Rouge-L to calculate the textual consistency of the generated text with the reference, offering an additional perspective on the evaluation.Llama2-7B records the highest Rouge-L score (31.58%), with manual identification revealing more similar code-blocks between its prediction and the reference.However, models with significantly lower scores, such as WizardCoder-15B (4.02%) and Mixtral-7B (7.51%), fail to follow our instructions to generate assembly code.The results show that current LLMs are not up to the task of reconstructing detailed low-level operations in binaries, possibly due to the inherent complexity of assembly-level instructions and the lack of domain expertise.</p>
<p>Answering RQ1:</p>
<p>The LLMs evaluated show overall potential for binary analysis, yet all perform poorly on CSR and AIG tasks, indicating that current non-expert models do not effectively address these analysis challenges.Additionally, each model exhibits expertise in specific analysis capabilities.For instance, GPT-4 excels in binary lifting and logical analysis, while WizardCoder and CodeLlama excel in semantic comprehension and assembly synthesis, respectively.</p>
<p>RQ2: LLMs Comparison</p>
<p>As shown in Figure 3 (a), we aggregate the scores of the models across various tasks to construct a radar chart, which illustrates the performance differences among the models.Specifically, we utilize the average score of the metrics used in each task as the task score and calculate the relative scores across the models based on the highest score achieved in each task.In general, GPT-4 dominates the arena, dramatically outperforming the other models in the DEC, CR, and AC tasks, although it underperforms in the AIG task.Trailing closely, WizardCoder-33B, CodeLlama-34B, and Mixtral-8x7B exhibit comparable overall performance, each revealing specific strengths and weaknesses.For instance, WizardCoder-33B, akin to GPT-4, struggles in assembly synthesis, whereas CodeLlama-34B excels in AIG and SR tasks.In addition, we perform statistical hypothesis tests to further analyze the performance differences among LLMs.We use Cohen's d indicator for practical significance analysis, and results are shown in Figure 4. Open-source v.s.Closed-source LLMs. Figure 3 (b) shows the average scores of each model over multiple tasks, and we use dashed lines to show the mean scores for both the open-source and closed-source models.The mean score of the closed-source model, at 27.46%, is significantly higher than the 22.93% achieved by its counterparts.However, some open-source models, such as CodeLlama-34B, surpass the average performance of their closed-source ones.This observation encourages the notion that open-source models are not significantly outpaced by closed-source models in binary analysis scenarios, offering strong confidence in developing expert LLMs in the binary domain based on these models.Parameter Size &amp; Code-specific Knowledge.Our study introduces two common sets of model sizes (7-15B &amp; 33-8x7B) and involves models with code domain knowledge from multiple LLM families.As shown in Figure 3 (b), the models with more parameters generally exhibit superior performance, potentially due to enhanced ability to follow instructions and more effectively embedded information, which can also be observed by the red box in Figure 4, where they have higher practical significance.Furthermore, domain-specific knowledge within the code domain yields a performance advantage on these models, despite the code dataset seldom includes decompiled binary code.</p>
<p>Answering RQ2: Based on the evaluation of our benchmark, GPT-4 leads other models in the binary analysis.Meanwhile, we recommend the CodeLlama-34B as the "open-source winner", which could be taken as a baseline to facilitate future research.In addition, the results encourage us to use the code-specific model and try larger sizes.</p>
<p>RQ3: Efficiency and Other Factors</p>
<p>To answer RQ3, we explore the overhead of LLMs on various binary analysis tasks.Meanwhile, We investigate the impact of prompts design and code length on the overall effectiveness of LLMs.Inference Efficiency.We measure the inference time of various LLMs across multiple tasks.Considering that closed-source LLMs are affected by network latency, rate limits, etc., our measurements only focus on open-source LLMs.The results are presented in Table 4. First, LLMs with larger parameters (33-8x7B) take 1.58× more time on average for inference compared to smaller LLMs (7-15B).However, as discussed in §6.4,larger LLMs generally have stronger analytical capabilities, which requires a trade-off between effectiveness and efficiency.In addition, different tasks show different levels of complexity.For example, the more complex DEC and AIG tasks require an average of 38.17s and 56.12s, respectively, while CSR and SR only require an average of 11.47s and 9.29s.However, an exception is noted: the overall score of Llama2-7B decreased by 10.79%.Upon analyzing its output, we find that it tends to replicate the example output provided in the one-shot prompt rather than generate new responses based on the context.This behavior may be attributed to its inherent capacity limitations, causing the demonstration example to disrupt its performance.Code Length.The input code length is a crucial factor that can significantly influence the effectiveness of LLMs in binary analysis tasks.To investigate this, we divide the problem items of each task into shorter and longer groups according to their code length.Figure 5 (b) compares the overall scores of LLMs on short and long input code.The results indicate a clear trend: the overall scores longer code snippets decrease by an average of 6.00% compared to shorter ones.This decline is observed across various tasks, highlighting the challenges when handling extended code snippets.</p>
<p>Answering RQ3: Larger LLMs offer better performance but at the cost of efficiency, with inference time 1.58× that of smaller LLMs on average.One-shot prompts effectively improve the overall effectiveness of LLMs.Besides, the effectiveness of LLMs is affected by the increase in input code length.In this paper, we conduct an empirical study on multiple LLMs via the BinMetric benchmark, revealing their potential and limitations in binary analysis tasks.Specifically, GPT-4 and WizardCoder-34B demonstrate impressive capabilities in algorithm classification and binary code summarization tasks, respectively, indicating that they can accurately extract high-level semantic information of binary codes and support deeper logical reasoning tasks, which are crucial for understanding complex binary files.However, existing LLMs still face challenges in call-site reconstruction and assembly instruction generation tasks.These tasks require the LLMs to reconstruct detailed low-level operations in binaries, which is difficult for current LLMs, probably due to the inherent complexity of assembly-level instructions and the lack of domain expertise.</p>
<p>In summary, LLMs demonstrate both potential and limitations in binary analysis tasks.Future research should focus on addressing these weaknesses, and with the continuous advancement of technology, LLMs still have broad prospects in this field.With the rapid development of foundation LLMs and the emergence of many domain expert LLMs, and given the potential shown by existing LLMs on binary analysis tasks, expert LLMs for binary analysis are feasible and promising.Just like source code LLM, which is evaluated in multiple tasks such as code completion, code infilling, and multilingual evaluation, we believe that binary code expert LLM should have comprehensive capabilities to handle multiple complex tasks in the binary analysis lifecycle.As a benchmark leaderboard designed for multi-tasks, BinMetric will assess and drive progress in this field.</p>
<p>To achieve this goal, the binary analysis expert LLM should incorporate extensive binary domain knowledge in the pre-training phase, including various assembly languages, compiler optimization modes, instruction set architecture differences, etc., so as to have a deeper grasp of code semantics and structure.The LLM can also be fine-tuned in a targeted manner in conjunction with specific binary analysis tasks to enhance its performance in key tasks.Furthermore, by combining retrievalaugmented generation strategies or designing specialized instruction fine-tuning datasets, the binary domain knowledge of LLM can be further extended.For example, retrieval enhancement strategies can allow LLM to dynamically query external knowledge bases, thereby providing more accurate judgments when processing complex binary patterns.Architectural innovation that supports longer context windows is also an important direction for improving binary expert LLM.Longer binary code snippets can provide more complete context information, thereby improving the analysis accuracy and generation quality of LLM.According to our empirical studies findings, different LLMs exhibit advantages in specific aspects of analysis.Future research could explore integrating the strengths of various LLMs to construct an expert ensemble model for binary analysis.</p>
<p>Threats to Validity.</p>
<p>Although we strive to maintain scientific and rigorous when constructing benchmark and conducting empirical studies, there are still some possible limitations that need to be noted and discussed.Selection of Analysis Tasks.As we mentioned in Section 3.2, we selected tasks that are common in each stage of the binary analysis lifecycle and suitable for solving with LLM.However, it is unrealistic to cover all potential tasks.For example, variable and data structure recovery, we regard them as part of the decompilation task.Although similarity detection is very common in binary analysis, existing solutions almost exclusively use encoder-only models for embedding representation, which is not suitable for sequence-to-sequence LLMs, and small-scale models have achieved excellent performance.</p>
<p>Scale of the Benchmark.The BinMetric benchmark contains 1,000 problem samples across six tasks.While the sample size is sufficient to draw certain conclusions, increasing the size could further stabilize the results.However, expanding the scale is challenging due to the significant manual effort required for annotation.For benchmark, we believe that attributes such as being lightweight, easily reproducible, and non-computational resource intensive are of great practical importance, can promote widespread adoption among researchers.Referring to benchmarks in related fields [7,21,98,100], especially HumanEval [12] for code generation and Defects4J [41] for automated program repair, have sample sizes ranging from 100 to 1,000.Therefore, we consider the current sample size is moderate, with the diversity and representativeness of the data sources ensuring the validity of our evaluation.Selection of LLMs.In the current study, our selection of LLMs is limited to a subset of the rapidly developing field of LLMs.Due to computational resource limitations, we are restricted to evaluating open-source models with 7B to 8x7B parameter scales, and some closed-source models.Although these models have been able to reflect the capabilities of LLMs in binary analysis, there is still room for exploration.Ignorance of obfuscated binaries.Our BinMetric benchmark does not consider any form of binary code obfuscation, which can significantly alter the appearance of disassembled or decompiled code, posing challenges for LLMs in terms of accurate comprehension and interpretation.Therefore, we plan to extend the benchmark to include analysis of obfuscated binary code in future work.</p>
<p>Conclusion</p>
<p>In this paper, we conduct a pioneering study on the binary analysis capabilities of LLMs.We design a standardized data collection and preprocessing pipeline, which led to the creation of BinMetric, a comprehensive benchmark with 1,000 question items covering 6 representative tasks, reflecting real-world binary analysis challenges.Our empirical study, which test various LLMs, reveals their strengths and limitations on various tasks.The findings indicate that while LLMs show strong potential in binary analysis, significant challenges remain.As LLMs continue to evolve, we believe that BinMetric, as a benchmark leaderboard, will become a crucial tool for assessing and driving progress in this field.</p>
<p>Fig. 1 .
1
Fig. 1.Binary analysis process in case story in §3.2.</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: May 2025.</p>
<p>Fig. 2 .
2
Fig. 2. Overview framework of BinMetric benchmark.</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: May 2025.</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: May 2025.</p>
<p>5.1. 1
1
Open-source Large Language Models.We locally deploy 10 open-source LLMs from 4 different model families, including:Llama2[84] / CodeLlama[76]: The Llama2 family of LLMs is developed and publicly released by Meta.It is a series of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters.We choose its Llama-2-7b-chat-hf version.CodeLlama is a code-specialized version of Llama2, designed for general code synthesis and understanding.We deploy the CodeLlama-7b-Instruct-hf and CodeLlama-34b-Instruct-hf versions.DeepSeek[8] / DeepSeek-Coder[26]: DeepSeek LLM is developed by DeepSeek-AI, and it has been trained from scratch on a vast dataset of 2 trillion English and Chinese tokens.We use its deepseek-llm-7b-chat version fine-tuned on extra instruction data.DeepSeek-Coder is a series of code language models, which achieveing excellent performance among open-source code LLMs , Vol. 1, No. 1, Article .Publication date: May 2025.</p>
<p>Fig. 3 .
3
Fig. 3. Performance comparison between LLMs.</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: May 2025.</p>
<p>Fig. 4 .
4
Fig. 4. Results of practical significance (Cohen's d).</p>
<p>Potential and Limitations of LLMs for Binary Analysis.</p>
<p>L la m a 2 CFig. 5 .
25
Fig. 5. Impact of prompt word and code length on performance</p>
<p>Table 1 .
1
Data sources of our benchmark dataset.
ProjectDomainProjectDomainaudio [6]AudioLlama2.c [53]Deep Learningminiaudio [59]AudioWhisper.cpp [91] Deep LearningOpenSSL [65]CryptoMongoose [60]Weblibsodium [50]Cryptolibhv [49]WebRedis [74]DatabaseCurl [14]NetworkSQLite [78]DatabaseMasscan [58]NetworkImageMagick [34]ImageLibexpat [48]FormatLibvips [51]ImageUltrajson [86]Format7z [1]</p>
<p>Table 2 .
2
Base prompt templates of LLMs.{Example} represents the golden demonstration example in the One-shot prompts, and {••• •••} represents the specific input for each piece of data.</p>
<p>•</p>
<p>RQ2: Which LLM we investigated performs the best?And which type of LLMs performs better?We specifically focus on performance comparisons between open-source and closedsource LLMs, general and code-specific LLMs, and LLMs of varying parameter sizes, with the goal of understanding the strengths and limitations of each LLM in detail.• RQ3: How efficient are LLMs?And what factors affect the effectiveness of LLMs?This research question investigates the efficiency of LLMs, as well as the impact of prompt design and code length on the effectiveness of LLMs.Our goal is to provide comprehensive guidelines for optimizing the use of LLMs in binary analysis applications.Environments.Our experimental environment operates on an Ubuntu 22.04 server, equipped with two 28-core Intel Xeon 6330 CPUs, 512GB RAM, 64TB storage, and eight NVIDIA RTX A6000 GPUs, each with 48GB of VRAM.These GPUs run Nvidia driver version 525.116.03along with CUDA version 12.0.6.2.2 Implementation Details.The BinMetric benchmark and all experiments are conducted using Python 3.8 with PyTorch [72] 2.0.1,Transformers [85] 4.37.2 and DeepSpeed [18] 0.13.0 packages.For closed-source LLMs, i.e.ChatGPT and GPT4, we call OpenAI's API to access gpt-3.5-turbo-16k-0613and gpt-4-0613 backend models.We download all the open-source LLMs from Huggingface
6.2 Experiment Setup6.2.1</p>
<p>Table 3 .
3
Overall effectiveness of LLMs and baseline method on BinMetric.↑and ↓represent a relative increase and decrease between two rows.
BinMetric BenchmarkTypeModelSizeCSRDECSRBCSACAIG𝑆𝑦𝑛𝑡𝑎𝑥𝐸𝑥𝑒𝑐𝑢𝑡𝑖𝑜𝑛𝑅𝑜𝑢𝑔𝑒-𝐿 𝐶𝑜𝑑𝑒𝐵𝐿𝐸𝑈 𝑅𝑜𝑢𝑔𝑒-𝐿𝐵𝐿𝐸𝑈𝑀𝐸𝑇 𝐸𝑂𝑅 𝑅𝑜𝑢𝑔𝑒-𝐿𝐴𝑐𝑐.𝑅𝑜𝑢𝑔𝑒-𝐿𝐶𝑜𝑟𝑟 .𝐶𝑜𝑟𝑟 .BaselineLLM4-Decompile[81]1.3B -6.7B -33B -21.53 22.97 24.48(↑6.7%) (↑6.6%)------------------------MethodsIDA Pro [29]-8.5218.5610.27-------BinT5 [3]220M ---31.581.824.28----HexT5 [93] 223M ---36.924.997.65----Llama2 CodeLlama7B 3.83 7B 4.91 34B 6.17(↑28.2%) (↑25.7%)24.71 23.62 (↓4.4%) 20.60 (↓12.8%)6.51 14.49 26.59(↑122.6%) (↑83.5%)35.83 16.32 (↓54.5%) 29.04 (↑77.9%)21.64 16.81 (↓22.3%) 28.89 (↑71.9%)19.00 13.05 (↓31.3%) 22.89 (↑75.4%)15.00 36.25 (↑141.7%) 65.00 (↑79.3%)1.00 68.00 63.00 (↓7.4%) (↑6700%)0.00 0.00 2.00(↑0.0%) (↑𝑖𝑛𝑓 %)31.58 (↓22.6%) 24.44 23.98 (↓1.9%)Open SourceDeepSeek DeepSeek-Coder 33B 6.35 7B 4.92 7B 2.99(↓39.2%) (↑112.4%)16.63 21.36 (↑28.4%) 22.77 (↑6.6%)8.95 11.66 19.97(↑30.3%) (↑71.3%)40.44 32.45 (↓19.8%) 43.58 (↑34.3%)26.97 30.23 (↑12.1%) 30.41 (↑0.6%)23.92 23.44 27.60(↓2.0%) (↑17.7%)20.00 47.50 (↑137.5%) 71.25 (↑50.0%)17.00 41.00 16.00(↑141.2%) (↓61.0%)0.00 4.00 1.00 (↓75.0%) (↑𝑖𝑛𝑓 %)14.50 24.54 (↑69.2%) 12.64 (↓48.5%)15B 0.05 WizardCoder 33B 8.34(↑𝑖𝑛𝑓 %)22.62 23.62 (↑4.4%)27.04 28.12(↑4.0%)25.76 45.93(↑78.3%)27.44 28.09 (↑2.4%)20.16 27.93 (↑38.5%)51.25 75.00 (↑46.3%)0.00 1.00(↑𝑖𝑛𝑓 %)0.00 0.00(↑0.0%)4.02 9.95 (↑147.5%)Mixtral7B 4.63 8x7B 7.27(↑57.0%)19.51 22.15 (↑13.5%)19.49 21.91(↑12.4%)41.39 43.19 (↑4.3%)31.00 29.88 (↓3.6%)26.63 26.73(↑0.4%)48.75 65.00 (↑33.3%)2.00 32.00(↑1500%)0.00 1.00(↑𝑖𝑛𝑓 %)7.51 27.89 (↑271.4%)CloseGPT-3.5-Turbo/4.0921.0419.6236.1630.6925.0960.0015.000.0023.97SourceGPT-4/9.6125.9920.6944.4824.8123.2583.7511.001.0018.62Average5.2622.0518.7536.2127.2423.3153.2322.250.7518.64</p>
<p>,</p>
<p>Vol. 1, No. 1, Article .Publication date: May 2025.
Signature RecoveryDecompilationBinary Code Summarization Algorithm ClassificationCall-site Reconstruction Assembly Instruction Generation DeepSeek DeepSeek-Coder 7B DeepSeek-Coder 33B Llama2 CodeLlama 7B CodeLlama 34B Mixtral 7B Mixtral 8x7B WizardCoder 15B WizardCoder 33B GPT-3.5-Turbo GPT-4GPT-4 GPT-3.5-Turbo CodeLlama-34B WizardCoder-33B Mistral-8x7B DeepSeek-Coder-33B DeepSeek-Coder-7B Mistral-7B WizardCoder-15B Llama2-7B DeepSeek-7B CodeLlama-7B141618 BinMetric Overall Score 20 22 24 Avg:22.9326 Closed-Source 28 Avg:27.46 30 LLMs Open-Source LLMs(a) Relative performance of LLMs on BinMetric(b) Overall scores of LLMs on BinMetric benchmarkbenchmark against the best in each tasks.across 6 tasks. Dashed lines for two LLM types' average.</p>
<p>Table 4 .
4
Analysis efficiency (in Seconds) of LLMs.
Prompts Design. As discussedin  §5.2, the effectiveness of LLMsmay be significantly influenced byprompt design. Figure 5 (a) compares the overall scores of LLMs usingModelSizeBinMetric Benchmark CSR DEC SR BCS AC AIGAvg.zero-shot and one-shot prompts. We observe that in most cases, one-shot prompts substantially improved theLlama2 CodeLlama 34B 2.58 52.24 4.51 75.03 1.71 103.96 40.01 7B 2.49 1.17 1.40 4.99 0.57 24.27 5.82 7B 27.82 23.28 2.26 42.16 58.51 58.49 35.42 DeepSeek 7B 0.66 15.01 13.21 5.13 0.43 22.54 9.50overall scores of LLMs, with an aver-age increase of 16.65%. This enhance-ment underscores the importance of providing demonstration examples to guide the LLMs' understanding7B DeepSeek-Coder 33B 3.01 85.99 9.70 22.95 1.59 62.55 30.97 0.80 28.22 2.92 7.91 1.09 21.24 10.36 15B 69.13 37.85 38.64 35.44 47.97 57.69 47.79 WizardCoder 33B 4.12 77.98 10.63 20.15 2.12 61.22 29.37 7B 1.03 14.42 3.86 5.95 0.44 51.55 12.88 Mixtral 8x7B 3.07 45.59 5.72 16.59 1.09 97.73 28.30and response generation in the com-Average/11.47 38.17 9.29 23.63 11.55 56.12 25.04plex domain of binary analysis.
, Vol. 1, No. 1, Article . Publication date: May 2025.
AcknowledgmentsThis work was supported in part by the Natural Science Foundation of China under Grant U20B2047, 62072421, 62002334, 62102386, and 62121002.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Extending source code pre-trained language models to summarise decompiled binarie. Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand Ashok Sawant, Premkumar Devanbu, Arie Van Deursen, 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2023</p>
<p>. Anthropic, 2023</p>
<p>SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. Jordi Armengol-Estapé, Jackson Woodruff, Chris Cummins, Fp O' Michael, Boyle, 2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE2024</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021. 2021. May 20251arXiv preprint</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv:2401.02954Deepseek llm: Scaling open-source language models with longtermism. 2024. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>. C-Algorithms , 2024</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, Shengxin Zhu, arXiv:2310.147352023. 2023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Augmenting decompiler output with learned variable names and types. Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Claire Le Goues, Graham Neubig, Bogdan Vasilescu, 31st USENIX Security Symposium. USENIX Security. 202222</p>
<p>Structured information extraction from scientific text with large language models. John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Gerbrand Andrew S Rosen, Kristin A Ceder, Anubhav Persson, Jain, Nature Communications. 1514182024. 2024</p>
<p>Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei, arXiv:2212.105592022. 2022arXiv preprint</p>
<p>Neural reverse engineering of stripped binaries using augmented control flow graphs. Yaniv David, Uri Alon, Eran Yahav, Proceedings of the ACM on Programming Languages. 42020. 2020OOPSLA</p>
<p>. Deepspeed, 2024</p>
<p>Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models. Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, Lingming Zhang, Proceedings of the 32nd ACM SIGSOFT international symposium on software testing and analysis. the 32nd ACM SIGSOFT international symposium on software testing and analysis2023</p>
<p>Towards understanding the capability of large language models on code clone detection: a survey. Shihan Dou, Junjie Shan, Haoxiang Jia, Wenhao Deng, Zhiheng Xi, Wei He, Yueming Wu, Tao Gui, Yang Liu, Xuanjing Huang, arXiv:2308.011912023. 2023arXiv preprint</p>
<p>Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, arXiv:2308.018612023. 2023arXiv preprint</p>
<p>. Evalplus, 2024</p>
<p>. Ffmpeg, 2024</p>
<p>International Organization for Standardization. Systems and Software Engineering: Systems and Software Quality Requirements and Evaluation (SQuaRE): System and Software Quality Models. ISO2011</p>
<p>How Far Have We Gone in Vulnerability Detection Using Large Language Models. Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang, arXiv:2311.124202023. 2023arXiv preprint</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, Li, arXiv:2401.14196DeepSeek-Coder: When the Large Language Model Meets Programming-The Rise of Code Intelligence. 2024. 2024arXiv preprint</p>
<p>Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li, Ruibo Liu, Yue Wang, arXiv:2404.03543CodeEditorBench: Evaluating Code Editing Capability of Large Language Models. 2024. 2024arXiv preprint</p>
<p>Ieee standard 730-2014 software quality assurance processes. Heimann, IEEE Std. 7302014. 2014. 2014IEEE Computer Society</p>
<p>IDA Pro. Hex-Rayssa, 2024</p>
<p>Beyond the C: Retargetable decompilation using neural machine translation. Iman Hosseini, Brendan Dolan-Gavitt, arXiv:2212.089502022. 2022arXiv preprint</p>
<p>Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, Haoyu Wang, arXiv:2308.10620Large Language Models for Software Engineering: A Systematic Literature Review. 2024. 2024arXiv preprint</p>
<p>DeGPT: Optimizing Decompiler Output with LLM. Peiwei Hu, Ruigang Liang, Kai Chen, Proceedings 2024 Network and Distributed System Security Symposium. 2024 Network and Distributed System Security Symposium2024. 2024267622140</p>
<p>. Huggingface, Article . Publication date. 112024. May 2025</p>
<p>. Imagemagick, 2024</p>
<p>I , Dwarf debugging information format version 4. 2010</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024. 2024arXiv preprint</p>
<p>Impact of code language models on automated program repair. Nan Jiang, Kevin Liu, Thibaud Lutellier, Lin Tan, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang, arXiv:2311.13721Nova + : Generative Language Models for Binaries. 2023. 2023arXiv preprint</p>
<p>Xin Jin, Jonathan Larson, Weiwei Yang, Zhiqiang Lin, arXiv:2312.09601Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models. 2023. 2023arXiv preprint</p>
<p>Symlm: Predicting function names in stripped binaries via context-sensitive execution-aware code embeddings. Xin Jin, Kexin Pei, Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. the 2022 ACM SIGSAC Conference on Computer and Communications SecurityJun Yeon Won, and Zhiqiang Lin. 2022</p>
<p>Defects4J: A database of existing faults to enable controlled testing studies for Java programs. René Just, Darioush Jalali, Michael D Ernst, Proceedings of the 2014 international symposium on software testing and analysis. the 2014 international symposium on software testing and analysis2014</p>
<p>Large language models are few-shot testers: Exploring llm-based general bug reproduction. Sungmin Kang, Juyeon Yoon, Shin Yoo, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Better zero-shot reasoning with role-play prompting. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, arXiv:2308.077022023. 2023arXiv preprint</p>
<p>The Meteor Metric for Automatic Evaluation of Machine Translation. Alon Lavie, Michael J Denkowski, 10.1007/s10590-009-9059-4Machine Translation. 232009. sep 2009</p>
<p>Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, Siddhartha Sen, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>On Automated Assistants for Software Development: The Role of LLMs. Mira Leung, Gail Murphy, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu, Kaibo Liu, Lecheng Wang, Zheng Fang, arXiv:2401.06401DevEval: Evaluating Code Generation in Practical Software Projects. 2024. 2024arXiv preprint</p>
<p>. Libexpat, 2024</p>
<p>. Libvips, 2024</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>. Llm-Perf, 2024</p>
<p>Clang Static Analyzer. LLVM. 2024</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evol-instruct. 2023. 2023arXiv preprint</p>
<p>Exploration, Analysis, and Manipulation of Source Code Using srcML. Jonathan I Maletic, Michael L Collard, 10.1109/ICSE.2015.3022015 IEEE/ACM 37th IEEE International Conference on Software Engineering. 20152</p>
<p>. Masscan, 2024</p>
<p>. Mongoose, 2024</p>
<p>Using an llm to help with code understanding. Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Ghidra. Nationalsecurityagency, 2024</p>
<p>Binary Representation Embedding and Deep Learning For Binary Code Similarity Detection in Software Security Domain. Thinh Nguyen, Hung , Hai Nguyen Phuc, Tran Khoa, Nhan Dinh, Tran Le, Nghia Thanh, Khoa To Trong, Duy Ngo Khanh, Hau Pham Phan The, Van, Proceedings of the 12th International Symposium on Information and Communication Technology. the 12th International Symposium on Information and Communication Technology2023. May 20251Publication date</p>
<ol>
<li>Trustworthy and Responsible AI. National Institute of Standards and Technology (NIST</li>
</ol>
<p>. Openssl, 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Probabilistic naming of functions in stripped binaries. James Patrick-Evans, Lorenzo Cavallaro, Johannes Kinder, Proceedings of the 36th Annual Computer Security Applications Conference. the 36th Annual Computer Security Applications Conference2020</p>
<p>Asleep at the keyboard? assessing the security of github copilot's code contributions. Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, 2022 IEEE Symposium on Security and Privacy (SP). IEEE2022</p>
<p>Stateformer: Fine-grained type recovery from binaries using generative state modeling. Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao, David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, Suman Jana, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2021</p>
<p>. Pintools, 2024</p>
<p>. Pytorch, 2024</p>
<p>. Redis, 2024</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.102972020. 2020arXiv preprint</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, Aran Komatsuzaki, arXiv:2307.13692Arb: Advanced reasoning benchmark for large language models. 2023. 2023arXiv preprint</p>
<p>. Sqlite, 2024</p>
<p>. Statista, 2024</p>
<p>An empirical examination of the reverse engineering process for binary files. Iain Sutherland, George E Kalb, Andrew Blyth, Gaius Mulley, Computers &amp; Security. 252006. 2006</p>
<p>Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang, arXiv:2403.05286LLM4Decompile: Decompiling Binary Code with Large Language Models. 2024. 2024arXiv preprint</p>
<p>Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, arXiv:2402.13446Large Language Models for Data Annotation: A Survey. 2024. 2024arXiv preprint</p>
<p>Kunsheng Tang, Wenbo Zhou, Jie Zhang, Aishan Liu, Gelei Deng, Shuai Li, Peigui Qi, Weiming Zhang, Tianwei Zhang, Nenghai Yu, arXiv:2408.12494GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models. 2024. 2024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023. 2023arXiv preprint</p>
<p>. Ultrajson, 2024</p>
<p>. Valgrind, 2024</p>
<p>Binary Ninja. Vector35. 2024</p>
<p>Hao Wang, Zeyu Gao, Chao Zhang, Zihan Sha, Mingyang Sun, Yuchen Zhou, Wenyu Zhu, Wenju Sun, Han Qiu, Xi Xiao, arXiv:2402.16928CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision. 2024. 2024arXiv preprint</p>
<p>Copiloting the copilots: Fusing large language models with completion engines for automated program repair. Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang, Engineering Conference and Symposium on the Foundations of Software Engineering. 2023. May 20251Proceedings of the 31st ACM Joint European Software</p>
<p>How effective are neural networks for fixing security vulnerabilities. Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, Sameena Shah, Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis. the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis2023</p>
<p>HexT5: Unified Pre-Training for Stripped Binary Code Information Inference. Jiaqi Xiong, Guoqiang Chen, Kejiang Chen, Han Gao, Shaoyin Cheng, Weiming Zhang, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Spain: security patch analysis for binaries towards understanding the pain and pills. Zhengzi Xu, Bihuan Chen, Mahinthan Chandramohan, Yang Liu, Fu Song, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE2017</p>
<p>A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT. Dapeng Yan, Zhipeng Gao, Zhiming Liu, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Asteria: Deep learningbased AST-encoding for cross-platform binary code similarity detection. Shouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, Zhiqiang Shi, 2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). IEEE2021</p>
<p>Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge. Shouguo Yang, Chaopeng Dong, Yang Xiao, Yiran Cheng, Zhiqiang Shi, Zhi Li, Limin Sun, ACM Transactions on Software Engineering and Methodology. 332023. 2023</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang, arXiv:2402.150432024. 2024arXiv preprint</p>
<p>No more manual tests? evaluating and improving chatgpt for unit test generation. Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng, arXiv:2305.042072023. 2023arXiv preprint</p>
<p>Zhengran Zeng, Yidong Wang, Rui Xie, Wei Ye, Shikun Zhang, arXiv:2403.19287CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios. 2024. 2024arXiv preprint</p>
<p>Yifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao, Scott Thomas Andersen, Huajie Shao, Kevin Leach, Yu Huang, arXiv:2210.05102Pre-Training Representations of Binary Code Using Contrastive Learning. 2022. 2022arXiv preprint</p>
<p>Stochfuzz: Sound and cost-effective fuzzing of stripped binaries by incremental and stochastic rewriting. Zhuo Zhang, Wei You, Guanhong Tao, Yousra Aafer, Xuwei Liu, Xiangyu Zhang, 2021 IEEE Symposium on Security and Privacy (SP). IEEE2021</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Du, arXiv:2303.18223A Survey of Large Language Models. 2023</p>
<p>. Article . Publication date. 11May 2025</p>            </div>
        </div>

    </div>
</body>
</html>
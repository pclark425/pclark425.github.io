<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5018 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5018</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5018</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-259204134</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.10723v2.pdf" target="_blank">Fine-tuning Large Enterprise Language Models via Ontological Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5018.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5018.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large-chase</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large fine-tuned on chase-derived (ontological) verbalizations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-large model fine-tuned using a corpus generated by ontological reasoning (the chase) over an Enterprise Knowledge Graph; intended to improve domain-specific logical question answering, explanation and description by injecting deductive consequences into the model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (chase fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large text-to-text transformer architecture used as the base LLM; in this work it was fine-tuned on a specifically synthesized corpus produced by Vadalog-based ontological reasoning and subsequent verbalization and paraphrasing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Domain-specific deductive QA / explanation / description (chase-derived reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring answers or explanations that depend on facts derived via ontological rules (deductive consequences computed by a Datalog-based chase over an Enterprise Knowledge Graph); includes closed-book question answering, explanation, and description about derived events.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neuro-symbolic fine-tuning pipeline: compute chase via Vadalog, deterministically verbalize chase steps and reasoning plans, apply a lifting/tokenization technique to avoid exposing raw ground values, synthesize prompt-response pairs (using a pre-trained LLM as a generator), filter/paraphrase/clean pairs, then fine-tune T5-large on resulting corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative proof-of-concept only: T5-large-chase outperformed the model fine-tuned on ground facts for several sample questions (notated as questions c, d, f in the paper) where correct answers required facts present only in the chase; produced more complete answers (e.g., succeeded on events related to trader EGTech and used domain rule 1 for question e). No quantitative metrics (accuracy, F1) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Preliminary, small-scale proof-of-concept without full-scale evaluation; no quantitative benchmarking; not evaluated on text-to-query translation in this study; closed-book fine-tuning may still face known LLM issues (e.g., forgetting inferred facts) noted by the authors; dependency on quality of verbalization and corpus-synthesis pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared directly to a baseline T5-large fine-tuned only on ground facts (T5-large-ground); T5-large-chase produced superior qualitative responses on questions requiring deductive consequences. The paper contrasts this approach with BloombergGPT and SKILL-style dataset fine-tuning, noting that those approaches either target entailed factual QA or factual verbalizations but may lack reasoning beyond entailed facts.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>An ablation-style comparison: both T5-large-chase and T5-large-ground were fine-tuned for 10 epochs with identical hyperparameters; qualitative analysis on randomly chosen sample questions showed T5-large-chase succeeded on several instances where T5-large-ground failed or produced incomplete answers. No further systematic ablations (e.g., corpus size, epochs, or component ablations) or quantitative breakdowns were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5018.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5018.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large-ground</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large fine-tuned only on ground facts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline T5-large model fine-tuned on textual verbalizations of ground database facts (without chase-derived intensional inferences) used to compare against the chase-driven fine-tuning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (ground fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large text-to-text transformer architecture fine-tuned on verbalizations of extensional (ground) enterprise data only, without including facts derived by ontological reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Domain-specific QA / explanation / description (ground-only fine-tuning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book tasks where the model is expected to answer questions or provide explanations using only ground facts present in the database verbalizations (no derived facts from chase).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning T5-large on a corpus generated from ground facts (verbalized) using same hyperparameters and epochs as the chase-based model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Served as baseline: comparable responses to T5-large-chase on questions directly answerable from ground facts (questions a and b), but produced incomplete or incorrect answers for questions that required chase-derived inferences (questions c, d, e, f in the paper’s examples). No quantitative scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Unable to answer or produced incomplete answers for queries that require intensional (rule-derived) knowledge; demonstrates limitation of fine-tuning only on extensional data for tasks needing deductive inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Directly compared to T5-large-chase; T5-large-ground underperformed on examples requiring derived facts. No comparisons to broader models beyond this internal baseline within experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Comparison with T5-large-chase constitutes an ablation-like study showing the benefit of including chase-derived text in fine-tuning; both models trained with identical settings (10 epochs) so differences attributed to training corpus content rather than hyperparameters. No further ablation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5018.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5018.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (corpus synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 used as a corpus synthesis assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained generative transformer (GPT-3) is used as a text synthesis tool to produce prompt-response pairs from verbalized chase/plans, as part of the pipeline that constructs the fine-tuning corpus for the T5-large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (used for corpus generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A powerful pre-trained generative language model (GPT-3) referenced as a tool to synthesize prompt-response pairs and to perform paraphrasing in the pipeline; used to minimize manual effort in constructing the fine-tuning corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Corpus synthesis for reasoning-based fine-tuning (not a formal logical reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate textual prompt-response pairs and paraphrases from deterministic verbalizations of chase steps and reasoning plans; task is text synthesis conditioned on logical verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used as an NLP engine to synthesize candidate prompt-response pairs from tokenized/verbalized plans; pipeline minimizes calls and avoids exposing ground values via lifting/tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not quantitatively evaluated in the paper; used as a component in the pipeline. The authors note two goals (minimize calls to reduce cost and avoid disclosing ground values) and apply lifting/tokenization to address the latter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential privacy/data-disclosure risk if ground values are provided directly, which the authors mitigate via lifting and tokenization; no quantitative assessment of synthesis quality reported beyond downstream qualitative improvements seen with T5-large-chase.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>No direct comparison to alternative synthesis approaches in this paper; GPT-3 is described as an example of a powerful pre-trained LLM used for synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5018.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5018.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BloombergGPT: A large language model for finance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized LLM fine-tuned on a mix of internal enterprise financial data and public datasets cited as an example of domain-oriented fine-tuning that improves QA accuracy when answers are present in factual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large language model fine-tuned on financial-domain corpora (internal and public) to improve performance on finance-specific tasks, particularly question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Financial-domain question answering (entailment from factual inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question-answering over financial data where correct answers are contained in or directly entailed by factual inputs (data or meta-data).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on a wide range of financial data (enterprise + public datasets) to specialize the model for financial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (by the cited BloombergGPT work) to outperform state-of-the-art counterparts on financial QA tasks when answers are contained in input facts; the current paper reports this qualitatively and does not provide the numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>According to the authors' discussion, BloombergGPT was tested primarily on questions whose answers are present in input databases and hence may lack logical capabilities to answer queries requiring deductions beyond entailed facts or to combine multiple rule-based sources of knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Mentioned as outperforming prior SOTA in its testbed for QA but contrasted with the present work which aims to go beyond entailed factual QA by injecting chase-derived deductive knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not provided in this paper; BloombergGPT is only cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5018.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5018.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SKILL / KELM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SKILL: Structured Knowledge Infusion; KELM: Verbalized Wikidata corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior project examples where factual triples (e.g., from Wikidata) are verbalized into natural language and used to train/fine-tune LLMs for question-answering, illustrating data-to-text fine-tuning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SKILL (approach) / KELM (verbalized corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SKILL trains LLMs directly on verbalized factual triples (KELM) derived from structured knowledge bases (Wikidata), to improve question-answering capabilities; represented here as a related approach transforming structured data into natural language for LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Question answering based on verbalized factual triples (data-to-text fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Closed-book QA where the model is trained on natural-language renderings of structured facts so it can answer questions based on that knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning/training LLMs on verbalizations of structured KB triples (KELM) to inject factual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Cited as an example of improving QA accuracy in related work; no quantitative results are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>These approaches primarily inject factual knowledge but may not enable the model to perform multi-step deductive reasoning or to combine intensional ontological rules with extensional data in the way the present work targets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Presented as complementary background: SKILL/KELM focuses on factual verbalizations, whereas the present work emphasizes using ontological reasoning (chase) to generate training material that includes derived facts.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not provided in this paper; SKILL/KELM are cited as related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5018.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5018.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deductive reasoning (Yuan et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can pretrained language models (yet) reason deductively?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study concluding that pre-trained language models are currently unable to reliably perform deductive reasoning, have difficulty generalizing logical rules, and tend to forget previously inferred facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained LMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General statement about pretrained transformer LMs (unspecified architectures) evaluated in the cited study for deductive reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Deductive reasoning evaluation (general deductive benchmarks / analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks and analyses assessing whether pretrained LMs can generalize logical rules and perform deductive inference; the cited work examines capabilities and limitations of LMs in deductive contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Empirical evaluation/analysis of pretrained LMs on deductive reasoning tasks (as summarized by the authors); the present paper cites the study to motivate the need for incorporating explicit ontological reasoning into fine-tuning corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Summarized claim: pretrained LMs cannot yet perform deductive reasoning reliably and tend to forget inferred facts; the present paper does not report numeric results from that study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The cited work reports failures in LMs' ability to generalize logical rules and a tendency to forget previously inferred facts — used in this paper to motivate the ontological reasoning augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used to contrast purely neural LLM capabilities with the proposed neuro-symbolic approach that injects chase-derived deductive knowledge into fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not provided in this paper beyond the citation and the qualitative summary of that study's conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning Large Enterprise Language Models via Ontological Reasoning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can pretrained language models (yet) reason deductively? <em>(Rating: 2)</em></li>
                <li>Bloomberggpt: A large language model for finance <em>(Rating: 2)</em></li>
                <li>SKILL: Structured knowledge infusion for large language models <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 2)</em></li>
                <li>Finbert: A pre-trained financial language representation model for financial text mining <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5018",
    "paper_id": "paper-259204134",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "T5-large-chase",
            "name_full": "T5-large fine-tuned on chase-derived (ontological) verbalizations",
            "brief_description": "A T5-large model fine-tuned using a corpus generated by ontological reasoning (the chase) over an Enterprise Knowledge Graph; intended to improve domain-specific logical question answering, explanation and description by injecting deductive consequences into the model parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (chase fine-tuned)",
            "model_description": "T5-large text-to-text transformer architecture used as the base LLM; in this work it was fine-tuned on a specifically synthesized corpus produced by Vadalog-based ontological reasoning and subsequent verbalization and paraphrasing steps.",
            "model_size": null,
            "logical_reasoning_task": "Domain-specific deductive QA / explanation / description (chase-derived reasoning)",
            "task_description": "Tasks requiring answers or explanations that depend on facts derived via ontological rules (deductive consequences computed by a Datalog-based chase over an Enterprise Knowledge Graph); includes closed-book question answering, explanation, and description about derived events.",
            "method_or_approach": "Neuro-symbolic fine-tuning pipeline: compute chase via Vadalog, deterministically verbalize chase steps and reasoning plans, apply a lifting/tokenization technique to avoid exposing raw ground values, synthesize prompt-response pairs (using a pre-trained LLM as a generator), filter/paraphrase/clean pairs, then fine-tune T5-large on resulting corpus.",
            "performance": "Qualitative proof-of-concept only: T5-large-chase outperformed the model fine-tuned on ground facts for several sample questions (notated as questions c, d, f in the paper) where correct answers required facts present only in the chase; produced more complete answers (e.g., succeeded on events related to trader EGTech and used domain rule 1 for question e). No quantitative metrics (accuracy, F1) reported.",
            "limitations_or_failure_cases": "Preliminary, small-scale proof-of-concept without full-scale evaluation; no quantitative benchmarking; not evaluated on text-to-query translation in this study; closed-book fine-tuning may still face known LLM issues (e.g., forgetting inferred facts) noted by the authors; dependency on quality of verbalization and corpus-synthesis pipeline.",
            "comparison": "Compared directly to a baseline T5-large fine-tuned only on ground facts (T5-large-ground); T5-large-chase produced superior qualitative responses on questions requiring deductive consequences. The paper contrasts this approach with BloombergGPT and SKILL-style dataset fine-tuning, noting that those approaches either target entailed factual QA or factual verbalizations but may lack reasoning beyond entailed facts.",
            "ablation_or_analysis_results": "An ablation-style comparison: both T5-large-chase and T5-large-ground were fine-tuned for 10 epochs with identical hyperparameters; qualitative analysis on randomly chosen sample questions showed T5-large-chase succeeded on several instances where T5-large-ground failed or produced incomplete answers. No further systematic ablations (e.g., corpus size, epochs, or component ablations) or quantitative breakdowns were provided.",
            "uuid": "e5018.0",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "T5-large-ground",
            "name_full": "T5-large fine-tuned only on ground facts",
            "brief_description": "A baseline T5-large model fine-tuned on textual verbalizations of ground database facts (without chase-derived intensional inferences) used to compare against the chase-driven fine-tuning approach.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (ground fine-tuned)",
            "model_description": "T5-large text-to-text transformer architecture fine-tuned on verbalizations of extensional (ground) enterprise data only, without including facts derived by ontological reasoning.",
            "model_size": null,
            "logical_reasoning_task": "Domain-specific QA / explanation / description (ground-only fine-tuning baseline)",
            "task_description": "Closed-book tasks where the model is expected to answer questions or provide explanations using only ground facts present in the database verbalizations (no derived facts from chase).",
            "method_or_approach": "Fine-tuning T5-large on a corpus generated from ground facts (verbalized) using same hyperparameters and epochs as the chase-based model.",
            "performance": "Served as baseline: comparable responses to T5-large-chase on questions directly answerable from ground facts (questions a and b), but produced incomplete or incorrect answers for questions that required chase-derived inferences (questions c, d, e, f in the paper’s examples). No quantitative scores provided.",
            "limitations_or_failure_cases": "Unable to answer or produced incomplete answers for queries that require intensional (rule-derived) knowledge; demonstrates limitation of fine-tuning only on extensional data for tasks needing deductive inference.",
            "comparison": "Directly compared to T5-large-chase; T5-large-ground underperformed on examples requiring derived facts. No comparisons to broader models beyond this internal baseline within experiments.",
            "ablation_or_analysis_results": "Comparison with T5-large-chase constitutes an ablation-like study showing the benefit of including chase-derived text in fine-tuning; both models trained with identical settings (10 epochs) so differences attributed to training corpus content rather than hyperparameters. No further ablation provided.",
            "uuid": "e5018.1",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3 (corpus synthesis)",
            "name_full": "GPT-3 used as a corpus synthesis assistant",
            "brief_description": "A pre-trained generative transformer (GPT-3) is used as a text synthesis tool to produce prompt-response pairs from verbalized chase/plans, as part of the pipeline that constructs the fine-tuning corpus for the T5-large models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (used for corpus generation)",
            "model_description": "A powerful pre-trained generative language model (GPT-3) referenced as a tool to synthesize prompt-response pairs and to perform paraphrasing in the pipeline; used to minimize manual effort in constructing the fine-tuning corpus.",
            "model_size": null,
            "logical_reasoning_task": "Corpus synthesis for reasoning-based fine-tuning (not a formal logical reasoning benchmark)",
            "task_description": "Generate textual prompt-response pairs and paraphrases from deterministic verbalizations of chase steps and reasoning plans; task is text synthesis conditioned on logical verbalizations.",
            "method_or_approach": "Used as an NLP engine to synthesize candidate prompt-response pairs from tokenized/verbalized plans; pipeline minimizes calls and avoids exposing ground values via lifting/tokenization.",
            "performance": "Not quantitatively evaluated in the paper; used as a component in the pipeline. The authors note two goals (minimize calls to reduce cost and avoid disclosing ground values) and apply lifting/tokenization to address the latter.",
            "limitations_or_failure_cases": "Potential privacy/data-disclosure risk if ground values are provided directly, which the authors mitigate via lifting and tokenization; no quantitative assessment of synthesis quality reported beyond downstream qualitative improvements seen with T5-large-chase.",
            "comparison": "No direct comparison to alternative synthesis approaches in this paper; GPT-3 is described as an example of a powerful pre-trained LLM used for synthesis.",
            "uuid": "e5018.2",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "BloombergGPT",
            "name_full": "BloombergGPT: A large language model for finance",
            "brief_description": "A domain-specialized LLM fine-tuned on a mix of internal enterprise financial data and public datasets cited as an example of domain-oriented fine-tuning that improves QA accuracy when answers are present in factual inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BloombergGPT",
            "model_description": "A large language model fine-tuned on financial-domain corpora (internal and public) to improve performance on finance-specific tasks, particularly question answering.",
            "model_size": null,
            "logical_reasoning_task": "Financial-domain question answering (entailment from factual inputs)",
            "task_description": "Question-answering over financial data where correct answers are contained in or directly entailed by factual inputs (data or meta-data).",
            "method_or_approach": "Fine-tuning on a wide range of financial data (enterprise + public datasets) to specialize the model for financial tasks.",
            "performance": "Reported (by the cited BloombergGPT work) to outperform state-of-the-art counterparts on financial QA tasks when answers are contained in input facts; the current paper reports this qualitatively and does not provide the numerical results.",
            "limitations_or_failure_cases": "According to the authors' discussion, BloombergGPT was tested primarily on questions whose answers are present in input databases and hence may lack logical capabilities to answer queries requiring deductions beyond entailed facts or to combine multiple rule-based sources of knowledge.",
            "comparison": "Mentioned as outperforming prior SOTA in its testbed for QA but contrasted with the present work which aims to go beyond entailed factual QA by injecting chase-derived deductive knowledge.",
            "ablation_or_analysis_results": "Not provided in this paper; BloombergGPT is only cited as related work.",
            "uuid": "e5018.3",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SKILL / KELM",
            "name_full": "SKILL: Structured Knowledge Infusion; KELM: Verbalized Wikidata corpus",
            "brief_description": "Prior project examples where factual triples (e.g., from Wikidata) are verbalized into natural language and used to train/fine-tune LLMs for question-answering, illustrating data-to-text fine-tuning approaches.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SKILL (approach) / KELM (verbalized corpus)",
            "model_description": "SKILL trains LLMs directly on verbalized factual triples (KELM) derived from structured knowledge bases (Wikidata), to improve question-answering capabilities; represented here as a related approach transforming structured data into natural language for LLM fine-tuning.",
            "model_size": null,
            "logical_reasoning_task": "Question answering based on verbalized factual triples (data-to-text fine-tuning)",
            "task_description": "Closed-book QA where the model is trained on natural-language renderings of structured facts so it can answer questions based on that knowledge.",
            "method_or_approach": "Fine-tuning/training LLMs on verbalizations of structured KB triples (KELM) to inject factual knowledge.",
            "performance": "Cited as an example of improving QA accuracy in related work; no quantitative results are given in this paper.",
            "limitations_or_failure_cases": "These approaches primarily inject factual knowledge but may not enable the model to perform multi-step deductive reasoning or to combine intensional ontological rules with extensional data in the way the present work targets.",
            "comparison": "Presented as complementary background: SKILL/KELM focuses on factual verbalizations, whereas the present work emphasizes using ontological reasoning (chase) to generate training material that includes derived facts.",
            "ablation_or_analysis_results": "Not provided in this paper; SKILL/KELM are cited as related work only.",
            "uuid": "e5018.4",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Deductive reasoning (Yuan et al. 2023)",
            "name_full": "Can pretrained language models (yet) reason deductively?",
            "brief_description": "A cited study concluding that pre-trained language models are currently unable to reliably perform deductive reasoning, have difficulty generalizing logical rules, and tend to forget previously inferred facts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Pretrained LMs (general)",
            "model_description": "General statement about pretrained transformer LMs (unspecified architectures) evaluated in the cited study for deductive reasoning capabilities.",
            "model_size": null,
            "logical_reasoning_task": "Deductive reasoning evaluation (general deductive benchmarks / analyses)",
            "task_description": "Benchmarks and analyses assessing whether pretrained LMs can generalize logical rules and perform deductive inference; the cited work examines capabilities and limitations of LMs in deductive contexts.",
            "method_or_approach": "Empirical evaluation/analysis of pretrained LMs on deductive reasoning tasks (as summarized by the authors); the present paper cites the study to motivate the need for incorporating explicit ontological reasoning into fine-tuning corpora.",
            "performance": "Summarized claim: pretrained LMs cannot yet perform deductive reasoning reliably and tend to forget inferred facts; the present paper does not report numeric results from that study.",
            "limitations_or_failure_cases": "The cited work reports failures in LMs' ability to generalize logical rules and a tendency to forget previously inferred facts — used in this paper to motivate the ontological reasoning augmentation.",
            "comparison": "Used to contrast purely neural LLM capabilities with the proposed neuro-symbolic approach that injects chase-derived deductive knowledge into fine-tuning data.",
            "ablation_or_analysis_results": "Not provided in this paper beyond the citation and the qualitative summary of that study's conclusions.",
            "uuid": "e5018.5",
            "source_info": {
                "paper_title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can pretrained language models (yet) reason deductively?",
            "rating": 2,
            "sanitized_title": "can_pretrained_language_models_yet_reason_deductively"
        },
        {
            "paper_title": "Bloomberggpt: A large language model for finance",
            "rating": 2,
            "sanitized_title": "bloomberggpt_a_large_language_model_for_finance"
        },
        {
            "paper_title": "SKILL: Structured knowledge infusion for large language models",
            "rating": 2,
            "sanitized_title": "skill_structured_knowledge_infusion_for_large_language_models"
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 2,
            "sanitized_title": "knowledge_graph_based_synthetic_corpus_generation_for_knowledgeenhanced_language_model_pretraining"
        },
        {
            "paper_title": "Finbert: A pre-trained financial language representation model for financial text mining",
            "rating": 1,
            "sanitized_title": "finbert_a_pretrained_financial_language_representation_model_for_financial_text_mining"
        }
    ],
    "cost": 0.01202075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-tuning Large Enterprise Language Models via Ontological Reasoning</p>
<p>Teodoro Baldazzi 
Università Roma Tre
Italy</p>
<p>Luigi Bellomarini 
Banca d'Italia
Italy</p>
<p>Stefano Ceri 
Politecnico di Milano
Italy</p>
<p>Andrea Colombo 
Politecnico di Milano
Italy</p>
<p>Andrea Gentili 
Banca d'Italia
Italy</p>
<p>Emanuel Sallinger 
WienAustria</p>
<p>University of Oxford
UK</p>
<p>Fine-tuning Large Enterprise Language Models via Ontological Reasoning
Ontological reasoning · Language models · Knowledge graphs
Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring businesslevel definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task-and domain-specific corpora for LLM fine-tuning.</p>
<p>Introduction: Context and Overview of the Approach</p>
<p>With the recent soar of AI-based chatbots, currently led by OpenAI's Chat-GPT, the field of Natural Language Processing (NLP) and, in particular, Large Language Models (LLMs), faced a major turning point and transcended its relevance in academia and industry, steering the attention of the general public towards generative AI. While many approaches are being proposed that exploit powerful pre-trained LLMs, such as T5 [18] and GPT [16], to address a plethora of industrial tasks, current solutions show limited effectiveness at specializing the models on enterprise domains, from finance to genomics. In our community, such domain-specific knowledge can be captured by combining factual data from corporate databases with business-level definitions as ontologies in Enterprise Knowledge Graphs (EKGs), and further augmented via ontological reasoning. In this paper, we build upon this domain representation and propose a novel solution to accurately specialize LLMs on core enterprise NLP tasks. Limits of task-specific fine-tuning. LLMs can be pre-trained on extensive datasets and, often, specialized with a fine-tuning process that customizes them so as to perform given NLP tasks [20], such as question-answering, language translation, named-entity recognition, document summarization, sentiment analysis, and more [7]. According to a very common usage pattern, general-purpose LLMs are fine-tuned for a specific NLP task based on extensive cross-or domaingeneric textual corpora that are publicly available [17].</p>
<p>While this approach highlights good generalization capabilities and a surprising human-style interaction, the obtained models have major shortcomings in that they lack enterprise knowledge and trivially fail to solve domain-specific NLP tasks. For instance, in the financial domain, state-of-the-art yet generalist models have shown poor performance for different NLP tasks, for which, on the other hand, further fine-tuning with large additional text corpora has been proved to be helpful in improving the results, such as in the case of FinBert [12].</p>
<p>Limits of domain-specific fine-tuning. Going further, recent studies are exploring the usage of factual data from enterprise databases to fine-tune LLMs and try to tackle domain-specific question-answering tasks: the factual information is leveraged to synthesize prompt-response pairs based on the data and customize the LLM in a task-and domain-specific direction. A primary example is the SKILL project [15], where an LLM is directly trained on factual triples derived from the translation into natural language-the so-called verbalization-of Wikidata (namely, the KELM corpus [2]) for question-answering tasks. Similarly, other approaches highlight possible improvements of accuracy in question-answering tasks, when textual information is first captured into a database, which is then verbalized and employed for fine-tuning [3].</p>
<p>Yet, even the combination of general-purpose knowledge of the pre-trained model and the domain data still offers an accuracy that is not acceptable for core tasks in specialized domains. For example, BloombergGPT [22] is an LLM fine-tuned on a wide range of financial data, combining internal enterprise knowledge with publicly-available datasets. The results show that the model fine-tuned for the question-answering task outperforms state-of-the-art counterparts by being able to correctly answer questions related to the financial domain. However, BloombergGPT has been tested only on questions whose answers are already contained in (or directly entailed by) the factual information of the input databases, either as data or meta-data (e.g., schema information). It is reasonable, in fact, that it does not have enough fine-tuning data or logical capabilities to go further.</p>
<p>A look beyond current solutions. Conversely, from an enterprise application perspective, it would be extremely useful to answer questions by means of intelligent combined uses of the input databases with other logic-intensive sources of knowledge (e.g., regulatory bodies, best practices, domain experts, etc.). For instance, in the context of financial cases like those of interest for a Central Bank, answering questions such as "why does shareholder X exert a relevant influence on the financial intermediary Y? " (explanation), or "how does this smart contract behave?" (description), or "is the merger of banks Y and W lawful from a regulatory perspective? " (question answering), or "based on data, how many ties with other intermediaries does Z have?" (text-to-query translation) would be an essential asset.</p>
<p>At present, all the mentioned tasks are far from being solved by off-theshelf libraries or, directly, by most recent LLMs, and are open research. Going into the details of each of them is beyond the scope of this paper, but the motivations laid out mainly in a question-answering perspective give the flavour of why LLMs are not enough. It is worth remarking, though, that even the translation task, for which thanks to LLMs much progress has been made in the transformation of natural language into the target query languages (say, SQL, SPARQL, etc.) [21,23] is still a largely unsolved problem, especially in the context of languages with an elaborate grammar and complex queries [9].</p>
<p>Ontological reasoning. From another perspective, in the Knowledge Representation and Reasoning [11] (KRR) community, the state-of-the-art research on ontological reasoning over EKGs makes a point of being able to offer a compact combination of factual database information (the extensional knowledge) and formally specified business awareness, for instance in the form of logical rules (the intensional knowledge), to serve domain-specific query answering in an accurate manner. For example, logical KGs exploiting efficient fragments of the Datalog ± family [8] have been successfully adopted for financial applications [6].</p>
<p>Yet, there is an impedance mismatch between NLP and ontological reasoning, which lacks the flexibility and the language orientation to solve explanation, description, question answering, and translation tasks: queries need to be specified in KRR formalisms; all the inputs and the results are facts/n-tuples/triples; the generation of new knowledge is possible only to the extent reasoning rules capture it. Conversely, while being very good at manipulating human language, LLMs lack a comprehensive domain model, a pillar of KRR approaches.</p>
<p>An integrated approach. This paper strives to strengthen LLMs in their use for task-and domain-specific applications, by letting the fine-tuning process be driven by an ontological reasoning task on an EKG. We operate in the context of the Vadalog [5] system, a Datalog-based reasoning engine for EKGs, that finds many industrial applications [6]. We use Vadalog to explore the factual information derived by applying the domain rules, via the chase procedure [13], to the enterprise data and synthesize a fine-tuning corpus that covers the entire "reasoning space" to convey domain-specificity to the LLM. A summary of the resulting fine-tuning pipeline, provided in Figure 1, will guide our discussion.</p>
<p>More in detail, our contributions can be summarized as follows.</p>
<p>-We present a reasoning verbalization technique that generates sets of prompt-response pairs from ground Datalog rules. We provide the algorithm and optimize it with a lifting technique exploiting reasoning regularities. -We deliver such an approach in a novel neurosymbolic architecture that fine-tunes task-specific LLMs for a set of four relevant NLP tasks, namely, explanation, description, question answering, and translation. -We discuss a preliminary proof-of-concept confirming the validity of our approach and comparing models fine-tuned on ground and chase data.</p>
<p>Overview. In Section 2 we present our architecture. A preliminary experimental validation is provided in Section 3. We draw our conclusions in Section 4. </p>
<p>A Neurosymbolic Pipeline to Fine-tune LLMs</p>
<p>The input blocks of the fine-tuning pipeline in Figure 1 are and Σ. They are, respectively, a database of domain facts and a set of reasoning rules, capturing the business dynamics. Our rules are expressed in Vadalog. An EKG is a combination Σ( ) of and Σ, obtained through reasoning. The set Σ( ) is computed via the chase [13]: starting from Σ( ) = , the chase augments Σ( ) with facts derived from the application of the rules in Σ to fixpoint.</p>
<p>Let us introduce our running example: a simple trading activity managed with a smart contract [14]. Here, contains a log over time of buy/sell orders from the traders who invest in the smart contract as well as market information, e.g., asset prices (Price), or market shutdowns (MarketClosed ). </p>
<p>Close( , 2 ), Price( 2 , 2 ), Position( , , , 1 ),
2 &gt; 1 , = * 2 − → Return(x,pl)(3)
If a trader wants to open a position (buy) on a certain asset of size at time 1 and the market is open at 1 , the order is accepted (rule 1). If the order by is accepted and the asset price at 1 is 1 , then holds a position on the market at time 1 of size and of notional (total value) equal to * 1 (rule 2). If, later at 2 , trader decides to close its position (sell) and the price at 2 is 2 , then gets returns (profits or losses) from its trading activity as * 2 − (rule 3).</p>
<p>Applying the vision we laid out to Example 1, the goal of our pipeline is finetuning an LLM to address explanation, description, question answering, and Algorithm 1 Reasoning-based LLM Fine-tuning. return ftModel text-to-query translation tasks for the simple trading activity at hand. Let us follow Figure 1 and Algorithm 1 to describe the application of the pipeline to a database = {Open(EGTech,0.3,1), Open(IEComp,0.5,1), Price(124,1), Price(147, 9), Close(EGTech, 9), MarketClose(5)}.</p>
<p>Chase generation. The first step of our pipeline builds the chase Σ( ), that is, the expansion of with the facts that can be derived by applying the rules of Σ (line 2, in the algorithm). Rule 1 generates the fact Accepted(EGTech, 0.3, 1), as the market is not closed at time 1. Then, Position(EGTech, 0.3, 37.2, 1) is derived via rule 2. Finally, as trader EGTech closes the position, i.e., sells the asset, at time 9 and the price goes up to 147$, then EGTech gets a profit of 6.9$.</p>
<p>Domain verbalization. Whenever a Vadalog rule is involved in the chase, it is translated into pure text with a deterministic transformation, based on the select-project-join semantics, which looks up a glossary of atom descriptions. When rules involve aggregation functions, allowed in Vadalog, the process is less straightforward and involves unfolding a chain of chase activations altogether [1] (line 7). At the end of this phase, we are in hold of a "sincethen closure" of our domain, that focuses on what can be obtained by activating the intensional knowledge of Σ. From another perspective, Σ can be seen as an attention mechanism, to select the fragment of that one wants to verbalize. For instance, with respect to our running example, the chase step Open (EGTech, 0.3, 1), ¬MarketClose (1) → Accepted (EGTech, 0.3, 1) (rule 1) is verbalized as: Since the trader EGTech at time 1 sends an order to open a position of size 0.3, and it is not true that 1 is a time when the market is closed, then the order of size 0.3 by EGTech is accepted at time 1.</p>
<p>Fine-tuning corpus generation. With the basic verbalization available, we are now ready to generate the fine-tuning corpus. We consider the corpus gen-eration itself as a text manipulation task and exploit the effectiveness of powerful pre-trained LLMs [7], such as GPT-3, to synthesize a finite set of possible prompt-response pairs. Here we have two goals: 1) minimising the number of "calls" to the LLM, for cost-and time-efficiency reasons; 2) avoiding any ground value (coming from the EKG) being disclosed to the LLM, for data protection reasons. We leverage the regularity of logical languages and resort to a lifting technique. We build a logic plan out of Σ (line 10). A plan is the equivalent in our context of a database execution plan and can be seen as the dependency graph of the rules of Σ, where nodes represent rules and edges stand for head-body dependencies. The plan is then verbalized, obtaining a text with tokens as placeholders for rule variables. Finally, a tokenized fine-tuning corpus is generated from the plan, after minor pre-processing (line 11). The form of the prompts depends on the task. Now, for each verbalized chase step, we look up the corresponding verbalized portion of the plan and instantiate its tokens (lines [13][14][15]. Note that no invocations to the corpus generator are needed in this phase. Figure 2 exemplifies the generation process in our example domain. Quality-driven optimization. The corpus undergoes a quality check where each pair is filtered according to an NLP-based scoring model in terms of specificity, plausibility, absence of bias, and other user-defined criteria. The filtered-in pairs are enhanced via NLP paraphrasing to improve generalization and finally cleansed with additional post-processing procedures (lines [16][17][18][19][20][21][22].</p>
<p>Model fine-tuning. The refined corpus is injected into an LLM for task-and domain-specific fine-tuning (line 23). In the case of Q&amp;A, the model operates in a closed-book approach, that is, it learns to map questions to the corresponding answers without extracting them from an input context, but rather encapsulating the knowledge of the domain into its internal parameters and weights [19]. The resulting specialized model is provided to the user via API, and will act as a natural language interface to the EKG and the ontological reasoning at its foundation in a neurosymbolic fashion.</p>
<p>Preliminary Validation via Proof-of-Concept</p>
<p>We implemented our fine-tuning pipeline in Vadalog. A full-scale evaluation of our architecture is beyond the scope of this short work. Conversely, in this section, we propose a conceptual validation of the approach, by briefly showing some executions of the pipeline. We will not consider the text-to-query translation task, as its evaluation would require semantic comparison, which is beyond the scope of this work.</p>
<p>For the proof-of-concept, we made use of a T5-large [10] model and considered the same domain as in Example 1. To obtain a dataset that could be visually inspected to informally assess the quality of the textual answers given by an LLM fine-tuned with our pipeline, we performed a kind of ablation study. For randomly chosen sets of sample questions, for the NLP tasks of interest, we compared the answers provided by an LLM fine-tuned only with ground facts (T5-large-ground ) and one fine-tuned with our pipeline (T5-large-chase). Both models were trained for 10 epochs and with the same hyperparameters. The fine-tuning corpora and the models are made available [4]. Figure 3 visually reports the comparison. Questions a and b are the baseline, as they can be answered by facts in . Apart from a less refined write-up, the LLMs show the same output behaviour. On the other hand, in questions c, d, and f T5-large-ground is outperformed by T5-large-chase, which succeeds in answering about events related to trader EGTech. Actually, the corresponding facts derive from Σ( ), which is not considered in the ground fine-tuning. Similarly, the answer to question e by T5-large-ground is incomplete and only T5-largechase is able to use the specific domain knowledge from rule 1 of Example 1. </p>
<p>Conclusion</p>
<p>According to a recent work [24] appeared in the European Chapter of the Association for Computational Linguistics, pre-trained language models cannot yet perform deductive reasoning: they are still unable to generalize logical rules and, even when rules are provided, LLMs tend to forget previously inferred facts. While no extensive comparison between transformer architectures and reasoning approaches has been conducted yet, our work showed that LLM performance for domain-specific NLP tasks can be visibly improved by producing a fine-tuning corpus as a byproduct of ontological reasoning. We capitalized on our experience in deductive reasoning to offer a first step towards a neuro-symbolic platform for reasoning on enterprise knowledge graphs.</p>
<p>Fig. 1 :
1Neurosymbolic pipeline for reasoning-based LLM fine-tuning.</p>
<p>Example 1 .
1The following set Σ contains the Vadalog rules governing the basic functioning of the market, i.e., under which conditions the orders are accepted and how profits and losses are computed.Open( , , 1 ), ¬ MarketClosed( 1 ) → Accepted( , , 1 )(1)Accepted( , , 1 ), Price( 1 , 1 ), = * 1 → Position( , , , 1 )</p>
<p>Fig. 2 :
2From plans to fine-tuning corpus, in our running example.</p>
<p>Fig. 3 :
3Proof-of-concept for our fine-tuning pipeline.
Acknowledgements. The work on this paper was partially supported by the Vienna Science and Technology Fund (WWTF) grant VRG18-013.
Linearisability on datalog programs. F N Afrati, M Gergatsoulis, F Toni, Theor. Comput. Sci. 3081-3Afrati, F.N., Gergatsoulis, M., Toni, F.: Linearisability on datalog programs. Theor. Comput. Sci. 308(1-3), 199-226 (2003)</p>
<p>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. O Agarwal, H Ge, S Shakeri, R Al-Rfou, arXiv:2010.12688arXiv preprintAgarwal, O., Ge, H., Shakeri, S., Al-Rfou, R.: Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint arXiv:2010.12688 (2020)</p>
<p>Enhanced story comprehension for large language models through dynamic document-based knowledge graphs. B R Andrus, Y Nasiri, S Cui, B Cullen, N Fulda, AAAI. 3610Andrus, B.R., Nasiri, Y., Cui, S., Cullen, B., Fulda, N.: Enhanced story compre- hension for large language models through dynamic document-based knowledge graphs. AAAI 36(10), 10436-10444 (2022)</p>
<p>. T Baldazzi, L Bellomarini, S Ceri, A Colombo, A Gentili, E Sallinger, Baldazzi, T., Bellomarini, L., Ceri, S., Colombo, A., Gentili, A., Sallinger, E.: Material. https://bit.ly/44249b5, accessed: 2023-06-17</p>
<p>Vadalog: A modern architecture for automated reasoning with large knowledge graphs. L Bellomarini, D Benedetto, G Gottlob, E Sallinger, IS. 105Bellomarini, L., Benedetto, D., Gottlob, G., Sallinger, E.: Vadalog: A modern ar- chitecture for automated reasoning with large knowledge graphs. IS 105 (2022)</p>
<p>Knowledge graphs and enterprise AI: the promise of an enabling technology. L Bellomarini, D Fakhoury, G Gottlob, E Sallinger, ICDE. Bellomarini, L., Fakhoury, D., Gottlob, G., Sallinger, E.: Knowledge graphs and enterprise AI: the promise of an enabling technology. In: ICDE. pp. 26-37 (2019)</p>
<p>Language models are few-shot learners. T Brown, In: NeurIPS. 33Curran Associates, IncBrown, T., et al.: Language models are few-shot learners. In: NeurIPS. vol. 33, pp. 1877-1901. Curran Associates, Inc. (2020)</p>
<p>A general datalog-based framework for tractable query answering over ontologies. A Calì, G Gottlob, T Lukasiewicz, J. Web Semant. 14Calì, A., Gottlob, G., Lukasiewicz, T.: A general datalog-based framework for tractable query answering over ontologies. J. Web Semant. 14, 57-83 (2012)</p>
<p>Catsql: Towards real world natural language to sql applications. H Fu, C Liu, B Wu, F Li, J Tan, J Sun, VLDB. 166Fu, H., Liu, C., Wu, B., Li, F., Tan, J., Sun, J.: Catsql: Towards real world natural language to sql applications. VLDB 16(6), 1534-1547 (2023)</p>
<p>Google: T5 large. Google: T5 large. https://huggingface.co/t5-large, accessed: 2023-06-17</p>
<p>Ontologies for knowledge graphs: Breaking the rules. M Krötzsch, V Thost, ISWC (1). 9981Krötzsch, M., Thost, V.: Ontologies for knowledge graphs: Breaking the rules. In: ISWC (1). LNCS, vol. 9981, pp. 376-392 (2016)</p>
<p>Finbert: A pre-trained financial language representation model for financial text mining. Z Liu, D Huang, K Huang, Z Li, J Zhao, IJCAI 2020. Liu, Z., Huang, D., Huang, K., Li, Z., Zhao, J.: Finbert: A pre-trained financial language representation model for financial text mining. In: IJCAI 2020 (2021)</p>
<p>Testing implications of data dependencies. D Maier, A O Mendelzon, Y Sagiv, ACM TODS. 44Maier, D., Mendelzon, A.O., Sagiv, Y.: Testing implications of data dependencies. ACM TODS 4(4), 455-469 (1979)</p>
<p>An overview of smart contract and use cases in blockchain technology. B K Mohanta, S S Panda, D Jena, ICCCNT. pp. Mohanta, B.K., Panda, S.S., Jena, D.: An overview of smart contract and use cases in blockchain technology. In: ICCCNT. pp. 1-4 (2018)</p>
<p>SKILL: Structured knowledge infusion for large language models. F Moiseev, Z Dong, E Alfonseca, M Jaggi, ACL 2022. Moiseev, F., Dong, Z., Alfonseca, E., Jaggi, M.: SKILL: Structured knowledge infusion for large language models. In: ACL 2022. pp. 1581-1588 (2022)</p>
<p>. A Radford, K Narasimhan, T Salimans, I Sutskever, Improving language understanding by generative pre-trainingRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan- guage understanding by generative pre-training (2018)</p>
<p>J W Rae, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprintRae, J.W., et al.: Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, J. Mach. Learn. Res. 2167Raffel, C., et al.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140:1-140:67 (2020)</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, In: EMNLP (1). ACLRoberts, A., Raffel, C., Shazeer, N.: How much knowledge can you pack into the parameters of a language model? In: EMNLP (1). pp. 5418-5426. ACL (2020)</p>
<p>Transfer learning in natural language processing. S Ruder, M E Peters, S Swayamdipta, T Wolf, NAACL: Tutorials. Ruder, S., Peters, M.E., Swayamdipta, S., Wolf, T.: Transfer learning in natural language processing. In: NAACL: Tutorials. pp. 15-18 (2019)</p>
<p>Rat-sql: Relation-aware schema encoding and linking for text-tosql parsers. B Wang, arXiv:1911.04942arXiv preprintWang, B., et al.: Rat-sql: Relation-aware schema encoding and linking for text-to- sql parsers. arXiv preprint arXiv:1911.04942 (2019)</p>
<p>Bloomberggpt: A large language model for finance. S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D S Rosenberg, G Mann, abs/2303.17564Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D.S., Mann, G.: Bloomberggpt: A large language model for finance. CoRR abs/2303.17564 (2023)</p>
<p>Neural machine translating from natural language to sparql. X Yin, D Gromann, S Rudolph, Future Generation Computer Systems. 117Yin, X., Gromann, D., Rudolph, S.: Neural machine translating from natural lan- guage to sparql. Future Generation Computer Systems 117, 510-519 (2021)</p>
<p>Can pretrained language models (yet) reason deductively? In: EACL. Z Yuan, S Hu, I Vulic, A Korhonen, Z Meng, Yuan, Z., Hu, S., Vulic, I., Korhonen, A., Meng, Z.: Can pretrained language models (yet) reason deductively? In: EACL. pp. 1439-1454 (2023)</p>            </div>
        </div>

    </div>
</body>
</html>
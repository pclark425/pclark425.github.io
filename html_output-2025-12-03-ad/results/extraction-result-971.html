<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-971 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-971</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-971</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-4f601b4e561557c7a0bd5a741a54cabaec7dc70e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f601b4e561557c7a0bd5a741a54cabaec7dc70e" target="_blank">Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners and shows that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model.</p>
                <p><strong>Paper Abstract:</strong> There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e971.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e971.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-acquired PDDL pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-acquired PDDL world model construction and correction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses large language models (primarily GPT-4, with comparisons to GPT-3.5-Turbo) to generate lifted PDDL action models (predicates, parameters, preconditions, effects), then uses validator feedback and human natural-language corrections (mediated by the LLM) to produce corrected deterministic PDDL domain models for use by external planners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-acquired PDDL pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system prompts an LLM with domain context, one/two blocksworld examples, an action description, and a dynamic predicate list; the LLM produces PDDL action schemas (parameters, preconditions, effects, and natural-language predicate descriptions). Syntax errors are caught by PDDL validators and simple scripts; factual errors are surfaced as natural-language feedback to humans or validators, and the LLM is used as an interface to incorporate corrections back into the PDDL models. The pipeline iterates over actions (action-by-action generation) and then reruns a pass with the accumulated predicate list to produce a coherent domain.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (lifted STRIPS-style action models)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>World model is an explicit symbolic PDDL domain: fluents are predicates (with typed parameters), actions are lifted operators with preconditions (conjunctions of predicates) and effects (add/delete lists). The representation used is deterministic STRIPS-style PDDL (no probabilistic effects, no PPDDL or belief states in the implemented pipeline). The initial PDDL output from LLMs may contain syntactic and factual errors which are corrected via validator and human feedback mediated by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model construction, translating PDDL to/from natural language for human inspection, translating user instructions to PDDL goals, mediating corrective feedback</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary); GPT-3.5-Turbo (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>External classical STRIPS planners (e.g., Fast Downward), local-search planner LPG used optionally to seed search</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Household domain (custom, similar to ALFWorld), Logistics (IPC), Tyreworld (IPC)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Custom Household domain: single-arm household robot with 22 mobile/manipulation skills, physically realistic constraints; Logistics and Tyreworld are IPC benchmark domains with specialized actions. These are symbolic planning domains (not probabilistic), used to evaluate PDDL model construction and downstream planning; Household is more complex than standard ALFWorld benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PDDL model quality (number of correction annotations), and planning success rate when using external planners (success rate on planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4 produced PDDL models for over 40 actions (41 actions mentioned); after correction the extracted PDDL models were used to solve planning tasks with a classical planner (Fast Downward) achieving ~95% success on sampled tasks (failures mostly due to goal translation errors). GPT-4 initial PDDL outputs had far fewer errors (e.g., 53 errors in Household) than GPT-3.5-Turbo (>>100 errors).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline: vanilla LLM planner (GPT-4) without symbolic PDDL validation had extremely low success; LLM plans validated and re-prompted via VAL improved but remained below 50% success. GPT-3.5-Turbo produced much noisier PDDL models and failed more often at correction.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation was reported that compares probabilistic vs. deterministic symbolic models or explicit uncertainty modeling; ablations focused on different LLMs (GPT-4 vs GPT-3.5) and on using validator/human feedback to correct models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (GPT-4) can be leveraged to construct readable, usable deterministic PDDL world models for multiple actions; using the LLM as an interface to validator/human feedback enables correction of factual and syntactic errors; pairing the extracted PDDL with sound external planners yields high planning reliability (e.g., ~95% success) while directly using LLMs as planners remains unreliable. The paper does not construct or use probabilistic symbolic models (PPDDL) or belief-state representations nor does it integrate LLM uncertainty into probabilistic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e971.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e971.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classical planner with LLM-acquired PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical planner (Fast Downward) using LLM-acquired PDDL domain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using the corrected PDDL domain produced by the LLM pipeline, a standard domain-independent classical planner (Fast Downward) is used to compute satisficing plans given grounded initial states and LLM-translated goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Classical planner with LLM-acquired PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The corrected PDDL domain (lifted operators and predicates) is combined with grounded initial states (obtained by translating environment descriptions into predicate assignments via LLMs or vision-language models) and PDDL goal specifications (translated from natural language by GPT-4). The classical planner Fast Downward is then used to search for plans (STRIPS search); optionally LLM-generated plans can seed a local-search planner (LPG) to accelerate search.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (STRIPS-style deterministic domain)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Domain: typed predicates and lifted action models (preconditions, add/delete effects). States: truth assignments to predicates. Transitions: deterministic application of action effects given preconditions. No probabilistic effects or belief-state representation used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model creator (PDDL), translating user goals into PDDL goal specs, optional seeding of search with LLM plans</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used to construct domain and to translate goals); GPT-3.5-Turbo evaluated as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Fast Downward (classical STRIPS planner); optionally LPG for local-search with LLM plan seeds</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Household (custom), Logistics (IPC), Tyreworld (IPC)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Symbolic planning benchmark domains (Household is a household-robot domain with richer constraints than ALFWorld; Logistics and Tyreworld are IPC domains). Not probabilistic/text-adventure environments per se.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Planning success rate (valid plan found & executable in symbolic model)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fast Downward with LLM-acquired and corrected domain models found valid plans in 95% of sampled tasks (failures mostly due to goal translation errors), demonstrating that PDDL extracted from LLMs can drive sound classical planners effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to vanilla LLM planners (which had extremely low success) and LLM planners augmented with validation feedback (which improved but stayed below 50%), the classical-planner + LLM-acquired PDDL approach had substantially higher success.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No uncertainty-ablation; comparisons were between planners that use the symbolic PDDL vs. LLM-based planning approaches and different LLMs for PDDL construction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using explicit deterministic PDDL domains constructed and corrected via LLMs enables reliable planning with classical planners; this approach sidesteps the need to rely on LLMs' internal (and unreliable) combinatorial search abilities and does not model or incorporate LLM output uncertainty probabilistically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e971.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e971.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-modulo-planner (ReAct + VAL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM planner (ReAct/GPT-4) back-prompted by VAL using LLM-acquired PDDL domain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where an LLM planner (ReAct with GPT-4) generates plans in natural language, which are translated to PDDL and validated by VAL; VAL's symbolic validation feedback (unmet preconditions/goal conditions) is translated back to natural language and used to re-prompt the LLM planner iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-modulo-planner backprompted by VAL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The LLM planner is given detailed natural-language descriptions of actions (produced by translating the PDDL domain to NL). It generates candidate plans; these are translated to symbolic form and validated by VAL to identify unmet preconditions/goal conditions. Validation outputs (PDDL errors) are converted to NL via GPT-4 and fed back to the LLM planner to iteratively refine the plan. The authors limit iteration counts in experiments and report practical behaviors (e.g., loops, repeated mistakes).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (used as symbolic simulator / validator)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>The PDDL domain (deterministic STRIPS) is used to symbolically validate LLM-generated plans: states are predicate assignments; plan steps are tested for precondition satisfaction and goal achievement deterministically. The system does not represent probabilistic transitions or belief states; validator reports deterministic unmet preconditions/effects.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>plan generation (ReAct prompting with GPT-4), translation between PDDL and natural language, integrating validator feedback</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (as the LLM planner implementing ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>ReAct prompting loop (LLM-based reasoning+acting) with symbolic plan validation by VAL; can be viewed as iterative re-prompting rather than an algorithmic search like A*</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Household domain (custom), Logistics</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Same symbolic domains as above; LLM planner uses NL action descriptions derived from the PDDL domain to generate plans, which are validated against the PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success rate after iterative validation-feedback re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ReAct (GPT-4) without validation: extremely low success; ReAct + VAL feedback improved correctness but overall success remained below 50% (limited to at most 8 feedback iterations in experiments). Authors report LLM often repeats the same mistakes or gets stuck in loops when trying to use feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to classical-planner + LLM-acquired PDDL (â‰ˆ95% success), the LLM-modulo-planner approach underperforms; compared to vanilla LLM planner it improves correctness but still falls short of classical planning reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No probabilistic/uncertainty ablation; experiments varied prompt examples and action descriptions but did not incorporate probabilistic belief models for LLM uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using PDDL-derived validation feedback (VAL) to back-prompt an LLM planner improves plan correctness relative to raw LLM outputs, but does not reach the reliability of classical planners using explicit PDDL domains; LLM planners tend not to effectively use iterative symbolic feedback and can loop or introduce new errors. No probabilistic modeling of LLM uncertainty or belief states was implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e971.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e971.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plan validation tool that checks PDDL plans against a PDDL domain and initial/goal specification, reporting syntactic errors or unmet preconditions/goal conditions deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VAL: Automatic plan validation, continuous effects and mixed initiative planning using pddl</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VAL plan validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>VAL is used to validate generated PDDL domains for syntax/type errors and to validate candidate plans by checking precondition satisfaction and goal achievement step-by-step. The validator's output (PDDL-format error messages) is translated to natural language by the LLM and used as feedback to correct PDDL models or to re-prompt LLM planners.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL validator (operates on deterministic PDDL domains and plans)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>VAL interprets typed PDDL domains and grounded plans; it deterministically examines action preconditions and effects against current symbolic state and reports violations (no probabilistic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Plan validation (symbolic simulation of PDDL plan execution), not a planner per se</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>A general-purpose PDDL validation tool used here on Household, Logistics, and Tyreworld domains to detect syntax/type errors and unmet preconditions/goal conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Ability to detect syntactic/type errors and unmet preconditions in candidate PDDL domains/plans</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used successfully to detect simple syntax/type issues and to supply concise error messages that GPT-4 could use to correct PDDL models; no numeric accuracy metric reported for VAL itself.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not applicable as VAL is used as a validator/tool in the pipeline; simple Python scripts supplemented VAL to detect lightweight syntax issues.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No uncertainty-related ablations for VAL; it operates deterministically on PDDL input.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VAL is an effective deterministic validator to detect syntax/type errors and unmet preconditions; its feedback, when translated by an LLM, can be used to correct PDDL models, but VAL does not provide probabilistic uncertainty estimates about LLM outputs nor belief-state reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llm+ p: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>Pddl planning with pretrained large language models <em>(Rating: 2)</em></li>
                <li>Planning with large language models via corrective re-prompting <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-971",
    "paper_id": "paper-4f601b4e561557c7a0bd5a741a54cabaec7dc70e",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "LLM-acquired PDDL pipeline",
            "name_full": "LLM-acquired PDDL world model construction and correction pipeline",
            "brief_description": "A pipeline that uses large language models (primarily GPT-4, with comparisons to GPT-3.5-Turbo) to generate lifted PDDL action models (predicates, parameters, preconditions, effects), then uses validator feedback and human natural-language corrections (mediated by the LLM) to produce corrected deterministic PDDL domain models for use by external planners.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-acquired PDDL pipeline",
            "system_description": "The system prompts an LLM with domain context, one/two blocksworld examples, an action description, and a dynamic predicate list; the LLM produces PDDL action schemas (parameters, preconditions, effects, and natural-language predicate descriptions). Syntax errors are caught by PDDL validators and simple scripts; factual errors are surfaced as natural-language feedback to humans or validators, and the LLM is used as an interface to incorporate corrections back into the PDDL models. The pipeline iterates over actions (action-by-action generation) and then reruns a pass with the accumulated predicate list to produce a coherent domain.",
            "world_model_type": "PDDL (lifted STRIPS-style action models)",
            "world_model_description": "World model is an explicit symbolic PDDL domain: fluents are predicates (with typed parameters), actions are lifted operators with preconditions (conjunctions of predicates) and effects (add/delete lists). The representation used is deterministic STRIPS-style PDDL (no probabilistic effects, no PPDDL or belief states in the implemented pipeline). The initial PDDL output from LLMs may contain syntactic and factual errors which are corrected via validator and human feedback mediated by the LLM.",
            "uses_llm": true,
            "llm_role": "world model construction, translating PDDL to/from natural language for human inspection, translating user instructions to PDDL goals, mediating corrective feedback",
            "llm_model_name": "GPT-4 (primary); GPT-3.5-Turbo (baseline comparisons)",
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "External classical STRIPS planners (e.g., Fast Downward), local-search planner LPG used optionally to seed search",
            "planning_integrates_uncertainty": false,
            "text_environment_name": "Household domain (custom, similar to ALFWorld), Logistics (IPC), Tyreworld (IPC)",
            "text_environment_description": "Custom Household domain: single-arm household robot with 22 mobile/manipulation skills, physically realistic constraints; Logistics and Tyreworld are IPC benchmark domains with specialized actions. These are symbolic planning domains (not probabilistic), used to evaluate PDDL model construction and downstream planning; Household is more complex than standard ALFWorld benchmarks.",
            "performance_metric": "PDDL model quality (number of correction annotations), and planning success rate when using external planners (success rate on planning tasks)",
            "performance_value": "GPT-4 produced PDDL models for over 40 actions (41 actions mentioned); after correction the extracted PDDL models were used to solve planning tasks with a classical planner (Fast Downward) achieving ~95% success on sampled tasks (failures mostly due to goal translation errors). GPT-4 initial PDDL outputs had far fewer errors (e.g., 53 errors in Household) than GPT-3.5-Turbo (&gt;&gt;100 errors).",
            "baseline_comparison": "Baseline: vanilla LLM planner (GPT-4) without symbolic PDDL validation had extremely low success; LLM plans validated and re-prompted via VAL improved but remained below 50% success. GPT-3.5-Turbo produced much noisier PDDL models and failed more often at correction.",
            "has_ablation_uncertainty": false,
            "ablation_results": "No ablation was reported that compares probabilistic vs. deterministic symbolic models or explicit uncertainty modeling; ablations focused on different LLMs (GPT-4 vs GPT-3.5) and on using validator/human feedback to correct models.",
            "key_findings": "LLMs (GPT-4) can be leveraged to construct readable, usable deterministic PDDL world models for multiple actions; using the LLM as an interface to validator/human feedback enables correction of factual and syntactic errors; pairing the extracted PDDL with sound external planners yields high planning reliability (e.g., ~95% success) while directly using LLMs as planners remains unreliable. The paper does not construct or use probabilistic symbolic models (PPDDL) or belief-state representations nor does it integrate LLM uncertainty into probabilistic planning.",
            "uuid": "e971.0",
            "source_info": {
                "paper_title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Classical planner with LLM-acquired PDDL",
            "name_full": "Classical planner (Fast Downward) using LLM-acquired PDDL domain",
            "brief_description": "Using the corrected PDDL domain produced by the LLM pipeline, a standard domain-independent classical planner (Fast Downward) is used to compute satisficing plans given grounded initial states and LLM-translated goals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Classical planner with LLM-acquired PDDL",
            "system_description": "The corrected PDDL domain (lifted operators and predicates) is combined with grounded initial states (obtained by translating environment descriptions into predicate assignments via LLMs or vision-language models) and PDDL goal specifications (translated from natural language by GPT-4). The classical planner Fast Downward is then used to search for plans (STRIPS search); optionally LLM-generated plans can seed a local-search planner (LPG) to accelerate search.",
            "world_model_type": "PDDL (STRIPS-style deterministic domain)",
            "world_model_description": "Domain: typed predicates and lifted action models (preconditions, add/delete effects). States: truth assignments to predicates. Transitions: deterministic application of action effects given preconditions. No probabilistic effects or belief-state representation used.",
            "uses_llm": true,
            "llm_role": "world model creator (PDDL), translating user goals into PDDL goal specs, optional seeding of search with LLM plans",
            "llm_model_name": "GPT-4 (used to construct domain and to translate goals); GPT-3.5-Turbo evaluated as baseline",
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Fast Downward (classical STRIPS planner); optionally LPG for local-search with LLM plan seeds",
            "planning_integrates_uncertainty": false,
            "text_environment_name": "Household (custom), Logistics (IPC), Tyreworld (IPC)",
            "text_environment_description": "Symbolic planning benchmark domains (Household is a household-robot domain with richer constraints than ALFWorld; Logistics and Tyreworld are IPC domains). Not probabilistic/text-adventure environments per se.",
            "performance_metric": "Planning success rate (valid plan found & executable in symbolic model)",
            "performance_value": "Fast Downward with LLM-acquired and corrected domain models found valid plans in 95% of sampled tasks (failures mostly due to goal translation errors), demonstrating that PDDL extracted from LLMs can drive sound classical planners effectively.",
            "baseline_comparison": "Compared to vanilla LLM planners (which had extremely low success) and LLM planners augmented with validation feedback (which improved but stayed below 50%), the classical-planner + LLM-acquired PDDL approach had substantially higher success.",
            "has_ablation_uncertainty": false,
            "ablation_results": "No uncertainty-ablation; comparisons were between planners that use the symbolic PDDL vs. LLM-based planning approaches and different LLMs for PDDL construction.",
            "key_findings": "Using explicit deterministic PDDL domains constructed and corrected via LLMs enables reliable planning with classical planners; this approach sidesteps the need to rely on LLMs' internal (and unreliable) combinatorial search abilities and does not model or incorporate LLM output uncertainty probabilistically.",
            "uuid": "e971.1",
            "source_info": {
                "paper_title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-modulo-planner (ReAct + VAL)",
            "name_full": "LLM planner (ReAct/GPT-4) back-prompted by VAL using LLM-acquired PDDL domain",
            "brief_description": "A method where an LLM planner (ReAct with GPT-4) generates plans in natural language, which are translated to PDDL and validated by VAL; VAL's symbolic validation feedback (unmet preconditions/goal conditions) is translated back to natural language and used to re-prompt the LLM planner iteratively.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "system_name": "LLM-modulo-planner backprompted by VAL",
            "system_description": "The LLM planner is given detailed natural-language descriptions of actions (produced by translating the PDDL domain to NL). It generates candidate plans; these are translated to symbolic form and validated by VAL to identify unmet preconditions/goal conditions. Validation outputs (PDDL errors) are converted to NL via GPT-4 and fed back to the LLM planner to iteratively refine the plan. The authors limit iteration counts in experiments and report practical behaviors (e.g., loops, repeated mistakes).",
            "world_model_type": "PDDL (used as symbolic simulator / validator)",
            "world_model_description": "The PDDL domain (deterministic STRIPS) is used to symbolically validate LLM-generated plans: states are predicate assignments; plan steps are tested for precondition satisfaction and goal achievement deterministically. The system does not represent probabilistic transitions or belief states; validator reports deterministic unmet preconditions/effects.",
            "uses_llm": true,
            "llm_role": "plan generation (ReAct prompting with GPT-4), translation between PDDL and natural language, integrating validator feedback",
            "llm_model_name": "GPT-4 (as the LLM planner implementing ReAct)",
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "ReAct prompting loop (LLM-based reasoning+acting) with symbolic plan validation by VAL; can be viewed as iterative re-prompting rather than an algorithmic search like A*",
            "planning_integrates_uncertainty": false,
            "text_environment_name": "Household domain (custom), Logistics",
            "text_environment_description": "Same symbolic domains as above; LLM planner uses NL action descriptions derived from the PDDL domain to generate plans, which are validated against the PDDL.",
            "performance_metric": "Task success rate after iterative validation-feedback re-prompting",
            "performance_value": "ReAct (GPT-4) without validation: extremely low success; ReAct + VAL feedback improved correctness but overall success remained below 50% (limited to at most 8 feedback iterations in experiments). Authors report LLM often repeats the same mistakes or gets stuck in loops when trying to use feedback.",
            "baseline_comparison": "Compared to classical-planner + LLM-acquired PDDL (â‰ˆ95% success), the LLM-modulo-planner approach underperforms; compared to vanilla LLM planner it improves correctness but still falls short of classical planning reliability.",
            "has_ablation_uncertainty": false,
            "ablation_results": "No probabilistic/uncertainty ablation; experiments varied prompt examples and action descriptions but did not incorporate probabilistic belief models for LLM uncertainty.",
            "key_findings": "Using PDDL-derived validation feedback (VAL) to back-prompt an LLM planner improves plan correctness relative to raw LLM outputs, but does not reach the reliability of classical planners using explicit PDDL domains; LLM planners tend not to effectively use iterative symbolic feedback and can loop or introduce new errors. No probabilistic modeling of LLM uncertainty or belief states was implemented.",
            "uuid": "e971.2",
            "source_info": {
                "paper_title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "VAL",
            "name_full": "VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL",
            "brief_description": "A plan validation tool that checks PDDL plans against a PDDL domain and initial/goal specification, reporting syntactic errors or unmet preconditions/goal conditions deterministically.",
            "citation_title": "VAL: Automatic plan validation, continuous effects and mixed initiative planning using pddl",
            "mention_or_use": "use",
            "system_name": "VAL plan validator",
            "system_description": "VAL is used to validate generated PDDL domains for syntax/type errors and to validate candidate plans by checking precondition satisfaction and goal achievement step-by-step. The validator's output (PDDL-format error messages) is translated to natural language by the LLM and used as feedback to correct PDDL models or to re-prompt LLM planners.",
            "world_model_type": "PDDL validator (operates on deterministic PDDL domains and plans)",
            "world_model_description": "VAL interprets typed PDDL domains and grounded plans; it deterministically examines action preconditions and effects against current symbolic state and reports violations (no probabilistic reasoning).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Plan validation (symbolic simulation of PDDL plan execution), not a planner per se",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": "A general-purpose PDDL validation tool used here on Household, Logistics, and Tyreworld domains to detect syntax/type errors and unmet preconditions/goal conditions.",
            "performance_metric": "Ability to detect syntactic/type errors and unmet preconditions in candidate PDDL domains/plans",
            "performance_value": "Used successfully to detect simple syntax/type issues and to supply concise error messages that GPT-4 could use to correct PDDL models; no numeric accuracy metric reported for VAL itself.",
            "baseline_comparison": "Not applicable as VAL is used as a validator/tool in the pipeline; simple Python scripts supplemented VAL to detect lightweight syntax issues.",
            "has_ablation_uncertainty": false,
            "ablation_results": "No uncertainty-related ablations for VAL; it operates deterministically on PDDL input.",
            "key_findings": "VAL is an effective deterministic validator to detect syntax/type errors and unmet preconditions; its feedback, when translated by an LLM, can be used to correct PDDL models, but VAL does not provide probabilistic uncertainty estimates about LLM outputs nor belief-state reasoning.",
            "uuid": "e971.3",
            "source_info": {
                "paper_title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llm+ p: Empowering large language models with optimal planning proficiency",
            "rating": 2
        },
        {
            "paper_title": "Pddl planning with pretrained large language models",
            "rating": 2
        },
        {
            "paper_title": "Planning with large language models via corrective re-prompting",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 1
        }
    ],
    "cost": 0.01579075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</h1>
<p>Lin Guan *<br>School of Computing \&amp; AI<br>Arizona State University<br>Tempe, AZ 85281<br>lguan9@asu.edu</p>
<p>Sarath Sreedharan<br>Department of Computer Science<br>Colorado State University<br>Fort Collins, CO 80523<br>sarath.sreedharan@colostate.edu</p>
<p>Karthik Valmeekam *<br>School of Computing \&amp; AI<br>Arizona State University<br>Tempe, AZ 85281<br>kvalmeek@asu.edu<br>Subbarao Kambhampati<br>School of Computing \&amp; AI<br>Arizona State University<br>Tempe, AZ 85281<br>rao@asu.edu</p>
<h4>Abstract</h4>
<p>There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.</p>
<h2>1 Introduction</h2>
<p>The field of artificial intelligence has been revolutionized with the advent of large pre-trained models. Of particular significance are transformer-based large language models (LLMs) which have showcased remarkable performance in natural language processing tasks. Along with these tasks, LLMs have been tested to perform another widely-studied crucial aspect of AI agents, namely, sequential decision-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>making or planning. Preliminary studies suggest that, in some everyday domains, LLMs are capable of suggesting sensible action plans [19, 1]. However, the correctness and executability of these plans are often limited. For instance, LLMs may regularly overlook the physical plausibility of actions in certain states and may not effectively handle long-term dependencies across multiple actions. Several approaches have been proposed to improve the planning capabilities of LLMs. One promising approach involves collecting feedback from the environment during plan execution and subsequently refining the plans. By incorporating various forms of feedback, such as sensory information [20], human corrections [60], or information of unmet preconditions [42, 56], the planners can re-plan and produce plans that are closer to a satisficing plan.</p>
<p>Despite the improvements in planning performance, LLMs are still far from being a usable and reliable planner due to various factors:
(a) LLMs have not yet demonstrated sufficient capabilities in reasoning and planning [24, 55, 53, 54, 31]. Recent investigations show that even when provided with detailed descriptions of actions, such as a PDDL domain model [33] or a natural-language version of a PDDL model, LLMs still struggle to produce correct and executable plans [48, 55].
(b) Existing LLMs-planning paradigms only allow for feedback collection in a fully online manner, meaning that the feedback signals are only available after the agent has started executing the plan. However, when a faithful simulator is not available or is expensive to use, collecting feedback through actual plan execution can be costly and may not fully exploit the advantages of provably sound planning, as seen in classical-planning literature [11, 13].
(c) LLMs exhibit complex behaviors that are not yet fully understood, particularly with respect to error occurrences. LLM planners are prone to repeating the same mistakes in slightly different scenarios. Repeatedly providing the same feedback can lead to frustration for end users.</p>
<p>To overcome these limitations, rather than using LLMs directly as planners, we advocate a modelbased paradigm, wherein a PDDL world model is teased out of LLMs. We follow the identical problem setup as existing approaches, which involves providing the planner with a set of actions and their brief natural language descriptions. However, instead of directly mapping user commands to plans, we utilize LLMs to extract a symbolic representation of the actions in the form of PDDL action models. This intermediate output can be used with an external domain-independent planner to reliably search for feasible plans, or it can be used to validate and correct "heuristic" plans generated by an LLM planner. Additionally, our modular method essentially divides the planning process into two distinct parts, namely modeling the causal dependencies of actions and determining the appropriate sequence of actions to accomplish the goals. LLMs, which have been trained on extensive web-scale knowledge, exhibit greater proficiency in the former task rather than the latter.</p>
<p>Nevertheless, we still take into account the fact that the LLMs may not be able to generate error-free PDDL models at the outset. To address this, we show that LLMs can also serve as an interface between PDDL and any feedback sources that can provide corrective feedback in natural language, such as humans and the PDDL validator in VAL [18]. The LLM middle layer translates PDDL representation to natural language and presents it to users for inspection. The acquired feedback is then incorporated and archived back to the PDDL models. This conceals the complexity of PDDL from users who do not have prior knowledge of PDDL, and enables seamless inclusion of feedback. We conducted an extensive evaluation of our methodology on two IPC domains [22] from classical planning literature and a household domain that has a more diverse set of actions and constraints than commonly used benchmarks such as ALFWORLD [47]. We assess the quality of the generated PDDL models through manual evaluation. Results show that GPT-4 [37] generates high-quality PDDL domain models with over 400 literals for 41 actions in total. Then, by replaying and continuing the PDDL-construction dialogue, we show that GPT-4 can readily correct all the errors according to natural language feedback from PDDL validators and humans.</p>
<p>We consider two use cases of the generated PDDL action models for downstream planning tasks. For one, by utilizing an LLM to translate user instructions into goal specifications in PDDL [58, 30], we can use any standard domain-independent planner to search for a plan. On the other hand, the extracted PDDL model can be used to validate plans suggested by an LLM planner and to provide corrective feedback in the form of unmet preconditions or goal conditions. In this case, the PDDL model is essentially serving as an inexpensive high-level simulator or a human proxy to ensure plan correctness.</p>
<p>This reduces the reliance on faithful simulators or extensive manual inspection of plans by domain experts. Compared to the first approach, the second approach potentially offers better flexibility in incorporating both explicit and implicit user constraints in common-sense domains because of the LLM planner. For instance, the LLM planner can directly incorporate ordering constraints such as "heat the potato first before mashing it" and "bring me a fork first, then a plate." On the contrary, an approach purely based on classical planners would require extra steps, such as introducing extra state variables in the PDDL models, in order to accommodate such constraints. However, as demonstrated in our experiments, although the validation feedback significantly improves the plan correctness on average, the performance of the second approach is still limited by the "planning capability" of LLMs.</p>
<h1>2 Related Work</h1>
<p>LLMs and planning. The growing interest in evaluating the emergent abilities of LLMs paved way into exploring their abilities in sequential decision-making tasks. Preliminary studies [24, 55] have shown that off-the-shelf LLMs are currently incapable of producing accurate plans. But their plans can be used as heuristics or seeds to either an external planner or a human in the loop [55, 48]. SayCan [1] and Text2Motion [29] employ an LLM as a heuristic by utilizing it to score high-level actions, followed by a low-level planner that grounds these actions to determine the executability in the physical world. In a similar vein, [28, 50] use LLMs to generate plans represented in Python-style code. Other works have aimed to improve the planning performance of LLMs through prompt engineering [60] or collecting various forms of feedback such as sensory information [51, 20, 34], human corrections [60], self-corrections [46] or information of unmet preconditions [42, 56].</p>
<p>Training transformers for sequential decision-making tasks. Along with using off-the-shelf LLMs, there are works that either fine-tune LLMs [55, 38] or train sequence models [62, 27, 7, 43] for sequential decision making tasks. Experiments in [26] have shown that training sequence models on a specific task gives rise to an internal world representation within the model. In this work, we use off-the-shelf LLMs to construct symbolic world models without performing any extra training.</p>
<p>Learning/acquiring symbolic domain models. In classical planning, the community has explored numerous learning-based methods [59, 61, 9, 25, 4] and interactive editor-based methods [49] for acquiring symbolic domain models. For a more comprehensive survey, we refer the reader to [2, 6]. Here, we are interested in leveraging the common-world knowledge embedded in LLMs and their in-context learning ability for constructing domain models. Recent studies have shown the efficacy of LLMs in translating natural language to formal descriptions [35] or constructing PDDL goals from natural-language instructions [58, 32]. Moreover, a contemporary work [15] considers the use of LLM as a parametric world model and plan critic. However, unlike a symbolic model that can simulate plan outcomes with guaranteed correctness, using LLMs directly as a world model actually adds another layer of errors. There is evidence that autoregressive models lack reliable capacity for reasoning about action effects [3, 31] and capturing errors in candidate plans [53, 54].</p>
<p>Language models with access to external tools. Since LLMs are approximately omniscient, they may not always outperform specialized models or tools in specific downstream tasks. To address this limitation, frameworks have been developed to enable LLMs to utilize external tools for performing sub-tasks like arithmetic [45] and logical reasoning [39, 57]. In this context, our work can be regarded as an exercise in employing external sound planners to augment the capacity of LLMs for more reliable plan generation.</p>
<h2>3 Problem Setting and Background</h2>
<p>Our work focuses on a scenario where an intelligent agent receives high-level instructions or tasks, denoted as $i$, from a user. The agent is capable of only executing skills or operations that are part of a skill library $\Pi$, where each skill $k$ has a short language description $l_{k}$. We assume that the agent is equipped with the low-level control policies corresponding to these high-level skills. In order to achieve the goal conditions specified in $i$, a planner, which can be either an LLM or an external planner [16, 12, 17], needs to come up with a sequence of high-level skills that the agent can execute. This type of problem is referred to as a sequential decision-making or planning problem. Similar to previous works such as [60, 20], we also allow for human-in-the-loop feedback during both the domain-model construction and plan execution stages. In the next subsections, we describe the formalism behind planning problems and a standard way in the literature to specify them.</p>
<h1>3.1 Classical planning problems</h1>
<p>The most fundamental planning formalism is goal-directed deterministic planning problem, referred to as a classical planning problem in the planning literature. A classical planning problem [44] can be formally represented with a tuple $\mathcal{P}=\langle\mathcal{D}, \mathcal{I}, \mathcal{G}\rangle . \mathcal{D}$ is referred to as the domain, $I$ is the initial state, and $\mathcal{G}$ is the goal specification. The state space of a planning problem consists of the truth assignments for predicates. The domain $\mathcal{D}$ is further defined by the tuple $\mathcal{D}=\langle\mathcal{F}, \mathcal{A}\rangle . \mathcal{F}$ corresponds to the set of fluents, i.e., the state variables used to define the state space with each fluent corresponding to a predicate with some arity. $\mathcal{A}$ corresponds to the set of actions that can be performed. Each action $a_{i}[\mathcal{V}] \in \mathcal{A}$ (where $\mathcal{V}$ is the set of variables used by the operator $a_{i}$ and each variable could be mapped to an object) can be further defined by two components, the precondition $\operatorname{prec}[\mathcal{V}]$ which describes when an action can be executed, and the effects eff $[\mathcal{V}]$ which defines what happens when an action is executed. We assume that $\operatorname{prec}[\mathcal{V}]$ consists of a set of predicates defined over the variables $\mathcal{V}$. An action is assumed to be executable only if its preconditions are met, i.e, the predicates in the precondition hold in the given state. The effect set eff $[\mathcal{V}]$ is further defined by the tuple $\langle$ add $[\mathcal{V}], \operatorname{del}[\mathcal{V}]\rangle$, where add $[\mathcal{V}]$ is the set of predicates that will be set true by the action and $\operatorname{del}[\mathcal{V}]$ is the set of predicates that will be set false by the action. An action is said to be grounded if we replace each of the variables with an object, else it is referred to as a lifted action model. A solution to a planning problem is called a plan, and it is a sequence of actions that once executed in the initial state would lead to a state where the goal specification holds. Classical planning problems are one of the simpler classes in planning and there are multiple extensions with more complex forms of preconditions, conditional effects, and also support for richer planning formalisms.</p>
<h3>3.2 PDDL</h3>
<p>Planning Definition and Domain Language (PDDL) [33], is the standard encoding language for classical planning problems. Here is an example of a lifted action in PDDL which corresponds to putting a block onto the table in the classical Blocksworld domain:</p>
<div class="codehilite"><pre><span></span><code>(:action PutDownBlock
    :parameters (?x - block)
    :precondition (and (robot-holding ?x))
    :effect (and (not (robot-holding ?x)) (block-clear ?x) (robot-hand-empty) (block-on-table ?x)))
</code></pre></div>

<p>The parameters line provides the possible variable(s), and in this case, ?x represents the block to put down. The precondition states that the robot must be holding the block in its gripper. The effects line describes the expected outcome of this action.</p>
<h2>4 Methodology</h2>
<p>PDDL provides a succinct and standardized way to represent a world model. Once a PDDL model is constructed, it can be seamlessly used by any domain-independent planner developed in the automated planning community to search for a plan given the initial state and goal conditions. In this section, we will introduce our solution for constructing PDDL models using LLMs. We then discuss techniques for correcting errors in the generated PDDL models. Finally, we present the full pipeline for utilizing the generated PDDL models to solve planning problems.</p>
<h3>4.1 Constructing PDDL models with LLMs</h3>
<p>Our approach involves prompting pre-trained LLMs with the following information: (a) detailed instructions for the PDDL generation task, outlining components of upcoming inputs and desired outputs; (b) one or two examples from other domains (e.g., the classical Blocksworld domain) for illustrating the input and output formats; (c) a description of the current domain, including contextual information about the agent's tasks and physical constraints due to the specific embodiment of the agent; (d) a description of the agent's action; and (e) a dynamically updated list of predicates that the LLM can reuse to maintain consistent use of symbols across multiple actions. Note that the predicate list is initialized to an empty list, and thus all predicates are introduced by the LLM. The structure of the prompt is illustrated in Fig. 2, and a complete prompt for the household-robot domain can be found at Appx. A.6.1.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our framework and existing methods that use LLMs directly as planners.</p>
<p>Depending on the information included in the action description or the domain context, users may gain varying levels of control over the extracted PDDL or receive differing levels of support from the LLMs. On one hand, when the user provides only a minimal description of the action, such as "this action enables the robot to use a microwave to heat food," we not only use the LLM as a PDDL constructor but also leverage the common world knowledge encoded within the model for knowledge acquisition. This is particularly useful when expanding the set of actions for an AI agent. For example, a robot engineer could set up a training environment for skill learning by following the suggested preconditions and effects. On the other hand, when some preconditions or effects are explicitly mentioned in the prompt, we rely more on the LLM's ability to parse the knowledge provided in natural language and to precisely represent it by devising a collection of predicates. This capability is useful when there could be different initial setups of a skill, and the engineers have already made some assumptions on the preconditions at the time of designing the skill. This capability is also crucial when constructing PDDL for specialized domains. For instance, robots such as Fetch and Spot Robot have only one robot arm, which is less flexible than a human arm, and are therefore subject to many uncommon physical constraints.</p>
<p>The desired output comprises the following elements: (a) the list of arguments for the action; (b) the preconditions and effects expressed in PDDL; and (c) a list of any newly defined predicates and their descriptions in natural language, if applicable. An example output is shown in Fig. 2. Our algorithm generates PDDL models for each action separately, one at a time, by iterating over the set of actions. Any newly defined predicates will be added to an actively maintained predicate list, such that the LLM can reuse existing predicates in subsequent actions without creating redundant ones. Once we obtain the initial PDDL models and the full predicate list, we repeat the entire process but with all of the extracted predicates presented to the LLM. Running the generation process twice is useful because the LLMs may be unaware of some precondition(s) during the first iteration, especially if the precondition(s) are not explicitly mentioned. For instance, the LLM may overlook the fact that a furniture piece can be openable, but a predicate created in the "open a furniture piece or appliance" skill can inform the LLM of this fact. One alternative to this action-by-action generation could be to include descriptions of all the actions in the prompt and require the LLM to construct the entire domain model in a single dialogue. An additional discussion on this can be found at Sec. A. 2 in Appendix.</p>
<p>It is worth noting that every time a new predicate is defined, the LLM is required to give the natural language description of it. As we will see in the following sections, this is crucial for enabling any user to easily understand and inspect the generated PDDL models without having to delve into the low-level symbolic representation. Additionally, natural language descriptions allow the predicate values of the initial state to be automatically grounded by using LLMs to translate environment description in natural language to PDDL [30], or leveraging pre-trained vision-language models</p>
<div class="codehilite"><pre><span></span><code><span class="n">Instructions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">PDDL</span><span class="w"> </span><span class="n">generation</span><span class="w"> </span><span class="n">task</span>
<span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">defining</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">preconditions</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">effects</span><span class="w"> </span><span class="p">(</span><span class="n">represented</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">PDDL</span><span class="w"> </span><span class="n">format</span><span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="s1">&#39;s</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="n">actions</span><span class="o">.</span><span class="w"> </span><span class="n">Information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="o">...</span>
<span class="n">One</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">domains</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">illustrating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">formats</span>
<span class="n">Here</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">classical</span><span class="w"> </span><span class="n">BlocksWorld</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">demonstrating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">format</span><span class="o">.</span>
<span class="n">Here</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="o">.</span>
<span class="n">A</span><span class="w"> </span><span class="n">natural</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">domain</span>
<span class="n">Domain</span><span class="w"> </span><span class="n">information</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">here</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">household</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">navigate</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="ow">and</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="n">normally</span><span class="w"> </span><span class="n">immovable</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="n">pieces</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">appliances</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="n">household</span><span class="w"> </span><span class="n">tasks</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="o">...</span>
<span class="n">A</span><span class="w"> </span><span class="n">natural</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">action</span>
<span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">enables</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">toggle</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">appliances</span><span class="w"> </span><span class="p">(</span><span class="n">like</span><span class="w"> </span><span class="n">humidifiers</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">light</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="n">bulbs</span><span class="p">)</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">toggleable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">switch</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="o">...</span>
<span class="n">The</span><span class="w"> </span><span class="n">dynamically</span><span class="w"> </span><span class="n">updated</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">predicates</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">define</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">predicates</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">reuse</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">predicates</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="p">(</span><span class="n">robot</span><span class="o">-</span><span class="n">at</span><span class="w"> </span><span class="err">?</span><span class="n">r</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="err">?</span><span class="n">f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">furnitureAppliance</span><span class="p">):</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">robot</span><span class="w"> </span><span class="err">?</span><span class="n">r</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="ow">or</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="err">?</span><span class="n">f</span>
<span class="mf">2.</span><span class="w"> </span><span class="p">(</span><span class="n">object</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">householdObject</span><span class="w"> </span><span class="err">?</span><span class="n">f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">furnitureAppliance</span><span class="p">):</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">on</span>
\<span class="n">llcorner</span><span class="o">-</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="err">?</span><span class="n">f</span>
\<span class="n">cdots</span>
<span class="n">Parameters</span><span class="p">:</span>
<span class="n">The</span><span class="w"> </span><span class="n">LLM</span><span class="p">:</span>
<span class="o">...</span>
<span class="mf">2.</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">householdObject</span><span class="p">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">toggled</span><span class="w"> </span><span class="n">on</span>
<span class="o">...</span>
<span class="n">Preconditions</span><span class="p">:</span>
<span class="p">(</span><span class="ow">and</span>
<span class="w">    </span><span class="o">...</span>
<span class="w">    </span><span class="p">(</span><span class="ow">not</span><span class="w"> </span><span class="p">(</span><span class="n">appliance</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">Effects</span><span class="p">:</span>
<span class="p">(</span><span class="ow">and</span>
<span class="w">    </span><span class="p">(</span><span class="n">appliance</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">New</span><span class="w"> </span><span class="n">Predicates</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="p">(</span><span class="n">appliance</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">householdObject</span><span class="p">):</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="err">?</span><span class="n">o</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">switched</span><span class="w"> </span><span class="n">on</span>
</code></pre></div>

<p>Figure 2: The prompt template for PDDL construction and an example of the LLM output for the household domain.
$[41,37,10]$ and querying them in a question-answering manner, based on observations from the environment.</p>
<h1>4.2 Correcting errors in the initial PDDL models</h1>
<p>As with any use case involving LLMs, there is no guarantee that the output is completely error-free. Therefore, it is essential to incorporate error-correction mechanisms. While it may be easy for PDDL experts to directly inspect and correct the generated PDDL models, we cannot assume that all end users possess this level of expertise. Our solution is to use the LLM as a middle layer or interface between the underlying PDDL model and any feedback source that can provide corrective feedback in natural language. We consider two feedback sources in this work, namely the PDDL model validation tools (e.g., the one in VAL [18]) and human domain experts. The former is used to detect basic syntax errors, while the latter is mainly responsible for catching factual errors, such as missing effects. It is worth noting that the feedback sources are not limited to those mentioned above, and we leave the investigation of other sources for future research.</p>
<p>For corrective feedback from PDDL validators, a generated PDDL model is directly presented to the validator to obtain brief but readable error messages. Examples of feedback messages for syntax errors are shown in Appx. A.3. For corrective feedback from users, a PDDL model is translated into its natural-language version based on the natural language descriptions of the predicates and parameters (Sec. 4.1). The user can then examine potentially erroneous action models. Human corrections can occur both during the construction of PDDL models and after the models have been used for planning. Although there are techniques available to assist users to locate errors in the models (as discussed in</p>
<p>Appx. A.4), this is beyond the scope of this work, since the focus here is to investigate the feasibility of using LLMs to correct PDDL models based on feedback. We also note that correcting action models is not more cognitively demanding than correcting plans or the "reasoning traces" of an LLM planner [60]. In fact, when correcting plans, humans must also maintain the action models and their causal chains in mind in order to validate the plans. More importantly, once the action models are corrected, users no longer need to provide similar feedback repeatedly. Finally, corrective feedback is integrated by replaying and continuing the PDDL-construction dialogue. Examples of such dialogues can be found in Sec. A.7, Sec. A.9, and Sec. A. 11 in Appendix.</p>
<h1>4.3 Generating plans with the extracted PDDL models</h1>
<p>Recall that given the set of extracted predicates and their natural language descriptions, we can get the grounded initial state by using LLMs to translate descriptions of the environment to PDDL, or by observing the environment and querying pre-trained vision-language models. Besides, the goal specification can be obtained by using an LLM to parse the user's command and convert it into a symbolic form, as done previously in [30, 58, 32]. With this setup, the following two methods can be used to generate the final plans.</p>
<p>Classical planner with LLM-acquired PDDL model. One straightforward approach is to employ a standard domain-independent planner to reliably find a satisficing or even optimal plan for the specified goal. In common-sense domains where LLMs may generate meaningful "heuristics", the LLM plans may also be used as seed plans for a local-search planner such as LPG [12] to accelerate the plan searching. This is similar to the approach suggested in [55], but with a higher degree of automation.</p>
<p>LLM modulo planner backprompted by VAL using LLM-acquired PDDL model. As outlined in Sec. 1, we can also use the extracted PDDL as a symbolic simulator or human proxy to provide corrective feedback based on validation information to an LLM planner. With this setup, the planner can iteratively refine the plans through re-prompting [42].</p>
<p>It is worth noting that depending on the specific problem settings, the extracted PDDL model can also be used for tasks other than task planning. For instance, in cases where reinforcement learning is permissible, the domain model can be used to guide skill learning [21, 8] or exploration even if the model is not fully situated [14].</p>
<h2>5 Empirical Evaluation</h2>
<p>We conduct our experiments ${ }^{2}$ on an everyday household-robot domain and two more specialized IPC domains (i.e., Tyreworld and Logistics). The Household domain is similar to other commonly used benchmarks like ALFWORLD [47] and VirtualHome [40]. However, in our household domain, a single-arm robot is equipped with a more diverse and extended set of 22 mobile and manipulation skills. In addition, we apply more rigorous physical-plausibility constraints to each skill. A detailed description of this domain can be found at Appx. A.5. In our experiments, we first evaluate the quality of PDDL models generated by the LLMs. Next, we assess the ability of the LLMs to incorporate corrective feedback from both PDDL validators and users in order to obtain error-free PDDL models. Lastly, we showcase multiple ways to use the corrected PDDL model for downstream planning tasks. We present the results of GPT-4 [37] and GPT-3.5-Turbo [36] for PDDL construction (we also conducted experiments with GPT-3 [5], and observe that its performance is comparable to that of GPT-3.5-Turbo).</p>
<h3>5.1 Constructing PDDL</h3>
<p>In PDDL construction tasks, we aim to investigate the extent to which LLMs can construct accurate PDDL models before getting corrective feedback from domain experts. For all the domains, two actions from the classical Blocksworld domain are used as demonstrations in the prompt so that the end user is not required to come up with any domain-specific example. To evaluate the degree of correctness, we recruit multiple graduate students who possess expertise in PDDL. These experts are responsible for annotating and correcting any errors present in the generated PDDL models. As an evaluation metric, we count and report the total number of annotations, which may include the removal of irrelevant preconditions, the addition of missing preconditions, the replacement of incorrect predicates, the inclusion of missing parameters, and other commonly made corrections. Note</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that the number of annotations can be viewed as the approximate distance between a generated PDDL model and its corrected version. In order to provide the reader with a comprehensive understanding of the quality of the generated models, we also list all the models and collected annotations in Appendix. In each of the figures, errors that affect the functionality of the PDDL model are highlighted in yellow, while minor issues are highlighted in green. One example of a minor issue is the redundant inclusion of (pickupdate ?o) in preconditions when (robot-holding ?o) has already been listed. The former is unnecessary because it can be implied by the latter, but this only affects conciseness rather than functionality.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;"># of actions</th>
<th style="text-align: left;"># of params and literals</th>
<th style="text-align: left;"># of GPT-4 errors</th>
<th style="text-align: left;"># of GPT-3.5-Turbo errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Household</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">271</td>
<td style="text-align: left;">53</td>
<td style="text-align: left;">$218+$</td>
</tr>
<tr>
<td style="text-align: left;">Logistics</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">54</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">38</td>
</tr>
<tr>
<td style="text-align: left;">Tyreworld</td>
<td style="text-align: left;">13</td>
<td style="text-align: left;">108</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">$94+$</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of errors in the domain models produced by the LLMs for each of the domains. A "+" mark indicates that the generated model is excessively noisy, making it challenging to determine an exact number of errors.</p>
<p>We first evaluate the PDDL models generated when partial constraint information is given, as this is closer to most of the practical use cases where constraints on skills in the library $\Pi$ are often pre-specified. In this setting, our evaluation focuses on the LLMs' ability to accurately recover a "ground truth PDDL" that captures the mentioned constraints and underlying dependencies among skills. Our results indicate that GPT-4 can produce high-quality PDDL models with significantly fewer errors when compared to GPT-3.5-Turbo. Table 1 presents the number of errors in the generated domain models for each domain. To help the readers understand the complexities of the action models, we additionally report the total number of parameters and literals in the final corrected domain models produced by GPT-4. Out of the total 59 errors made by GPT-4, three of them are syntax errors and the rest are factual errors such as missing preconditions and effects. This observation suggests that while GPT-4 demonstrates proficiency in adhering to the grammar of PDDL, it may still have an inaccurate understanding of the actions. By examining the set of predicates (listed in the Appendix), we also find that GPT-4 can devise a set of intuitively-named predicates that can concisely and precisely describe the states of objects and events in the domain. In contrast, GPT-3.5-Turbo produces highly noisy outputs with over 350 errors. This suggests that our framework relies heavily on GPT-4's improved capability in understanding symbols, and future work may investigate how to enable the use of more lightweight models (e.g., by fine-tuning on some PDDL datasets). Furthermore, recall that when the action description contains minimal information, LLMs could also be utilized to propose preconditions and effects to assist with knowledge acquisition. To verify this hypothesis, we conduct additional experiments on the Household domain that can have a more open-ended action design. In this setting, the correctness of the action models is determined based on whether the preconditions and effects establish correct connections among the actions. Our results show that GPT-4 can suggest meaningful action models, and the generated PDDL models have only around 45 errors.</p>
<p>Although GPT-4 has shown improved performance in the PDDL construction task, our experiments still uncover some limitations. Firstly, GPT-4 still exhibits a shallow understanding of the causal relationships between actions, particularly when it comes to tasks involving reasoning skills such as spatial reasoning. For instance, when constructing the model of action "pick up an object from a furniture piece," GPT-4 fails to consider that there could be other objects stacked on top of the target object, even if relevant predicates are provided (which were created in the action "stack objects"). In addition, although it occurs rarely, GPT-4 may output contradictory effects. For instance, in the action of mashing food with a blender, GPT-4 lists both (not (object-in-receptacle ...)) and (object-in-receptacle ...) as effects at the same time.</p>
<h1>5.2 Correcting PDDL with domain experts</h1>
<p>We proceed with the PDDL models generated by GPT-4 when the constraint information is partially given. Our objective is to demonstrate the feasibility of using GPT-4 as a middle layer to incorporate natural-language feedback and correct the PDDL models. As discussed in Sec. 4.2, we use PDDL validators to capture basic syntax errors. In the Household domain, there are two syntax errors</p>
<p>associated with improper usage of relevant predicates due to issues with the object types of parameters ${ }^{3}$. As shown in Appx. A.7.1, by continuing the PDDL-construction dialogue with a feedback message "the second parameter of object-on should be a furnitureAppliance but a householdObject was given," GPT-4 can locate the inaccurate PDDL snippet and replace it with a correct one. For the other factual errors, GPT-4 successfully corrects all of them based on the natural language feedback. An example feedback message on factual errors is "there is a missing effect: the item is no longer pickupable after being mashed." More PDDL-correction conversations can be found in Appendix. We also experiment with feedback written in various ways, and GPT-4 is able to understand all the messages and successfully correct the models. To quantify how effectively GPT-4 utilizes feedback from domain experts, we count the number of feedback messages concerning factual errors. Our result shows that GPT-4 required 59 feedback messages to address a total of 56 factual errors. There are three instances where additional feedback was needed. One case involved the user reiterating the error, while the other two cases involved GPT-4 introducing new errors. Furthermore, we attempt to correct the same errors using GPT-3.5-Turbo. Results show that GPT-3.5-Turbo not only fails to correct all the errors but also occasionally introduces new errors, again confirming its lack of ability to manipulate symbols. Some examples can be found in Appendix starting from Sec. A.7.3.</p>
<h1>5.3 Generating plans with the extracted PDDL models</h1>
<p>For planning tasks (i.e., user instructions and initial states), we use the Household domain and Logistics domain, where state-of-the-art LLM planners struggle to find valid plans. We sampled 27 tasks for Household and 21 for Logistics. For the initial states, we assume the grounding is provided, and for the goals, we leverage GPT-4 to translate user instructions into PDDL goal specifications in terms of the extracted predicates (an example prompt can be found at Appx. A.13), and send it over to a standard STRIPS planner which already has access to the domain model acquired through LLMs. With this setup, a classical planner Fast Downward [16] can effectively find valid plans in $95 \%$ of the cases (the failures were only due to goal translation errors). Note that in contrast to earlier methods such as [30] that use LLMs only as a mechanism for translating user goals to PDDL format, and throw that over to external sound planners with hand-crafted correct PDDL domain models, our approach uses LLMs themselves to develop the PDDL world model driving the external planner.</p>
<p>On the other hand, for the approach that utilizes PDDL models to validate LLM plans (i.e., LLM modulo planner back-prompted by VAL using LLMacquired domain model), we employ the state-of-the-art algorithm ReAct [60] with GPT-4 as the underlying LLM planner. However, we made two modifications to the prompt design. Firstly, we provide a detailed description of all actions in natural language, including parameters, preconditions, and effects. These descriptions are obtained by using another LLM to translate the generated PDDL domain model into natural language. Secondly, we use only two fixed examples for each domain because end users might not always be able to provide a large pool of examples, and the planner should rely on the action model information. The LLM plans, symbolic goal specifications, initial states and domain models are passed to a plan validation system (i.e., VAL) to check for unmet precondition(s) or goal condition(s). The validation results (given in PDDL) are then translated into natural language with GPT-4 and provided to the LLM planner by continuing the planning dialogue (see Appx. A.12.1 for examples). In our experiments, we limit the number of feedbacks per task to 8 due to the restricted access to GPT-4. Table 2 provides a summary of the average success rates of all approaches. Not surprisingly, the vanilla LLM planner constantly overlooks action preconditions and achieves an extremely low success rate. With the integration of validation feedback, we observe a notable improvement in plan correctness. Despite this improvement, the overall performance is still not satisfactory, as the success rate remains below</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Success rates of different planning approaches in the Household domain and the Logistics domain.</p>
<p>$50 \%$. Furthermore, we have observed that GPT-4 fails to effectively utilize the feedback, often getting stuck in a loop by repeatedly generating the same plan. In some cases, it may also introduce new errors while attempting to rectify the plans.</p>
<p>Beyond the notion of correctness, the experiments also uncover intriguing properties of the LLM planner. In the Household domain, we intentionally introduce ordering constraints in some instructions that cannot be expressed using existing predicates (refer to Appx. A. 12 for examples). Remarkably, upon manual examination of the generated plans, we observe that all LLM plans adhere to the specified ordering, despite not being entirely correct or executable. Furthermore, also in the Household domain, we observe that classical planners occasionally generate physically plausible but unconventional actions, such as placing a knife on a toaster when the knife is not being used. In contrast, the LLM planner rarely exhibits such actions, suggesting that LLMs possess knowledge of implicit human preferences. It would be meaningful to explore methods that more effectively combine the strengths of LLM planners and the correctness guarantee provided by symbolic domain models, particularly in determining which information from LLM plans should be preserved.</p>
<h1>6 Conclusion</h1>
<p>We introduce a new paradigm for leveraging LLMs in planning tasks, which involves maintaining an explicit world model instead of directly mapping user prompts to plans. This is motivated by the insight that LLMs, while incapable of the combinatorial search needed to produce correct plans, may be better suited as the source of world models. We present a complete pipeline that begins with generating high-quality PDDL models using GPT-4, then corrects the PDDL models with naturallanguage feedback, and finally utilizes the extracted domain models to reliably plan in multiple ways. Our experiments demonstrate that pairing LLMs with an external planner significantly outperforms existing methods when applied to two IPC domains and a household-robot domain that has more action-wise constraints than commonly used benchmarks such as ALFWorld. Apart from directions for further research that we have previously mentioned, there are several exciting opportunities for extending this work. Firstly, the complexity of our evaluation domains is still lower than that of many domains used in the classical planning literature. It remains to be seen whether LLMs can effectively scale to write PDDL models that express more intricate logic. Secondly, our framework assumes full observability, meaning that the agent must fully explore the environment to acquire object states at the beginning. It would be useful to support partial observability. Finally, our experiments assume the grounding of predicate values is done perfectly. However, it would be useful to take into account that perception can be noisy in practice.</p>
<h2>Acknowledgement</h2>
<p>This research was supported by ONR grants N00014-18-1-2442, N00014-18-1-2840, N00014-19-1-2119 and N00014-23-1-2409, AFOSR grant FA9550-18-1-0067, DARPA SAIL-ON grant W911NF-19-2-0006, and a JP Morgan AI Faculty Research Grant to Kambhampati. Sreedharan was supported in part by NSF grant 2303019.</p>
<h2>References</h2>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
[2] Ankuj Arora, Humbert Fiorino, Damien Pellier, Marc MÃ©tivier, and Sylvie Pesty. A review of learning planning action models. The Knowledge Engineering Review, 33:e20, 2018.
[3] Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C Son, and Neeraj Varshney. Can transformers reason about effects of actions? arXiv preprint arXiv:2012.09938, 2020 .
[4] Blai Bonet and Hector Geffner. Learning first-order symbolic representations for planning from the structure of the state space. arXiv preprint arXiv:1909.05546, 2019.
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>[6] Ethan Callanan, Rebecca De Venezia, Victoria Armstrong, Alison Paredes, Tathagata Chakraborti, and Christian Muise. Macq: A holistic view of model acquisition techniques. In The ICAPS Workshop on Knowledge Engineering for Planning and Scheduling (KEPS), 2022.
[7] Hongyi Chen, Yilun Du, Yiye Chen, Joshua B. Tenenbaum, and Patricio A. Vela. Planning with sequence models through iterative energy minimization. In The Eleventh International Conference on Learning Representations, 2023.
[8] Shuo Cheng and Danfei Xu. Guided skill learning and abstraction for long-horizon manipulation. arXiv preprint arXiv:2210.12631, 2022.
[9] Stephen N Cresswell, Thomas L McCluskey, and Margaret M West. Acquiring planning domain models using locm. The Knowledge Engineering Review, 28(2):195-213, 2013.
[10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.
[11] Richard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial intelligence, 2(3-4):189-208, 1971.
[12] Alfonso Gerevini and Ivan Serina. Lpg: A planner based on local search for planning graphs with action costs. In AIPS, volume 2, pages 281-290, 2002.
[13] Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and practice. Elsevier, 2004.
[14] Lin Guan, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging approximate symbolic models for reinforcement learning via skill diversity. In International Conference on Machine Learning, pages 7949-7967. PMLR, 2022.
[15] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.
[16] Malte Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191-246, 2006.
[17] JÃ¶rg Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14:253-302, 2001.
[18] Richard Howey, Derek Long, and Maria Fox. Val: Automatic plan validation, continuous effects and mixed initiative planning using pddl. In 16th IEEE International Conference on Tools with Artificial Intelligence, pages 294-301. IEEE, 2004.
[19] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[20] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.
[21] LeÃ³n Illanes, Xi Yan, Rodrigo Toro Icarte, and Sheila A McIlraith. Symbolic plans as high-level instructions for reinforcement learning. In Proceedings of the international conference on automated planning and scheduling, volume 30, pages 540-550, 2020.
[22] IPC. International planning competition, 1998.
[23] Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, and Lin Guan. Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable ai systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12262-12267, 2022.</p>
<p>[24] Subbarao Kambhampati, Karthik Valmeekam, Matthew Marquez, and Lin Guan. On the role of large language models in planning, July 2023. Tutorial presented at the International Conference on Automated Planning and Scheduling (ICAPS), Prague. https://yochan-lab.github. io/tutorial/ICAPS-2023/.
[25] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez. From skills to symbols: Learning symbolic representations for abstract high-level planning. Journal of Artificial Intelligence Research, 61:215-289, 2018.
[26] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda ViÃ©gas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations, 2023.
[27] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin AkyÃ¼rek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-making. Advances in Neural Information Processing Systems, 35:31199-31212, 2022.
[28] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.
[29] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.
[30] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.
[31] Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, et al. Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. arXiv preprint arXiv:2310.00836, 2023.
[32] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.
[33] Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, and David E. Wilkins. Pddl-the planning domain definition language. 1998.
[34] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023.
[35] Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Gpt3-to-plan: Extracting plans from text using gpt-3. arXiv preprint arXiv:2106.07131, 2021.
[36] OpenAI. Introducing chatgpt by openai, 2022.
[37] OpenAI. Gpt-4 technical report, 2023.
[38] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, and Andrea Loreggia. Plansformer: Generating symbolic plans using transformers. arXiv preprint arXiv:2212.08681, 2022.
[39] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295, 2023.
[40] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018.</p>
<p>[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[42] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.
[43] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.
[44] Stuart Jonathan Russell. Norvig (2003). Artificial intelligence: a modern approach, 25:26, 2003.
[45] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[46] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[47] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[48] Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, TomÃ¡s Lozano-PÃ©rez, and Leslie Pack Kaelbling. Pddl planning with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.
[49] Ron M Simpson, Diane E Kitchin, and Thomas Leo McCluskey. Planning domain definition using gipo. The Knowledge Engineering Review, 22(2):117-134, 2007.
[50] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.
[51] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022.
[52] Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Yasaman Khazaeni, and Subbarao Kambhampati. -d3wa+-a case study of xaip in a model acquisition task for dialogue planning. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 30, pages 488-497, 2020.
[53] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesnâ€™t know itâ€™s wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.
[54] Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.
[55] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. arXiv preprint arXiv:2305.15771, 2023.
[56] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.</p>
<p>[57] Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672, 2023.
[58] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128, 2023.
[59] Qiang Yang, Kangheng Wu, and Yunfei Jiang. Learning action models from plan examples using weighted max-sat. Artificial Intelligence, 171(2-3):107-143, 2007.
[60] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.
[61] Hankz Hankui Zhuo, Qiang Yang, Derek Hao Hu, and Lei Li. Learning complex action models with quantifiers and logical implications. Artificial Intelligence, 174(18):1540-1569, 2010.
[62] Hankz Hankui Zhuo, Yantian Zha, Subbarao Kambhampati, and Xin Tian. Discovering underlying plans based on shallow models. ACM Transactions on Intelligent Systems and Technology (TIST), 11(2):1-30, 2020.</p>
<h1>A Appendix</h1>
<h2>Contents</h2>
<p>1 Introduction ..... 1
2 Related Work ..... 3
3 Problem Setting and Background ..... 3
3.1 Classical planning problems ..... 4
3.2 PDDL ..... 4
4 Methodology ..... 4
4.1 Constructing PDDL models with LLMs ..... 4
4.2 Correcting errors in the initial PDDL models ..... 6
4.3 Generating plans with the extracted PDDL models ..... 7
5 Empirical Evaluation ..... 7
5.1 Constructing PDDL ..... 7
5.2 Correcting PDDL with domain experts ..... 8
5.3 Generating plans with the extracted PDDL models ..... 9
6 Conclusion ..... 10
A Appendix ..... 15
A. 1 Broader impact on using LLMs ..... 16
A. 2 Additional discussion on alternative to action-by-action PDDL construction ..... 16
A. 3 Examples of feedback messages that capture syntax errors ..... 17
A. 4 Techniques that assist users to locate errors in PDDL models ..... 17
A. 5 Detailed description of the Household domain ..... 18
A. 6 Household: Constructing PDDL Models ..... 18
A.6.1 An example prompt for constructing PDDL models of the action "close a small receptacle" ..... 19
A.6.2 Navigate to a furniture piece or an appliance ..... 20
A.6.3 Pick up an object on or in a furniture piece or an appliance ..... 21
A.6.4 Put an object on or in a furniture piece or an appliance ..... 21
A.6.5 Stack Objects ..... 22
A.6.6 Unstack Objects ..... 22
A.6.7 Open a furniture piece or an appliance ..... 23
A.6.8 Close a furniture piece or an appliance ..... 23
A.6.9 Toggle a small appliance on ..... 24
A.6.10 Toggle a small appliance off ..... 24
A.6.11 Slice an object ..... 25
A.6.12 Heat food with a microwave ..... 25
A.6.13 Heat food with pan ..... 26
A.6.14 Transfer food from one small receptacle to another ..... 27
A.6.15 Put an object onto or into a small receptacle like a bowl and plate ..... 27
A.6.16 Pick up an object on or in a small receptacle like a bowl and plate ..... 28
A.6.17 Open a small receptacle such as a lunch box with a lid ..... 29
A.6.18 Close a small receptacle such as a lunch box with a lid ..... 30
A.6.19 Mash food with a blender ..... 30
A.6.20 Wash an object ..... 31
A.6.21 Wipe a surface ..... 31
A.6.22 Vacuum a carpet ..... 32
A.6.23 Empty a vacuum cleaner ..... 32
A.6.24 Examples of PDDL action models constructed by GPT-3.5-Turbo ..... 33
A.6.25 The initial set of predicates extracted by GPT-4 ..... 34
A. 7 Household: Correcting PDDL Models ..... 35
A.7.1 Heat food with a pan ..... 35</p>
<p>A.7.2 Slice an object ..... 37
A.7.3 GPT-3.5-Turbo trying to correct the action model of "slice an object" ..... 38
A. 8 Logistics: Constructing PDDL Models ..... 39
A.8.1 Load a package into a truck ..... 39
A.8.2 Unload a package from a truck ..... 39
A.8.3 Load a package into an airplane ..... 40
A.8.4 Unload a package from an airplane ..... 40
A.8.5 Drive a truck from one location to another in a city ..... 40
A.8.6 Fly an airplane from one city to another ..... 41
A.8.7 Examples of PDDL action models constructed by GPT-3.5-Turbo ..... 41
A.8.8 The initial set of predicates extracted by GPT-4 ..... 42
A. 9 Logistics: Correcting PDDL Models ..... 42
A.9.1 Fly an airplane from one city to another ..... 42
A.9.2 GPT-3.5-Turbo trying to correct the action model of "fly an airplane from one city to another" ..... 43
A. 10 Tyreworld: Constructing PDDL Models ..... 44
A.10.1 Open a container ..... 44
A.10.2 Close a container ..... 45
A.10.3 Fetch an object from a container ..... 45
A.10.4 Put an object into a container ..... 45
A.10.5 Loosen a nut in a hub ..... 46
A.10.6 Tighten a nut in a hub ..... 46
A.10.7 Jack up a hub ..... 47
A.10.8 Jack down a hub ..... 47
A.10.9 Unfasten a hub ..... 47
A.10.10 Fasten a hub ..... 48
A.10.11 Remove wheel from hub ..... 48
A.10.12 Put wheel on hub ..... 49
A.10.13 Inflate wheel ..... 49
A.10.14 Examples of PDDL action models constructed by GPT-3.5-Turbo ..... 50
A.10.15 The initial set of predicates extracted by GPT-4 ..... 50
A. 11 Tyreworld: Correcting PDDL Models ..... 51
A.11.1 Unfasten a hub ..... 51
A.11.2 Inflate wheel ..... 52
A. 12 LLM planners back-prompted by VAL using LLM-acquired PDDL model ..... 53
A.12.1 Examples of prompts for LLM planners ..... 53
A.12.2 Translation to admissible actions ..... 55
A.12.3 Back-prompting LLM planners with validation feedback by VAL ..... 56
A.12.4 Examples of validation feedback and instructions with ordering constraints ..... 57
A. 13 Translating user instructions into PDDL goal specifications ..... 59</p>
<h1>A. 1 Broader impact on using LLMs</h1>
<p>There is a general temptation to use LLMs for a variety of tasks, including plan generation. Given the fact that LLMs cannot guarantee the generation of correct plans, this can lead to safety and security issues downstream. Our approach of teasing a domain model from LLMs, and using it in conjunction with external sound planners aims to mitigate these safety concerns. Nevertheless, given that humans are still in charge of verifying the correctness of the domain models extracted from LLMs, there is still the possibility that an incorrect or undesirable domain model is inadvertently certified correct, leading to undesirable plans and agent behaviors down the line.</p>
<p>Improved explainability is another advantage of extracting explicit domain models. As opposed to just directly querying LLMs for plans, generating behaviors with (intermediate) symbolic models offers additional opportunities for explanation, drawing upon existing works in explainable AI [23]. In cases where the user's understanding does not align with the transcribed model, debugging and model reconciliation techniques such as D3wa+ [52] can also be directly applied.</p>
<h2>A. 2 Additional discussion on alternative to action-by-action PDDL construction</h2>
<p>One alternative to our action-by-action generation could be to include descriptions of all the actions in the prompt and require the LLM to construct the entire domain model in a single dialogue. This</p>
<p>approach may allow the LLM to better establish a global view of all the actions. However, we do not pursue this alternative here for the following reasons: (a) the inclusion of all actions might result in a lengthy prompt, potentially exceeding the context window size of an LLM. This could pose practical issues for utilizing smaller language models (e.g., GPT-3.5-Turbo [36]) or attempting to train smaller specialized models; (b) our integration of corrective feedback relies on continuing the construction dialogue (Sec. 4.2), which necessitates a shorter initial prompt to fit within the context window; (c) our experiments indicate that the action-by-action construction approach already achieves satisfactory results</p>
<h1>A. 3 Examples of feedback messages that capture syntax errors</h1>
<p>Recall that we rely on the PDDL validator in VAL to identify syntax errors. However, several "simpler" syntax errors can be easily detected using simple Python scripts. In our experiments, we wrote our scripts to capture such syntax errors. Also note that since these errors can be detected at a minimal cost, the corresponding feedback messages are directly provided to the LLMs, and they are not counted in the results reported in Table 1. These "simpler" syntax errors include:</p>
<ol>
<li>In this work, we only consider standard base-level PDDL. However, it's possible that the LLMs have seen various extensions of PDDL and might use them in the constructed domain models. Hence, we provide a feedback message to the LLM whenever we detect an unsupported keyword. One example feedback is: "The precondition or effect contain the keyword 'forall' that is not supported in a standard STRIPS style model. Please express the same logic in a simplified way. You can come up with new predicates if needed (but note that you should use existing predicates as much as possible)."</li>
<li>Newly created predicates might have the same names as existing object types, which is not allowed in PDDL. In such cases, a feedback message is provided to notify the LLM about the name clash. For instance, a message might state: "The following predicate(s) have the same name(s) as existing object types: 1. 'smallReceptacle'. Please rename these predicates."</li>
<li>Newly created predicates might have the same names as existing predicates, which is not allowed in PDDL. Also, LLMs often mistakenly list existing predicates under the 'New Predicates' section. In such cases, a feedback message is provided to notify the LLM about the name clash or mistake. For instance, a message might state: "The following predicate(s) have the same name(s) as existing predicate(s): 1. (cutting-board ?z - smallReceptacle), true if the small receptacle ?z is a cutting board I existing predicate with the same name: (cutting-board ?z - householdObject), true if the object ?z is a cutting board. You should reuse existing predicates whenever possible. If you are reusing existing predicate(s), you shouldn't list them under 'New Predicates'. If existing predicates are not enough and you are devising new predicate(s), please use names that are different from existing ones. Please revise the PDDL model to fix this error." This is the most common syntax error made by GPT-4 in our experiments.</li>
<li>The LLMs might fail to only use the object types given in the prompt. An example feedback can be: "There is an invalid object type 'pump' for the parameter ?p."</li>
</ol>
<p>In our experiments, the above error types encompass the majority of syntax errors made by GPT-4. In some less-common cases, GPT-4 may have problems with predicate usage, usually caused by mis-matched object types. This kind of error can be captured by VAL and an example feedback message can be: "There is a syntax error, the second parameter of 'object-on' should be a furnitureAppliance, but a householdObject was given. Please use the correct predicate or devise new one(s) if needed."</p>
<h2>A. 4 Techniques that assist users to locate errors in PDDL models</h2>
<p>Several well-established techniques and tools are available for locating errors in PDDL models. For example, graphical tools like GIPO [49] can effectively visualize the causal dependencies of actions. However, these advanced tools or techniques are beyond the scope of this work. Here, we outline a viable solution as a starting point for users who are unfamiliar with these tools.</p>
<p>There are two stages in which corrective feedback can be obtained: during the construction of the PDDL model, and when the domain model is used to generate a plan. In the first stage, end users can direclty review the domain model and identify potential factual errors. Since all the predicates and parameters are accompanied by natural language descriptions, we can easily convert the PDDL model into natural language and present it to the users. This allows users to pre-screen the preconditions and effects. Note that we do not expect all factual errors to be caught in this stage, because the users may not be aware of certain constraints until they review the final plans. In the second stage, the PDDL model is used to solve downstream planning problems by following the procedure outlined in Sec. 4.3. Two possible cases may arise here: (a) no plan can be found for the given goal specification, or (b) at least one plan is found but it either gets rejected by the users or results in execution failure in the actual environment. To address the first case, we can request the users to suggest a goal-satisficing plan, which is supposed to be executable (but not necessarily optimal). We then use the generated PDDL model to "validate" the suggested plan. This allows us to find the first step in the plan that has an unsatisfied precondition(s). The models of all actions up to this step, along with the unmet precondition(s), are then converted to natural language and presented to the user for inspection. As an example, in the model of "slice an object" extracted by GPT-4 (Sec. A.6.11 in Appendix), the model requires the object to be placed on a cutting board and a furniture piece at the same time, which is not physically possible. By leveraging a user-suggested plan, we can identify the potentially erroneous model(s) and flag incorrect precondition(s). In the second case where an invalid plan is afforded by the PDDL model, there are typically missing preconditions or effects in the actions taken, both during and prior to the execution failure. The users can bring attention to these actions.</p>
<h1>A. 5 Detailed description of the Household domain</h1>
<p>We consider a single-arm robot model that closely resembles the SPOT robot and the Fetch Mobile Manipulator. Consequently, the robot is incapable of grasping multiple objects simultaneously or executing manipulation actions while holding irrelevant items (e.g., opening a fridge door while holding a mug). We also ensure the constraints align with real-world robotic capabilities. For example, we recognize that robot arms may be significantly less flexible than human arms, and therefore, we require certain manipulation tasks to be performed on furniture pieces with open and flexible surfaces (e.g., the robot can only pick up food items from a lunch box when the lunch box is placed on a kitchen countertop instead of inside a fridge). The list of actions and their descriptions can be found in Appendix starting from Sec. A.6.2. The PDDL-construction prompt for this domain includes a general description of the domain, which outlines the tasks to be performed by the robot, the types of objects involved, and details of the robot's morphology. An example of a complete prompt can be found in Fig. A.6.1.</p>
<h2>A. 6 Household: Constructing PDDL Models</h2>
<p>The AI agent here is a household robot that can navigate to various large and normally immovable furniture pieces or appliances in the house to carry out household tasks. Note that the robot has only one gripper, so (a) it can only hold one object; (b) it shouldn't hold any other irrelevant objects in its gripper while performing some manipulation tasks (e.g., opening a drawer or closing a window); (c) operations on small household items should be carried out on furniture with a flat surface to get enough space for manipulation. There are three major types of objects in this domain: robot, furnitureAppliance, and householdObject. The object type furnitureAppliance covers large and normally immovable furniture pieces or appliances, such as stove burners, side tables, dining tables, drawer, cabinets, or microwaves. The object type householdObject covers all other small household items, such as handheld vacuum cleaners, cloth, apples, bananas, and small receptacles like bowls and lunch boxes. There is a subtype of householdObject called smallReceptacle that covers small receptacles like bowls, lunch boxes, plates, etc. In this domain, the locations of the robot and small household items (e.g., apples, oranges, bowls, lunch boxes or lamps) are determined by large and normally immovable furniture pieces or appliances.</p>
<h1>A.6.1 An example prompt for constructing PDDL models of the action "close a small receptacle"</h1>
<h2>An example prompt for constructing PDDL models of the action "close a small receptacle"</h2>
<p>Instructions for the PDDL generation task
You are defining the preconditions and effects (represented in PDDL format) of an AI agent's
$\sim$ actions. Information about the AI agent will be provided in the domain description. Note that
$\sim$ individual conditions in preconditions and effects should be listed separately. For example,
$\sim$ "object_1 is washed and heated" should be considered as two separate conditions "object_1 is
$\sim$ washed" and "object_1 is heated". Also, in PDDL, two predicates cannot have the same name even
$\sim$ if they have different parameters. Each predicate in PDDL must have a unique name, and its
$\sim$ parameters must be explicitly defined in the predicate definition. It is recommended to define
$\sim$ predicate names in an intuitive and readable way.
One or two examples from other domains for illustrating the input and output formats
Here are two examples from the classical BlocksWorld domain for demonstrating the output format.
Domain information: BlocksWorld is a planning domain in artificial intelligence. The AI agent here
$\sim$ is a mechanical robot arm that can pick and place the blocks. Only one block may be moved at a
$\sim$ time: it may either be placed on the table or placed atop another block. Because of this, any
$\sim$ blocks that are, at a given time, under another block cannot be moved. There is only one type
$\sim$ of object in this domain, and that is the block.
Example 1
Action: This action enables the robot to put a block onto the table. For example, the robot puts
$\sim$ block_1 onto the table.
You can create and define new predicates, but you may also reuse the following predicates:
No predicate has been defined yet
Parameters:</p>
<ol>
<li>?x - block: the block to put down</li>
</ol>
<p>Preconditions:
(and
(robot-holding ?x)
)
$\cdots$
Effects:
(and
(not (robot-holding ?x))
(block-clear ?x)
(robot-hand-empty)
(block-on-table ?x)
)</p>
<p>New Predicates:</p>
<ol>
<li>(robot-holding ?x - block): true if the robot arm is holding the block ?x</li>
<li>(block-clear ?x - block): true if the block ?x is not under any another block</li>
<li>(robot-hand-empty): true if the robot arm is not holding any block</li>
<li>(block-on-table ?x - block): true if the block ?x is placed on the table</li>
</ol>
<p>Example 2
Action: This action enables the robot to pick up a block on the table.
You can create and define new predicates, but you may also reuse the following predicates:</p>
<ol>
<li>(robot-holding ?x - block): true if the robot arm is holding the block ?x</li>
<li>(block-clear ?x - block): true if the block ?x is not under any another block</li>
<li>(robot-hand-empty): true if the robot arm is not holding any block</li>
<li>(block-on-table ?x - block): true if the block ?x is placed on the table</li>
</ol>
<p>Parameters:</p>
<ol>
<li>?x - block: the block to pick up</li>
</ol>
<p>Preconditions:
(and
(block-clear ?x)
(block-on-table ?x)
(robot-hand-empty)
)</p>
<div class="codehilite"><pre><span></span><code>Effects:
(and
    (not (block-on-table ?x))
    (not (block-clear ?x))
    (not (robot-hand-empty))
    (robot-holding ?x)
}
</code></pre></div>

<p>New Predicates:
No newly defined predicate</p>
<p>Here is the task.
A natural language description of the domain
Domain information: The AI agent here is a household robot that can navigate to various large and
$\sim$ normally immovable furniture pieces or appliances in the house to carry out household tasks.
$\sim$ Note that the robot has only one gripper, so (a) it can only hold one object; (b) it shouldn't
$\sim$ hold any other irrelevant objects in its gripper while performing some manipulation tasks
$\sim$ (e.g., opening a drawer or closing a window); (c) operations on small household items should
$\sim$ be carried out on furniture with a flat surface to get enough space for manipulation. There
$\sim$ are three types of objects in this domain: robot, furnitureAppliance, and householdObject. The object type furnitureAppliance covers large and normally immovable furniture pieces or appliances, such as stove burners, side tables, dining tables, drawer, cabinets, or microwaves. The object type householdObject covers all other small household items, such as handheld vacuum cleaners, cloth, apples, bananas, and small receptacles like bowls and lunch boxes. In this domain, the locations of the robot and small household items (e.g., apples, oranges, bowls, lunch boxes or lamps) are determined by large and normally immovable furniture pieces or appliances.
A natural language description of the action
Action: This action enables the robot to close a small receptacle receptacle_1 that is openable (e.g. small storage boxes or lunch boxes with lids). For example, the robot closes lunch_box_2, or the robot closes storage box_1.
The dynamically updated list of predicates
You can create and define new predicates, but you may also reuse the following predicates:</p>
<ol>
<li>(robot-at ?x - furnitureAppliance): true if the robot is at the furniture or appliance ?x</li>
<li>(object-on ?x - householdObject ?y - furnitureAppliance): true if the object ?x is on or in the furniture or appliance ?y</li>
<li>(pickupable ?x - householdObject): true if the object ?x can be picked up by the robot</li>
<li>(closed-receptacle ?x - smallReceptacle): true if the small receptacle ?x is closed</li>
</ol>
<p>Parameters:</p>
<h1>A.6.2 Navigate to a furniture piece or an appliance</h1>
<h2>Action description</h2>
<p>This action enables the robot to navigate from one normally immovable piece of furniture to $\sim$ another (e.g., dining tables, side tables, cabinets, and sinks) or an appliance (e.g., refrigerators, coffee makers, microwaves, and washers).</p>
<h2>GPT-4: Navigate to a furniture piece or an appliance</h2>
<h2>Parameters:</h2>
<ol>
<li>?x - furnitureAppliance: the furniture or appliance the robot is currently at</li>
<li>?y - furnitureAppliance: the furniture or appliance the robot wants to navigate to</li>
</ol>
<p>Preconditions:
(and
(robot-at ?x)
(not (= ?x ?y))
)</p>
<p>Effects:
(and
(not (robot-at ?x))
(robot-at ?y)</p>
<h1>A.6.3 Pick up an object on or in a furniture piece or an appliance</h1>
<h2>Action description</h2>
<p>This action enables the robot to pick up an object object_1 in/on a large piece of furniture (e.g., $\sim$ dining tables, drawers, cabinets) or an appliance (e.g., dishwashers and refrigerators). For $\sim$ example, the robot pick up coffee_mug_1 on dining_table_1, the robot pick up fork_1 in $\sim$ drawer_1, or the robot pick up apple_1 in refrigerator_1.</p>
<h2>Additional information from the user</h2>
<p>(a) This action is only applicable for objects that are pickupable by the robot.
(b) The furniture piece or appliance should be opened if it is openable.
(c) The object to pick up should not be stacked on top of other household items.</p>
<h2>GPT-3: Pick up an object on or in a furniture piece or an appliance</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Parameters</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">householdObject</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">furnitureAppliance</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">located</span>
<span class="n">Preconditions</span><span class="o">:</span>
<span class="o">(</span><span class="n">and</span>
<span class="w">    </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">at</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">object</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">pickupable</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="o">)</span>
<span class="w">    </span><span class="n">Inaccurate</span><span class="w"> </span><span class="n">precondition</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;?x is not stacked on top of other object&quot;</span><span class="w"> </span><span class="n">should</span>
<span class="w">        </span><span class="n">only</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">stackable</span><span class="w"> </span><span class="n">object</span>
<span class="w">    </span><span class="o">(</span><span class="n">not</span><span class="w"> </span><span class="o">(</span><span class="n">object</span><span class="o">-</span><span class="n">stacked</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="o">))</span>
<span class="w">    </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">hand</span><span class="o">-</span><span class="n">empty</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">or</span>
<span class="w">        </span><span class="o">(</span><span class="n">not</span><span class="w"> </span><span class="o">(</span><span class="n">openable</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">))</span>
<span class="w">        </span><span class="o">(</span><span class="n">opened</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">)</span>
<span class="w">    </span><span class="o">)</span>
<span class="w">    </span><span class="n">Missing</span><span class="w"> </span><span class="n">precondition</span><span class="o">:</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">stacked</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="n">of</span>
<span class="w">        </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">stackable</span><span class="w"> </span><span class="n">object</span>
<span class="o">)</span>
<span class="n">Effects</span><span class="o">:</span>
<span class="o">(</span><span class="n">and</span>
<span class="w">    </span><span class="o">(</span><span class="n">not</span><span class="w"> </span><span class="o">(</span><span class="n">object</span><span class="o">-</span><span class="n">on</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">))</span>
<span class="w">    </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">holding</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">not</span><span class="w"> </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">hand</span><span class="o">-</span><span class="n">empty</span><span class="o">))</span>
<span class="o">)</span>
</code></pre></div>

<h2>A.6.4 Put an object on or in a furniture piece or an appliance</h2>
<h2>Action description</h2>
<p>This action enables the robot to put an object object_1 in/on a large piece of furniture (e.g., $\sim$ dining tables, drawers, cabinets) or an appliance (e.g., dishwashers and refrigerators). For $\sim$ example, the robot puts coffee_mug_1 on dining_table_1, the robot puts fork_1 in drawer_1, or $\sim$ the robot puts apple_1 in refrigerator_1.</p>
<h2>Additional information from the user</h2>
<p>(a) The furniture piece or appliance should be opened if it is openable.</p>
<h2>GPT-4: Put an object on or in a furniture piece or an appliance</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Parameters</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">householdObject</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="k">in</span><span class="o">/</span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">appliance</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">furnitureAppliance</span><span class="o">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">furniture</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">appliance</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="k">in</span><span class="o">/</span><span class="n">on</span>
<span class="n">Preconditions</span><span class="o">:</span>
<span class="o">(</span><span class="n">and</span>
<span class="w">    </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">at</span><span class="w"> </span><span class="o">?</span><span class="n">y</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">robot</span><span class="o">-</span><span class="n">holding</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="o">)</span>
<span class="w">    </span><span class="o">(</span><span class="n">pickupable</span><span class="w"> </span><span class="o">?</span><span class="n">x</span><span class="o">)</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ At present, our approach only permits a fixed set of object types that are specified in the prompt. Future extensions may explore ways of enabling LLMs to create object-type hierarchy or expand the set of object types as needed.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9830 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9830</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9830</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-76427fe94e4564fd5df2177bb259d93527fddca5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76427fe94e4564fd5df2177bb259d93527fddca5" target="_blank">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training and achieves new state-of-the-art results on the BEIR benchmark.</p>
                <p><strong>Paper Abstract:</strong> Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9830.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9830.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars version 2 (synthetic dataset generator for information retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses an open-source LLM to generate synthetic queries from documents and a pretrained reranker to filter those query-document pairs, producing training data to finetune rerankers for retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InPars-v2 pipeline (GPT-J + monoT5 + BM25)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Composite pipeline: GPT-J-6B (open-source autoregressive LM) is prompted (3-shot with MS MARCO examples, 'gbq' template, greedy decoding) to generate one synthetic query per sampled document; monoT5-3B (pretrained and finetuned on MS MARCO) is used to score and filter generated query-document pairs and then is further finetuned on the resulting synthetic dataset; BM25 (Pyserini) is used for retrieval and for mining negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Corpora from each dataset in the BEIR benchmark; for each dataset they sample up to 100k documents from that dataset's corpus (if the corpus has fewer documents, they generate as many queries as there are documents).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>100000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>No single global theory/topic — the process generates one synthetic natural-language query per sampled document, guided by 3-shot MS MARCO examples and the 'gbq' prompt template (document-to-query generation).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Document-to-query generation using an LLM: for each sampled document GPT-J-6B is prompted with 3 MS MARCO examples to generate a query (greedy decoding). Generated query-document pairs (100k per dataset) are scored by monoT5-3B to estimate relevance; the top 10k highest-scoring pairs are kept as positives. Negatives are sampled by issuing the synthetic query to BM25 and randomly selecting a document from the top-1000 retrievals. The synthetic positive/negative pairs are used to further finetune monoT5-3B (Adafactor optimizer, lr=1e-3, batch contains equal numbers positive/negative). Final evaluation uses BM25 to retrieve 1000 documents per query and the finetuned monoT5 reranker to rerank them.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic query-document training dataset (positive and negative pairs) and finetuned reranker models (monoT5-3B variants per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Zero-shot retrieval evaluation on BEIR benchmark: nDCG@10 measured after using BM25 to retrieve candidates and the finetuned monoT5-3B to rerank; comparisons against BM25 baseline, monoT5-3B finetuned on MS MARCO, InPars-v1, Promptagator and RankT5.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across BEIR, monoT5-3B finetuned on InPars-v2 synthetic data achieved average nDCG@10 of 0.545 (table reports per-dataset nDCG@10; InPars-v2 improves over InPars-v1 and MS MARCO baseline on average and shows particular improvement on TREC-News, Climate-FEVER, Robust and Touche). The paper reports per-dataset values (e.g., Avg: BM25 0.424, monoT5-MARCO 0.533, +InPars-v1 0.539, +InPars-v2 0.545).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Uses an open-source LLM (GPT-J) and open-source code/data; scalable generation (100k queries per dataset) and efficient filtering with a strong reranker; produces synthetic training data that improves reranker performance and yields state-of-the-art average results on BEIR in this study; training and scoring steps use reasonable compute budgets (generation ~30 hours on A100 for 100k queries; scoring ~1.5 hours on TPU v3-8).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Method is focused on generating synthetic queries for IR training rather than explicit synthesis of scientific theories; underperforms approaches that use dataset-specific prompts or different pipelines on certain dataset types (argument retrieval datasets like ArguAna and Touche), and thus may be domain-sensitive; relies on per-dataset generation and finetuning (authors finetune one reranker per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The pipeline did not surpass BM25 on some argument-retrieval datasets without dataset-specific prompts; preliminary experiments showed dataset-specific prompts could improve ArguAna by >10 nDCG@10 points, indicating failure modes when prompts are not tailored to dataset semantics. Some BEIR corpora smaller than 100k yield fewer generated queries (less synthetic data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9830.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9830.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (autoregressive language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 6 billion parameter autoregressive transformer model used to generate synthetic queries from documents via few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer LLM; in this work it is prompted with 3 MS MARCO examples ('gbq' template) to generate one query per document using greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Documents sampled from each BEIR dataset's corpus (up to 100k documents per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>100000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Document-to-query generation: the 'topic' is the content of each sampled document; prompts include 3-shot MS MARCO examples to steer query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot prompting (3-shot) with the 'gbq' prompt template from InPars-v1; greedy decoding used to produce a single synthetic query per document. No additional retrieval-augmented generation or chain-of-thought used.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language queries (synthetic queries) paired with the originating document to form training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quality of outputs indirectly evaluated by training rerankers on the synthetic pairs and measuring downstream retrieval effectiveness on BEIR (nDCG@10).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>When GPT-J was used to generate queries and the pipeline applied, the resulting synthetic data used to finetune monoT5-3B produced better average BEIR nDCG@10 (InPars-v2) than InPars-v1 which used OpenAI's curie; overall pipeline reached a new state of the art on BEIR averages reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Open-source alternative to proprietary LLMs, enabling reproducibility and public release of synthetic data and models; able to generate large volumes of synthetic queries at scale (100k per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller and potentially less capable than some proprietary LLMs (paper contrasts with Promptagator's use of FLAN); generation quality depends on prompt template and few-shot examples; generation is compute-intensive (~30 hours on A100 for 100k queries).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly reported as generating incorrect queries, but downstream limitations include datasets where synthetic-data-trained rerankers did not outperform other methods (e.g., argument retrieval datasets) — indicating generated queries may be less well-suited for some domains or tasks without prompt adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9830.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9830.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>monoT5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>monoT5-3B reranker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3-billion-parameter sequence-to-sequence model adapted for document reranking; used both to filter synthetic query-document pairs and as the final reranker finetuned on synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Document ranking with a pretrained sequence-to-sequence model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>monoT5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained T5-based sequence-to-sequence model adapted for ranking tasks; authors initialize from a checkpoint finetuned on MS MARCO (one epoch) and then use it both to score synthetic query-document pairs (filtering) and to further finetune on the selected synthetic dataset (one epoch) for final reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used to score 100k generated query-document pairs per dataset and to be finetuned on the top 10k positive and 10k negative synthetic pairs per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>100000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Not a single topic — used to estimate relevance between each synthetic query and its source document (and later to rerank candidate documents for test queries).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Scoring/filtering: monoT5-3B (pretrained on MS MARCO) computes a relevancy score for each generated query-document pair; top 10k pairs are kept as positives. Training: monoT5-3B is finetuned further on synthetic positives and sampled negatives (Adafactor, lr=1e-3, batch=64 pos + 64 neg).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Relevance scores used to filter synthetic pairs; finetuned reranker model for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream retrieval performance on BEIR using BM25 retrieval followed by monoT5-3B reranking; evaluation metric is nDCG@10.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using monoT5-3B as filter and finetuning it on the filtered synthetic data (InPars-v2) produced higher BEIR average nDCG@10 (0.545) than monoT5-3B finetuned only on MS MARCO (0.533) and higher than the InPars-v1 variant in many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides an effective, learned filtering mechanism that improves the quality of synthetic training pairs and helps produce stronger rerankers; efficient to score 100k pairs (~1.5 hours on TPU v3-8).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires an existing MS MARCO-finetuned checkpoint as starting point; finetuning is performed per-dataset (authors train a separate model for each BEIR dataset), which may limit scalability across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific monoT5 hallucination failure cases reported; primary failure modes are downstream (datasets where finetuning on InPars-v2 did not improve over other approaches, indicating limits in the synthetic-data + monoT5 approach for some domains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9830.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9830.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Promptagator / FLAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptagator (uses FLAN family models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses few-shot prompting with a proprietary finetuned LLM (FLAN) and dataset-specific prompts to generate synthetic queries and build retrieval models; cited for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finetuned language models are zero-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN (used by Promptagator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN refers to finetuned LMs (as in the referenced FLAN work); Promptagator uses a (proprietary) FLAN model with dataset-specific prompts and a fully trainable retrieval pipeline with smaller models to generate alternative queries for documents in an unsupervised manner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this paper; Promptagator is described as operating in a similar unsupervised manner to InPars, using prompts to generate queries for dataset corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Dataset-specific prompts tailored to the target dataset/task to generate queries appropriate for that dataset (particularly beneficial for argument retrieval datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Few-shot or dataset-specific prompting of a finetuned LM (FLAN) to generate synthetic queries; Promptagator differs from InPars by using dataset-specific prompts, a larger LLM to generate queries, and a fully trainable pipeline with smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic queries / retrieval training data and trained retrieval pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared on BEIR benchmark in this paper; Promptagator reports per-dataset results (table includes Promptagator columns for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Promptagator performs especially well on argument-retrieval datasets (e.g., ArguAna and Touche) where dataset-specific prompts give a large advantage; in some datasets Promptagator outperforms InPars-v2.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Dataset-specific prompting can yield large gains on specialized retrieval tasks (argument retrieval); can operate without supervised MS MARCO data and uses smaller models in the retrieval/reranking steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Uses proprietary LLMs (FLAN) which reduces reproducibility; code/checkpoints may not be publicly available (as contrasted by InPars-v2's open-source release).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper beyond comparative results showing that Promptagator excels in datasets where InPars-v2 underperforms — indicating differing strengths depending on dataset prompt suitability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9830.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9830.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InPars-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InPars (original)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original InPars method that uses LLMs to generate synthetic queries from documents and filters examples using the LM's generation probabilities; predecessor to InPars-v2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inpars: Data augmentation for information retrieval using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InPars-v1 pipeline (curie + log-prob filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original pipeline: used OpenAI's curie model to generate queries from documents (few-shot prompting) and filtered synthetic pairs by selecting the top-scoring pairs according to the log probability assigned by the LLM to the generated query given the examples and document.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to BEIR corpora in prior work; similar document-to-query generation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Document-to-query generation guided by few-shot examples (MS MARCO) and a prompt template ('gbq').</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Generate queries with a proprietary LLM (curie), then filter by the LLM's own generation log-probabilities (select top 10k pairs). Use resulting pairs for training rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic query-document pairs for retriever/reranker training.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream retrieval performance on BEIR; compared directly against InPars-v2 and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>InPars-v1 improved retrieval effectiveness over baselines in prior work; InPars-v2 replaces curie with GPT-J and replaces log-prob filtering with a monoT5 reranker filter, achieving further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrated that few-shot LLM-generated synthetic queries can be effective for data augmentation in retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relied on proprietary LLM (OpenAI curie) and used log-prob filtering which InPars-v2 found less effective than a reranker-based filter.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Compared to InPars-v2, InPars-v1 is less effective on several BEIR datasets (InPars-v2 shows substantial gains on TREC-News, Climate-FEVER, Robust and Touche).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inpars: Data augmentation for information retrieval using large language models. <em>(Rating: 2)</em></li>
                <li>Promptagator: Few-shot dense retrieval from 8 examples. <em>(Rating: 2)</em></li>
                <li>Document ranking with a pretrained sequence-to-sequence model. <em>(Rating: 2)</em></li>
                <li>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners. <em>(Rating: 2)</em></li>
                <li>Rankt5: Fine-tuning t5 for text ranking with ranking losses. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9830",
    "paper_id": "paper-76427fe94e4564fd5df2177bb259d93527fddca5",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "InPars-v2",
            "name_full": "InPars version 2 (synthetic dataset generator for information retrieval)",
            "brief_description": "A pipeline that uses an open-source LLM to generate synthetic queries from documents and a pretrained reranker to filter those query-document pairs, producing training data to finetune rerankers for retrieval tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InPars-v2 pipeline (GPT-J + monoT5 + BM25)",
            "model_description": "Composite pipeline: GPT-J-6B (open-source autoregressive LM) is prompted (3-shot with MS MARCO examples, 'gbq' template, greedy decoding) to generate one synthetic query per sampled document; monoT5-3B (pretrained and finetuned on MS MARCO) is used to score and filter generated query-document pairs and then is further finetuned on the resulting synthetic dataset; BM25 (Pyserini) is used for retrieval and for mining negative examples.",
            "model_size": null,
            "input_corpus_description": "Corpora from each dataset in the BEIR benchmark; for each dataset they sample up to 100k documents from that dataset's corpus (if the corpus has fewer documents, they generate as many queries as there are documents).",
            "input_corpus_size": 100000,
            "topic_query_description": "No single global theory/topic — the process generates one synthetic natural-language query per sampled document, guided by 3-shot MS MARCO examples and the 'gbq' prompt template (document-to-query generation).",
            "distillation_method": "Document-to-query generation using an LLM: for each sampled document GPT-J-6B is prompted with 3 MS MARCO examples to generate a query (greedy decoding). Generated query-document pairs (100k per dataset) are scored by monoT5-3B to estimate relevance; the top 10k highest-scoring pairs are kept as positives. Negatives are sampled by issuing the synthetic query to BM25 and randomly selecting a document from the top-1000 retrievals. The synthetic positive/negative pairs are used to further finetune monoT5-3B (Adafactor optimizer, lr=1e-3, batch contains equal numbers positive/negative). Final evaluation uses BM25 to retrieve 1000 documents per query and the finetuned monoT5 reranker to rerank them.",
            "output_type": "Synthetic query-document training dataset (positive and negative pairs) and finetuned reranker models (monoT5-3B variants per dataset).",
            "output_example": "",
            "evaluation_method": "Zero-shot retrieval evaluation on BEIR benchmark: nDCG@10 measured after using BM25 to retrieve candidates and the finetuned monoT5-3B to rerank; comparisons against BM25 baseline, monoT5-3B finetuned on MS MARCO, InPars-v1, Promptagator and RankT5.",
            "evaluation_results": "Across BEIR, monoT5-3B finetuned on InPars-v2 synthetic data achieved average nDCG@10 of 0.545 (table reports per-dataset nDCG@10; InPars-v2 improves over InPars-v1 and MS MARCO baseline on average and shows particular improvement on TREC-News, Climate-FEVER, Robust and Touche). The paper reports per-dataset values (e.g., Avg: BM25 0.424, monoT5-MARCO 0.533, +InPars-v1 0.539, +InPars-v2 0.545).",
            "strengths": "Uses an open-source LLM (GPT-J) and open-source code/data; scalable generation (100k queries per dataset) and efficient filtering with a strong reranker; produces synthetic training data that improves reranker performance and yields state-of-the-art average results on BEIR in this study; training and scoring steps use reasonable compute budgets (generation ~30 hours on A100 for 100k queries; scoring ~1.5 hours on TPU v3-8).",
            "limitations": "Method is focused on generating synthetic queries for IR training rather than explicit synthesis of scientific theories; underperforms approaches that use dataset-specific prompts or different pipelines on certain dataset types (argument retrieval datasets like ArguAna and Touche), and thus may be domain-sensitive; relies on per-dataset generation and finetuning (authors finetune one reranker per dataset).",
            "failure_cases": "The pipeline did not surpass BM25 on some argument-retrieval datasets without dataset-specific prompts; preliminary experiments showed dataset-specific prompts could improve ArguAna by &gt;10 nDCG@10 points, indicating failure modes when prompts are not tailored to dataset semantics. Some BEIR corpora smaller than 100k yield fewer generated queries (less synthetic data).",
            "uuid": "e9830.0",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J-6B (autoregressive language model)",
            "brief_description": "An open-source 6 billion parameter autoregressive transformer model used to generate synthetic queries from documents via few-shot prompting.",
            "citation_title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_description": "Open-source autoregressive transformer LLM; in this work it is prompted with 3 MS MARCO examples ('gbq' template) to generate one query per document using greedy decoding.",
            "model_size": "6B",
            "input_corpus_description": "Documents sampled from each BEIR dataset's corpus (up to 100k documents per dataset).",
            "input_corpus_size": 100000,
            "topic_query_description": "Document-to-query generation: the 'topic' is the content of each sampled document; prompts include 3-shot MS MARCO examples to steer query generation.",
            "distillation_method": "Few-shot prompting (3-shot) with the 'gbq' prompt template from InPars-v1; greedy decoding used to produce a single synthetic query per document. No additional retrieval-augmented generation or chain-of-thought used.",
            "output_type": "Natural-language queries (synthetic queries) paired with the originating document to form training examples.",
            "output_example": "",
            "evaluation_method": "Quality of outputs indirectly evaluated by training rerankers on the synthetic pairs and measuring downstream retrieval effectiveness on BEIR (nDCG@10).",
            "evaluation_results": "When GPT-J was used to generate queries and the pipeline applied, the resulting synthetic data used to finetune monoT5-3B produced better average BEIR nDCG@10 (InPars-v2) than InPars-v1 which used OpenAI's curie; overall pipeline reached a new state of the art on BEIR averages reported in the paper.",
            "strengths": "Open-source alternative to proprietary LLMs, enabling reproducibility and public release of synthetic data and models; able to generate large volumes of synthetic queries at scale (100k per dataset).",
            "limitations": "Smaller and potentially less capable than some proprietary LLMs (paper contrasts with Promptagator's use of FLAN); generation quality depends on prompt template and few-shot examples; generation is compute-intensive (~30 hours on A100 for 100k queries).",
            "failure_cases": "Not directly reported as generating incorrect queries, but downstream limitations include datasets where synthetic-data-trained rerankers did not outperform other methods (e.g., argument retrieval datasets) — indicating generated queries may be less well-suited for some domains or tasks without prompt adaptation.",
            "uuid": "e9830.1",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "monoT5-3B",
            "name_full": "monoT5-3B reranker",
            "brief_description": "A 3-billion-parameter sequence-to-sequence model adapted for document reranking; used both to filter synthetic query-document pairs and as the final reranker finetuned on synthetic data.",
            "citation_title": "Document ranking with a pretrained sequence-to-sequence model.",
            "mention_or_use": "use",
            "model_name": "monoT5-3B",
            "model_description": "Pretrained T5-based sequence-to-sequence model adapted for ranking tasks; authors initialize from a checkpoint finetuned on MS MARCO (one epoch) and then use it both to score synthetic query-document pairs (filtering) and to further finetune on the selected synthetic dataset (one epoch) for final reranking.",
            "model_size": "3B",
            "input_corpus_description": "Used to score 100k generated query-document pairs per dataset and to be finetuned on the top 10k positive and 10k negative synthetic pairs per dataset.",
            "input_corpus_size": 100000,
            "topic_query_description": "Not a single topic — used to estimate relevance between each synthetic query and its source document (and later to rerank candidate documents for test queries).",
            "distillation_method": "Scoring/filtering: monoT5-3B (pretrained on MS MARCO) computes a relevancy score for each generated query-document pair; top 10k pairs are kept as positives. Training: monoT5-3B is finetuned further on synthetic positives and sampled negatives (Adafactor, lr=1e-3, batch=64 pos + 64 neg).",
            "output_type": "Relevance scores used to filter synthetic pairs; finetuned reranker model for retrieval.",
            "output_example": "",
            "evaluation_method": "Downstream retrieval performance on BEIR using BM25 retrieval followed by monoT5-3B reranking; evaluation metric is nDCG@10.",
            "evaluation_results": "Using monoT5-3B as filter and finetuning it on the filtered synthetic data (InPars-v2) produced higher BEIR average nDCG@10 (0.545) than monoT5-3B finetuned only on MS MARCO (0.533) and higher than the InPars-v1 variant in many datasets.",
            "strengths": "Provides an effective, learned filtering mechanism that improves the quality of synthetic training pairs and helps produce stronger rerankers; efficient to score 100k pairs (~1.5 hours on TPU v3-8).",
            "limitations": "Requires an existing MS MARCO-finetuned checkpoint as starting point; finetuning is performed per-dataset (authors train a separate model for each BEIR dataset), which may limit scalability across many domains.",
            "failure_cases": "No specific monoT5 hallucination failure cases reported; primary failure modes are downstream (datasets where finetuning on InPars-v2 did not improve over other approaches, indicating limits in the synthetic-data + monoT5 approach for some domains).",
            "uuid": "e9830.2",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Promptagator / FLAN",
            "name_full": "Promptagator (uses FLAN family models)",
            "brief_description": "A method that uses few-shot prompting with a proprietary finetuned LLM (FLAN) and dataset-specific prompts to generate synthetic queries and build retrieval models; cited for comparison.",
            "citation_title": "Finetuned language models are zero-shot learners.",
            "mention_or_use": "mention",
            "model_name": "FLAN (used by Promptagator)",
            "model_description": "FLAN refers to finetuned LMs (as in the referenced FLAN work); Promptagator uses a (proprietary) FLAN model with dataset-specific prompts and a fully trainable retrieval pipeline with smaller models to generate alternative queries for documents in an unsupervised manner.",
            "model_size": null,
            "input_corpus_description": "Not specified in detail in this paper; Promptagator is described as operating in a similar unsupervised manner to InPars, using prompts to generate queries for dataset corpora.",
            "input_corpus_size": null,
            "topic_query_description": "Dataset-specific prompts tailored to the target dataset/task to generate queries appropriate for that dataset (particularly beneficial for argument retrieval datasets).",
            "distillation_method": "Few-shot or dataset-specific prompting of a finetuned LM (FLAN) to generate synthetic queries; Promptagator differs from InPars by using dataset-specific prompts, a larger LLM to generate queries, and a fully trainable pipeline with smaller models.",
            "output_type": "Synthetic queries / retrieval training data and trained retrieval pipelines.",
            "output_example": "",
            "evaluation_method": "Compared on BEIR benchmark in this paper; Promptagator reports per-dataset results (table includes Promptagator columns for comparison).",
            "evaluation_results": "Promptagator performs especially well on argument-retrieval datasets (e.g., ArguAna and Touche) where dataset-specific prompts give a large advantage; in some datasets Promptagator outperforms InPars-v2.",
            "strengths": "Dataset-specific prompting can yield large gains on specialized retrieval tasks (argument retrieval); can operate without supervised MS MARCO data and uses smaller models in the retrieval/reranking steps.",
            "limitations": "Uses proprietary LLMs (FLAN) which reduces reproducibility; code/checkpoints may not be publicly available (as contrasted by InPars-v2's open-source release).",
            "failure_cases": "Not detailed in this paper beyond comparative results showing that Promptagator excels in datasets where InPars-v2 underperforms — indicating differing strengths depending on dataset prompt suitability.",
            "uuid": "e9830.3",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "InPars-v1",
            "name_full": "InPars (original)",
            "brief_description": "The original InPars method that uses LLMs to generate synthetic queries from documents and filters examples using the LM's generation probabilities; predecessor to InPars-v2.",
            "citation_title": "Inpars: Data augmentation for information retrieval using large language models.",
            "mention_or_use": "mention",
            "model_name": "InPars-v1 pipeline (curie + log-prob filtering)",
            "model_description": "Original pipeline: used OpenAI's curie model to generate queries from documents (few-shot prompting) and filtered synthetic pairs by selecting the top-scoring pairs according to the log probability assigned by the LLM to the generated query given the examples and document.",
            "model_size": null,
            "input_corpus_description": "Applied to BEIR corpora in prior work; similar document-to-query generation approach.",
            "input_corpus_size": null,
            "topic_query_description": "Document-to-query generation guided by few-shot examples (MS MARCO) and a prompt template ('gbq').",
            "distillation_method": "Generate queries with a proprietary LLM (curie), then filter by the LLM's own generation log-probabilities (select top 10k pairs). Use resulting pairs for training rerankers.",
            "output_type": "Synthetic query-document pairs for retriever/reranker training.",
            "output_example": "",
            "evaluation_method": "Downstream retrieval performance on BEIR; compared directly against InPars-v2 and other baselines.",
            "evaluation_results": "InPars-v1 improved retrieval effectiveness over baselines in prior work; InPars-v2 replaces curie with GPT-J and replaces log-prob filtering with a monoT5 reranker filter, achieving further improvements.",
            "strengths": "Demonstrated that few-shot LLM-generated synthetic queries can be effective for data augmentation in retrieval.",
            "limitations": "Relied on proprietary LLM (OpenAI curie) and used log-prob filtering which InPars-v2 found less effective than a reranker-based filter.",
            "failure_cases": "Compared to InPars-v2, InPars-v1 is less effective on several BEIR datasets (InPars-v2 shows substantial gains on TREC-News, Climate-FEVER, Robust and Touche).",
            "uuid": "e9830.4",
            "source_info": {
                "paper_title": "InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inpars: Data augmentation for information retrieval using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Promptagator: Few-shot dense retrieval from 8 examples.",
            "rating": 2
        },
        {
            "paper_title": "Document ranking with a pretrained sequence-to-sequence model.",
            "rating": 2
        },
        {
            "paper_title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners.",
            "rating": 2
        },
        {
            "paper_title": "Rankt5: Fine-tuning t5 for text ranking with ranking losses.",
            "rating": 1
        }
    ],
    "cost": 0.012896,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</h1>
<p>Vitor Jeronymo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Marzieh Fadaee<br>Zeta Alpha, Netherlands</p>
<p>Luiz Bonifacio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Roberto Lotufo<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Rodrigo Nogueira<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Zeta Alpha, Netherlands</p>
<p>Hugo Abonizio<br>NeuralMind, Brazil<br>FEEC-UNICAMP, Brazil<br>Jakub Zavrel<br>Zeta Alpha, Netherlands</p>
<h4>Abstract</h4>
<p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/legacy/inpars-v2</p>
<h2>1 Introduction and Background</h2>
<p>Data augmentation has been a reliable tool to improve the effectiveness of AI models in the face of the scarcity of high-quality in-domain training data, which is a common problem in practical applications. Previous work by Bonifacio et al. [1] and Dai et al. [2] successfully leveraged the few-shot capabilities of LLMs to generate reliable synthetic training data for information retrieval models. These training data helped their models achieve state-of-the-art (SOTA) results on the BEIR benchmark [6].
Bonifacio et al. [1] propose InPars where they generate queries from documents in the corpus using LLMs. Similarly to Bonifacio et al. [1], the recently published Promptagator [2] model also feeds prompts to LLMs in order to generate alternative queries for a given document in an unsupervised manner. It differs primarily from InPars in that it uses dataset-specific prompts, a larger LLM to generate queries, and a fully trainable retrieval pipeline with smaller models.
This work extends the method of Bonifacio et al. [1] by using a reranker as a filtering mechanism to select the best synthetically generated examples and further improving retrieval effectiveness</p>
<p>on BEIR. We also use an open-source query generator as opposed to the proprietary one used by Bonifacio et al. and provide the source code and data to reproduce our results on TPUs. We refer to Bonifacio et al. [1] model as Inpars-v1 and the model presented in this paper as Inpars-v2.</p>
<h1>2 Methodology</h1>
<p>In this section, we explain the experiments we performed and how they differ from InPars-v1 [1].
To generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace OpenAI's curie model used in InPars-v1. For each dataset in the BEIR benchmark, we sample 100k documents from its corpus and generate one synthetic query per document using GPT-J prompted with 3 examples from MS MARCO. We use greedy decoding and the "gbq" prompt template from InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these cases, we generate as many synthetic queries as there are documents in the corpus. It takes on average 30 hours on an A100 GPU to generate 100k queries.
Once the synthetic queries are generated, we apply a filtering step to select query-document pairs that are more likely to be relevant to each other. In InPars-v1, this filtering step consisted of selecting the top 10 k query-document pairs with the highest log probabilities of generating a query given the 3 -shot examples and the document as input. In InPars-v2, we use monoT5-3B [4] already finetuned on MS MARCO for one epoch ${ }^{1}$ to estimate a relevancy score for each of the 100k query-document pairs. Then, we keep only the top 10 k pairs with the highest scores as our positive query-document pairs for training. It takes approximately 1.5 hours to score 100 k query-document pairs on a TPU v3-8. It should take twice as much on a A100.
To obtain negatives (i.e., non-relevant) query-document pairs, we randomly sample one document from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists of 10 k positive query-document pairs and 10 k negative query-document pairs.
The rerankers are finetuned in the same manner as in InPars-v1: monoT5-3B is finetuned on MS MARCO for one epoch and then further finetuned for one epoch on the synthetic data. We use the Adafactor optimizer [5] with a constant learning rate of 1e-3. Each batch has 64 positive and 64 negative query-document pairs randomly sampled from the training dataset. We finetune one model on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset, which are then evaluated on the corresponding test sets. Finetuning on each synthetic dataset takes less than 10 minutes on a TPU v3-8.
Evaluation is performed using the following pipeline: first we use Pyserini's [3] flat indexes ${ }^{2}$ to retrieve a thousand documents for each query using BM25 with default parameters ( $\mathrm{k} 1=0.9, \mathrm{~b}=0.4$ ), for each dataset. Then we use the finetuned monoT5-3B models to rerank these documents.</p>
<h2>3 Results</h2>
<p>Table 1 presents results for BM25 (2nd column), monoT5-3B finetuned on MS MARCO (3rd column), monoT5-3b finetuned on MS MARCO and further finetuned on InPars-v1 (4th column), and monoT5-3B finetuned on MS MARCO and then finetuned on InPars-v2 data (5th column). Compared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust and Touche. Additionally, we compare our method with Promptagator [2] and RankT5 [10]. Taking into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.
Promptagator and RankT5 strive on datasets that monoT5 and InPars-v2 cannot even surpass BM25, such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly different from other datasets in the BEIR benchmark. As a result, they benefit from using custom prompts. ${ }^{3}$ Promptagator does this without using supervised data from MS MARCO and using smaller T5 models with 110M parameters for the retrieval and reranking steps.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">monoT5-3B <br> MARCO</th>
<th style="text-align: center;">+InPars-v1</th>
<th style="text-align: center;">+InPars-v2</th>
<th style="text-align: center;">PrGator</th>
<th style="text-align: center;">RankT5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TREC-Covid</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.823</td>
</tr>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FiQA</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">SciDocs</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: left;">SciFact</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.760</td>
</tr>
<tr>
<td style="text-align: left;">NFCorpus</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.579</td>
</tr>
<tr>
<td style="text-align: left;">Natural Questions</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">HotpotQA</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: left;">TREC-News</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Quora</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.819</td>
</tr>
<tr>
<td style="text-align: left;">FEVER</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: left;">Climate-FEVER</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.275</td>
</tr>
<tr>
<td style="text-align: left;">Signal</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.319</td>
</tr>
<tr>
<td style="text-align: left;">ArguAna</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.406</td>
</tr>
<tr>
<td style="text-align: left;">Touche</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: left;">CQADupstack</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg PrGator</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.536</td>
</tr>
</tbody>
</table>
<p>Table 1: nDCG@10 on BEIR. "Avg PrGator" is the average of datasets reported by Promptagator.</p>
<p>Promptagator uses a proprietary model, FLAN [9], to generate synthetic queries. The RankT5 model is a modified version of the monoT5 reranker, but its checkpoint and code are not published. In this work, we make the code, models, and data open-source and publicly available.</p>
<h1>4 Conclusion</h1>
<p>In this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. Our results show that we achieve effectiveness on par with the state of the art on BEIR. The synthetic data and finetuned models were publicly released.</p>
<h2>Acknowledgments</h2>
<p>This research was partially supported by Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) (project id 2022/01640-2). We also thank Centro Nacional de Processamento de Alto Desempenho (CENAPAD-SP) and Google Cloud for computing credits.</p>
<h2>References</h2>
<p>[1] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.
[2] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. arXiv preprint arXiv:2102.10073, 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence-to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 708-718, 2020.</p>
<p>[5] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.
[7] H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241-251, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[8] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[9] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/castorini/monot5-3b-msmarco-10k
${ }^{2}$ As opposed to the multifield index.
${ }^{3}$ In preliminary experiments, we also observed an improvement of more than 10 nDCG@10 points on ArguAna by using a dataset-specific prompt to generate synthetic queries. More details and results on the full BEIR benchmark will appear in an upcoming paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
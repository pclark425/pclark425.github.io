<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9170 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9170</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9170</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-264288848</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.669.pdf" target="_blank">CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</a></p>
                <p><strong>Paper Abstract:</strong> Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9170.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9170.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used in this paper to generate open-ended text simulations of human personas (demographic subgroups) across contexts (online forum, interview, Twitter) to study how LLMs reproduce or exaggerate persona-defining characteristics (caricature).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art OpenAI LLM (OpenAI, 2023) used as the simulator in all experiments of this paper; the paper does not specify parameter counts or training-data details beyond the OpenAI citation, but treats GPT-4 as the highest-quality model compared to older/open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / social science experiments / HCI / public opinion research</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of responses from specified personas (demographic subgroups: race/ethnicity, gender, age, political ideology, nonbinary, neutral 'person') to topics framed in contexts including online forum posts, interview-style survey questions (open-ended conversion of Pew survey items), and Twitter posts; goal: model what a 'persona' would say on a given topic/context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Two-part caricature metric: (1) Individuation measured as differentiability from default-persona via a supervised classifier (random forest) on contextualized embeddings (Sentence-BERT all-mpnet-base-v2) reported as classifier accuracy on test split; (2) Exaggeration measured as normalized cosine similarity of simulation embeddings to a persona–topic semantic axis (constructed via Fightin' Words seed words and contextualized embeddings), yielding a 0–1 normalized exaggeration score. Additional robustness used unsupervised V-measure clustering and alternative embedding model (all-distilroberta-v1).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Individuation: mean classifier accuracy > 0.5 for every persona (better than chance); in the interview context mean individuation > 0.95 for every persona. Power-analysis summary: lowest mean individuation observed across configurations was 0.65 with SD up to 0.2. Exaggeration (caricature) scores: reported as normalized cosine similarity (0–1); examples: some simulations have mean exaggeration as high as ~0.6–0.8 on general/uncontroversial topics (figures show high values for e.g. Health and other general topics), and the paper reports a lowest mean exaggeration of 0.23 for some less-exaggerated simulations with SD up to 0.37. Standard errors for plotted means were reported < 0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Identified factors that affect simulation behavior (and thus the measured individuation/exaggeration): (a) Topic specificity — less specific/general topics produce higher exaggeration (more caricature); more specific topics reduce caricature. (b) Persona type — political leanings and certain marginalized demographic personas (e.g., nonbinary, some race/ethnicity groups) show higher exaggeration. (c) Context — interview context had higher individuation (lower variability) than online-forum; context interacts with topic specificity. (d) Model quality — authors note older/open-source LLMs and earlier models perform worse at realistic simulation (yielding lower-quality outputs). (e) Prompt phrasing and norms encoded in context (LLM sensitivity to prompt engineering). (f) Memorization dynamics and implicit model defaults (models default to particular perspectives present in training data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines used in analysis: (1) default-persona simulation S_{_,t,c} (prompt with generic 'person') and (2) default-topic simulation S_{p,_,c} (prompt without specified topic). Individuation is measured relative to default-persona; exaggeration axis is constructed from words distinguishing default-topic vs default-persona. The paper does not report direct human-expert accuracy comparisons as quantitative baselines in these experiments, but references prior replication-style studies that compare LLM output distributions to human survey/multiple-choice results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Caricature (outputs that exaggerate persona-defining characteristics and produce homogenous/stereotyped narratives); possible false positives from the metric (simulations passing the metric could still contain other biases); method focuses on one-round open-ended responses and subpopulation personas (not multi-round or complex longitudinal simulations); semantic-axis measurement depends on seed-word selection and context-specific defaults; no comprehensive labeling of all stereotypes — some stereotypes may persist even when caricature score is low.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Recommendations include: (1) use more specific topics to mitigate caricature; (2) run the proposed caricature metric to test simulations in the intended context before deploying or interpreting results; (3) document positionality, prompt/context, and model choices (CoMPosT dimensions); (4) approach simulation of politicized or marginalized groups with particular care; (5) consider injecting variation or multifaceted personas to better capture heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9170.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9170.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 / davinci (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in the paper as the LLM used in prior work to simulate human participants for social science experiments and public-survey-style tasks (multiple-choice and open-response in referenced studies).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier generation OpenAI model (davinci variant) referenced via prior work in the literature mapping; the current paper does not provide new training-data or parameter details beyond citing the prior studies that used it.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social science experiments / public opinion research / psychology</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>In prior cited work, used to simulate responses of public-survey participants and social-science experiment participants (often framed as replication of multiple-choice human experiments or generating participant responses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>In the cited prior literature, evaluation was often replication of human multiple-choice distributions or believability; this paper does not report new accuracy numbers for GPT-3 davinci.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Paper-level notes: older models (including GPT-3 variants) tend to produce lower-quality and less-realistic simulations compared to state-of-the-art models (authors note open-source/older models fare worse). Specific factors such as prompt/context and memorization also affect outcomes (discussed generally).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in mapping of prior work (Table A6) but no direct quantitative comparisons provided in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not analyzed directly in this paper; authors state earlier models yield unrealistic outputs and lower quality when used for simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use state-of-the-art models for higher-fidelity simulation; nevertheless, even with powerful models, run caricature checks and document CoMPosT dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9170.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9170.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / GPT-3.5-turbo (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in the paper's literature mapping as used by several prior studies to simulate human participants for surveys, interviews, and open-response tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (including turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Intermediate-generation OpenAI models referenced in prior work; the present paper cites those works in its mapping but does not run experiments with GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion surveys / HCI / psychology / social computing</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Prior work used these models for simulating public-survey responses, interview-style outputs, and populated prototypes/agents in social computing contexts (see referenced papers mapped in Table A6).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Prior-cited evaluation styles include replication of multiple-choice human survey distributions and qualitative believability; this paper does not provide new metric values for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Mentioned factors: model generation quality (GPT-3.5 lower than GPT-4), prompt/context specifics, and dataset/training biases; no new quantitative analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced prior works used GPT-3.5 as the model under study; this paper reports that GPT-4 produces higher-quality simulations than open-source/older models based on authors' and cited works' findings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly evaluated here; implicitly less capable than GPT-4 for realistic simulation tasks according to authors' statements.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>When possible, prefer higher-capability models and evaluate caricature susceptibility; document context and persona choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9170.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9170.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source / older LLMs (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source and older LLMs (aggregated mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated mention that open-source and older LLMs produce lower-quality, less-realistic simulations and thus were not used for the main experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various open-source / older LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Refers generically to non-state-of-the-art or earlier-generation models (authors cite others e.g., Dubois et al., 2023 observing such differences); specific models are not enumerated or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>General text-based simulation across social science / HCI contexts (as a contrast to GPT-4 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text simulation of human personas in earlier cited studies or as attempted but judged lower quality by the authors (not used for the main experiments in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model capability and training differences lead to lower realism/quality; prompt engineering and context dependence also affect outputs. Authors note they observed open-source/older models produce unrealistic outputs and significantly lower quality compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively against GPT-4 (GPT-4 judged superior for these simulation tasks); no quantitative head-to-head reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower fidelity, unrealistic outputs, more prone to failures in capturing nuanced persona/topic interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend using higher-capability models for simulation research and running targeted evaluation (e.g., the proposed caricature metrics) regardless of the model chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of one, many: Using language models to simulate human samples <em>(Rating: 2)</em></li>
                <li>NLPositionality: Characterizing design biases of datasets and models <em>(Rating: 2)</em></li>
                <li>Social simulacra: Creating populated prototypes for social computing systems <em>(Rating: 2)</em></li>
                <li>CommunityLM: Probing partisan worldviews from language models <em>(Rating: 2)</em></li>
                <li>Using large language models to simulate multiple humans and replicate human subject studies <em>(Rating: 2)</em></li>
                <li>Alpaca-Farm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
                <li>Evaluating large language models in generating synthetic HCI research data: a case study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9170",
    "paper_id": "paper-264288848",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4 (simulations)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "Used in this paper to generate open-ended text simulations of human personas (demographic subgroups) across contexts (online forum, interview, Twitter) to study how LLMs reproduce or exaggerate persona-defining characteristics (caricature).",
            "citation_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art OpenAI LLM (OpenAI, 2023) used as the simulator in all experiments of this paper; the paper does not specify parameter counts or training-data details beyond the OpenAI citation, but treats GPT-4 as the highest-quality model compared to older/open-source models.",
            "scientific_subdomain": "Computational social science / social science experiments / HCI / public opinion research",
            "simulation_task": "Text-based simulation of responses from specified personas (demographic subgroups: race/ethnicity, gender, age, political ideology, nonbinary, neutral 'person') to topics framed in contexts including online forum posts, interview-style survey questions (open-ended conversion of Pew survey items), and Twitter posts; goal: model what a 'persona' would say on a given topic/context.",
            "evaluation_metric": "Two-part caricature metric: (1) Individuation measured as differentiability from default-persona via a supervised classifier (random forest) on contextualized embeddings (Sentence-BERT all-mpnet-base-v2) reported as classifier accuracy on test split; (2) Exaggeration measured as normalized cosine similarity of simulation embeddings to a persona–topic semantic axis (constructed via Fightin' Words seed words and contextualized embeddings), yielding a 0–1 normalized exaggeration score. Additional robustness used unsupervised V-measure clustering and alternative embedding model (all-distilroberta-v1).",
            "simulation_accuracy": "Individuation: mean classifier accuracy &gt; 0.5 for every persona (better than chance); in the interview context mean individuation &gt; 0.95 for every persona. Power-analysis summary: lowest mean individuation observed across configurations was 0.65 with SD up to 0.2. Exaggeration (caricature) scores: reported as normalized cosine similarity (0–1); examples: some simulations have mean exaggeration as high as ~0.6–0.8 on general/uncontroversial topics (figures show high values for e.g. Health and other general topics), and the paper reports a lowest mean exaggeration of 0.23 for some less-exaggerated simulations with SD up to 0.37. Standard errors for plotted means were reported &lt; 0.02.",
            "factors_affecting_accuracy": "Identified factors that affect simulation behavior (and thus the measured individuation/exaggeration): (a) Topic specificity — less specific/general topics produce higher exaggeration (more caricature); more specific topics reduce caricature. (b) Persona type — political leanings and certain marginalized demographic personas (e.g., nonbinary, some race/ethnicity groups) show higher exaggeration. (c) Context — interview context had higher individuation (lower variability) than online-forum; context interacts with topic specificity. (d) Model quality — authors note older/open-source LLMs and earlier models perform worse at realistic simulation (yielding lower-quality outputs). (e) Prompt phrasing and norms encoded in context (LLM sensitivity to prompt engineering). (f) Memorization dynamics and implicit model defaults (models default to particular perspectives present in training data).",
            "comparison_baseline": "Baselines used in analysis: (1) default-persona simulation S_{_,t,c} (prompt with generic 'person') and (2) default-topic simulation S_{p,_,c} (prompt without specified topic). Individuation is measured relative to default-persona; exaggeration axis is constructed from words distinguishing default-topic vs default-persona. The paper does not report direct human-expert accuracy comparisons as quantitative baselines in these experiments, but references prior replication-style studies that compare LLM output distributions to human survey/multiple-choice results.",
            "limitations_or_failure_cases": "Caricature (outputs that exaggerate persona-defining characteristics and produce homogenous/stereotyped narratives); possible false positives from the metric (simulations passing the metric could still contain other biases); method focuses on one-round open-ended responses and subpopulation personas (not multi-round or complex longitudinal simulations); semantic-axis measurement depends on seed-word selection and context-specific defaults; no comprehensive labeling of all stereotypes — some stereotypes may persist even when caricature score is low.",
            "author_recommendations_or_insights": "Recommendations include: (1) use more specific topics to mitigate caricature; (2) run the proposed caricature metric to test simulations in the intended context before deploying or interpreting results; (3) document positionality, prompt/context, and model choices (CoMPosT dimensions); (4) approach simulation of politicized or marginalized groups with particular care; (5) consider injecting variation or multifaceted personas to better capture heterogeneity.",
            "uuid": "e9170.0",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 / davinci (mentioned)",
            "name_full": "GPT-3 (davinci)",
            "brief_description": "Mentioned in the paper as the LLM used in prior work to simulate human participants for social science experiments and public-survey-style tasks (multiple-choice and open-response in referenced studies).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (davinci)",
            "model_description": "Earlier generation OpenAI model (davinci variant) referenced via prior work in the literature mapping; the current paper does not provide new training-data or parameter details beyond citing the prior studies that used it.",
            "scientific_subdomain": "Social science experiments / public opinion research / psychology",
            "simulation_task": "In prior cited work, used to simulate responses of public-survey participants and social-science experiment participants (often framed as replication of multiple-choice human experiments or generating participant responses).",
            "evaluation_metric": "In the cited prior literature, evaluation was often replication of human multiple-choice distributions or believability; this paper does not report new accuracy numbers for GPT-3 davinci.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Paper-level notes: older models (including GPT-3 variants) tend to produce lower-quality and less-realistic simulations compared to state-of-the-art models (authors note open-source/older models fare worse). Specific factors such as prompt/context and memorization also affect outcomes (discussed generally).",
            "comparison_baseline": "Mentioned in mapping of prior work (Table A6) but no direct quantitative comparisons provided in this paper's experiments.",
            "limitations_or_failure_cases": "Not analyzed directly in this paper; authors state earlier models yield unrealistic outputs and lower quality when used for simulation.",
            "author_recommendations_or_insights": "Use state-of-the-art models for higher-fidelity simulation; nevertheless, even with powerful models, run caricature checks and document CoMPosT dimensions.",
            "uuid": "e9170.1",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 / GPT-3.5-turbo (mentioned)",
            "name_full": "GPT-3.5 / GPT-3.5-turbo",
            "brief_description": "Mentioned in the paper's literature mapping as used by several prior studies to simulate human participants for surveys, interviews, and open-response tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 (including turbo variants)",
            "model_description": "Intermediate-generation OpenAI models referenced in prior work; the present paper cites those works in its mapping but does not run experiments with GPT-3.5.",
            "scientific_subdomain": "Public opinion surveys / HCI / psychology / social computing",
            "simulation_task": "Prior work used these models for simulating public-survey responses, interview-style outputs, and populated prototypes/agents in social computing contexts (see referenced papers mapped in Table A6).",
            "evaluation_metric": "Prior-cited evaluation styles include replication of multiple-choice human survey distributions and qualitative believability; this paper does not provide new metric values for GPT-3.5.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Mentioned factors: model generation quality (GPT-3.5 lower than GPT-4), prompt/context specifics, and dataset/training biases; no new quantitative analysis in this paper.",
            "comparison_baseline": "Referenced prior works used GPT-3.5 as the model under study; this paper reports that GPT-4 produces higher-quality simulations than open-source/older models based on authors' and cited works' findings.",
            "limitations_or_failure_cases": "Not directly evaluated here; implicitly less capable than GPT-4 for realistic simulation tasks according to authors' statements.",
            "author_recommendations_or_insights": "When possible, prefer higher-capability models and evaluate caricature susceptibility; document context and persona choices.",
            "uuid": "e9170.2",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Open-source / older LLMs (mentioned)",
            "name_full": "Open-source and older LLMs (aggregated mention)",
            "brief_description": "Aggregated mention that open-source and older LLMs produce lower-quality, less-realistic simulations and thus were not used for the main experiments in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various open-source / older LLMs (unspecified)",
            "model_description": "Refers generically to non-state-of-the-art or earlier-generation models (authors cite others e.g., Dubois et al., 2023 observing such differences); specific models are not enumerated or evaluated in this paper.",
            "scientific_subdomain": "General text-based simulation across social science / HCI contexts (as a contrast to GPT-4 experiments)",
            "simulation_task": "Text simulation of human personas in earlier cited studies or as attempted but judged lower quality by the authors (not used for the main experiments in this work).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Model capability and training differences lead to lower realism/quality; prompt engineering and context dependence also affect outputs. Authors note they observed open-source/older models produce unrealistic outputs and significantly lower quality compared to GPT-4.",
            "comparison_baseline": "Compared qualitatively against GPT-4 (GPT-4 judged superior for these simulation tasks); no quantitative head-to-head reported in this paper.",
            "limitations_or_failure_cases": "Lower fidelity, unrealistic outputs, more prone to failures in capturing nuanced persona/topic interactions.",
            "author_recommendations_or_insights": "Authors recommend using higher-capability models for simulation research and running targeted evaluation (e.g., the proposed caricature metrics) regardless of the model chosen.",
            "uuid": "e9170.3",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of one, many: Using language models to simulate human samples",
            "rating": 2,
            "sanitized_title": "out_of_one_many_using_language_models_to_simulate_human_samples"
        },
        {
            "paper_title": "NLPositionality: Characterizing design biases of datasets and models",
            "rating": 2,
            "sanitized_title": "nlpositionality_characterizing_design_biases_of_datasets_and_models"
        },
        {
            "paper_title": "Social simulacra: Creating populated prototypes for social computing systems",
            "rating": 2,
            "sanitized_title": "social_simulacra_creating_populated_prototypes_for_social_computing_systems"
        },
        {
            "paper_title": "CommunityLM: Probing partisan worldviews from language models",
            "rating": 2,
            "sanitized_title": "communitylm_probing_partisan_worldviews_from_language_models"
        },
        {
            "paper_title": "Using large language models to simulate multiple humans and replicate human subject studies",
            "rating": 2,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        },
        {
            "paper_title": "Alpaca-Farm: A simulation framework for methods that learn from human feedback",
            "rating": 1,
            "sanitized_title": "alpacafarm_a_simulation_framework_for_methods_that_learn_from_human_feedback"
        },
        {
            "paper_title": "Evaluating large language models in generating synthetic HCI research data: a case study",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_in_generating_synthetic_hci_research_data_a_case_study"
        }
    ],
    "cost": 0.016166,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</p>
<p>Myra Cheng 
Department of Computer Science
Stanford University</p>
<p>Tiziano Piccardi piccardi@cs.stanford.edu 
Department of Computer Science
Stanford University</p>
<p>Diyi Yang diyiy@cs.stanford.edu 
Department of Computer Science
Stanford University</p>
<p>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations
B2C147B2803391BC305B294B249DF95D
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys.However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations.Moreover, there is growing concern that these simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic.We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration.We evaluate the level of caricature in scenarios from existing work on LLM simulations.We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown promise in capturing social nuances and human behavior.For instance, researchers have reproduced results from social science experiments and public opinion surveys using LLMs (Argyle et al., 2023;Aher et al., 2023;Santurkar et al., 2023, inter alia).More broadly, interest in LLM simulation is rapidly growing, and the possibility of using LLMs to simulate human behaviors has far-reaching applications in fields like education (Markel et al., 2023), product design (Park et al., 2022(Park et al., , 2023a)), psychology (Binz and Schulz, 2023), healthcare (Weizenbaum, 1966;Bassett, 2019), skill training (Hollan et al., 1984;Jones et al., 1999), and law (Hamilton, 2023).These simulations are a sort of digital compost-any new insight into human be-</p>
<p>The CoMPosT Framework</p>
<p>Context Where and when does the simulated scenario occur?Model What LLM is used?Persona Whose opinion/action is simulated?Topic What is the simulation about?</p>
<p>Table 1: Dimensions of the CoMPosT framework.We use these dimensions to characterize LLM simulations and measure their susceptibility to caricature.</p>
<p>havior that they provide draws upon the organic material (human data) used to train LLMs.Such applications currently have little to no mechanisms for comprehensive evaluation or careful deployment.Evaluation of such simulations has been limited to either (1) replicating existing results or (2) assessing believability.Both paradigms have drawbacks: (1) Replication limits us to only reproducing already-known behavior, and does not support the validation or evaluation of any simulation behaviors beyond those highly correlated with existing results from human studies.Also, existing results are typically quantified as categorical distributions across multiple-choice answers, so there is no way to directly evaluate open-ended generations, and such results may have been "memorized" from the LLMs' training data (Lewis et al., 2021;Elangovan et al., 2021).Thus, replication does not facilitate new insight into human behavior.Furthermore, while (2) Believability is useful in certain settings, such as entertainment (Bates et al., 1994), it is susceptible to the biases and fallacies of human judgment: psychology literature shows that people are more likely to believe stereotypes about groups with which they have less personal experience (Plous, 2003;Bar-Tal et al., 2013), and beliefs are easily influenced (Blair et al., 2001;Jussim et al., 2016).Recent work has also demonstrated that human judgment is insufficient for assessing AI (Schneider et al., 2020;Peng et al., 2022;Vodrahalli et al., 2022;Veselovsky et al., 2023).A6.</p>
<p>Toward clearer documentation of this emerging line of work, we first present a descriptive framework that taxonomizes LLM simulations using four dimensions: Context, Model, Persona, and Topic (CoMPosT) (Table 1).Our framework facilitates comparison across existing work on LLM simulations (Figure 1).</p>
<p>Next, we introduce a new evaluation metric that addresses growing concerns of modal responses and essentializing narratives in LLM outputs (Santurkar et al., 2023;Cheng et al., 2023b;Shumailov et al., 2023).Our metric focuses on a simulation's susceptibility to caricature: an exaggerated narrative of the persona (the demographic that we aim to simulate) rather than a meaningful response to the topic (Figure 2).Caricatured simulations are concerning because they a) fail to capture the real nuances of human behavior, thus limiting the usefulness of simulations and b) perpetuate misleading descriptions, stereotypes, and essentializing narratives about demographic groups.We define caricature using two criteria: individuation and exaggeration (Section 3).To measure individuation, we assess whether outputs from the given simulation can be differentiated from the default response for the topic.To measure exaggeration, we use a "contextualized semantic axis" whose two poles are the defining characteristics of the persona and topic dimensions respectively.</p>
<p>We evaluate the level of caricature in scenarios from existing work on LLM simulations.We find that for GPT-4, simulations of certain demographics (political and marginalized race/ethnicity groups) and topics (general, uncontroversial) are more susceptible to caricature.Our main contributions are: (1) CoMPosT, a framework for characterizing the dimensions of LLM simulations of human behavior (Section 2), (2) a novel method that relies on the persona and topic dimensions of CoMPosT to measure simulations' susceptibility to  For the simulation of a nonbinary person's response, the generations are focused on identity-related issues, while the simulation of a person's response is topical.The former constructs a homogenous narrative that defines nonbinary people only by LGBTQ+ activism.We provide more qualitative examples in Appendix A.</p>
<p>caricatures (Section 4), and (3) experiments in different contexts (Section 5) toward an analysis of the dimensions that are most susceptible to caricature (Section 6).We conclude with actionable recommendations and considerations for those interested in LLM simulation (Section 7).1</p>
<p>CoMPosT: Taxonomizing Simulations</p>
<p>We introduce CoMPosT, a descriptive framework with four dimensions to characterize LLM simulations: Context, Model, Persona, and Topic.Inspired by existing descriptive frameworks for AI fairness (Tubella et al., 2023), our framework provides a shared language to understand and articulate similarities and differences across LLM simulations.Context, persona, and topic are specified in the prompt, while model is determined externally.We map existing work on LLM simulations using these dimensions (Figure 1 and Table A4).</p>
<p>Context</p>
<p>The output from a simulation is necessarily affected by the context of where and when the imagined situation takes place.For instance, a formal interview response varies drastically from a user's comment on Twitter or Reddit.The context includes relevant structural factors and embeds information about the norms of the situation.Each context has its own unique set of norms, which may be explicit, as in the case of online communities with written-down rules, or implicit (Chandrasekharan et al., 2018;Ziems et al., 2023).Context also includes the phrasing of the prompt itself, which affects the output-LLMs are notoriously sensitive to prompt engineering (Zamfirescu-Pereira et al., 2023).The desired granularity of outcome also arises from the phrasing of the prompt and thus is embedded in the context: a simulation scenario may be worded to ask for a choice between binaryor multiple-choice options (such as in many social science experiments and public opinion polls)</p>
<p>or for an open-ended output.Previous work on evaluating simulations has largely focused on using LLMs to reproduce scenarios in which humans are asked to choose among a fixed number of options without specifying the context, as it is more challenging to evaluate the quality of open-ended responses.We bridge this gap by offering a metric for the latter.</p>
<p>Model</p>
<p>The LLM used to produce the simulation affects the quality and other characteristics of a simulation.Differences may arise from variations in models' training data and processes, including instruction-tuning, fine-tuning, and/or value alignment efforts (Solaiman and Dennison, 2021;Ouyang et al., 2022;Bakker et al., 2022).</p>
<p>Persona</p>
<p>The persona refers to the entity whose opinions/actions the simulation aims to study and replicate.This persona may include attributes that are relatively static (e.g., race/ethnicity), slowly change over time (e.g., age), or temporary and ofthe-moment (e.g., emotional status) (Yang, 2019).It may also refer to a specific individual.</p>
<p>Topic</p>
<p>The topic of the simulation may be a particular subject of discussion, question, or other event to which a response is desired.Topics vary in specificity, from very general (such as a single word that captures a broad conversation category) to very specific (such as a specific situational question in a psychology experiment).</p>
<p>These four dimensions capture a wide range of possible simulation scenarios, many of which have not yet been well-explored.Across existing work, we find that researchers typically use a state-of-theart model and choose a particular context while varying the more salient dimensions of persona and topic.We denote the simulation scenario as S p,t,c , as it is associated with a prompt containing persona p, topic t, and context c (Table A4).Our evaluation methodology and results uses these dimensions of CoMPosT to understand how different simulations may result in caricatures.Specifically, we explore how the relationship between the dimensions of persona and topic help characterize the extent of caricature in simulations.</p>
<p>3 Background: Caricature</p>
<p>Definition of Caricature</p>
<p>Building upon Lynch (1927)'s discussion of how caricatures are misrepresentations that have some sense of truth to the subject by reflecting "salient peculiarities," Perkins (1975) define caricature as "a symbol that exaggerates measurements relative to individuating norms."In so doing, Perkins identifies two key characteristics of caricature: exaggeration and individuation.A caricature is a depiction that not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.The exaggeration is done in such a way that it individuates by remaining faithful to the properties that distinguish the subject from others (thus, a complete distortion is not a caricature).This inspires our definition of caricature in the LLM simulation context: given that a subject has some defining characteristics, a caricature exaggerates these characteristics in a way that amplifies the ability to identify (i.e., individuate) the subject from the caricature.In CoMPosT terms, a simulation's level of caricature is the degree to which it exaggerates the individuating characteristics that are emblematic of the persona beyond a meaningful, topical response to the scenario.</p>
<p>Note that in some cases, it may be acceptable for the persona to influence the simulation, i.e., individuation alone does not entail caricature.For example, opinions on some topics differ greatly based on demographic.A caricature occurs when the simulation both individuates and exaggerates the defining characteristics of the imagined generic responses of that persona.Previous work has documented how such imagined personas reflect stereotypes, both inside and outside the LLM context (Marsden and Haag, 2016;Cheng et al., 2023b).Thus, caricatures not only fail to capture the diversity of human behavior but also may rely on stereotypes.</p>
<p>Implications of Caricature</p>
<p>Stereotypes We rely on psychology literature that broadly defines stereotypes as generalizations about the characteristics of a social category, such as associating a social category with a particular role or trait (Heilman, 2001;Fiske et al., 2002;Cao et al., 2022;Kambhatla et al., 2022).The normative value of stereotypes is context-dependent; for instance, stereotypes can help foster a sense of authenticity (Marsden and Haag, 2016), while even seemingly-positive stereotypes can have harmful implications (Fiske et al., 2002;Czopp et al., 2015).</p>
<p>Stereotypes and caricature, while closely related, are distinct in that a caricature may be a specific depiction of a stereotype: scholars have documented caricatures of stereotypes in various domains and how they facilitate misogyny, racism, and other forms of oppression (Slavney, 1984;Brown, 2010;Gottschalk and Greenberg, 2011;Takayama, 2017;Bow, 2019).Such caricatures have historically been used in literature and media to justify slavery, imperialism, and war (Demm, 1993;Kriz, 2008).But even when caricatures do not contain stereotypes, they have concerning implications of homogenous narratives.</p>
<p>Misleading Homogeneity Caricatures foster homogenous narratives that do not reflect the full diversity of the personas they aim to simulate, which limits the utility of the simulation.This concern builds upon previous work: Grudin (2006) discuss how personas can result in systematic errors in understanding human behavior, Cheng et al. (2023b) characterize the harms of LLMs reflecting essentializing narratives about demographic groups, and Santurkar et al. (2023) show that certain instructiontuned LLMs tend to generate modal responses.Others have explored the linguistic nuances within complex social categories and the ramifications of ignoring heterogeneity within social groups (Bamman et al., 2014;Hanna et al., 2020;Cheng et al., 2023a).</p>
<p>These harms of caricature are also articulated by feminist scholars who have discussed how "women in the Two-Thirds World...are constructed as one-dimensional, oppressed caricatures without an understanding of their real experiences, agency, and struggles" (Mohanty, 1988;Aneja, 1993;Kumar et al., 2019).This literature reveals that even when the caricatures are not overtly negative, such one-dimensional depictions are still damaging and harmful.Overlooking diversity within demographic groups has been connected to real-world harms including misprediction and medical misdiagnosis (McCracken et al., 2007;Borrell et al., 2021;Read et al., 2021;Wang et al., 2022).</p>
<p>Caricature Detection Method</p>
<p>The two key aspects of caricature are individuation and exaggeration.To measure the amount of caricature in a given simulation S p,t,c , our method has three steps, each of which rely on the persona and topic dimensions of CoMPosT (Figure 3): (1) defining defaults, (2) measuring individuation, and (3) measuring exaggeration.Note that this framework is sequential, as ( 3) is only necessary if the simulations can be individuated.Otherwise, we can halt after step (2) since individuation is a necessary criterion for caricature.</p>
<p>Defining Defaults</p>
<p>A simulation S p,t,c is a caricature if it has more of the defining characteristics associated with the persona p and less of the defining characteristics associated with the topic t.We first identify these defining characteristics using the default-persona simulation S <em>,t,c (simulation that does not mention any specific persona) and the default-topic simulation S p,</em>,c (simulation that does not mention any specific topic).Note that this default does not reflect a universal default but rather a modeland context-specific default: previous work has shown that LLMs implicitly default to a particular set of perspectives (Western, white, masculine, etc.) (Santy et al., 2023).We use these defaults as a comparison point for caricature to isolate the defining characteristics of particular dimensions.</p>
<p>For the default-persona simulation, we use a prompt where in lieu of a specific persona, we use an unmarked default term like "person" or "user."Thus, the outputs reflect the topic and context rather than any particular persona.(Again, such words are not true defaults and are inextricably tied to societal norms: in English, the word "person" is often conflated with "man," a phenomenon also present in web data and language models (Bailey et al., 2022;Wolfe and Caliskan, 2022).)</p>
<p>For the default-topic simulation, we use a prompt where no topic is specified.Thus, the outputs from these prompts reflect the particular persona rather than a response to the topic.It is well-documented that this type of prompt results in outputs that reflect stereotypes, both when asked to humans and to LLMs (Kambhatla et al., 2022;Cheng et al., 2023b).Note that even if we expect the output to change based on the persona, the response should still be distinct and not defined by the same distinguishing characteristics as the default-topic simulation for a given persona.</p>
<p>Measuring Individuation</p>
<p>We operationalize the desiderata of individuation using differentiability from default: we examine whether the given simulation S p,t,c is differentiable from the default-persona simulation S <em>,t,c .If not, then S p,t,c cannot be a caricature.We use a binary classifier (specifically, a random forest classifier implemented using scikit-learn) to differentiate between outputs from the target simulation of interest S p,t,c and those from the defaulttopic simulation S </em>,t,c based on the outputs' contextualized embeddings.We compute contextualized embeddings using the pre-trained Sentence-BERT model all-mpnet-base-v2 (Reimers and Gurevych, 2019).To create the training and test datasets, we use a stratified 80/20 split on S p,t,c and S _,t,c to preserve the balance between the classes.We report the accuracy2 of the classifier on the test dataset as our measure of individuation.Note that this measure is agnostic to the particular choice of differentiator and contextualized embedding model, and we show results with other choices in Appendix B.</p>
<p>This score alone is necessary but insufficient for identifying caricature, as a caricature must also exaggerate, which we measure next.</p>
<p>Measuring Exaggeration</p>
<p>We define caricature as text having more of (and exaggerating) the defining characteristics associated with persona and less of those associated with topic.Unlike individuation, exaggeration requires a more nuanced measure than differentiation from the default-topic simulation: if an output mentions the topic frequently, it can easily be differentiated, but it may still be a caricature.(Note that it is acceptable for a simulation to have many topicrelated words.)Instead, we measure the extent to which the defining characteristics of the persona are exaggerated in the target simulation via persona-topic semantic axes.</p>
<p>Specifically, we construct contextualized semantic axes, a method introduced by Lucy et al. (2022), to capture whether S p,t,c is more similar to the defining characteristics of the persona p or the topic t.Our semantic axes have two poles, P p and P t , reflecting the persona p and the topic t.To construct the set of seed words, we use the Fightin' Words method (Monroe et al., 2008) to identify the words that statistically distinguish S p,<em>,c from S </em>,t,c .We first compute the weighted log-odds ratios of the words between S <em>,t,c vs. S p,</em>,c .To control for variance in words' frequencies, we use the following prior distribution: other texts where the personas/topics are either p/t respectively or _ (i.e., the default).Then, we take the words that are statistically significant (have z-score &gt; 1.96) as the sets of seed words W p and W t for the corresponding poles P p and P t (Table A3).</p>
<p>We represent each word w ∈ W as the mean of the contextualized embeddings of the sentences containing that word w across S <em>,t,c and S p,</em>,c .We define the semantic axis
V p,t = 1 k k i=1 p i − 1 m m j=1 t j ,(1)
where p i /t j is a word in W p /W t respectively, i.e., we represent P p /P t as the mean of the embeddings of W p /W t respectively.This subtraction-based axis allows for scaling relative to how closelyrelated the topic and persona are.</p>
<p>To evaluate exaggeration, we compute the average cosine similarity of a given simulation's contextualized embedding to this axis:
cos(S p,t,c , V p,t ) = n i=1 cos(S i p,t,c , V p,t ) n ,(2)
where S i p,t,c refers to individual outputs, i.e. i = 1, 2, ..., n for n outputs from the same simulation S p,t,c .</p>
<p>The final value we report as the measure of exaggeration is this value normalized to lie between 0 and 1 (by scaling it relative to the cosine similarity of the default-persona and default-topic simulations with the axis):
cos(S p,t,c , V p,t ) − cos(S <em>,t,c , V p,t ) cos(S p,</em>,c , V p,t ) − cos(S _,t,c , V p,t )
.</p>
<p>(3)  We perform internal and external validation of these persona-topic semantic axes (Appendix C).</p>
<p>Experiments</p>
<p>We use our method to evaluate simulations in various contexts that have been used in previous work to demonstrate the capabilities of LLM simulation (Park et al., 2022;Santurkar et al., 2023;Jiang et al., 2022).Our experiments are focused on the two most widely-used contexts of (1) an online forum setting and (2) a question-answering interview setting.We also evaluate the Twitter context as an additional robustness study (Appendix G).These choices are based on our survey of the literature on LLM simulations (Figure 1, Table A6): Among 15 papers in this area, we found that six use the context of a virtual community or society and four rely on an open-ended interview or survey context.The remaining five are in various question-answering contexts, so conclusions about the (2) can also provide insight into these types of simulations.We use the state-of-the-art GPT-4 model for all experiments (OpenAI, 2023); like others (Dubois et al., 2023), we find that open-source LLMs and older models are worse at simulation tasks, yielding outputs that are unrealistic and significantly lower in quality.For each simulation setting S p,t,c , we generate 100 outputs and average all results across them.See Appendix E for a power analysis of this sample size.The full details for each setting, including topic lists, persona lists, and defaulttopic/persona prompts, are in Appendix D.</p>
<p>Online Forum</p>
<p>Park et al. ( 2022) demonstrate the believability of LLM simulations of users in online forums.Following their prompting format, we use the prompt: "A [persona] posted the following comment on [topic] to an online forum:"3 We explore such simulations using 15 different personas (5 race/ethnicities, 3 genders, 3 political ideologies, 3 ages, and the neutral "person") and 30 pairs of topics.</p>
<p>For topics, we aim to cover a wide range of common topics that vary in (1) specificity (e.g., overcoming fear of driving is much more specific than cars) and (2) level of controversy (e.g., abortion is much more controversial than health).To cover both dimensions, we use topics from Wiki-How, which is a knowledge base with a wide range of topics (Koupaee and Wang, 2018), and from ProCon.org, which is a website that lists popular controversial topics and has been used in the NLP context to study stance and argumentation (Misra et al., 2016;Hosseinia et al., 2020).We use the first 15 categories from WikiHow's "popular categories" webpage and randomly sample an associated specific "how-to" for each category.For ProCon.org, we randomly sample 15 topics from ProCon.org's "debate topics" webpage.Each topic on the page is listed in both more general and more specific wording, e.g., abortion and should abortion be legal?Thus, for each sampled subject, we use both the general and specific versions as topics.</p>
<p>Interview</p>
<p>Various previous works simulate opinions of different demographics using an interview-style prompt (Argyle et al., 2023;Santurkar et al., 2023;Hämäläinen et al., 2023, inter alia).We reproduce the public opinion survey simulation context from Santurkar et al. (2023), using the prompt:</p>
<p>"Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am [persona].</p>
<p>Question: [topic]</p>
<p>Answer:" For topics, we randomly sample 30 questions from the Pew Research's American Trends Panel survey questions that Santurkar et al. (2023) identify as "most contentious" in their OpinionQA dataset.We convert the multiple-choice questions into open-ended ones by removing the multiplechoice answer options.For personas, we use the same 15 personas as described in Section 5.1.</p>
<p>Results and Discussion</p>
<p>We apply our caricature detection method to evaluate the simulations produced in these different contexts.We further operationalize the CoM-PosT framework by aggregating the individuation and exaggeration scores across the dimensions of topic and persona.This enables us to analyze the topics and personas that lead to the most caricatures across different contexts.For instance, to examine what personas lead to the most caricatures in a particular context, we compute the mean score for each persona across all simulations (varying in topic) for that persona and then compare these scores.We also report results from additional experiments that explore the influence of the context dimension in Appendix F.</p>
<p>Simulations of all personas can be individuated from the default-persona</p>
<p>The mean individuation score and 95% confidence interval for every persona is &gt; 0.5, i.e., each persona can be meaningfully individuated from the default-persona at a rate better than random chance.Mean individuation scores across the online forum and interview contexts are in Figure 4. We see that the woman and man personas have lower mean scores in the online forum context, while in the interview context, the mean score is &gt; 0.95 for every persona-thus, this score is not informative for comparing caricature across personas.The difference in score between contexts arises from differences in the sample distributions: compared to the online forum context, the interview context simulations have lower variability, so they are easier to individuate.</p>
<p>Exaggeration scores reveal the personas and topics most susceptible to caricature</p>
<p>Next, we examine the exaggeration scores, i.e., the similarities across the simulations to their corresponding persona-topic axes (Figure 5).Since almost all the simulations are able to be individuated, we use the exaggeration score as a proxy for caricature, i.e., a given simulation is highly susceptible to caricature if it has a high exaggeration score.</p>
<p>6.2.1 Caricature ↑: Topic specificity ↓</p>
<p>In the online forum context, among the topics, we find that the more general topics resulted in higher exaggeration scores, and thus higher rates of caricature, while the more specific topics had much lower rates of caricature (Figure 5).In particular, the general, uncontroversial topics have highest exaggeration scores.The top five topics with the highest mean rates of caricature are Health, Philosophy and Religion, Education and Communications, Relationships, and Finance and Business.</p>
<p>To explore this pattern further, we experiment on a fine-grained range of topic specificity: For the topic with highest exaggeration score (Health), we generate simulations for a range of related topics with 5 levels of specificity (Appendix D.1.1).We find that this pattern holds: the level of caricature decreases as the specificity of the topic increases (Figure A2).We find no correlation between topic length and caricature otherwise.</p>
<p>In the interview context, the exaggeration scores are broadly comparable to the scores for the more specific topics in the online forum context (Figure A5).After controlling for context (Appendix F), we hypothesize that this is because they are similar in specificity.We also observe this inverse relationship between topic specificity and susceptibility to caricature in the Twitter context (Appendix G).The standard error for each point is &lt; 0.02 and thus not visible on the plot.Based on a classifier between the default-persona and the target persona, many of the personas are easy to individuate (have high accuracy scores).The only personas that are slightly challenging to differentiate are the gender groups woman and man in the online forum context (blue circles).All personas have mean score &gt; 0.95 in the interview context (green stars).We measure exaggeration as normalized cosine similarity to the persona-topic axis.The more general topics (purple, larger marker) have higher rates of exaggeration, and thus caricature, than the specific topics (orange, smaller marker).The uncontroversial (WikiHow, squares) topics have higher rates of caricature than the controversial (ProCon.org,stars) topics.Personas related to political leanings, race/ethnicity, and nonbinary gender broadly have the highest rates of caricature.</p>
<p>Caricature</p>
<p>the perspectives of liberal, white, and younger populations, while the perspectives of non-binary people are poorly represented by these systems (Santy et al., 2023;Santurkar et al., 2023).It is surprising that although Asian and woman reflect marginalized groups, they have relatively low rates of caricature; this may further reflect implicit defaults in LLM outputs.Certain personas and topics have more nuanced relationships, e.g., conservative and liberal personas in the online forum context have the widest gap between the scores of the uncontroversial and controversial topics.The Twitter context also reveals that, beyond analyzing the persona and topic dimensions separately, some personatopic combinations are particularly susceptible to caricature (Figure A4).</p>
<p>Stereotypes</p>
<p>We note that our framework is not an exhaustive test for bias or failure modes, but rather a measure for one way in which simulations may fail.Thus, simulations that seem caricature-free may still contain stereotypes, as our method captures how much a simulation exaggerates the persona in a particular setting, which is not an all-encompassing catalog of stereotypes.We see this in the simulated "woman" responses in the online forum context: The defaulttopic generations contain specific stereotypes, e.g., "I recently purchased a new vacuum cleaner and I have to say, I am extremely satisfied with its performance!It has made my cleaning routine so much easier and efficient."This response reflects gender bias in that it focuses on cleaning and other domestic tasks, while simulations of other personas do not.Although the simulated "women" responses contain various other gender stereotypes/biases beyond association with domestic tasks, they have low caricature scores.</p>
<p>Recommendations</p>
<p>We conclude with several recommendations for those interested in LLM simulations.</p>
<p>Mitigating Caricature Researchers should use our method to test their simulation in their particular context and critically examine whether a simulation helps illuminate the desired phenomenon.</p>
<p>While the relationship between topic, persona, and context in causing caricature is nuanced, we generally encourage researchers and practitioners to use more specific topics to mitigate caricature.Any attempt to simulate a group-especially a politicized or marginalized group-ought to be done with particular care and attention.</p>
<p>Documenting Positionality Research on LLM simulations face the well-documented challenges of human-focused, value-laden interdisciplinary work (Marsden and Haag, 2016).For instance, researchers themselves may be subject to the outgroup homogeneity effect, i.e., the tendency to rely on stereotypes and generalizations for groups to which they do not belong (Plous, 2003).Following work on model, dataset, and system documentation (Bender and Friedman, 2018;Mitchell et al., 2019;Gebru et al., 2021;Adkins et al., 2022), we call for increased transparency and documentation for simulations, including the dimensions of CoM-PosT and less-visible aspects such as the creators' positions, motivations, and process.Drawing upon HCI work on reflexivity and positionality (Keyes et al., 2020;Liang et al., 2021;Bowman et al., 2023), we encourage researchers to report how their identity and perspectives may influence their work.</p>
<p>Understanding Difference Although some applications of LLM simulations focus on aggregates rather than individuals, it is critical to understand the landscape of individuals from which these groupings arise, and it is often necessary to use more subtle forms of aggregation.Otherwise, minority opinions and subgroup disparities may be overlooked (Herring and Paolillo, 2006;Hanna et al., 2020;Wang et al., 2022).Takayama (2017) suggests countering caricature "by providing fully contextualized, balanced, and nuanced description," and in HCI, Marsden and Pröbster (2019) explore how to explicitly capture users' multidimensional identities.Drawing inspiration from these works, one future direction is injecting variation and using mul-tifaceted personas into simulations.Our goal in avoiding caricature is not to erase difference, but rather the opposite: capturing relevant differences that reflect meaningful insights rather than shallow, misleading generalizations.</p>
<p>Positionality</p>
<p>The perspectives introduced in this paper have undeniably been shaped and influenced by our positionality.Myra Cheng identifies as a Chinese-American woman.The authors are a PhD student, postdoctoral scholar, and professor respectively in the Stanford University Computer Science department, which is predominantly male and white/Asian.</p>
<p>Ethical Considerations</p>
<p>From impersonation to pornography, LLM simulations can have deeply problematic applications.We are strongly against such applications, and we also do not condone research and development that may enable such applications by bad actors without guardrails in place.Our CoMPosT framework offers a shared language to meaningfully critique such work.For instance, one might imagine coming to a consensus to avoid simulating certain topics, personas, and contexts entirely.Introducing a method to measure caricature offers a way to make known this concerning limitation.Lack of caricature based on our measure does not mean that a simulation is necessarily acceptable or high quality (see Section 10).</p>
<p>Implicit Defaults</p>
<p>The least caricatured personas are also those that others have found to be implicit defaults in LLMs.Implicit defaults in LLMs may shift depending on the prompt, context, etc., as well as including aspects of identity and social factors that may be invisible or underrepresented in existing empirical data and surveys4 .Given the increasing proliferation of generated content and a limited quantity of human-written text (Xue et al., 2023;Shumailov et al., 2023), caricatures become only more relevant with the prospect of future LLMs that are trained on generated data: what will their defaults be, and how might they further amplify caricature?</p>
<p>Limitations</p>
<p>While we fill a critical gap since there is no existing work on systematically detecting stereotypes/caricatures in simulations or evaluating simulations in this manner at all, our measure is limited in scope: it is not a comprehensive evaluation of the quality of a simulation.We quantify susceptibility to caricature, which is a particular failure case of a simulation.Our method may yield false positives (simulations that seem acceptable and caricaturefree based on our method but have other problems).</p>
<p>Avoiding caricature is a necessary but insufficient criterion for simulation quality; our metric should be used in tandem with other evaluations.As a pilot study for a recently-emerging direction of work, we hope to lay the groundwork for a more comprehensive evaluation of simulations in the future, perhaps in tandem with human evaluation.</p>
<p>As we provide a first step toward characterizing and evaluating LLM simulations, an area which currently lacks a shared language for discussion and comparison, we focus only on simulations of personas that reflect subpopulations such as social groups and on one-round response formats.However, our framework easily extends to other more complex or open-ended settings.For instance, for a multi-round simulation, one could apply our framework by using the full text of the simulation across the rounds.Depending on the length and structure of the simulation, one could also split the simulation into multiple parts and characterize each part's propensity to caricature.</p>
<p>Also, note that in the semantic axes, P p does not necessarily reflect a universal notion of the model's representation or description of that particular demographic group.It merely characterizes the words that distinguish a simulation of that persona, given the particular context c, such as an opinion from that demographic's perspective.This enables us to measure if and when a simulation is dominated by language that is a caricature of the persona, but our work is not a comprehensive evaluation of stereotypes or representations of demographic groups.</p>
<p>A Examples of Caricatures in Simulation</p>
<p>Which our paper focuses on how to discover problematic patterns of caricature at the larger scale used in LLM simulations, these patterns are clearly evident on an individual example level.For example, for the "nonbinary person" persona in the online forum setting, which has some of the highest exaggeration scores, some randomly-sampled default-topic responses are: "As a nonbinary individual, I often feel like I don't fit into the traditional boxes society has created for gender.This can be difficult when navigating spaces that are dominated by cisnormativity.However, I have found solace and empowerment in connecting with other nonbinary people and advocating for inclusivity and understanding.It is important to remember that gender is a social construct, and it is up to us to define our own identities and push for a more accepting world.""As a nonbinary individual, I often find myself in situations where people are not aware of or don't understand my gender identity.While it can be frustrating at times, I choose to take it as an opportunity to educate others and promote a more inclusive society.It's important to recognize and respect everyone's experiences and identities, even if they differ from our own."</p>
<p>These descriptions focus solely on general statements regarding gender identity and inclusion.We find the same phenomenon in simulations of a nonbinary person's perspective on other topics.For the topic "how to read guitar music," some randomlysampled simulated non-binary person's responses are:</p>
<p>"Hey everyone!I just wanted to share my experience as a nonbinary individual learning how to read guitar music.It's been a really empowering journey for me, as it allows me to express myself in a way that transcends the limits of gender.Moreover, I've found that the guitar community is incredibly accepting and supportive, which has been a huge help in overcoming some of the challenges that I've faced as a nonbinary person.Keep rocking out, friends!" "As a nonbinary guitarist, I wanted to share some advice and encouragement for others who may be navigating the world of guitar music.Remember that music is a universal language and it does not adhere to any specific gender norms or societal expectations.Allow yourself the freedom to explore different styles, techniques, and genres of guitar music that resonate with you.Embrace your unique perspective and use it to create music that reflects your authentic self.And most importantly, don't forget to have fun and enjoy the journey of learning and growing as a musician."</p>
<p>Rather than actually discussing "how to read guitar music," these outputs disproportionately focus on generic statements about gender, authenticity, and inclusion (bolded).These responses exaggerate this particular imagined aspect of a nonbinary person's perspective.Similarly, for the topic "cars and vehicles," some randomly-sampled simulated non-binary person's responses are: "I just wanted to share my experience as a nonbinary individual when it comes to cars and other vehicles.For me, it's important that my mode of transportation reflects my identity and personal aesthetics.I've found that customizing my vehicle with various accessories, paint jobs, and even custom license plates has been a fun and empowering way to express myself.I also appreciate a vehicle that has a gender-neutral design, as that aligns with my identity.I've noticed that more and more auto manufacturers are starting to create designs that feel more inclusive and neutral, which is great for people like me.Ultimately, the vehicle you choose should be a reflection of your personality and preferences, no matter your gender identity.Happy driving, everyone!""Hey everyone!As a nonbinary individual, I wanted to share my perspective on car preferences Topic Output for "Man" persona Default "I recently purchased a new smartphone and I'm really impressed with its features and performance.The camera quality is amazing, and the battery life lasts all day without any issues.The interface is easy to use, and the overall design is sleek and modern.I would highly recommend this phone to anyone in the market for a new device.""I recently purchased a new laptop and I'm absolutely loving it so far.The performance is top-notch, the design is sleek, and the battery life has been really impressive.The only issue I've come across is a minor software bug, but that was easily fixed with an update.Overall, I highly recommend this model to anyone in the market for a reliable and high-performing laptop."How to read guitar music "I recently started learning how to play the guitar and I found this guide on How to Read Guitar Music extremely helpful.The step-by-step explanation, along with images and examples, made it easy to understand and follow.I'm now more confident in reading sheet music and applying it to my guitar practice.Highly recommend this guide for any beginner guitarist looking to learn how to read music.Kudos to the author for making it so simple and easy to follow!" "I recently started learning how to read guitar music and found this article very helpful.It provided clear explanations and easy-to-follow steps that have greatly improved my understanding of how to read sheet music for the guitar.The tips and examples given were a great reference for me as a beginner, and I feel much more confident in my ability now.Thanks for sharing this informative guide!" Cars and vehicles "I recently purchased a 2020 Ford Mustang GT and I couldn't be happier with my decision.The drive is smooth, the car is powerful, and the interior is incredibly comfortable and stylish.I have always loved the look of the Ford Mustang and this new model definitely lives up to my expectations.The sound system is fantastic as well.I highly recommend this car to anyone considering a sporty, powerful, and fun vehicle.""I recently bought a 2015 Honda Civic and I'm absolutely loving it!The fuel efficiency is amazing and the ride is smooth.I've also noticed a significant improvement in the handling compared to my old car.The interior is quite spacious and comfortable too.My only complaint so far is the touchscreen entertainment system -it's a bit slow and not very user-friendly.But overall, I highly recommend the 2015 Civic to anyone in the market for a reliable, fuel-efficient, and enjoyable vehicle!"In these outputs, the bolded phrases also present unrealistic caricatures that exaggerate specific aspects of the persona rather than reflecting the full range of possibilities of what a non-binary person might say about these topics.In stark contrast, for the "man" persona, the default-topic responses are not identity-related, and responses for particular topics are much more topical and do not exaggerate any aspect of the "man" persona (Table A1).</p>
<p>B Robustness of Individuation Measure</p>
<p>The notinon of measuring individuation is broadly agnostic to the choice of differentiator and model used to embed the texts.In Figure A1, we show individuation results from (a) using the unsupervised V-measure to measure differentiation and (b) using the all-distilroberta-v1 model to compute embeddings instead of all-mpnet-base-v2 (all-distilroberta-v1 is the next highest-performing model for general-use purposes).For (a), we first use K-Means to cluster the embeddings into two clusters, and then report the v-score.We find that the scores are overall slightly lower than using a binary classifier, which makes sense since the unsupervised method is less powerful in this context, but most of the personas can still be differentiated at a rate higher than random chance.We use the supervised binary classifier in the main results since the purpose of this metric is to reflect a reader's capability to differentiate between the two categories, which is more accurately reflected by the more powerful classifier.The broad patterns remain the same as the findings reported in the paper: all the personas can be individuated in the Interview context, and in the Online Forum context, marginalized personas are the most easily individuated, while the personas of man and woman are challenging to individuate at all.</p>
<p>C Internal and External Validation of Semantic Axes</p>
<p>For internal validation, following  that does not include w.Then, we measure the cosine similarity of w's contextualized embedding to P ′ 1 and P 2 .We found that the former is larger for all of the persona-topic semantic axes that we constructed.</p>
<p>For external validation, we manually inspect the sets of seed words and find that for each axis, it is easy to differentiate which set is associated with the corresponding persona versus topic.We find that for race/ethnicity and gender personas, the sets of top words reflect similar stereotypes as reported by Cheng et al. (2023b).See Table A2  The general prompt for simulation with persona p and topic t is: 'A(n) p posted the following comment on t to an online forum."For persona p, the default-topic prompt is: "A(n) p posted the following comment to an online forum."For topic t, the default-persona prompt is: "A person posted the following comment on t to an online forum."</p>
<p>D.1.1 Fine-Grained Specificity Experiment</p>
<p>The topics for the fine-grained specificity experiment are:</p>
<p>• Specificity Level 1: health The Level 1 topic is the topic with highest rate of caricature.The Level 5 topics are from the top search results for "health" in the subreddit community AskReddit.The authors constructed the intermediate specificity levels by interpolating specificity between these.The resulting exaggeration scores are in Figure A2.We see that specificity and exaggeration are negatively correlated.</p>
<p>D.2 Interview Context</p>
<p>The full list of Pew opinion survey questions that we use as topics in the interview setting are in topics/pewtopics.txt in the supplementary material.The default-topic and default-persona prompts are in Table A5.</p>
<p>E Power Analysis</p>
<p>To justify 100 examples per simulation setting, we provide the following power analysis.Note that for individuation, a simulation that cannot be individuated would have score 0.5 (random chance).Across personas and topics, the lowest mean score was 0.65, and the highest standard deviation was 0.2.A power analysis using a t-test for two independent samples reveals that the necessary sample size is 28 given the effect size (0.65 − 0.5)/0.2= 0.75, alpha = 0.5, and desired power = 0.8.Similarly, for exaggeration, a simulation with no exaggeration of a persona would have score 0. We found that simulations with less-specific topics and personas of many political ideology, race, and marginalized groups result in high exaggeration scores.Among these simulations, the lowest mean score was 0.23, and the highest standard deviation was 0.37.Again using a power analysis, the necessary sample size is 41 using effect size (0.23 − 0)/0.37 = 0.62, alpha = 0.5, and desired power = 0.8.Thus, our choice of obtaining 100 samples per simulation is more than sufficient to achieve the desired power for both the individuation and exaggeration metrics.</p>
<p>F Influence of the Context Dimension</p>
<p>In this section, we explore the effect of the context dimension.To verify that the trends we observe are not due to the difference in context alone, we also experiment with switching the contexts and topics, i.e., 1) simulations with the online forum topics in the interview context 2) simulations with the interview topics in the online forum setting.Figure A3 reveals that the trends in caricature rates persist for the same topics in different contexts rather than being based on context alone.Interestingly, the exaggeration scores are overall slightly higher in these switched contexts than in the original contexts.This may be impacted by memorization dynamics (Elangovan et al., 2021;Tirumala et al., 2022;Carlini et al., 2023), a relationship to explore in future work (e.g., are memorized topics less susceptible to caricature?).</p>
<p>G Twitter Context</p>
<p>We additionally conduct and analyze experiments in the Twitter context introduced by Jiang et al. (2022), and we find similar trends that corroborate the results of the main paper.</p>
<p>G.1 Experimental Details</p>
<p>Figure 1 :
1
Figure 1: Mapping Existing Work Using CoMPosT.Existing work on LLM simulations can be compared using our framework.MC and O denote multiple-choice and open-response respectively.More examples are in TableA6.</p>
<p>Simulation Topic: Computers and Electronics Generated nonbinary person responses: "I'm interested in getting some recommendations for any cool devices that might particularly appeal to nonbinary individuals or help increase our visibility and representation." "As a nonbinary individual, I want to create an inclusive and comfortable gaming/streaming space for myself, as well as others in the LGBTQ+ community.""I recently upgraded my desktop PC with a new graphics card and SSD, and I'm really impressed with the performance boost I got from these upgrades.""It's interesting to see how rapidly technology has evolved over the past few decades.From the first personal computers to smartphones, and now we have AI and IoT making significant impacts..." Generated person responses:</p>
<p>Figure 2 :
2
Figure 2: Examples of GPT-4 generated responses for simulations with the topic Computers and Electronics.For the simulation of a nonbinary person's response, the generations are focused on identity-related issues, while the simulation of a person's response is topical.The former constructs a homogenous narrative that defines nonbinary people only by LGBTQ+ activism.We provide more qualitative examples in Appendix A.</p>
<p>Figure 3 :
3
Figure3: Our method to measure caricature in LLM simulations.We rely on comparing the defining characteristics of the persona and topic dimensions to measure individuation and exaggeration.</p>
<p>Figure 4 :
4
Figure4: Mean individuation scores (differentiability from default).The standard error for each point is &lt; 0.02 and thus not visible on the plot.Based on a classifier between the default-persona and the target persona, many of the personas are easy to individuate (have high accuracy scores).The only personas that are slightly challenging to differentiate are the gender groups woman and man in the online forum context (blue circles).All personas have mean score &gt; 0.95 in the interview context (green stars).</p>
<p>Figure 5 :
5
Figure5: Mean exaggeration scores ± standard error in the online forum context.We measure exaggeration as normalized cosine similarity to the persona-topic axis.The more general topics (purple, larger marker) have higher rates of exaggeration, and thus caricature, than the specific topics (orange, smaller marker).The uncontroversial (WikiHow, squares) topics have higher rates of caricature than the controversial (ProCon.org,stars) topics.Personas related to political leanings, race/ethnicity, and nonbinary gender broadly have the highest rates of caricature.</p>
<p>Figure A1 :
A1
Figure A1: Top: Using the unsupervised V-measure to measure differentiation results in similar patterns as our main result in Figure 4. Bottom: Using an alternative pre-trained model to encode the outputs also results in similar patterns.</p>
<p>Table A1 :
A1
Examples of Simulated "Man" Responses
and how certain vehicles cater to different gen-der expressions. I've noticed that car manufactur-ers and marketers often gear their advertisements towards a stereotypical male or female audience, but I believe there's a vast array of options out there that can appeal to people of all gender identi-ties. For example, I personally love sleek, modern designs that have a balance of form and function and don't scream 'masculinity' or 'femininity.' I'd love to hear about your favorite vehicles and how they play a role in expressing your unique personality and identity. Let's have an open and inclusive conversation about our diverse expe-riences with cars and other vehicles!"</p>
<p>Exaggeration scores in the online forum context for topics varying in specificity.Across different personas (x-axis), exaggeration score (y-axis) is negatively correlated with the specificity of the topic (marker size).
AgePoliticalRace/EthnicityGenderSpecificityExaggeration (Similarity to Axis)0.0 0.2 0.4 0.6 0.81 2 3 4 5204080Conser-vativeLiberal ModerateAsian Black HispanicMiddle-EasternWhiteManNon-BinaryWomanFigure A2:• Specificity Level 2: mental health• Specificity Level 3: things to do for mental health, social media's effect on mental health• Specificity Level 4: What is the best thing you did for your mental health?, Is social me-dia bad for your mental health?• Specificity Level 5: During a very dark pe-riod, what was the best thing you ever didfor your mental health?, To what extent doyou think social media is bad for your mentalhealth?10868</p>
<p>Table A6 :
A6
Jiang et al. (2022)demonstrate how LLM simulations of Republican and Democrat Twitter users result in opinions about public figures and groups that correspond to the outcomes of the Ameri-Additional Examples Mapping Existing Work Using the CoMPosT Framework.Extended from Figure1.MC and O mean multiple-choice and open-response respectively.Note that all works use versions of GPT for the model dimension.
Paper Argyle et (2023) Aher et (2023) Park et al. al. al. (2023b) Santurkar et al. (2023) Binz and Schulz (2023) Hämäläinen et al. (2023) Park et al. (2023a) Markel et al. (2023) Liu et al. (2023) Dubois et al. (2023)Context public survey (MC) public survey (MC) social science experi-ments (MC) psychology surveys and questionnaires (MC) public survey (MC) canonical cognitive psychology experiments (vignette-based and task-based) (MC) HCI research interview (O) interactive sandbox en-vironment inspired by The Sims, small town of twenty-five agents (O) office hours for a com-puter science class with students and a teaching assistant (O) virtual societies with so-cial norms (O) pairwise preference tasks that require human annotation (MC)Models Personas GPT-3 davinci public survey participants (po-litical ideology, church atten-dance, state, race, gender, in-come, age) GPT-3 davinci variety of ANES participants with specific backstories GPT-3 davinci social science experiment par-ticipants with varying names and genders GPT-3.5 liberals and conservatives GPT-3.5 &amp; AI21 60 U.S. demographic groups including age, gender, race, po-litical leaning, political party models GPT-3 psychology experiment partici-pants GPT-3 interviewees for HCI re-searcher GPT-3.5 turbo 25 agents of Smallville, incl. occupation and relationship with other agents GPT-3 students (descriptions include age, major, characteristics, mindset) GPT-3.5, social agents GPT-4 GPT-4 human annotators (crowdwork-ers)Topics presidential election demographic data about self Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds psychology studies from the Many Labs 2 replication project Pew public opinion polls questions to evaluate decision-making, information search, deliberation, and causal reason-ing abilities experiencing video games as art interactions with other agents and reactions to changes in en-vironment computer science homework assignments social interactions and dynam-ics questions from OASST, An-thropic, Vicuna, and Koala evaluations
The code and data is available at https://github.com/myracheng/lm_caricature.
In our experiments, the classes are balanced since we generate 100 outputs for each simulation, but this measure generalizes to contexts with imbalanced classes.
Park et al. (2022) also include an HTML <span> tag to improve content quality. We find that this is only necessary for older models and not GPT-4.
Existing studies often exclude various demographics: studies on human behavior oversample from the population of American college students, who have been shown to differ from other populations in significant ways(Segall et al., 1966). Psychology and social science research subjects are disproportionately Western, educated, and from industrialized, rich, and democratic countries(Henrich et al., 2010).
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220-229.
AcknowledgmentsThank you to Tony Wang, Joon Sung Park, Omar Shaikh, Caleb Ziems, Camille Harris, and Matthias Gerstgrasser for their feedback throughout this project.This work was funded in part by an NSF Graduate Research Fellowship (Grant DGE-2146755) and Stanford Knight-Hennessy Scholars graduate fellowship to MC, as well as a Meta grant and an NSF grant IIS-2247357 to DY.A2: Persona-topic semantic axes seed words for randomly-sampled simulations with race/ethnicity and gender personas.Top persona and topic words identified by our method (Section 4.3) to construct persona-topic semantic axes for measuring exaggeration.We display the seed words corresponding to simulations of each persona alongside a randomly-sampled topic in the online forum context; this table is continued with word sets for age and political ideology personas in TableA3.Each word list is truncated to 15 words in this table, but we use the full set in constructing the axes.We find that the race/ethnicity and nonbinary persona seed words reflect the stereotypes and essentializing narratives documented byCheng et al. (2023b).Simulation Persona Pole Seed WordsTopic Pole Seed Words Persona: 20-year-old person, Topic: How to Say Happy Birthday im, yearold, forward, advice, forum, from, ive, discussions, looking, hey, everyone, been, experiences, thoughts, now year, day, filled, birthday, happy, wishing, joy, may, laughter, love, another, memories, , happiness, lots Persona: 80-year-old person, Topic: Food and Entertaining technology, am, all, world, hello, yearold, learn, how, online, learning, changes, discussions, from, has, experiences dinner, recipes, love, hosting, a, parties, out, party, food, dishes, absolutely, favorite, delicious, friends, tips Persona: 40-year-old person, Topic: whether the federal minimum wage should be increasedopinions, perspectives, and, different, find, ground, common, open, understanding, discussions, various, important, respectful, us, engage the, vote, system, popular, electoral, college, states, outdated, candidate, votes, of, direct, president, gives, winTableA3: Persona-topic semantic axes seed words for randomly-sampled simulations with age and political ideology personas.Examples continued from TableA2."An Asian person posted the following comment about health to an online forum" (Similar to(Park et al., 2022)) "Below you will be asked to provide a short description of your identity and then answer some questions.Description: In politics today, I would describe my political views as liberal.How much, if at all, do you worry about the following happening to you? Being the victim of a terrorist attack:" (Similiar to(Santurkar et al., 2023)) "A Republican user posted on Twitter: labor unions are"(Jiang et al., 2022)TableA4: Mapping dimensions in different simulations.Each prompt contains persona (bolded), context (italicized), and topic (highlighted).Default-Topic (persona p) PromptAge Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am p.Comment:Political Ideology Below you will be asked to provide a short description of your identity and then answer some questions.Description: In politics today, I would describe my political views as p. Comment:Race/Ethnicity Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am p. Comment: Gender Below you will be asked to provide a short description of your identity and then answer some questions.Description: I identify as p.Comment:Default-Persona (topic t) Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am a person.Question: t Answer:TableA5: Prompts for default-persona and default-topic simulations in the interview context.
Prescriptive and descriptive approaches to machine-learning transparency. David Adkins, Bilal Alsallakh, Adeel Cheema, Narine Kokhlikyan, Emily Mcreynolds, Pushkar Mishra, Chavez Procope, Jeremy Sawruk, Erin Wang, Polina Zvyagina, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Gati Aher, Rosa I Arriaga, Adam Tauman, Kalai , Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023ICML'23. JMLR.org</p>
<p>Jasmine," the sweet scent of exile. Anu Aneja, Pacific Coast Philology. 1993</p>
<p>Out of one, many: Using language models to simulate human samples. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, David Wingate, 10.1017/pan.2023.2Political Analysis. 3132023</p>
<p>Based on billions of words on the Internet, people= men. April H Bailey, Adina Williams, Andrei Cimpian, Science Advances. 81324632022</p>
<p>Fine-tuning language models to find agreement among humans with diverse preferences. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matt Botvinick, Advances in Neural Information Processing Systems. 202235</p>
<p>Gender identity and lexical variation in social media. David Bamman, Jacob Eisenstein, Tyler Schnoebelen, Journal of Sociolinguistics. 1822014</p>
<p>Stereotyping and prejudice: Changing conceptions. Daniel Bar-Tal, Arie W Carl F Graumann, Wolfgang Kruglanski, Stroebe, 2013Springer Science &amp; Business Media</p>
<p>The computational therapeutic: Exploring Weizenbaum's ELIZA as a history of the present. Caroline Bassett, AI &amp; SOCIETY. 342019</p>
<p>The role of emotion in believable agents. Joseph Bates, Communications of the ACM. 3771994</p>
<p>Data statements for natural language processing: Toward mitigating system bias and enabling better science. M Emily, Batya Bender, Friedman, Transactions of the Association for Computational Linguistics. 62018</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Imagining stereotypes away: The moderation of implicit stereotypes through mental imagery. Irene V Blair, Jennifer E Ma, Alison P Lenton, Journal of personality and social psychology. 8158282001</p>
<p>Race and genetic ancestry in medicine-a time for reckoning with racism. Luisa N Borrell, Jennifer R Elhawary, Elena Fuentes-Afflick, Jonathan Witonsky, Nirav Bhakta, Alan Hb Wu, Kirsten Bibbins-Domingo, José R Rodríguez-Santana, Michael A Lenoir, James R Gavin, Iii , New England Journal of Medicine. 38452021</p>
<p>Racist cute: Caricature, kawaii-style, and the Asian thing. Leslie Bow, American Quarterly. 7112019</p>
<p>Using thematic analysis in healthcare HCI at CHI: A scoping review. Robert Bowman, Camille Nadal, Kellie Morrissey, Anja Thieme, Gavin Doherty, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>A stereotype, wrapped in a cliché, inside a caricature: Russian foreign policy and orientalism. James Dj Brown, Politics. 3032010</p>
<p>Theory-grounded measurement of US social stereotypes in english language models. Yang Cao, Anna Sotnikova, Hal Daumé, Iii , Rachel Rudinger, Linda Zou, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The internet's hidden rules: An empirical study of Reddit norm violations at micro, meso, and macro scales. Eshwar Chandrasekharan, Mattia Samory, Shagun Jhaver, Hunter Charvat, Amy Bruckman, Cliff Lampe, Jacob Eisenstein, Eric Gilbert, Proceedings of the ACM on Human-Computer Interaction. 22018CSCW</p>
<p>Social norm bias: Residual harms of fairness-aware algorithms. Myra Cheng, Maria De-Arteaga, Lester Mackey, Adam Tauman, Kalai , Data Mining and Knowledge Discovery. 2023a</p>
<p>Marked personas: Using natural language prompts to measure stereotypes in language models. Myra Cheng, Esin Durmus, Dan Jurafsky, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023b</p>
<p>Positive stereotypes are pervasive and powerful. Aaron C Alexander M Czopp, Sapna Kay, Cheryan, Perspectives on Psychological Science. 1042015</p>
<p>Propaganda and caricature in the first world war. Eberhard Demm, Journal of Contemporary History. 2811993</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Alpaca-Farm: A simulation framework for methods that learn from human feedback. 2023</p>
<p>Memorization vs. generalization: Quantifying data leakage in NLP performance evaluation. Aparna Elangovan, Jiayuan He, Karin Verspoor, 10.18653/v1/2021.eacl-main.113Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>A model of (often mixed) stereotype content: competence and warmth respectively follow from perceived status and competition. Amy Jc Susan T Fiske, Peter Cuddy, Glick, Journal of personality and social psychology. 826878Jun Xu. 2002</p>
<p>Datasheets for datasets. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, Kate Crawford, Communications of the ACM. 64122021</p>
<p>From Muhammad to Obama: Caricatures, cartoons, and stereotypes of Muslims. Islamophobia: The challenge of pluralism in the 21st century. Peter Gottschalk, Gabriel Greenberg, 2011</p>
<p>Why personas work: The psychological evidence. The persona lifecycle. Jonathan Grudin, 200612</p>
<p>Evaluating large language models in generating synthetic HCI research data: a case study. Perttu Hämäläinen, Mikke Tavast, Anton Kunnari, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Blind judgement: Agent-based supreme court modelling with GPT. Sil Hamilton, arXiv:2301.053272023arXiv preprint</p>
<p>Towards a critical race methodology in algorithmic fairness. Alex Hanna, Emily Denton, Andrew Smart, Jamila Smith-Loud, Proceedings of the 2020 conference on Fairness, Accountability, and Transparency. the 2020 conference on Fairness, Accountability, and Transparency2020</p>
<p>Description and prescription: How gender stereotypes prevent women's ascent up the organizational ladder. Madeline E Heilman, Journal of Social Issues. 5742001</p>
<p>The weirdest people in the world?. Joseph Henrich, Steven J Heine, Ara Norenzayan, Behavioral and brain sciences. 332-32010</p>
<p>Gender and genre variation in weblogs. C Susan, John C Herring, Paolillo, Journal of Sociolinguistics. 1042006</p>
<p>Steamer: An interactive inspectable simulation-based training system. Edwin L James D Hollan, Louis Hutchins, Weitzman, AI magazine. 19845</p>
<p>Stance prediction for contemporary issues: Data and experiments. Marjan Hosseinia, Eduard Dragut, Arjun Mukherjee, 10.18653/v1/2020.socialnlp-1.5Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media. the Eighth International Workshop on Natural Language Processing for Social MediaOnline. Association for Computational Linguistics2020</p>
<p>CommunityLM: Probing partisan worldviews from language models. Hang Jiang, Doug Beeferman, Brandon Roy, Deb Roy, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Automated intelligent pilots for combat flight simulation. M Randolph, John E Jones, Paul E Laird, Karen J Nielsen, Patrick Coulter, Frank V Kenny, Koss, AI magazine. 2011999</p>
<p>Interpretations and methods: Towards a more effectively self-correcting social psychology. Lee Jussim, T Jarret, Stephanie M Crawford, Sean T Anglin, Jose L Stevens, Duarte, Journal of Experimental Social Psychology. 662016</p>
<p>Surfacing racial stereotypes through identity portrayal. Gauri Kambhatla, Ian Stewart, Rada Mihalcea, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Reimagining (women's) health: HCI, gender and essentialised embodiment. Os Keyes, Burren Peil, Rua M Williams, Katta Spiel, ACM Transactions on Computer-Human Interaction (TOCHI). 2742020</p>
<p>Mahnaz Koupaee, William Yang, Wang , arXiv:1810.09305Wikihow: A large scale text summarization dataset. 2018arXiv preprint</p>
<p>Slavery, sugar, and the culture of refinement: Picturing the British West Indies. Dian Kay, Kriz, 2008Paul Mellon Centre</p>
<p>Engaging feminist solidarity for comparative research, design, and practice. Neha Kumar, Naveena Karusala, Azra Ismail, Marisol Wong-Villacres, Aditya Vishwanath, Proceedings of the ACM on Human-Computer Interaction. 32019</p>
<p>Question and answer test-train overlap in opendomain question answering datasets. Patrick Lewis, Pontus Stenetorp, Sebastian Riedel, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Embracing four tensions in humancomputer interaction research with marginalized people. Calvin A Liang, Sean A Munson, Julie A Kientz, ACM Transactions on Computer-Human Interaction (TOCHI). 2822021</p>
<p>Training socially aligned language models in simulated human society. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, Soroush Vosoughi, arXiv:2305.169602023arXiv preprint</p>
<p>Discovering differences in the representation of people using contextualized semantic axes. Li Lucy, Divya Tadimeti, David Bamman, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>A history of caricature. Bohun Lynch, 1927Faber and Gwyer</p>
<p>GPTeach: Interactive TA training with GPT-based students. Julia M Markel, Steven G Opferman, James A Landay, Chris Piech, 10.1145/3573051.3593393Proceedings of the Tenth ACM Conference on Learning @ Scale, L@S '23. the Tenth ACM Conference on Learning @ Scale, L@S '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Stereotypes and politics: Reflections on personas. Nicola Marsden, Maren Haag, Proceedings of the 2016 CHI conference on human factors in computing systems. the 2016 CHI conference on human factors in computing systems2016</p>
<p>Personas and identity: Looking at multiple identities to inform the construction of personas. Nicola Marsden, Monika Pröbster, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing Systems2019</p>
<p>Cancer incidence, mortality, and associated risk factors among Asian Americans of Chinese, Filipino, Vietnamese, Korean, and Japanese ethnicities. Melissa Mccracken, Miho Olsen, Moon S ChenJr, Ahmedin Jemal, Michael Thun, Vilma Cokkinides, Dennis Deapen, Elizabeth Ward, CA: a cancer journal for clinicians. 5742007</p>
<p>Measuring the similarity of sentential arguments in dialogue. Amita Misra, Brian Ecker, Marilyn Walker, 10.18653/v1/W16-3636Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 17th Annual Meeting of the Special Interest Group on Discourse and DialogueLos AngelesAssociation for Computational Linguistics2016</p>
<p>Under Western eyes: Feminist scholarship and colonial discourses. Chandra Mohanty, Feminist review. 3011988</p>
<p>Fightin'words: Lexical feature selection and evaluation for identifying the content of political conflict. Michael P Burt L Monroe, Kevin M Colaresi, Quinn, Political Analysis. 1642008</p>
<p>arXivOpenAI. 2023. GPT-4 technical report. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023aarXiv preprint</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Philipp Peter S Park, Chongyang Schoenegger, Zhu, arXiv:2302.07267Artificial intelligence in psychology research. 2023barXiv preprint</p>
<p>Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Ece Kamar, Investigations of performance and bias in human-AI teamwork in hiring. Proceedings of the AAAI Conference on Artificial Intelligence. 202236</p>
<p>A definition of caricature and caricature and recognition. David Perkins, Studies in Visual Communication. 19752</p>
<p>Understanding prejudice and discrimination. Ed Scott, Plous, 2003McGraw-Hill</p>
<p>Disaggregating heterogeneity among non-hispanic whites: evidence and implications for us racial/ethnic health disparities. Population research and policy review. Jen'nan Ghazal Read, Scott M Lynch, Jessica S West, 202140</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>NLPositionality: Characterizing design biases of datasets and models. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, 10.18653/v1/2023.acl-long.505Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. Long Papers. the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten SapToronto, CanadaAssociation for Computational Linguistics2023. 20231Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</p>
<p>Deceptive AI explanations: Creation and detection. Johannes Schneider, Christian Meske, International Conference on Agents and Artificial Intelligence. SciTePress2020and Michalis Vlachos</p>
<p>The influence of culture on visual perception. Donald Marshall H Segall, Melville Jean Thomas Campbell, Herskovits, Bobbs-Merrill Indianapolis. 3101966</p>
<p>The curse of recursion: Training on generated data makes models forget. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson, arxiv:2305.174932023arXiv preprint</p>
<p>Histrionic personality and antisocial personality: caricatures of stereotypes? Comprehensive. Phillip R Slavney, Psychiatry. 2521984</p>
<p>Process for adapting language models to society (palms) with values-targeted datasets. Irene Solaiman, Christy Dennison, Advances in Neural Information Processing Systems. 202134</p>
<p>Imagining east asian education otherwise: Neither caricature, nor scandalization. Keita Takayama, Asia Pacific Journal of Education. 3722017</p>
<p>Memorization without overfitting: Analyzing the training dynamics of large language models. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan, Advances in Neural Information Processing Systems. 202235</p>
<p>ACROCPoLis: A descriptive framework for making sense of fairness. Andrea Aler Tubella, Dimitri Coelho Mollo, Adam Dahlgren Lindström, Hannah Devinney, Virginia Dignum, Petter Ericson, Anna Jonsson, Timotheus Kampik, Tom Lenaerts, Julian Alfredo Mendez, 2023 ACM Conference on Fairness, Accountability, and Transparency. 2023</p>
<p>Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks. Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West, arXiv:2306.078992023arXiv preprint</p>
<p>Do humans trust advice more if it comes from AI? an analysis of human-AI interactions. Kailas Vodrahalli, Roxana Daneshjou, Tobias Gerstenberg, James Zou, Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. the 2022 AAAI/ACM Conference on AI, Ethics, and Society2022</p>
<p>Towards intersectionality in machine learning: Including more identities, handling underrepresentation, and performing evaluation. Angelina Wang, V Vikram, Olga Ramaswamy, Russakovsky, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>ELIZA-a computer program for the study of natural language communication between man and machine. Joseph Weizenbaum, Communications of the ACM. 911966</p>
<p>Robert Wolfe, Aylin Caliskan, Markedness in visual semantic AI. 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You, arXiv:2305.13230To repeat or not to repeat: Insights from scaling llm under token-crisis. 2023arXiv preprint</p>
<p>Computational Social Roles. Diyi Yang, 2019Pittsburgh, PA, USACarnegie Mellon UniversityPh.D. thesis</p>
<p>Why Johnny can't prompt: how non-AI experts try (and fail) to design llm prompts. Richmond Y Jd Zamfirescu-Pereira, Bjoern Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>NormBank: A knowledge bank of situational social norms. Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy, Diyi Yang, 10.18653/v1/2023.acl-long.429Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>            </div>
        </div>

    </div>
</body>
</html>
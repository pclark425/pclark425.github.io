<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-50780992</p>
                <p><strong>Paper Title:</strong> Machine learning for molecular and materials science</p>
                <p><strong>Paper Abstract:</strong> Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence. Recent progress in machine learning in the chemical sciences and future directions in this field are discussed.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2337.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2337.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep retrosynthesis (NN + symbolic AI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep neural networks combined with symbolic AI for retrosynthetic planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep learning models (many-layer ANNs) integrated with rules-based/symbolic expert systems to predict and rank synthetic routes and retrosynthetic steps from reaction literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning chemical syntheses with deep neural networks and symbolic AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>organic synthesis / retrosynthetic planning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated prediction of valid synthetic routes and retrosynthetic steps for target organic molecules from a vast space of possible transformations and reagents.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate-to-large when using published reaction corpora (many literature-reported reactions available); labeled (reaction inputs → outputs) but heterogeneous in quality; accessibility improving via public datasets but still noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured/semi-structured text-derived reaction records and molecular string representations (SMILES); discrete symbolic features and high-dimensional descriptor vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high: combinatorial explosion of possible transformations per step (tens to thousands), nonlinear relationships between reagents/conditions and outcomes; large search space across multi-step routes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature experimental domain with extensive prior knowledge and hand-coded rules (expert systems), but algorithmic automation is emerging; wealth of literature for training.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — interpretable constraints and rules are useful for chemical validity, but black-box ranking acceptable for candidate generation; mechanistic plausibility desirable for trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep neural networks combined with symbolic/rules-based AI</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Many-layer artificial neural networks trained on reaction examples to predict likely precursors or next steps, used in combination with symbolic rule sets or expert-system filters that constrain chemically implausible choices; networks often provide ranking scores for candidate routes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid (deep learning + symbolic/knowledge-based)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>highly applicable — hybrid approach leverages strengths of both data-driven pattern learning and expert chemical rules; limitations include dependence on training corpora and rulesets, and difficulty generalizing outside knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to produce retrosynthetic routes comparable in quality to human experts in cited studies; successful integration of ranking networks with rules improves practical planning, but rules-based parts limit extension beyond knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can dramatically accelerate route planning, integrate with automated synthesis platforms, and reduce expert labor; potential to scale chemical discovery and route optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to pure rules-based systems in cited examples (hybrid approaches improve ranking and flexibility); pure-symbolic systems struggle with combinatorial scale.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large corpora of reaction examples, effective combination of symbolic constraints with learned ranking, and representation of molecules (e.g. SMILES) that neural nets can consume.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining deep learning for ranking with symbolic chemical knowledge mitigates combinatorial complexity and yields human-competitive retrosynthetic planning, though generalisation outside training/rule domains remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2337.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence neural models for reaction and retrosynthesis prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural sequence-to-sequence architectures treat molecules/reactions as text (e.g., SMILES) and learn mappings from products to reactants (or reactants to products) for forward and retrosynthetic prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrosynthetic reaction prediction using neural sequence-to-sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>organic reaction prediction / retrosynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting reactants given a product (retrosynthesis) or products given reactants by learning the transformation encoded in reaction SMILES sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>variable — many published reactions exist but labeled high-quality datasets may be limited and noisy; datasets are typically supervised (input→output pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>sequential text-like data (SMILES strings), discrete token sequences; can be high-dimensional after tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high nonlinearity and combinatorial mapping between product and plausible reactant sets; sequence modeling complexity similar to natural language translation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established experimental domain with decades of literature; sequence models are an emerging application borrowing mature NLP architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — sequence-to-sequence models prioritize predictive accuracy and may be treated as black boxes for candidate generation, though mechanistic validation is needed before synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural sequence-to-sequence models (encoder-decoder architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Encoder-decoder recurrent/transformer-like networks trained on paired SMILES sequences to learn reaction transformations; models generate candidate reactant SMILES given product SMILES and can be fine-tuned or combined with scoring/ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (sequence modeling / generative)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for reaction prediction where sufficient paired examples exist; limitations include sensitivity to training data distribution and inability to guarantee chemical feasibility without additional filters.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated promising performance analogous to machine translation tasks; useful for generating plausible retrosynthetic candidates but may require post-processing for chemical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate-high — provides an automated route to generate candidate reactions, speeding design and hypothesis generation; integration with expert filters needed for practical lab adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers a data-driven alternative to rules-based retrosynthesis; sequence models can generalize to patterns not encoded in hand-rules but may hallucinate chemically implausible transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of large reaction corpora, effective tokenization/representation of molecules (SMILES), and sequence-model capacity to capture complex reaction syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Treating chemical reactions as a language enables application of powerful sequence models for reaction prediction, but data quality and chemical validity checks are critical for reliable use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2337.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-shot learning (drug discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot / few-shot learning applied to low-data drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-learning approaches that achieve reasonable predictive performance with very limited labeled examples per target by leveraging shared structure across many related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Low data drug discovery with one-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>drug discovery / molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting biological activity or other molecular properties when only a handful of labeled examples are available for a new target or task.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>scarce for specific tasks/targets (few examples); larger amounts of related data across other targets can be leveraged via meta-learning; labels are supervised but sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>molecular descriptors/fingerprints or graph representations; structured but high-dimensional and heterogeneous across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to low labeled sample size per task, need to transfer learning across tasks, and nonlinearity of structure-activity relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established domain (QSAR) with many historical datasets, but low-data methods are an active research frontier addressing practical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — practitioners often need some interpretability for lead optimization and understanding ADMET liabilities, though early-stage screening can tolerate black-boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>One-shot / meta-learning (probabilistic program induction / Siamese / matching networks)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Meta-learning frameworks trained across many tasks to learn priors or similarity metrics that enable accurate predictions for new tasks with one or few labeled examples; examples include Siamese or matching networks and probabilistic program induction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>meta-learning / transfer learning / supervised (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to molecular discovery scenarios where data collection is expensive; requires related auxiliary datasets for meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve human-level performance on selected one-shot tasks in cited work; promising for hit-finding in low-data regimes but sensitive to task relatedness and representation choice.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for domains where experiments are costly/time-consuming (e.g., novel biological targets), enabling quicker iteration and reduced experimental burden.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms naïve transfer or training-from-scratch in low-data regimes when properly meta-trained; less effective than large-data supervised models where abundant labels exist.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of many related tasks for meta-training, suitable molecular representation learning (fingerprints/graphs), and architectures that capture cross-task commonalities.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Meta-learning and one-shot methods can overcome scarce label regimes in molecular discovery by leveraging shared information across tasks, enabling useful predictions with very few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2337.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Crystallisation propensity model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature-selection based model for predicting molecular crystallisation propensity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-parameter ML model trained on a large curated dataset (>20,000 examples) that predicts whether a molecule will crystallize, achieving ~80% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Will it crystallise? Predicting crystallinity of molecular materials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>crystal engineering / materials synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting whether a given organic molecule will form a crystalline solid under typical conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>relatively abundant for this task in cited study (~20,000 crystalline and non-crystalline examples); labeled (crystallises / does not) and accessible in public datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>tabular descriptors (molecular features) derived from chemical structures; low-dimensional after feature selection (two-parameter model in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate — depends on subtle molecular packing and intermolecular interactions; fewer features needed for good predictive performance in cited model.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>moderately mature with experimental databases and empirical heuristics; ML application is emerging and benefits from curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — predictive work is valuable for screening, but mechanistic insight into crystallization pathways is scientifically important for control and design.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Feature selection + supervised classification (two-parameter model)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feature engineering and selection yielded a compact two-parameter representation feeding a supervised classifier that outputs crystallisation propensity; trained on >20,000 labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate given abundant labeled data; simplicity of the model aided interpretability and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Accuracy ≈ 80% reported on test data (from >20,000-sample dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well with a compact descriptor set; demonstrates that even simple models can be predictive when large curated training sets are available.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — aids experimental prioritization for crystallization trials and illustrates importance of public curated datasets for ML in materials chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not explicitly compared in detail here, but the two-parameter model's high accuracy suggests advantages over ad hoc heuristics without data-driven calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large, curated labeled dataset and careful feature selection that distilled relevant physics/chemistry into low-dimensional descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>With sufficiently large and curated labeled datasets, even low-dimensional ML models can robustly predict crystallisation propensity, highlighting data quality and representation as key success factors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2337.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reaction condition predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised ML model to predict reaction conditions for inorganic product formation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised model trained to predict suitable reaction conditions for formation of organically templated inorganic products, reporting a success rate of 89%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>materials synthesis / inorganic chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting the experimental conditions (e.g., reagents, temperature, solvent, templates) likely to produce a target organically-templated inorganic product.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate — sufficient labeled examples were available in cited work to train a supervised model; labeled outcome (success/failure under conditions) present but heterogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured tabular dataset of reaction conditions and outcomes; categorical and numerical features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high due to combinatorial condition space and sensitivity to subtle experimental variables; non-linear relationships between condition parameters and product formation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>experimental domain mature but data standardization is limited; systematic datasets of conditions are emerging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — predicting conditions is useful even if mechanistic understanding is partial, but mechanistic insight helps generalization and trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised classification/regression model (unspecified algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A supervised ML model trained on historical reaction-condition → product outcome pairs to predict likely successful conditions for new targets; model outputs probability/success scores and was validated on held-out cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and effective for domains with curated reaction condition datasets; constrained by data heterogeneity and experimental reporting standards.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported success rate: 89% (as stated in the Perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>High reported success rate indicates strong practical utility for condition selection; depends on quality and representativeness of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can accelerate identification of viable synthetic conditions and reduce trial-and-error experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not detailed here; likely outperforms unguided experimental search but depends on dataset coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of labeled reaction-condition datasets and choice of features describing experimental setups and templates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised ML on curated reaction-condition datasets can reliably predict experimental conditions with high success rates when representative labeled data exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2337.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning for synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning to suggest optimal next experiments in synthesis/crystallisation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Active learning approaches select experiments predicted to maximally improve understanding or cover experimental space, demonstrated to explore crystallisation space far more efficiently than human experimenters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>experimental design / materials synthesis (crystallisation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficiently exploring experimental parameter spaces (e.g., crystallisation conditions) to discover successful syntheses and map phase/formation behavior with minimal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>initial labeled data (both failed and successful experiments) required; typically scarce but augmented iteratively by experiment-autonomy loop.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured experimental condition vectors with binary/continuous outcomes (success/failure, crystallisation metrics); time-series when iterative.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high combinatorial parameter space; experiments are costly/time-consuming; data are sparse relative to space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>experimental domain mature; application of active learning is an emerging approach within materials synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — the goal is efficient discovery rather than mechanistic explanation, but some interpretability aids human trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Active learning (sequential experiment suggestion)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Iterative loop where a model trained on initial experiments predicts where information gain is maximized, selects next experiments, and incorporates new results to refine the model; reported to cover ~6x more crystallisation space than a human in same number of experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>active learning (closed-loop experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable where experiments are expensive and initial labeled examples exist; requires automation/robotics or human-in-the-loop to execute suggested experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported coverage improvement: ~6× more crystallisation space explored for same number of experiments (as stated in the Perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to substantially accelerate space coverage and discovery efficiency compared to unguided human experimentation; performance depends on surrogate model quality and exploration-exploitation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — reduces experimental cost/time and accelerates discovery in synthesis-intensive domains, especially when integrated with robotic platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms unguided or heuristic-driven experimental selection in cited examples; comparison to other active strategies not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Inclusion of failed experiments in training, effective uncertainty-aware models to guide exploration, and experimental throughput/automation to execute selected experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Active learning that leverages both failed and successful experiments can dramatically increase experimental discovery efficiency by prioritizing the most informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2337.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN surface characterization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional neural networks (CNNs) for surface structure characterization from imaging/simulation data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-stage pattern-recognition pipelines using convolutional neural networks combined with ab initio simulations to identify and characterise complex surface reconstructions from microscopy data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning surface molecular structures via machine vision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>surface science / microscopy-based characterisation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Inferring atomic arrangements and surface reconstructions from high-resolution imaging data (e.g., microscopy), where patterns are complex and multi-scale.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate — microscopy datasets and simulated images from ab initio calculations are available but require preprocessing; labels may be limited and require expert annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>image data (2D/3D) and simulation outputs; high-dimensional pixel/voxel arrays; multimodal when combined with simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: complex patterns, noisy measurements, multi-scale features and degeneracy of image→structure mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established experimental imaging field; application of deep CNNs is an emerging and rapidly growing area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — outputs used to guide scientific interpretation require credible mappings to atomic structure; interpretability of CNN features helpful but not always necessary for practical classification.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional neural networks within multi-stage pattern recognition</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNNs trained (often on simulated and experimental images) to detect features and classify/segment patterns indicative of surface reconstructions, sometimes combined with ab initio simulation data to form training labels and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate and effective for image-based characterisation tasks; requires representative training images and careful handling of noise/artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated capability to resolve complex surface reconstructions and complement physical modelling; deep learning enables automated interpretation of large imaging datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scaling image analysis, enabling real-time feedback and integration with experimental workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Surpasses manual/heuristic image analysis in throughput and ability to detect subtle patterns; success depends on training labels and simulation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of realistic simulated images for training, multi-stage pipelines combining physics and ML, and robust pre-processing to mitigate noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Physics-informed training (combining ab initio simulations with CNNs) enables reliable automated interpretation of complex microscopy data, bridging experiment and modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2337.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phase classification NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural networks to identify phases of matter and phase transitions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks trained on system configurations to encode phases and detect phase transitions in highly-correlated many-body systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning phases of matter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>condensed matter physics / phase transition identification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classifying phases and locating phase transitions in complex many-body systems using configuration or observable data where traditional order parameters may be unknown or hard to compute.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>problem-dependent — simulated configurations (Monte Carlo, etc.) are abundant for many models; labeled phase data available via simulation or experiment for supervised approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional configuration data (lattice states, correlation functions), images or arrays representing spatial degrees of freedom.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high due to many-body interactions, emergent phenomena, and non-trivial order parameters; nonlinear separability between phases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>theoretical physics is mature with rich prior knowledge, but ML-based phase detection is an emerging computational methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for scientific discovery — interpretability is valuable to extract physical insights, although black-box classifiers can be used for detection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised neural networks (classification) for phase identification</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train neural classifiers on labeled configurations to learn mappings from microscopic states to phase labels; internal representations can be probed to infer emergent order parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for simulated many-body systems and experimental datasets where labels or proxy order parameters can be provided; care needed to ensure models learn physical features rather than artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Successfully identifies known phase transitions and can reveal subtle distinctions between phases, offering a useful complement to traditional analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for detecting complex or novel phases where standard diagnostic tools fail, and for accelerating exploration of parameter spaces in condensed matter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides an alternative to hand-crafted order parameters; may discover discriminative features automatically but requires caution to avoid overfitting to simulation details.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality simulation datasets, careful choice of input representation, and validation against known physical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Neural networks can learn discriminative features that encode phases of matter, enabling detection of transitions even when conventional order parameters are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2337.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML density functionals</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning density functionals and density→energy maps with machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised ML methods trained on quantum data to learn improved exchange-correlation functionals or direct density-to-energy/potential maps, bypassing traditional functional development.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finding density functionals with machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>electronic structure theory / density functional theory (DFT)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improving DFT accuracy by learning functionals (or direct maps) from high-quality quantum mechanical reference data to reduce systematic errors of approximate exchange-correlation models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited-to-moderate — high-quality quantum reference data (e.g., from higher-level calculations) are expensive to generate but several datasets exist; labeled supervised pairs (density→energy) required.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>functional/field data (electron density grids) and scalar energies; structured but high-dimensional continuous data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high: functional mapping is infinite-dimensional and must respect physical constraints (e.g., invariances); nonlinearity and requirement for generalization across chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature theoretical domain with established approximations (LDA, GGA, hybrids); ML approaches are an active frontier seeking to complement/replace parts of DFT.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — scientific interpretability and physical constraints (e.g., conservation laws, asymptotic behavior) are important for trust and transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised learning of density functionals / density-to-energy maps (kernel methods, neural networks, Bayesian approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train regression models (e.g., kernel methods, neural networks) on pairs of electron densities and reference energies/potentials to learn functionals or direct mappings, sometimes incorporating Bayesian error estimation or physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when reliable quantum reference data are available; models must incorporate physical constraints to generalize and be trusted for scientific use.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated potential to match or exceed some approximations in specific domains and to bypass solving Kohn–Sham equations in targeted cases; general-purpose, transferable functionals remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could reduce computational cost and increase accuracy of electronic structure predictions if transferability and physical consistency are achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>ML functionals can outperform standard approximations in trained domains but may lack broad transferability; Bayesian approaches (e.g., BEEF) provide error estimation advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality quantum training data, incorporation of physical constraints and symmetries, and rigorous validation on out-of-sample systems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Data-driven learning of functionals or direct density-to-energy mappings can reduce DFT errors in targeted domains, but physical constraints and representative training data are essential for transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2337.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bypass Kohn-Sham</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct density-to-energy / density-to-potential mapping that bypasses Kohn–Sham equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Machine-learned mappings that predict energies and potentials directly from electron densities, enabling bypass of computationally expensive self-consistent Kohn–Sham calculations for selected systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bypassing the Kohn-Sham equations with machine learning. paving the way for higher-accuracy approaches</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>electronic structure / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Speeding up quantum energy predictions by replacing the iterative solution of Kohn–Sham equations with direct ML mappings from density inputs to energies/potentials.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited — requires paired density and high-quality energy/potential labels from ab initio calculations; generating such datasets is computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>continuous field data (electron density grids) mapped to scalar energies and potentials.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very high due to high dimensionality of density space and need to preserve physical invariances and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging ML augmentation of a mature computational field; standard DFT remains dominant but ML shortcuts are developing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high — ensuring physical plausibility and interpretability is important for scientific acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised regression mapping (neural networks / kernel methods) of density→energy/potential</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train ML regressors on density representations to predict total energies or potentials directly, validated against Kohn–Sham results; methods may incorporate invariant features and regularisation to encode physics.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Promising for families of systems represented in training data; limited generalization beyond trained chemical/structural space without additional data or physics constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to be feasible in cited studies, offering orders-of-magnitude speedups for target systems, but broader applicability requires more research.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High if transferability challenges are solved — could enable rapid high-accuracy screening across chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers faster predictions than full Kohn–Sham runs; compared to standard DFT functionals, ML mappings can match or exceed accuracy in trained domains but risk extrapolation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Careful density representation, high-quality training data, and incorporation of symmetries and physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Direct density→energy ML mappings can bypass costly self-consistent quantum calculations for trained domains, delivering large computational savings provided training data and physical priors are adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2337.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML interatomic potentials (GAP, ANI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learned interatomic potentials (Gaussian Approximation Potentials, ANI neural potentials)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML models (kernel methods like GAP and neural network potentials like ANI) trained on quantum mechanical energies and forces to produce transferable, low-cost interatomic potentials for reactive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gaussian approximation potentials: the accuracy of quantum mechanics, without the electrons</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>atomistic simulation / molecular dynamics / materials modelling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reproducing quantum mechanical potential energy surfaces for molecules and solids at a fraction of the cost to enable large-scale or long-time reactive simulations including bond formation/breaking.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate — quantum mechanical training data (energies and forces) are expensive but commonly produced for target chemistries; datasets vary in size and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured sets of configurations (atomic positions) with scalar energies and vector forces; high-dimensional but permutation-/rotation-invariant representations used (descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: detailed many-body interactions, need for chemical transferability and accurate force derivatives; smoothness and invariance constraints are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>rapidly maturing — these ML potentials are increasingly adopted as surrogate models for quantum calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high for scientific simulation fidelity — potentials must respect physics (energy conservation, symmetries) and provide reliable forces for dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Kernel-based Gaussian Approximation Potentials (GAP) and neural network potentials (ANI)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>GAP uses kernel regression on symmetry-aware descriptors to fit energies/forces; ANI and similar neural potentials train feed-forward neural networks on atomic environments to predict atomic contributions to total energy, matching DFT accuracy at force-field computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (regression / physics-informed ML)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for targeted chemistries where adequate quantum training data exist; provides major speedups enabling larger-scale simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported orders-of-magnitude speedups in computational cost compared to QM; ANI claims 'DFT accuracy at force field computational cost' (qualitative); exact numeric RMSEs depend on dataset and study.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective in reproducing QM energies and enabling reactive MD; transferability beyond training domain is a key limitation to monitor.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables simulations of large systems and longer timescales previously infeasible with QM methods, accelerating mechanistic studies and materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Much faster than QM calculations and more accurate than classical empirical force fields for trained chemistries; however, classical force fields remain more general without QM training.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Representative and diverse QM training data, invariant local descriptors or architectures, and careful validation of forces and energies across relevant configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML-fitted interatomic potentials can approach QM accuracy while giving orders-of-magnitude computational savings for trained systems, but require careful data coverage and physics-aware representations for transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2337.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heusler / half-Heusler classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised learning (random forest) models to predict crystal structure adoption (Heusler / half-Heusler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Random-forest type classifiers trained on experimental/computed datasets to predict the probability that a given composition will adopt a specific crystal structure (e.g., Heusler, half-Heusler), enabling screening and discovery of new compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>inorganic materials design / crystal-structure prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting whether a given chemical composition will stabilize in a targeted crystal prototype, facilitating screening of hypothetical compositions for experimental synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>moderate — experimental databases of known crystal structures exist (e.g., ICSD) but coverage is sparse relative to combinatorial composition space; labels are supervised (structure type).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>tabular compositional descriptors or engineered features summarizing element properties; structured and moderate-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high combinatorial composition space; structural adoption depends on subtle thermodynamic and electronic factors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature materials science with established crystallography databases; ML application to structure-type classification is relatively recent.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — probabilistic predictions are useful for screening but mechanistic/theoretical validation (DFT, experiments) is required for confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Random forest classifiers (ensemble decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train ensemble tree models on labeled composition→structure data to learn feature importance and predict probabilities for structure adoption; used to screen large sets and prioritize candidates for DFT/experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (classification / ensemble methods)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for classification among discrete structure types when labeled examples exist; limited by representativeness of training data across chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Examples cited include discovery and experimental verification of new gallide compounds (12 identified) using such approaches; specific classifier metrics not provided in the Perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective at prioritizing plausible compositions and reducing search space; success contingent on training data quality and feature design.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for accelerating materials discovery by screening large compositional spaces prior to expensive computations or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More scalable and faster than exhaustive DFT screening; complementary to ab initio methods where ML guides candidate selection.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Curated structure databases for training, informative compositional descriptors, and integration with validation pipelines (DFT/experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised classifiers can efficiently predict structure-type adoption across large compositional spaces when trained on representative experimental data, enabling targeted experimental discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2337.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elpasolite energy ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning regression for screening energies of elpasolite (ABC2D6) crystals</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML models trained to reproduce formation energies (or other energetic properties) across all combinatorial permutations of elpasolite stoichiometry, enabling virtual screening of millions of compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning energies of 2 million elpasolite (ABC2D6) crystals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>materials discovery / high-throughput screening</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Rapidly estimate energies/properties across all combinatorial element substitutions for a given prototype structure to identify promising candidate materials.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>training data likely derived from high-throughput DFT calculations for a subset; overall screened set is very large (~2×10^6 combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>tabular compositional descriptors mapping to scalar energies; relatively low-dimensional after encoding element identities and features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>very large combinatorial search space (millions of candidates) but each instance prediction is a regression in descriptor space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging ML-driven high-throughput materials discovery with established workflows for DFT-calculated training data.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — ML can prioritize candidates, but DFT/experimental follow-up needed to confirm mechanistic and stability details.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised regression models (kernel ridge regression / other regressors)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train regression models on DFT-calculated energies for representative subsets to predict energies across the full combinatorial space, enabling fast screening and trend analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to prototype-based screening where structure is fixed and compositional permutations are the variable.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Applied to screen ≈2×10^6 elpasolite combinations and report chemical trends and candidate identification (128 new materials cited in other contexts for different studies).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled exhaustive screening that would be infeasible with direct DFT for every composition, revealing trends and candidates for further study.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — accelerates identification of candidate materials across vast combinatorial spaces and focuses expensive computations/experiments on promising leads.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Far more computationally efficient than brute-force DFT across the entire space; quality depends on representativeness of training set.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Representative DFT training set, suitable compositional descriptors, and validation with targeted calculations/experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML regression trained on quantum calculations permits screening of massive compositional spaces for fixed structure prototypes, greatly expanding discovery throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2337.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GANs / ORGAN for de novo molecules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Networks and ORGAN for de novo molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative adversarial frameworks (GANs) and reinforcement-learning-augmented GANs (ORGAN) generate novel molecular structures (e.g., SMILES) biased toward desired chemical features or properties through an adversarial game and reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>molecular design / drug discovery / de novo generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generating chemically valid and diverse novel molecules optimized for target properties (activity, physical properties) from learned distributions rather than exhaustive enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>large molecular libraries exist (public and proprietary); training data are typically abundant for many tasks but property labels may be limited for some objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>discrete sequence data (SMILES) or graph representations; generation involves sequential token generation or graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high — need to satisfy chemical validity constraints, optimize multiple objectives, and maintain diversity; generation is a high-dimensional discrete optimization problem.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>molecular generative modeling is a rapidly advancing field leveraging developments from NLP and generative modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — black-box generation acceptable for ideation, but interpretability and synthetic accessibility are important for downstream utility.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Generative Adversarial Networks (GANs) and ORGAN (reinforcement learning augmented GAN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>GANs train a generator to produce samples and a discriminator to distinguish real vs generated molecules; ORGAN augments generator training with reinforcement learning reward functions to bias generation toward desired molecular properties.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling (with reinforcement learning augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Very suitable for de novo design and exploration of chemical space, especially when large molecular corpora are available; requires mechanisms to ensure chemical validity and synthesizability.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated ability to produce chemically diverse molecules with tailored features; practical impact depends on quality of reward design and downstream filtering for synthesizability and ADMET.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables rapid ideation of novel molecules and directed exploration of chemical space, potentially accelerating lead discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Generative models can explore chemical space beyond simple library enumeration or mutation-based approaches; must be compared to VAEs, reinforcement learning-only generators, and fragment-based methods on validity/diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large training corpora, appropriate molecular representations (SMILES/graphs), robust reward engineering, and post-generation filters for synthetic feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adversarial and reinforcement-augmented generative models can propose novel molecules tailored to objectives, but their utility hinges on reward design and integration with synthesizability assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2337.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph conv nets for molecular fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional networks on graphs for learning molecular fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph convolutional neural networks learn task-specific molecular fingerprints directly from graph-structured molecular representations, improving property prediction and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional networks on graphs for learning molecular fingerprints</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>molecular property prediction / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting molecular properties by automatically learning representations (fingerprints) from molecular graph structures rather than relying on hand-crafted descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often large for common tasks (e.g., public QSAR datasets), but can be limited for specialized properties; labeled supervised data required.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>graph-structured data (atoms as nodes, bonds as edges); inherently relational and variable-sized.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate-to-high due to graph combinatorics, need to capture local and global structural patterns, and variable molecular sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established chemoinformatics domain; graph-based deep learning is a modern, effective approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — learned fingerprints are less interpretable than simple descriptors but can be probed; predictive performance often prioritized.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph convolutional neural networks (GCNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks that apply convolution-like operations over nodes and local neighborhoods on molecular graphs to produce fixed-length learned fingerprints used for downstream supervised prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised representation learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for diverse molecular property prediction tasks; particularly effective when molecular structure is the primary determinant of the property.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shown to outperform many traditional fingerprint-based models in various tasks, and to learn task-specific representations that improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — reduces need for hand-crafted descriptors and can improve predictive accuracy across many QSAR applications.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Often outperforms classical fingerprint + shallow model pipelines, though depends on dataset size and task.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sufficient labeled data, appropriate network depth to capture relevant structural scales, and good message-passing/aggregation design.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning molecular representations directly from graphs yields more expressive and task-adapted fingerprints, improving many property-prediction tasks over hand-crafted descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2337.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2337.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text mining for materials synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text mining / ML extraction of synthesis knowledge from scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combining text processing and machine learning to extract structured facts, reaction conditions, and synthesis insights from heterogeneous unstructured literature to create searchable databases and support decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Materials synthesis insights from scientific literature via text extraction and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>knowledge extraction / literature mining for materials science and chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Converting unstructured textual and supplemental scientific literature into structured, machine-readable datasets of synthesis conditions, experimental outcomes, and relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>abundant in raw form (vast literature), but labeled/curated machine-readable datasets are limited and heterogenous; text is unstructured and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>unstructured text (articles, supplementary materials), figures, tables; requires NLP pipelines to produce structured tabular outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high due to variable writing styles, inconsistent metadata, and extraction of nuanced experimental details; linking dispersed information across documents is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging application area with growing tools and datasets but lacking standardized reporting conventions; interdisciplinary (NLP + domain knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low in terms of extraction (facts are primary), but downstream usage for mechanistic modelling benefits from structured, interpretable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Text mining with NLP and machine learning (named-entity recognition, relation extraction, classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Use of tokenization, entity recognition, relation extraction, and ML classifiers to parse papers and supplemental data into structured fields (reagents, conditions, outcomes), often combined with domain ontologies and human curation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / supervised NLP / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for building searchable synthesis databases and enabling meta-analyses; limited by variability of reporting and lack of standardized metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Valuable for aggregating dispersed experimental knowledge and enabling downstream ML workflows; extraction accuracy depends on NLP model sophistication and labeled corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can unlock hidden experimental knowledge across literature, support automated discovery, and improve data accessibility for ML modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms manual curation at scale but requires human-in-the-loop validation; complementary to structured data submission initiatives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of labeled corpora for training, domain ontologies, and community moves toward standardized reporting and open data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Text mining can convert the vast unstructured literature into structured datasets that power ML-driven discovery, but success depends on NLP robustness and improvements in reporting standards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for molecular and materials science', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Planning chemical syntheses with deep neural networks and symbolic AI <em>(Rating: 2)</em></li>
                <li>Retrosynthetic reaction prediction using neural sequence-to-sequence models <em>(Rating: 2)</em></li>
                <li>Low data drug discovery with one-shot learning <em>(Rating: 2)</em></li>
                <li>Will it crystallise? Predicting crystallinity of molecular materials. <em>(Rating: 2)</em></li>
                <li>Machine-learning-assisted materials discovery using failed experiments <em>(Rating: 2)</em></li>
                <li>Learning surface molecular structures via machine vision <em>(Rating: 2)</em></li>
                <li>Machine learning phases of matter <em>(Rating: 2)</em></li>
                <li>Finding density functionals with machine learning <em>(Rating: 2)</em></li>
                <li>Bypassing the Kohn-Sham equations with machine learning. paving the way for higher-accuracy approaches <em>(Rating: 2)</em></li>
                <li>Gaussian approximation potentials: the accuracy of quantum mechanics, without the electrons <em>(Rating: 2)</em></li>
                <li>Ani-1: an extensible neural network potential with DFT accuracy at force field computational cost <em>(Rating: 2)</em></li>
                <li>Machine learning energies of 2 million elpasolite (ABC2D6) crystals <em>(Rating: 2)</em></li>
                <li>Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models <em>(Rating: 2)</em></li>
                <li>Convolutional networks on graphs for learning molecular fingerprints <em>(Rating: 2)</em></li>
                <li>Materials synthesis insights from scientific literature via text extraction and machine learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2337",
    "paper_id": "paper-50780992",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Deep retrosynthesis (NN + symbolic AI)",
            "name_full": "Deep neural networks combined with symbolic AI for retrosynthetic planning",
            "brief_description": "Deep learning models (many-layer ANNs) integrated with rules-based/symbolic expert systems to predict and rank synthetic routes and retrosynthetic steps from reaction literature.",
            "citation_title": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "mention_or_use": "mention",
            "scientific_problem_domain": "organic synthesis / retrosynthetic planning",
            "problem_description": "Automated prediction of valid synthetic routes and retrosynthetic steps for target organic molecules from a vast space of possible transformations and reagents.",
            "data_availability": "moderate-to-large when using published reaction corpora (many literature-reported reactions available); labeled (reaction inputs → outputs) but heterogeneous in quality; accessibility improving via public datasets but still noisy.",
            "data_structure": "structured/semi-structured text-derived reaction records and molecular string representations (SMILES); discrete symbolic features and high-dimensional descriptor vectors.",
            "problem_complexity": "very high: combinatorial explosion of possible transformations per step (tens to thousands), nonlinear relationships between reagents/conditions and outcomes; large search space across multi-step routes.",
            "domain_maturity": "mature experimental domain with extensive prior knowledge and hand-coded rules (expert systems), but algorithmic automation is emerging; wealth of literature for training.",
            "mechanistic_understanding_requirements": "medium — interpretable constraints and rules are useful for chemical validity, but black-box ranking acceptable for candidate generation; mechanistic plausibility desirable for trust.",
            "ai_methodology_name": "Deep neural networks combined with symbolic/rules-based AI",
            "ai_methodology_description": "Many-layer artificial neural networks trained on reaction examples to predict likely precursors or next steps, used in combination with symbolic rule sets or expert-system filters that constrain chemically implausible choices; networks often provide ranking scores for candidate routes.",
            "ai_methodology_category": "hybrid (deep learning + symbolic/knowledge-based)",
            "applicability": "highly applicable — hybrid approach leverages strengths of both data-driven pattern learning and expert chemical rules; limitations include dependence on training corpora and rulesets, and difficulty generalizing outside knowledge base.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to produce retrosynthetic routes comparable in quality to human experts in cited studies; successful integration of ranking networks with rules improves practical planning, but rules-based parts limit extension beyond knowledge base.",
            "impact_potential": "High — can dramatically accelerate route planning, integrate with automated synthesis platforms, and reduce expert labor; potential to scale chemical discovery and route optimization.",
            "comparison_to_alternatives": "Compared favorably to pure rules-based systems in cited examples (hybrid approaches improve ranking and flexibility); pure-symbolic systems struggle with combinatorial scale.",
            "success_factors": "Large corpora of reaction examples, effective combination of symbolic constraints with learned ranking, and representation of molecules (e.g. SMILES) that neural nets can consume.",
            "key_insight": "Combining deep learning for ranking with symbolic chemical knowledge mitigates combinatorial complexity and yields human-competitive retrosynthetic planning, though generalisation outside training/rule domains remains a challenge.",
            "uuid": "e2337.0",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Seq2Seq reaction prediction",
            "name_full": "Sequence-to-sequence neural models for reaction and retrosynthesis prediction",
            "brief_description": "Neural sequence-to-sequence architectures treat molecules/reactions as text (e.g., SMILES) and learn mappings from products to reactants (or reactants to products) for forward and retrosynthetic prediction.",
            "citation_title": "Retrosynthetic reaction prediction using neural sequence-to-sequence models",
            "mention_or_use": "mention",
            "scientific_problem_domain": "organic reaction prediction / retrosynthesis",
            "problem_description": "Predicting reactants given a product (retrosynthesis) or products given reactants by learning the transformation encoded in reaction SMILES sequences.",
            "data_availability": "variable — many published reactions exist but labeled high-quality datasets may be limited and noisy; datasets are typically supervised (input→output pairs).",
            "data_structure": "sequential text-like data (SMILES strings), discrete token sequences; can be high-dimensional after tokenization.",
            "problem_complexity": "high nonlinearity and combinatorial mapping between product and plausible reactant sets; sequence modeling complexity similar to natural language translation.",
            "domain_maturity": "well-established experimental domain with decades of literature; sequence models are an emerging application borrowing mature NLP architectures.",
            "mechanistic_understanding_requirements": "low-to-medium — sequence-to-sequence models prioritize predictive accuracy and may be treated as black boxes for candidate generation, though mechanistic validation is needed before synthesis.",
            "ai_methodology_name": "Neural sequence-to-sequence models (encoder-decoder architectures)",
            "ai_methodology_description": "Encoder-decoder recurrent/transformer-like networks trained on paired SMILES sequences to learn reaction transformations; models generate candidate reactant SMILES given product SMILES and can be fine-tuned or combined with scoring/ranking.",
            "ai_methodology_category": "supervised learning (sequence modeling / generative)",
            "applicability": "Appropriate for reaction prediction where sufficient paired examples exist; limitations include sensitivity to training data distribution and inability to guarantee chemical feasibility without additional filters.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated promising performance analogous to machine translation tasks; useful for generating plausible retrosynthetic candidates but may require post-processing for chemical validity.",
            "impact_potential": "Moderate-high — provides an automated route to generate candidate reactions, speeding design and hypothesis generation; integration with expert filters needed for practical lab adoption.",
            "comparison_to_alternatives": "Offers a data-driven alternative to rules-based retrosynthesis; sequence models can generalize to patterns not encoded in hand-rules but may hallucinate chemically implausible transformations.",
            "success_factors": "Availability of large reaction corpora, effective tokenization/representation of molecules (SMILES), and sequence-model capacity to capture complex reaction syntax.",
            "key_insight": "Treating chemical reactions as a language enables application of powerful sequence models for reaction prediction, but data quality and chemical validity checks are critical for reliable use.",
            "uuid": "e2337.1",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "One-shot learning (drug discovery)",
            "name_full": "One-shot / few-shot learning applied to low-data drug discovery",
            "brief_description": "Meta-learning approaches that achieve reasonable predictive performance with very limited labeled examples per target by leveraging shared structure across many related tasks.",
            "citation_title": "Low data drug discovery with one-shot learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "drug discovery / molecular property prediction",
            "problem_description": "Predicting biological activity or other molecular properties when only a handful of labeled examples are available for a new target or task.",
            "data_availability": "scarce for specific tasks/targets (few examples); larger amounts of related data across other targets can be leveraged via meta-learning; labels are supervised but sparse.",
            "data_structure": "molecular descriptors/fingerprints or graph representations; structured but high-dimensional and heterogeneous across tasks.",
            "problem_complexity": "High due to low labeled sample size per task, need to transfer learning across tasks, and nonlinearity of structure-activity relationships.",
            "domain_maturity": "Established domain (QSAR) with many historical datasets, but low-data methods are an active research frontier addressing practical constraints.",
            "mechanistic_understanding_requirements": "medium — practitioners often need some interpretability for lead optimization and understanding ADMET liabilities, though early-stage screening can tolerate black-boxes.",
            "ai_methodology_name": "One-shot / meta-learning (probabilistic program induction / Siamese / matching networks)",
            "ai_methodology_description": "Meta-learning frameworks trained across many tasks to learn priors or similarity metrics that enable accurate predictions for new tasks with one or few labeled examples; examples include Siamese or matching networks and probabilistic program induction approaches.",
            "ai_methodology_category": "meta-learning / transfer learning / supervised (few-shot)",
            "applicability": "Well-suited to molecular discovery scenarios where data collection is expensive; requires related auxiliary datasets for meta-training.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve human-level performance on selected one-shot tasks in cited work; promising for hit-finding in low-data regimes but sensitive to task relatedness and representation choice.",
            "impact_potential": "High for domains where experiments are costly/time-consuming (e.g., novel biological targets), enabling quicker iteration and reduced experimental burden.",
            "comparison_to_alternatives": "Outperforms naïve transfer or training-from-scratch in low-data regimes when properly meta-trained; less effective than large-data supervised models where abundant labels exist.",
            "success_factors": "Availability of many related tasks for meta-training, suitable molecular representation learning (fingerprints/graphs), and architectures that capture cross-task commonalities.",
            "key_insight": "Meta-learning and one-shot methods can overcome scarce label regimes in molecular discovery by leveraging shared information across tasks, enabling useful predictions with very few examples.",
            "uuid": "e2337.2",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Crystallisation propensity model",
            "name_full": "Feature-selection based model for predicting molecular crystallisation propensity",
            "brief_description": "A two-parameter ML model trained on a large curated dataset (&gt;20,000 examples) that predicts whether a molecule will crystallize, achieving ~80% accuracy.",
            "citation_title": "Will it crystallise? Predicting crystallinity of molecular materials.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "crystal engineering / materials synthesis",
            "problem_description": "Predicting whether a given organic molecule will form a crystalline solid under typical conditions.",
            "data_availability": "relatively abundant for this task in cited study (~20,000 crystalline and non-crystalline examples); labeled (crystallises / does not) and accessible in public datasets.",
            "data_structure": "tabular descriptors (molecular features) derived from chemical structures; low-dimensional after feature selection (two-parameter model in cited work).",
            "problem_complexity": "moderate — depends on subtle molecular packing and intermolecular interactions; fewer features needed for good predictive performance in cited model.",
            "domain_maturity": "moderately mature with experimental databases and empirical heuristics; ML application is emerging and benefits from curated datasets.",
            "mechanistic_understanding_requirements": "medium — predictive work is valuable for screening, but mechanistic insight into crystallization pathways is scientifically important for control and design.",
            "ai_methodology_name": "Feature selection + supervised classification (two-parameter model)",
            "ai_methodology_description": "Feature engineering and selection yielded a compact two-parameter representation feeding a supervised classifier that outputs crystallisation propensity; trained on &gt;20,000 labeled examples.",
            "ai_methodology_category": "supervised learning (classification)",
            "applicability": "Applicable and appropriate given abundant labeled data; simplicity of the model aided interpretability and deployment.",
            "effectiveness_quantitative": "Accuracy ≈ 80% reported on test data (from &gt;20,000-sample dataset).",
            "effectiveness_qualitative": "Worked well with a compact descriptor set; demonstrates that even simple models can be predictive when large curated training sets are available.",
            "impact_potential": "Moderate — aids experimental prioritization for crystallization trials and illustrates importance of public curated datasets for ML in materials chemistry.",
            "comparison_to_alternatives": "Not explicitly compared in detail here, but the two-parameter model's high accuracy suggests advantages over ad hoc heuristics without data-driven calibration.",
            "success_factors": "Large, curated labeled dataset and careful feature selection that distilled relevant physics/chemistry into low-dimensional descriptors.",
            "key_insight": "With sufficiently large and curated labeled datasets, even low-dimensional ML models can robustly predict crystallisation propensity, highlighting data quality and representation as key success factors.",
            "uuid": "e2337.3",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Reaction condition predictor",
            "name_full": "Supervised ML model to predict reaction conditions for inorganic product formation",
            "brief_description": "A supervised model trained to predict suitable reaction conditions for formation of organically templated inorganic products, reporting a success rate of 89%.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "materials synthesis / inorganic chemistry",
            "problem_description": "Predicting the experimental conditions (e.g., reagents, temperature, solvent, templates) likely to produce a target organically-templated inorganic product.",
            "data_availability": "moderate — sufficient labeled examples were available in cited work to train a supervised model; labeled outcome (success/failure under conditions) present but heterogeneous.",
            "data_structure": "structured tabular dataset of reaction conditions and outcomes; categorical and numerical features.",
            "problem_complexity": "high due to combinatorial condition space and sensitivity to subtle experimental variables; non-linear relationships between condition parameters and product formation.",
            "domain_maturity": "experimental domain mature but data standardization is limited; systematic datasets of conditions are emerging.",
            "mechanistic_understanding_requirements": "medium — predicting conditions is useful even if mechanistic understanding is partial, but mechanistic insight helps generalization and trust.",
            "ai_methodology_name": "Supervised classification/regression model (unspecified algorithm)",
            "ai_methodology_description": "A supervised ML model trained on historical reaction-condition → product outcome pairs to predict likely successful conditions for new targets; model outputs probability/success scores and was validated on held-out cases.",
            "ai_methodology_category": "supervised learning",
            "applicability": "Applicable and effective for domains with curated reaction condition datasets; constrained by data heterogeneity and experimental reporting standards.",
            "effectiveness_quantitative": "Reported success rate: 89% (as stated in the Perspective).",
            "effectiveness_qualitative": "High reported success rate indicates strong practical utility for condition selection; depends on quality and representativeness of training data.",
            "impact_potential": "High — can accelerate identification of viable synthetic conditions and reduce trial-and-error experimentation.",
            "comparison_to_alternatives": "Not detailed here; likely outperforms unguided experimental search but depends on dataset coverage.",
            "success_factors": "Availability of labeled reaction-condition datasets and choice of features describing experimental setups and templates.",
            "key_insight": "Supervised ML on curated reaction-condition datasets can reliably predict experimental conditions with high success rates when representative labeled data exist.",
            "uuid": "e2337.4",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Active learning for synthesis",
            "name_full": "Active learning to suggest optimal next experiments in synthesis/crystallisation",
            "brief_description": "Active learning approaches select experiments predicted to maximally improve understanding or cover experimental space, demonstrated to explore crystallisation space far more efficiently than human experimenters.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "experimental design / materials synthesis (crystallisation)",
            "problem_description": "Efficiently exploring experimental parameter spaces (e.g., crystallisation conditions) to discover successful syntheses and map phase/formation behavior with minimal experiments.",
            "data_availability": "initial labeled data (both failed and successful experiments) required; typically scarce but augmented iteratively by experiment-autonomy loop.",
            "data_structure": "structured experimental condition vectors with binary/continuous outcomes (success/failure, crystallisation metrics); time-series when iterative.",
            "problem_complexity": "high combinatorial parameter space; experiments are costly/time-consuming; data are sparse relative to space.",
            "domain_maturity": "experimental domain mature; application of active learning is an emerging approach within materials synthesis.",
            "mechanistic_understanding_requirements": "low-to-medium — the goal is efficient discovery rather than mechanistic explanation, but some interpretability aids human trust.",
            "ai_methodology_name": "Active learning (sequential experiment suggestion)",
            "ai_methodology_description": "Iterative loop where a model trained on initial experiments predicts where information gain is maximized, selects next experiments, and incorporates new results to refine the model; reported to cover ~6x more crystallisation space than a human in same number of experiments.",
            "ai_methodology_category": "active learning (closed-loop experimental design)",
            "applicability": "Highly applicable where experiments are expensive and initial labeled examples exist; requires automation/robotics or human-in-the-loop to execute suggested experiments.",
            "effectiveness_quantitative": "Reported coverage improvement: ~6× more crystallisation space explored for same number of experiments (as stated in the Perspective).",
            "effectiveness_qualitative": "Shown to substantially accelerate space coverage and discovery efficiency compared to unguided human experimentation; performance depends on surrogate model quality and exploration-exploitation strategy.",
            "impact_potential": "High — reduces experimental cost/time and accelerates discovery in synthesis-intensive domains, especially when integrated with robotic platforms.",
            "comparison_to_alternatives": "Outperforms unguided or heuristic-driven experimental selection in cited examples; comparison to other active strategies not detailed here.",
            "success_factors": "Inclusion of failed experiments in training, effective uncertainty-aware models to guide exploration, and experimental throughput/automation to execute selected experiments.",
            "key_insight": "Active learning that leverages both failed and successful experiments can dramatically increase experimental discovery efficiency by prioritizing the most informative experiments.",
            "uuid": "e2337.5",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "CNN surface characterization",
            "name_full": "Convolutional neural networks (CNNs) for surface structure characterization from imaging/simulation data",
            "brief_description": "Multi-stage pattern-recognition pipelines using convolutional neural networks combined with ab initio simulations to identify and characterise complex surface reconstructions from microscopy data.",
            "citation_title": "Learning surface molecular structures via machine vision",
            "mention_or_use": "mention",
            "scientific_problem_domain": "surface science / microscopy-based characterisation",
            "problem_description": "Inferring atomic arrangements and surface reconstructions from high-resolution imaging data (e.g., microscopy), where patterns are complex and multi-scale.",
            "data_availability": "moderate — microscopy datasets and simulated images from ab initio calculations are available but require preprocessing; labels may be limited and require expert annotation.",
            "data_structure": "image data (2D/3D) and simulation outputs; high-dimensional pixel/voxel arrays; multimodal when combined with simulations.",
            "problem_complexity": "high: complex patterns, noisy measurements, multi-scale features and degeneracy of image→structure mapping.",
            "domain_maturity": "well-established experimental imaging field; application of deep CNNs is an emerging and rapidly growing area.",
            "mechanistic_understanding_requirements": "medium — outputs used to guide scientific interpretation require credible mappings to atomic structure; interpretability of CNN features helpful but not always necessary for practical classification.",
            "ai_methodology_name": "Convolutional neural networks within multi-stage pattern recognition",
            "ai_methodology_description": "CNNs trained (often on simulated and experimental images) to detect features and classify/segment patterns indicative of surface reconstructions, sometimes combined with ab initio simulation data to form training labels and validation.",
            "ai_methodology_category": "supervised learning (computer vision)",
            "applicability": "Appropriate and effective for image-based characterisation tasks; requires representative training images and careful handling of noise/artifacts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated capability to resolve complex surface reconstructions and complement physical modelling; deep learning enables automated interpretation of large imaging datasets.",
            "impact_potential": "High for scaling image analysis, enabling real-time feedback and integration with experimental workflows.",
            "comparison_to_alternatives": "Surpasses manual/heuristic image analysis in throughput and ability to detect subtle patterns; success depends on training labels and simulation fidelity.",
            "success_factors": "Availability of realistic simulated images for training, multi-stage pipelines combining physics and ML, and robust pre-processing to mitigate noise.",
            "key_insight": "Physics-informed training (combining ab initio simulations with CNNs) enables reliable automated interpretation of complex microscopy data, bridging experiment and modelling.",
            "uuid": "e2337.6",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Phase classification NN",
            "name_full": "Neural networks to identify phases of matter and phase transitions",
            "brief_description": "Neural networks trained on system configurations to encode phases and detect phase transitions in highly-correlated many-body systems.",
            "citation_title": "Machine learning phases of matter",
            "mention_or_use": "mention",
            "scientific_problem_domain": "condensed matter physics / phase transition identification",
            "problem_description": "Classifying phases and locating phase transitions in complex many-body systems using configuration or observable data where traditional order parameters may be unknown or hard to compute.",
            "data_availability": "problem-dependent — simulated configurations (Monte Carlo, etc.) are abundant for many models; labeled phase data available via simulation or experiment for supervised approaches.",
            "data_structure": "high-dimensional configuration data (lattice states, correlation functions), images or arrays representing spatial degrees of freedom.",
            "problem_complexity": "very high due to many-body interactions, emergent phenomena, and non-trivial order parameters; nonlinear separability between phases.",
            "domain_maturity": "theoretical physics is mature with rich prior knowledge, but ML-based phase detection is an emerging computational methodology.",
            "mechanistic_understanding_requirements": "high for scientific discovery — interpretability is valuable to extract physical insights, although black-box classifiers can be used for detection.",
            "ai_methodology_name": "Supervised neural networks (classification) for phase identification",
            "ai_methodology_description": "Train neural classifiers on labeled configurations to learn mappings from microscopic states to phase labels; internal representations can be probed to infer emergent order parameters.",
            "ai_methodology_category": "supervised learning",
            "applicability": "Well-suited for simulated many-body systems and experimental datasets where labels or proxy order parameters can be provided; care needed to ensure models learn physical features rather than artifacts.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Successfully identifies known phase transitions and can reveal subtle distinctions between phases, offering a useful complement to traditional analysis.",
            "impact_potential": "High for detecting complex or novel phases where standard diagnostic tools fail, and for accelerating exploration of parameter spaces in condensed matter.",
            "comparison_to_alternatives": "Provides an alternative to hand-crafted order parameters; may discover discriminative features automatically but requires caution to avoid overfitting to simulation details.",
            "success_factors": "High-quality simulation datasets, careful choice of input representation, and validation against known physical benchmarks.",
            "key_insight": "Neural networks can learn discriminative features that encode phases of matter, enabling detection of transitions even when conventional order parameters are unknown.",
            "uuid": "e2337.7",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "ML density functionals",
            "name_full": "Learning density functionals and density→energy maps with machine learning",
            "brief_description": "Supervised ML methods trained on quantum data to learn improved exchange-correlation functionals or direct density-to-energy/potential maps, bypassing traditional functional development.",
            "citation_title": "Finding density functionals with machine learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "electronic structure theory / density functional theory (DFT)",
            "problem_description": "Improving DFT accuracy by learning functionals (or direct maps) from high-quality quantum mechanical reference data to reduce systematic errors of approximate exchange-correlation models.",
            "data_availability": "limited-to-moderate — high-quality quantum reference data (e.g., from higher-level calculations) are expensive to generate but several datasets exist; labeled supervised pairs (density→energy) required.",
            "data_structure": "functional/field data (electron density grids) and scalar energies; structured but high-dimensional continuous data.",
            "problem_complexity": "very high: functional mapping is infinite-dimensional and must respect physical constraints (e.g., invariances); nonlinearity and requirement for generalization across chemical space.",
            "domain_maturity": "mature theoretical domain with established approximations (LDA, GGA, hybrids); ML approaches are an active frontier seeking to complement/replace parts of DFT.",
            "mechanistic_understanding_requirements": "high — scientific interpretability and physical constraints (e.g., conservation laws, asymptotic behavior) are important for trust and transferability.",
            "ai_methodology_name": "Supervised learning of density functionals / density-to-energy maps (kernel methods, neural networks, Bayesian approaches)",
            "ai_methodology_description": "Train regression models (e.g., kernel methods, neural networks) on pairs of electron densities and reference energies/potentials to learn functionals or direct mappings, sometimes incorporating Bayesian error estimation or physical constraints.",
            "ai_methodology_category": "supervised learning / physics-informed ML",
            "applicability": "Applicable when reliable quantum reference data are available; models must incorporate physical constraints to generalize and be trusted for scientific use.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated potential to match or exceed some approximations in specific domains and to bypass solving Kohn–Sham equations in targeted cases; general-purpose, transferable functionals remain challenging.",
            "impact_potential": "High — could reduce computational cost and increase accuracy of electronic structure predictions if transferability and physical consistency are achieved.",
            "comparison_to_alternatives": "ML functionals can outperform standard approximations in trained domains but may lack broad transferability; Bayesian approaches (e.g., BEEF) provide error estimation advantages.",
            "success_factors": "High-quality quantum training data, incorporation of physical constraints and symmetries, and rigorous validation on out-of-sample systems.",
            "key_insight": "Data-driven learning of functionals or direct density-to-energy mappings can reduce DFT errors in targeted domains, but physical constraints and representative training data are essential for transferability.",
            "uuid": "e2337.8",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Bypass Kohn-Sham",
            "name_full": "Direct density-to-energy / density-to-potential mapping that bypasses Kohn–Sham equations",
            "brief_description": "Machine-learned mappings that predict energies and potentials directly from electron densities, enabling bypass of computationally expensive self-consistent Kohn–Sham calculations for selected systems.",
            "citation_title": "Bypassing the Kohn-Sham equations with machine learning. paving the way for higher-accuracy approaches",
            "mention_or_use": "mention",
            "scientific_problem_domain": "electronic structure / computational chemistry",
            "problem_description": "Speeding up quantum energy predictions by replacing the iterative solution of Kohn–Sham equations with direct ML mappings from density inputs to energies/potentials.",
            "data_availability": "limited — requires paired density and high-quality energy/potential labels from ab initio calculations; generating such datasets is computationally intensive.",
            "data_structure": "continuous field data (electron density grids) mapped to scalar energies and potentials.",
            "problem_complexity": "very high due to high dimensionality of density space and need to preserve physical invariances and constraints.",
            "domain_maturity": "emerging ML augmentation of a mature computational field; standard DFT remains dominant but ML shortcuts are developing.",
            "mechanistic_understanding_requirements": "high — ensuring physical plausibility and interpretability is important for scientific acceptance.",
            "ai_methodology_name": "Supervised regression mapping (neural networks / kernel methods) of density→energy/potential",
            "ai_methodology_description": "Train ML regressors on density representations to predict total energies or potentials directly, validated against Kohn–Sham results; methods may incorporate invariant features and regularisation to encode physics.",
            "ai_methodology_category": "supervised learning / physics-informed ML",
            "applicability": "Promising for families of systems represented in training data; limited generalization beyond trained chemical/structural space without additional data or physics constraints.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to be feasible in cited studies, offering orders-of-magnitude speedups for target systems, but broader applicability requires more research.",
            "impact_potential": "High if transferability challenges are solved — could enable rapid high-accuracy screening across chemical space.",
            "comparison_to_alternatives": "Offers faster predictions than full Kohn–Sham runs; compared to standard DFT functionals, ML mappings can match or exceed accuracy in trained domains but risk extrapolation errors.",
            "success_factors": "Careful density representation, high-quality training data, and incorporation of symmetries and physical constraints.",
            "key_insight": "Direct density→energy ML mappings can bypass costly self-consistent quantum calculations for trained domains, delivering large computational savings provided training data and physical priors are adequate.",
            "uuid": "e2337.9",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "ML interatomic potentials (GAP, ANI)",
            "name_full": "Machine-learned interatomic potentials (Gaussian Approximation Potentials, ANI neural potentials)",
            "brief_description": "ML models (kernel methods like GAP and neural network potentials like ANI) trained on quantum mechanical energies and forces to produce transferable, low-cost interatomic potentials for reactive simulations.",
            "citation_title": "Gaussian approximation potentials: the accuracy of quantum mechanics, without the electrons",
            "mention_or_use": "mention",
            "scientific_problem_domain": "atomistic simulation / molecular dynamics / materials modelling",
            "problem_description": "Reproducing quantum mechanical potential energy surfaces for molecules and solids at a fraction of the cost to enable large-scale or long-time reactive simulations including bond formation/breaking.",
            "data_availability": "moderate — quantum mechanical training data (energies and forces) are expensive but commonly produced for target chemistries; datasets vary in size and coverage.",
            "data_structure": "structured sets of configurations (atomic positions) with scalar energies and vector forces; high-dimensional but permutation-/rotation-invariant representations used (descriptors).",
            "problem_complexity": "high: detailed many-body interactions, need for chemical transferability and accurate force derivatives; smoothness and invariance constraints are critical.",
            "domain_maturity": "rapidly maturing — these ML potentials are increasingly adopted as surrogate models for quantum calculations.",
            "mechanistic_understanding_requirements": "high for scientific simulation fidelity — potentials must respect physics (energy conservation, symmetries) and provide reliable forces for dynamics.",
            "ai_methodology_name": "Kernel-based Gaussian Approximation Potentials (GAP) and neural network potentials (ANI)",
            "ai_methodology_description": "GAP uses kernel regression on symmetry-aware descriptors to fit energies/forces; ANI and similar neural potentials train feed-forward neural networks on atomic environments to predict atomic contributions to total energy, matching DFT accuracy at force-field computational cost.",
            "ai_methodology_category": "supervised learning (regression / physics-informed ML)",
            "applicability": "Highly applicable for targeted chemistries where adequate quantum training data exist; provides major speedups enabling larger-scale simulations.",
            "effectiveness_quantitative": "Reported orders-of-magnitude speedups in computational cost compared to QM; ANI claims 'DFT accuracy at force field computational cost' (qualitative); exact numeric RMSEs depend on dataset and study.",
            "effectiveness_qualitative": "Effective in reproducing QM energies and enabling reactive MD; transferability beyond training domain is a key limitation to monitor.",
            "impact_potential": "High — enables simulations of large systems and longer timescales previously infeasible with QM methods, accelerating mechanistic studies and materials discovery.",
            "comparison_to_alternatives": "Much faster than QM calculations and more accurate than classical empirical force fields for trained chemistries; however, classical force fields remain more general without QM training.",
            "success_factors": "Representative and diverse QM training data, invariant local descriptors or architectures, and careful validation of forces and energies across relevant configurations.",
            "key_insight": "ML-fitted interatomic potentials can approach QM accuracy while giving orders-of-magnitude computational savings for trained systems, but require careful data coverage and physics-aware representations for transferability.",
            "uuid": "e2337.10",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Heusler / half-Heusler classifiers",
            "name_full": "Supervised learning (random forest) models to predict crystal structure adoption (Heusler / half-Heusler)",
            "brief_description": "Random-forest type classifiers trained on experimental/computed datasets to predict the probability that a given composition will adopt a specific crystal structure (e.g., Heusler, half-Heusler), enabling screening and discovery of new compounds.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "inorganic materials design / crystal-structure prediction",
            "problem_description": "Predicting whether a given chemical composition will stabilize in a targeted crystal prototype, facilitating screening of hypothetical compositions for experimental synthesis.",
            "data_availability": "moderate — experimental databases of known crystal structures exist (e.g., ICSD) but coverage is sparse relative to combinatorial composition space; labels are supervised (structure type).",
            "data_structure": "tabular compositional descriptors or engineered features summarizing element properties; structured and moderate-dimensional.",
            "problem_complexity": "high combinatorial composition space; structural adoption depends on subtle thermodynamic and electronic factors.",
            "domain_maturity": "mature materials science with established crystallography databases; ML application to structure-type classification is relatively recent.",
            "mechanistic_understanding_requirements": "medium — probabilistic predictions are useful for screening but mechanistic/theoretical validation (DFT, experiments) is required for confirmation.",
            "ai_methodology_name": "Random forest classifiers (ensemble decision trees)",
            "ai_methodology_description": "Train ensemble tree models on labeled composition→structure data to learn feature importance and predict probabilities for structure adoption; used to screen large sets and prioritize candidates for DFT/experiment.",
            "ai_methodology_category": "supervised learning (classification / ensemble methods)",
            "applicability": "Appropriate for classification among discrete structure types when labeled examples exist; limited by representativeness of training data across chemical space.",
            "effectiveness_quantitative": "Examples cited include discovery and experimental verification of new gallide compounds (12 identified) using such approaches; specific classifier metrics not provided in the Perspective.",
            "effectiveness_qualitative": "Effective at prioritizing plausible compositions and reducing search space; success contingent on training data quality and feature design.",
            "impact_potential": "High for accelerating materials discovery by screening large compositional spaces prior to expensive computations or experiments.",
            "comparison_to_alternatives": "More scalable and faster than exhaustive DFT screening; complementary to ab initio methods where ML guides candidate selection.",
            "success_factors": "Curated structure databases for training, informative compositional descriptors, and integration with validation pipelines (DFT/experiment).",
            "key_insight": "Supervised classifiers can efficiently predict structure-type adoption across large compositional spaces when trained on representative experimental data, enabling targeted experimental discovery.",
            "uuid": "e2337.11",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Elpasolite energy ML",
            "name_full": "Machine learning regression for screening energies of elpasolite (ABC2D6) crystals",
            "brief_description": "ML models trained to reproduce formation energies (or other energetic properties) across all combinatorial permutations of elpasolite stoichiometry, enabling virtual screening of millions of compositions.",
            "citation_title": "Machine learning energies of 2 million elpasolite (ABC2D6) crystals",
            "mention_or_use": "mention",
            "scientific_problem_domain": "materials discovery / high-throughput screening",
            "problem_description": "Rapidly estimate energies/properties across all combinatorial element substitutions for a given prototype structure to identify promising candidate materials.",
            "data_availability": "training data likely derived from high-throughput DFT calculations for a subset; overall screened set is very large (~2×10^6 combinations).",
            "data_structure": "tabular compositional descriptors mapping to scalar energies; relatively low-dimensional after encoding element identities and features.",
            "problem_complexity": "very large combinatorial search space (millions of candidates) but each instance prediction is a regression in descriptor space.",
            "domain_maturity": "emerging ML-driven high-throughput materials discovery with established workflows for DFT-calculated training data.",
            "mechanistic_understanding_requirements": "medium — ML can prioritize candidates, but DFT/experimental follow-up needed to confirm mechanistic and stability details.",
            "ai_methodology_name": "Supervised regression models (kernel ridge regression / other regressors)",
            "ai_methodology_description": "Train regression models on DFT-calculated energies for representative subsets to predict energies across the full combinatorial space, enabling fast screening and trend analysis.",
            "ai_methodology_category": "supervised learning (regression)",
            "applicability": "Highly applicable to prototype-based screening where structure is fixed and compositional permutations are the variable.",
            "effectiveness_quantitative": "Applied to screen ≈2×10^6 elpasolite combinations and report chemical trends and candidate identification (128 new materials cited in other contexts for different studies).",
            "effectiveness_qualitative": "Enabled exhaustive screening that would be infeasible with direct DFT for every composition, revealing trends and candidates for further study.",
            "impact_potential": "High — accelerates identification of candidate materials across vast combinatorial spaces and focuses expensive computations/experiments on promising leads.",
            "comparison_to_alternatives": "Far more computationally efficient than brute-force DFT across the entire space; quality depends on representativeness of training set.",
            "success_factors": "Representative DFT training set, suitable compositional descriptors, and validation with targeted calculations/experiments.",
            "key_insight": "ML regression trained on quantum calculations permits screening of massive compositional spaces for fixed structure prototypes, greatly expanding discovery throughput.",
            "uuid": "e2337.12",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "GANs / ORGAN for de novo molecules",
            "name_full": "Generative Adversarial Networks and ORGAN for de novo molecular generation",
            "brief_description": "Generative adversarial frameworks (GANs) and reinforcement-learning-augmented GANs (ORGAN) generate novel molecular structures (e.g., SMILES) biased toward desired chemical features or properties through an adversarial game and reward signals.",
            "citation_title": "Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models",
            "mention_or_use": "mention",
            "scientific_problem_domain": "molecular design / drug discovery / de novo generation",
            "problem_description": "Generating chemically valid and diverse novel molecules optimized for target properties (activity, physical properties) from learned distributions rather than exhaustive enumeration.",
            "data_availability": "large molecular libraries exist (public and proprietary); training data are typically abundant for many tasks but property labels may be limited for some objectives.",
            "data_structure": "discrete sequence data (SMILES) or graph representations; generation involves sequential token generation or graph construction.",
            "problem_complexity": "high — need to satisfy chemical validity constraints, optimize multiple objectives, and maintain diversity; generation is a high-dimensional discrete optimization problem.",
            "domain_maturity": "molecular generative modeling is a rapidly advancing field leveraging developments from NLP and generative modelling.",
            "mechanistic_understanding_requirements": "low-to-medium — black-box generation acceptable for ideation, but interpretability and synthetic accessibility are important for downstream utility.",
            "ai_methodology_name": "Generative Adversarial Networks (GANs) and ORGAN (reinforcement learning augmented GAN)",
            "ai_methodology_description": "GANs train a generator to produce samples and a discriminator to distinguish real vs generated molecules; ORGAN augments generator training with reinforcement learning reward functions to bias generation toward desired molecular properties.",
            "ai_methodology_category": "unsupervised / generative modeling (with reinforcement learning augmentation)",
            "applicability": "Very suitable for de novo design and exploration of chemical space, especially when large molecular corpora are available; requires mechanisms to ensure chemical validity and synthesizability.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated ability to produce chemically diverse molecules with tailored features; practical impact depends on quality of reward design and downstream filtering for synthesizability and ADMET.",
            "impact_potential": "High — enables rapid ideation of novel molecules and directed exploration of chemical space, potentially accelerating lead discovery.",
            "comparison_to_alternatives": "Generative models can explore chemical space beyond simple library enumeration or mutation-based approaches; must be compared to VAEs, reinforcement learning-only generators, and fragment-based methods on validity/diversity metrics.",
            "success_factors": "Large training corpora, appropriate molecular representations (SMILES/graphs), robust reward engineering, and post-generation filters for synthetic feasibility.",
            "key_insight": "Adversarial and reinforcement-augmented generative models can propose novel molecules tailored to objectives, but their utility hinges on reward design and integration with synthesizability assessments.",
            "uuid": "e2337.13",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Graph conv nets for molecular fingerprints",
            "name_full": "Convolutional networks on graphs for learning molecular fingerprints",
            "brief_description": "Graph convolutional neural networks learn task-specific molecular fingerprints directly from graph-structured molecular representations, improving property prediction and generalization.",
            "citation_title": "Convolutional networks on graphs for learning molecular fingerprints",
            "mention_or_use": "mention",
            "scientific_problem_domain": "molecular property prediction / cheminformatics",
            "problem_description": "Predicting molecular properties by automatically learning representations (fingerprints) from molecular graph structures rather than relying on hand-crafted descriptors.",
            "data_availability": "often large for common tasks (e.g., public QSAR datasets), but can be limited for specialized properties; labeled supervised data required.",
            "data_structure": "graph-structured data (atoms as nodes, bonds as edges); inherently relational and variable-sized.",
            "problem_complexity": "moderate-to-high due to graph combinatorics, need to capture local and global structural patterns, and variable molecular sizes.",
            "domain_maturity": "well-established chemoinformatics domain; graph-based deep learning is a modern, effective approach.",
            "mechanistic_understanding_requirements": "medium — learned fingerprints are less interpretable than simple descriptors but can be probed; predictive performance often prioritized.",
            "ai_methodology_name": "Graph convolutional neural networks (GCNNs)",
            "ai_methodology_description": "Neural networks that apply convolution-like operations over nodes and local neighborhoods on molecular graphs to produce fixed-length learned fingerprints used for downstream supervised prediction tasks.",
            "ai_methodology_category": "supervised representation learning / deep learning",
            "applicability": "Highly applicable for diverse molecular property prediction tasks; particularly effective when molecular structure is the primary determinant of the property.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Shown to outperform many traditional fingerprint-based models in various tasks, and to learn task-specific representations that improve generalization.",
            "impact_potential": "High — reduces need for hand-crafted descriptors and can improve predictive accuracy across many QSAR applications.",
            "comparison_to_alternatives": "Often outperforms classical fingerprint + shallow model pipelines, though depends on dataset size and task.",
            "success_factors": "Sufficient labeled data, appropriate network depth to capture relevant structural scales, and good message-passing/aggregation design.",
            "key_insight": "Learning molecular representations directly from graphs yields more expressive and task-adapted fingerprints, improving many property-prediction tasks over hand-crafted descriptors.",
            "uuid": "e2337.14",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Text mining for materials synthesis",
            "name_full": "Text mining / ML extraction of synthesis knowledge from scientific literature",
            "brief_description": "Combining text processing and machine learning to extract structured facts, reaction conditions, and synthesis insights from heterogeneous unstructured literature to create searchable databases and support decision-making.",
            "citation_title": "Materials synthesis insights from scientific literature via text extraction and machine learning",
            "mention_or_use": "mention",
            "scientific_problem_domain": "knowledge extraction / literature mining for materials science and chemistry",
            "problem_description": "Converting unstructured textual and supplemental scientific literature into structured, machine-readable datasets of synthesis conditions, experimental outcomes, and relationships.",
            "data_availability": "abundant in raw form (vast literature), but labeled/curated machine-readable datasets are limited and heterogenous; text is unstructured and noisy.",
            "data_structure": "unstructured text (articles, supplementary materials), figures, tables; requires NLP pipelines to produce structured tabular outputs.",
            "problem_complexity": "high due to variable writing styles, inconsistent metadata, and extraction of nuanced experimental details; linking dispersed information across documents is challenging.",
            "domain_maturity": "emerging application area with growing tools and datasets but lacking standardized reporting conventions; interdisciplinary (NLP + domain knowledge).",
            "mechanistic_understanding_requirements": "low in terms of extraction (facts are primary), but downstream usage for mechanistic modelling benefits from structured, interpretable outputs.",
            "ai_methodology_name": "Text mining with NLP and machine learning (named-entity recognition, relation extraction, classifiers)",
            "ai_methodology_description": "Use of tokenization, entity recognition, relation extraction, and ML classifiers to parse papers and supplemental data into structured fields (reagents, conditions, outcomes), often combined with domain ontologies and human curation.",
            "ai_methodology_category": "unsupervised / supervised NLP / information extraction",
            "applicability": "Highly applicable for building searchable synthesis databases and enabling meta-analyses; limited by variability of reporting and lack of standardized metadata.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Valuable for aggregating dispersed experimental knowledge and enabling downstream ML workflows; extraction accuracy depends on NLP model sophistication and labeled corpora.",
            "impact_potential": "High — can unlock hidden experimental knowledge across literature, support automated discovery, and improve data accessibility for ML modelling.",
            "comparison_to_alternatives": "Outperforms manual curation at scale but requires human-in-the-loop validation; complementary to structured data submission initiatives.",
            "success_factors": "Availability of labeled corpora for training, domain ontologies, and community moves toward standardized reporting and open data.",
            "key_insight": "Text mining can convert the vast unstructured literature into structured datasets that power ML-driven discovery, but success depends on NLP robustness and improvements in reporting standards.",
            "uuid": "e2337.15",
            "source_info": {
                "paper_title": "Machine learning for molecular and materials science",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "rating": 2,
            "sanitized_title": "planning_chemical_syntheses_with_deep_neural_networks_and_symbolic_ai"
        },
        {
            "paper_title": "Retrosynthetic reaction prediction using neural sequence-to-sequence models",
            "rating": 2,
            "sanitized_title": "retrosynthetic_reaction_prediction_using_neural_sequencetosequence_models"
        },
        {
            "paper_title": "Low data drug discovery with one-shot learning",
            "rating": 2,
            "sanitized_title": "low_data_drug_discovery_with_oneshot_learning"
        },
        {
            "paper_title": "Will it crystallise? Predicting crystallinity of molecular materials.",
            "rating": 2,
            "sanitized_title": "will_it_crystallise_predicting_crystallinity_of_molecular_materials"
        },
        {
            "paper_title": "Machine-learning-assisted materials discovery using failed experiments",
            "rating": 2,
            "sanitized_title": "machinelearningassisted_materials_discovery_using_failed_experiments"
        },
        {
            "paper_title": "Learning surface molecular structures via machine vision",
            "rating": 2,
            "sanitized_title": "learning_surface_molecular_structures_via_machine_vision"
        },
        {
            "paper_title": "Machine learning phases of matter",
            "rating": 2,
            "sanitized_title": "machine_learning_phases_of_matter"
        },
        {
            "paper_title": "Finding density functionals with machine learning",
            "rating": 2,
            "sanitized_title": "finding_density_functionals_with_machine_learning"
        },
        {
            "paper_title": "Bypassing the Kohn-Sham equations with machine learning. paving the way for higher-accuracy approaches",
            "rating": 2,
            "sanitized_title": "bypassing_the_kohnsham_equations_with_machine_learning_paving_the_way_for_higheraccuracy_approaches"
        },
        {
            "paper_title": "Gaussian approximation potentials: the accuracy of quantum mechanics, without the electrons",
            "rating": 2,
            "sanitized_title": "gaussian_approximation_potentials_the_accuracy_of_quantum_mechanics_without_the_electrons"
        },
        {
            "paper_title": "Ani-1: an extensible neural network potential with DFT accuracy at force field computational cost",
            "rating": 2,
            "sanitized_title": "ani1_an_extensible_neural_network_potential_with_dft_accuracy_at_force_field_computational_cost"
        },
        {
            "paper_title": "Machine learning energies of 2 million elpasolite (ABC2D6) crystals",
            "rating": 2,
            "sanitized_title": "machine_learning_energies_of_2_million_elpasolite_abc2d6_crystals"
        },
        {
            "paper_title": "Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models",
            "rating": 2,
            "sanitized_title": "objectivereinforced_generative_adversarial_networks_organ_for_sequence_generation_models"
        },
        {
            "paper_title": "Convolutional networks on graphs for learning molecular fingerprints",
            "rating": 2,
            "sanitized_title": "convolutional_networks_on_graphs_for_learning_molecular_fingerprints"
        },
        {
            "paper_title": "Materials synthesis insights from scientific literature via text extraction and machine learning",
            "rating": 2,
            "sanitized_title": "materials_synthesis_insights_from_scientific_literature_via_text_extraction_and_machine_learning"
        }
    ],
    "cost": 0.02947025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Machine learning for molecular and materials science</p>
<p>Keith T Butler 
ISIS Facility
Rutherford Appleton Laboratory</p>
<p>Harwell Campus
OX11 0QXUK</p>
<p>Daniel W Davies 
Department of Chemistry
University of Bath
BA2 7AYBathUK</p>
<p>Hugh Cartwright 
Department of Chemistry
Oxford University
OX1 3QZOxfordUK</p>
<p>Olexandr Isayev 
UNC Eshelman School of Pharmacy
University of North Carolina at Chapel Hill
Chapel Hill27599NCUSA</p>
<p>Aron Walsh 
Department of Materials Science and Engineering
Yonsei University
03722SeoulKorea</p>
<p>Department of Materials
Imperial College London
SW7 2AZLondonUK</p>
<p>Machine learning for molecular and materials science
E41E73705A080E740405925BCDFF3ACA10.1038/s41586-018-0337-22018 Document Version Peer reviewed version Link to publication Publisher Rights Unspecified This is the Authors' Accepted Manuscript of an article
University of BathGeneral rightsCopyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.Take down policyIf you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim.</p>
<p>The emergence of contemporary artificial intelligence (AI) methods has the potential to significantly alter, and enhance, the role of computers in science and engineering.The combination of big data and AI has been referred to as both the fourth paradigm of science 14 and the fourth industrial revolution, 15 and the number of applications in the chemical domain is growing at an astounding rate.A subfield of AI that has evolved rapidly in recent years is machine learning (ML).At the heart of ML applications lie statistical algorithms whose performance, much like that of a novice chemical researcher, improves with experience.There is a growing infrastructure of machine learning tools for generating, testing, and refining scientific models.Such techniques are suitable for addressing complex problems involving massive combinatorial spaces or nonlinear processes, which conventional procedures either cannot solve or can only tackle at great computational cost.</p>
<p>As the machinery for AI and ML matures, significant advances are being made not only by those in mainstream AI research, but also by experts in other fields (domain experts) who have the vision and the drive to adopt these approaches for their purposes.As we detail in the Learning to learn box, the resources and tools that facilitate the application of ML techniques by non-computer scientists mean that the barrier to entry is lower than ever.</p>
<p>In the rest of this Perspective, we discuss progress in the application of machine learning to meet challenges in molecular and materials research.We review the basics of machine learning approaches, identify areas where existing methods have the potential to accelerate research, and consider the developments required to enable more wide-ranging impacts.</p>
<p>Nuts and Bolts of Machine Learning</p>
<p>Given enough data, could a computer determine all known physical laws (and potentially also those that are currently unknown) without human input?Yes, given a rule-discovery algorithm.In traditional computational approaches, the computer is little more than a calculator, employing an algorithm provided by a human expert.By contrast, ML approaches learn the rules that underlie a dataset through assessment of a portion of that data.We consider in turn the basic steps involved in the construction of a model, as illustrated in Figure 1; this constitutes a blueprint of the generic workflow required for successful application of ML in a materials discovery process.</p>
<p>Data collection</p>
<p>Machine learning comprises models that learn from existing (training) data.Data may require initial pre-processing, during which missing or spurious elements are identified and handled.For example, the inorganic crystal structure database (ICSD) currently contains 188,000 entries, which have been checked for technical mistakes, but are still subject to human and measurement errors.Identifying and removing such errors is essential if ML algorithms are not to be misled by their presence.There is a growing public concern about the lack of reproducibility and error propagation of experimental data published in peerreviewed scientific literature.In certain fields like cheminformatics, best practices and guidelines are established to address these problems. 16e training of an ML model may be supervised, semi-supervised or unsupervised, depending upon the type and amount of available data.In supervised learning, the training data consist of sets of input and associated output values.The goal of the algorithm is to derive a function that, given a specific set of input values, predicts the output values to an acceptable degree of fidelity.If the available data set consists of only input values, unsupervised learning can be used in an attempt to identify trends, patterns or clustering in the data.Semi-supervised learning may be of value if there is a large amount of input data, but only a limited amount of corresponding output values.Supervised learning is the most mature and powerful of these approaches, and is used in most ML studies in the physical sciences, for example, in the mapping of chemical composition to a property of interest.Unsupervised learning is less common, but can be used for more general analysis and classification of data or to identify previously unrecognised patterns in large datasets 17 .</p>
<p>Data representation</p>
<p>Even though raw scientific data are usually numerical, the form in which data are presented often affects learning.In many types of spectroscopy, the signal is acquired in time-domain but for interpretation it is converted to a frequency-domain with the Fourier transform.Just like scientists, a ML algorithm may learn more effectively using one format rather than the other.The process of converting raw data into something more suitable for an algorithm is called featurisation or feature engineering.</p>
<p>The more suitable the representation of the input data, the more accurately can an algorithm map it to the output data.Selecting how best to represent the data may require insight into both the underlying scientific problem and the operation of the learning algorithm, since it is not always obvious which choice of representation will give the best performance; this is an active topic of research for chemical systems. 18ny representations are available to encode structures and properties.For example, the Coulomb matrix 19 contains information on atomic nuclear repulsion, as well as the potential energy of free atoms; the matrix is invariant to molecular translations and rotations.Molecular systems also lend themselves to description as graphs. 20In the solid-state, the conventional description of crystal structures by translation vectors and fractional coordinates of the atoms is not appropriate for ML, since a lattice can be represented in an infinite number of ways by choosing a different coordinate system.Representations based on radial distribution functions, 21 Voronoi tessellations, 22 and property-labelled materials fragments 23 are amongst the new ways in which this problem is being tackled.</p>
<p>Choice of learner</p>
<p>When the data set has been collected and represented appropriately, it is time to choose a model to represent it.A wide range of model types (or learners) exists for model building and prediction.Supervised learning models may predict output values within a discrete set (e.g. the categorisation of a material as a metal or an insulator) or a continuous set (e.g.polarisability).Building a model for the former requires classification, while the latter requires regression.A range of different learning algorithms can be applied (see Figure 2), depending on the type of data and the question posed.It may be helpful to use an ensemble of different algorithms, or of similar algorithms with different values for their internal parameters, ("bagging" or "stacking") to create a more robust overall model.</p>
<p>Common algorithms (learners) include:</p>
<p>Naïve Bayes 25 is a collection of classification algorithms based on Bayes' theorem that identify the most probable hypothesis, given the data as our prior knowledge about the problem.Bayes' theorem provides a formal way to calculate the probability that a hypothesis is correct, given a set of existing data.New hypotheses can then be tested and the prior knowledge updated.In this way one can select the hypothesis (or model) with the highest probability of correctly representing the data.</p>
<p>In nearest neighbour (k-NN) 26 methods the distances between samples and training data in a descriptor hyperspace are calculated.k-NN methods are so-called because the output value for a prediction relies on the values of the k nearest neighbours, where k is an integer.k-NN models can be used in both classification and regression models; in classification the prediction is determined by the class of the majority of the k nearest points, while in a regressor the value is the average of the k nearest points.</p>
<p>Decision trees 27 are flowchart-like diagrams used to determine a course of action or outcomes.Each branch of the tree represents a possible decision, occurrence or reaction.The tree is structured to show how and why one choice may lead to the next, with branches indicating that each option is mutually exclusive.Decision trees comprise a root node, leaf nodes, and branches.The root node is the starting point of the tree.Both root and leaf nodes contain questions or criteria to be answered.Branches are arrows connecting nodes, showing the flow from question to answer.Decision trees are often used in ensemble methods (meta-algorithms) that combine multiple trees into one predictive model in order to improve performance.</p>
<p>Kernel methods are a class of algorithms; whose best known members are the support vector machine (SVM) and kernel ridge regression (KRR). 28The name "kernel" comes from use of the kernel function, a "trick" that transforms input data into a high-dimensional representation, where the problem is easier to solve.In a sense, a kernel is a similarity function provided by the domain expert.It takes two inputs and, from them, creates an output that quantifies how similar they are.</p>
<p>Artificial neural networks (ANNs) and deep neural networks (DNNs) 29 loosely mimic the operation of the brain, with artificial neurons (the processing unit) arranged in input, output and hidden layers.In the hidden layers, each neuron receives input signals from other neurons, integrates those signals, and then uses the result in a straightforward computation.Connections between neurons have weights, the values of which represent the network's stored knowledge.Learning is the process of adjusting the weights so that the training data are reproduced as accurately as possible.</p>
<p>Whatever the model, most learners are not fully autonomous, requiring at least some guidance.The values of internal variables (hyperparameters) are estimated beforehand using systematic and random searches, or heuristics.Even modest changes in the values of hyperparameters may substantially improve or impair learning, and the selection of optimal values is often problematic.Consequently, the development of automatic optimisation algorithms is an area of active investigation, as is their incorporation into accessible packages for non-expert users (see Table 1).</p>
<p>Model optimisation</p>
<p>When the learner (or set of learners) has been chosen and predictions are being made, a trial model must be evaluated to allow for optimisation and ultimate selection of the best model.Three principal sources of error arise and must be taken into account: model bias, model variance, and irreducible errors.</p>
<p>Total Error = Bias + Variance + Irreducible Errors Bias is the error from incorrect assumptions in the algorithm and can result in the model missing underlying relationships.Variance on the other hand is sensitivity to small fluctuations in the training set.Even well-trained ML models may contain errors arising from noise in the training data, measurement limitations, calculation uncertainties, or simply outliers or missing data.Poor model performance usually indicates a high bias or a high variance, as illustrated in Fig. 3.</p>
<p>High bias (underfitting) occurs when the model is not flexible enough to adequately describe the relationship between inputs and predicted outputs, or when the data are insufficiently detailed to allow the discovery of suitable rules.High variance (overfitting) occurs when a model becomes too complex; typically this occurs as the number of parameters is increased.The diagnostic test for overfitting is that the accuracy of a model in representing training data continues to improve, whilst the performance in estimating test data plateaus or declines.</p>
<p>The key test for the accuracy of a machine learning model is its successful application to unseen data.A widely-used method to determine the quality of a model is to withhold a randomly-selected portion of data during training.This withheld data set, known as a test set, is shown to the model once training is complete (Figure 3).The extent to which the output data in the validation set is accurately predicted then provides a measure of the effectiveness of training.Cross-validation is reliable only when the samples used for training and validation are representative of the whole population, which may present problems if the sample size is small, or if the model is applied to data from compounds that are very different to those in the original dataset.A careful selection of methods to evaluate the transferability and applicability of a model are required in such cases.</p>
<p>Accelerating the Scientific Method</p>
<p>Whether through the enumeration and analysis of experimental data, or the codification of chemical intuition, the application of informatics to guide laboratory chemists is advancing rapidly.In this section, we explore how ML is helping to progress, and reduce the barriers between, the areas of chemical/materials design, synthesis, characterisation and modelling.We finally describe some of the important developments in the field of AI for data-mining existing literature.</p>
<p>Guiding chemical synthesis</p>
<p>Organic chemists were amongst the first scientists to recognise the potential of computational methods in laboratory practice.E.J. Corey's OCSS program, 33 developed more than 50 years ago, was an attempt to automate retrosynthetic analysis.In a synthetic chemistry route, the number of possible transformations per step can range from around 80 to several thousand, 34 which compares to the order of tens of potential moves at each game position in chess. 35In chemical synthesis, human experts are required to specify conditional and contextual rules, which exclude large sets of potential reagents at a given step, thus limiting the number of choices available to the algorithm.The contextual rules (typically many thousands of them) are of the utmost importance if a machine relying on a traditional algorithm is to compete with an expert.Recent breakthroughs in the Chematica program have shown that computers can be more efficient than humans in these tasks. 32e combination of extremely complex systems and huge numbers of potential solutions, arising from competing objective functions (cost, purity, time, toxicity etc.) make synthetic chemistry ill-suited to the application of traditional algorithmic approaches.However, because of this complexity, synthesis is one area of research that can benefit most from the application of artificial intelligence.</p>
<p>Deep learning approaches, which most commonly rely on many-layered ANNs or a combination of ANNs with other learning techniques such as Boltzmann machines, are showing particular promise for predicting chemical synthesis routes by combining rulesbased expert systems with neural networks that rank the candidates, 36 or rank the likelihood of a predicted product by applying the rules. 37One ANN that learned from chemical literature examples was able to achieve a level of sophistication such that trained chemists could not distinguish between computer and human expert designed routes. 34owever, a severe drawback of rules-based systems is that they have difficulty operating outside their knowledge base.</p>
<p>Alternatives to rules-based synthesis prediction have also been proposed, based on socalled 'sequence-to-sequence' approaches, rooted in the relationships between organic chemistry and linguistics.By casting molecules as text strings, these relationships have been applied in several chemical design studies. 38,39In sequence-to-sequence approaches a model is fed an input of products and then outputs reactants as a SMILES string. 40A similar approach has also been applied to retrosynthesis. 41Future developments in areas such as one-shot learning (as recently applied to drug discovery) 42 could lead to wider application in fields like natural product synthesis, where training data are scarce.</p>
<p>Beyond the synthesis of a target molecule, ML models can been applied to assess the likelihood that a product will crystallise.By applying feature selection techniques, Wicker and Cooper developed a two-parameter model, capable of predicting the propensity of a given molecule to crystallise with an accuracy of ~ 80%. 43Crucially this model had access to a training set of more than 20,000 crystalline and non-crystalline compounds.The availability of such open-access databases is pivotal for the further development of similar predictive models. 44Another study trained a model to predict the reaction conditions for new organically templated inorganic product formation with a success rate of 89%. 45less explored avenue of ML is how to best sample the set of possible experimental set-ups.Active learning predicts the optimal future experiments required to better understand a given problem.It was recently applied to understand the conditions for the synthesis and crystallisation of complex polyoxometalate clusters. 46Starting from initial data on failed and successful experiments, the ML approach then directed future experiments and was shown to be capable of covering six times as much crystallisation space as a human researcher in the same number of experiments.</p>
<p>Computational assistance for the planning and direction of chemical synthesis has come a long way since the early days of hand-coded expert systems.Much of this progress has been achieved in the past five years.Incorporation of AI-based chemical planners, with great advances in robotic synthesis 46 promises a rich new frontier in the production of new compounds.</p>
<p>Assisting multi-dimensional characterisation</p>
<p>The structure of molecules and materials is typically deduced by a combination of experimental methods, such as X-ray and neutron diffraction, magnetic and spin resonance, and vibrational spectroscopy.Each approach has a certain sensitivity and length-scale, and information from each method is complementary.Unfortunately, it is rare that data are fully assimilated into a coherent description of atomic structure.Analyses of individual streams often result in conflicting descriptions of the same compound. 47A solution would be to incorporate real-time data into the modelling with results that are then returned to the experiment, forming a feedback loop. 48ML offers the promise of a unifying framework allowing synergy of synthesis, imaging, theory and simulations.</p>
<p>The power of ML methods for enhancing the link between modelling and experiment has been demonstrated in the field of surface science.Combining ab initio simulations with multi-stage pattern recognition systems that use convolutional neural networks Ziatdinov and co-workers were able to characterize complex surface reconstructions. 49ML methods have also shown recent promise in areas such as microstructural characterisation 50 and the identification of interesting regions in large complex neutron scattering 3D volumetric datasets. 51A different example of ML opening new avenues in an area of complicated characterisation is phase transitions of highly-correlated systems; neural networks have been trained to encode phases of matter and thus identify transitions. 52</p>
<p>Enhancing theoretical chemistry</p>
<p>Modelling is now commonly considered as an equally important component to synthesis and characterisation for successful programmes of research.Using atomistic simulations, the properties of a molecule or material can, in principle, be calculated for any chemical composition and atomic structure.In practice, the computations rapidly grow in complexity as the size of the system increases, so considerable effort is devoted to finding short-cuts and approximations that might allow one to calculate properties to an acceptable degree of fidelity, without the need for unreasonable amounts of computer time.</p>
<p>Approaches based on DFT have been successful in predicting properties of many classes of compounds, offering generally high accuracy at reasonable cost.However, the Achilles heel of DFT remains the exchange-correlation functional that describes non-classical interactions between electrons.There are notable limitations of current approximations for weak chemical interactions (e.g.layered materials), highly correlated (d and f electron) systems, and the latest generation of quantum materials (e.g.iron pnictide superconductors), which often require a more expensive many-body Hamiltonian.Drawing from the growing number of structure-property databases (Table 2), accurate universal density functionals can be learned from data. 53,54Early examples include the Bayesian error estimation functional (BEEF) 55 as well as combinatorially-optimised DFT functionals. 56Going beyond the standard approach to DFT, the need to solve the Kohn-Sham equations is by-passed by learning density-to-energy and density-to-potential maps directly from training systems. 57ually challenging is the description of chemical processes across length and time scales, for example, the ubiquitous corrosion of metals in the presence of oxygen and water.The description of realistic chemical interactions (bond forming and breaking) including solvents, interfaces, and disorder is still limited by the computational cost of quantum mechanical approaches.The task of developing transferrable analytic forcefields is a well-defined problem for machine learning. 58,59It has been demonstrated that, in simple materials, approximate potential energy surfaces learned from quantum mechanical data can save orders of magnitude in processing cost. 60,61Whilst the combination of methods with varying levels of approximation is promising, much work is needed in the quantification and minimisation of error propagation across methods.In this context, initiatives for error estimation such as the DAKOTA package 62 are critically important.</p>
<p>Targeting discovery of new compounds</p>
<p>Until now we have considered how ML can be used to enhance and integrate the areas of synthesis, characterisation and modelling.However, ML can be used to reveal new ways to discover compounds.Models that relate system descriptors to desirable properties are already used to reveal structure-property relationships. 63,64So far, the fields of molecular (primarily pharmaceutical/medicinal) and materials chemistry have experienced different degrees of uptake of ML approaches to the design of new compounds, in part due to the challenges of representing the crystal structure and morphology of extended solids.</p>
<p>Crystalline solids</p>
<p>The application of ML to the discovery of functional materials is an emerging field.7][68] The complexity of games like "Go" is reminiscent of certain problems in materials science, 69,70 for example the description of on-lattice interactions that govern chemical disorder, magnetism, and ferroelectricity.Even for small unit cell representations, the number of configurations of a disordered crystal can quickly exceed the limitations of conventional approaches.An inverse-design procedure illustrated how such a combinatorial space for an alloy could be harnessed to realise specific electronic structure features. 71Similar inverse design approaches have also been applied in molecular chemistry to tailor ground and excited state properties. 72ediction of the likelihood of a composition to adopt a given crystal structure is a good example of a supervised classification problem in ML.Some recent examples involve the prediction of how likely a given composition is to adopt the so-called Heusler and half-Heusler crystal structures.One method predicts the likelihood a given composition will adopt the Heusler structure and is trained on experimental data. 73This approach was applied to screen hypothetical compositions and successfully identified 12 new gallide compounds, which were subsequently experimentally verified.Similarly, a random forest model was trained on experimental data to learn the probability that a given ABC stoichiometry would adopt the half-Heusler structure. 74s an alternative to learning from experimental data, calculated properties can be used as a training set for ML.Moot and co-workers showed how assessing the degree of similarity between electronic band structures could yield improved photocathodes for dye-sensitised solar cells. 75A ML model, trained to reproduce energies for the elpasolite crystal structure (ABC 2 D 6 ), was applied to screen all 2×10 6 possible combinations of elements that satisfy the formula, revealing chemical trends and identifying 128 new materials. 76Such models are expected to become a central feature in the next generation of high-throughput virtual screening procedures.</p>
<p>It is notable that the majority of crystal solid ML studies to date have concentrated on a particular crystal structure type.This is because of the difficulty of representing crystalline solids in a format which can easily be fed to a statistical learning procedure.By concentrating on a single structure type, the representation is inherently built into the model.Developing flexible, transferrable representations is one of the critical areas in ML for crystalline solids (see section 2 subsection "Data representation").As we will see below, the use of ML in molecular chemistry is more advanced than in the solid state, to a large extent this is due to greater ease with which molecules can be described in a manner amenable to algorithmic interpretation.</p>
<p>Molecular science</p>
<p>The QSAR (Quantitative Structure-Activity Relationship) approach is now a firmly established tool for drug discovery and molecular design.With the development of massive databases of assayed and virtual molecules, 77,78 methods for rapid, reliable virtual screening of these molecules for pharmacological (or other) activity are required to unlock their potential.QSARs can be described as the application of statistical methods to the problem of finding empirical relationships of the type P i = k'(D 1 ,D 2 , …, D n ), where P i is the property of interest, k' is a (most commonly linear) mathematical transformation and the D i are calculated or measured structural properties. 79ML has a long history in the development of QSARs, stretching back over half a century. 80lecular science is benefitting from cutting edge algorithmic developments in ML such as generative adversarial networks (GANs) 81 and reinforcement learning for the computational design of novel putative biologically active compounds.In a GAN, two models are trained simultaneously: a generative model G captures the distribution of data, and a discriminative model D estimates the probability that a sample came from the training set rather than G.The training procedure for G is to maximize the probability of D making an error (Figure 4).The ORGAN (Objective-Reinforced Generative Adversarial Networks) 82 model is capable of generating novel organic molecules from scratch.Such a model can be trained to produce diverse molecules that contain specific chemical features and physical responses, through a reward mechanism that resembles classical conditioning in psychology.Using reinforcement learning, one could bias newly generated chemical structures towards those with desired physical and biological properties (de novo design).</p>
<p>Reclaiming the literature</p>
<p>A final area for which we consider the recent progress of ML (across all disciplines) is tapping into the vast wealth of knowledge that already exists.While the scientific literature provides a wealth of information to researchers, it is increasingly difficult to navigate due to the proliferation of journals, articles, and databases.Text mining has become a popular approach to identify and extract information from unstructured text sources.This approach can be used to extract facts and relationships in a structured form to create specialised databases, to transfer knowledge between domains, and more generally to support research decision-making. 83Text mining is applied to answer many different research questions, ranging from the discovery of novel drug-protein target associations, or analysis of high throughput experiments, to developing systematic materials databases. 84Due to the heterogeneous nature of written resources, the automated extraction of relevant information is far from trivial.To address this, text mining has evolved into a sophisticated and specialised field where text processing and machine learning techniques are combined.</p>
<p>In the cases where supplemental data is provided with a publication, it is made available in various formats and databases, often without validated or standardised metadata.The issue of data and metadata interoperability is key.There are some leading examples of forward looking initiatives that are pushing accessible, reusable data in scientific research, such as The Molecular Sciences Software Institute (http://molssi.org)and the Open Science Monitor (https://ec.europa.eu/research/openscience).</p>
<p>Frontiers in Machine Learning</p>
<p>Many opportunities exist for further breakthroughs in ML to provide even greater advances in the automated design and discovery of molecules and materials.Here we highlight some frontiers in the field.</p>
<p>1.More knowledge from smaller data sets.ML approaches typically require large amounts of data for learning to be effective.While this is rarely an issue in fields such as image recognition, in which millions of input data sets are available, in chemistry or materials science.We are often limited to hundreds or thousands, if not fewer, highquality data points.We researchers need to become better at making the data associated with our publications accessible in computer readable form.Another promising solution to the problem of limited datasets is meta-learning, where knowledge is learned within and across problems. 85New developments such as neural Turing machines 86 or imitation learning 87 are enabling the realisation of this process.A Bayesian framework has recently been reported to achieve human-level performance on one-shot learning problems with limited data 88 , which has consequences for molecular and materials science where data is sparse and generally expensive and slow to obtain.2. Efficient chemical representations.The standard description of chemical reactions, in terms of composition, structure and properties has been optimised for human learning.Most machine learning approaches for chemical reactions or properties use molecular or atomic descriptors to build models, the success of which is determined by the validity and relevance of these descriptors.A good descriptor must be simpler to obtain than the target property and of as low dimensionality as possible. 89In the context of materials, useful descriptors 90 and new approaches for adapting simple existing heuristics for machine learning have been outlined; 91 however, much work remains to develop powerful new descriptions.In the field of molecular reactions exciting advances, such as the use of neural networks to create fingerprints for molecules in reactions are leading to advances in synthesis prediction. 92As has been demonstrated by the successful adoption of the concept of molecular fragments, 23 the field of crystalline materials design can learn much from advances in molecular nomenclature and representation.Chemists have a lot to learn from a field of representation learning i.e., learning representations of the data that make it easier to extract new information and knowledge.3. Quantum learning.While classical computing processes bits that are either 1 or 0, quantum computers use the quantum superposition of states to process qubits that are both 1 and 0 at the same time. 93This parallelisation leads to an exponential speedup in computational efficiency as the number of (qu)bits used increases. 94Quantum chemistry is a strong candidate to benefit, because solving Schrödinger's equation on a quantum computer has a natural fit. 95One of the challenges for quantum computing is knowing how to detect and correct errors that may occur in the data.Despite significant efforts in industry and academia, no error-corrected qubits has been built so far.Quantum machine learning explores the application of ML approaches to quantum problems, and vice versa, the application of quantum computing to ML problems.The possibility of exponential speedups in optimisation problems means that quantum machine learning has enormous potential.In problems such as optimising synthetic routes 96 or improving a given metric (e.g.optical absorption for solar energy materials) where multiple acceptable solutions exist, loss of qubit fidelity is less serious than when certainty is required.. Establishing new principles.Automatic discovery of scientific laws and principles [99][100] by inspection of the weights of trained ML systems is a potentially transformational development in science.Although models developed from machine learning are predictive, they are not necessarily (or even usually) interpretable; there are several reasons for this.First, the way in which a ML model represents knowledge rarely maps directly onto forms that scientists are familiar with.Given suitable data, an ANN might discover the Ideal Gas Law, pV=nRT, but the translation of connection weights to a formula, typically through statistical learning, is not trivial, even for a law this simple.A more subtle issue exists: the laws that underlie the behaviour of a material might depend upon knowledge that scientists do not yet possess, e.g. a many-body interaction giving rise to a new type of superconductivity.If an advanced ML system was able to learn key aspects of quantum mechanics, it is hard to envisage how its connection weights could be turned into a comprehensible theory if the scientist lacked understanding of a fundamental component of it.Finally, there may be scientific laws which at heart are so complex that, were they to be discovered by a ML system, would be too challenging for even a knowledgeable scientist to understand.A ML system that could discern and use such laws would truly be a computational black box.</p>
<p>As scientists embrace the inclusion of machine learning with statistically driven design in their research programmes, the number of applications is growing at an extraordinary rate.This new generation of computational science, supported by a platform of open source tools and data sharing, has the potential to revolutionise the molecular and materials discovery process.A simple model may suffer from high bias (underfitting), while a complex model may suffer from high variance (overfitting) leading to a bias-variance trade-off.The model shown here is built on an example from kaggle.com,available at https://keeeto.github.io/blog/bias_variance/.
Figure legends</p>
<p>Figure 4</p>
<p>The Generative Adversarial Networks (GAN) 81 approach to molecular discovery.Two models G (generator) and D (discriminator) play a continuous "game", where the generator is learning to produce more and more realistic samples, which can vary in structure and composition, and the discriminator is learning to get better and better at distinguishing fake data from real data.</p>
<p>Box 1</p>
<p>Learning to Learn</p>
<p>One of the most exciting aspects of machine learning techniques is their promise to democratise molecular and materials modelling, by reducing the computer power and prior knowledge required for entry.Just as Pople's Gaussian software made quantum chemistry more accessible to a generation of experimental chemists, ML approaches, if developed and implemented correctly, can broaden routine application of computer models by nonspecialists.The accessibility of ML technology relies critically on three factors: open data, open software and open education.There is an increasing drive to open data within the physical sciences and the best practice has been outlined in recent articles. 30,31Some of the open software being developed is listed in Table 1.There are also many excellent open education resources, such as massive open online courses (MOOCs) available.</p>
<p>http://www.fast.ai is a course that aims to "make neural nets uncool again"!One of the great advantages of fast.ai is that the novice user starts to build working machine learning models almost immediately.The course, however, is not for absolute beginners, and requires a working knowledge of computer programming and high-school level mathematics.</p>
<p>https://www.datacamp.comoffers an excellent introduction to coding for data-driven science, and covers many practical analysis tools relevant to chemical datasets.This course features extremely useful interactive environments to develop and test code and is suitable for non-coders, as it teaches the student Python at the same time as ML.</p>
<p>Academic MOOCs are the best locations for those who wish to get more involved with the theory and principles of AI and ML, as well as the practice.The Stanford MOOC (https://www.coursera.org/learn/machine-learning) is popular, with excellent alternatives available from sources such as https://www.edx.org(Learning from Data) and https://www.udemy.com(Machine Learning A-Z).The underlying mathematics is the topic of a course from Imperial College (https://www.coursera.org/specializations/mathematicsmachine-learning).</p>
<p>Data blogs and podcasts.Many ML professionals run informative blogs and podcasts dealing with specific aspects of ML practice.These are useful resources for general interest as well as broadening and deepening knowledge.There are too many to provide an exhaustive list here, but we do recommend https://machinelearningmastery.com and http://lineardigressions.com to get started.</p>
<p>Figure 1
1
Figure 1 Illustration of a machine learning workflow applied to interpret real world observations.It consists of four basic steps: (i) data collection -acquisition of data from experiment, simulations or other sources; (ii) data representation -processing of data to ensure its correctness, integrity and transformation into a form suitable for ML; (iii) choice of learner -selection of the types of ML model used to represent the problem; (iv) model optimisation -rigorous testing of the resultant model(s) to minimise error and choose the optimal representation.</p>
<p>Figure 2
2
Figure 2 Classes of machine learning techniques (following Ref. 24) and examples of problems that can be posed to them by a curious scientist.Whilst evolutionary algorithms are often integrated into machine learning procedures, they form part of a wider class of stochastic search algorithms.</p>
<p>Figure 3
3
Figure 3 Errors that arise in machine learning approaches, both during the training of a new model (blue line) and the application of a built model (red line).A simple model may suffer from high bias (underfitting), while a complex model may suffer from high variance (overfitting) leading to a bias-variance trade-off.The model shown here is built on an example from kaggle.com,available at https://keeeto.github.io/blog/bias_variance/.</p>
<p>Table 1 .
1
A collection of publically-accessible learning resources and tools relating to machine learning.
NAMEDESCRIPTIONURLGENERAL PURPOSE MACHINE LEARNING FRAMEWORKSCARETPackage for machine learning in Rtopepo.github.io/caretDEEPLEARNING4JDistributed deep learning for Javadeeplearning4j.orgH2O.AIMachine learning platform written in Javah2o.aithat can be imported as a Python or RlibraryKERASHigh-level neural networks API written inkeras.ioPythonMLPACKScalable machine learning library writtenmlpack.orgin C++SCIKIT-LEARNMachine learning and data miningscikit-learn.orgmember of the 'scikit' family of toolboxesbuilt around the SciPy Python librarySTATISTICS ANDMachine learning library for MATLABmathworks.com/machinelearningMACHINE LEARNINGTOOLBOXWEKACollection of machine learning algorithmscs.waikato.ac.nz/ml/wekaand tasks that can be applied directly orfrom Java codeMACHINE LEARNING TOOLS FOR MOLECULES AND MATERIALSAMPPackage to facilitate machine-learning forbitbucket.org/andrewpeterson/aatomistic calculationsmpANINeural network potentials for organicgithub.com/isayev/ASE_ANImolecules with python interfaceCOMBOPython library with emphasis on scalabilitygithub.com/tsudalab/comboand efficiencyDEEPCHEMPython library for deep learning ofdeepchem.iochemical systemsGAPGaussian Approximation Potentialslibatoms.org/Home/SoftwareMATMINERPython library for assisting machinehackingmaterials.github.io/matmilearning in materials sciencenerNOMADCollection of tools to explore correlationsanalytics-toolkit.nomad-coe.euin materials datasetsPROPHETCode to integrate machine learninggithub.com/biklooost/PROPhettechniques with quantum chemistryapproachesTENSORMOLNeural network chemistry packagegithub.com/jparkhill/TensorMol</p>
<p>Table 2 .
2
A representative collection of publically-accessible structure and property databases for molecules and solids that can be used to feed machine learning approaches.</p>
<p>Acknowledgments This work has been supported by the EPSRC (grant no.EP/M009580/1, EP/K016288/1 and EP/L016354/1), the Royal Society, and the Leverhulme Trust.O.I. acknowledges support from DOD-ONR (N00014-16-1-2311) and Eshelman Institute for Innovation award.Author Contributions All authors contributed equally to the design, writing, and editing of the manuscript.Author Information Reprints and permissions information is available at www.nature.com/reprints.The authors declare no competing financial interests.Readers are welcome to comment on the online version of the paper.Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Correspondence and requests for materials should be addressed to O.I. (olexandr@olexandrisayev.com) or A.W. (a.walsh@imperial.ac.uk).
Quantum mechanics of many-electron systems. P A M Dirac, Proc. R. Soc. London A Math. Phys. Eng. Sci. 1237141929</p>
<p>Quantum chemical models (Nobel Lecture). J A Pople, Angew. Chemie Int. Ed. 381999</p>
<p>Quantum chemistry program exchange, facilitator of theoretical and computational chemistry in pre-internet history. D B Boyd, ACS Symp. Ser. 11222013</p>
<p>Stable and efficient linear scaling first-principles molecular dynamics for 10000+ atoms. M Arita, D R Bowler, T Miyazaki, J. Chem. Theory Comput. 102014</p>
<p>Hybrid mpi-openmp parallelism in the Onetep linearscaling electronic structure code: application to the delamination of cellulose nanofibrils. K A Wilkinson, N D M Hine, C.-K Skylaris, J. Chem. Theory Comput. 102014</p>
<p>Efficient O(N) integration for all-electron electronic structure calculation using numeric basis functions. V Havu, V Blum, P Havu, M Scheffler, J. Comput. Phys. 2282009</p>
<p>Computational approaches to energy materials. A Walsh, A A Sokol, C R A Catlow, 2013Wiley-Blackwell</p>
<p>Inhomogeneous electron gas. P Hohenberg, W Kohn, Phys. Rev. 1361964</p>
<p>Self-consistent equations including exchange and correlation effects. W Kohn, L J Sham, Phys. Rev. 1401965</p>
<p>Reproducibility in density functional theory calculations of solids. K Lejaeghere, Science. 35130002016</p>
<p>The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid. J Hachmann, J. Phys. Chem. Lett. 22011</p>
<p>Commentary: the materials project: a materials genome approach to accelerating materials innovation. A Jain, APL Mater. 1110022013</p>
<p>The AFLOW standard for high-throughput materials science calculations. C E Calderon, Comput. Mater. Sci. 1082015</p>
<p>Perspective: materials informatics and big data: realization of the 'fourth paradigm' of science in materials science. A Agrawal, A Choudhary, APL Mater. 4532082016</p>
<p>The fourth industrial revolution. K Schwab, Foreign Affairs. 2015</p>
<p>Trust, but verify: on the importance of chemical structure curation in cheminformatics and QSAR modeling research. D Fourches, E Muratov, A Tropsha, J. Chem. Inf. Model. 502010</p>
<p>Generative topographic mapping (GTM): Universal tool for data visualization, structure-activity modeling and dataset comparison. N Kireeva, Mol. Inform. 312012</p>
<p>Prediction errors of molecular machine learning models lower than hybrid DFT error. F A Faber, J. Chem. Theory Comput. 132017</p>
<p>Fast and accurate modeling of molecular atomization energies with machine learning. M Rupp, A Tkatchenko, K.-R Müller, O A Von Lilienfeld, Phys. Rev. Lett. 108583012012</p>
<p>Chemical graph theory: introduction and fundamentals. D Bonchev, D H Rouvray, Erkenntnis. 681991</p>
<p>A radial distribution function description of periodic solids is adapted for ML models and applied to predict the electronic density of states for a range of materials. K T Schütt, Phys. Rev. B. 892051182014How to represent crystal structures for machine learning: towards fast prediction of electronic properties</p>
<p>Including crystal structure attributes in machine learning models of formation energies via Voronoi tessellations. L Ward, Phys. Rev. B. 96241042017</p>
<p>Universal fragment descriptors for predicting electronic properties of inorganic crystals. O Isayev, Nat. Commun. 8156792017</p>
<p>. P Domingos, Master Algorithm.2015Basic Books</p>
<p>Idiot's Bayes: not so stupid after all? Int. D J Hand, K Yu, Stat. Rev. / Rev. Int. Stat. 693852001</p>
<p>Nearest-neighbor methods in learning and vision: theory and practice. G Shakhnarovich, T Darrell, P Indyk, The MIT Press</p>
<p>Decision trees. in Data Mining and Knowledge Discovery Handbook 165-192. L Rokach, O Maimon, 2010</p>
<p>Kernel methods for pattern analysis. J Shawe-Taylor, N Cristianini, Elements. 2004</p>
<p>Deep learning in neural networks: an overview. J Schmidhuber, Neural Networks. 612015</p>
<p>Reproducible research in computational chemistry of materials. F.-X Coudert, Chem. Mater. 292017</p>
<p>Public (Q)SAR services, integrated modeling environments, and model repositories on the web: state of the art and perspectives for future development. I V Tetko, U Maran, A Tropsha, Mol. Inform. 3616000822017</p>
<p>Efficient syntheses of diverse, medicinally relevant targets planned by computer and executed in the laboratory. T Klucznik, Chem. 42018</p>
<p>Lhasa-logic and heuristics applied to synthetic analysis. D A Pensak, E J Corey, Computer-Assisted Organic Synthesis. 1977</p>
<p>A computer-driven retrosynthesis tool was trained on most published reactions in organic chemistry. M H S Segler, M Preuss, M P Waller, Nature. 5552018Planning chemical syntheses with deep neural networks and symbolic AI</p>
<p>Computer-assisted synthetic planning: the end of the beginning. S Szymkuć, Angew. Chemie Int. Ed. 552016</p>
<p>Neural-symbolic machine learning for retrosynthesis and reaction prediction. Chem. -A Eur. M H S Segler, M P Waller, J. 232017</p>
<p>Generation of crystal structures using known crystal structures as analogues. J C Cole, Acta Crystallogr. Sect. B Struct. Sci. Cryst. Eng. Mater. 722016</p>
<p>This study uses ML to guide all stages of a materials discovery workflow from quantum chemical calculations to materials synthesis. R Gómez-Bombarelli, Nat. Mater. 152016Design of efficient molecular organic light-emitting diodes by a highthroughput virtual screening and experimental approach</p>
<p>Learning to smile(s). S Jastrzębski, D Leśniak, W M Czarnecki, ArXiv 1602.062892016</p>
<p>Linking the neural machine translation and the prediction of organic chemistry reactions. J Nam, J Kim, ArXiv 1612.095292016</p>
<p>Retrosynthetic reaction prediction using neural sequence-to-sequence models. B Liu, ACS Cent. Sci. 32017</p>
<p>Low data drug discovery with one-shot learning. H Altae-Tran, B Ramsundar, A S Pappu, V Pande, ACS Cent. Sci. 32017</p>
<p>Will it crystallise? Predicting crystallinity of molecular materials. J G P Wicker, CrystEngComm. 172015</p>
<p>A crystal engineering application of ML to assess the probability of a given molecule forming a highquality crystal. </p>
<p>A publicly available crystallisation data set and its application in machine learning. M Pillong, CrystEngComm. 2017</p>
<p>The study trains an ML model to predict the success of a chemical reaction, incorporating the results of uncessfull attempts rather than siimply known (sucessful) reactions. P Raccuglia, Nature. 5332016Machine-learning-assisted materials discovery using failed experiments</p>
<p>An autonomous organic reaction search engine for chemical reactivity. V Dragone, V Sans, A B Henson, J M Granda, L Cronin, Nat. Commun. 8157332017</p>
<p>The problem with determining atomic structure at the nanoscale. S J L Billinge, I Levin, Science. 3162007</p>
<p>Big-deep-smart data in imaging for guiding materials design. S V Kalinin, B G Sumpter, R K Archibald, Nat. Mater. 142015</p>
<p>Learning surface molecular structures via machine vision. M Ziatdinov, A Maksov, S V Kalinin, Comput. Mater. 3312017</p>
<p>A new solution for automatic microstructures analysis from images based on a backpropagation artificial neural network. V H C De Albuquerque, P C Cortez, A R De Alexandria, J M R S Tavares, Nondestruct. Test. Eval. 232008</p>
<p>Volumetric data exploration with machine learning-aided visualization in neutron science. Y Hui, Y Liu, ArXiv. 171059942017</p>
<p>Machine learning phases of matter. J Carrasquilla, R G Melko, Nat. Phys. 132017</p>
<p>Identifying systematic dft errors in catalytic reactions. R Christensen, H A Hansen, T Vegge, Catal. Sci. Technol. 52015</p>
<p>Finding density functionals with machine learning. J C Snyder, M Rupp, K Hansen, K.-R Müller, K Burke, Phys. Rev. Lett. 1082530022012</p>
<p>Density functionals for surface science: exchange-correlation model development with bayesian error estimation. J Wellendorff, Phys. Rev. B. 852351492012</p>
<p>ωB97M-V a combinatorially optimized, range-separated hybrid, meta-GGA density functional with VV10 nonlocal correlation. N Mardirossian, M Head-Gordon, J. Chem. Phys. 1442016</p>
<p>This study transcends the standard approach to DFT by providing a direct mapping from density to energy. F Brockherde, Nat. Commun. 88722017Bypassing the Kohn-Sham equations with machine learning. paving the way for higher-accuracy approaches</p>
<p>First principles neural network potentials for reactive simulations of large molecular and condensed systems. J Behler, Angew. Chemie Int. Ed. 562017</p>
<p>Ani-1: an extensible neural network potential with DFT accuracy at force field computational cost. J S Smith, O Isayev, A E Roitberg, Chem. Sci. 82017</p>
<p>In this study, ML is used to fit interatomic potentials that reproduce the total energy and derivates from quantum mechanical calculations and enable accurate low-cost simulations. A P Bartók, M C Payne, R Kondor, G Csányi, Phys. Rev. Lett. 1041364032010Gaussian approximation potentials: the accuracy of quantum mechanics, without the electrons</p>
<p>Potential energy surfaces fitted by artificial neural networks. C M Handley, P L A Popelier, J. Phys. Chem. A. 1142010</p>
<p>Dakota | explore and predict with confidence. April 20185</p>
<p>Functional materials discovery using energy-structure-function maps. A Pulido, Nature. 5432017</p>
<p>Materials science with large-scale data and informatics: unlocking new opportunities. J Hill, MRS Bull. 412016</p>
<p>Computational materials design using artificial intelligence methods. N N Kiselyova, V P Gladun, N D Vashchenko, J. Alloys Compd. 2791998</p>
<p>Materials design and discovery with highthroughput density functional theory: the open quantum materials database (OQMD). J E Saal, S Kirklin, M Aykol, B Meredig, C Wolverton, Jom. 652013</p>
<p>Accelerating materials property predictions using machine learning. G Pilania, C Wang, X Jiang, S Rajasekaran, R Ramprasad, Sci. Rep. 328102013</p>
<p>Finding natures missing ternary oxide compounds using machine learning and density functional theory. G Hautier, C C Fischer, A Jain, T Mueller, G Ceder, Chem. Mater. 222010An early example of harnessing materials databases. Information on known compounds is used to construct an ML model to predict the viability of previously unreported chemisties</p>
<p>The quest for new functionality. A Walsh, Nat. Chem. 72015</p>
<p>Computational screening of all stoichiometric inorganic materials. D W Davies, 20161</p>
<p>The inverse band-structure problem of finding an atomic configurationwith given electronic properties. A Franceschetti, A Zunger, Nature. 4021999</p>
<p>Inverse strategies for molecular design. C Kuhn, D N Beratan, J. Phys. Chem. 1001996</p>
<p>High-throughput machine-learning-driven synthesis of full-heusler compounds. A O Oliynyk, Chem. Mater. 282016</p>
<p>Materials screening for the discovery of new half-heuslers: machine learning versus ab initio methods. F Legrain, J Carrete, A Van Roekeghem, G K H Madsen, N Mingo, J. Phys. Chem. B. 1222018</p>
<p>Material informatics driven design and experimental validation of lead titanate as an aqueous solar photocathode. T Moot, Mater. Discov. 62017</p>
<p>Machine learning energies of 2 million elpasolite (ABC 2 D 6 ) crystals. F A Faber, A Lindmaa, O A Von Lilienfeld, R Armiento, Phys. Rev. Lett. 1171355022016</p>
<p>Target, chemical and bioactivity databases -integration is key. T I Oprea, A Tropsha, Drug Discov. Today Technol. 32006</p>
<p>Zinc 15 -ligand discovery for everyone. T Sterling, J J Irwin, J. Chem. Inf. Model. 552015</p>
<p>Best practices for QSAR model development, validation, and exploitation. A Tropsha, Mol. Inform. 292010</p>
<p>P σ-π analysis. a method for the correlation of biological activity and chemical structure. C Hansch, T Fujita, J. Am. Chem. Soc. 861964</p>
<p>Generative adversarial networks. I J Goodfellow, Advances in Neural Information Processing Systems. 201427</p>
<p>Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models. G L Guimaraes, B Sanchez-Lengeling, C Outeiral, P L C Farias, A Aspuru-Guzik, arXiv 1705.108432017</p>
<p>Application of text mining in the biomedical domain. W W M Fleuren, W Alkema, Methods. 742015</p>
<p>Materials synthesis insights from scientific literature via text extraction and machine learning. E Kim, Chem. Mater. 292017</p>
<p>Meta-learning in computational intelligence. N Jankowski, W Duch, K Grabczewski, 2011Springer</p>
<p>A Graves, G Wayne, I Danihelka, arXiv 1410.5401Neural turing machines. 2014</p>
<p>One-shot imitation learning. Y Duan, ArXiv 1703.073262017</p>
<p>Human-level concept learning through probabilistic program induction. B M Lake, R Salakhutdinov, J B Tenenbaum, Science. 3502015</p>
<p>Big data of materials science: critical role of the descriptor. L M Ghiringhelli, J Vybiral, S V Levchenko, C Draxl, M Scheffler, Phys. Rev. Lett. 1141055032015</p>
<p>The high-throughput highway to computational materials design. S Curtarolo, Nat. Mater. 122013</p>
<p>Descriptors for machine learning of materials data. A Seko, A Togo, I Tanaka, 2018</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. D Duvenaud, Arxiv 1509.092922015</p>
<p>Quantum computing. A Steane, Rep. Prog. Phys. 611171998</p>
<p>Quantum algorithm for linear systems of equations. A W Harrow, A Hassidim, S Lloyd, Phys. Rev. Lett. 1031505022009</p>
<p>An early application of quantum computing to molecular problems. A quantum algorithm that scales linearly in the number of basis functions is demonstrated for calculating properties of chemical interest. A Aspuru-Guzik, A D Dutoi, P J Love, M Head-Gordon, Science. 3092005Simulated quantum computation of molecular energies</p>
<p>Elucidating reaction mechanisms on quantum computers. M Reiher, N Wiebe, K M Svore, D Wecker, M Troyer, Proc. Natl. Acad. Sci. U. S. A. 1142017</p>
<p>Quantum-enhanced machine learning. V Dunjko, J M Taylor, H J Briegel, Phys. Rev. Lett. 1171305012016</p>
<p>Quantum machine learning. J Biamonte, Nature. 5492017</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 3242009</p>
<p>Data-driven discovery of partial differential equations. S H Rudy, S L Brunton, J L Proctor, J N Kutz, Sci. Adv. 3e16026142017</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1729 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1729</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1729</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-263909457</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.07889v2.pdf" target="_blank">LangNav: Language as a Perceptual Representation for Navigation</a></p>
                <p><strong>Paper Abstract:</strong> We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision- and language-based representations for VLN. Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1729.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1729.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangNav (LLaMA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangNav: Language as a Perceptual Representation for Navigation (LLaMA / LLaMA2 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation agent that uses natural-language descriptions (from off-the-shelf captioning and object detectors) as the perceptual representation and finetunes a pretrained language model (LLaMA / LLaMA2 7B) to select discrete navigation actions; evaluated on R2R and in sim-to-real transfer from ALFRED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LangNav (LLaMA-7B / LLaMA2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pipeline: (1) perception converted to text via BLIP image captioning and Deformable DETR object detection for each panoramic view; (2) textual template builds per-candidate-view descriptions and history; (3) a pretrained LM (LLaMA-7B or LLaMA2-7B) is finetuned by behavior cloning to output which candidate view caption (or STOP) to select. Training uses imitation learning on trajectories (gold and augmented synthetic).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language model pretraining on large text corpora (LLaMA family) plus task-specific language instruction-following data (synthetic GPT-4 trajectories and ALFRED instruction-following pairs when used for pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Two kinds of task-level pretraining/augmentation used in experiments: (a) synthetic language trajectories: up to 10,000 trajectories generated by prompting GPT-4 (cost ~ $500 for 10k trajectories) from 10 R2R seed trajectories (and other variants: GPT-4-turbo/GPT-3.5 used in ablations); (b) ALFRED-derived instruction-following pairs: 128K instruction-following pairs (textualized observations) used to pretrain the LangNav policy in simulated environment. The underlying LLaMA / LLaMA2 backbones are pretrained LMs (open foundation models) but those general pretraining corpora are not enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) Vision-and-Language Navigation; also ALFRED (for pretraining / multi-task experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>R2R: real-world Matterport3D panoramic navigation where an egocentric agent receives a natural language instruction and must select navigation actions among discrete navigable panoramic view candidates to reach a goal (STOP within 3m). ALFRED: synthetic AI2-THOR environment tasks (split into GotoLocation sub-trajectories) used to generate instruction-following pairs for pretraining; ALFRED differs from R2R in appearance, step size, action discretization, and shorter/simpler instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Text pretraining / synthetic trajectories produce sequences of high-level navigation steps described in natural language (e.g., step descriptions like 'to your 30 degree right is "a living room with a couch"' and chosen caption strings); GPT-based outputs are purely language tokens representing chosen view captions or 'Stop'.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable-view selection in panoramic space (R2R: irregular set of candidate headings per step — up to 36 possible headings/elevations in theory but only navigable views considered); ALFRED: discrete actions like RotateLeft/RotateRight/MoveAhead consolidated (in experiments 4 MoveAhead steps were sometimes aggregated to match R2R step size and heading offsets of ±30° were added).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Language-to-action mapping is implemented by having the LM output the textual caption string corresponding to a candidate navigable view; that chosen caption is mapped back to the corresponding discrete navigation action (move to that view). Perception-to-language mapping uses BLIP captions and object-detection outputs inserted into fixed templates that enumerate candidate views and relative headings.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic views processed by BLIP image captioner and Deformable DETR object detector (ALFRED used ground-truth object detections in pretraining experiments). No explicit depth was required for the LangNav textual interface (though some baselines used ground-truth depth filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>In low-data regimes (10 or 100 seed R2R trajectories) finetuning LLaMA2-7B on a mixture of GPT-4 synthetic trajectories (e.g., 10k) plus the small in-domain seed set produced a LangNav agent that outperformed vision-based baselines finetuned on the same seed data; pretraining on ALFRED (128K pairs) gave additional transfer benefits when finetuning on R2R. Exact tabulated metrics in the paper show nontrivial SR / SPL improvements in the 10–100 shot regime (paper reports improvements vs RecBert/DuET baselines in these settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>With only the same 10/100 R2R seed trajectories and no synthetic augmentation or ALFRED pretraining, finetuned LLaMA (LangNav) performed comparably to vision-based policies but worse than when augmented with GPT-4 synthetic data; RecBert (vision baseline) pretraining on ALFRED hurt transfer (see paper). The paper reports that finetuning LLaMA2-7B only on 10/100 gold trajectories 'does not perform well, although it is comparable to the vision-based policies.'</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Demonstrated effective learning from as few as 10 seed in-domain trajectories when augmented by synthetic GPT-4 trajectories (experiment uses 10 real trajectories + 10,000 GPT-4 synthetic trajectories); pretraining on 128K ALFRED pairs also reduced need for in-domain R2R data (LangNav shows some zero-shot transfer from ALFRED to R2R).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Without synthetic augmentation or ALFRED pretraining, agent training relied directly on the available 10 / 100 seed R2R trajectories and showed substantially worse navigation performance; the paper frames the baseline as needing many more in-domain trajectories (hundreds/complete dataset) to reach strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative gain: enabled competitive performance in the extremely low-data regime (10–100 trajectories) by leveraging (a) GPT-4-generated synthetic language trajectories (10k) and/or (b) ALFRED pretraining (128K pairs). The paper does not present a single scalar multiplier (e.g., 'x times fewer') but demonstrates practical success where pure vision-based methods overfit or require more real trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Language abstracts away domain-specific visual details, yielding domain-invariant perceptual representations; (1) strong LM world knowledge distilled via GPT-4 synthetic trajectories; (2) object- and caption-based, object-centric textual summaries provide salient, navigation-relevant features; (3) disentangling perception (captions / detectors) from policy (LM) reduces overfitting to low-level visual idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations where language fails to capture fine-grained perceptual information ('a picture is worth a thousand words'): ambiguous or incorrect captions produce errors; LangNav underperforms state-of-the-art vision models in full-data regimes; action-space mismatches between ALFRED and R2R (different step sizes and heading discretizations) require adaptation; synthetic-instruction diversity can be limited when prompted from few seed scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining/augmentation in language space enables effective transfer to 3D embodied navigation in low-data regimes: (1) GPT-4 can synthesize spatially consistent, richly described language trajectories that, when used to finetune a smaller LM (LLaMA2-7B), improve few-shot performance on R2R; (2) LangNav pretrained on ALFRED (textualized instruction-following pairs) shows improved sim-to-real transfer to R2R versus a vision-first baseline (RecBert), indicating language can serve as a domain-invariant bridge; (3) however, language-only perceptual bottlenecks lose some information compared to continuous visual features and underperform in full-data settings, though combining language with vision features further improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangNav: Language as a Perceptual Representation for Navigation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1729.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1729.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used as data generator / zero/few-shot agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pretrained Transformer 4) - used via API</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source language model used in this work both to generate synthetic navigation instructions/trajectories (data augmentation) and as a zero-/few-shot navigation baseline when prompted with textual views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4 (API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Off-the-shelf strong LM accessed via API and prompted in two phases: (1) generate synthetic navigation instructions from 3 example human instructions; (2) generate step-by-step text-only trajectories conditioned on a template of view captions to produce synthetic instruction-following pairs (used to finetune smaller LM). Also evaluated as a zero-shot / few-shot navigation agent by directly prompting it to select captions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Proprietary large-scale internet and curated text corpora (not enumerated in paper); treated as a general-purpose text-only pretrained LM.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper (GPT-4 is referenced as a closed-source LM). The paper reports using GPT-4, GPT-4-turbo, and GPT-3.5 variants for synthetic data generation in ablations; the best quality synthetic data came from GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to generate synthetic language-only data for R2R navigation and evaluated as a text-only navigation agent on R2R-formatted textual observations</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>GPT-4 was not executed inside the 3D environment; instead it was prompted with textual templates of panoramic view captions to generate candidate-step selections or synthetic trajectories. Its outputs were mapped to discrete navigable-view choices for evaluation in the R2R setting.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Outputs are natural-language sequences indicating chosen view captions (or 'Stop') representing high-level navigation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Mapped to discrete panoramic candidate view moves in R2R (same mapping as LangNav: selected caption → move to corresponding view).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct: GPT-4 is given the same per-view textual templates as LangNav; the caption string it outputs is matched to the corresponding navigable action and executed in the simulator (or the generated trajectory is stored as synthetic data).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>None internal to GPT-4 — it operates on precomputed textual descriptions produced by BLIP / object detectors; therefore perception pipeline (BLIP/DETR) is required externally to produce GPT-4 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>As a synthetic-data generator, GPT-4 produced high-quality trajectories that, when used to finetune LLaMA2-7B, improved few-shot navigation performance; ablations show switching synthetic data source from GPT-4 to GPT-3.5 / GPT-4-turbo degraded results. As a direct zero-/few-shot navigation agent, GPT-4 zero/few-shot underperformed NavGPT and finetuned LangNav in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>N/A (GPT-4 itself is the pretrained LM). As a baseline, GPT-4 zero/few-shot performance was reported as worse than some specialized systems (NavGPT) and worse than LangNav after finetuning with synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Used to generate up to 10,000 synthetic trajectories from as few as 10 seed trajectories; the paper reports the generated 10k trajectories cost ≈ $500 via the API. Using 10k GPT-4 synthetic trajectories + 10/100 gold trajectories produced meaningful few-shot gains.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Practical result: GPT-4 generation enabled a small LM (LLaMA2-7B) to reach substantially better performance given the same tiny real dataset (10–100 trajectories) by augmenting with 10k synthetic trajectories; the paper does not give a single scalar multiplier but demonstrates that this augmentation materially reduced the amount of real data required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>GPT-4's strong world priors, spatial consistency in generated trajectories, and rich descriptive captions made synthetic trajectories realistic and useful for finetuning smaller models; quality of synthetic instructions correlated with final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Direct zero-shot application of GPT-4 to navigation without finetuning underperformed specialized prompting (NavGPT) likely due to missing ground-truth distance filtering and chain-of-thought style prompting used by NavGPT; potential overlap of training data with R2R instructions cannot be fully ruled out but trajectories (vision-based captions) are unlikely to be in GPT-4 pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large closed LMs (GPT-4) can be leveraged as high-quality data generators in the language space for embodied tasks: synthetic trajectories produced by GPT-4 are spatially consistent and rich and, when used to finetune a smaller LM, substantially improve few-shot embodied navigation performance. However, direct zero-/few-shot deployment of GPT-4 as a navigation policy is less competitive than using it for high-quality data synthesis or specialized prompting pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangNav: Language as a Perceptual Representation for Navigation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1729.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1729.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecBert (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecBert: A recurrent vision-and-language BERT for navigation (Hong et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language navigation baseline that processes continuous visual features (ResNet-152) and language instructions via a Transformer/BERT-like architecture; used in the paper as a vision-first baseline and for ALFRED→R2R pretraining experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RecBert (vision-language navigation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>RecBert is a recurrent vision-and-language model that encodes panoramic visual embeddings (ResNet) and instruction language (BERT) to predict navigation actions. In this paper RecBert is used as a baseline and is also pretrained on ALFRED (using MLM and single action prediction) and finetuned on R2R to test sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-and-language pretraining on simulated environment instruction-following data (ALFRED) and R2R masked-language modeling and action prediction tasks for few-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining performed on ALFRED (simulated) for transfer experiments; RecBert baselines in some tables were also pre-trained on few-shot R2R trajectories with MLM and SAP tasks per the implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R Vision-and-Language Navigation (transfer target); ALFRED (pretraining source)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same R2R / ALFRED tasks as for LangNav: agent must follow language instructions to navigate in panoramic 3D environments selecting discrete navigable views.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>During pretraining, tasks include predicting next action in terms of discrete navigable-view labels paired with language (SAP), and masked-language modeling over instruction tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigable-view selection (R2R) and ALFRED's smaller discrete action set (RotateLeft/RotateRight/MoveAhead etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Standard vision-language fusion: continuous image encodings (ResNet) combined with language encodings are mapped to action logits over discrete candidate navigation moves. No explicit text-only action mapping as in LangNav.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Continuous visual encodings from ResNet-152, plus textual instructions; when pretraining on ALFRED, ground-truth object detections were used in data creation/augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When pretrained on ALFRED and then finetuned on R2R with 0/10/100 trajectories, RecBert did not benefit from ALFRED pretraining — paper reports that pretraining on ALFRED actually hurt RecBert's performance and RecBert performed near chance with zero R2R data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>RecBert finetuned directly on the small in-domain R2R seed sets (10/100) performed comparably to LLaMA finetuned on the same small set but did not gain from ALFRED pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Pretraining on 128K ALFRED pairs did not improve RecBert sample efficiency for transfer to R2R in the experiments; the model overfit to simulated rendering idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>With only 10/100 R2R trajectories, RecBert achieved modest performance but required more in-domain data to match full-data baselines; exact sample counts to reach thresholds are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>None observed in the ALFRED→R2R transfer experiments; ALFRED pretraining degraded RecBert performance according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>N/A for successful transfer in this case; RecBert benefitted from being vision-grounded when sufficient in-domain data were available.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Overfitting to synthetic visual appearance (rendering differences), mismatch in step size and action discretization between ALFRED and R2R, and instruction complexity differences made ALFRED pretraining harmful for RecBert in sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vision-first models pretrained on simulated rendered environments (ALFRED) can overfit to rendering-specific details and fail to transfer to real-world panoramic environments (R2R); in contrast, language-mediated perceptual representations (LangNav) generalized better in the paper's sim-to-real transfer tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangNav: Language as a Perceptual Representation for Navigation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1729.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1729.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NavGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NavGPT: Explicit reasoning in vision-and-language navigation with large language models (Zhou et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that uses GPT-4 with image captioning/object information to perform navigation via language; cited and compared against in the paper as a high-performing GPT-4-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Navgpt: Explicit reasoning in vision-and-language navigation with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>NavGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A GPT-4 based navigation approach that uses language descriptions of observations plus explicit chain-of-thought and ground-truth distance/object filtering to make navigation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>GPT-4 pretrained on large-scale text (proprietary); NavGPT adds prompting strategies and filtered object/distance information for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not detailed in this paper beyond the cited reference; NavGPT uses ground-truth distance filtering for objects and chain-of-thought prompting per the NavGPT publication as stated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) via language-based perceptual representations</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>NavGPT executes navigation by selecting textualized view descriptions, leveraging ground-truth distance filtering to focus object information and chain-of-thought prompting to improve decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural-language reasoning over candidate view captions; outputs chosen caption representing next move.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable-view selection in panoramic R2R environment (mapping: chosen caption → move to that view).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prompt-driven: GPT-4's chosen caption mapped to the corresponding candidate action. NavGPT additionally uses ground-truth distance filtering of object detections and chain-of-thought prompting to improve selections.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Image captioning plus object detections with ground-truth distance filtering; textual templates of views provided to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>NavGPT is reported in the paper as a strong baseline that outperforms the authors' zero/few-shot GPT-4 prompts; the LangNav finetuned LLaMA2-7B approaches NavGPT performance despite being much smaller. Exact NavGPT numbers are cited from the NavGPT paper and included in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Use of ground-truth distance filtering for object information and chain-of-thought prompting improved navigation performance in NavGPT per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NavGPT demonstrates that powerful closed LMs (GPT-4) can be used directly for navigation with specialized prompting and filtered perception inputs; the LangNav approach can distill such capabilities into a much smaller LM via synthetic data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangNav: Language as a Perceptual Representation for Navigation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Navgpt: Explicit reasoning in vision-and-language navigation with large language models <em>(Rating: 2)</em></li>
                <li>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks <em>(Rating: 2)</em></li>
                <li>Room-to-Room: Vision-and-Language Navigation (R2R) <em>(Rating: 2)</em></li>
                <li>A recurrent vision-and-language BERT for navigation <em>(Rating: 2)</em></li>
                <li>BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation <em>(Rating: 1)</em></li>
                <li>Deformable DETR: Deformable Transformers for End-to-End Object Detection <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1729",
    "paper_id": "paper-263909457",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "LangNav (LLaMA-based)",
            "name_full": "LangNav: Language as a Perceptual Representation for Navigation (LLaMA / LLaMA2 backbone)",
            "brief_description": "A navigation agent that uses natural-language descriptions (from off-the-shelf captioning and object detectors) as the perceptual representation and finetunes a pretrained language model (LLaMA / LLaMA2 7B) to select discrete navigation actions; evaluated on R2R and in sim-to-real transfer from ALFRED.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "LangNav (LLaMA-7B / LLaMA2-7B)",
            "model_agent_description": "Pipeline: (1) perception converted to text via BLIP image captioning and Deformable DETR object detection for each panoramic view; (2) textual template builds per-candidate-view descriptions and history; (3) a pretrained LM (LLaMA-7B or LLaMA2-7B) is finetuned by behavior cloning to output which candidate view caption (or STOP) to select. Training uses imitation learning on trajectories (gold and augmented synthetic).",
            "pretraining_data_type": "Language model pretraining on large text corpora (LLaMA family) plus task-specific language instruction-following data (synthetic GPT-4 trajectories and ALFRED instruction-following pairs when used for pretraining).",
            "pretraining_data_details": "Two kinds of task-level pretraining/augmentation used in experiments: (a) synthetic language trajectories: up to 10,000 trajectories generated by prompting GPT-4 (cost ~ $500 for 10k trajectories) from 10 R2R seed trajectories (and other variants: GPT-4-turbo/GPT-3.5 used in ablations); (b) ALFRED-derived instruction-following pairs: 128K instruction-following pairs (textualized observations) used to pretrain the LangNav policy in simulated environment. The underlying LLaMA / LLaMA2 backbones are pretrained LMs (open foundation models) but those general pretraining corpora are not enumerated in the paper.",
            "embodied_task_name": "Room-to-Room (R2R) Vision-and-Language Navigation; also ALFRED (for pretraining / multi-task experiments)",
            "embodied_task_description": "R2R: real-world Matterport3D panoramic navigation where an egocentric agent receives a natural language instruction and must select navigation actions among discrete navigable panoramic view candidates to reach a goal (STOP within 3m). ALFRED: synthetic AI2-THOR environment tasks (split into GotoLocation sub-trajectories) used to generate instruction-following pairs for pretraining; ALFRED differs from R2R in appearance, step size, action discretization, and shorter/simpler instructions.",
            "action_space_text": "Text pretraining / synthetic trajectories produce sequences of high-level navigation steps described in natural language (e.g., step descriptions like 'to your 30 degree right is \"a living room with a couch\"' and chosen caption strings); GPT-based outputs are purely language tokens representing chosen view captions or 'Stop'.",
            "action_space_embodied": "Discrete navigable-view selection in panoramic space (R2R: irregular set of candidate headings per step — up to 36 possible headings/elevations in theory but only navigable views considered); ALFRED: discrete actions like RotateLeft/RotateRight/MoveAhead consolidated (in experiments 4 MoveAhead steps were sometimes aggregated to match R2R step size and heading offsets of ±30° were added).",
            "action_mapping_method": "Language-to-action mapping is implemented by having the LM output the textual caption string corresponding to a candidate navigable view; that chosen caption is mapped back to the corresponding discrete navigation action (move to that view). Perception-to-language mapping uses BLIP captions and object-detection outputs inserted into fixed templates that enumerate candidate views and relative headings.",
            "perception_requirements": "RGB panoramic views processed by BLIP image captioner and Deformable DETR object detector (ALFRED used ground-truth object detections in pretraining experiments). No explicit depth was required for the LangNav textual interface (though some baselines used ground-truth depth filtering).",
            "transfer_successful": true,
            "performance_with_pretraining": "In low-data regimes (10 or 100 seed R2R trajectories) finetuning LLaMA2-7B on a mixture of GPT-4 synthetic trajectories (e.g., 10k) plus the small in-domain seed set produced a LangNav agent that outperformed vision-based baselines finetuned on the same seed data; pretraining on ALFRED (128K pairs) gave additional transfer benefits when finetuning on R2R. Exact tabulated metrics in the paper show nontrivial SR / SPL improvements in the 10–100 shot regime (paper reports improvements vs RecBert/DuET baselines in these settings).",
            "performance_without_pretraining": "With only the same 10/100 R2R seed trajectories and no synthetic augmentation or ALFRED pretraining, finetuned LLaMA (LangNav) performed comparably to vision-based policies but worse than when augmented with GPT-4 synthetic data; RecBert (vision baseline) pretraining on ALFRED hurt transfer (see paper). The paper reports that finetuning LLaMA2-7B only on 10/100 gold trajectories 'does not perform well, although it is comparable to the vision-based policies.'",
            "sample_complexity_with_pretraining": "Demonstrated effective learning from as few as 10 seed in-domain trajectories when augmented by synthetic GPT-4 trajectories (experiment uses 10 real trajectories + 10,000 GPT-4 synthetic trajectories); pretraining on 128K ALFRED pairs also reduced need for in-domain R2R data (LangNav shows some zero-shot transfer from ALFRED to R2R).",
            "sample_complexity_without_pretraining": "Without synthetic augmentation or ALFRED pretraining, agent training relied directly on the available 10 / 100 seed R2R trajectories and showed substantially worse navigation performance; the paper frames the baseline as needing many more in-domain trajectories (hundreds/complete dataset) to reach strong performance.",
            "sample_complexity_gain": "Qualitative gain: enabled competitive performance in the extremely low-data regime (10–100 trajectories) by leveraging (a) GPT-4-generated synthetic language trajectories (10k) and/or (b) ALFRED pretraining (128K pairs). The paper does not present a single scalar multiplier (e.g., 'x times fewer') but demonstrates practical success where pure vision-based methods overfit or require more real trajectories.",
            "transfer_success_factors": "Language abstracts away domain-specific visual details, yielding domain-invariant perceptual representations; (1) strong LM world knowledge distilled via GPT-4 synthetic trajectories; (2) object- and caption-based, object-centric textual summaries provide salient, navigation-relevant features; (3) disentangling perception (captions / detectors) from policy (LM) reduces overfitting to low-level visual idiosyncrasies.",
            "transfer_failure_factors": "Limitations where language fails to capture fine-grained perceptual information ('a picture is worth a thousand words'): ambiguous or incorrect captions produce errors; LangNav underperforms state-of-the-art vision models in full-data regimes; action-space mismatches between ALFRED and R2R (different step sizes and heading discretizations) require adaptation; synthetic-instruction diversity can be limited when prompted from few seed scenes.",
            "key_findings": "Pretraining/augmentation in language space enables effective transfer to 3D embodied navigation in low-data regimes: (1) GPT-4 can synthesize spatially consistent, richly described language trajectories that, when used to finetune a smaller LM (LLaMA2-7B), improve few-shot performance on R2R; (2) LangNav pretrained on ALFRED (textualized instruction-following pairs) shows improved sim-to-real transfer to R2R versus a vision-first baseline (RecBert), indicating language can serve as a domain-invariant bridge; (3) however, language-only perceptual bottlenecks lose some information compared to continuous visual features and underperform in full-data settings, though combining language with vision features further improves performance.",
            "uuid": "e1729.0",
            "source_info": {
                "paper_title": "LangNav: Language as a Perceptual Representation for Navigation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (used as data generator / zero/few-shot agent)",
            "name_full": "GPT-4 (Generative Pretrained Transformer 4) - used via API",
            "brief_description": "A large closed-source language model used in this work both to generate synthetic navigation instructions/trajectories (data augmentation) and as a zero-/few-shot navigation baseline when prompted with textual views.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4 (API)",
            "model_agent_description": "Off-the-shelf strong LM accessed via API and prompted in two phases: (1) generate synthetic navigation instructions from 3 example human instructions; (2) generate step-by-step text-only trajectories conditioned on a template of view captions to produce synthetic instruction-following pairs (used to finetune smaller LM). Also evaluated as a zero-shot / few-shot navigation agent by directly prompting it to select captions.",
            "pretraining_data_type": "Proprietary large-scale internet and curated text corpora (not enumerated in paper); treated as a general-purpose text-only pretrained LM.",
            "pretraining_data_details": "Not specified in this paper (GPT-4 is referenced as a closed-source LM). The paper reports using GPT-4, GPT-4-turbo, and GPT-3.5 variants for synthetic data generation in ablations; the best quality synthetic data came from GPT-4.",
            "embodied_task_name": "Used to generate synthetic language-only data for R2R navigation and evaluated as a text-only navigation agent on R2R-formatted textual observations",
            "embodied_task_description": "GPT-4 was not executed inside the 3D environment; instead it was prompted with textual templates of panoramic view captions to generate candidate-step selections or synthetic trajectories. Its outputs were mapped to discrete navigable-view choices for evaluation in the R2R setting.",
            "action_space_text": "Outputs are natural-language sequences indicating chosen view captions (or 'Stop') representing high-level navigation decisions.",
            "action_space_embodied": "Mapped to discrete panoramic candidate view moves in R2R (same mapping as LangNav: selected caption → move to corresponding view).",
            "action_mapping_method": "Direct: GPT-4 is given the same per-view textual templates as LangNav; the caption string it outputs is matched to the corresponding navigable action and executed in the simulator (or the generated trajectory is stored as synthetic data).",
            "perception_requirements": "None internal to GPT-4 — it operates on precomputed textual descriptions produced by BLIP / object detectors; therefore perception pipeline (BLIP/DETR) is required externally to produce GPT-4 prompts.",
            "transfer_successful": true,
            "performance_with_pretraining": "As a synthetic-data generator, GPT-4 produced high-quality trajectories that, when used to finetune LLaMA2-7B, improved few-shot navigation performance; ablations show switching synthetic data source from GPT-4 to GPT-3.5 / GPT-4-turbo degraded results. As a direct zero-/few-shot navigation agent, GPT-4 zero/few-shot underperformed NavGPT and finetuned LangNav in many settings.",
            "performance_without_pretraining": "N/A (GPT-4 itself is the pretrained LM). As a baseline, GPT-4 zero/few-shot performance was reported as worse than some specialized systems (NavGPT) and worse than LangNav after finetuning with synthetic data.",
            "sample_complexity_with_pretraining": "Used to generate up to 10,000 synthetic trajectories from as few as 10 seed trajectories; the paper reports the generated 10k trajectories cost ≈ $500 via the API. Using 10k GPT-4 synthetic trajectories + 10/100 gold trajectories produced meaningful few-shot gains.",
            "sample_complexity_without_pretraining": "N/A.",
            "sample_complexity_gain": "Practical result: GPT-4 generation enabled a small LM (LLaMA2-7B) to reach substantially better performance given the same tiny real dataset (10–100 trajectories) by augmenting with 10k synthetic trajectories; the paper does not give a single scalar multiplier but demonstrates that this augmentation materially reduced the amount of real data required.",
            "transfer_success_factors": "GPT-4's strong world priors, spatial consistency in generated trajectories, and rich descriptive captions made synthetic trajectories realistic and useful for finetuning smaller models; quality of synthetic instructions correlated with final performance.",
            "transfer_failure_factors": "Direct zero-shot application of GPT-4 to navigation without finetuning underperformed specialized prompting (NavGPT) likely due to missing ground-truth distance filtering and chain-of-thought style prompting used by NavGPT; potential overlap of training data with R2R instructions cannot be fully ruled out but trajectories (vision-based captions) are unlikely to be in GPT-4 pretraining.",
            "key_findings": "Large closed LMs (GPT-4) can be leveraged as high-quality data generators in the language space for embodied tasks: synthetic trajectories produced by GPT-4 are spatially consistent and rich and, when used to finetune a smaller LM, substantially improve few-shot embodied navigation performance. However, direct zero-/few-shot deployment of GPT-4 as a navigation policy is less competitive than using it for high-quality data synthesis or specialized prompting pipelines.",
            "uuid": "e1729.1",
            "source_info": {
                "paper_title": "LangNav: Language as a Perceptual Representation for Navigation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RecBert (baseline)",
            "name_full": "RecBert: A recurrent vision-and-language BERT for navigation (Hong et al., 2021)",
            "brief_description": "A vision-language navigation baseline that processes continuous visual features (ResNet-152) and language instructions via a Transformer/BERT-like architecture; used in the paper as a vision-first baseline and for ALFRED→R2R pretraining experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "RecBert (vision-language navigation model)",
            "model_agent_description": "RecBert is a recurrent vision-and-language model that encodes panoramic visual embeddings (ResNet) and instruction language (BERT) to predict navigation actions. In this paper RecBert is used as a baseline and is also pretrained on ALFRED (using MLM and single action prediction) and finetuned on R2R to test sim-to-real transfer.",
            "pretraining_data_type": "Vision-and-language pretraining on simulated environment instruction-following data (ALFRED) and R2R masked-language modeling and action prediction tasks for few-shot baselines.",
            "pretraining_data_details": "Pretraining performed on ALFRED (simulated) for transfer experiments; RecBert baselines in some tables were also pre-trained on few-shot R2R trajectories with MLM and SAP tasks per the implementation details.",
            "embodied_task_name": "R2R Vision-and-Language Navigation (transfer target); ALFRED (pretraining source)",
            "embodied_task_description": "Same R2R / ALFRED tasks as for LangNav: agent must follow language instructions to navigate in panoramic 3D environments selecting discrete navigable views.",
            "action_space_text": "During pretraining, tasks include predicting next action in terms of discrete navigable-view labels paired with language (SAP), and masked-language modeling over instruction tokens.",
            "action_space_embodied": "Discrete panoramic navigable-view selection (R2R) and ALFRED's smaller discrete action set (RotateLeft/RotateRight/MoveAhead etc.).",
            "action_mapping_method": "Standard vision-language fusion: continuous image encodings (ResNet) combined with language encodings are mapped to action logits over discrete candidate navigation moves. No explicit text-only action mapping as in LangNav.",
            "perception_requirements": "Continuous visual encodings from ResNet-152, plus textual instructions; when pretraining on ALFRED, ground-truth object detections were used in data creation/augmentation.",
            "transfer_successful": false,
            "performance_with_pretraining": "When pretrained on ALFRED and then finetuned on R2R with 0/10/100 trajectories, RecBert did not benefit from ALFRED pretraining — paper reports that pretraining on ALFRED actually hurt RecBert's performance and RecBert performed near chance with zero R2R data.",
            "performance_without_pretraining": "RecBert finetuned directly on the small in-domain R2R seed sets (10/100) performed comparably to LLaMA finetuned on the same small set but did not gain from ALFRED pretraining.",
            "sample_complexity_with_pretraining": "Pretraining on 128K ALFRED pairs did not improve RecBert sample efficiency for transfer to R2R in the experiments; the model overfit to simulated rendering idiosyncrasies.",
            "sample_complexity_without_pretraining": "With only 10/100 R2R trajectories, RecBert achieved modest performance but required more in-domain data to match full-data baselines; exact sample counts to reach thresholds are not enumerated.",
            "sample_complexity_gain": "None observed in the ALFRED→R2R transfer experiments; ALFRED pretraining degraded RecBert performance according to the paper.",
            "transfer_success_factors": "N/A for successful transfer in this case; RecBert benefitted from being vision-grounded when sufficient in-domain data were available.",
            "transfer_failure_factors": "Overfitting to synthetic visual appearance (rendering differences), mismatch in step size and action discretization between ALFRED and R2R, and instruction complexity differences made ALFRED pretraining harmful for RecBert in sim-to-real transfer.",
            "key_findings": "Vision-first models pretrained on simulated rendered environments (ALFRED) can overfit to rendering-specific details and fail to transfer to real-world panoramic environments (R2R); in contrast, language-mediated perceptual representations (LangNav) generalized better in the paper's sim-to-real transfer tests.",
            "uuid": "e1729.2",
            "source_info": {
                "paper_title": "LangNav: Language as a Perceptual Representation for Navigation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "NavGPT",
            "name_full": "NavGPT: Explicit reasoning in vision-and-language navigation with large language models (Zhou et al., 2023a)",
            "brief_description": "A prior system that uses GPT-4 with image captioning/object information to perform navigation via language; cited and compared against in the paper as a high-performing GPT-4-based baseline.",
            "citation_title": "Navgpt: Explicit reasoning in vision-and-language navigation with large language models",
            "mention_or_use": "mention",
            "model_agent_name": "NavGPT",
            "model_agent_description": "A GPT-4 based navigation approach that uses language descriptions of observations plus explicit chain-of-thought and ground-truth distance/object filtering to make navigation decisions.",
            "pretraining_data_type": "GPT-4 pretrained on large-scale text (proprietary); NavGPT adds prompting strategies and filtered object/distance information for navigation.",
            "pretraining_data_details": "Not detailed in this paper beyond the cited reference; NavGPT uses ground-truth distance filtering for objects and chain-of-thought prompting per the NavGPT publication as stated in this paper.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) via language-based perceptual representations",
            "embodied_task_description": "NavGPT executes navigation by selecting textualized view descriptions, leveraging ground-truth distance filtering to focus object information and chain-of-thought prompting to improve decision-making.",
            "action_space_text": "Natural-language reasoning over candidate view captions; outputs chosen caption representing next move.",
            "action_space_embodied": "Discrete navigable-view selection in panoramic R2R environment (mapping: chosen caption → move to that view).",
            "action_mapping_method": "Prompt-driven: GPT-4's chosen caption mapped to the corresponding candidate action. NavGPT additionally uses ground-truth distance filtering of object detections and chain-of-thought prompting to improve selections.",
            "perception_requirements": "Image captioning plus object detections with ground-truth distance filtering; textual templates of views provided to GPT-4.",
            "transfer_successful": null,
            "performance_with_pretraining": "NavGPT is reported in the paper as a strong baseline that outperforms the authors' zero/few-shot GPT-4 prompts; the LangNav finetuned LLaMA2-7B approaches NavGPT performance despite being much smaller. Exact NavGPT numbers are cited from the NavGPT paper and included in comparisons.",
            "performance_without_pretraining": "",
            "sample_complexity_with_pretraining": "",
            "sample_complexity_without_pretraining": "",
            "sample_complexity_gain": "",
            "transfer_success_factors": "Use of ground-truth distance filtering for object information and chain-of-thought prompting improved navigation performance in NavGPT per the paper's discussion.",
            "transfer_failure_factors": "",
            "key_findings": "NavGPT demonstrates that powerful closed LMs (GPT-4) can be used directly for navigation with specialized prompting and filtered perception inputs; the LangNav approach can distill such capabilities into a much smaller LM via synthetic data generation.",
            "uuid": "e1729.3",
            "source_info": {
                "paper_title": "LangNav: Language as a Perceptual Representation for Navigation",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Navgpt: Explicit reasoning in vision-and-language navigation with large language models",
            "rating": 2,
            "sanitized_title": "navgpt_explicit_reasoning_in_visionandlanguage_navigation_with_large_language_models"
        },
        {
            "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "Room-to-Room: Vision-and-Language Navigation (R2R)",
            "rating": 2,
            "sanitized_title": "roomtoroom_visionandlanguage_navigation_r2r"
        },
        {
            "paper_title": "A recurrent vision-and-language BERT for navigation",
            "rating": 2,
            "sanitized_title": "a_recurrent_visionandlanguage_bert_for_navigation"
        },
        {
            "paper_title": "BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation",
            "rating": 1,
            "sanitized_title": "blip_bootstrapping_languageimage_pretraining_for_unified_visionlanguage_understanding_and_generation"
        },
        {
            "paper_title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
            "rating": 1,
            "sanitized_title": "deformable_detr_deformable_transformers_for_endtoend_object_detection"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.02178225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LangNav: Language as a Perceptual Representation for Navigation
30 Mar 2024</p>
<p>Bowen Pan bpan@mit.edu 
Dartmouth College</p>
<p>Rameswar Panda rpanda@ibm.com 
Dartmouth College</p>
<p>Souyoung Jin souyoung.jin@dartmouth.edu 
Dartmouth College</p>
<p>Rogerio Feris rsferis@us.ibm.com 
Dartmouth College</p>
<p>Aude Oliva oliva@mit.edu 
Dartmouth College</p>
<p>Phillip Isola phillipi@mit.edu 
Dartmouth College</p>
<p>Yoon Kim yoonkim@mit.edu 
Dartmouth College</p>
<p>Mit Csail 
Dartmouth College</p>
<p>Mit-Ibm Watson 
Dartmouth College</p>
<p>A I Lab 
Dartmouth College</p>
<p>LangNav: Language as a Perceptual Representation for Navigation
30 Mar 202459AE3DA2A6DB2868CAF9AAF76F59C30FarXiv:2310.07889v2[cs.CV]
We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings.Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent's egocentric panoramic view at each time step into natural language descriptions.We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions.In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation.We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision-and language-based representations for VLN.Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.</p>
<p>Introduction</p>
<p>Applications of large language models (LMs) to non-linguistic embodied tasks have generally focused on using the implicit world knowledge within LMs to predict sub-tasks and actions for planning (Ahn et al., 2022;Huang et al., 2022b,a;Singh et al., 2022).For instance, recent work has shown that LMs can be prompted to create a list of actions (e.g., GoToBathroom, LocateToothbrush) given a highlevel goal given in natural language (e.g., "brush teeth") (Huang et al., 2022a).These approaches rely on the LM's priors on action sequences and inter-object correlations acquired through largescale pretraining (Zhou et al., 2023b;Li et al., 2023;Zhao et al., 2023), and it has not been clear whether text-only models can be finetuned for tasks such as vision-and-language navigation which requires an egocentric agent follow instructions to navigate a 3D environment using visual input.</p>
<p>To be clear, there is a substantial body of work on using pretrained LMs for vision-and-language navigation tasks (Hong et al., 2021;Qi et al., 2021;Qiao et al., 2022, inter alia).The standard approach is to use a pretrained LM over the natural language instructions to extract text features that are combined with the agent's perceptual representations, which are given by continuous image features extracted from pretrained vision models (Wang et al., 2019;Hao et al., 2020).While effective in data-rich regimes, the direct use of vision features makes the approach difficult to apply in cases where only a few labeled trajectories exist (e.g., 10 trajectories), as these approaches need to learn a full joint vision-language module that combines a pretrained vision model with a pretrained text model.A popular strategy in such low data regimes is to generate synthetic data or transfer knowledge from other domains.However, generating realistic perception data is itself a difficult task, and domain transfer with models that rely purely on visual features can overfit to the non-transferable features (Anderson et al., 2021).</p>
<p>This paper explores an alternative approach for vision-and-language navigation by exploiting language itself as the perceptual representation space.Our approach uses off-the-shelf vision models to obtain textual descriptions of the agent's egocentric panoramic view.The text descriptions are then fed to an LM which must select the next action given the instruction and (text descriptions of) the previous actions or observations.See Figure 1 for an overview.The use of language to represent an You are a navigation agent who must navigate according to instructions given only descriptions of your current position via natural language.The natural language description is sometimes incorrect.</p>
<p>Instruction: "Go down the stairs and straight into the living room.In the living room walkout onto the patio.On the patio stop outside the doorway."</p>
<p>[Trajectory History]</p>
<p>Step 4: To your straight ahead is, "a living room with a couch a table and chairs" To your left is, "a modern kitchen with a stainless steel refrigerator" Behind you is, "a long hallway with wooden steps leading to a black door" You go towards: "Go down the stairs and straight into the living room.In the living room walkout onto the patio.On the patio stop outside the doorway."</p>
<p>Language Instructions and Visual Images</p>
<p>LangNav Agent</p>
<p>I should go towards: "a living room with a couch a table and chairs"</p>
<p>Converting Observations into Language Prompts Observations at Time Step 4</p>
<p>A B C</p>
<p>Figure 1: Overview of language-based navigation (LangNav).We describe the task instructions and visual observations (from off-the-shelf vision systems) through text.A language model is then finetuned to predict which direction to move towards based on the language descriptions.Here, views A, B, and C correspond to the front, left, and rear views of the agent.</p>
<p>agent's perceptual field makes it possible to readily utilize the myriad capabilities of language models, especially when the training data is limited.In our first case study, we show how we can use a small amount of seed training data (10-100 trajectories) to cheaply obtain synthetic "trajectories" from a powerful but closed-source LM (GPT-4;OpenAI, 2023).We find that finetuning a smaller language model (LLaMA/LLaMA2; Touvron et al., 2023a,b) on the generated trajectories mixed with the original seed data results in a langauge-based navigation agent that outperforms a vision-based agent that is finetuned on the same seed data.In our second study, we explore the use of language as a domain-invariant representation to perform domain transfer, where we transfer an agent trained on a computer-generated environment (ALFRED; Shridhar et al., 2020) to the real-world R2R (Anderson et al., 2018b) environment.Insofar as language is hypothesized to have co-evolved with the human brain to enable efficient communication (Deacon, 1997), it naturally abstracts away low-level perceptual details, and we indeed find that LangNav exhibits improved transfer compared to the visionbased agent.We further show that language can provide further benefits even in the presence of vision-based features.Our results collectively suggest that language as a perceptual representation can be helpful in the low-data navigation settings.</p>
<p>2 Background: Room-to-Room Vision-language Navigation</p>
<p>A popular testbed for vision-and-language navigation (VLN) is the room-to-room dataset (R2R; Anderson et al., 2018b), in which an agent must perceive and navigate a real-world 3D environment based on a language instruction U and an initial state S 0 .At each time step t, the agent uses the current observation O t , the original language instructions U , and the trajectory history H t , to predict the panoramic action a t .The current observation is given by a set of panoramic images that describe the agent's egocentric view, i.e., O t = {I t,0 , ..., I t,V } where V corresponds to the number of discretized view angles. 1 The panoramic action a t corresponds to which navigable view in O t to go towards, i.e., a t ∈ O t .After selecting an action, the state transitions from S t to S t+1 .The aim is to output the command STOP after reaching the goal G specified by U in state S 0 .The standard approach in R2R is to process the panoramic images {I t,0 , ..., I t,V } with a pretrained visual encoder E v to extract continuous visual features F t,v = {E v (I t,0 ), ..., E(I t,V )}.The language instruction is typically processed by a pretrained language encoder E l (e.g., BERT (Devlin et al., 2019)) to extract the language features F l = E l (U ).These features, along with a hidden state representation of the trajectory history h t−1 , are fed to a joint vision-language module (e.g., another Transformer) that attends over {I t,0 , ..., I t,V } to select the action a t .</p>
<p>Language as a Perceptual Representation for Navigation</p>
<p>We begin by describing the perception-to-text models employed for converting visual observations into text ( § 3.1).We then discuss the prompt templates for converting the text into natural language ( § 3.2), followed by a description of the offline imitation learning algorithm for learning ( § 3.3).</p>
<p>Vision-to-text System</p>
<p>We use off-the-shelf vision models to convert visual observations into language descriptions.Specifically, we use an image captioning model (BLIP; Li et al., 2022a) and an object detection model (Deformable DETR; Zhu et al., 2020) over each view angle I t,j to obtain the text descriptions, C t,j = IMAGECAPTIONER(I t,j ),</p>
<p>x t,j,0 , . . ., x t,j,M = OBJECTDETECTOR(I t,j ),</p>
<p>where M is the number of detected objects. 2</p>
<p>Prompt Templates</p>
<p>Figure 1 illustrates how the image caption and the detected objects are combined via templates to construct pieces of text on which to condition the language model.Based on the prompt template, the language model will be finetuned on the (language representations of) output actions {a 1 , . . ., a T }.We briefly describe the prompt template (see appendix G for a full example).</p>
<p>Task description D. The task description is given by:</p>
<p>You are a navigation agent who must navigate according to instructions given only descriptions of your current [...].</p>
<p>Navigation instruction U .The navigation instruction, which provides instructions to the agent on how to reach the goal, can be from R2R (our main dataset), synthesized by GPT-4 (for data augmentation), or ALFRED (for domain transfer).An example instruction from R2R is:</p>
<p>Travel forward past the wall with all the light switches and into the first room on your right.</p>
<p>Current observation O t .We use templates to convert the image caption C t,j and objects obtained x t,j,0 , • • • , x t,j,M from I t,j ( § 3.1).For instance, if the agent is facing a heading of 90 degrees and an elevation of 0 degrees and there is a candidate navigable direction I t,j located at a heading of 120 degrees and an elevation of 0 degrees, the text description for this view angle would be:</p>
<p>2 We did not experiment much with different off-the-shelf vision systems and quickly converged on these two models which seemed to produce reasonable results.Since LangNav separates perception from navigation, we expect that advances made in perception (e.g., through better captioning systems) will automatically result in improvements to our system, which is a nontrivial advantage of our approach compared to systems that entangle perception and navigation into a single model.</p>
<p>To your 30 degree right is "{Ct,j}".Details: {xt,j,0}, . . ., {xt,j,M }.</p>
<p>We create such templates for all the navigable view angles {I t,0 , . . ., I t,V }.Action a t .Selecting an action involves choosing a navigable view out of O t to move towards, i.e., a t ∈ O t .For example, suppose a t = I t,j , i.e., the agent decided to go to the j-th view angle.Then this is recorded as: This history serves to inform the model about its current position within the high-level instruction, enabling it to make more informed decisions when selecting actions.</p>
<p>Imitation Learning on Demonstrations</p>
<p>We create an instruction-following dataset by transforming the expert trajectory from the original dataset into instruction-following demonstrations.Formally, let D = {W (i) } N i=1 be the set of training trajectories, where each W (i) can be represented as a natural language sequence from the above template,
W (i) = (D (i) , U (i) , H (i) 1 , O (i) 1 , a (i) 1 , . . . , H (i) T (i) , O (i) T (i) , a (i) T (i) ).
Here T (i) is the number of actions in the example W (i) , which is typically between 5 to 7. Given the above, we optimize the log likelihood of the (language descriptions of) actions, i.e., the objective for trajectory W (i) is given by,
T (i) t=1 log p LM (a (i) t | D (i) , U (i) , H (i) t , O (i) t ).
While behavior cloning on gold trajectories is simple, it is prone to error propagation.In particular, the history trajectory is obtained by a shortestpath algorithm (which has knowledge of the goal)</p>
<p>I am going to give you example instructions [……].</p>
<p>-{real_instruction_1} -{real_instruction_2} -{real_instruction_3} Your goal is to write 10 more instructions like the above [……] make sure that the instruction can be completed by an agent in 5 to 7 steps.</p>
<p>Phase I: Prompting GPT-4 to generate instructions</p>
<p>GPT-4 API random sampling</p>
<p>Phase II: Prompting GPT-4 to generate the trajectory</p>
<p>Here is an example of [……] following template: To your [VIEW] is [CAPTION], where [……] #Example 1 ### Instruction: Go to the right of the entrance, [……] ### Trajectory:</p>
<p>Step 1: and thus adheres closely to an optimal policy π * .However, during prediction, trajectories can deviate significantly from the optimal policy, leading to a distribution shift that can adversely affect performance.To allow for the policy to recover from deviations from the optimal path, we adopt the following strategy to create our imitation learning dataset: (1) at each time step, we sample a random action with probability ρ;</p>
<p>(2) once a random action is selected, we use the shortest-path algorithm to obtain the ground truth next action;</p>
<p>(3) we repeat this process until the goal is reached; (4) once the goal is reached, this becomes part of the training demonstration data.(See appendix F for details.)</p>
<p>Empirical Study</p>
<p>Our primary experiments with LangNav target the low-data setting, motivated by the observation that obtaining annotated data for embodied tasks such as vision-language navigation can be very costly (often more so than is the case for text-only or vision-only tasks).Specifically, we are interested in learning the most performant system based on a small number (10 or 100) of in-domain seed navigation trajectories.We sample our seed trajectories from the Room-to-Room (R2R) dataset (Anderson et al., 2018b), a popular vision-and-language navigation dataset consisting of 21,567 navigation instructions in the Matterport3D environment.The dataset includes 90 scenes, with 61 scenes in the train and validation "seen" sets, and 11 scenes in the validation "unseen" set.Our 10-shot dataset is randomly sampled the train set within 1 scene, while our 100-shot dataset spans 2 scenes.</p>
<p>Evaluation.To contextualize our approach against prior work, we evaluate LangNav on both "seen" and "unseen" sets from R2R.The "seen" set contains scenes identical to the training set (but the instructions and trajectories differ).However, this distinction is less important for our low-data regime, since we only make use of 1 scene (for the 10-shot case) or 2 scenes (for the 100-shot case).I.e., the majority of scenes in the "seen" validation subset are actually never seen by the agent.We use the standard R2R task performance metrics (Anderson et al., 2018a): Navigation Error (NE), the average distance between the agent's final position and the goal in meters; Success Rate (SR), the ratio of trajectories in which the agent stopped within 3 meters of the goal; Oracle Success Rate (OSR), the ratio of trajectories in which the agent stopped within 3 meters to the goal with a view of the goal; and Success weighted by the normalized inverse of the Path Length (SPL).</p>
<p>Case Study 1: Language Enables Efficient Synthetic Data Generation</p>
<p>In NLP, obtaining synthetic data from an appropriately prompted large LM with which to learn a smaller model has been shown to be an effective approach in data-scarce settings (Wang et al., 2021;Lang et al., 2022;Taori et al., 2023;Dai et al., 2023;Gunasekar et al., 2023, inter alia).4However, this approach is difficult to extend to non-linguistic perceptual tasks such as VLN since generating realistic perception data is itself difficult.In our first case study, we show that working in pure language space makes it possible to easily generate synthetic</p>
<p>Step 1: Step 2: Step 4: Step 5:  data from a large LM based on a few seed trajectories.We further show that finetuning a smaller LM on a mixture of synthetic and R2R trajectories improves upon vision-based models.</p>
<p>Synthetic trajectory generation.We generate synthetic trajectories by using only the 10 R2R trajectories from a single scene.In R2R each trajectory has 3 navigation instructions given by 3 different annotators.Thus we have 30 navigation instructions {U (i) } 30 i=1 in total.Our data generation pipeline can be divided into two phases.In phase 1, we randomly choose 3 R2R instructions as prompt examples and ask GPT-4 to create 10 more instructions similar to the examples, as shown in Figure 2. In phase 2, for each generated instruction, we prompt GPT-4 to generate a trajectory to fulfill the instruction, conditioned on a real demonstration instruction and trajectory.The real trajectory is obtained by selecting the trajectory whose instruction is closest to the synthetic instruction based on the CLIP (Radford et al., 2021) text features.See Figure 2 for an overview and appendix H for the prompts. 5e present an illustrative example in Figure 3 to demonstrate some qualitative characteristics of generated trajectories.We find that the generated trajectories have: strong real-world priors, i.e., they exhibit adherence to real-world room-object and object-object correlations, as evident from descriptions like "a bathroom with a sink, mirror, [...]"; spatial consistency, where the examples maintain spatial consistency within the generated trajectories-for instance, in Step 4, the generated position identifies the door with a potted plant, consistent with its position in Step 3; and rich descriptions-the generated trajectories have descriptive captions and objects that do not only relate to the given instruction, which makes it possible to successfully navigate through language only.</p>
<p>Experimental setup.We compare LangNav, which is a LLaMA2-7b model finetuned on a mixture of the 10,000 synthetic trajectories and 10/100 real trajectories, against the following baselines: 1. Random walk, which selects a random action at each time step; 2. GPT-4 (Zero-shot / Few-shot), where we prompt GPT-4 to complete the trajectory by changing the task description of the template in § 3.2 (see appendix I for the full prompt).For the few-shot baseline, due to the context length we use one full navigation trajectory as a demonstration example; 3. NavGPT, a recent work that also uses language as a perceptual representation (via image captioning and object detection) to perform navigation, but purely with GPT-4 (Zhou et al., 2023a); 4. RecBert, a vision-based method that adopts a recurrent architecture proposed by Hong et al. (2021) to keep track of the trajectory history; 5. DuET, another vision-based method which additionally builds representations of the global map during learning (Chen et al., 2022) Table 2: Performance on the R2R val unseen set as we vary the number of synthetically generated data, the underlying LM from which the synthetic data is generated, and number of seed scenes.Here the seed scenes refer to the scans from which trajectories are sampled, with multiple trajectories originating from each seed scene.</p>
<p>language-only baseline that does not make use of the synthetic data from GPT-4.All finetuning methods use the same set of 10/100 trajectories.For these experiments, we did not find significant differences in performance when using the object detection module, and hence we only relied on the image captioning system to give the language description of each view angle in the prompt template.See appendix A for the training setup including hyperparameters.</p>
<p>Results.The results are shown in table 1.We find that our GPT-4 zero-and few-shot results underperform the NavGPT baseline despite using the same backbone model, potentially due to NavGPT's use of ground truth distance information and chain-ofthought prompting (Wei et al., 2022;Kojima et al., 2023).Just finetuning LLaMA2-7B on the 10/100 gold trajectories does not perform well, although it is comparable to the vision-based policies.Training on a mixture of synthetic and R2R trajectories improves performance by a nontrivial margin, and the LLaMA2-7B-based LangNav approaches the performance of NavGPT despite being many times smaller, indicating the effectiveness of our pipelined prompting strategy for distilling the rich navigation-relevant world knowledge within GPT-4 to a smaller (and more efficient) language model.6</p>
<p>Ablation study.In table 2 we vary both the number of synthetic trajectories and the data-generating LM.Switching the synthetic data source from GPT-4 to GPT-3.5/GPT-4-turbo results in noticeable declines, highlighting the importance of using a strong LM.Increasing the number of synthetic trajectories increases performance, although the gains are marginal when going from 2,000 to 10,000 trajectories.This is potentially due to the use of only  10 real trajectories from a single scene to prompt LLMs which results in lack of instruction diversity (see examples in appendix E).To investigate the influence of the scene diversity, we use 1,000 navigation instructions sampled from various R2R scenes to prompt GPT-4-turbo7 to generate 2,000 additional synthetic trajectories.We can see that although the 2,000 trajectories generated by GPT-4turbo are not of the same quality as those generated by GPT-4, scaling up using these trajectories outperforms the results from the 10,000-trajectory set.</p>
<p>Case Study 2: Language as a Bridge for Domain Transfer</p>
<p>We next experiment with using language as a domain-invariant representation space to transfer a policy that has been trained on a different (rendered) environment (ALFRED; Shridhar et al., 2020), to the real-world R2R environment.There are significant differences between ALFRED and R2R which makes straightforward domain transfer challenging.ALFRED uses images rendered from the synthetic AI2THOR environment (Kolve et al., 2017), while R2R, based on the Matterport3D, incorporates images captured from real indoor environments.ALFRED's navigation trajectories and instructions are also simpler and shorter compared to R2R's instructions: R2R instructions involve guiding the agent between rooms, whereas AL-FRED trajectories mainly keep the agent within a single room and provides instructions for household tasks.Finally in ALFRED, the agent is limited to rotating left/right by 90°and moving forward, while in R2R, the agent can move in any combination of 12 candidate heading directions and 3 elevation directions.See appendix B for detailed discussion of these differences, and see appendix A for the experimental setup.</p>
<p>Results.We pretrain both RecBert (Hong et al., 2021) 8 and LangNav on the simulated ALFRED environment and finetune on 0/10/100 R2R trajectories with object information.LangNav uses LLaMA1-7b (Touvron et al., 2023a) as the language model.The evaluation results for both methods are presented in table 3. Interestingly, for RecBert, pretraining on ALFRED actually hurts performance, potentially due to the model's overfitting to the idiosyncracies of the rendered environment.And without any R2R data, RecBert performs at near chance, whereas LangNav is able to exhibit some level of zero-shot transfer.Pretraining in ALFRED consistently leads to performance improvements for LangNav.</p>
<p>Case Study 3: Combining Language and Vision Representations</p>
<p>Our final case study explores whether languagebased perceptual representations can improve performance on top of traditional continuous vision features.This is motivated by the observation that (1) in the full data setting, LangNav still underperforms the state-of-the-art approaches which rely on pure vision features (see table 5 of appendix C),</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3</p>
<p>History steps</p>
<p>Instruction: Turn 180 degrees away from the television.Walk towards the top of the stairs.Walk down the stairs.Go through the doorway at the bottom of the steps.Turn right and walk into the first part of the room.Wait next to the sitting area across from the china closet.</p>
<p>Example # 1</p>
<p>To your straight ahead is, a hallway with a picture of a woman on the wall a set of stairs leading down to a gateway candidate 1 (decision after editing)</p>
<p>To your back and 60 degree up is, a set of stairs leading up to a window candidate 2 (original decision)</p>
<p>Figure 4: Interpreting and editing a model's predictions through language.At the beginning, the agent incorrectly selected "candidate 2" to ascend the stairs.The failure might stem from the ambiguous interpretation of mistaking the stairs for a hallway in "candidate 1".After editing the description (marked in green), the agent correctly alters its choice to walk down the stairs.We extend the RecBert (Hong et al., 2021) by concatenating language features to the visual features to represent the candidate image view.Concretely, the original RecBert uses ResNet-152 (He et al., 2016) to extract the visual feature to represent each view; our extension simply concatenates the caption representations (from BERT-base (Devlin et al., 2019)) to the image representation for each view.We train this new model on both the 100-shot and the full training set case.</p>
<p>Results.The results are listed in table 4. We find that language features improve the performance in both 100-shot and full training set cases, which indicates that language as a perceptual representation can provide additional benefits on top of continuous visual features, even in non-low-data settings.This is potentially due to language serving as useful prior for aspects of images that are salient for navigation.</p>
<p>Discussion</p>
<p>Interpretability and editability through language.Our use of language as a "bottleneck" per-ceptual representation makes it possible to (more easily) interpret and edit a model's predictions.As a qualitative case study, we inspect trajectories where the model made a mistake and manually inspect the captions.We find that model mistakes are generally due to incorrect or ambiguous captions.We manually edit the captions to be correct, and find that in many cases, this is able to change the model's predictions to be correct.See Figure 4 for a concrete example.We applied this procedure to 10 randomly selected trajectories which contained an error, and found that we were able to edit the model's decision to the correct one in 7 out of 10 trajectories.(For the other 3 trajectories, the failure was not due to incorrect captions).</p>
<p>Disentangling vision and language models.One the one hand, LangNav's use of a vision pipeline might seem like a step back from pure deep learning-based approaches which generally favor learning everything "end-to-end".On the other, the disentangling of the image module from the language module means our approach can readily make use of independent advances in vision and language models.This might become especially important given the recent trend in only providing API access to state-of-the-art language models.Non-standard navigation environments.Our main experiments are on the R2R benchmark, which is realistic insofar as it makes use of real household environments.Another testbed for Lang-Nav would be environments that lack existing datasets, such as offices or supermarkets.While the lack of existing benchmarks precludes our testing of LangNav on such non-standard environments, we performed a preliminary study where we tried generating synthetic trajectories from an office environment.We show an example in appendix J, where we find that GPT-4 is able to generate synthetic trajectories that contain common object-scene correlations in office environments and moreover exhibit great spatial consistency.Testing language as a perceptual representation in a variety of environments remains an interesting avenue for future work.</p>
<p>Related Work</p>
<p>Language Models for Task Planning.Several studies have explored language-based planning (Jansen, 2020; Sharma et al., 2021;Li et al., 2022b;Huang et al., 2022a;Ahn et al., 2022;Huang et al., 2022b).Huang et al. (2022a) use GPT-3 (Brown et al., 2020) and Codex (Chen et al., 2021a) for action plan generation with semantic translation using Sentence-RoBERTa (Huang et al., 2022a).SayCan (Ahn et al., 2022) grounds actions using FLAN (Wei et al., 2021) and action value functions (Shah et al., 2021).Huang et al. (2022b) explore incorporating grounded feedback into LLMs, while Xiang et al. (2023) propose enhancing LLMs' with embodied task instructions.</p>
<p>Instruction Tuning.There has been much recent work finetuning smaller language models such as LLaMA on synthetic instruction-following data generated by GPT-3.5/GPT-4(Peng et al., 2023;Taori et al., 2023;Chiang et al., 2023;Wu et al., 2023).Existing works have generally focused on traditional language tasks.Our work instead finetunes LMs for embodied navigation tasks using language descriptions.</p>
<p>Vision-and-Language Navigation.There has been much work on vision and language navigation on the R2R dataset (Anderson et al., 2018a).Approaches such as the speaker-follower model (Fried et al., 2018) and environmental dropout method (Tan et al., 2019), reinforced cross-modal matching (Wang et al., 2019), and self-monitoring (Ma et al., 2019) have been proposed.Recent advancements include VLBERT-based methods (Hong et al., 2021) and object-informed sequential BERT (Qi et al., 2021).Qiao et al. (2022) incorporate additional pretext tasks into VLN pre-training based on Hong et al. (2021).ALFRED (Shridhar et al., 2020) involves interactive actions in a synthetic environment (Kolve et al., 2017), with methods utilizing dense single vector representations (Shridhar et al., 2020;Singh et al., 2021;Pashevich et al., 2021;Kim et al., 2021;Blukis et al., 2022) or a panoramic view space (Suglia et al., 2021).CLIP-Nav (Dorbala et al., 2022) explores the zero-shot VLN with CLIP while Kurita and Cho (2020) proposes a generative language model-based navigation approach.For instruction synthesis, Nguyen and Daumé III (2019) and Thomason et al. (2020) studies rule-based instruction synthesis in Matter-port3D environment.Finally, our work is closely related to Zhou et al. (2023a) and Schumann et al. (2023), which also use language descriptions of an agent's perceptual representation to perform navigation with an LM.</p>
<p>Conclusion</p>
<p>We show that we can learn to navigate in a realworld environments by using language as a perceptual representation.Language naturally abstracts away low-level perceptual details, which we find to be beneficial for efficient data generation and sim-to-real transfer.However, this is also a serious limitation insofar as a picture really is worth a "thousand words" in some cases; we are certainly not suggesting the abandonment of traditional (continuous) vision features for vision-language navigation.But our case studies nonetheless demonstrate the promise of language as a perceptual representation for vision-language navigation.</p>
<p>Limitations</p>
<p>While we find that LangNav is promising in settings where only a handful of real trajectories are available, on the full dataset it still underperforms vision-based agents by a nontrivial margin, as shown in table 5 of appendix C.This is especially true when compared to state-of-the-art approaches such as ScaleVLN (Wang et al., 2023) which make use of large-scale pretraining data as well as more involved imitation/reinforcement learning algorithms that require access to an environment oracle.However, we note that while Lang-Nav underperforms baselines in data-rich regimes, it overfits less to scenes seen during training, as demonstrated by the smaller drop in performance when applying the policy to unseen scenes during training.</p>
<p>A Implementations Details</p>
<p>We used the LLaMA-7B model (Touvron et al., 2023a) and the LLaMA2-7B model (Touvron et al., 2023b) for our method, fine-tuning it on 72 V100-32GB GPUs with a batch size of 144.The training tokens had a maximum length of 1024, while during inference, the maximum length was set to 2048.The AdamW optimizer (Loshchilov and Hutter, 2017) with a learning rate of 2 × 10 −5 and weight decay of 0 was employed for optimization.The WarmupDecayLR learning rate scheduler was used for learning rate scheduling.For image captioning in both the R2R and ALFRED tasks, BLIP (Li et al., 2022a) was utilized.Deformable DETR (Zhu et al., 2020) was used for object detection in the R2R dataset, with suppression of outdoor object categories.We used the ground-truth object detection results provided in ALFRED when we generated the instruction-following pairs in § 4.2.When prompting GPT-4 / GPT-4-turbo / GPT-3.5 API, we set the temperature as 1 and top_p as 1.The cost of collecting the generated 10,000 trajectories by prompting GPT-4 API (OpenAI, 2023) was around $500.In the few-shot learning experiments in § 4.1 and § 4.2, we set ρ = 0.While when fine-tuning with the full train set in appendix D, we set ρ = 0.2.We pretrain on 128K ALFRED instruction-following pairs whose format is given in § 3.2.We augment the observations in ALFRED to 12 views and randomly mask a variable number of views to mimic the irregular number of candidates in R2R.The RecBERT baselines in table 1, table 3, and table 4 are pre-trained on 10/100 trajectories from R2R with masked language modeling (MLM) and single action prediction (SAP) tasks (Hao et al., 2020).The DUET baselines in table 1 are pre-trained on 10/100 trajectories with MLM, SAP, and masked region classification (MRC) tasks (Chen et al., 2022).</p>
<p>B Differences between ALFRED and R2R.</p>
<p>The primary cause of the vast difference between ALFRED and R2R lies in their environmental rendering: ALFRED utilizes images from the synthetic AI2THOR environment (Kolve et al., 2017), whereas R2R, drawing from the Matterport3D database, features images from actual indoor environments.We summarize the differences in the following aspects: Visual appearance.ALFRED uses images ren-dered from the synthetic AI2THOR environment, while R2R, based on the Matterport3D, incorporates images captured from real indoor environments.These image sources differ in texture, occlusion, illumination, and other visual aspects.</p>
<p>Step size.There is a difference in step sizes between the two tasks (see the right part of fig.5).ALFRED uses a step size of 0.25 meters, while R2R has larger and more variable step sizes.To bridge this gap, we consolidate four consecutive MoveAhead steps into a single step along the AL-FRED trajectory.</p>
<p>Action type.A complete ALFRED trajectory includes not only navigation actions but also interaction actions, where the interaction actions are combined with a target object to change the state of the surrounding environment.In order to filter the interaction actions in ALFRED, we divide each ALFRED trajectory into multiple sub-trajectories and keep the sub-trajectories that are labeled with the GotoLocation tag.</p>
<p>Instruction complexity.Due to trajectory splitting, ALFRED's navigation trajectories and instructions appear simpler and shorter compared to R2R's instructions.R2R instructions involve guiding the agent between rooms, whereas ALFRED trajectories mainly keep the agent within a single room.</p>
<p>Action space.In ALFRED, the agent is limited to rotating left/right by 90°and moving forward, while in R2R, the agent can move in any combination of 12 candidate heading directions and 3 elevation directions.The number of available movement directions is irregular.This difference in action space makes R2R trajectories more human-like.To address this, we introduce randomness by adding or reducing a heading offset of ±30°to the agent's direction at each step in ALFRED, allowing rotations of 30°or 60°in addition to 90°.</p>
<p>C Performance on full data</p>
<p>In Table 5 we show the performance of LangNav on the full dataset, as well as comparisons against the state-of-the-art.While we find that LangNav is promising in settings where only a handful of real trajectories are available, on the full dataset it still underperforms vision-based agents by a nontrivial margin.This is especially true when compared to state-of-the-art approaches such as ScaleVLN (Wang et al., 2023) which make use of large-scale pretraining data as well as more involved imitation/reinforcement learning algorithms that require Appearance ALFRED R2R</p>
<p>Step Size
ALFRED R2R t = T -1 t = T</p>
<p>Complexity of Instructions</p>
<p>ALFRED: Carry the bowl to the glass coffee table.</p>
<p>R2R: Go to the left of the bed and out of the bedroom.Then go down the hall and make a right at the top of the stairs, go past the stairs and go a couple steps into the bedroom and wait there.</p>
<p>Figure 5: Task gap between ALFRED and R2R.We highlight notable distinctions between the navigation tasks in ALFRED and R2R, encompassing variations in appearance, step size, and instruction complexity.See appendix B for more details.</p>
<p>access to an environment oracle during training.However, we note that while LangNav underperforms baselines in data-rich regimes, it overfits less to scenes seen during training, as demonstrated by the smaller drop in performance when applying the policy to unseen scenes during training.</p>
<p>D Multi-Task Performance</p>
<p>One of the advantages of our approach is its inherent suitability for multitasking.Similar to LLMs use instruction to handle multiple language tasks concurrently, we consolidate task information and inputs into instructions.To validate the multitasking capability of our method, we extend its application to the ALFRED task.</p>
<p>Metrics on ALFRED.We evaluate our model on ALFRED using two metrics: Task Success (Task) and Goal-Condition Success (GC).Task Success measures the ratio of trajectories where object positions and state changes accurately match all task goal conditions at the end.GC assesses the ratio of completed goal conditions in each action sequence.</p>
<p>Task Success is only considered successful when GC is also 1. On average, each ALFRED task has 2.55 goal conditions.We also calculate the Path Length Weighted Metrics (PLW) for both Task and GC, which normalize the metrics based on the actual action sequence length.</p>
<p>Results of the Multi-Task Model.In ALFRED task, we set ρ = 0 as the expert policy in ALFRED is suboptimal.To save training time and balance the data amount between R2R and ALFRED, we utilize only 50% of the training dataset, resulting in a dataset for ALFRED with 386K data pairs.For R2R task training, we maintain ρ = 0.2 and run each demonstration trajectory twice, resulting in a training set size of 235K for R2R.Consequently, the merged dataset for the multitask model contains a total of 621K instruction-following data pairs.We select VLN Bert (Hong et al., 2021) as the baseline for the R2R task and Seq2seq model (Shridhar et al., 2020) for the ALFRED task.Given the substantial differences between the R2R task and the ALFRED task ( § 4.2), our method is, to the best of our knowledge, the first model that simultaneously addresses these two tasks.In table 6 and table 7, we find that the multitask model exhibits superior performance compared to the single-task models.These results underscore the capability of our method to effectively handle multiple highly diverse tasks.</p>
<p>E Bias of generated navigation instructions</p>
<p>We list four generated instructions from one output of GPT-4 with 10 seed trajectories as we mentioned in § 4.1 as bellow:  We demonstrate the multi-task capability of the LM agent.For single-task models, each model is trained within the task data.We trained the multi-task model with data from both R2R and ALFRED tasks.</p>
<p>ALFRED Seen</p>
<p>ALFRED Unseen Task↑ GC↑ Task↑ GC↑ ST 0.0 (0.0) 6.0 (4.7) 0.5 (0.1) 9.5(7.8)MT 0.0 (0.0) 6.4 (5.0) 0.6 (0.2) 9.8 (7.8)We can see from the above synthetic instructions that (a) patterns of the synthetic instructions are similar, which are like "Start from place A, go pass place B, stop at place C", (b) scenes are limited to the living area and a single floor, however, the R2R tasks always require the agent navigating across floors and in some non-living area.</p>
<p>F Impact of the randomness factor</p>
<p>We the ablation study to investigate the impact of the randomness factor ρ on the model's performance in both few-shot learning and full-set fine-tuning scenarios.Interestingly, we observe different behaviors of the model with varying ρ values in these scenarios.Fig. 6 illustrates our findings.In the 10-shot scenario, increasing ρ negatively affected the model's performance.However, in the full-set scenario, the model achieved optimal performance at ρ = 0.2.</p>
<p>G Example of the Full Trajectory</p>
<p>We plot an example of a full text-based trajectory in R2R as we mentioned in § 3.2 as bellow:</p>
<p>Full R2R trajectory Task description:</p>
<p>You are a navigation agent who must navigate according to instructions given only descriptions of your current position via natural language.The natural language description is sometimes incorrect.</p>
<p>Navigation instruction:</p>
<p>Go across the kitchen passed the pizza oven into the next room.Stop just inside the bedroom.</p>
<p>Navigation trajectory:</p>
<p>Step 1:</p>
<p>( environment:) To your 60 degree left is, a kitchen with a stove, sink, and refrigerator Details: oven, bowl, dining table, microwave, refrigerator To your 30 degree left is, a kitchen with white cabinets and a white stove top oven Details: oven, bowl, microwave, potted plant, chair To your 60 degree right is, a living room filled with furniture and a fire place Details: couch, chair To your back is, a room with a bed and a painting on the wall</p>
<p>You chose:</p>
<p>( navigation agent:) a living room filled with furniture and a fire place</p>
<p>Step 2:
( environment:)
To your 150 degree left is, a kitchen with a stove top oven sitting next to a counter Details: oven, dining ( navigation agent:) a living room with a couch a chair and a window</p>
<p>Step 3:</p>
<p>( environment:) To your back is, a kitchen with a table, chairs, and stairs Details: oven, dining table, refrigerator, potted plant, chair To your 150 degree right is, a room with a table, chairs, and stairs Details: oven, chair, refrigerator, dining table To your straight ahead and 30 degree down is, a view of a hallway from the top of stairs Details: refrigerator To your 90 degree right and 30 degree up is, a room with a staircase and a mirror on the wall Details: toilet You chose:</p>
<p>( navigation agent:) a view of a hallway from the top of stairs</p>
<p>Step 4:</p>
<p>( environment:) To your back and 60 degree up is, a living room filled with furniture and a ceiling fan Details: oven, potted plant, refrigerator To your 90 degree left and 30 degree up is, a living room a couch and a table To your straight ahead and 30 degree up is, a bedroom with a bed and a mirror on the wall Details: bed You chose:</p>
<p>( navigation agent:) a bedroom with a bed and a mirror on the wall</p>
<p>Step 5: We list our complete templates for prompting GPT-4 to generate synthetic instructions (Phase I) and synthetic trajectories to fulfill the instruction (Phase II).</p>
<p>Phase I: Prompt template for generating synthetic instructions System prompt: I am going to give you example instructions written by humans to train a deep learning-based navigation agent acting inside a home.These example instructions are intended to be completed by the navigation agent in 5-7 steps.</p>
<p>Few shot examples:</p>
<p>-{real_instruction_1} -{real_instruction_2} -{real_instruction_3}</p>
<p>User:</p>
<p>Your goal is to write 10 more instructions like the above that can be used to train a navigation agent.Since the navigation agent will be navigating in different home environments, your instructions should also be diverse and cover a wide range of home environments and rooms.You should make sure that the instruction can be completed by an agent in 5 to 7 steps.</p>
<p>Phase II: Prompt template for generating synthetic trajectories System prompt:</p>
<p>Here is an example of a large language model acting as a blind navigation agent in an indoor environment through text descriptions.The agent is given an instruction at the start and must follow the instruction.At each time step, the agent is given descriptions of its field of view via the following template:</p>
<p>To Here we attach the the task description D in the prompt template for prompting GPT-4 to navigate in the R2R evaluation dataset.</p>
<p>Zero-shot</p>
<p>System prompt: You are a navigation agent who must navigate according to instructions given only descriptions of your current position via natural language.The natural language description is sometimes incorrect.</p>
<p>User:</p>
<p>At each step, you will be given several directions and captions for each direction.You must choose one direction by printing only the [caption_of_the_direction] or choose "Stop" if you think the goal is reached.</p>
<p>For example: Input:
To your [direction_1] is, [caption of the direction_1]. ...... To your [direction_N] is, [caption of the direction_N].
You choose:</p>
<p>Output: [caption of the direction_3]</p>
<p>Hint: You should use the information inside the instructions, history steps, and current observations to make the decision.</p>
<p>Few-shot</p>
<p>System prompt: You are a navigation agent who must navigate according to instructions given only descriptions of your current position via natural language.The natural language description is sometimes incorrect.</p>
<p>User:</p>
<p>At each step, you will be given several directions and captions for each direction.You must choose one direction by printing only the [caption_of_the_direction] or choose "Stop" if you think the goal is reached.For example: Input:
To your [direction_1] is, [caption of the direction_1]. ...... To your [direction_N] is, [caption of the direction_N].
You choose:</p>
<p>Output: [caption of the direction_3]</p>
<p>Few shot examples: And here is an example trajectory:</p>
<h3>Instruction: Go down the stairs.Turn right and go down the hallway.Turn right and stand near the fireplace.</h3>
<h3>Trajectory:</h3>
<p>Step 1:</p>
<p>To your straight ahead is, an ornate doorway leading to another room</p>
<p>To your 60 degree right is, a red carpeted staircase leading to a chandelier</p>
<p>To your 120 degree right is, a room with a red carpet and a large mirror</p>
<p>To your back and 30 degree down is, a room with a red carpet and two windows</p>
<p>To your 120 degree left is, a room with a red carpet and gold trim</p>
<p>You chose: a room with a red carpet and gold trim</p>
<p>Step 2:</p>
<p>To your 150 degree right is, a very ornate staircase in a house with red and white striped chairs Step 3:</p>
<p>To your back is, a hallway with a red carpet and a chandelier</p>
<p>To your straight ahead is, a hallway with a red carpet and a gold ceiling You chose: a hallway with a red carpet and a gold ceiling</p>
<p>Step 4:</p>
<p>To your 90 degree right is, a living room with a chandelier and a fireplace</p>
<p>To your 120 degree right is, a room with a fireplace and a chandelier in it</p>
<p>To your back is, a hallway with a red carpet and gold trim</p>
<p>To your 90 degree left is, a room with a chandelier and a table in it</p>
<p>To your 30 degree right is, a living room with a chandelier and a couch You chose: a living room with a chandelier and a fireplace</p>
<p>Step 5:</p>
<p>To your straight ahead is, a living room filled with furniture and a fire place</p>
<p>To your 30 degree left is, a living room filled with furniture and a chandelier</p>
<p>To your 150 degree right is, a room with a chandelier and a table in it</p>
<p>To your back is, a hallway with a chandelier and a mirror on the wall</p>
<p>To your 90 degree left is, a living room filled with furniture and a chandelier</p>
<p>You chose: Stop</p>
<p>User:</p>
<p>Hint: You should use the information inside the instructions, history steps, and current observations to make the decision.Now let's start!</p>
<p>J Examples of Generated Trajectories in Exotic Environments</p>
<p>To validate that LLMs can synthesize useful data in more exotic environments, we conduct an experiment where we handcraft a trajectory in a real office environment and then prompt GPT-4 to generate synthetic trajectories within the scope of the office environment.Here we first plot the language trajectory we have sampled from a real office environment.And then, we attach the generated trajectories from GPT-4 to demonstrate the ability of a strong LLM to generate trajectories in more exotic environments.</p>
<p>Real-world trajectory Navigation instruction:</p>
<p>Start from the pantry on your left, pass the board room to enter the atrium.Go through the door to get into the office area.Turn right at the first corner, and then turn right to stop at the printer.</p>
<p>Navigation trajectory:</p>
<p>Step 1: Step 4:</p>
<p>( environment:) To your straight ahead is, An office area where there is a black board and a work pod room.To your 90 degree left is, An array of office desks with monitors and office chairs.To your 90 degree right is, A large office area with multiple arrays of desks and chairs.To your back is, An exit door with a phone room on the right.</p>
<p>You chose:</p>
<p>( navigation agent:) A large office area with multiple arrays of desks and chairs</p>
<p>Step 5:</p>
<p>( environment:) To you straight ahead is, A large office area with multiple arrays of desks and chairs, as well as trash bins.To your 60 degree left is, A black board in the middle of an office area.To your 90 degree right is, A copy/print room with recycle bins.To your back is, An array of office desks and chairs with a black board.</p>
<p>You chose:</p>
<p>( navigation agent:) A copy/print room with recycle bins</p>
<p>Step 6:  To your back is, an office space with several workstations.To your 90 degree left is, a hallway leading to the break room.To your 90 degree right is, a corner office space with a large round table.</p>
<p>You chose:</p>
<p>( navigation agent:) an office space with rows of desks and office chairs.</p>
<p>Step 5:</p>
<p>( environment:) To your straight ahead is, an open office space with rows of desks and office chairs.To your 30 degree left is, a meeting room with a large table and multiple chairs.To your 120 degree right is, a corridor leading to a door.</p>
<p>You chose:</p>
<p>( navigation agent:) a corridor leading to a door.</p>
<p>Step 6:</p>
<p>( environment:) To your straight ahead is, a door at the end of the hallway To your 90 degree right is, a coffee station with a variety of coffee machines</p>
<p>You chose:</p>
<p>( navigation agent:) a coffee station with a variety of coffee machines</p>
<p>Step 7:</p>
<p>( environment:) To your straight ahead is, a small kitchenette with a fridge and microwave To your immediate left is, a conference room featuring a large glass table To your second left is, a second conference room with a project screen and chairs</p>
<p>You chose:</p>
<p>( navigation agent:) a second conference room with a project screen and chairs</p>
<p>Step 8:</p>
<p>( environment:) To your straight ahead is, a conference room with a projection screen and chairs.To your back is, a coffee station with various coffee machines.</p>
<p>K More Examples of Generated Trajectories</p>
<p>We plot more examples of the generated trajectories from GPT-4 below:</p>
<p>Example #1</p>
<p>Navigation instruction:</p>
<p>Begin in the basement, walk towards the pool table.Pass the pool table and ascend the staircase.At the top of the stairs, turn right and walk through the hallway.Turn left into the den, walk through and stop by the window.</p>
<p>Navigation trajectory:</p>
<p>Step 1:       ( navigation agent:) a cozy family room with a TV and a fireplace
( environment:)
Step 5:</p>
<p>( environment:) To your 30 degree left is, a sliding glass door leading to the backyard To your 90 degree right is, a comfortable sofa with a coffee table in front of a TV To your back is, a large kitchen with a stove and a range hood  ( navigation agent:) a well-maintained backyard with a barbecue grill and a dining table</p>
<p>Step 7: Step 2:</p>
<p>( environment:) To your 60 degree left is, a living room with a fireplace and a floor lamp To your 30 degree right is, a living room with a dark brown couch and a coffee table To your straight ahead is, a living room with a white rug in the middle You chose:</p>
<p>( navigation agent:) a living room with a dark brown couch and a coffee table</p>
<p>Step 3:</p>
<p>Figure 3 :
3
Figure 3: An example of a generated trajectory from GPT-4.The example demonstrates a generated trajectory by following the pipeline in Figure 2. See more examples in appendix K.</p>
<p>Figure 6 :
6
Figure 6: Investigating the Impact of the Randomness Factor ρ on Model Performance.This image caption depicts an ablation study exploring the influence of the randomness factor ρ on our model's performance in both few-shot learning and full-set training scenarios.We test ρ with values of 0.0, 0.1, 0.2, and 0.3.</p>
<p>To your back is, a hallway leading to a kitchen and living room Details: refrigerator, potted plant To your 30 degree left is, a room with a wooden door and a mirror To your straight ahead is, a bedroom with a bed, dresser, mirror and a ceiling fan Details: potted plant, bed To your 30 degree right is, a bedroom with a bed and a ceiling fan Details: potted plant, bed To your 60 degree right is, a bedroom with a bed, dresser and mirror Details: potted plant, bed You chose: ( navigation agent:) stop H Complete Prompt Template of Generating Trajectories for GPT-4</p>
<p>Few</p>
<p>your [VIEW] is [CAPTION] -[VIEW] consists of the agent's visible field of view (e.g., 30 degrees right, 120 degrees left, etc.) -[CAPTION] is the text description of that view obtained from an image captioning model give you another instruction.Please generate a trajectory of 5-7 steps that would complete the instruction.# Example 2 ### Instruction: {synthetic_instruction} I Prompts of Zero-shot and Few-shot Navigation for GPT-4</p>
<p>To your back is, a red carpeted hallway leading to a staircase To your 150 degree left is, a hallway with a red carpet and a chandelier To your 120 degree left is, a room with a red carpet and a chandelier To your 90 degree left is, a room with a chandelier and two windows To your 60 degree left is, a room with a red carpet and a large mirror To your 30 degree right is, a hallway with a red carpet and wooden doors You chose: a hallway with a red carpet and wooden doors</p>
<p>To your straight ahead is, a long hallway with light on the ceiling and a fridge on the left To your 120 degree left is, a lounge with a screen on the wall, floor-to-ceiling winders, desks, and many high chairs To your 90 degree right is, an auditorium with three big screens and tiered seating facing the stage You chose: ( navigation agent:) a long hallway with light on the ceiling and a fridge on the left Step 2: ( environment:) To your straight ahead is, a long hallway with four posters on the left leading to the atrium To your back is, a lounge with a sofa and a shelf stocked with snacks You chose: ( navigation agent:) a long hallway with four posters on the left leading to the atrium Step 3: ( environment:) To your straight ahead is, an atrium with lounge chairs on the right and a door on the straight ahead To your 90 degree left is, a front desk in the corner with a monitor on the desk.To your 90 degree right is, an exit gate towards a set of commercial elevators.To your back is, a long hallway with posters on the right wall You chose: ( navigation agent:) an atrium with lounge chairs on the right and a door on the straight ahead</p>
<p>To your 90 degree right is, A printer on the ground and several instruction sheet on the wall To your back is, A black board and an array of office desks Begin at the elevator lobby.Exit the lobby and turn left, heading towards the open office space.Walk past two rows of desks, then take a right.Continue straight until you see the coffee station.Turn left and stop at the second conference room.To your straight ahead is, an elevator lobby with multiple elevators and a directory stand To your 90 degree left is, a closed door with a sign "Stairs" To your 90 degree right is, a wide corridor leading to a To your straight ahead is, a reception area with a white desk and three chairs To your 120 degree left is, a corridor leading to an open office space with several workstations To your 90 degree right is, a closed door marked "Emergency Exit" To your straight ahead is, an open office space filled with rows of desks and office chairs To your 90 degree left is, an array of cubicles with computers.To your 90 degree right is, an exit door with a sign "Stairs"</p>
<p>To your straight ahead is, an open office space with rows of desks and office chairs.</p>
<p>Start outside the IT department's office.Walk straight and take the first left towards the stairwell.Go up one flight of stairs.Upon reaching the top, turn right and proceed to the end of the corridor.The library is on the right.To your straight ahead is, the hallway leading to an intersection with doors for the office spaces on the sides To your 90 degree left is, a door with a sign "IT Department" followed by some tech carts To your 90 degree right is, a hallway towards the elevators with seating areas on the sides To your straight ahead is, a hallway with the elevator and murals on its sides To your 90 degree left is, a hallway that moves towards a stairwell To your back is, a hallway leading towards the IT department</p>
<p>To your straight ahead is, a stairwell leading upwards with railing and stair lights To your 90 degree left is, a hallway towards a coffee machine and cafeteria entrance To your 90 degree right is, a door to a rest room with signage To your back is, a hallway with murals leading back to the IT department's office.To your straight-ahead is, another flight of stairs leading to a large window To your 90 degree right is, a hallway lined with posters and a bench To your back is, a flight of stairs that leads down with railing and stair lights You chose: To your straight-ahead is, a corridor leading to an intersection with a water fountain to the right To your 90 degree left is, a large window that illuminates the corridor and gives a view of the street To your 90 degree right is, a library entrance with a signage To your back is, the hallway lined with posters and a bench</p>
<p>To your back is, a staircase leading downwards into the basement To your straight ahead is, a long hallway with several doors leading to other rooms To your 120 degree right is, a den with a comfortable seating area and a bookshelf You chose: ( navigation agent:) a long hallway with several doors leading to other roomsStep 6:</p>
<p>To your back is, a hallway with various framed photos on the walls To your 90 degree left is, a den filled with cozy furniture, bookshelves, and a television To your 90 degree right is, a bedroom with a large bed, dresser, and a window You chose: ( navigation agent:) a den filled with cozy furniture, bookshelves, and a television Step 7: ( environment:) To your back is, a hallway leading to other parts of the house To your 90 degree left is, a den with several bookshelves, a fireplace, and a large rug To your straight ahead is, a window overlooking the garden outside From the garage entrance, walk into the kitchen.Turn left and walk past the stove.Continue straight into the family room, then walk towards the sliding glass doors.Step outside and stop near the barbecue grill.</p>
<p>To your straight ahead is, a clean garage with a parked car and a workbench To your 90 degree left is, a laundry room with a washer and a dryer To your back is, a driveway leading up to a garage door To your 120 degree left is, a garage entrance with an open door leading inside the house</p>
<p>To your 30 degree left is, a kitchen with a stove, a refrigerator, and a microwave oven To your 90 degree right is, a small mudroom with a shoe rack and a coat hook To your back is, a view of a garage with a car and a workbench You chose: To your straight ahead is, a large kitchen with a stove and a range hood To your 30 degree right is, a kitchen with a center island and a sink on the countertop To your 150 degree left is, a kitchen with a refrigerator and a microwave oven To your 120 degree right is, a tiled floor leading to a hallway with a picture on the wall You chose:( navigation agent:) a large kitchen with a stove and a range hoodStep 4:</p>
<p>To your straight ahead is, a cozy family room with a TV and a fireplace To your 90 degree right is, a kitchen with a sink and a dishwasher To your back is, a kitchen with a countertop, a cooktop, and an oven You chose:</p>
<p>(</p>
<p>environment:)To your straight ahead is, a well-maintained backyard with a barbecue grill and a dining table To your 90 degree left is, a sliding glass door leading inside the family room To your back is, a cozy family room with a TV and a fireplace You chose:</p>
<p>To your 90 degree left is, a barbecue grill on a patio in the backyard To your straight ahead is, a backyard with a dining table and chairs To your 120 degree right is, a backyard with a garden and a swing set To your back is, a sliding glass door leading inside the Stand in front of the fireplace in the living room.Walk diagonally across the room, passing the sofa.Proceed through the door into the hallway.Turn left and continue toward the guest room.Stop just before entering and wait.To your straight ahead is, a living room with a fireplace and two chairs To your 30 degree left is, a living room with a large window and a dark brown couch To your 90 degree right is, a living room with a flat screen tv and a view of the backyard You chose: ( navigation agent:) a living room with a fireplace and two chairs</p>
<p>(</p>
<p>environment:) To your back is, a living room with a fireplace and a christmas tree To your 150 degree left is, a living room with two chairs and a painting on the wall To your straight ahead is, a wooden door leading to a hallway To your 90 degree left is, a hallway with a view of a staircase and a table To your straight ahead is, a hallway with a painting on the wall and an open door You chose: ( navigation agent:) a hallway with a painting on the wall and an open doorStep 5:</p>
<p>To your back is, a hallway with a wooden floor and a closed door To your 120 degree left is, a guest bedroom with a neatly made bed and a dresser To your 30 degree right is, a hallway with white walls and floor-to-ceiling mirrors You chose: ( navigation agent:) Stop just before entering the guest bedroom</p>
<p>To actually have the agent generate a t we simply decode from an LM's distribution,p LM (• | D, U, H t , O t ), via greedy decoding.Here H t = {O i , a i } t−1i=0 encodes the observation and action trajectory.3Updatingtrajectory history H t .We update the observation and action trajectory history via appending the text representations of O t and a t to H t :
You go towards: "{Ct,j}"Step {t}: To your {direction_1} is{caption_1};To your {direction_2}is {caption_2}; [...]; You chose:{caption_of_selected_direction}.</p>
<p>Enter the living room through [……] locate the large bookshelf.
1. {synthetic_instruction_1}[……]9. {synthetic_instruction_9}10.</p>
<p>To your [……] Now I will give you another instruction. Please generate a trajectory [……] #Example 2 ### Instruction: Enter the living room through [……] locate the large bookshelf. CLIP feature matching ### Trajectory: Step 1: To your straight ahead is, a living room with a sofa, coffee table, and a television To your 30 degree left is, [……] You chose: [a living room with a sofa, coffee table, and a television] [… More Steps …] GPT-4 API
GPT-4 promptGPT-4 promptGPT-4 responseGPT-4 responseFigure 2: Pipeline for generating synthetic navigation trajectories from GPT-4. We first prompt GPT-4 with 3randomly sampled navigation instructions U to generate 10 more synthetic navigation instructions (Phase 1). Thenfor each generated navigation instruction, we prompt GPT-4 to generate the trajectory that fulfills the generatedinstruction (Phase 2). See appendix H for details.</p>
<p>To your straight ahead is, a bright living room with a comfortable sofa and a coffee table To your 60 degree right is, a kitchen with modern appliances and a countertop To your back is, a dining area with a large table and chairs To your 90 degree left is, a hallway with framed pictures on the wall You chose: [a hallway with framed pictures on the wall]
Instruction:Enter the hallway and follow ituntil you see a door with apotted plant nearby. Enter theroom and locate the bookshelf.Stop next to the bookshelf.</p>
<p>Step 3: To your 30 degree left is, a door with a potted plant beside it To your 90 degree left is, a small office with a desk and a computer To your straight ahead is, the end of the hallway with a window To your back is, a hallway with picture frames on the wall You chose: [a door with a potted plant beside it]</p>
<p>To your 90 degree left is, a cozy bedroom with a neatly made bed To your 60 degree right is, a bathroom with a sink, mirror, and a bathtub To your straight ahead is, a long hallway with doors on both sides To your back is, a living room with a sofa and a coffee table You chose: [a long hallway with doors on both sides]</p>
<p>To your 60 degree right is, a large bookshelf filled with books and decorations To your back is, a hallway with a potted plant by the door To your 30 degree left is, a cozy lounge area with a sofa and a coffee table To your 90 degree left is, a home office with a desk, chair, and a computer You chose: [a large bookshelf filled with books and decorations]</p>
<p>To your 90 degree right is, a window with a view of the backyard To your straight ahead is, a bookshelf with various items and books To your back is, an entrance with a potted plant and a door leading to the hallway To your 150 degree left is, a comfortable lounge area with a sofa and a coffee table You chose: [Stop]</p>
<p>Table 1 :
1
; and 6.LLaMA2-7B, a Results on the R2R dataset with 10 or 100 real world trajectories.LangNav finetunes LLaMA2-7B on the mixture of the real-world trajectories and 10,000 synthetic trajectories from GPT-4.
Methods# realVal SeenVal UnseenNE↓ OSR↑ SR↑ SPL↑ NE↓ OSR↑ SR↑ SPL↑Random Walk010.25319.5632LLaMA2-7B (Zero-shot)010.20009.5000GPT-4 (Zero-shot)010.5159810.217108GPT-4 (Few-shot)110.1171099.9221311NavGPT (Zhou et al., 2023a)0----6.5423429RecBert (Hong et al., 2021)1010.897610.11399DuET (Chen et al., 2022)1010.02114129.9201211LLaMA2-7B1010.21511109.616119LangNav (with LLaMA2-7B) 107.53931277.0423228RecBert (Hong et al., 2021)1009.32720199.4261917DuET (Chen et al., 2022)1009.23121189.4322319LLaMA2-7B1009.62921189.1301917LangNav (with LLaMA2-7B) 1007.44032287.14534292,000GPT-3.5109.831.015.6 12.22,000GPT-4-turbo10008.142.924.9 19.6500GPT-4108.038.224.5 20.62,000GPT-4107.042.231.1 26.610,000GPT-4107.041.931.6 27.52,000 + 2,000GPT-4 + GPT-4-turbo 10 + 10007.143.232.6 28.3</p>
<h1>synthetic data Data-generating LM # seed scenes NE↓ OSR↑ SR↑ SPL↑</h1>
<p>Table 3 :
3
(Hong et al., 2021)ults where we pretrain a navigation agent on the simulated ALFRED environment (which uses rendered images) and finetune on the real-world R2R environment.We use LLaMA-7B(Touvron et al.,  2023a)as our backbone model, and compare against the RecBert(Hong et al., 2021)baseline.</p>
<p>Table 4 :
4
Results when combining continuous visual features with language features with RecBert.Evaluations are conducted on R2R val unseen set.</p>
<h1>Training Perceptual features SR↑ SPL↑100Vision only19.0 17.4100Vision + language19.3 18.0Full trainVision only47.1 43.4Full trainVision + language48.8 44.1</h1>
<p>and (2) realistic VLN scenarios would likely have access to continuous vision features as well.</p>
<p>Table 5 :
5
Comparison with state-of-the-art vision-based methods on the R2R dataset when trained on the full dataset.We use success rate (SR) as the performance metric."Needs oracle" indicates that the model needs to rely on an oracle during training that can give the ground-truth next action based on a sampled path from the model.(M):Multi-Task model.
Examples of generated instructions1. Start from the main entrance door, passthe living room, and enter the kitchen onyour right. Locate the refrigerator, thenturn left and stop just before the dining</p>
<p>Table 7 :
7
Performance</p>
<p>of the Multi-task Model on AL-FRED.ST: Single-Task.MT: Multi-Task.</p>
<p>table ,
,microwave, refrigerator, chairTo your back is,a kitchen with a wooden table andwhite appliancesDetails: oven, bowl, dining table,microwave, refrigerator, apple,couch, chairTo your 30 degree right is,a living room with a couch a chairand a windowDetails: couchTo your 120 degree right is,a dining room table with a bowl offruit on itDetails: chair, bowl, dining tableTo your 150 degree right is,a bowl of fruit sits on a woodentableDetails: couch, chair, bowl, diningtableYou chose:</p>
<p>To your straight ahead is, a basement with a pool table, a bar, and a seating area To your 90 degree left is, a wall with various framed photos and a door To your 150 degree right is, a storage area filled with boxes and shelves To your back is, a basement with a seating area and framed photos on the wall To your straight ahead is, a pool table surrounded by chairs and a dartboard on the wall To your 150 degree left is, a basement with a bar, television, and several chairs To your 90 degree right is, a staircase leading upwards with a wooden railing To your back is, a basement with a pool table and a dartboard on the wall To your 90 degree left is, a hallway with various framed photos on the walls To your straight ahead is, a den with a large bookshelf, couch, and coffee table
You chose:(navigation agent:)a staircase leading upwards with awooden railingStep 4:(environment:)You chose:You chose:(navigation agent:)a basement with a pool table, a bar,and a seating areaStep 2:(environment:)To your back is,a basement entrance with a staircaseleading downTo your 120 degree left is,a basement with a bar, television,and several chairsTo your straight ahead is,a basement with a pool table, adartboard, and a barTo your 90 degree right is,a staircase leading upwards with awooden railingYou chose:(navigation agent:)a basement with a pool table, adartboard, and a barStep 3:(environment:)
In R2R this can be as many as 36 (12 headings and 3 elevations). However we follow previous works only consider the navigable views, which is often many fewer than 36.
In general we found the finetuned LM to have no issue generating from the set of navigable directions (i.e., {Ct,0, . . . , Ct,V }) without constrained decoding.
However seeGudibande et al. (2023) for a critical discussion of this approach.
We cannot entirely rule out the possibility that the GPT-4 training set included the text instructions seen in R2R. However, while the text instructions may have been encountered, the trajectories were unlikely to have been encountered during pretraining since we used vision systems to obtain the captions/objects. Out of the 10,000 generated instructions, we did not find any instructions that were in the actual R2R dataset.
While we still underperform NavGPT, the performance gap is relatively narrow-within 1% in terms of SPL. We observe that NavGPT employs object information filtered by a ground-truth depth map, limiting the data to objects within a 3-meter range. Such filtering is important to mitigate the redundancy and noise often associated with unfiltered object information (i.e., often too many irrelevant objects aredetected). As highlighted in the NavGPT paper, this selective use of object information is important for achieving good performance.
We chose GPT-4-turbo for its lower cost.
Given that RecBert(Hong et al., 2021) has similar performance to DuET(Chen et al., 2022) in the few-shot setting according to table 1, we choose RecBert to be the baseline because it is simpler and does not require a topological map.
AcknowledgementsThis work is supported by MIT-IBM Watson AI Lab.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.06757On evaluation of embodied navigation agents. 2018aarXiv preprint</p>
<p>Sim-to-real transfer for vision-andlanguage navigation. Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee, Conference on Robot Learning. PMLR2021</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018b</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, Conference on Robot Learning. PMLR2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021aarXiv preprint</p>
<p>History aware multimodal transformer for vision-and-language navigation. Advances in neural information processing systems. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, 2021b34</p>
<p>Think global, act local: Dual-scale graph transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, arXiv:2302.13007Chataug: Leveraging chatgpt for text data augmentation. 2023arXiv preprint</p>
<p>The symbolic species: The co-evolution of language and the brain. Terrence William Deacon, 1997WW Norton &amp; Company202</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Human Language Technologies. the 2019 Conference of the North American ChapterMinnesotaAssociation for Computational Linguistics20191Long and Short Papers. Minneapolis</p>
<p>Clip-nav: Using clip for zeroshot vision-and-language navigation. Gunnar Vishnu Sashank Dorbala, Robinson Sigurdsson, Jesse Piramuthu, Thomason, Gaurav S Sukhatme, arXiv:2211.166492022arXiv preprint</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems. 201831</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary llms. 2023arXiv preprint</p>
<p>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, arXiv:2306.11644Textbooks are all you need. 2023arXiv preprint</p>
<p>Towards learning a generic agent for vision-and-language navigation via pretraining. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, Conference on Computer Vision and Pattern Recognition (CVPR). 2020</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>A recurrent visionand-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions. Peter A Jansen, arXiv:2009.142592020arXiv preprint</p>
<p>Agent with the big picture: Perceiving surroundings for interactive instruction following. Byeonghwi Kim, Suvaansh Bhambri, Pratap Kunal, Roozbeh Singh, Jonghyun Mottaghi, Choi, Embodied AI Workshop CVPR. 202127</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXivAI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>Shuhei Kurita, Kyunghyun Cho, arXiv:2009.07783Generative language-grounded policy in vision-andlanguage navigation with bayes' rule. 2020arXiv preprint</p>
<p>Co-training improves promptbased learning for large language models. Hunter Lang, Monica N Agrawal, Yoon Kim, David Sontag, International Conference on Machine Learning. PMLR2022</p>
<p>Lampp: Language models as probabilistic priors for perception and action. Belinda Z Li, William Chen, Pratyusha Sharma, Jacob Andreas, 20232302arXiv e-prints</p>
<p>Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, ICML. 2022a</p>
<p>Pre-trained language models for interactive decisionmaking. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Ilya Loshchilov, Frank Hutter, ; Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-Regib, Zsolt Kira, Richard Socher, Caiming Xiong, arXiv:1711.05101arXiv:1901.03035Decoupled weight decay regularization. 2017. 2019arXiv preprint</p>
<p>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. Khanh Nguyen, Hal Daumé, Iii , arXiv:1909.018712019arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Episodic transformer for vision-and-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao, arXiv:2304.03277Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Anton Van Den Hengel, and Qi Wu. 2021. The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation. Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision</p>
<p>Hop: history-andorder aware pre-training for vision-and-language navigation. Yanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, Qi Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Velma: Verbalization embodiment of llm agents for vision and language navigation in street view. Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang, Wang , arXiv:2307.060822023arXiv preprint</p>
<p>Value function spaces: Skill-centric state abstractions for long-horizon reasoning. Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, Brian Ichter, arXiv:2111.031892021arXiv preprint</p>
<p>Pratyusha Sharma, Antonio Torralba, Jacob Andreas, arXiv:2110.01517Skill induction and planning with latent language. 2021arXiv preprint</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Factorizing perception and policy for interactive instruction following. Pratap Kunal, Suvaansh Singh, Byeonghwi Bhambri, Roozbeh Kim, Jonghyun Mottaghi, Choi, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.049272021arXiv preprint</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, arXiv:1904.041952019arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Vision-and-dialog navigation. Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, Conference on Robot Learning. PMLR2020</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. </p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng, arXiv:2108.13487Want to reduce labeling cost? gpt-3 can help. 2021arXiv preprint</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Scaling data generation in vision-and-language navigation. Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao ; Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, arXiv:2307.15644arXiv:2109.01652Finetuned language models are zero-shot learners. Quoc V Dai, Le, 2023arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2022In Proceedings of NeurIPS</p>
<p>Lamini-lm: A diverse herd of distilled models from large-scale instructions. Minghao Wu, Abdul Waheed, Chiyu Zhang, CoRR, abs/2304.144022023Muhammad Abdul-Mageed, and Alham Fikri Aji</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, 2023</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, arXiv:2305.140782023arXiv preprint</p>
<p>Navgpt: Explicit reasoning in vision-and-language navigation with large language models. Gengze Zhou, Yicong Hong, Qi Wu, arXiv:2305.169862023aarXiv preprint</p>
<p>Esc: Exploration with soft commonsense constraints for zero-shot object navigation. Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, Xin Eric, Wang , arXiv:2301.131662023barXiv preprint</p>
<p>Deformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, arXiv:2010.041592020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
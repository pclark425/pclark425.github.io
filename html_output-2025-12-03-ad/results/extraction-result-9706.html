<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9706 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9706</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9706</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-279118470</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.02058v1.pdf" target="_blank">Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?</a></p>
                <p><strong>Paper Abstract:</strong> Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9706.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9706.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Summation (KnowSum) Unseen Knowledge Estimation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular five-step pipeline (generation, verification, clustering, prevalence estimation, unseen estimation) that treats LLM outputs as samples and extrapolates unobserved discrete knowledge items using prevalence histograms and a smoothed Good–Turing estimator to estimate total internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (ChatGPT-4o-chat, ChatGPT-3.5-turbo-chat, LLaMA-V3-70B-instruct, LLaMA-V3-3B-instruct, Mistral-7B-instruct-V0.1, Qwen2.5-7B-instruct, Claude-3.7-Sonnet, DeepSeek-V3, Gemini-1.5-flash)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A set of instruction-tuned and commercial LLMs evaluated in the paper (listed above); sizes and training details are model-specific and reported in the paper's model list.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (applied to mathematical theorems, human disease names, biomedical retrieval, and open-ended generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated sampling-based estimation pipeline: repeated randomized generation (large n), external verification against authoritative sources, semantic clustering to form distinct knowledge items, construction of prevalence histogram, and extrapolation of unseen items using smoothed Good–Turing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Counts of distinct verified knowledge items (N_seen), extrapolated unseen items (N_unseen(t)), and derived Seen Knowledge Ratio SKR(t) = N_seen / (N_seen + N_unseen(t)); sensitivity to prompt, sampling, truncation k, and extrapolation factor t is analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to (1) mathematical theorem sources (Wikipedia, MathSciNet, ProofWiki) for theorem counting; (2) Human Disease Ontology for disease counting; (3) BioASQ-QA Task12b (PubMed/MeSH) for IR; (4) open-ended prompts with semantic embeddings for diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>KnowSum reveals substantial unseen knowledge across tasks: in experiments LLMs typically express only 20–50% of estimated internal knowledge; SKR values vary by task (theorem/disease/counting vs IR vs diversity) and model, and including unseen estimates changes relative model rankings in several cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires task-specific verification and clustering pipelines (most brittle steps); applies only to discrete, countable knowledge units; accuracy depends on i.i.d. sampling assumption and verifier coverage; extrapolation uncertainty grows with t.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Complementary to traditional benchmark evaluations — rather than scoring correctness on fixed test sets, KnowSum estimates latent capacity (what models could output) and can reverse model rankings compared to observed-count-only evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use authoritative external verifiers and canonical clustering for the domain; select truncation k by held-out cross-validation; use high sampling temperature (Temp=1) and disable nucleus sampling to better reveal latent diversity; default extrapolation factor t chosen as 100 after saturation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9706.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smoothed Good–Turing Estimator (SGT) / Efron–Thisted-style estimator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smoothed variant of the Good–Turing estimator used to predict the number of distinct unseen items that would appear in additional queries, computed as a weighted sum of low-frequency prevalence counts with a truncation parameter k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set as KnowSum)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Used as the statistical core of KnowSum to extrapolate unseen knowledge from prevalence histograms obtained from sampled LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General statistical estimation (applied to language-model knowledge evaluation in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Estimate N_unseen(t) = sum_{s=1}^k h_s * n_s where n_s counts clusters observed exactly s times and h_s are SGT weights depending on extrapolation factor t and truncation k; choose k by cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy of predicted unseen counts evaluated by held-out validation (split sampled responses, predict unseen in held-out fraction) and normalized MSE across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used on prevalence histograms formed from (1) theorem names (n = 30,000 * 20 responses), (2) disease names (n = 3,000 * 50), (3) MeSH terms from BioASQ runs, and (4) clustered open-ended outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Held-out cross-validation shows SGT estimates closely match ground truth unseen counts for extrapolation factors up to tested ranges (r_obs = 1/2,1/3,1/4 corresponding to t up to 3 in validation); using first k prevalence counts (k≈6–10) gives stable results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Classic SGT has high variance if unsmoothed; selection of smoothing/truncation parameter k is important; estimates become more uncertain for large extrapolation factors t (variance grows and saturates around t≈80 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Statistically principled alternative to naive counting; unlike fixed-set benchmarks, SGT quantifies unseen mass and informs how far observed outputs undercount internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Adopt smoothed Good–Turing with truncation k selected by cross-validation; focus on low-frequency prevalence counts (singletons/doubletons) and avoid reliance on high-frequency tail; report SKR alongside N_seen/N_tot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9706.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SKR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seen Knowledge Ratio (SKR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dimensionless metric defined as SKR(t) = N_seen / (N_seen + N_unseen(t)) that summarizes the fraction of estimated total knowledge that has been exposed through sampling; decreases with larger extrapolation factor t.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same evaluated set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Metric applied uniformly across tasks to normalize estimates and compare knowledge exposure between models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (used for math theorems, diseases, IR MeSH terms, and diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute N_seen directly from clustered verified outputs and N_unseen(t) via SGT; SKR provides an interpretable fraction between 0 and 1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used as a summary statistic to report exposure; comparative analyses and sensitivity plots over t reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Reported for theorem counting, disease counting, BioASQ IR subtasks, and diversity tasks; typical experimental settings: t=100 by default.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Observed SKR values: generally 20–50% in theorem/disease counting experiments; IR Subtask 1 SKR ≈ 25%, Subtask 2 SKR ≈ 11%; diversity tasks SKR ≈ 23%; some models show higher SKR due to low diversity (e.g., DeepSeek-V3 and Gemini-1.5-flash showing SKR 0.7 and 0.3 respectively in one table entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SKR depends on choice of t and accuracy of SGT; high SKR may reflect low generative diversity rather than genuinely better exposure; SKR alone does not measure correctness or depth of understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike accuracy or F1, SKR quantifies exposure of internal knowledge rather than surface correctness; can reorder model rankings when combined with total estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report SKR with the extrapolation factor t and sample budget (n); examine SKR sensitivity over t (report curve) and interpret alongside diversity and correctness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9706.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification Criteria (strict/relaxed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific Verification Criteria (strict vs relaxed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-specific rules to accept generated items as valid: for theorems a strict rule requires the canonical name to contain 'theorem' while a relaxed rule accepts 12 math-concept terms; for diseases strict accepts only 'anatomical disease' while relaxed accepts any Disease Ontology entry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same evaluated set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Verification implemented via external sources: Wikipedia/MathSciNet/ProofWiki for theorems, Disease Ontology (DOID) matching with fuzzy matching for diseases, and ground-truth document matching for MeSH in BioASQ.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (theorems) and Biomedicine (diseases/MeSH)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated verification: sequential lookup across databases (stop at first match) and fuzzy matching (rapidfuzz threshold 0.9) for ontology synonyms; in IR verify retrieved PubMed IDs against gold-standard references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary validity of items according to strict/relaxed filters; affects N_seen and downstream SGT estimates; two validation levels reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Verification sources: Wikipedia, MathSciNet, ProofWiki (theorems); Human Disease Ontology OBO file (diseases); PubMed & NCBI E-utilities to map documents to MeSH (BioASQ).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Relaxed criteria increase both N_seen and N_tot; e.g., theorem 'theorem only' vs 'all math concepts' produce higher counts and can affect model ranking; theorems labeled 'theorem' are only ~10% of all math concepts in their reference set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verification coverage limited by external databases; fuzzy matching thresholds and URL-based canonicalization can merge or split items improperly; strictness trade-off influences both observed and estimated totals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More permissive (relaxed) criteria capture broader notion of knowledge but risk noise; mirrors human adjudication choices in corpus labeling (strict vs broad inclusion).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both strict and relaxed results when feasible; document verification procedure and thresholds; use canonical identifiers (URLs, DOIDs, PubMed IDs) for clustering to minimize ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9706.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theorem Counting (Application 1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application 1: Mathematical Theorem Knowledge Estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Counting and unseen estimation of mathematical theorem names produced by LLMs using repeated prompting (N_query=30,000, N_ans=20), verification against Wikipedia/MathSciNet/ProofWiki, clustering by URL/title, and SGT extrapolation (t=100).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (nine models listed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Instruction-tuned/commercial models prompted to list theorem names with sampling temperature 1 and nucleus sampling disabled to maximize diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (theorem enumeration / knowledge count)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sampling-based generation with verifier pipeline and clustering; prevalence histogram constructed and SGT applied; report N_seen, N_tot, SKR per model and under strict/relaxed validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N_seen (distinct verified theorems observed), N_tot = N_seen + N_unseen(t) (estimated total), SKR(t) summarizing exposure; sensitivity checks for prompts, N_query, N_ans, truncation k.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Reference theorem sources: Wikipedia, MathSciNet, ProofWiki combined to validate and canonicalize theorem names.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Most models show SKR well below 1 (commonly 20–50%); LLaMA-V3-70B-instruct reported the highest estimated total knowledge for theorems; accounting for unseen items changes model rankings (e.g., ChatGPT-4o-chat outranks ChatGPT-3.5 when considering N_tot despite lower N_seen).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Difficulty in defining 'theorem' (necessitates strict vs relaxed criteria); heavy-tailed prevalence due to over-generation of a few canonical theorems; clustering requires canonicalization to avoid duplicate names.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Extends beyond fixed benchmark tests by estimating the latent catalog of theorems an LLM encodes rather than scoring correctness on a fixed set.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use large sampling budgets to expose rare items, canonicalize via authoritative URLs, report both strict and relaxed validity results, choose k via held-out cross-validation, and present SKR curves across t.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9706.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioASQ IR Evaluation (Application 2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application 2: Information Retrieval on BioASQ-QA (Document Retrieval & QA MeSH-term estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate LLMs' retrieval capabilities by converting BioASQ questions into PubMed Boolean queries (document retrieval) and by QA conditioned on gold snippets; count retrieved MeSH terms as atomic knowledge units, then apply KnowSum to estimate additional unseen MeSH coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models generate PubMed queries or answers for N_query=340 biomedical questions; verification maps retrieved PubMed IDs to MeSH terms via NCBI E-utilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical information retrieval / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Traditional IR metrics reported (precision, recall, F1, ROUGE) and KnowSum used to estimate N_seen and N_tot of MeSH keywords recoverable by each model; clustering normalizes MeSH by top-level hierarchy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Document-level precision/recall/F1, snippet-level metrics, ROUGE for ideal answers; KnowSum metrics: N_seen (MeSH terms observed), N_tot (estimated total MeSH), SKR.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>BioASQ-QA Task12b Test dataset (340 questions) with gold-standard relevant PubMed documents and annotated MeSH terms.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Traditional IR: DeepSeek-V3 highest observed document-level F1 (16.76%). KnowSum: ChatGPT-3.5-turbo-chat had highest estimated total MeSH (N_tot=10367) for document retrieval, surpassing DeepSeek-V3 (N_tot=7750), showing latent retrieval capacity missed by standard metrics. SKR ≈25% for document retrieval and ≈11% for QA subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Counting MeSH from gold-matched documents may over-count relevance if documents contain extra MeSH not directly pertinent to question; QA subtask SKR lower due to stricter correctness criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>KnowSum reveals latent retrieval breadth beyond document-level F1/ROUGE, potentially reordering model rankings when considering unseen coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate both traditional IR metrics and KnowSum estimates to capture surface accuracy and latent coverage; normalize MeSH terms and cluster consistently; interpret N_tot relative to SKR and sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9706.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity Estimation (Application 3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application 3: Semantic Diversity Measurement via Embedding Clustering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Estimate how many semantically distinct open-ended outputs an LLM could generate by embedding responses (text-embedding-ada-002) into 1536-d vectors, threshold-based clustering (q-quantile of 10-NN distances), counting observed clusters, and applying SGT to extrapolate unseen distinct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models prompted with open-ended prompts (e.g., 'Name a possible LLM application') with N_query=1000 and Temp=1; embeddings obtained via OpenAI's text-embedding-ada-002 for clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General generative diversity / creativity assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No correctness verification; clustering by embedding distances with threshold determined by q-quantile of aggregated 10-NN distances (q in {0.2,0.3,0.5,0.7}); compute N_seen, SGT N_unseen, and SKR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Observed distinct clusters (N_seen), estimated total distinct outputs (N_tot), SKR; sensitivity to clustering threshold q reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Custom open-ended prompt sets used in the paper; embeddings via text-embedding-ada-002.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Most models show SKR ≈23% for diversity tasks; Claude-3.7-Sonnet observed the largest number of LLM applications, Mistral-7B achieved highest estimated total diversity in one task; DeepSeek-V3 and Gemini-1.5-flash produced fewer distinct outputs (more deterministic) and hence higher SKRs in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Clustering depends on embedding model and threshold q; semantic-equivalence thresholding can under- or over-merge distinct creative outputs; no ground-truth 'correct' diversity measure exists.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides a scalable, interpretable complement to heuristic diversity metrics (type-token ratio, perplexity) and human creativity tests by estimating count of semantically distinct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report results across multiple q thresholds and show SKR robustness; use strong semantic embeddings and justify chosen quantile; pair diversity estimates with qualitative inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9706.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Held-out Cross-validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Held-out Validation Procedure for SGT Parameter Selection and Accuracy Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation procedure that splits sampled responses into observed and held-out fractions (r_obs in {1/2,1/3,1/4}) to estimate unseen items in the held-out set and to select truncation k by minimizing normalized MSE across repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Applied to each model's sampled outputs to validate SGT predictions and choose k in {6,8,10} or other candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical validation for LLM knowledge estimation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Randomly shuffle full sampled dataset, treat first r_obs fraction as observed, predict number of unique items that appear only in held-out fraction, repeat 100 times and compare average predicted vs ground-truth unseen counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average predicted unseen vs actual held-out unseen counts; normalized mean squared error (MSE) used to select truncation k.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on theorem and disease sampled outputs and prevalence histograms derived therefrom.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SGT estimates closely match ground truth unseen counts across tested r_obs settings; most models perform best at k=8 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Validation limited by available sampled data (must be large enough); held-out splits correspond to limited extrapolation factors (t up to ~3 in validation examples), not necessarily to large t used in production (t=100).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides empirical check of extrapolation accuracy against held-out empirical observations, analogous to cross-validation in conventional ML.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use repeated random splits and report variance; select truncation k adaptively per model/task using this procedure; report validation plots comparing predicted vs ground-truth unseen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9706.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling & Prompt Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling Strategy and Prompt Design Sensitivity Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical findings on how sampling temperature, nucleus sampling, prompt clarity, and number of requested responses per query (N_ans) affect N_seen and N_tot: higher temperature (Temp=1) and disabling nucleus sampling increase observed and estimated diversity; clearer prompts and higher N_ans expose more internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Settings explored in experiments: Temp in {0.7,1}, nucleus sampling applied/disabled, prompt instruction clarity toggled, N_ans in {10,20,50} depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General LLM evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Controlled ablations across sampling temperature, nucleus sampling, prompt clarity, and N_ans with measurement of resulting N_seen, N_tot, and SKR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Changes in N_seen and N_tot, SKR, and variance of SGT estimates under different sampling/prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Analyzed within theorem and human disease counting experiments and reported in sensitivity figures.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Temp=1 and disabling nucleus sampling produce highest N_seen and N_tot; clearer instructions and higher N_ans increase diversity and both N_seen and N_tot; extrapolation factor t shows saturation around 80, motivating t=100 choice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher-temperature sampling may generate lower-quality or hallucinated items (necessitating stronger verification); experimental trade-off between diversity and factuality needs domain-specific verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Highlights that sampling policy and prompt engineering substantially change what is observed from LLMs, a consideration often omitted in fixed-benchmark evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use high temperature and disable nucleus sampling when goal is to reveal latent diversity, but pair with robust verifiers; test prompt variants and report sensitivity; choose N_ans to balance per-query cost and exposure of rare items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9706.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9706.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clustering & Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Decoding and Semantic Clustering Procedures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-specific decoding (extract canonical names from responses, split multi-answer responses) followed by clustering identical/semantically-equivalent items using canonical identifiers (URL-derived titles for theorems, DOIDs for diseases, embedding thresholds for open-ended outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (same set)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Clustering pipelines differ by application: deterministic canonicalization via URL parsing for theorem sources, fuzzy string matching to map to DOID for diseases, and embedding-distance thresholding (text-embedding-ada-002) for open-ended responses.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Preprocessing for discrete knowledge counting across mathematics, biomedical, and general generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Normalize extracted core knowledge via decoders, group semantically-equivalent instances into clusters, then count cluster prevalences to form {n_s}.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Cluster identity correctness (minimize false merges/splits) directly impacts n_s and thus SGT estimates; threshold choices and canonical parsing rules are documented and sensitivity-analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Uses URL/title normalization for Wikipedia/ProofWiki/MathSciNet, DOID mapping for Disease Ontology, and OpenAI embeddings for diversity tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Canonical identifiers (URLs/DOIDs) reduce duplication and make prevalence histograms stable; embedding-based clustering requires choice of q-quantile threshold but produced robust SKR (~23%) over several q values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Clustering errors (over/under-aggregation) bias prevalence counts and unseen estimates; embedding thresholds may conflate semantically distinct items when outputs are nuanced.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated clustering scales beyond human adjudication but requires careful validation and domain knowledge to design decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer canonical external identifiers where possible; perform sensitivity checks on clustering thresholds and report robustness; log mappings for auditability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Estimating the number of unseen species: How many words did Shakespeare know? <em>(Rating: 2)</em></li>
                <li>Good-Turing frequency estimation without tears <em>(Rating: 2)</em></li>
                <li>BioASQ-QA: A manually curated corpus for biomedical question answering <em>(Rating: 2)</em></li>
                <li>Human disease ontology 2022 update <em>(Rating: 2)</em></li>
                <li>Optimal prediction of the number of unseen species <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9706",
    "paper_id": "paper-279118470",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "KnowSum",
            "name_full": "Knowledge Summation (KnowSum) Unseen Knowledge Estimation Pipeline",
            "brief_description": "A modular five-step pipeline (generation, verification, clustering, prevalence estimation, unseen estimation) that treats LLM outputs as samples and extrapolates unobserved discrete knowledge items using prevalence histograms and a smoothed Good–Turing estimator to estimate total internal knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (ChatGPT-4o-chat, ChatGPT-3.5-turbo-chat, LLaMA-V3-70B-instruct, LLaMA-V3-3B-instruct, Mistral-7B-instruct-V0.1, Qwen2.5-7B-instruct, Claude-3.7-Sonnet, DeepSeek-V3, Gemini-1.5-flash)",
            "llm_description": "A set of instruction-tuned and commercial LLMs evaluated in the paper (listed above); sizes and training details are model-specific and reported in the paper's model list.",
            "scientific_domain": "General (applied to mathematical theorems, human disease names, biomedical retrieval, and open-ended generation tasks)",
            "evaluation_method": "Automated sampling-based estimation pipeline: repeated randomized generation (large n), external verification against authoritative sources, semantic clustering to form distinct knowledge items, construction of prevalence histogram, and extrapolation of unseen items using smoothed Good–Turing.",
            "evaluation_criteria": "Counts of distinct verified knowledge items (N_seen), extrapolated unseen items (N_unseen(t)), and derived Seen Knowledge Ratio SKR(t) = N_seen / (N_seen + N_unseen(t)); sensitivity to prompt, sampling, truncation k, and extrapolation factor t is analyzed.",
            "benchmark_or_dataset": "Applied to (1) mathematical theorem sources (Wikipedia, MathSciNet, ProofWiki) for theorem counting; (2) Human Disease Ontology for disease counting; (3) BioASQ-QA Task12b (PubMed/MeSH) for IR; (4) open-ended prompts with semantic embeddings for diversity.",
            "results_summary": "KnowSum reveals substantial unseen knowledge across tasks: in experiments LLMs typically express only 20–50% of estimated internal knowledge; SKR values vary by task (theorem/disease/counting vs IR vs diversity) and model, and including unseen estimates changes relative model rankings in several cases.",
            "limitations_or_challenges": "Requires task-specific verification and clustering pipelines (most brittle steps); applies only to discrete, countable knowledge units; accuracy depends on i.i.d. sampling assumption and verifier coverage; extrapolation uncertainty grows with t.",
            "comparison_to_human_or_traditional": "Complementary to traditional benchmark evaluations — rather than scoring correctness on fixed test sets, KnowSum estimates latent capacity (what models could output) and can reverse model rankings compared to observed-count-only evaluations.",
            "recommendations_or_best_practices": "Use authoritative external verifiers and canonical clustering for the domain; select truncation k by held-out cross-validation; use high sampling temperature (Temp=1) and disable nucleus sampling to better reveal latent diversity; default extrapolation factor t chosen as 100 after saturation analysis.",
            "uuid": "e9706.0",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SGT",
            "name_full": "Smoothed Good–Turing Estimator (SGT) / Efron–Thisted-style estimator",
            "brief_description": "A smoothed variant of the Good–Turing estimator used to predict the number of distinct unseen items that would appear in additional queries, computed as a weighted sum of low-frequency prevalence counts with a truncation parameter k.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set as KnowSum)",
            "llm_description": "Used as the statistical core of KnowSum to extrapolate unseen knowledge from prevalence histograms obtained from sampled LLM outputs.",
            "scientific_domain": "General statistical estimation (applied to language-model knowledge evaluation in this paper)",
            "evaluation_method": "Estimate N_unseen(t) = sum_{s=1}^k h_s * n_s where n_s counts clusters observed exactly s times and h_s are SGT weights depending on extrapolation factor t and truncation k; choose k by cross-validation.",
            "evaluation_criteria": "Accuracy of predicted unseen counts evaluated by held-out validation (split sampled responses, predict unseen in held-out fraction) and normalized MSE across runs.",
            "benchmark_or_dataset": "Used on prevalence histograms formed from (1) theorem names (n = 30,000 * 20 responses), (2) disease names (n = 3,000 * 50), (3) MeSH terms from BioASQ runs, and (4) clustered open-ended outputs.",
            "results_summary": "Held-out cross-validation shows SGT estimates closely match ground truth unseen counts for extrapolation factors up to tested ranges (r_obs = 1/2,1/3,1/4 corresponding to t up to 3 in validation); using first k prevalence counts (k≈6–10) gives stable results.",
            "limitations_or_challenges": "Classic SGT has high variance if unsmoothed; selection of smoothing/truncation parameter k is important; estimates become more uncertain for large extrapolation factors t (variance grows and saturates around t≈80 in experiments).",
            "comparison_to_human_or_traditional": "Statistically principled alternative to naive counting; unlike fixed-set benchmarks, SGT quantifies unseen mass and informs how far observed outputs undercount internal knowledge.",
            "recommendations_or_best_practices": "Adopt smoothed Good–Turing with truncation k selected by cross-validation; focus on low-frequency prevalence counts (singletons/doubletons) and avoid reliance on high-frequency tail; report SKR alongside N_seen/N_tot.",
            "uuid": "e9706.1",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SKR",
            "name_full": "Seen Knowledge Ratio (SKR)",
            "brief_description": "A dimensionless metric defined as SKR(t) = N_seen / (N_seen + N_unseen(t)) that summarizes the fraction of estimated total knowledge that has been exposed through sampling; decreases with larger extrapolation factor t.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (same evaluated set)",
            "llm_description": "Metric applied uniformly across tasks to normalize estimates and compare knowledge exposure between models and tasks.",
            "scientific_domain": "General (used for math theorems, diseases, IR MeSH terms, and diversity)",
            "evaluation_method": "Compute N_seen directly from clustered verified outputs and N_unseen(t) via SGT; SKR provides an interpretable fraction between 0 and 1.",
            "evaluation_criteria": "Used as a summary statistic to report exposure; comparative analyses and sensitivity plots over t reported.",
            "benchmark_or_dataset": "Reported for theorem counting, disease counting, BioASQ IR subtasks, and diversity tasks; typical experimental settings: t=100 by default.",
            "results_summary": "Observed SKR values: generally 20–50% in theorem/disease counting experiments; IR Subtask 1 SKR ≈ 25%, Subtask 2 SKR ≈ 11%; diversity tasks SKR ≈ 23%; some models show higher SKR due to low diversity (e.g., DeepSeek-V3 and Gemini-1.5-flash showing SKR 0.7 and 0.3 respectively in one table entry).",
            "limitations_or_challenges": "SKR depends on choice of t and accuracy of SGT; high SKR may reflect low generative diversity rather than genuinely better exposure; SKR alone does not measure correctness or depth of understanding.",
            "comparison_to_human_or_traditional": "Unlike accuracy or F1, SKR quantifies exposure of internal knowledge rather than surface correctness; can reorder model rankings when combined with total estimates.",
            "recommendations_or_best_practices": "Report SKR with the extrapolation factor t and sample budget (n); examine SKR sensitivity over t (report curve) and interpret alongside diversity and correctness metrics.",
            "uuid": "e9706.2",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Verification Criteria (strict/relaxed)",
            "name_full": "Task-specific Verification Criteria (strict vs relaxed)",
            "brief_description": "Domain-specific rules to accept generated items as valid: for theorems a strict rule requires the canonical name to contain 'theorem' while a relaxed rule accepts 12 math-concept terms; for diseases strict accepts only 'anatomical disease' while relaxed accepts any Disease Ontology entry.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (same evaluated set)",
            "llm_description": "Verification implemented via external sources: Wikipedia/MathSciNet/ProofWiki for theorems, Disease Ontology (DOID) matching with fuzzy matching for diseases, and ground-truth document matching for MeSH in BioASQ.",
            "scientific_domain": "Mathematics (theorems) and Biomedicine (diseases/MeSH)",
            "evaluation_method": "Automated verification: sequential lookup across databases (stop at first match) and fuzzy matching (rapidfuzz threshold 0.9) for ontology synonyms; in IR verify retrieved PubMed IDs against gold-standard references.",
            "evaluation_criteria": "Binary validity of items according to strict/relaxed filters; affects N_seen and downstream SGT estimates; two validation levels reported in tables.",
            "benchmark_or_dataset": "Verification sources: Wikipedia, MathSciNet, ProofWiki (theorems); Human Disease Ontology OBO file (diseases); PubMed & NCBI E-utilities to map documents to MeSH (BioASQ).",
            "results_summary": "Relaxed criteria increase both N_seen and N_tot; e.g., theorem 'theorem only' vs 'all math concepts' produce higher counts and can affect model ranking; theorems labeled 'theorem' are only ~10% of all math concepts in their reference set.",
            "limitations_or_challenges": "Verification coverage limited by external databases; fuzzy matching thresholds and URL-based canonicalization can merge or split items improperly; strictness trade-off influences both observed and estimated totals.",
            "comparison_to_human_or_traditional": "More permissive (relaxed) criteria capture broader notion of knowledge but risk noise; mirrors human adjudication choices in corpus labeling (strict vs broad inclusion).",
            "recommendations_or_best_practices": "Report both strict and relaxed results when feasible; document verification procedure and thresholds; use canonical identifiers (URLs, DOIDs, PubMed IDs) for clustering to minimize ambiguity.",
            "uuid": "e9706.3",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Theorem Counting (Application 1)",
            "name_full": "Application 1: Mathematical Theorem Knowledge Estimation",
            "brief_description": "Counting and unseen estimation of mathematical theorem names produced by LLMs using repeated prompting (N_query=30,000, N_ans=20), verification against Wikipedia/MathSciNet/ProofWiki, clustering by URL/title, and SGT extrapolation (t=100).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (nine models listed in paper)",
            "llm_description": "Instruction-tuned/commercial models prompted to list theorem names with sampling temperature 1 and nucleus sampling disabled to maximize diversity.",
            "scientific_domain": "Mathematics (theorem enumeration / knowledge count)",
            "evaluation_method": "Sampling-based generation with verifier pipeline and clustering; prevalence histogram constructed and SGT applied; report N_seen, N_tot, SKR per model and under strict/relaxed validation.",
            "evaluation_criteria": "N_seen (distinct verified theorems observed), N_tot = N_seen + N_unseen(t) (estimated total), SKR(t) summarizing exposure; sensitivity checks for prompts, N_query, N_ans, truncation k.",
            "benchmark_or_dataset": "Reference theorem sources: Wikipedia, MathSciNet, ProofWiki combined to validate and canonicalize theorem names.",
            "results_summary": "Most models show SKR well below 1 (commonly 20–50%); LLaMA-V3-70B-instruct reported the highest estimated total knowledge for theorems; accounting for unseen items changes model rankings (e.g., ChatGPT-4o-chat outranks ChatGPT-3.5 when considering N_tot despite lower N_seen).",
            "limitations_or_challenges": "Difficulty in defining 'theorem' (necessitates strict vs relaxed criteria); heavy-tailed prevalence due to over-generation of a few canonical theorems; clustering requires canonicalization to avoid duplicate names.",
            "comparison_to_human_or_traditional": "Extends beyond fixed benchmark tests by estimating the latent catalog of theorems an LLM encodes rather than scoring correctness on a fixed set.",
            "recommendations_or_best_practices": "Use large sampling budgets to expose rare items, canonicalize via authoritative URLs, report both strict and relaxed validity results, choose k via held-out cross-validation, and present SKR curves across t.",
            "uuid": "e9706.4",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BioASQ IR Evaluation (Application 2)",
            "name_full": "Application 2: Information Retrieval on BioASQ-QA (Document Retrieval & QA MeSH-term estimation)",
            "brief_description": "Evaluate LLMs' retrieval capabilities by converting BioASQ questions into PubMed Boolean queries (document retrieval) and by QA conditioned on gold snippets; count retrieved MeSH terms as atomic knowledge units, then apply KnowSum to estimate additional unseen MeSH coverage.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set)",
            "llm_description": "Models generate PubMed queries or answers for N_query=340 biomedical questions; verification maps retrieved PubMed IDs to MeSH terms via NCBI E-utilities.",
            "scientific_domain": "Biomedical information retrieval / question answering",
            "evaluation_method": "Traditional IR metrics reported (precision, recall, F1, ROUGE) and KnowSum used to estimate N_seen and N_tot of MeSH keywords recoverable by each model; clustering normalizes MeSH by top-level hierarchy.",
            "evaluation_criteria": "Document-level precision/recall/F1, snippet-level metrics, ROUGE for ideal answers; KnowSum metrics: N_seen (MeSH terms observed), N_tot (estimated total MeSH), SKR.",
            "benchmark_or_dataset": "BioASQ-QA Task12b Test dataset (340 questions) with gold-standard relevant PubMed documents and annotated MeSH terms.",
            "results_summary": "Traditional IR: DeepSeek-V3 highest observed document-level F1 (16.76%). KnowSum: ChatGPT-3.5-turbo-chat had highest estimated total MeSH (N_tot=10367) for document retrieval, surpassing DeepSeek-V3 (N_tot=7750), showing latent retrieval capacity missed by standard metrics. SKR ≈25% for document retrieval and ≈11% for QA subtask.",
            "limitations_or_challenges": "Counting MeSH from gold-matched documents may over-count relevance if documents contain extra MeSH not directly pertinent to question; QA subtask SKR lower due to stricter correctness criteria.",
            "comparison_to_human_or_traditional": "KnowSum reveals latent retrieval breadth beyond document-level F1/ROUGE, potentially reordering model rankings when considering unseen coverage.",
            "recommendations_or_best_practices": "Evaluate both traditional IR metrics and KnowSum estimates to capture surface accuracy and latent coverage; normalize MeSH terms and cluster consistently; interpret N_tot relative to SKR and sample budgets.",
            "uuid": "e9706.5",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Diversity Estimation (Application 3)",
            "name_full": "Application 3: Semantic Diversity Measurement via Embedding Clustering",
            "brief_description": "Estimate how many semantically distinct open-ended outputs an LLM could generate by embedding responses (text-embedding-ada-002) into 1536-d vectors, threshold-based clustering (q-quantile of 10-NN distances), counting observed clusters, and applying SGT to extrapolate unseen distinct outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set)",
            "llm_description": "Models prompted with open-ended prompts (e.g., 'Name a possible LLM application') with N_query=1000 and Temp=1; embeddings obtained via OpenAI's text-embedding-ada-002 for clustering.",
            "scientific_domain": "General generative diversity / creativity assessment",
            "evaluation_method": "No correctness verification; clustering by embedding distances with threshold determined by q-quantile of aggregated 10-NN distances (q in {0.2,0.3,0.5,0.7}); compute N_seen, SGT N_unseen, and SKR.",
            "evaluation_criteria": "Observed distinct clusters (N_seen), estimated total distinct outputs (N_tot), SKR; sensitivity to clustering threshold q reported.",
            "benchmark_or_dataset": "Custom open-ended prompt sets used in the paper; embeddings via text-embedding-ada-002.",
            "results_summary": "Most models show SKR ≈23% for diversity tasks; Claude-3.7-Sonnet observed the largest number of LLM applications, Mistral-7B achieved highest estimated total diversity in one task; DeepSeek-V3 and Gemini-1.5-flash produced fewer distinct outputs (more deterministic) and hence higher SKRs in some cases.",
            "limitations_or_challenges": "Clustering depends on embedding model and threshold q; semantic-equivalence thresholding can under- or over-merge distinct creative outputs; no ground-truth 'correct' diversity measure exists.",
            "comparison_to_human_or_traditional": "Provides a scalable, interpretable complement to heuristic diversity metrics (type-token ratio, perplexity) and human creativity tests by estimating count of semantically distinct outputs.",
            "recommendations_or_best_practices": "Report results across multiple q thresholds and show SKR robustness; use strong semantic embeddings and justify chosen quantile; pair diversity estimates with qualitative inspection.",
            "uuid": "e9706.6",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Held-out Cross-validation",
            "name_full": "Held-out Validation Procedure for SGT Parameter Selection and Accuracy Assessment",
            "brief_description": "A validation procedure that splits sampled responses into observed and held-out fractions (r_obs in {1/2,1/3,1/4}) to estimate unseen items in the held-out set and to select truncation k by minimizing normalized MSE across repeats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set)",
            "llm_description": "Applied to each model's sampled outputs to validate SGT predictions and choose k in {6,8,10} or other candidates.",
            "scientific_domain": "Statistical validation for LLM knowledge estimation",
            "evaluation_method": "Randomly shuffle full sampled dataset, treat first r_obs fraction as observed, predict number of unique items that appear only in held-out fraction, repeat 100 times and compare average predicted vs ground-truth unseen counts.",
            "evaluation_criteria": "Average predicted unseen vs actual held-out unseen counts; normalized mean squared error (MSE) used to select truncation k.",
            "benchmark_or_dataset": "Applied on theorem and disease sampled outputs and prevalence histograms derived therefrom.",
            "results_summary": "SGT estimates closely match ground truth unseen counts across tested r_obs settings; most models perform best at k=8 in experiments.",
            "limitations_or_challenges": "Validation limited by available sampled data (must be large enough); held-out splits correspond to limited extrapolation factors (t up to ~3 in validation examples), not necessarily to large t used in production (t=100).",
            "comparison_to_human_or_traditional": "Provides empirical check of extrapolation accuracy against held-out empirical observations, analogous to cross-validation in conventional ML.",
            "recommendations_or_best_practices": "Use repeated random splits and report variance; select truncation k adaptively per model/task using this procedure; report validation plots comparing predicted vs ground-truth unseen.",
            "uuid": "e9706.7",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Sampling & Prompt Sensitivity",
            "name_full": "Sampling Strategy and Prompt Design Sensitivity Analysis",
            "brief_description": "Empirical findings on how sampling temperature, nucleus sampling, prompt clarity, and number of requested responses per query (N_ans) affect N_seen and N_tot: higher temperature (Temp=1) and disabling nucleus sampling increase observed and estimated diversity; clearer prompts and higher N_ans expose more internal knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set)",
            "llm_description": "Settings explored in experiments: Temp in {0.7,1}, nucleus sampling applied/disabled, prompt instruction clarity toggled, N_ans in {10,20,50} depending on task.",
            "scientific_domain": "General LLM evaluation methodology",
            "evaluation_method": "Controlled ablations across sampling temperature, nucleus sampling, prompt clarity, and N_ans with measurement of resulting N_seen, N_tot, and SKR.",
            "evaluation_criteria": "Changes in N_seen and N_tot, SKR, and variance of SGT estimates under different sampling/prompt settings.",
            "benchmark_or_dataset": "Analyzed within theorem and human disease counting experiments and reported in sensitivity figures.",
            "results_summary": "Temp=1 and disabling nucleus sampling produce highest N_seen and N_tot; clearer instructions and higher N_ans increase diversity and both N_seen and N_tot; extrapolation factor t shows saturation around 80, motivating t=100 choice.",
            "limitations_or_challenges": "Higher-temperature sampling may generate lower-quality or hallucinated items (necessitating stronger verification); experimental trade-off between diversity and factuality needs domain-specific verifier.",
            "comparison_to_human_or_traditional": "Highlights that sampling policy and prompt engineering substantially change what is observed from LLMs, a consideration often omitted in fixed-benchmark evaluations.",
            "recommendations_or_best_practices": "Use high temperature and disable nucleus sampling when goal is to reveal latent diversity, but pair with robust verifiers; test prompt variants and report sensitivity; choose N_ans to balance per-query cost and exposure of rare items.",
            "uuid": "e9706.8",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Clustering & Decoding",
            "name_full": "Knowledge Decoding and Semantic Clustering Procedures",
            "brief_description": "Domain-specific decoding (extract canonical names from responses, split multi-answer responses) followed by clustering identical/semantically-equivalent items using canonical identifiers (URL-derived titles for theorems, DOIDs for diseases, embedding thresholds for open-ended outputs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (same set)",
            "llm_description": "Clustering pipelines differ by application: deterministic canonicalization via URL parsing for theorem sources, fuzzy string matching to map to DOID for diseases, and embedding-distance thresholding (text-embedding-ada-002) for open-ended responses.",
            "scientific_domain": "Preprocessing for discrete knowledge counting across mathematics, biomedical, and general generation",
            "evaluation_method": "Normalize extracted core knowledge via decoders, group semantically-equivalent instances into clusters, then count cluster prevalences to form {n_s}.",
            "evaluation_criteria": "Cluster identity correctness (minimize false merges/splits) directly impacts n_s and thus SGT estimates; threshold choices and canonical parsing rules are documented and sensitivity-analyzed.",
            "benchmark_or_dataset": "Uses URL/title normalization for Wikipedia/ProofWiki/MathSciNet, DOID mapping for Disease Ontology, and OpenAI embeddings for diversity tasks.",
            "results_summary": "Canonical identifiers (URLs/DOIDs) reduce duplication and make prevalence histograms stable; embedding-based clustering requires choice of q-quantile threshold but produced robust SKR (~23%) over several q values.",
            "limitations_or_challenges": "Clustering errors (over/under-aggregation) bias prevalence counts and unseen estimates; embedding thresholds may conflate semantically distinct items when outputs are nuanced.",
            "comparison_to_human_or_traditional": "Automated clustering scales beyond human adjudication but requires careful validation and domain knowledge to design decoders.",
            "recommendations_or_best_practices": "Prefer canonical external identifiers where possible; perform sensitivity checks on clustering thresholds and report robustness; log mappings for auditability.",
            "uuid": "e9706.9",
            "source_info": {
                "paper_title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Estimating the number of unseen species: How many words did Shakespeare know?",
            "rating": 2,
            "sanitized_title": "estimating_the_number_of_unseen_species_how_many_words_did_shakespeare_know"
        },
        {
            "paper_title": "Good-Turing frequency estimation without tears",
            "rating": 2,
            "sanitized_title": "goodturing_frequency_estimation_without_tears"
        },
        {
            "paper_title": "BioASQ-QA: A manually curated corpus for biomedical question answering",
            "rating": 2,
            "sanitized_title": "bioasqqa_a_manually_curated_corpus_for_biomedical_question_answering"
        },
        {
            "paper_title": "Human disease ontology 2022 update",
            "rating": 2,
            "sanitized_title": "human_disease_ontology_2022_update"
        },
        {
            "paper_title": "Optimal prediction of the number of unseen species",
            "rating": 1,
            "sanitized_title": "optimal_prediction_of_the_number_of_unseen_species"
        }
    ],
    "cost": 0.02033125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?
June 1, 2025</p>
<p>Xiang Li 
University of Pennsylvania</p>
<p>Jiayi Xin jiayixin@seas.upenn.edu 
University of Pennsylvania</p>
<p>Qi Long qlong@upenn.edu 
University of Pennsylvania</p>
<p>Weijie J Su 
University of Pennsylvania</p>
<p>User Thm3, Thm1Thm5, Thm4</p>
<p>Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?
June 1, 2025C3B2FFAB06C843016E3F23967D0F6E24arXiv:2506.02058v1[cs.CL]
Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development.However, current evaluations often inconsistently reflect the actual capacities of these models.In this paper, we demonstrate that one of many contributing factors to this evaluation crisis is the oversight of unseen knowledge-information encoded by LLMs but not directly observed or not yet observed during evaluations.We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks.KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances.We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity.Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance.Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have emerged as a transformative technology in artificial intelligence, demonstrating remarkable capabilities across diverse domains [4,1].Originally developed for natural language tasks such as translation, summarization, and dialogue generation, LLMs now exhibit increasingly sophisticated forms of reasoning and problem-solving [6,60,7].They can contribute to clinical and biomedical decision-making [53], tackle mathematical problems with verifier assistance [14], advance research in chemistry and physics [55], and synthesize knowledge across multiple scientific disciplines [7].As the scope of their application continues to expand, LLMs are poised to become integral tools in scientific research, significantly influencing how knowledge is generated and utilized.</p>
<p>This remarkable progress has been facilitated, in large part, by LLM evaluation [18,13,10].LLM evaluation refers to the process of assessing and understanding model effectiveness through benchmark datasets and standardized protocols, which inform practitioners and guide model development.LLaMA-V3-70B-instruct [24], ChatGPT-3.5-turbo-chat[38],</p>
<p>ChatGPT-4o-chat [28],</p>
<p>Mistral-7B-instruct-V0.1 [29], and Qwen2.5-7B-instruct[62].</p>
<p>While evaluation has been fundamental to machine learning and AI progress since its early days, it has become increasingly critical nowadays as AI research has evolved into a predominantly empirical science, with advancement driven largely by experimentation and iterative refinement [54,51].Consequently, comprehensive evaluation of LLM capabilities is essential for identifying promising research directions and optimizing resource allocation, particularly given the substantial computational and energy requirements of AI development [46,5].Nevertheless, the field currently confronts a crisis of evaluation [32,52], characterized by a growing disconnect between benchmark performance and genuine model generalization capacity.Multiple factors contribute to this crisis, including benchmark contamination [47], overfitting through repeated leaderboard submissions [52], and narrow test-time optimization strategies [48,15,63], all of which artificially inflate performance metrics.However, a fundamental statistical limitation underlying this evaluation crisis remains largely unaddressed.Current evaluation methodologies typically consider only observable outputs from benchmark datasets.Due to the stochastic nature of generation, LLMs may possess substantial knowledge that remains unexpressed under constrained query budgets [61].For instance, when attempting to assess an LLM's knowledge of mathematical theorems by directly prompting it to enumerate such conditions, the model would predominantly list common theorems, while rare theorems-despite being within the model's knowledge repertoire-might not surface even after hundreds of queries (see Figure 2). 1he aim of this paper is to quantify the extent of latent knowledge possessed by LLMs beyond observable outputs and to develop robust statistical methodologies for this purpose.While estimating the unseen may initially appear intractable, we demonstrate that by conceptualizing LLM outputs as a sampling process-analogous to drawing samples from Poisson or multinomial distributions-we can apply statistically principled techniques to infer unexpressed knowledge from observed knowledge counts.This challenge aligns with classical statistical problems such as estimating unseen species in ecology [22] and inferring the number of unrecorded words in linguistics [19].</p>
<p>To address this challenge, we introduce KnowSum, a general-purpose statistical framework specifically designed to estimate unseen knowledge in LLMs.Rather than relying solely on observed outputs, KnowSum quantifies the internal knowledge of LLMs by extrapolating observed frequency distributions and employing the smoothed Good-Turing estimator [41].A particularly attractive feature of KnowSum is that it incurs no additional computational cost.Moreover, the framework is statistically grounded, conceptually simple, and applicable to any evaluation task that can be formulated as knowledge counting.We demonstrate KnowSum's versatility through three distinct applications: (1) estimating LLM knowledge of human diseases and mathematical theorems , quantifying retrieval coverage in biomedical document retrieval and question answering, and ( 3) measuring semantic diversity in open-ended generation tasks.Our findings reveal two key insights: (i) LLMs typically express only 20-50% of their estimated internal knowledge in our experiments, and (ii) accounting for unseen knowledge can markedly alter model rankings (see Figure 1).By shifting focus from "what the model outputs" to "what the model could potentially output," our work provides a novel perspective on model evaluation-one that more accurately reflects intrinsic capacity rather than merely observed performance.</p>
<p>Related Work</p>
<p>LLM evaluation.A common approach to evaluating LLM capabilities is to pose a fixed set of questions and assess the model's responses based on correctness, much like a human exam [10,13].Numerous benchmark datasets have been proposed for this purpose, spanning domains such as mathematics [14], healthcare [53], science [26,17], and reasoning [44].However, these evaluations are limited to observable outputs on predefined questions and fail to capture knowledge that the model may possess but does not express.Our work complements this by estimating what a model could say but has not yet-which broadens the scope of standard evaluations.</p>
<p>In information retrieval, evaluation typically relies on ranking-based metrics (e.g., precision, recall, MRR [16]) or generation-based metrics (e.g., BLEU [42], ROUGE [35]).These approaches rely on query sets collected in advance, which may not fully capture the range of inputs a model can handlethereby overlooking its potential to retrieve relevant information for unseen or underrepresented queries.In contrast, our method estimates how much additional information a model could retrieve if more queries were drawn i.i.d.from the same distribution as the existing dataset, offering a broader view of retrieval coverage.For diversity measurement, traditional metrics often rely on heuristics like perplexity [27] or type-token ratios [57].Recent work [59] applies hypothesis testing to assess whether LLMs exhibit human-like creativity.In contrast, we estimate how many semantically distinct outputs a model could generate for open-ended prompts, providing a scalable and interpretable measure of diversity beyond what is directly observed.</p>
<p>Estimating the unseen.Estimating the number of unseen from the observed is a fundamental problem arising across many domains [19,11].Despite differences in context, these problems often share a common structure: inferring properties of an underlying distribution from limited samples, where rare observations provide important information about what remains unobserved.A classic and widely used approach is the Good-Turing estimator [22], originally developed to estimate the number of unseen species, and later extended in contexts such as vocabulary diversity in computational linguistics [19,21] and species richness in ecology [11,12].In the context of LLMs, it has been used to quantify unseen facts in training data [30] and to estimate memorized but unobserved facts in model outputs [37].Although the Good-Turing estimator is theoretically grounded [40,39,20] and provides unbiased estimates, it suffers from extremely high variance.To address this, smoothed variants have been proposed [19,41] that better balance bias and variance, resulting in more stable and more accurate estimates.Therefore, we adopt such a smoothed estimator as a component of our pipeline.</p>
<p>Memorization, knowledge retrieval, and knowledge inference.Pre-trained language models are known to store a vast amount of knowledge [43,6,58], and extensive research has investigated this capability.In many contexts, this knowledge resembles memorization-specifically, the ability to reproduce subtexts from the training dataset.Several works have focused on quantifying such memorization [8,65,37,50], typically by crafting prompts that elicit memorized subtexts.Beyond memorization, several studies have examined how and when LLMs acquire factual knowledge during pretraining [34,2,9,45], typically using external datasets as ground truth to score correctness.In contrast, we do not rely on predefined answers for grading or comparison.Instead, our focus is on estimating internal knowledge, that is, information encoded in the model that has not surfaced in sampled outputs.Additionally, there is emerging work that infers properties of an LLM's internal knowledge.For example, [23] predicts how knowledgeable a model is about a specific entity based on its internal activations before any output is produced.[61] estimates the probability of a binary output property using importance sampling, particularly when the event is too rare to be observed directly.While these methods similarly move beyond surface-level outputs, we focus on a different task-estimating the amount of internal knowledge.</p>
<p>Method</p>
<p>KnowSum: Unseen Knowledge Estimation Pipeline</p>
<p>Pipeline description.Algorithm 1 outlines our method which follows a five-step pipeline: generation, verification, clustering, prevalence estimation, and unseen estimation (see Figure 3).It begins by querying the model n times to generate candidate knowledge items.A verifier is then used to filter out invalid or hallucinated responses using external sources such as search engines or domain-specific databases.From each valid response, we extract the core knowledge using a decoder C, which typically removes stylistic variation to retain the essential content, for example, the name of a theorem.When a single response contains multiple answers, this step also splits them into individual items.The resulting knowledge items are then clustered based on semantic similarity, with each cluster representing a distinct piece of knowledge.For example, "Theorem of Pythagoras" and "Pythagorean theorem" would be treated as equivalent.We then construct a histogram {n s } s≥1 of item frequencies and apply the smoothed Good-Turing (SGT) estimator to predict the number of unseen items.Denoting by N unseen (t) the number of items that do not appear in the observed n queries but would appear in the subsequent t • n queries, the final estimate of total knowledge is given by N tot = N seen + N unseen (t), where N seen = s≥1 n s is the number of observed knowledge.5: Unseen Estimation: For given t, use the smoothed Good-Turing estimator N unseen (t) to compute the number of unseen knowledge items in tn additional queries.</p>
<p>Output: Estimated total knowledge: N tot := N seen + N unseen (t).</p>
<p>Modularity.KnowSum is highly modular: once verification and clustering are specified for a given task, the rest of the pipeline applies out of the box.These two steps are the most task-dependent, as they reflect how knowledge is defined and validated in each domain.With them in place, the remaining steps remain unchanged, making the framework broadly applicable to any domain with discrete, countable knowledge.We detail the implementation choices in Section 4.</p>
<p>Smoothed Good-Turing Estimator</p>
<p>To quantify the amount of knowledge that remains unexpressed by an LLM, we adopt the smoothed Good-Turing estimator N unseen (t) [41].This estimator predicts the number of new knowledge items that would appear in the next t • n queries, based on the empirical prevalence counts {n s } s≥1 collected from the first n queries, where n s denotes the number of items that appear exactly s times.The SGT estimator combines the first k of these counts into a weighted sum:
N unseen (t) := k s=1 h s • n s , where h s = −(−t) s P Bin k, 1 t + 1 ≥ s ,(1)
with k denoting a user-specified truncation parameter, and Bin(k, 1/(t + 1)) representing a binomial distribution with k trials and success probability 1/(t + 1).Initially known as the Efron-Thisted estimator [19], this estimator is provably near-optimal in terms of worst-case normalized mean squared error, up to subpolynomial factors [41].</p>
<p>The effectiveness of this estimator lies in a fundamental statistical insight: if each knowledge item emerges according to its own Poisson process, then rare items-those observed only once or twice-serve as indicators of a much larger set of unseen items with similarly low occurrence rates.Although the truly unseen items never appear in the data, their presence is inferred through these "near misses"-items that almost went unobserved.These rare prevalence counts act as statistical fingerprints of the hidden mass, enabling reasonable extrapolation of how many new items would surface with continued querying.</p>
<p>While the most direct signals come from singletons n 1 and doubletons n 2 , incorporating the first k prevalence counts {n s } k s=1 allows the estimator to more robustly capture the contribution of low-frequency items and thus improves stability when extrapolating further into the unseen.The following result shows how k controls the growth of the estimate N unseen (t) relative to the number of previously seen items N seen .For practical use, we select the optimal value of k from a candidate set using a cross-validation procedure (see Section 5.1 for details).</p>
<p>Theorem 1.If t ≥ k − 1, then the following holds :
N unseen (t) ≤ e kt t+1 • N seen .
This inequality also implies that the total predicted knowledge is at most a multiplicative factor of the observed knowledge.Motivated by this relationship, we introduce the seen knowledge ratio (SKR), which normalizes the estimate to yield a dimensionless quantity that reflects the proportion of exposed knowledge: Definition 1 (Seen Knowledge Ratio).The seen knowledge ratio for a fixed t is defined as
SKR(t) = N seen N seen + N unseen (t) .
When both t and n are sufficiently large (e.g., t = 10 2 , n = 10 5 in our experiments), the denominator N seen + N unseen (t) serves as an estimate of the total knowledge encoded in the model.The SKR thus offers an interpretable summary of knowledge exposure: a value close to 1 indicates that most of the model's accessible knowledge has already surfaced, while a lower value suggests that much remains hidden.Moreover, since N unseen (t) is non-decreasing in t, SKR naturally decreases as we consider further extrapolation into the model's unexpressed knowledge (see Figure 8).</p>
<p>Applications</p>
<p>In this section, we apply our KnowSum pipeline to three applications and evaluate the knowledge capabilities of several mainstream LLMs.At a high level, we focus on instruction-tuned or commercial models, as they are generally better at following prompts and adhering to task guidelines.Specifically, we evaluate nine popular LLMs, listed in the first column of Table 1.Due to space limits, additional experimental details are provided in Appendix B.</p>
<p>Application 1: Knowledge Estimation</p>
<p>Setup.Our first and most basic application is to estimate the total amount of knowledge an LLM possesses.We focus on two domains: mathematical theorems and human diseases.The pipeline begins by querying the target LLM N query times with a fixed prompt, each time requesting N ans instances of domain-specific knowledge.To validate the outputs, we use external databases and cluster the responses based on their external identifiers.Specifically, for mathematical theorems, we consult Wikipedia, MathSciNet, and ProofWiki, treating two theorems as identical if they are verified by the same webpage.For human diseases, we match each generated name against the Human Disease Ontology [49] and merge those that map to the same Disease Ontology Identifier (DOID).We set the sampling temperature to 1 to encourage diverse responses.To further promote diversity, we use (N query , N ans ) = (30,000,20) for theorems and (3,000,50) for diseases.Throughout all experiments, we fix the extrapolation factor at t = 100 with the number of total observations n = N query • N ans .We selected the best truncation level k ∈ {6, 8, 10} by cross-validation.An sensitivity study on the effects of the prompt, N query , and N ans is presented in Section 5.</p>
<p>Table 1: Results of knowledge estimation (Application 1) across different LLMs.</p>
<p>Model</p>
<p>Theorem only All math concepts Anatomical disease Human diseases Two verification criteria.Our analysis estimates an LLM's total knowledge across two domains: human diseases and mathematical theorems.Table 1 presents the counting results under two distinct validation criteria for each domain.For theorem counting, the first two columns of Table 1 detail our findings.We apply two validation criteria.The first, more restrictive setting, considers a response valid only if its name explicitly includes "theorem"-even if the item is on Wikipedia, we mark it invalid without that specific word.The second, broader criterion accepts a response if it contains any of twelve related terms: "theorem," "lemma," "law," "principle," "formula," "criterion," "identity," "conjecture," "rule," "equation," "postulate," or "corollary."The term "theorem" itself accounts for roughly 10% (2,558 entries) of the 24,762 identified mathematical concepts.The last two columns of Table 1 show results for human diseases.Similarly, we validate LLM-generated disease names using two criteria.Our initial, narrower criterion accepts responses only if they fall into the "anatomical disease" category (diseases affecting specific body parts), including recognized synonyms.Anatomical diseases make up about 51% (6,036 entries) of the 11,872 human diseases in the Disease Ontology.Our second, broader criterion considers a response valid if it matches any human disease within the Disease Ontology, encompassing all its subcategories.The distribution of these subcategories is provided in Figure 10 in the appendix.
N seen N tot SKR N seen N tot SKR N seen N tot SKR N seen N tot SKR ① ChatGPT-4o-chat702
A gap between observed and total knowledge.We observe that most SKR values are strictly less than 1, indicating that all evaluated LLMs possess unrevealed internal knowledge.For theorem counting, LLaMA-V3-70B-instruct achieves the highest estimated total knowledge under both criteria.In the disease domain, it also reports the largest N tot and the lowest SKR, suggesting broader coverage in biomedical knowledge.Gemini-1.5-flashalso exhibits a low SKR, likely due to its limited number of observed outputs, which implies a greater portion of its knowledge remains hidden.ChatGPT-4o-chat ChatGPT Impact of unseen knowledge on model ranking.A notable finding is that accounting for unseen knowledge can meaningfully affect model comparison.Take the "theorem only" setting as an example: based on the observed count N seen , ChatGPT-3.5-turbo-chatappears to outperform ChatGPT-4o-chat.However, when including unseen knowledge, ChatGPT-4o-chat is estimated to know more.This aligns with expectations, as the latter is trained on a larger corpus [1].A similar shift occurs under the broader validation criterion, where both N seen and N tot increase, but rankings still change.Moreover, we observe the same reversal in the disease domain: although DeepSeek-V3 and Gemini-1.5-flashappear comparable in observed counts, accounting for unseen knowledge shows that DeepSeek-V3 has greater estimated coverage.
-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instruct Mistral-7B-instruct-V0.1 Qwen2.5-7B-instruct Claude-3.7-Sonnet DeepSeek-V3 Gemini-1.5-flash 1 2 3 4 5 6 7 8 • • • 0 200 400 ChatGPT-4o-chat ChatGPT-3.5-turbo-chat LLaMa-V3-70B-instruct LLaMa-V3-3B-instruct Mistral-7B-instruct-V0.1 Qwen2.5-7B-instruct Claude-3.7-Sonnet DeepSeek-V3 Gemini-1.5-flash
Why unseen knowledge alters model ranking.A natural question is why accounting for unseen knowledge can change the ranking of LLMs.One might assume that a higher observed count N seen should imply a higher estimated total knowledge N tot , but this is misleading-N tot is not determined by N seen alone.Formally, N tot = s≥1 n s + N unseen (t), where n s is the number of knowledge items seen exactly s times.While N seen = s≥1 n s only counts distinct items, the SGT estimator depends on the prevalence distribution {n s } s≥1 , with coefficients that alternate in sign.Thus, even with the same N seen , differences in prevalence (that is, how frequently items appear) can lead to different unseen estimates.Figure 4 illustrates this: all models show a decay in n s , but their scales and shapes differ.Notably, for theorems, ChatGPT-3.5-turbo-chathas uniformly higher n s values than ChatGPT-4o-chat, yet receives a lower total knowledge estimate.This is because the sharper decay in the latter's prevalence suggests that it generates more rare or distinct items-leading to a higher unseen estimate and thus greater overall knowledge, despite having fewer observed items.</p>
<p>Heavy-tailed prevalence.The prevalence {n s } s≥1 has a heavy-tailed distribution, that is, we observe nonzero counts n s ≥ 1 even for large values of s.This reflects LLMs' tendency to repeatedly generate a small number of highly common items, a pattern consistently observed across domains.For theorem counting, as illustrated in Figure 5 (left panel), LLMs frequently generate well-known examples like the Pythagorean Theorem, Fermat's Last Theorem, and Gödel's Incompleteness Theorems.Despite all models being queried 30,000 times (up to 20 responses per query), the number of valid theorems varies, affecting normalization.Nonetheless, the relative frequencies of these top responses-often exceeding 5%-highlight their dominance and account for the heavy tail.Similarly, for human diseases, the figure on the right demonstrates LLMs repeatedly mention a small set of common human diseases (for example, Alzheimer's disease, Asthma, and Parkinson's disease).This motivates truncating the prevalence sequence in the SGT estimator.For instance, using k = 6 means that only {n s } 6 s=1 contribute to estimating N seen (t), excluding high-frequency items.This truncation helps improve stability by reducing sensitivity to overrepresented responses.</p>
<p>Application 2: Information Retrieval</p>
<p>Setup.Our second application evaluates the information retrieval (IR) capabilities of LLMs using the BioASQ-QA Task 12b Test dataset [33], following the evaluation framework of [64], focusing on two subtasks.The first subtask is document retrieval, where the LLM is prompted to generate Boolean search queries to retrieve relevant documents from the PubMed database.These queries consist of multiple Medical Subject Headings (MeSH) keywords, combined using logical operators (AND, OR, NOT), and are submitted to a search engine to return candidate documents.The second subtask is question answering, in which the LLM answers biomedical research questions-curated by domain experts-based on the retrieved documents.In both subtasks, each question is associated with a set of ground-truth documents, and each document is annotated with a list of MeSH keywords.Instead of evaluating at the document level, we focus on MeSH keywords as retrieval-relevant knowledge units.Each keyword reflects a specific biomedical concept that the model successfully identifies through retrieval.This offers a finer-grained view of what content the model can retrieve, beyond simply matching entire documents.Indeed, traditional IR metrics-such as F1 score and ROUGE score [35]-assess retrieval and answer quality based on document relevance.In contrast, our pipeline estimates how many additional relevant MeSH keywords an LLM could potentially retrieve-beyond those observed-if more questions were posed from a similar distribution and evaluated under the same validation criteria.</p>
<p>In our experiments, each LLM is evaluated on N query = 340 biomedical questions using custom few-shot prompts (see Appendix B.2 for templates), with N ans = 1 answer collected per question.The verification procedures differ across subtasks.In the document retrieval subtask, if the LLM retrieves a document that appears in the ground-truth set, all MeSH keywords associated with that document are counted as valid observed knowledge.In the question answering subtask, if the LLM's response is deemed correct, we include all MeSH keywords from the documents linked to that question.All MeSH keywords are normalized and clustered by the top-level concept of the MeSH hierarchy for consistent aggregation.1 for the full list).The highest values are in red.</p>
<p>Results.Results for Application 2 are summarized in the left two columns of Table 2, with selected traditional IR metrics shown in Figure 6 and full results provided in Appendix B.10.In Subtask 1, DeepSeek-V3 achieves the highest observed count (N seen = 2260), consistent with its top document-level F1 score of 16.76%.However, when incorporating estimated unseen knowledge, ChatGPT-3.5-turbo-chatsurpasses all others with a total of N tot = 10367, significantly ahead of DeepSeek-V3's 7750-demonstrating how our pipeline can reveal latent retrieval capacity not reflected in standard metrics.In Subtask 2, ChatGPT-4o-chat leads in both observed and total MeSH terms, although its ROUGE score is slightly lower than the highest performer.Overall, models exhibit SKR values around 25% for Subtask 1 and 11% for Subtask 2, both of which are lower than in Application 1.</p>
<p>Application 3: Diversity Measure</p>
<p>Setup.Our final application evaluates the diversity of different LLMs in response to open-ended prompts, such as "Name a possible LLM application and explain why."Unlike the previous two applications, correctness verification is unnecessary, as any fluent and meaningful response is considered valid, which is typically satisfied by sufficiently large models.To quantify diversity, we design two tasks.The first asks the LLM to describe a real or imagined application of LLMs.The second asks it to invent a job it might find appealing, if it could dream like a human.In both tasks, we instruct the models to generate short, simple responses with brief explanations.Each LLM is independently queried N query = 1000 times, collecting N ans = 1 response per query.To cluster similar responses, we embed each response using OpenAI's text-embedding-ada-002 model, which produces a 1536-dimensional semantic vector.Two responses are considered equivalent if the distance between their embeddings is smaller than a specified threshold.This clustering step is necessary, as LLMs may produce nearly identical applications or jobs but provide different reasons.</p>
<p>To determine the threshold, we first compute the 10-nearest neighbor distances for each response within each LLM, resulting in one set of distances per model.We then aggregate these distances across all models and set the threshold to the q-quantile of the combined set.</p>
<p>Results.Results for diversity estimation at q = 0.5 are shown in the right two columns of Table 2. Claude-3.7-Sonnetproduces the largest number of observed LLM applications, while Mistral-7B-instruct-v0.1 has the highest estimated total knowledge.For the "dream jobs" task, LLaMA-V3-3B-instruct achieves the highest coverage in both observed count N seen and total estimate N tot .In contrast, DeepSeek-V3 and Gemini-1.5-flashproduce the fewest outputs across both tasks, likely reflecting more deterministic generation behavior.Most models exhibit an SKR near 23%, indicating that only a small fraction of their internal knowledge is expressed through sampling.While DeepSeek-V3 and Gemini-1.5-flashshow higher SKRs (0.7 and 0.3, respectively), this likely results from limited generation diversity, leading to fewer unseen items.This 23% SKR is robust for different clustering thresholds q ∈ {0.2, 0.3, 0.5, 0.7}, with the top six models remaining within the 20-25% range.This suggests a common tendency across LLMs: only a modest portion of their generative potential is typically realized.</p>
<p>Validation and Sensitivity Analysis</p>
<p>Cross-Validation of Unseen Knowledge Estimates</p>
<p>To evaluate the accuracy of our pipeline, we perform held-out validation by splitting the N query responses into two parts: the first r obs fraction is treated as observed data, and we estimate the number of unique theorems that appear only in the remaining (1 − r obs ) fraction.We repeat this procedure 100 times, each with a random shuffle of the full dataset so that the observed subset differs across runs.We report the average estimated and ground truth unseen counts in Figure 7, with error bars indicating the standard deviation over 100 repetitions.We consider three values for r obs ∈ {1/2, 1/3, 1/4}, corresponding to a largest extrapolation factor of t = 1, 2, 3, respectively.Across all tested LLMs, the estimated unseen counts closely match the ground truth, implying that the SGT estimator yields highly accurate predictions for a wide range of extrapolation factors.</p>
<p>Sensitivity Analysis</p>
<p>Effect of the extrapolation factor t. Next, we examine the effect of the extrapolation factor t. By definition, as t increases, the estimated number of unseen items N seen (t) should grow so that the SKR decreases.This trend is confirmed in the left two plots of Figure 8, where we observe a monotonic increase in N seen (t) and a corresponding decrease in SKR.Additionally, the variance of the estimate becomes larger as t increases, reflecting greater uncertainty in longer-range extrapolation.</p>
<p>In practice, we find that both quantities tend to saturate around t = 80, with little gain beyond that point.This motivates our choice of t = 100 as a default in all experiments.Effect of the prompt.An important practical issue is how to design the prompt.We examine two key factors: the clarity of the instruction (Inst ↑ or Inst ↓) and the number of requested responses per query (N ans ∈ {10, 20}).We test four configurations that vary along these two dimensions.The results of theorem counting are shown in the right two plots of Figure 8. Overall, prompts with clearer instructions and a higher number of requested responses lead to more diverse and informative outputs, increasing both N seen and N tot in our experiments, and thereby improving access to the model's internal knowledge.The set of used prompts is listed in Appendix B.2.</p>
<p>Effect of the truncation level k.The first plot in Figure 9 shows how the estimated total knowledge N tot varies with the truncation level k, while the second plot presents the normalized mean square error (MSE).Overall, the estimator is relatively insensitive to the choice of k, suggesting that the prevalence histogram n s is generally stable and informative, which enables accurate extrapolation over a broad range of truncation levels.Nonetheless, we recommend using the held-out validation procedure described in Section 5.1 to select k adaptively for each setting.As illustrated in the second plot, most models perform best when k = 8.The complete set of selected k values for each LLM and application is provided in Appendix B.5.Effect of temperatures and sampling methods.LLMs generate responses through next-token prediction, meaning that sampling strategy directly influences their expressed capabilities.We consider two key factors: the temperature Temp ∈ {0.7, 1} and the use of nucleus sampling (denoted as Nuc✓ if applied and Nuc× if not), which truncates the token distribution to the smallest set of tokens whose cumulative probability exceeds 0.9 [27].The right two plots of Figure 9 present results under these four configurations.We observe that using a higher temperature and disabling truncation yields the highest values for both N seen and N tot .Therefore, we adopt a temperature of 1 and disable nucleus sampling in all experiments to best capture each model's capabilities.</p>
<p>Discussion</p>
<p>We have presented KnowSum, a five-step modular pipeline for estimating the unseen knowledge encoded in LLMs.By focusing on what models could output rather than what they do output, our method provides a complementary perspective to standard evaluation.We show its effectiveness across three applications: knowledge estimation, information retrieval, and diversity measurement.</p>
<p>Our framework opens several avenues for extension.While this work focuses on estimating the number of unseen knowledge items, similar estimators can be adapted to quantify knowledge that appears at least s times [25].Currently, our framework focuses on well-defined, countable knowledge, such as named theorems or diseases, whose correctness can be easily verified.However, real-world knowledge is often more complex and less structured.For example, knowing the name of a theorem is not the same as understanding it, where "understanding" itself is difficult to define.As another example, if we shift our focus to proof techniques, it is generally hard for machines to automatically identify which techniques are used-especially since complex theorem proofs often involve multiple techniques and intricate relationships among them.In principle, if appropriate verification and clustering methods can be developed, our pipeline could still apply in these settings.However, doing so is generally difficult, and we leave it as an open problem.Finally, a more ambitious direction is to move beyond estimation and toward extraction-actively surfacing knowledge that the model encodes but rarely generates.Transitioning from passive statistical inference to active knowledge discovery introduces both theoretical and practical challenges, and we view this as an important and impactful direction for future research.</p>
<p>A Proofs of Theorem 1</p>
<p>Proof of Theorem 1. Recall that n is the total number of valid items.So we have that n = s≥1 sn s .By the expression (1) of the SGT estimator, we have N unseen (t) = k s=1 h s n s .To prove the result, it suffices to show that N unseen (t)
N obs ≤ e kt t+1 .(2)
We now proceed to prove (2).It follows that
N unseen (t) N obs = k s=1 h s n s s≥1 n s ≤ max 1≤s≤k |h s | ≤ max 1≤s≤k t s • P Bin k, 1 t + 1 ≥ s (a)
≤ max P Bin k,
1 t + 1 ≥ s ≤ ek (t + 1)s s . Proof of Lemma A.1. Let X ∼ Bin(k, 1t+1
), and define the mean µ = E[X] = k t+1 .We aim to upper bound the tail probability P(X ≥ s) for s &gt; µ.To that end, we apply the multiplicative Chernoff bound:
P(X ≥ s) ≤ e δ (1 + δ) 1+δ µ , where δ = s−µ µ = s µ − 1.
Substituting this into the bound, we obtain:
P(X ≥ s) ≤    e s µ −1 s µ s µ    µ .
Now we simplify the right-hand side.Taking logarithms:
log P(X ≥ s) ≤ µ s µ − 1 − s µ log s µ = −s log s µ + s − µ.
Table 3: Prompts used across different applications and tasks.</p>
<p>Application Task Prompt</p>
<p>Application 1</p>
<p>Theorems counting</p>
<p>Test your knowledge of mathematical theorems by listing 20 theorem names, separated by commas (e.g., Theorem 1,Theorem 2,Theorem 3,etc.).If a theorem is known by multiple names, choose the most classic one.Aim for variety and try to include rare or less commonly known theorems.</p>
<p>Keep your response concise and to the point.</p>
<p>Diseases counting</p>
<p>You are a biomedical expert helping to compile a list of human diseases.Please provide 50 human diseases along with their corresponding DOID identifiers.Format your response as: -Disease Name (DOID:xxxxx) Make sure the Disease Name matches with DOID, and the DOIDs are valid from Disease Ontology.</p>
<p>Application 2</p>
<p>BioASQ retrieval [64] Given a question, expand into a search query for PubMed by incorporating synonyms and additional terms that would yield relevant search results from PubMed to the provided question while not being too restrictive.Assume that phrases are not stemmed; therefore, generate useful variations.Return only the query that can directly be used without any explanation text.Question: What is the mode of action of Molnupiravir?Query: Molnupiravir AND ("mode of action" OR mechanism).Question: Is dapagliflozin effective for COVID-19?Query: dapagliflozin AND (COVID-19 OR SARS-CoV-2 OR coronavirus) AND (efficacy OR effective OR treatment).Question: Name monoclonal antibody against SLAMF7.Query: "SLAMF7" AND ("monoclonal antibody" OR "monoclonal antibodies").Question: {context} Query: {body}</p>
<p>Application 3 LLMs applications</p>
<p>Briefly describe one real or imagined application of large language models (LLMs).Keep the response short and simple, and explain why it's useful.</p>
<p>Dream jobs</p>
<p>Imagine that a large language model (LLM) could dream like a human.Describe, in one or two sentences, what kind of job it might wish to have.Be creative, and explain briefly why this job would appeal to an LLM.</p>
<p>B.3 Details of Verification and Clustering</p>
<p>B.3.1 Application 1: Knowledge Estimation</p>
<p>Mathematical theorems.In the verification step, we sequentially cross-reference each generated item against three external sources: Wikipedia, MathSciNet, and ProofWiki.The process stops as soon as a match is found.Two theorems are considered identical if they are verified by the same webpage or share the same URL.</p>
<p>In the clustering step, since a single theorem may have multiple equivalent names, we assign a canonical identifier to each theorem by extracting a standardized name from the corresponding URL.For Wikipedia and ProofWiki, we parse the page title from the URL path, convert it to lowercase, and replace underscores with spaces.For MathSciNet, we extract the search query parameter.All extracted names are further passed through a normalization routine to reduce inconsistencies due to formatting or spelling variations.This process ensures that each valid theorem instance is mapped to a unique and consistent name across sources.</p>
<p>Human Disease Oncology.In the verification step, each LLM-generated disease name is evaluated by matching it against official disease terms and their synonyms.These synonyms are drawn from the "term.synonym"field in the OBO file provided by the Disease Ontology.Matching is performed using fuzzy string comparison via the "rapidfuzz" Python package, with a similarity threshold of 0.9.The verification process terminates once a valid match is found.</p>
<p>In the clustering step, all matched terms are grouped according to their corresponding Disease Ontology Identifier (DOID).For example, if an LLM generates a disease name along with two of its synonyms, all three generated names will be mapped to the same DOID, and the count for that DOID will be incremented by three.</p>
<p>B.3.2 Application 2: Information Retrieval</p>
<p>We evaluate the information retrieval capabilities of LLMs using the official Task12BGolden test set from the BioASQ Challenge [33].Each question in this dataset is annotated with a set of relevant biomedical documents, each uniquely identified by a PubMed ID.For each PubMed ID (or equivalently each biomedical document), we retrieve the associated Medical Subject Headings (MeSH) terms using the NCBI E-utilities API (https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi).Throughout this application, normalized MeSH terms serve as the countable unit of biomedical knowledge-analogous to the role of theorems in Application 1.In the clustering step, keywords were normalized to a unique form by splitting the original name on the slash delimiter ("/") and retaining the last segment in lowercase.</p>
<p>Subtask 1: Document Retrieval.This subtask assesses an LLM's ability to formulate effective Boolean queries for retrieving relevant scientific articles from the PubMed database.To measure unseen knowledge, we estimate the number of additional, correct MeSH terms the model could uncover if queried with semantically similar inputs.During the generation phase, we prompt the LLM with a BioASQ question using a fixed template (see Table 3), and use the resulting query to search PubMed via the E-utilities API (db = "pubmed").In the verification phase, we check whether retrieved PubMed IDs match any gold-standard references.If a match occurs, we accumulate all MeSH terms associated with the matching documents.Because each article can have multiple MeSH terms, and each question can be linked to multiple documents, this setup provides a rich space of retrievable knowledge.Subtask 2: Question Answering.The second subtask evaluates LLMs' ability to answer questions based on retrieved biomedical content.In the generation phase, to ensure a fair comparison, all models receive the same gold-standard document snippets as input.This setting makes use of in-context learning.See Table 4 for the prompt template.For each BioASQ question, the model generates two types of responses: (1) an exact answer, which may be of type yes/no, list, or factoid, and is evaluated using standard metrics such as accuracy, precision, recall, and F1 scores; and ( 2) an ideal answer, which is a free-form summary evaluated using ROUGE.</p>
<p>In the verification phase, for yes/no and factoid questions, we compare the LLM-generated answer directly against the reference label.For list and summary questions, we apply threshold-based criteria to assess correctness.Unlike document-level evaluation in Subtask 1, this subtask operates at the question level.If the model's output is deemed correct, we credit all the MeSH terms associated with that question's gold documents as successfully recovered.We then apply our unseen knowledge estimation framework to infer the number of MeSH terms the model could generate if exposed to additional questions.</p>
<p>B.3.3 Application 3: Diversity Measure</p>
<p>We don't verify responses in this application.We describe how we cluster responses: each response is embedded using OpenAI's text-embedding-ada-002 model, which maps text to a 1536-dimensional semantic vector.Two responses are considered equivalent if the distance between their embeddings is below a chosen threshold.To set this threshold, we compute the 10-nearest neighbor distances for each response within each LLM, aggregate the distances across all models, and use the q-quantile of the combined set.We evaluate q ∈ {0.2, 0.3, 0.5, 0.7}.</p>
<p>B.4 Verification Criteria</p>
<p>In the verification step for Application 1, we apply two levels of verification criteria: one strict and one more relaxed.Since all valid knowledge items-whether theorems or human diseases-are sourced from external databases, we visualize the scope of these datasets to assess how permissive the relaxed verification criterion can be while still maintaining validity in Figure 10.</p>
<p>Mathematical theorems.For theorems, the strict criterion requires that the final name (extracted from the URL) contains the word "theorem", while the relaxed criterion accepts any name that includes one of 12 math concepts (i.e., "theorem," "lemma," "law," "principle," "formula," "criterion," "identity," "conjecture," "rule," "equation," "postulate," or "corollary.").The left plot in Figure 10 shows the distribution of different theorem types in the reference databases.Notably, law is the most common type, with 11,314 entries, while the theorem type includes only 2,558 entries.</p>
<p>Human diseases.For human diseases, the strict criterion includes only the "anatomical disease" category-i.e., diseases that affect specific body parts.The relaxed criterion allows any disease listed in the Disease Ontology database, regardless of category.The right plot in Figure 10 displays the distribution of disease types in the reference databases.Notably, anatomical disease is the most common category, with 6036 entries.</p>
<p>B.5 Used Truncation Levels</p>
<p>All the truncation levels used in different applications are collected in Table 5.</p>
<p>B.6 Cross-Validation of Unseen Human Disease Estimates</p>
<p>We present the human disease counting results from the held-out validation analysis, complementing the theorem results shown in Figure 7.As shown in Figure 11, the estimator achieves similarly strong performance.The estimated counts of unseen diseases closely match the ground truth, confirming that the SGT estimator yields highly accurate predictions across a wide range of extrapolation factors.</p>
<p>B.7 Additional Results on Extrapolation and Prompts</p>
<p>We present the counterpart of human diseases of ChatGPT-3.5-turbo-Chat.From the right two plots of Figure 12, we can see that clear instruction and high number of response can increase the diversity and elicit more LLM Unseen knowledge.</p>
<p>B.8 Additional Results on Truncation Levels and Sampling Strategies</p>
<p>We present additional results for human diseases in Figure 13, complementing Figure 9.The left two plots show that the estimator is relatively robust to different values of the truncation level k.The right two plots indicate that using a temperature of 1 and disabling nucleus sampling consistently yields higher values of both N seen and N tot across models.</p>
<p>B.9 Responses Analysis</p>
<p>Table 6 presents the most frequently generated mathematical theorems across nine LLMs, each prompted to list 20 theorem names over 30,000 runs.The table reports the top-8 theorems per model, along with their absolute counts and relative frequencies.Classical results such as the "Pythagorean Theorem," "Fermat's Last Theorem," and "Gödel's Incompleteness Theorems" appear consistently across nearly all models, suggesting that these iconic theorems are embedded as core knowledge.At the same time, notable differences emerge: Qwen2.5-7B-instructstrongly favors the "Fundamental Theorem of Calculus," while LLaMA-V3-3B-instruct uniquely highlights the "Sylvester-Gallai Theorem" and "Navier-Stokes Equations."These variations likely reflect differences in pretraining data coverage or model specialization.</p>
<p>Table 7 shows the top-8 most frequently generated human diseases from each LLM, prompted to list 50 disease names over 3000 runs.As in the theorem case, many models converge on a core set of common diseases, including "Alzheimer's Disease," "Parkinson's Disease," "Asthma," and "Multiple Sclerosis."These results reflect the centrality of these conditions in biomedical corpora.Nonetheless, distinct model behaviors are evident: for example, Gemini-1.5-flashemphasizes "Coronary Artery Disease" and "Type 2 Diabetes Mellitus," while LLaMA-V3-3B-instruct features rarer conditions like "Maturity-Onset Diabetes of the Young Type 1."Such differences likely stem from variations in training data granularity and domain emphasis.</p>
<p>Table 8 focuses on anatomical diseases and follows the same experimental setup.Common conditions like "Hepatitis," "Alzheimer's Disease," "Parkinson's Disease," and "Asthma" again dominate the responses, with "Hepatitis" especially prominent across models such as ChatGPT-4o-chat, DeepSeek-V3, and Claude-3.7-Sonnet.However, each model still exhibits unique tendencies: for instance, LLaMA-V3-70B-instruct frequently generates "Frontotemporal Dementia" and "Chronic Kidney Disease," while Mistral-7B-Instruct-v0.1 leans toward "Cerebrovascular Disease" and "Heart Disease."These findings further underscore how LLMs encode not only general but also model-specific biomedical knowledge, shaped by their underlying training regimes.ChatGPT-4o-chat (14.93%), and ChatGPT-3.5-turbo-chat(14.36%).Snippet-level metrics are uniformly low across all models, with LLaMA-V3-70B-instruct (2.94%) and DeepSeek-V3 (2.76%) performing best.These results suggest that while LLMs are somewhat effective at retrieving relevant documents, they struggle with extracting precise snippet-level answers.Notably, while DeepSeek-V3 ranks highest under traditional metrics, it also has the highest estimated number of unseen MeSH keywords in our pipeline-demonstrating strong potential for retrieving latent biomedical knowledge.However, if we account for both observed and estimated unseen knowledge, ChatGPT-3.5-turbo-chatemerges as the best performer, with the highest total knowledge count (N tot = 10367 in Table 2).This ranking shift illustrates how our estimation pipeline reveals model capabilities not captured by conventional evaluation-highlighting output diversity and latent retrieval capacity rather than solely surface-level accuracy.Subtask 2: Question Answering (QA).Table 10 reports standard evaluation metrics for biomedical question answering, including ROUGE, accuracy on yes/no and factoid questions, and F1 for list-type answers.DeepSeek-V3, Gemini-1.5-flash,and Claude-3.7-Sonnetall demonstrate strong performance across several metrics.For example, DeepSeek-V3 and Gemini-1.5-flashachieve high yes/no accuracy (92.16%) and list F1 scores above 18%, while Claude-3.7-Sonnetachieves the best factoid performance (10.59% lenient accuracy) and strong list-level precision.These results reflect the models' ability to generate correct and structured answers when evaluated against expert annotations.However, our pipeline uncovers a different aspect of model capability.While ChatGPT-4o-chat only ranks moderately under traditional metrics, it demonstrates the highest estimated total knowledge (N tot = 19965), followed by DeepSeek-V3 and Claude-3.7-Sonnet(Table 2).This suggests that ChatGPT-4o-chat may encode substantially more answer-relevant MeSH knowledge than is directly observed-revealing latent retrieval potential beyond what conventional metrics capture.Once again, our estimator provides a more complete picture of what LLMs could retrieve, even when their immediate outputs fall short under surface-level evaluation.</p>
<p>Figure 1 :
1
Figure 1: Estimating the number of unseen theorems changes LLM rankings based on the observed theorems alone.The evaluated models are:LLaMA-V3-70B-instruct[24], ChatGPT-3.5-turbo-chat[38],ChatGPT-4o-chat[28],Mistral-7B-instruct-V0.1[29], and Qwen2.5-7B-instruct[62].</p>
<p>Figure 2 :
2
Figure 2: (a) Examples of common and rare theorems from an LLM.(b) Obtain theorem names by repeatedly prompting the LLM.(c) Comparison between observed results and our proposed approach.</p>
<p>Figure 3 :Algorithm 1 4 :
314
Figure 3: Schematic illustration of the unseen knowledge estimation framework.Algorithm 1 Knowledge Summation (KnowSum) Input: Verifier A, knowledge decoder C, number of queries n, extrapolation factor t. 1: Generation: Independently query the target LLM n times to produce responses x 1 , . . ., x n .2: Verification: Use the verifier A to filter out invalid or hallucinated responses. 3: Clustering: Apply the decoder C to extract the core knowledge c i = C(x i ) from each valid response, and group {c i } n i=1 into m semantically distinct clusters {k j } m j=1 . 4: Prevalence Estimation: Construct the frequency histogram {n s } s≥1 , where n s denotes the number of clusters that appear exactly s times.Estimate the observed knowledge by N seen = s≥1 n s .</p>
<p>Figure 4 :
4
Figure 4: Frequency histogram of theorems (top) and human diseases (bottom) found in LLM responses.The x-axis shows how many times a knowledge appears, and the y-axis shows how many distinct knowledges occur with that frequency.The shaded region shows additional counts due to relaxed validation criteria.</p>
<p>Figure 5 :
5
Figure 5: Top three most frequently generated theorems (left) and human diseases (right).The black dotted line indicates the average relative frequency, defined by 1/N ans , where N ans is the number of knowledge items requested per query (20 for theorems and 50 for diseases).</p>
<p>Figure 6 :
6
Figure6: Performance on selected traditional IR metrics.M 1 denotes Model ① (ChatGPT-4o-chat), and similarly for the others (see Table1for the full list).The highest values are in red.</p>
<p>Figure 7 :
7
Figure 7: SGT estimates (colored curves) versus ground truth (black dotted lines) for theorem estimation.From left to right, the observed fraction r obs is 1/2, 1/3, and 1/4.Counterpart results for human diseases are shown in Figure 11 in the Appendix.</p>
<p>Figure 8 :
8
Figure 8: Effects of the extrapolation factor t (left two plots) and prompts (right two plots) for theorem estimation.Counterpart results for human diseases are shown in Figure 12 in the Appendix.</p>
<p>Figure 9 :
9
Figure 9: Effects of the truncation level k (left two plots) and sampling strategies (right two plots) for theorem estimation.Counterpart results for human diseases are shown in Figure 13 in the Appendix.</p>
<p>(a) uses Lemma A.1 and (b) uses the fact that ek s s achieves its maximum when s = k so that ek s s ≤ e k .Lemma A.1 (Tail bound).</p>
<p>Figure 10 :
10
Figure 10: Distribution of math concepts (left) and human disease subcategories (right).These distributions are used as filtering criteria to exclude invalid knowledge items under the strict criteria setting in Application 1.</p>
<p>Figure 8 .Figure 11 :
811
Figure 11: SGT estimates (colored curves) versus ground truth (black dotted lines) for disease estimation.From left to right, the observed fraction r obs is 1/2, 1/3, and 1/4.</p>
<p>Figure 12 :
12
Figure 12: Effects of the extrapolation factor t (left two plots) and prompts (right two plots) for disease estimation.</p>
<p>Figure 13 :
13
Figure 13: Effects of the truncation level k (left two plots) and sampling strategies (right two plots) for disease estimation.</p>
<p>Table 2 :
2
Results of information retrieval (Application 2, left two columns) and diversity measures (Application 3, right two columns) across different LLMs.
ModelDocument Retrieval Question Answering LLM Applications N seen N tot SKR N seen N tot SKR N seen N tot SKR N seen N tot SKR Dream Jobs① ChatGPT-4o-chat201596760.212351 199650.121657140.234091680 0.24② ChatGPT-3.5-turbo-chat2190 103670.211850 157330.1232213390.241315600.23③ LLaMA-V3-70B-instruct199084880.231928 142700.1443719180.233441487 0.23④ LLaMA-V3-3B-instruct793960.201653 141990.1242819260.22770 3386 0.23⑤ Mistral-7B-instruct-v0.1 136456460.2463065960.10658 3155 0.212331093 0.21⑥ Qwen2.5-7B-instruct139948530.281585 107100.1542118400.235072094 0.24⑦ Claude-3.7-Sonnet205088310.232023 172300.1269630130.231335430.24⑧ DeepSeek-V3226077500.302290 197440.1217480.357100.7⑨ Gemini-1.5-flash202766160.312222 148980.1521370.573100.3</p>
<p>Table 4 :
4
Context prompt templates used for each question type in Application 2, Subtask 2 (Question Answering) of the BioASQ challenge.
Question Type Prompt TemplatesHere are some example yesno questions with answers.###Context: Papilins are homologous, secreted extracellularmatrix proteins which share a common order of proteindomains.Question: Is the protein Papilin secreted?Ideal answer: Yes, papilin is a secreted proteinExact answer: yes###Context: Most lncRNAs are under lower sequenceconstraints than protein-coding genes and lack conservedYes/Nosecondary structures, making it hard to predict them computationally.Question: Are long non coding RNAs as conserved in sequenceas protein coding genes?Ideal answer: No. Most long non coding RNAs are underlower sequence constraints than protein-coding genes.Exact answer: no###Now answer the following yesno question:###Context: {context}Question: {body}Your Answer should be the following format:Ideal answer:<Your Ideal Answer>. Exact answer:<Your ExactAnswer>.Here are some example summary questions with answers.###Context: Hirschsprung disease (HSCR) is amultifactorial, non-mendelian disorder in which rarehigh-penetrance coding sequence mutations in the receptortyrosine kinase RET contribute to risk in combination withmutations at other genes.Question: Is Hirschsprung disease a mendelian or aSummarymultifactorial disorder?Answer: Coding sequence mutations in RET, GDNF, EDNRB,EDN3, and SOX10 are involved in the development ofHirschsprung disease. The majority of these genes was shownto be related to Mendelian syndromic forms...###Now answer the following summary question:Context: {context}Question: {body}Answer:Here are some example list questions with answers.###Context: The FGFR3 P250R mutation was the single largestcontributor (24%) to the genetic group.Question: Which human genes are more commonly related tocraniosynostosis?Ideal answer: The genes that are most commonly linked toListcraniosynostoses are...Exact answer: FGFR3;FGFR2;FGFR1;...###Now answer the following list question:Context: {context}Question: {body}Ideal answer:<Your Ideal Answer>. Exact answer:<Your ExactAnswer>.Here are some example factoid questions with answers.###Context: Ewing sarcoma is the second most common bonemalignancy in children and young adults.Question: Which fusion protein is involved in thedevelopment of Ewing sarcoma?Ideal answer: Ewing sarcoma is driven by oncogenic fusionFactoidprotein EWS/FLI1...Exact answer: EWS;FLI1###Now answer the following factoid question:Context: {context}Question: {body}Ideal answer:<Your Ideal Answer>. Exact answer:<Your ExactAnswer>.</p>
<p>Table 5 :
5
Truncation levels used in different applications.
ModelApplication 1 TheoremsApplication 1 Human diseasesApplication 2 RetrievalApplication 2 Question AnswerApplication 3 Diversity① ChatGPT-4o-chat88886② ChatGPT-3.5-turbo-chat88886③ LLaMA-V3-70B-instruct88886④ LLaMA-V3-3B-instruct68886⑤ Mistral-7B-instruct-v0.161010106⑥ Qwen2.5-7B-instruct88666⑦ Claude-3.7-Sonnet88886⑧ DeepSeek-V388886⑨ Gemini-1.5-flash86666</p>
<p>Table 6 :
6
Top-8 theorem outputs per model with their counts and fractions.</p>
<p>Table 7 :
7
Human diseases outputs per model with their counts and fractions.</p>
<p>Table 9 :
9
Performance of traditional IR metrics on the BioASQ dataset for the document retrieval task.Document Retrieval.Table9presents classic IR metrics (precision, recall, F1) at the document and snippet levels.According to traditional IR metrics, DeepSeek-V3 achieves the highest document-level F1 score (16.76%), followed closely by Claude-3.7-Sonnet(15.91%),
ModelDoc Precision Doc Recall Doc F1 Snip Precision Snip Recall Snip F1① ChatGPT-4o-chat16.42%22.22%14.93%4.46%2.77%2.57%② ChatGPT-3.5-turbo-chat17.20%20.32%14.36%4.68%2.81%2.72%③ LLaMa-V3-70b-instruct15.93%20.88%14.08%4.67%3.29%2.94%④ LLaMa-V3-3b-instruct0.48%1.09%0.56%0.06%0.08%0.06%⑤ Mistral-7B-instruct-V0.19.68%12.69%8.22%2.70%1.54%1.33%⑥ Qwen2.5-7B-instruct11.68%13.55%9.21%3.23%1.29%1.46%⑦ Claude-3.7-Sonnet15.82%23.99%15.91%4.15%2.68%2.73%⑧ DeepSeek-V319.40%23.03%16.76%4.91%2.76%2.76%⑨ Gemini-1.5-flash16.10%21.65%14.36%4.40%2.73%2.62%Subtask 1:</p>
<p>Table 10 :
10
Performance of traditional IR metrics on the BioASQ dataset for the QA task.ROUGE Yes/No Acc.Factoid Strict Acc.Factoid Lenient Acc.List F1 List Prec.List Rec.
Model① ChatGPT-4o-chat17.41%83.33%3.53%4.71%31.42%36.61%30.53%② ChatGPT-3.5-turbo-chat20.15%88.24%1.18%1.18%7.02%8.32%7.02%③ LLaMa-V3-70b-instruct19.13%91.18%3.53%3.53%1.70%1.92%1.65%④ LLaMa-V3-3b-instruct8.21%88.24%3.53%3.53%4.04%4.92%3.99%⑤ Mistral-7B-instruct-V0.114.63%27.45%0.00%0.00%4.04%4.83%3.72%⑥ Qwen2.5-7B-instruct11.95%87.25%1.18%1.18%0.57%0.52%0.65%⑦ Claude-3.7-Sonnet16.86%85.29%8.24%10.59%16.67%17.99%16.46%⑧ DeepSeek-V319.42%92.16%1.18%1.18%18.58%22.52%18.34%⑨ Gemini-1.5-flash17.03%92.16%0.00%0.00%18.40%21.89%18.65%
Direct queries such as "how many theorems do you know?" is equally unreliable, as such responses are often inconsistent or exaggerated[31].
AcknowledgmentsThis work was supported in part by NIH grants, RF1AG063481 and U01CA274576, NSF DMS-2310679, a Meta Faculty Research Award, and Wharton AI for Business.The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.Exponentiating both sides gives:Finally, plugging in µ = k t+1 , we get:B Experiments DetailsB.1 General SetupIn our work, we evaluate nine widely used LLMs: ChatGPT-4o-chat[28], ChatGPT-3.5-turbo-chat[38], LLaMA-V3-70B-instruct and LLaMA-V3-3B-instruct[24], Mistral-7B-instruct-V0.1[29], Qwen2.5-7B-instruct[62], Claude-3.7-Sonnet[3], DeepSeek-V3[36], and Gemini-1.5-flash[56].To ensure fair comparisons, we query each model using carefully designed prompt templates (see Appendix B.2), set the temperature to 1, and collect their responses.We then apply our estimation pipeline (Algorithm 1) to estimate the amount of unseen knowledge.Details on the verification and clustering procedures are provided in Appendix B.3, along with additional experimental results in the subsequent subsections.B.2 Used PromptsThe prompts used in Section 4 are listed in Table3, where the underlined words indicate the parts modified for sensitivity analysis.For Application 2 Subtask 2, the prompt templates are shown in Table4.In general, we use different templates for different types of questions to provide appropriate examples that facilitate effective in-context learning.To test prompt sensitivity, we remove instructional content from the original prompt.In the theorem counting task, the simplified version becomes: "Test your knowledge of mathematical theorems by listing 20 theorem names, separated by commas."In the human disease counting task, the simplified version becomes: "You are a biomedical expert helping to compile a list of human diseases.Please provide 50 human diseases along with their corresponding DOID identifiers."
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. GPT-4 technical report. 2023arXiv preprint</p>
<p>Physics of language models: Part 3.1, knowledge storage and extraction. Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2309.143162023arXiv preprint</p>
<p>Claude 3.7 Sonnet system card. Anthropic, February 2025</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Eight things to know about large language models. Samuel R Bowman, Critical AI. 222024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, International Conference on Learning Representations. 2023</p>
<p>How do large language models acquire factual knowledge during pretraining?. Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo, Neural Information Processing Systems. 2024</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM transactions on intelligent systems and technology. 1532024</p>
<p>Estimating the number of species in a stochastic abundance model. Anne Chao, John Bunge, Biometrics. 5832002</p>
<p>A new statistical approach for assessing similarity of species composition with incidence and abundance data. Anne Chao, Robin L Chazdon, Robert K Colwell, Tsung-Jen Shen, Ecology letters. 822005</p>
<p>Chatbot arena: An open platform for evaluating LLMs by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, Ion Stoica, International Conference on Machine Learning. PMLR2024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Forget what you know about LLMs evaluations-LLMs are like a chameleon. Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen, arXiv:2502.074452025arXiv preprint</p>
<p>Mean Reciprocal Rank. Nick Craswell, 10.1007/978-0-387-39940-9_4882009Springer USBoston, MA</p>
<p>CURIE: Evaluating LLMs on multitask scientific long-context understanding and reasoning. Zahra Hao Cui, Gowoon Shamsi, Xuejian Cheon, Shutong Ma, Maria Li, Peter Christian Tikhanovskaya, Nayantara Norgaard, Mudur, Beata Martyna, Paul Plomecka, Raccuglia, International Conference on Learning Representations. 2025</p>
<p>Data science at the singularity. David Donoho, Harvard Data Science Review. 612024</p>
<p>Estimating the number of unseen species: How many words did Shakespeare know?. Bradley Efron, Ronald Thisted, Biometrika. 6331976</p>
<p>Near-optimal estimation of the unseen under regularly varying tail populations. Stefano Favaro, Zacharie Naulet, Bernoulli. 2942023</p>
<p>Good-turing frequency estimation without tears. A William, Geoffrey Gale, Sampson, Journal of quantitative linguistics. 231995</p>
<p>The population frequencies of species and the estimation of population parameters. J Irving, Good, Biometrika. 403-41953</p>
<p>Estimating knowledge in large language models without generating a single token. Daniela Gottesman, Mor Geva, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>The Llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Optimal prediction of the number of unseen species with multiplicity. Yi Hao, Ping Li, Advances in Neural Information Processing Systems. 202033</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, International Conference on Learning Representations. 2020</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276GPT-4o system card. 2024arXiv preprint</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B2024arXiv preprint</p>
<p>Calibrated language models must hallucinate. Adam Tauman, Kalai Santosh, Vempala, Proceedings of the 56th Annual ACM Symposium on Theory of Computing. the 56th Annual ACM Symposium on Theory of Computing2024</p>
<p>Sahil Kale, Vijaykant Nadadur, arXiv:2503.11256Line of duty: Evaluating LLM self-knowledge via consistency in feasibility boundaries. 2025arXiv preprint</p>
<p>Accessed. Leonid Khomenko, Too many AIs. 2025</p>
<p>BioASQ-QA: A manually curated corpus for biomedical question answering. Anastasia Krithara, Anastasios Nentidis, Scientific Data. 1011702023Konstantinos Bougiatiotis, and Georgios Paliouras</p>
<p>How pre-trained language models capture factual knowledge? A causal-inspired analysis. Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Cheng-Jie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-V3 technical report. 2024arXiv preprint</p>
<p>Scalable extraction of training data from aligned, production language models. Milad Nasr, Javier Rando, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Florian Tramèr, Katherine Lee, International Conference on Learning Representations. 2025</p>
<p>ChatGPT-3.5-turbo. 2023OpenAI</p>
<p>Competitive distribution estimation: Why is Good-Turing good. Alon Orlitsky, Ananda Theertha, Suresh , Advances in Neural Information Processing Systems. 201528</p>
<p>Always Good Turing: Asymptotically optimal probability estimation. Alon Orlitsky, Junan Narayana P Santhanam, Zhang, Science. 30256442003</p>
<p>Optimal prediction of the number of unseen species. Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu, Proceedings of the National Academy of Sciences. 113472016</p>
<p>BLEU: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing2019</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo, Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, arXiv:2501.142492025arXiv preprint</p>
<p>Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani, Sarath Chandar, arXiv:2502.19573Do large language models know how much they know?. 2025arXiv preprint</p>
<p>AI and the everything in the whole wide world benchmark. Deborah Inioluwa, Emily Raji, Emily M Denton, Alex Bender, Amandalynne Hanna, Paullada, Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Oier Lopez de Lacalle, and Eneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Mark Saroufim, Yotam Perlitz, Leshem Choshen, Luca Antiga, Greg Bowyer, Christian Puhrsch, Driss Guessous, Supriya Rao, Geeta Chauhan, Ashvini Kumar, Rajpoot Ankur Jindal Pawan Kumar, Joe Parikh, Weiwei Isaacson, Yang, arXiv:2503.13507LLM efficiency fine-tuning competition. 2023. 2025arXiv preprint</p>
<p>Human disease ontology 2022 update. Evangelos Lynn M Schriml, Jamie Mitraka, Bethany Munro, Marina Tauber, Lynne Schor, Vivian Nickle, Lisa Felix, Catherine Jeng, Robert Bearer, Lichenstein, 10.1093/nar/gkab920Nucleic Acids Research. 50D12022</p>
<p>Auto-Prompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020</p>
<p>Welcome to the era of experience. David Silver, Richard S Sutton, Google AI. 2025</p>
<p>Shivalika Singh, Yiyang Nan, Alex Wang, D' Daniel, Sayash Souza, Ahmet Kapoor, Sanmi Üstün, Yuntian Koyejo, Shayne Deng, Noah A Longpre, Beyza Smith, Marzieh Ermis, Sara Fadaee, Hooker, arXiv:2504.20879The leaderboard illusion. 2025arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Richard Sutton, The bitter lesson. Incomplete Ideas (blog). 20191338</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Certain Language Skills in Children: Their Development and Interrelationships. Mildred C Templin, 10.5749/j.ctttvInstitute of Child Welfare Monograph Series. 261957University of Minnesota Pressnew edition edition</p>
<p>Language models are open knowledge graphs. Chenguang Wang, Xiao Liu, Dawn Song, arXiv:2010.119672020arXiv preprint</p>
<p>Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi, arXiv:2401.01623Can AI be as creative as humans?. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Estimating the probabilities of rare outputs in language models. Gabriel Wu, Jacob Hilton, International Conference on Learning Representations. 2025</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue, Advances in Neural Information Processing Systems. 202437</p>
<p>Using pretrained large language model with prompt engineering to answer biomedical questions. Wenxin Zhou, Thuy Hang Ngo, arXiv:2407.067792024arXiv preprint</p>
<p>Quantifying and analyzing entity-level memorization in large language models. Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Model Top-8 Outputs with Counts and Fractions ① ChatGPT-4o-chat Pythagorean Theorem (29848, 0.0557) Brouwer Fixed Point Theorem. 29813, 0.0557</p>
<p>Gödel Incompleteness Theorems (29807, 0.0557) Noether Theorem. 29583, 0.0552</p>
<p>Fermat Last Theorem (29337, 0.0548) Ramsey Theorem. 25767, 0.0481</p>
<p>Green Theorem, Cantor Theorem. 19582, 0.0366. 18837, 0.0352</p>
<p>② ChatGPT-3.5-turbo-chat Pythagorean Theorem (32631, 0.0599) Fermat Last Theorem. 29370, 0.0539</p>
<p>Bayes Theorem (25364, 0.0465) Fundamental Theorem of Calculus. 23909, 0.0439</p>
<p>Gödel Incompleteness Theorems. 20820, 0.0382. 20291, 0.0372Brouwer Fixed Point Theorem</p>
<p>Bolzano Weierstrass Theorem (18585, 0.0341) Jordan Curve Theorem. 17976, 0.0330</p>
<p>Fermat Last Theorem (18687, 0.0382) Pythagorean Theorem (18214, 0.0373) Brouwer Fixed Point Theorem. Llama, V3-70B-instruct Fundamental Theorem of Algebra. 19825, 0.0406. 13398, 0.0274</p>
<p>Gödel Incompleteness Theorems (13089, 0.0268) Gauss Bonnet Theorem. 12233, 0.0250</p>
<p>. Jordan Curve, Theorem , 11030, 0.0226Intermediate Value Theorem (9936, 0.0203</p>
<p>LLaMA-V3-3B-instruct Fermat Last Theorem. 30185</p>
<p>. Sylvester Gallai, Theorem , 16568, 0.0387</p>
<p>Poincaré Conjecture. 14174, 0.0331. 13602, 0.0317</p>
<p>Brouwer Fixed Point Theorem (11178, 0.0261) Euler Identity. 10231, 0.0239</p>
<p>Lagrange Theorem (9576, 0.0223) Navier Stokes Equations. 9028, 0.0211</p>
<p>Gauss Bonnet Theorem (279, 0.0161) Sylow Theorems. 236, 0.0136</p>
<p>5-7B-instruct Fundamental Theorem of Calculus (14258, 0.1500) Fundamental Theorem of Arithmetic (5276, 0.0555) Gödel Incompleteness Theorems (3159, 0.0332) Modularity Theorem. ⑥ Qwen2, 2227, 0.0234</p>
<p>Dirichlet Approximation Theorem (2142, 0.0225) Mean Value Theorem. 1754, 0.0185</p>
<p>Poincaré Bendixson Theorem (1668, 0.0175) Law of Cosines. 1641, 0.0173</p>
<p>Bayes Theorem (4860, 0.0528) Gauss Bonnet Theorem. 47790</p>
<p>4643, 0.0505) Sylow Theorems (4056, 0.0441Borsuk Ulam Theorem. </p>
<p>Deepseek, V3 Pythagorean Theorem (29999, 0.0526) Brouwer Fixed Point Theorem. 29982, 0.0526</p>
<p>Hahn Banach, Theorem , Gödel Incompleteness Theorems. 29835, 0.0523. 29402, 0.0516</p>
<p>Central Limit Theorem (29223, 0.0513) Fundamental Theorem of Algebra. 27156, 0.0476</p>
<p>Noether Theorem (27038, 0.0474) Heine Borel Theorem. 26967, 0.0473</p>
<p>Gemini, -1.5-flash Pythagorean Theorem (30000, 0.0651Prime Number Theorem. 29989, 0.0651</p>
<p>Central Limit Theorem (29936, 0.0649) Jordan Curve Theorem. 29825, 0.0647</p>
<p>Mean Value Theorem (29225, 0.0634) Law of Cosines. 28753, 0.0624</p>
<p>Bolzano Weierstrass Theorem (28101, 0.0610) Intermediate Value Theorem. 27045, 0.0587</p>
<p>10 Traditional Evaluation of Biomedical Information Retrieval We report the results of traditional evaluation metrics for Application 2 in this subsection. In our setup, each biomedical question is paired with a set of ground-truth documents, each annotated with expertcurated MeSH keywords. These annotations serve as a concrete proxy for the biomedical knowledge a model retrieves during document search or question answering. Standard IR metrics-such as precision, recall, F1 score, and mean reciprocal rank (MRR)-evaluate model performance based on how accurately the retrieved documents match these ground-truth references. In the following, we provide a detailed analysis of the results under these classic IR metrics. Model Top-8 Outputs with Counts and Fractions ① ChatGPT-4o-chat Alzheimer. B , S Disease (3794, 0.0262) Parkinson'S Disease (2995, 0.0207</p>
<p>Multiple Sclerosis. Asthma. 2966, 0.0205. 2941, 0.0203</p>
<p>. ' Crohn, Disease, Tuberculosis. 2879, 0.0199. 2843, 0.0196</p>
<p>② ChatGPT-3.5-turbo-chat Alzheimer'S Disease (2969, 0.0221) Parkinson'S Disease. 2888, 0.0215</p>
<p>Rheumatoid Arthritis. Asthma. 2826, 0.0211. 2793, 0.0208</p>
<p>. Breast Cancer. 2786, 0.0208. 2784, 0.0208Hypertension</p>
<p>Multiple Sclerosis. Leukemia. 2772, 0.0207. 2761, 0.0206</p>
<p>LLaMA-V3-70B-instruct Alzheimer'S Disease (2376, 0.0211) Breast Cancer. 2243, 0.0199</p>
<p>. Cystic Fibrosis, Asthma. 1971, 0.0175. 1935, 0.0172</p>
<p>Chronic Obstructive Pulmonary Disease (1891, 0.0168) Parkinson'S Disease. 1884, 0.0167</p>
<p>Multiple Sclerosis (1861, 0.0165) Amyotrophic Lateral Sclerosis. 1853, 0.0164</p>
<p>LLaMA-V3-3B-instruct Maturity-Onset Diabetes Of The Young Type 1 (3013, 0.0260) Epilepsy (2481, 0.0214) Hypertension (2397, 0.0207) Hypothyroidism. 2278, 0.0196</p>
<p>. ' Parkinson, Disease, Autism Spectrum Disorder. 2272, 0.0196. 2187, 0.0189</p>
<p>Multiple Sclerosis. ' Alzheimer, Disease, 2148, 0.0185. 1950, 0.0168</p>
<p>. ' Alzheimer, Disease, 1399, 0.0369</p>
<p>. Asthma. 1304, 0.0344</p>
<p>' Parkinson, Disease, Diabetes Mellitus. 1200, 0.0316. 1090, 0.0287</p>
<p>. 909, 0.0239) Hypertension (876, 0.0231Cancer. </p>
<p>5-7B-instruct Maturity-Onset Diabetes Of The Young Type 1 (3295, 0.0243). ⑥ Qwen2, Parkinson'S Disease. 2941, 0.0217Rheumatoid Arthritis (2919, 0.0215) Multiple Sclerosis (2901, 0.0214</p>
<p>Alzheimer'S Disease. Asthma. 2865, 0.0211. 2842, 0.0210</p>
<p>. Epilepsy, Hypertension. 2649, 0.0195. 2482, 0.0183</p>
<p>⑦ Claude-3.7-Sonnet Parkinson'S Disease (1088, 0.0202) Alzheimer'S Disease. 1087, 0.0202</p>
<p>Multiple Sclerosis (1086, 0.0202) Rheumatoid Arthritis. 1086, 0.0202</p>
<p>. 1086, 0.0202) Psoriasis (1086, 0.0202Hypertension. </p>
<p>. Schizophrenia, 1086, 0.0202) Osteoarthritis (1086, 0.0202</p>
<p>⑧ DeepSeek-V3 Asthma (2831, 0.0203) Alzheimer'S Disease. 2814, 0.0202</p>
<p>. Malaria, Tuberculosis. 2813, 0.0202. 2813, 0.0202</p>
<p>. 's Huntington, Disease, Parkinson'S Disease. 2813, 0.0202. 2813, 0.0202</p>
<p>. Gemini, -1.5-flash Rheumatoid Arthritis (3002, 0.0202</p>
<p>. 3001, 0.0202Asthma. </p>
<p>. ' Alzheimer, Disease, Parkinson'S Disease. 30000202</p>
<p>Coronary Artery Disease (2984, 0.0201) Type 2 Diabetes Mellitus. 2979, 0.0200</p>
<p>Top-8 Anatomical diseases outputs per model with their counts and fractions. Model Top-8 Outputs with Counts and Fractions ① ChatGPT-4o-chat Hepatitis. 84874, 0.0691) Alzheimer'S Disease (3794, 0.0538</p>
<p>. ' Parkinson, Disease, Asthma. 2995, 0.0425. 2966, 0.0421</p>
<p>Multiple Sclerosis (2941, 0.0417) Rheumatoid Arthritis. 2919, 0.0414</p>
<p>Crohn'S Disease. 2885, 0.0409. 2879, 0.0408</p>
<p>ChatGPT-3.5-turbo-chat Hepatitis (3140, 0.0443) Alzheimer'S Disease. 2969, 0.0419</p>
<p>. ' Parkinson, Disease, Asthma. 2888, 0.0407. 2826, 0.0399</p>
<p>LLaMA-V3-70B-instruct Hepatitis (2572, 0.0421) Alzheimer'S Disease. 2376, 0.0389</p>
<p>Frontotemporal Dementia. 1935, 0.0317. 1925, 0.0315</p>
<p>Chronic Obstructive Pulmonary Disease (1891, 0.0310) Parkinson'S Disease (1884, 0.0308) Multiple Sclerosis (1861, 0.0305) Chronic Kidney Disease. 1689, 0.0277</p>
<p>④ LLaMA-V3-3B-instruct Epilepsy. 2481, 0.0331. 2397, 0.0320</p>
<p>. Hypothyroidism, Parkinson'S Disease. 2278, 0.0304. 2272, 0.0303</p>
<p>Multiple Sclerosis. ' Alzheimer, Disease, 2148, 0.0287. 1950, 0.0260</p>
<p>Rheumatoid Arthritis. Osteoarthritis, 1866, 0.0249. 1855, 0.0248</p>
<p>. ' Alzheimer, Disease, 1399, 0.0608</p>
<p>. Asthma. 1304, 0.0567</p>
<p>. ' Parkinson, Disease, 1200, 0.0522) Hypertension (876, 0.0381</p>
<p>5-7B-instruct Parkinson'S Disease (2941, 0.0343) Rheumatoid Arthritis. ⑥ Qwen2, 2919, 0.0340</p>
<p>. ' Alzheimer, Disease, 2842, 0.0331) Epilepsy (2649, 0.0309</p>
<p>. 2482, 0.0289) Hepatitis (2382, 0.0278Hypertension. </p>
<p>. Parkinson'S Disease. 1088, 0.0334⑦ Claude-3.7-Sonnet Hepatitis (1671, 0.0512</p>
<p>. ' Alzheimer, Disease, 1087, 0.0333) Osteoarthritis (1086, 0.0333</p>
<p>Multiple Sclerosis. Epilepsy, 1086, 0.0333. 1086, 0.0333</p>
<p>. Psoriasis, Asthma. 1086, 0.0333. 1086, 0.0333</p>
<p>. -V3 Deepseek, Hepatitis, 5526, 0.0715</p>
<p>. Asthma. 2831, 0.0366</p>
<p>. ' Alzheimer, Disease, Parkinson'S Disease. 2814, 0.0364. 2813, 0.0364</p>
<p>Multiple Sclerosis (2812, 0.0364) Frontotemporal Dementia. 2807, 0.0363</p>
<p>. Gemini, -1.5-flash Hepatitis (3361, 0.0390) Rheumatoid Arthritis (3002, 0.0348</p>
<p>. Asthma. 3001, 0.0348) Alzheimer'S Disease (3000, 0.0348</p>
<p>. ' Parkinson, Disease, 3000, 0.0348) Multiple Sclerosis (2999, 0.0348</p>
<p>Coronary Artery Disease (2984, 0.0346) Crohn'S Disease. 2979, 0.0346</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-327 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-327</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-327</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a" target="_blank">Exploring Length Generalization in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.</p>
                <p><strong>Paper Abstract:</strong> The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e327.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e327.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity (bitstring / coin-flip)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parity task (bitstring parity / coin-flip natural language framing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary parity task: predict whether a sequence of bits (or an equivalent sequence of coin flips in natural language) has an even or odd number of ones/flips; used to probe sequential state-tracking vs. parallel shortcut strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (pretrained checkpoints used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>244M, 422M, 1B, 64B, 128B (various sizes evaluated; largest experiments used 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>parity (mod-2 counting; state tracking over a sequence); cast as coin-flip natural language parity</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>bitstring lengths evaluated from 3 up to 40; training lengths often 3–20 or 10–21 depending on split; alternate split: fixed token-length (30) with number of ones varied (training 10–20 ones, test 1–30 ones)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>vanilla finetuning; finetuning with scratchpad; few-shot prompting with scratchpad (chain-of-thought / in-context exemplars); padded scratchpad; masking experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: vanilla finetuning achieves near-perfect in-distribution accuracy but rapidly degrades on out-of-distribution (longer) lengths; in the varied-number-of-ones split, OOD performance was roughly equivalent to random. Few-shot scratchpad prompting on pretrained models (not finetuned) produced a large improvement, enabling correct extrapolation of scratchpad templates to much longer queries (they report high, nontrivial accuracy up to 20 flips). Exact numeric accuracies are shown in paper figures but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Transformers trained via finetuning tended to learn non-sequential, parallel 'counting/pooling' shortcuts (e.g., attend-and-threshold counting number of ones) rather than a left-to-right sequential state-tracking algorithm; few-shot scratchpad prompting lets pretrained models perform variable-length template matching and infer state transitions from exemplars. Failures are driven by attention patterns (distractors) and not primarily by untrained position biases or EOS prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Model scale (across evaluated LaMDA sizes) had little effect on finetuned models' length generalization; by contrast, few-shot scratchpad prompting abilities improve with model size (this approach 'scales with model size' per paper discussion and references).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Rapid degradation on out-of-distribution (longer) lengths; when trained on number-of-ones distribution, models default to threshold/count heuristics producing near-random OOD behavior; distractor tokens in input/scratchpad harm generalization; per-step scratchpad error rate can remain roughly constant causing aggregate failure on long instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared vanilla finetuning vs. scratchpad finetuning vs. few-shot scratchpad prompting; compared multiple model sizes; compared padded-scratchpad and masked-distractor interventions; evaluated a shuffled-operations baseline to probe reliance on sequential structure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Vanilla finetuning yields non-sequential shortcut solutions that fail to extrapolate parity to longer sequences, whereas few-shot scratchpad prompting on pretrained LLMs can induce template-like sequential solutions that extrapolate substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e327.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e327.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable Assignment (Boolean)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boolean Variable Assignment task (synthetic Python-style programs with boolean ops)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predict value of a queried boolean variable after executing a sequence of single-line boolean assignments; constructed to require iterative state updates and to expose dependency-chain (computational graph depth) difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (pretrained checkpoints used in experiments); also evaluated Codex (for comparison shown in figures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>244M, 422M, 1B, 64B, 128B (various sizes; main experiments used LaMDA family incl. 128B); an OpenAI Codex result is presented for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step Boolean operations (assign, and, or, xor, negate, conditional assigns) — algorithmic boolean execution rather than numeric arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>program lengths: chain-like split training lengths 3–8, evaluation up to 19; diverse split ranges: training min/max operations 8–32; number of variables varied (chain-like 2–3, diverse 4–10)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>vanilla finetuning; scratchpad finetuning (scratchpad = copy program + comments showing variable values); few-shot scratchpad prompting; shuffled-ops baseline; masked-distractor experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: finetuned models reach (near) perfect in-distribution accuracy but degrade quickly on longer OOD lengths; few-shot scratchpad prompting provided little extra benefit unless the base pretrained model already had non-trivial performance on the task. Shuffled-ops baseline matched poor OOD performance, indicating reliance on non-sequential cues.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Transformers learn strategies that favor resolving shallow dependency (small computational graph depth) via parallel attention rather than true sequential execution; 'computational graph depth' (longest dependency chain) is a better predictor of difficulty than number of operations. Distractor tokens in input cause attention to misroute leading to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scaling model size/data/compute during finetuning did not meaningfully improve OOD length generalization; few-shot scratchpad prompting effectiveness depends on pretrained base performance and can benefit from larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on programs with larger computational graph depth; models trained on shuffled operation orders can still exploit spurious correlations and achieve similar OOD failures; distractor input tokens and preceding scratchpad tokens impede learning sequential attention patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared chain-like vs diverse splits; compared finetuning vs finetuning on shuffled-ops baseline; compared few-shot prompting vs. no-finetuning; examined masking of distractors and padded-scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Transformers finetuned on variable-assignment learn non-sequential shortcuts that fail as dependency chains deepen, and computational graph depth—not raw program length—governs difficulty; few-shot scratchpad helps only when pretrained models already possess relevant capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e327.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e327.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad Finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad finetuning (finetuning models to generate intermediate solution steps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning transformers to generate intermediate step-by-step computations (scratchpad / chain-of-thought) before producing final answers, tested to assess whether stepwise outputs improve length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>244M, 422M, 1B, 64B, 128B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>applied to parity and boolean variable assignment tasks (multi-step reasoning / sequential state updates)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>scratchpad sequences matched input lengths; parity up to 40 bits in evaluation, coin-flip up to 20 flips in prompting experiments</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>finetuning with scratchpad targets; padded-scratchpad (pad input and scratchpad so positional bins align); masking distractors in input/scratchpad; EOS token and positional bias investigations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: scratchpad finetuning improved in-distribution performance but still failed to generalize to longer OOD lengths, showing qualitatively similar pathologies as vanilla finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Poor OOD performance arises because the attention mechanism does not learn to reliably attend to the correct input tokens when constructing scratchpad steps; distractor tokens are the main culprit; position-bias training and EOS prediction help somewhat but do not explain the core failure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scratchpad finetuning did not remedy length generalization across model scales; scale alone insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Attention misalignment due to distractors in input or preceding scratchpad, premature or problematic EOS handling can exacerbate but are not primary causes; per-step error rates remain roughly constant leading to compounded failures on longer instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to vanilla finetuning, few-shot scratchpad prompting, padded-scratchpad, and masked-distractor variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Simply finetuning to produce scratchpad steps does not solve length generalization—the model still learns attention patterns that do not generalize, primarily due to distractor-induced attention failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e327.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e327.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot Scratchpad Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context prompting with scratchpad (chain-of-thought) exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number of exemplar input–scratchpad–answer chains in-context (no weight updates) to induce the model to generate intermediate reasoning steps and extrapolate those templates to longer instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (pretrained; prominently reported: 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>128B (reported prominently), other sizes discussed</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>used on parity (coin-flip) and variable assignment tasks (sequential state updates / multi-step boolean reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Demonstrated mapping of 3-step exemplars to successful 20-step scratchpad generations (coin-flip/parity); parity prompts up to 20 flips evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot (in-context) exemplars with scratchpad-format examples; greedy decoding; natural-language coin-flip phrasing for parity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: few-shot scratchpad prompting substantially improves extrapolation to longer sequences on parity (high, nontrivial accuracy up to 20 flips); for variable assignment, few-shot prompting helps only when pretrained model already has nontrivial base capability. Paper reports significant improvement vs. zero-shot or finetuned-only baselines (figures contain detailed curves).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>In-context exemplars let the pretrained model perform variable-length template matching: the model reuses learned generation templates to fill extended scratchpad steps and infer state transitions rather than learning a new algorithm via gradient updates; this leverages pretraining priors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Authors note that few-shot scratchpad prompting improves with model size (this method 'scales with model size' as discussed and backed by related work), unlike vanilla finetuning for length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Effectiveness depends on prompt style and the non-finetuned base model skill; poor prompt styles yield pathologies similar to finetuning; when base performance is low, few-shot finetuning does not reliably fix OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against zero-shot, finetuning, scratchpad finetuning, and alternative prompt styles; compared to prior few-shot results that test smaller flip counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Few-shot scratchpad prompting can induce pretrained LLMs to generalize stepwise templates to much longer instances, providing a practical way to improve length generalization without weight updates, but its success depends on base model capability and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e327.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e327.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mechanistic Conclusions (attention / distractors / shortcuts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic insights: attention-driven parallel shortcuts, computational graph depth, and distractor-induced failures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthesis of mechanistic observations: transformers tend to use attention-based parallel strategies (pooling/counting) instead of sequential state machines; difficulty aligns with computational graph depth; distractor tokens disrupt attention patterns and cause OOD failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (studies performed on LaMDA family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see individual experiment entries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>applies to parity and boolean multi-step reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>diagnostic analyses including varied-number-of-ones parity split, padded-scratchpad, masking distractors, shuffled-ops baseline, per-step error rate tracking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Key mechanistic findings: (1) Transformers favor parallel pooling-like attention patterns enabling shortcut solutions (e.g., counting ones globally) rather than left-to-right sequential state updates; (2) 'computational graph depth' (longest dependency chain) better predicts difficulty than raw token count; (3) distractor tokens (irrelevant inputs or prior scratchpad tokens) are primary causes of failure because they prevent the model from learning attention patterns that implement the sequential algorithm; (4) position-bias limitations and EOS prediction issues are secondary contributors but not main causes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scaling model size/data during finetuning does not correct the learned parallel-shortcut bias; however, in-context scratchpad generalization benefits from larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Parallel shortcut learning (count/threshold) leading to random-like OOD behavior under distribution shifts; attention misrouting due to distractors; compounded per-step error across long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Diagnostic comparisons: padded vs unpadded scratchpad, masked distractors vs full input, shuffled-ops baseline, finetuning vs prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Length generalization failures arise from attention-driven shortcut strategies and distractor-induced misattention; interventions that reduce distractors or induce template-based sequential generation (few-shot scratchpad) can restore extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e327.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e327.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior mentions: Integer arithmetic & memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior observations about arithmetic performance (term-frequency/memorization effects and modest integer arithmetic generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>References and mentions to earlier findings that LLM arithmetic-like abilities often reflect memorization or dataset frequency effects and that scratchpad finetuning gave modest integer-arithmetic length generalization in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>integer arithmetic (mentioned in related work summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>mentioned prior work (term-frequency analyses; scratchpad finetuning in other studies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper cites Razeghi et al. showing correlation of arithmetic performance with term frequency in pretraining data (implying memorization rather than algorithmic competence); Nye et al. previously reported modest integer-arithmetic OOD results with scratchpad finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Authors situate their findings in prior observations that some arithmetic abilities may be due to memorization and that prior scratchpad finetuning reported only modest integer-arithmetic extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-327",
    "paper_id": "paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Parity (bitstring / coin-flip)",
            "name_full": "Parity task (bitstring parity / coin-flip natural language framing)",
            "brief_description": "A binary parity task: predict whether a sequence of bits (or an equivalent sequence of coin flips in natural language) has an even or odd number of ones/flips; used to probe sequential state-tracking vs. parallel shortcut strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (pretrained checkpoints used in experiments)",
            "model_size": "244M, 422M, 1B, 64B, 128B (various sizes evaluated; largest experiments used 128B)",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "parity (mod-2 counting; state tracking over a sequence); cast as coin-flip natural language parity",
            "number_range_or_complexity": "bitstring lengths evaluated from 3 up to 40; training lengths often 3–20 or 10–21 depending on split; alternate split: fixed token-length (30) with number of ones varied (training 10–20 ones, test 1–30 ones)",
            "method_or_intervention": "vanilla finetuning; finetuning with scratchpad; few-shot prompting with scratchpad (chain-of-thought / in-context exemplars); padded scratchpad; masking experiments",
            "performance_result": "Qualitative: vanilla finetuning achieves near-perfect in-distribution accuracy but rapidly degrades on out-of-distribution (longer) lengths; in the varied-number-of-ones split, OOD performance was roughly equivalent to random. Few-shot scratchpad prompting on pretrained models (not finetuned) produced a large improvement, enabling correct extrapolation of scratchpad templates to much longer queries (they report high, nontrivial accuracy up to 20 flips). Exact numeric accuracies are shown in paper figures but not tabulated in text.",
            "mechanistic_insight": "Transformers trained via finetuning tended to learn non-sequential, parallel 'counting/pooling' shortcuts (e.g., attend-and-threshold counting number of ones) rather than a left-to-right sequential state-tracking algorithm; few-shot scratchpad prompting lets pretrained models perform variable-length template matching and infer state transitions from exemplars. Failures are driven by attention patterns (distractors) and not primarily by untrained position biases or EOS prediction.",
            "performance_scaling": "Model scale (across evaluated LaMDA sizes) had little effect on finetuned models' length generalization; by contrast, few-shot scratchpad prompting abilities improve with model size (this approach 'scales with model size' per paper discussion and references).",
            "failure_modes": "Rapid degradation on out-of-distribution (longer) lengths; when trained on number-of-ones distribution, models default to threshold/count heuristics producing near-random OOD behavior; distractor tokens in input/scratchpad harm generalization; per-step scratchpad error rate can remain roughly constant causing aggregate failure on long instances.",
            "comparison_baseline": "Compared vanilla finetuning vs. scratchpad finetuning vs. few-shot scratchpad prompting; compared multiple model sizes; compared padded-scratchpad and masked-distractor interventions; evaluated a shuffled-operations baseline to probe reliance on sequential structure.",
            "key_finding": "Vanilla finetuning yields non-sequential shortcut solutions that fail to extrapolate parity to longer sequences, whereas few-shot scratchpad prompting on pretrained LLMs can induce template-like sequential solutions that extrapolate substantially better.",
            "uuid": "e327.0",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Variable Assignment (Boolean)",
            "name_full": "Boolean Variable Assignment task (synthetic Python-style programs with boolean ops)",
            "brief_description": "Predict value of a queried boolean variable after executing a sequence of single-line boolean assignments; constructed to require iterative state updates and to expose dependency-chain (computational graph depth) difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (pretrained checkpoints used in experiments); also evaluated Codex (for comparison shown in figures)",
            "model_size": "244M, 422M, 1B, 64B, 128B (various sizes; main experiments used LaMDA family incl. 128B); an OpenAI Codex result is presented for comparison",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "multi-step Boolean operations (assign, and, or, xor, negate, conditional assigns) — algorithmic boolean execution rather than numeric arithmetic",
            "number_range_or_complexity": "program lengths: chain-like split training lengths 3–8, evaluation up to 19; diverse split ranges: training min/max operations 8–32; number of variables varied (chain-like 2–3, diverse 4–10)",
            "method_or_intervention": "vanilla finetuning; scratchpad finetuning (scratchpad = copy program + comments showing variable values); few-shot scratchpad prompting; shuffled-ops baseline; masked-distractor experiments",
            "performance_result": "Qualitative: finetuned models reach (near) perfect in-distribution accuracy but degrade quickly on longer OOD lengths; few-shot scratchpad prompting provided little extra benefit unless the base pretrained model already had non-trivial performance on the task. Shuffled-ops baseline matched poor OOD performance, indicating reliance on non-sequential cues.",
            "mechanistic_insight": "Transformers learn strategies that favor resolving shallow dependency (small computational graph depth) via parallel attention rather than true sequential execution; 'computational graph depth' (longest dependency chain) is a better predictor of difficulty than number of operations. Distractor tokens in input cause attention to misroute leading to failures.",
            "performance_scaling": "Scaling model size/data/compute during finetuning did not meaningfully improve OOD length generalization; few-shot scratchpad prompting effectiveness depends on pretrained base performance and can benefit from larger models.",
            "failure_modes": "Fails on programs with larger computational graph depth; models trained on shuffled operation orders can still exploit spurious correlations and achieve similar OOD failures; distractor input tokens and preceding scratchpad tokens impede learning sequential attention patterns.",
            "comparison_baseline": "Compared chain-like vs diverse splits; compared finetuning vs finetuning on shuffled-ops baseline; compared few-shot prompting vs. no-finetuning; examined masking of distractors and padded-scratchpad.",
            "key_finding": "Transformers finetuned on variable-assignment learn non-sequential shortcuts that fail as dependency chains deepen, and computational graph depth—not raw program length—governs difficulty; few-shot scratchpad helps only when pretrained models already possess relevant capability.",
            "uuid": "e327.1",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Scratchpad Finetuning",
            "name_full": "Scratchpad finetuning (finetuning models to generate intermediate solution steps)",
            "brief_description": "Finetuning transformers to generate intermediate step-by-step computations (scratchpad / chain-of-thought) before producing final answers, tested to assess whether stepwise outputs improve length generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA (various sizes)",
            "model_size": "244M, 422M, 1B, 64B, 128B (various)",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "applied to parity and boolean variable assignment tasks (multi-step reasoning / sequential state updates)",
            "number_range_or_complexity": "scratchpad sequences matched input lengths; parity up to 40 bits in evaluation, coin-flip up to 20 flips in prompting experiments",
            "method_or_intervention": "finetuning with scratchpad targets; padded-scratchpad (pad input and scratchpad so positional bins align); masking distractors in input/scratchpad; EOS token and positional bias investigations",
            "performance_result": "Qualitative: scratchpad finetuning improved in-distribution performance but still failed to generalize to longer OOD lengths, showing qualitatively similar pathologies as vanilla finetuning.",
            "mechanistic_insight": "Poor OOD performance arises because the attention mechanism does not learn to reliably attend to the correct input tokens when constructing scratchpad steps; distractor tokens are the main culprit; position-bias training and EOS prediction help somewhat but do not explain the core failure.",
            "performance_scaling": "Scratchpad finetuning did not remedy length generalization across model scales; scale alone insufficient.",
            "failure_modes": "Attention misalignment due to distractors in input or preceding scratchpad, premature or problematic EOS handling can exacerbate but are not primary causes; per-step error rates remain roughly constant leading to compounded failures on longer instances.",
            "comparison_baseline": "Compared to vanilla finetuning, few-shot scratchpad prompting, padded-scratchpad, and masked-distractor variants.",
            "key_finding": "Simply finetuning to produce scratchpad steps does not solve length generalization—the model still learns attention patterns that do not generalize, primarily due to distractor-induced attention failures.",
            "uuid": "e327.2",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Few-shot Scratchpad Prompting",
            "name_full": "Few-shot in-context prompting with scratchpad (chain-of-thought) exemplars",
            "brief_description": "Providing a small number of exemplar input–scratchpad–answer chains in-context (no weight updates) to induce the model to generate intermediate reasoning steps and extrapolate those templates to longer instances.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA (pretrained; prominently reported: 128B)",
            "model_size": "128B (reported prominently), other sizes discussed",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "used on parity (coin-flip) and variable assignment tasks (sequential state updates / multi-step boolean reasoning)",
            "number_range_or_complexity": "Demonstrated mapping of 3-step exemplars to successful 20-step scratchpad generations (coin-flip/parity); parity prompts up to 20 flips evaluated",
            "method_or_intervention": "few-shot (in-context) exemplars with scratchpad-format examples; greedy decoding; natural-language coin-flip phrasing for parity",
            "performance_result": "Qualitative: few-shot scratchpad prompting substantially improves extrapolation to longer sequences on parity (high, nontrivial accuracy up to 20 flips); for variable assignment, few-shot prompting helps only when pretrained model already has nontrivial base capability. Paper reports significant improvement vs. zero-shot or finetuned-only baselines (figures contain detailed curves).",
            "mechanistic_insight": "In-context exemplars let the pretrained model perform variable-length template matching: the model reuses learned generation templates to fill extended scratchpad steps and infer state transitions rather than learning a new algorithm via gradient updates; this leverages pretraining priors.",
            "performance_scaling": "Authors note that few-shot scratchpad prompting improves with model size (this method 'scales with model size' as discussed and backed by related work), unlike vanilla finetuning for length generalization.",
            "failure_modes": "Effectiveness depends on prompt style and the non-finetuned base model skill; poor prompt styles yield pathologies similar to finetuning; when base performance is low, few-shot finetuning does not reliably fix OOD generalization.",
            "comparison_baseline": "Compared against zero-shot, finetuning, scratchpad finetuning, and alternative prompt styles; compared to prior few-shot results that test smaller flip counts.",
            "key_finding": "Few-shot scratchpad prompting can induce pretrained LLMs to generalize stepwise templates to much longer instances, providing a practical way to improve length generalization without weight updates, but its success depends on base model capability and prompt design.",
            "uuid": "e327.3",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Mechanistic Conclusions (attention / distractors / shortcuts)",
            "name_full": "Mechanistic insights: attention-driven parallel shortcuts, computational graph depth, and distractor-induced failures",
            "brief_description": "Synthesis of mechanistic observations: transformers tend to use attention-based parallel strategies (pooling/counting) instead of sequential state machines; difficulty aligns with computational graph depth; distractor tokens disrupt attention patterns and cause OOD failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (studies performed on LaMDA family)",
            "model_size": "various (see individual experiment entries)",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "applies to parity and boolean multi-step reasoning tasks",
            "number_range_or_complexity": null,
            "method_or_intervention": "diagnostic analyses including varied-number-of-ones parity split, padded-scratchpad, masking distractors, shuffled-ops baseline, per-step error rate tracking",
            "performance_result": null,
            "mechanistic_insight": "Key mechanistic findings: (1) Transformers favor parallel pooling-like attention patterns enabling shortcut solutions (e.g., counting ones globally) rather than left-to-right sequential state updates; (2) 'computational graph depth' (longest dependency chain) better predicts difficulty than raw token count; (3) distractor tokens (irrelevant inputs or prior scratchpad tokens) are primary causes of failure because they prevent the model from learning attention patterns that implement the sequential algorithm; (4) position-bias limitations and EOS prediction issues are secondary contributors but not main causes.",
            "performance_scaling": "Scaling model size/data during finetuning does not correct the learned parallel-shortcut bias; however, in-context scratchpad generalization benefits from larger models.",
            "failure_modes": "Parallel shortcut learning (count/threshold) leading to random-like OOD behavior under distribution shifts; attention misrouting due to distractors; compounded per-step error across long sequences.",
            "comparison_baseline": "Diagnostic comparisons: padded vs unpadded scratchpad, masked distractors vs full input, shuffled-ops baseline, finetuning vs prompting.",
            "key_finding": "Length generalization failures arise from attention-driven shortcut strategies and distractor-induced misattention; interventions that reduce distractors or induce template-based sequential generation (few-shot scratchpad) can restore extrapolation.",
            "uuid": "e327.4",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Prior mentions: Integer arithmetic & memorization",
            "name_full": "Prior observations about arithmetic performance (term-frequency/memorization effects and modest integer arithmetic generalization)",
            "brief_description": "References and mentions to earlier findings that LLM arithmetic-like abilities often reflect memorization or dataset frequency effects and that scratchpad finetuning gave modest integer-arithmetic length generalization in prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "integer arithmetic (mentioned in related work summaries)",
            "number_range_or_complexity": null,
            "method_or_intervention": "mentioned prior work (term-frequency analyses; scratchpad finetuning in other studies)",
            "performance_result": null,
            "mechanistic_insight": "Paper cites Razeghi et al. showing correlation of arithmetic performance with term frequency in pretraining data (implying memorization rather than algorithmic competence); Nye et al. previously reported modest integer-arithmetic OOD results with scratchpad finetuning.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "Authors situate their findings in prior observations that some arithmetic abilities may be due to memorization and that prior scratchpad finetuning reported only modest integer-arithmetic extrapolation.",
            "uuid": "e327.5",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1
        }
    ],
    "cost": 0.015478249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Length Generalization in Large Language Models</h1>
<p>Cem Anil ${ }^{\text {1, }}$, Yuhuai $\mathrm{Wu}^{2}$, Anders Andreassen ${ }^{1}$, Aitor Lewkowycz ${ }^{1}$<br>Vedant Misra ${ }^{1}$, Vinay Ramasesh ${ }^{1}$, Ambrose Slone ${ }^{1}$, Guy Gur-Ari ${ }^{1}$, Ethan Dyer ${ }^{1}$, Behnam Neyshabur ${ }^{1}$<br>${ }^{1}$ Google Research, Blueshift Team<br>${ }^{2}$ Google Research<br>${ }^{3}$ University of Toronto, Vector Institute</p>
<h4>Abstract</h4>
<p>The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
<h2>1 Introduction</h2>
<p>Many natural problems, such as theorem proving and program synthesis, have a notion of length that strongly correlates with the difficulty of the task. However, in these domains, the number of available problems typically drops rapidly as a function of problem length (e.g. Figure 2). Hence, it is desirable to learn from examples of shorter lengths to generalize to longer ones or at least reduce the number of samples required for longer examples. We refer to this type of problem as length generalization.</p>
<p>Recent work on large language models (LLMs) has shown consistent improvement in their performance by scaling model and dataset size. However, such models are still incapable of length generalization. For example, [1] shows that even though scale helps with solving arithmetic problems, scale alone is likely insufficient for learning to solve instances of arbitrary lengths. This implies that models fail to learn the general algorithms that would enable this kind of generalization. Indeed, Razeghi et al. [2] showed that the performance of LLMs on mathematical calculations correlates with term frequency in the training data. This suggests that LLMs might have gained their current performance from surface-level memorization instead of learning to apply the correct algorithm.</p>
<p>A recent line of work proposes to use a scratchpad, or chain-of-thought reasoning, when prompting LLMs [3, 4, 5] on multi-step tasks. Breaking down tasks into multiple small steps and presenting these steps to the model leads to improved performance across a variety of reasoning tasks including word problems, arithmetic, and code execution.</p>
<p>We perform a systematic study of length generalization with transformer-based large language models. We consider problems in which learning an algorithm can in principle enable a model to extrapolate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of variable assignment problems: Can transformer language models learn from short instances of the Variable Assignment task (left) to extrapolate to much longer instances (right)? Length generalization is the ability to learn from shorter/easier instances of a problem to handle longer/harder instances.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Techniques</th>
<th style="text-align: center;">In-distribution</th>
<th style="text-align: center;">Out-of-distribution</th>
<th style="text-align: center;">Improves with scale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tune</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark^{+}$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on length generalization tasks of three techniques that language models admit: (1) Finetuning, (2) Prompting (or in-context few-shot learning) and (3) Scratchpad (Chain-of-Thought reasoning). We find that each technique (and the combinations thereof) have different modes of failure and present different trade-offs regarding in and out-of-distribution coverage. $\times$ signifies poor $\checkmark$ signifies nontrivial, $\checkmark \checkmark$ signifies near-perfect performance. (*) Refers to task-dependency.
from short examples to problems of arbitrary length. In particular, we focus on two simple algorithmic tasks, parity and variable assignment, in which the model needs to keep track of a state in order to extrapolate to longer lengths (see Figure 1). These problems are illuminating because their simplicity allows us to probe the failure modes as well as contrast the learned solutions with the ground truth algorithm. They provide us with a setting to study how/when these large language models start to fail.</p>
<p>We study combinations of three kinds of techniques for LLMs: finetuning, few shot prompting (also referred to as in-context learning), and use of a scratchpad (also referred to as chain-of-thought), to understand the role of each method and the interplay among the three in length generalization. Interestingly, we observe non-trivial interactions among the three techniques; see Table 1.</p>
<p>Contributions Our main contributions are as follows:</p>
<ul>
<li>We define and characterize the problem of length generalization using notions such as state tracking, execution depth, and per-step error rate. We study and carefully design two tasks, parity and variable assignment, that measure length generalization (Section 2).</li>
<li>We find that in the finetuning regime, scaling data, model sizes, and compute does not improve length generalization (Section 3.1). We also observe that even when the model attains perfect in-distribution accuracy, it performs poorly in out-of-distribution domains. Surprisingly, different hyperparameter choices for finetuning have a large effect on length generalization performance, while having minimal effect on the final in-distribution performance (Section 3.3).</li>
<li>We establish finetuning with scratchpad also fails to generalize to longer problems, in contrast to what is suggested by previous works [3]. We look into three potential failure cases: positional encoding, the presence of distractors, and end of token prediction, and conclude that distractors are the main culprit of failures for length generalization (Section 4).</li>
<li>We show that in the in-context learning regime, use of a scratchpad shows a qualitatively different behavior and significantly alleviates the decay of performance on longer problems. This capability is significant, as it implies that for LLMs, there are certain skills, like length generalization, that can be learned through in-context learning rather than through finetuning even in the presence of infinite data. This is in stark contrast to the common norms of machine learning (Section 5).</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Real world datasets have heavy tails in length: (left) Histogram of lengths for proofs presented in the Archive of Formal Proofs (right) Histogram of the number of tokens for solutions in the MATH dataset. [6]</p>
<h1>2 Length Generalization</h1>
<p>Many sequence tasks-especially ones that require reasoning capabilities-have problem instances that differ in terms of their lengths. Shorter instances are often easier to state, process, and handle, and require less compute to find the answer. By contrast, longer instances are more challenging to parse and require more compute to solve. Tasks that have a reasoning component are especially well represented in this category - multi-hop reasoning [7], program execution [8], deductive reasoning [9] and theorem proving [10], to name a few. Note that having to deal with differing problem lengths poses two significant challenges. First, it is often the case that one encounters longer problem instances than the ones ever encountered during training, and is required to extrapolate. Second, even though longer problem instances have much more variety, real-world datasets often contain few long instances (see Figure 2). Both of these challenges are exacerbated if learning agents are not able to generalize across and beyond the lengths they learn from during training. This paper is about investigating to what extent transformer based language models are able to observe short problem instances and extrapolate to longer ones.
Instance Length as Number of Steps in a Markov Process It is possible to define problem length in many different ways to capture different aspects of problem difficulty. Does there exist a notion of length that would expose the same length-generalization-related problem structure observed in qualitatively very different settings? Such a framing would enable researchers to design algorithms and interventions that have the potential to generalize across a broad range of tasks. To this end, we take the approach of characterizing length in the context of a deterministic Markov process. From this perspective, length is simply the number of state transitions experienced by an initial world state. In other words, the data-generation process can be described as sampling an (1) initial state and a (2) variable number of state transformations to be applied sequentially on the initial state. The agent is provided both the initial state and the transformations, and is asked to predict the final state. This framing applies to a wide range of sequence problems, if not all of them-ranging from more mechanical tasks such as code and algorithm execution and theorem proving, to less structured tasks, such as solving math problems and summarizing novels.
In our empirical investigation we focus on two synthetic tasks: parity and variable assignment. These tasks avoid problem-specific subtleties that could mislead our analyses, while strongly capturing the deterministic Markov process structure.</p>
<h3>2.1 Tasks</h3>
<p>Parity: The parity task is an age-old learning problem that requires the trained agent to predict whether a bit-string has an even or odd number of ones in it. For example, the parity of the bitstring $[0,1,1,0,1]$ is "odd" (or 1) as opposed to "even" (or 0 ), because there is an odd number of 1 s in the bit-string. The parity task admits a sequential solution that enables length generalization in a straightforward way: simply process the bits left-to-right and record the parity of the bits processed so far as the state. The default notion of length in the parity task is the number of bits in the input. However, we also experiment with a version where the number of bits is kept constant, and the number of 1 s (i.e. the parity flipping bit) is systematically varied. The number of 1 s stands for the number of state changes contained in the input bit-string, and actually appears to capture a more relevant notion of length for transformer models (see Section 3.1).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Finetuned Length Generalization performance doesn't improve with scale: Models of vastly different scales fail at length generalization on both Parity and Variable Assignment tasks, displaying identical generalization pathologies. The x -axis represents problem length and the y -axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>Boolean Variable Assignment Task: The Boolean Variable Assignment task is designed to capture arbitrarily long, potentially branching unidirectional execution flows. An instance of this task can be seen in Figure 1. The inputs consist of semantically correct (i.e. bug-free) Python programs in which each line contains a boolean variable assignment operation. The output is simply the value of the variable presented in the final line of the program. The sequential solution to this task is to simply execute the program line by line while keeping track of the state of all variables.</p>
<p>The data generation procedure involves randomly generating execution flows that involve Boolean operations; see Supplementary Material (SM) for details.</p>
<p>We focus our evaluations on two variants of this dataset. (1) The diverse variable assignment split consists of a wide range of boolean operators available and is intended to contain maximally diverse programs. (2) The chain-like variable assignment split consists only of operations that compose the values of already defined variables. This results in long chains of dependencies between the initial values of the variables and the queried one, ensuring that there are almost no redundant operations in the program (i.e. operations that can be removed without affecting the output of the program). This split emphasizes the sequential nature of the variable assignment problem.</p>
<h1>3 Standard Finetuning Fails at Length Generalization</h1>
<p>We begin by demonstrating that finetuning transformer models on length-generalization tasks results in poor out-of-distribution performance. In experiments we use LaMDA ${ }^{2}$ decoder-only models. These checkpoints were trained using general natural language data. We use the AdaFactor optimizer [11] during finetuning, and tune the learning rate, batch size and dropout. We trained the networks until the in-distribution validation accuracy settles (20000 gradient steps for parity and 18000 gradient steps for variable assignment). The loss was only computed on the target tokens (i.e. the model wasn't trained to model the input questions).</p>
<h3>3.1 Scale Doesn't Improve Length Generalization</h3>
<p>Parity: We finetuned four pretrained LaMDA models with $244 \mathrm{~m}, 422 \mathrm{~m}, 1 \mathrm{~b}$ and 64 b parameters on the parity task, where the training distribution included randomly sampled bitstrings of length 10 to 21. We then evaluated the performance on bitstrings of length 3 to 40; see Figure 3. We find that model scale has a little effect on length generalization.
Variable Assignment: We finetuned the same models on the chain-like Variable Assignment Task, described in Section 2. We kept the in-distribution lengths at 3 to 8 , and evaluated the test performance on lengths 3 to 19. The results can be seen in Figure 3. Just like in the parity task, while the indistribution performance is (near) perfect, out-of-distribution performance degrades rapidly as length increases. To get a sense of just how weak the out-of-distribution performance is, we also trained a 422 m model on the same dataset, except we shuffled the operations before feeding it to the model. This removes the sequential dependency between the operations, and helps us establish a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Scratchpad finetuning displays poor length generalization: Scratchpad finetuning displays qualitatively similar length generalization pathologies as vanilla finetuning. The x-axis represents problem length and the y-axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>strong baseline that only predicts the answers based on non-sequential, spurious correlations. The accuracy-length curves for the baseline can be found in SM.</p>
<h3>3.2 Transformers Prefer Parallel Strategies over Sequential Ones</h3>
<p>The results presented in Section 3.1 establish that, when presented with sequential length generalization problems, transformers are biased toward learning non-sequential "shortcut" solutions that fail at longer problem instances. We ran additional experiments to gain a better understanding of the nature of this generalization pattern.</p>
<p>On parity, we ran finetuning on a different distribution of bit-strings: Instead of first randomly sampling the number of bits in the input bit-string, then sampling the values of the bits, we fixed the total number of bits in the input, and only varied the <em>number of ones</em> in the bit-string uniformly. We trained with 10 to 20 ones in the input distribution and tested on an interval containing 1 to 30 ones. This makes sure that the number of tokens (now fixed at 30) is now disambiguated from number of state changes, which for parity is equal to the number of ones. The difference between in and out-of-distribution performance is even starker for this data distribution (Figure 5): while in-distribution performance was 100%, OOD performance was roughly equivalent to random prediction<sup>3</sup>. This suggests that the transformers are learning a non-sequential solution that involves counting the number of ones in the input, and then thresholding the output. This is not surprising, given that self-attention is an equivariant transformation capable of performing pooling operations like max-pooling [12]. This strategy doesn't allow for knowledge transfer between problems of different lengths. Note that this bottom-up counting behaviour is complementary to the left-to-right counting behaviour displayed by recurrent models Suzgun et al. [13]. On the variable assignment dataset, we finetuned a 255m LaMDA model on the <em>diverse</em> split of the variable assignment dataset of programs up to 16 lines, and evaluated on the same data generating distribution up to 32 lines. We measured the evolution of the model's accuracy with respect to training iterations on different program lengths (quantified by number of lines). The results are in SM.</p>
<p>We again observed that a different notion of length (which we call <em>computational graph depth</em>) captures the difficulty of problem instances better than number of program operations. A variable assignment program can be represented as a computational graph where each node corresponds to a variable, and each edge corresponds to an operation. Computational graph depth is the length of the longest dependency chain that connects to the queried variable node. This notion of length corresponds to the highly parallelizable strategy of executing programs by iteratively resolving computational graph dependencies. We present two results that suggest that computational graph depth is a more relevant notion of length for transformers. (1) Inspecting the order of problem instances in which the trained transformer correctly solves this task, we find that performance is strongest on examples with small computational graph depth, even if these examples are long in</p>
<p><sup>3</sup>The periodic 0% and near 100% performance on OOD lengths is due to the models' tendency to output 0 or 1 depending on whether the input has a significantly higher ratio of 0s or 1s. On average, the accuracy on OOD length is not better than random guess.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (left) Complete lack of length generalization: Transformers trained on the parity task have difficulty generalizing to bit-strings that have a different number of $1 s$. (right) Sensitivity to hyperparameters: Trained networks sharing architecture, data and in-distribution loss can have very different length generalization performances. $l r$ stands "learning rate" and $b s$ stands for "batch size".
terms of number of operations. (2) The transformer does a good job of handling programs with an out-of-distribution number of operations, but for which computational graph depth is in-distribution.</p>
<h1>3.3 In-Distribution Generalization Doesn't Predict OOD Generalization on Length Generalization Tasks</h1>
<p>Prior work on out-of-distribution generalization establishes that in many tasks, in-distribution loss is a strong predictor of out-of-distribution generalization [14]. Our experiments on the parity task indicate that the distribution shift induced by changing problem lengths falls outside of the this category. Figure 5 shows how the same model trained on the same data achieving roughly the same in-distribution cross entropy loss behaves on OOD data, where the difference is solely induced by the choice of different hyperparameters.</p>
<h2>4 Scratchpad Finetuning Still Fails at Length Generalization</h2>
<p>It has been shown in prior work that it's possible to get pretrained LLMs to solve a given task by not only outputting the answer, but also the solution steps behind it. Nye et al. [15] use scratchpad finetuning to achieve strong in-distribution performance on execution based tasks such as code execution and computing polynomials. While they also report modest length generalization results on integer arithmetic, we find that scratchpad finetuning suffers from similar length generalization pathologies than vanilla finetuning does. The results on parity and variable assignment tasks can be seen in Figure 4. The precise scratchpad strategies used for these tasks are described in detail in SM.</p>
<p>Error analysis: To understand the causes of failure in training scratchpad strategies, we focused on two architectural choices that could account for the poor performance: (1) how transformers encode position information, and (2) whether the transformers are trained to predict an end-of-sequence (EOS) token. LaMDA models use T5 position biases [16] to handle position information. If the network is only trained with short instances, position biases that handle longer positional distances might not be trained, explaining poor length generalization. Similarly, Newman et al. [17] report that networks trained with EOS token prediction often suffer from generalizing to longer problem instances, because of the models' tendency to emit EOS tokens prematurely, as well as the EOS tokens' effect on the representations that get learned.</p>
<p>We tested the extent to which these effects can explain lack of length generalization as follows. We padded both the input bit-strings and the scratchpad content with dummy padding tokens to make the token count the same. We also augmented the input and scratchpad targets with the same number of padding tokens on the left and right so that the relevant bit to attend to when executing the sequential scratchpad strategy corresponds to the same T5 position bias bin. Examples of the updated input-target pairs can be seen in SM. While this intervention helps, the trained models still display significant length generalization issues.</p>
<p>To gain further insight about the source of the problem, we plotted how the scratchpad target prediction error rates change as a function of (1) how far along one is in constructing the scratchpad, and (2) the length of the input bit-string. The results can be seen in Figure 6. The fact that the model makes</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (left) Effect of input length on per-step scratchpad accuracy: Points corresponds to the accuracy (y-axis) of the first $x$ scratchpad steps (x-axis) on parity instances of variable length (color). If the input length is out-of-distribution, even in-distribution scratchpad steps are inaccurate, implying the model hasn't learned an attention pattern that generalizes to longer bit-strings. (right) Roughly constant per-step error rate: The per-step error rates of the LaMDA 128b model, few-shot finetuned on the coin-flip version of the parity task remain roughly constant across the scratchpad steps. This is in stark contrast with zero-shot scratchpad finetuned models, where the per-step error rates increase abruptly when the model is evaluated on OOD lengths.
mistakes in in-distribution scratchpad steps when the input has an OOD length implies that the attention mechanism isn't capturing the relevant part of the input to form the scratchpad output. See SM for additional analysis.</p>
<h1>5 Scratchpad Prompting Significantly Improves Length Generalization</h1>
<p>Wei et al. [4], Nye et al. [15] and Lewkowycz et al. [5] showed that combining prompting (i.e. in-context learning) with scratchpad strategies present a powerful combination. They demonstrate that pretrained LLMs, without the help of any finetuning, can solve grade school math word problems and execute pieces of code with nontrivial correctness [15], when prompted with the right scratchpad strategy. We corroborate these findings, and report that scratchpad prompting endows pretrained LLMs with the capability of variable length template matching (see Figure 8). That is, in-context learning enables the model to "learn" solution steps from a small number of short instances, and apply the same template on significantly longer instances with a high degree of accuracy.</p>
<h3>5.1 Few-shot scratchpad</h3>
<p>Contrary to vanilla and scratchpad finetuning, we find that under the right conditions, few-shot scratchpad strategies sometimes significantly improves LLMs' capability to extrapolate to lengths much further than what pretraining weights grant them.</p>
<p>To evaluate the performance of few-shot conditioning with scratchpad inputs without any finetuning, we phrase the parity problem in natural language as a coin flipping task. An example for the few-shot prompts we used can be seen in Figure 8. Wei et al. [4] also report results on the coin-flip task: the scratchpad format we used differs from theirs in that while ours respects the sequential nature of the task (i.e. each coin flip corresponds to a step in the scratchpad solution), Wei et al. [4]'s scratchpad strategy involves summing up the number of coin flips, then deciding on the final output based on the evenness/oddness of the sum. Also, while they only test up to 4 flips, we go up to 20 flips while still attaining highly nontrivial accuracy levels.</p>
<p>For the variable assignment task, our scratchpad strategy involves copying over the program that's being executed, with comments added in between lines specifying the value of the variable that was assigned in the line above. Instances of this scratchpad strategy can be seen in SM.</p>
<p>Figure 7 shows the performance of the pretrained LaMDA 128b model on the coin-flip version of the parity task. Figure 8 shows an instance of how a length 3 prompt can induce the model to correctly output a 20 step scratchpad. We find that with the right scratchpad prompt, LLMs are able to generate correct scratchpad solutions. This reduces the problem to simply filling in the content of</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Few-shot finetuning with scratchpad displays qualitatively different behaviour on parity and variable assignment tasks. On parity, where the non-finetuned model already performs very well, few-shotfinetuning with scratchpad leads to a significant performance boost over zero-shot finetuning with scratchpad. On variable assignment, where the base model doesn't perform poorly, there's not a significant gap between few-shot finetuning and zero-shot finetuning with scrathpad. The performance of OpenAI's Codex model [18] on the variable assignment task is also provided.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Few-shot length generalization: The largest LaMDA model is able to map the scratchpad solution template from a few short exemplars onto much longer queries.</p>
<p>the generation correctly by inferring the right state transitions without having to figure out how to extrapolate the solution template.</p>
<p>Few-Shot Finetuning with Scratchpad Strategies: Does combining finetuning, few-shot prompting, and scratchpad strategies improve length generalization?</p>
<p>We find that the answer is <strong>yes</strong> in the case of parity. As seen in Figure 7, few-shot finetuning performs significantly better than the baseline model, both on in- and out-of-distribution lengths. Note that the vanilla (i.e. no shot) finetuning baseline also outperforms the no-finetuning baseline, it actually does worse on the larger lengths — a pathology that doesn't appear with few-shot finetuning.</p>
<p>The results point to a qualitatively different picture for the variable assignment task. Both few-shot finetuning and vanilla finetuning result in similar length generalization behavior (Figure 7). We hypothesize that this distinction is caused by the different pretrained performances that the model displays on these tasks: while length generalization is already strong with no finetuning on parity, that's not the case for variable assignment. In the latter case, the model is forced to acquire a new skill via finetuning, which displays the same pathologies as zero-shot finetuning with scratchpad. As a sanity check, we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining: As expected by the aforementioned hypothesis, we observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. The results can be found in SM. We leave a more rigorous evaluation of this hypothesis as future work.</p>
<h1>6 Related Works</h1>
<p>There have been many attempts to study generalization from shorter/easier to longer/harder examples.
Challenges in length generalization: Several existing works have investigated pathologies that arise when models are asked to generalize to processing and generating longer (measured by number of tokens) sequences. Newman et al. [17] find that sequence models trained with and in the absence of the end-of-sequence token display qualitatively different length extrapolation behaviour and learn different representations. Dubois et al. [19] proposes modifications to the commonly used dot-product attention to improve the models' ability to extrapolate to longer sequences. Murray and Chiang [20] demonstrate that neural machine translation models tend to have a bias towards generating shorter-than-desired translations. Yehudai et al. [21] show that length generalization issues are also present in training graph neural networks, where extrapolating across graph size presents a challenge. Ju et al. [22] propose a new attention mechanism to facilitate recurrent processing in transformer models. Press et al. [23] propose modifying transformer attention biases to facilitate generalization beyond the training context length. Concurrent work [24] propose a synthetic dataset named LEGO (Learning Equality and Group Operations), an instantiation of which resembles our variable assignment task where the only boolean operations allowed are assign and negate and assign, and overriding the values of variables is not allowed. Their analyses on OOD generalization largely complement ours: while we focus on decoder-only architectures and scratchpad strategies as a way of carrying over state, they focus on encoder-only architectures, and investigate the effect of weight-sharing.
Easy-to-Hard generalization: Schwarzschild et al. [25] and Bansal et al. [26] use weight-tied neural networks to generalize from easy to hard examples. Schwarzschild et al. [25] also provide three tasks to benchmark easy-to-hard generalization. Dehghani et al. [27] and Kaiser and Sutskever [28] assess the capabilities of their proposed architectures on easy-to-hard generalization problems.
Inductive Biases Related to Lenght Generalization: McCoy et al. [29] study the inductive bias of seq-to-seq learners on English question formation and English tense reinflection tasks and find that LSTM and GRU networks often display differing strategies, caused by the use of differing activation functions. Suzgun et al. [13] find that recurrent networks can perform dynamical counting, and encode hierarchical representations, which enables them to solve nontrivial Dyck tasks using k-counters. Kharitonov and Chaabouni [30] also study the inductive bias of different architectures, and conclude that transformer and LSTM architectural have a tendency to learn hierarchical strategies, whereas CNN based strategies display more linear structure. He et al. [31] propose a method to learn natural inference models that are not biased on spurious correlations. McCoy et al. [32] show that transformer models that display strong performance in natural language inference can have superficial biases that fool them in systematic ways and proposes a framework to think about these biases.</p>
<h2>7 Conclusion</h2>
<p>The ability to learn from shorter/easier problem instances to generalize to longer/harder ones is a key capability in a large number of tasks, especially ones requiring reasoning. We defined the concept of length generalization and measured language models' length generalization capabilities. After conducting careful experiments using finetuning, scratchpads, and few-shot prompting, we reached the following conclusions: (1) Generalizing in length is a challenge for language models at least up to the 100B parameter scale. Both vanilla finetuning and finetuning with scratchpads suffer from a lack of length generalization caused by models' tendency to pick up non-sequential pattern that don't apply to longer problem instances. (2) Few-shot scratchpad prompting enables pretrained large language models to pick up scratchpad-templates that extrapolate to arbitrary lengths, leading to dramatic improvements on longer problem instances. Unlike raw finetuning, this approach does scale with model size [4]. (3) Trying to further enhance the performance of few-shot scratchpad prompted LLMs via finetuning yields mixed results, depending on the non-finetuned performance of the base model at the target task. We emphasize that the aforementioned few-shot variable length pattern matching capability - something that doesn't require changing model architecture - offers a qualitatively different approach to handle length generalization in contrast to prior art that introduced architectural modifications to achieve the same goal. This capability is also significant in that it implies that for LLMs, there are certain skills, like length generalization, that can be learned better through in-context learning rather than through finetuning, even in the presence of infinite data.</p>
<h1>References</h1>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
[3] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022. URL https://arxiv.org/abs/2201.11903.
[5] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.
[6] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[7] Haoyu Wang, Mo Yu, Xiaoxiao Guo, Rajarshi Das, Wenhan Xiong, and Tian Gao. Do multi-hop readers dream of reasoning chains? arXiv preprint arXiv:1910.14520, 2019.
[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[9] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020.
[10] Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse. Int: An inequality benchmark for evaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924, 2020.
[11] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[12] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744-3753. PMLR, 2019.
[13] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks can perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.
[14] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. arXiv preprint arXiv:2010.15775, 2020.
[15] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
[17] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. arXiv preprint arXiv:2010.07174, 2020.</p>
<p>[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[19] Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.
[20] Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006, 2018.
[21] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning, pages 11975-11986. PMLR, 2021.
[22] Da Ju, Stephen Roller, Sainbayar Sukhbaatar, and Jason Weston. Staircase attention for recurrent processing of sequences. arXiv preprint arXiv:2106.04279, 2021.
[23] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.
[24] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
[25] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.
[26] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. arXiv preprint arXiv:2202.05826, 2022.
[27] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
[28] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.
[29] R Thomas McCoy, Robert Frank, and Tal Linzen. Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks. Transactions of the Association for Computational Linguistics, 8:125-140, 2020.
[30] Eugene Kharitonov and Rahma Chaabouni. What they do when in doubt: a study of inductive biases in seq2seq learners. arXiv preprint arXiv:2006.14953, 2020.
[31] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. arXiv preprint arXiv:1908.10763, 2019.
[32] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.
[33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Parity problem instances: (left) A sample 8-bit parity problem instance, along with the scratchpad targets. The scratchpad represent the intermediate parity state as the sequence is processed left to right. (right) A parity problem instance with the padded scratchpad strategy. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant even when there are different number of bits in the input.</p>
<h1>A Data Generation Details</h1>
<p>We describe the data generation procedures for the parity and variable assignment tasks in detail.</p>
<h2>A. 1 Parity Datasets:</h2>
<p>Synthetic Parity Dataset: A 8-bit example of the synthetic parity example, along with the corresponding scratchpad targets, can be seen in Figure 9. We added the prefix " $&gt;&gt;&gt;$ " to signify the start of the parity sequence, and the suffix "==" to signify the start of the target or scratchpad tokens. There's no special meaning associated with the particular prefixes and suffixed used.</p>
<p>We experimented with two version of the synthetic parity dataset: In one split, we varied the number of bits in the input, and in the other one, we varied the number of ones.</p>
<ul>
<li>Varied number of bit split: To generate the samples in this split, we first sampled the number of bits, then sampled each bit individually from a uniform Bernoulli distribution. For training, we used lengths between 3 and 20, and for validation/testing, we used lengths between 3 and 40 .</li>
<li>Varied number of ones split: Here, we fixed the number of bits at 30. To sample each instance, we first uniformly sampled the number of ones, then randomly placed each one in the fixed-length bitstring by randomly shuffling the bits. We used 10 to 20 ones in the training split, and 1 to 30 ones in the validation/test splits.
Padded scratchpad: The padded scratchpad format can be seen in 9. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens respectively, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant. Moreover the total number of characters/tokens is also kept constant. The number of tokens to pad on the left and right is determined (uniformly) randomly.</li>
</ul>
<p>The parity datasets contain 1000000 samples.
Natural Language Parity Dataset: In order to tap into the natural language understanding capabilities of pretrained language models, we situated the parity task as a "coin flip problem". In this framing, flipping a coin corresponds to 1 and not flipping a coin corresponds to 0 . To make the inputs as close as possible to English without occupying too many tokens, we used the sentence templates "Then <NAME> flips." and "Then <NAME> doesn't flip." to represent whether the coin was flipped or not respectively, where "<NAME>" refers to a randomly sampled given name. We also prepended each step with an integer id that count backwards from the total number of steps there are in the input sequence. We've experimented with versions where the integer ids are incremented. This didn't lead to a significant difference in the overall performance.</p>
<p>Two representative sample input-target pairs (including the exemplars) are provided in Figure 10.</p>
<h2>A. 2 Boolean Variable Assignment Dataset:</h2>
<p>An instance of the variable assignment dataset can be seen in Figure 11. The data generation procedure is aimed at synthesizing large number of qualitatively different programs: (1) A subset of boolean variable assignment operations is uniformly sampled from a large pool of operations. (2) The number</p>
<p>Input:
Question The coin is heads up. (4) Then Williams flips. (3) Then Ward flips. (2) Then Valentine doesn't flip. (1) Then Son doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Williams flips, coin turns to tails. (3) After Ward flips, coin becomes heads. (2) Valentine doesn't flip, so coin stays heads. (1) Son doesn't flip, so coin remains heads. DONE
#
Question The coin is heads up. (4) Then Shade flips. (3) Then Kong doesn't flip. (2) Then Kodi flips. (1) Then Charleston flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Shade flips, coin turns to tails. (3) Kong doesn't flip, so coin stays tails. (2) After Kodi flips, coin becomes heads. (1) After Charleston flips, coin turns to tails. DONE
#
Question The coin is heads up. (4) Then Ka doesn't flip. (3) Then Justice flips. (2) Then Johan flips. (1) Then Jamaica flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) Ka doesn't flip, so coin remains heads. (3) After Justice flips, coin becomes tails. (2) After Johan flips, coin turns to heads. (1) After Jamaica flips, coin becomes tails. DONE
#
Question The coin is heads up. (4) Then Cy flips. (3) Then Booker flips. (2) Then Ace doesn't flip. (1) Then Ren flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Cy flips, coin turns to tails. (3) After Booker flips, coin becomes heads. (2) Ace doesn't flip, so coin stays heads. (1) After Ren flips, coin turns to tails. DONE #
Question The coin is heads up. (6) Then Tristan flips. (5) Then Hillary flips. (4) Then Olivia flips. (3) Then Rosa doesn't flip. (2) Then Kurt flips. (1) Then Glenn doesn't flip. Is the coin still heads up?"</p>
<p>Scratchpad targets:
"Solution Coin is initially heads up. (6) After Tristan flips, coin becomes tails. (5) After Hillary flips, coin turns to heads. (4) After Olivia flips, coin becomes tails. (3) Rosa doesn't flip, so coin remains tails. (2) After Kurt flips, coin turns to heads. (1) Glenn doesn't flip, so coin stays heads. DONE</p>
<p>Input:
Question The coin is heads up. (4) Then Katarina doesn't flip. (3) Then January flips. (2) Then Duke flips. (1) Then Cal doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Katarina doesn't flip, so coin remains heads. (3) After January flips, coin becomes tails. (2) After Duke flips, coin turns to heads. (1) Cal doesn't flip, so coin stays heads. DONE
#
Question The coin is heads up. (4) Then Berry flips. (3) Then Abbi flips. (2) Then Tam flips. (1) Then Ikea doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Berry flips, coin becomes tails. (3) After Abbi flips, coin turns to heads. (2) After Tam flips, coin becomes tails. (1) Ikea doesn't flip, so coin remains tails. DONE #
Question The coin is heads up. (4) Then Cain doesn't flip. (3) Then Woody flips. (2) Then Von doesn't flip. (1) Then Thu doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Cain doesn't flip, so coin stays heads. (3) After Woody flips, coin turns to tails. (2) Von doesn't flip, so coin remains tails. (1) Thu doesn't flip, so coin stays tails. DONE
#
Question The coin is heads up. (4) Then Russ flips. (3) Then Williams flips. (2) Then Ward doesn't flip. (1) Then Valentine flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Russ flips, coin becomes tails. (3) After Williams flips, coin turns to heads. (2) Ward doesn't flip, so coin remains heads. (1) After Valentine flips, coin becomes tails. DONE
#
Question The coin is heads up. (8) Then Melissa flips. (7) Then Kevin doesn't flip. (6) Then Steven flips. (5) Then Thomas flips. (4) Then Timothy doesn't flip. (3) Then Kyle doesn't flip. (2) Then Rachel doesn't flip. (1) Then Laura doesn't flip. Is the coin still heads up?</p>
<h1>Scratchpad targets:</h1>
<p>Solution Coin is initially heads up. (8) After Melissa flips, coin turns to tails. (7) Kevin doesn't flip, so coin stays tails. (6) After Steven flips, coin becomes heads. (5) After Thomas flips, coin turns to tails. (4) Timothy doesn't flip, so coin remains tails. (3) Kyle doesn't flip, so coin stays tails. (2) Rachel doesn't flip, so coin remains tails. (1) Laura doesn't flip, so coin stays tails. DONE</p>
<p>Figure 10: Natural Language parity problem instances: The coin flip task - which consists of tracking the state of a coin as it undergoes a number of flip or no-flip operations - shares the same underlying problem structure as the parity task.
of operations and number of variables (along with their names, which are single-letter characters) are uniformly sampled based on pre-set hyperparameters. (3) One by one, operations and the variables included in the operations are sampled, while making sure that each added operations retains the semantic correctness of the program.
We now outline the hyperparameters used to generate the chain-like and diverse splits.</p>
<h2>Chain-like split:</h2>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate</li>
<li>Minimum/maximum number of operations: 3, 19</li>
<li>Minimum/maximum number of variables in the program: 2, 3</li>
</ul>
<h2>Diverse split:</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Variable assignment problem instance: A problem instance from the Boolean Variable Assignment dataset. The scratchpad strategy consists of outputting the value of the recenly updated variable in form of comments. Note that both the input and the scratchpad are valid Python programs.</p>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate, assign to and with boolean, assign to or with boolean, assign to xor with a boolean, conditional assign to a boolean, assign to another variable, conditional assign to another variable</li>
<li>Minimum/maximum number of operations: 8,32</li>
<li>Minimum/maximum number of variables in the program: 4, 10</li>
</ul>
<p>The scratchpad format (seen in Figure 11) consists of copying over the input program with comments after each line specifying the value of the recently updated variable. This ensures that the scratchpad itself is a valid Python program and can be used with models pretrained with Python data.
Both splits have 1500000 samples.</p>
<h1>B Baseline for Vanilla Finetuning on Variable Assignment</h1>
<p>Just how weak are the vanilla finetuned models on the OOD lengths on the variable assignment task? We trained baseline models with the same parameter count on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. While this leaves in some of the spurious features that correlate with the right answer, it completely eliminates the possibility of running a sequential algorithm to get to the final answer.
The results can be found in Figure 12. On OOD data, the performance of the shuffled-ops baseline is on par with the models trained with the clean version of the dataset. Note that the length generalization deficiency that the shuffled ops baselines display is even more dramatic than that of the models'</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Results of finetuning on the shuffled-ops variable assignment dataset: We trained baseline models with the same parameter counts on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. The generalization gap between in and out-of-distribution data persists here as well, due to transformers' tendency to prefer parallel strategies that don't generalize to larger lengths.
trained with clean data. This is not surprising, as the primary source of lack of length generalization is the transformers' tendency to prefer parallel strategies that don't generalize to larger lengths over picking up sequential algorithm</p>
<h1>C Experimental Conditions</h1>
<p>shuff We outline the training conditions and hyperparameter used in the main experiments.
In our experiments on different datasets, we first did a learning rate sweep over different model sizes, and preferred the largest learning rates that ensured training stability. This is due to the fact that we observed the best length generalization when we used larger learning rates (see Figure 5). We used the AdaFactor optimizer in all of our finetuning experiments [11]. We didn't use dropout [33] in the parity experiments and used a dropout rate of 0.05 in the variable assignment experiments. Our initial experiments suggest that one can often reach similar in and out-of-distribution performance either by training the weights from scratch, or finetuning from the pretrained weights. To keep the experiments consistent, we always initialized training using the pretrained weights. In variable assignment, we did a learning rate sweep over $0.0033,0.00033$ and 0.000033 . For parity, we search over learning rate velus of $0.002,0.0002$ and 0.00002 . We used a constant learning rate profile all throughout learning. Due to memory constraints, we used a batch size of 32 when training the $64 b$ and $128 b$ models.</p>
<p>We used greedy decoding in all of our experiments (including few-shot scratchpad ones). We experimented with temperature sampling with reranking based on sentence likelihoods, but found that doing this doesn't lead to qualitatively different results.</p>
<h2>D Computational Graph Depth is the Relevant Notion of Difficulty on Variable Assignment</h2>
<p>Computational graph depth captures a more relevant notion of difficulty on the variable assignment task for transformers. In Figure 13, we show how the accuracy of a transformer model evolves on samples of problem instances with different computational graph depths (left), and how the same</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Evolution of Performance over Training Iterations on Different Computational Graph Depths: Computational graph depth corresponds to the length of the longest dependency chain linking to the queried variable in the variable assignment task. This quantity captures a more suitable notion of length/difficulty for transformer models. On the left plot, we show how the accuracy of a transformer model evolves for problem instances with different computational graph depths. In the middle, we show the same, except that we constrain the number of operations in the program to an out-of-distribution number. The accuracy values roughly remain unchanged, indicating that it's not the number of operations, but computational graph depth that determines the difficulty of a problems instance. On the right, we show the evolution of accuracy on instances with different number of operations for reference.
quantity behaves if we fix the number of operations in a program at an out-of-distribution length (middle) ${ }^{4}$. Two takeaways from this analysis are:</p>
<ul>
<li>The fact that the accuracy values on samples with different computational graph depths roughly remain unchanged on OOD program lengths indicates that it's not the number of operations, but computational graph depth that captures a more relevant notion of difficulty.</li>
<li>Transformers initially pick up how to handle programs with a small computational graph depth throughout training and then move to more difficult programs.</li>
</ul>
<h1>E Effect of Prompt Style on Few-Shot Finetuning Performance</h1>
<p>In Section 5.1, we hypothesized that few-shot finetuning only leads to significant improvements in length generalization performance if the non-finetuned performance on the same task already at a nontrivial level. To provide a sanity check for this, we ran few-shot finetuning using an alternative prompt style for the coin-flip task that yields poor non-finetuned performance. As can be seen in Figure 14, the few-shot finetuned performance shows significant length generalization pathologies. We leave a systematic study of how prompt style affects length generalization as future work.</p>
<h2>F Distractor Analysis for Scratchpad Strategies</h2>
<p>Our analysis in Section 4 indicates that length generalization pathologies persist even when we use the padded scratchpad strategy that makes sure that it's not untrained position encodings and/or the EOS token prediction that causes the aforementioned pathologies. This points to the fact that the transformer doesn't learn to attend to the "right" section of the input and scratchpad that implements the sequential strategy that generalizes to longer lengths - it's thrown off by distractor tokens in the input and/or the preceding scratchpad targets. The distractor tokens at which section of the transformer context window (input or scratchpad) contribute more to the performance deterioration? If we remove all the distractor tokens, can we achieve perfect length generalization?
To answer these questions, we trained four transformer models under the following conditions: (1) We used the padded scratchpad strategy described in Section 4 with no modification, (2) We manually masked the preceding scratchpad tokens that don't contribute to the correct sequential algorithm (i.e.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Effect of prompt style on few-shot finetuning performance: we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining. We observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. This is in line with our hypothesis that the non-finetuned performance of the base model should be non-trivial for few-shot finetuning to consistently yield strong length generalization results.
we masked the distractor tokens in the input), (3) We masked the input tokens that don't contribute to the correct sequential algorithm, (4) we masked the distractor tokens in both the input and the preceding distractor tokens. To give an example, let's say the input bitstring is "[1 1011 ]", and the model has emitted the scratchpad tokens "[100]" so far. Masking the distracting sratchpad tokens simply means replacing all but the last scratchpad token with dummy padding tokens: "[x x 0]". Similarly, removing the distracting input tokens corresponds to masking out the part of the input that the network doesn't need to attend to while predicting the next bit: "[x x x 1 x]". Masking both corresponds to removing the distractor tokens in both the input and the target, so that the input and the preceding scratchpad become "[x x x 1 x]" and "[x x 0]" respectively.</p>
<p>The results can be seen in Figure 15. For all four experimental conditions, we plotted the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. We conclude from this experiment that:</p>
<ul>
<li>Removing all distractor tokens does result in perfect length generalization.</li>
<li>The distracting input tokens are hurt length generalization performance more.</li>
</ul>
<p>Based on this analysis, we conclude that innovations in transformer architectures and/or training methodology/objective that alleviate the issues caused by distractor tokens have a chance at significantly improving length generalization.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Distractor analysis: We trained scratchpad-augmented transformers to solve the parity task, where we systematically masked out the tokens in the input (left bottom) and the scratchpad (right top) that don't need to be attended to while implementing the correct sequential algorithm that solves parity. The plots illustrate the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. This analysis shows that (1) removing all distracting tokens leads to perfect length generalization (right bottom), (2) the distractor tokens in the input contribute more to the length generalization pathologies (right top versus left bottom).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The longest in-distribution number of operations is 15 , and we fix this quantity at 20 in the middle plot.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-279250852</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07398v2.pdf" target="_blank">G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Memory(ALFWorld+MacNet+Qwen-2.5-14b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Memory hierarchical agentic memory applied to ALFWorld with MacNet using Qwen-2.5-14b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>G-Memory is a three-tier hierarchical memory (insight, query, interaction graphs) that retrieves role-specific, multi-granularity memory (high-level insights and condensed interaction subgraphs) to augment multi-agent teams solving ALFWorld text-based embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Multi-agent system (MacNet) + G-Memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A decentralized multi-agent orchestration (MacNet) with edge agents; agents have roles and receive role-customized memory cues from G-Memory (insights + condensed interaction fragments) before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen-2.5-14b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Open-source Qwen series LLM (2.5 family, 14B parameters) used as the backbone LLM for agents in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A text-based embodied environment of household tasks where agents navigate and manipulate objects via natural-language actions; challenges include long multi-turn interactions, object state checks, and procedural correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical cross-trial + inside-trial (retrieval-augmented, role-personalized)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Three-tier graph: Interaction Graph (utterance nodes, temporal edges) stores fine-grained dialogue trajectories; Query Graph stores query nodes with metadata and links; Insight Graph stores distilled, generalizable insights linked to supporting queries. Memory is updated after each episode (interaction trace stored, new query node created, new insight distilled and connected).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Coarse similarity retrieval over query graph (embedding cosine), 1-hop graph expansion, bi-directional traversal: upward (query→insight) via query-to-insight projector to get I_S, downward (query→interaction) via an LLM-based graph sparsifier to extract core interaction subgraphs; then per-agent initialization Mem_i = Φ(I_S, {Ĝ_inter}, Role_i, Q) which filters/summarizes memories into role-specific cues concatenated or provided as context to each agent before MAS execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>79.10% success rate (ALFWorld) for MacNet + G-Memory with Qwen-2.5-14b</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>58.21% success rate (ALFWorld) for MacNet without G-Memory (no-memory baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to multiple baselines (Voyager, MemoryBank, Generative Agents, MetaGPT-M, ChatDev-M, MacNet-M) G-Memory produced large gains; specific reported comparison: with Qwen-2.5-14b G-Memory boosted MacNet ALFWorld from 58.21%→79.10% (+20.89%). Ablation shows removing interaction condensation or high-level insights each reduces performance; interactions removal caused larger drops than insights-only.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Hierarchical bi-directional memory (insight + condensed interaction subgraphs) with role-specific tailoring and limited retrieval (1-hop expansion, k in {1,2}) is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No catastrophic failure cases reported specifically for ALFWorld; general limitations include possible irrelevant insights from excessive hop expansion and noisy retrieval when k is too large. The paper notes that naive feeding of full long-context trajectories is ineffective and can harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use G-Memory's three-tier structure with 1-hop expansion and k ∈ {1,2}; perform LLM-based sparsification to condense interaction trajectories; provide role-specific distilled memories to agents (Φ operator); combine both high-level insights and fine-grained interaction snippets because both contribute (interactions slightly more impactful). Avoid naive full-context concatenation of long-scale MAS dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Memory(PDDL+AutoGen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Memory hierarchical memory applied to PDDL strategic games with AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>G-Memory provides query-level retrieval and interaction-trajectory condensation to AutoGen multi-agent teams solving PDDL strategic planning tasks, enabling improved division of labor via role-specific memory cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AutoGen + G-Memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>AutoGen orchestration (Solver, Ground Truth, Executor or similar roles) augmented with G-Memory which supplies retrieved insights and condensed historical trajectories tailored to each role.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen-2.5-(7b/14b) and gpt-4o-mini tested across experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Experiments used Qwen-2.5-7b, Qwen-2.5-14b and proprietary gpt-4o-mini as LLM backbones for agents; these serve as the base models generating utterances and performing memory summarization and sparsification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PDDL (AgentBoard)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A strategic game dataset using PDDL expressions requiring complex planning and division of labor across agents; tasks are sensitive to role-specific strategies and long multi-turn coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical cross-trial + inside-trial (retrieval-augmented, trajectory condensation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Same G-Memory three-tier graph structure; retrieves top-k queries (embedding), expands by 1-hop, upward retrieval of insights and downward LLM sparsification of interaction graphs to extract core procedural trajectories, then role-specific memory initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Retrieved memories are evaluated for role relevance (Φ) and inserted into each agent's Mem_i as filtered summaries/snippets prior to MAS reasoning epochs; invoked at the start of solving a query (configurable per-round).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported up to 10.32% improvement over no-memory on PDDL+AutoGen (performance improvement figure reported in cost analysis). Exact absolute numbers vary by LLM backbone and MAS configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (no-memory) used for comparison; exact baseline percentage not always reported explicitly in text for the specific PDDL+AutoGen case within main text, but the stated improvement is 10.32% ↑ relative to no-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Sensitivity analysis: 1-hop expansion outperforms 2-hop/3-hop (2-hop/3-hop often degrade PDDL performance, e.g., PDDL drops to 49.79% for 2-hop in one reported setting). Baselines like Voyager and MemoryBank sometimes degrade AutoGen's PDDL performance (Voyager -4.17%, MemoryBank -1.34% on AutoGen+PDDL), indicating naive single-agent memory transfers are suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Use G-Memory with 1-hop expansion, small k (1–2), both insight and condensed interaction components; role-specific memory is critical for PDDL where division of labor matters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Excessive hop expansion or retrieving many queries (large k) introduces irrelevant insights/noise and reduces performance; some single-agent memory strategies degrade performance in MAS PDDL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Favor concise, role-specific memories (limited k and 1-hop expansion), perform trajectory condensation with an LLM sparsifier to extract the core collaborative steps, and combine insight-level and interaction-level cues to support planning and division-of-labor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Memory(SciWorld+DyLAN/Qwen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Memory hierarchical memory applied to ScienceWorld with DyLAN and various LLM backbones</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>G-Memory supplies hierarchical, role-customized memory (insights + core interaction snippets) to DyLAN debate-style MAS for ScienceWorld text-based embodied science tasks, improving progress rates via distilled collaborative experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DyLAN + G-Memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Debate-style MAS (DyLAN) with early-stopping and agent importance scoring, augmented by G-Memory which retrieves supporting queries, distilled insights, and sparsified interaction trajectories for role-specific initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Same LLM backbones used across experiments; provide capabilities for embedding, scoring relevancy (R_LLM), summarization (J), and sparsification (S_LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based embodied environment for interactive science tasks requiring procedural reasoning, experimentation and object interactions; long trajectories and fine-grained action reasoning are challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical (insight + query + interaction) retrieval-augmented memory with role personalization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Three-tier graphs; coarse retrieval via embedding similarity over Query Graph, 1-hop augmentation, upward insight retrieval via intersection with supporting queries, downward LLM-based sparsification to extract core interaction subgraphs, role-specific memory initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Per-agent Mem_i initialized via Φ(I_S, {Ĝ_inter}, Role_i, Q); memory supplied at start of query solving; flexible invocation possible per round or per agent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported improvements across domains including ScienceWorld; example aggregated improvements reported (G-Memory surpassing baselines on SciWorld across multiple backbones). Exact per-setting numbers are in tables (examples: improvements cited in Table summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baseline used; performance gains reported relative to no-memory but exact pairwise numbers depend on LLM and MAS (tables in appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablation: removing interactions drops average scores by 3.82% for DyLAN; removing insights drops by 3.39% — both modules contribute. ChatDev-M (storing only final artifacts) caused drops in embodied action environments like SciWorld when applied naively.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Combined insight + condensed interaction memory with role-specific tailoring; apply conservative retrieval (1-hop, k in {1,2}) and LLM-based trajectory condensation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Methods that store only final artifacts or naively adopt single-agent memory designs may perform poorly in embodied science tasks; too-large retrieval scopes add noise.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Provide both high-level lessons and compact dialogue trajectories; prioritize role-specific personalization; limit retrieved scope (1-hop) and small k to avoid noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines_applied_to_text_games</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative single-agent and simple MAS memory baselines evaluated on text-game-like benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper evaluates several existing memory methods (Voyager, MemoryBank, Generative Agents, MetaGPT-M, ChatDev-M, MacNet-M) adapted to MAS and tested on ALFWorld, ScienceWorld, PDDL to compare against G-Memory's tailored MAS memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager / MemoryBank / Generative Agents / MetaGPT-M / ChatDev-M / MacNet-M (as adapted baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Various memory designs: Voyager (single-agent continual memory for embodied domains), MemoryBank (selective preserve/forget with decay), Generative Agents (raw observations + reflective memory), MetaGPT-M/ChatDev-M/MacNet-M (MAS frameworks with simplistic memory designs like inside-trial buffers or final artifact storage).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen series and gpt-4o-mini (used across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Various LLM backbones used to instantiate agents and memory components.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, ScienceWorld, PDDL (text-based embodied and planning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See entries above for dataset descriptions; all are text-based environments requiring long interaction histories and role coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>varies (single-agent episodic/reflective, simple inside-trial or final-artifact cross-trial stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Voyager: single-agent evolving artifacts; MemoryBank: importance-weighted temporal decay; Generative Agents: observational + reflective memory; MetaGPT-M: inside-trial memory; ChatDev-M: inside-trial + final-solution cross-trial; MacNet-M: stores final answers only, discards trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Typically retrieval-augmented (similarity-based) or passing previous artifacts as guidance at start of rounds; many are not role-personalized for MAS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mixed: Voyager and Generative Agents often help in single-agent-like settings; but in MAS some baselines degrade performance. Example: Voyager and MemoryBank degraded AutoGen's performance on PDDL by as much as 4.17% and 1.34% respectively. ChatDev-M led to a 2.32% drop when applied to MacNet+SciWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baselines reported; comparisons often show G-Memory outperforming these baselines on average.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparisons show many single-agent memory designs do not transfer well to MAS; storing only final artifacts (ChatDev-M, MacNet-M) provides limited utility in embodied action tasks. G-Memory outperforms these baselines consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Paper concludes that MAS requires role-specific, hierarchical memory (G-Memory) rather than naive single-agent memory transfers or artifact-only cross-trial storage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Single-agent memory designs can harm MAS performance (introducing noise, failing to support division-of-labor), cross-trial artifact-only memories offer limited guidance for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Avoid applying single-agent memory schemes unchanged to MAS; instead build multi-granularity, role-personalized memory with trajectory condensation and limited retrieval scope.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFWorld <em>(Rating: 2)</em></li>
                <li>Voyager: An Open-Ended Embodied Agent with Large Language Models <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Expel: LLM agents are experiential learners <em>(Rating: 2)</em></li>
                <li>A-Mem: Agentic memory for LLM agents <em>(Rating: 2)</em></li>
                <li>Synapse: Trajectory-as-exemplar prompting with memory for computer control <em>(Rating: 1)</em></li>
                <li>Generative Agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
                <li>ChatDev <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8227",
    "paper_id": "paper-279250852",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "G-Memory(ALFWorld+MacNet+Qwen-2.5-14b)",
            "name_full": "G-Memory hierarchical agentic memory applied to ALFWorld with MacNet using Qwen-2.5-14b",
            "brief_description": "G-Memory is a three-tier hierarchical memory (insight, query, interaction graphs) that retrieves role-specific, multi-granularity memory (high-level insights and condensed interaction subgraphs) to augment multi-agent teams solving ALFWorld text-based embodied tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Multi-agent system (MacNet) + G-Memory",
            "agent_description": "A decentralized multi-agent orchestration (MacNet) with edge agents; agents have roles and receive role-customized memory cues from G-Memory (insights + condensed interaction fragments) before execution.",
            "llm_model_name": "Qwen-2.5-14b",
            "llm_model_description": "Open-source Qwen series LLM (2.5 family, 14B parameters) used as the backbone LLM for agents in experiments.",
            "benchmark_name": "ALFWorld",
            "benchmark_description": "A text-based embodied environment of household tasks where agents navigate and manipulate objects via natural-language actions; challenges include long multi-turn interactions, object state checks, and procedural correctness.",
            "memory_used": true,
            "memory_type": "hierarchical cross-trial + inside-trial (retrieval-augmented, role-personalized)",
            "memory_architecture": "Three-tier graph: Interaction Graph (utterance nodes, temporal edges) stores fine-grained dialogue trajectories; Query Graph stores query nodes with metadata and links; Insight Graph stores distilled, generalizable insights linked to supporting queries. Memory is updated after each episode (interaction trace stored, new query node created, new insight distilled and connected).",
            "memory_integration_strategy": "Coarse similarity retrieval over query graph (embedding cosine), 1-hop graph expansion, bi-directional traversal: upward (query→insight) via query-to-insight projector to get I_S, downward (query→interaction) via an LLM-based graph sparsifier to extract core interaction subgraphs; then per-agent initialization Mem_i = Φ(I_S, {Ĝ_inter}, Role_i, Q) which filters/summarizes memories into role-specific cues concatenated or provided as context to each agent before MAS execution.",
            "performance_with_memory": "79.10% success rate (ALFWorld) for MacNet + G-Memory with Qwen-2.5-14b",
            "performance_without_memory": "58.21% success rate (ALFWorld) for MacNet without G-Memory (no-memory baseline)",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to multiple baselines (Voyager, MemoryBank, Generative Agents, MetaGPT-M, ChatDev-M, MacNet-M) G-Memory produced large gains; specific reported comparison: with Qwen-2.5-14b G-Memory boosted MacNet ALFWorld from 58.21%→79.10% (+20.89%). Ablation shows removing interaction condensation or high-level insights each reduces performance; interactions removal caused larger drops than insights-only.",
            "best_memory_strategy": "Hierarchical bi-directional memory (insight + condensed interaction subgraphs) with role-specific tailoring and limited retrieval (1-hop expansion, k in {1,2}) is recommended.",
            "limitations_or_failure_cases": "No catastrophic failure cases reported specifically for ALFWorld; general limitations include possible irrelevant insights from excessive hop expansion and noisy retrieval when k is too large. The paper notes that naive feeding of full long-context trajectories is ineffective and can harm performance.",
            "recommendations_or_conclusions": "Use G-Memory's three-tier structure with 1-hop expansion and k ∈ {1,2}; perform LLM-based sparsification to condense interaction trajectories; provide role-specific distilled memories to agents (Φ operator); combine both high-level insights and fine-grained interaction snippets because both contribute (interactions slightly more impactful). Avoid naive full-context concatenation of long-scale MAS dialogues.",
            "uuid": "e8227.0",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "G-Memory(PDDL+AutoGen)",
            "name_full": "G-Memory hierarchical memory applied to PDDL strategic games with AutoGen",
            "brief_description": "G-Memory provides query-level retrieval and interaction-trajectory condensation to AutoGen multi-agent teams solving PDDL strategic planning tasks, enabling improved division of labor via role-specific memory cues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AutoGen + G-Memory",
            "agent_description": "AutoGen orchestration (Solver, Ground Truth, Executor or similar roles) augmented with G-Memory which supplies retrieved insights and condensed historical trajectories tailored to each role.",
            "llm_model_name": "Qwen-2.5-(7b/14b) and gpt-4o-mini tested across experiments",
            "llm_model_description": "Experiments used Qwen-2.5-7b, Qwen-2.5-14b and proprietary gpt-4o-mini as LLM backbones for agents; these serve as the base models generating utterances and performing memory summarization and sparsification tasks.",
            "benchmark_name": "PDDL (AgentBoard)",
            "benchmark_description": "A strategic game dataset using PDDL expressions requiring complex planning and division of labor across agents; tasks are sensitive to role-specific strategies and long multi-turn coordination.",
            "memory_used": true,
            "memory_type": "hierarchical cross-trial + inside-trial (retrieval-augmented, trajectory condensation)",
            "memory_architecture": "Same G-Memory three-tier graph structure; retrieves top-k queries (embedding), expands by 1-hop, upward retrieval of insights and downward LLM sparsification of interaction graphs to extract core procedural trajectories, then role-specific memory initialization.",
            "memory_integration_strategy": "Retrieved memories are evaluated for role relevance (Φ) and inserted into each agent's Mem_i as filtered summaries/snippets prior to MAS reasoning epochs; invoked at the start of solving a query (configurable per-round).",
            "performance_with_memory": "Reported up to 10.32% improvement over no-memory on PDDL+AutoGen (performance improvement figure reported in cost analysis). Exact absolute numbers vary by LLM backbone and MAS configuration.",
            "performance_without_memory": "Baseline (no-memory) used for comparison; exact baseline percentage not always reported explicitly in text for the specific PDDL+AutoGen case within main text, but the stated improvement is 10.32% ↑ relative to no-memory.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Sensitivity analysis: 1-hop expansion outperforms 2-hop/3-hop (2-hop/3-hop often degrade PDDL performance, e.g., PDDL drops to 49.79% for 2-hop in one reported setting). Baselines like Voyager and MemoryBank sometimes degrade AutoGen's PDDL performance (Voyager -4.17%, MemoryBank -1.34% on AutoGen+PDDL), indicating naive single-agent memory transfers are suboptimal.",
            "best_memory_strategy": "Use G-Memory with 1-hop expansion, small k (1–2), both insight and condensed interaction components; role-specific memory is critical for PDDL where division of labor matters.",
            "limitations_or_failure_cases": "Excessive hop expansion or retrieving many queries (large k) introduces irrelevant insights/noise and reduces performance; some single-agent memory strategies degrade performance in MAS PDDL tasks.",
            "recommendations_or_conclusions": "Favor concise, role-specific memories (limited k and 1-hop expansion), perform trajectory condensation with an LLM sparsifier to extract the core collaborative steps, and combine insight-level and interaction-level cues to support planning and division-of-labor.",
            "uuid": "e8227.1",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "G-Memory(SciWorld+DyLAN/Qwen)",
            "name_full": "G-Memory hierarchical memory applied to ScienceWorld with DyLAN and various LLM backbones",
            "brief_description": "G-Memory supplies hierarchical, role-customized memory (insights + core interaction snippets) to DyLAN debate-style MAS for ScienceWorld text-based embodied science tasks, improving progress rates via distilled collaborative experience.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DyLAN + G-Memory",
            "agent_description": "Debate-style MAS (DyLAN) with early-stopping and agent importance scoring, augmented by G-Memory which retrieves supporting queries, distilled insights, and sparsified interaction trajectories for role-specific initialization.",
            "llm_model_name": "Qwen-2.5-7b, Qwen-2.5-14b, gpt-4o-mini (used across experiments)",
            "llm_model_description": "Same LLM backbones used across experiments; provide capabilities for embedding, scoring relevancy (R_LLM), summarization (J), and sparsification (S_LLM).",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Text-based embodied environment for interactive science tasks requiring procedural reasoning, experimentation and object interactions; long trajectories and fine-grained action reasoning are challenging.",
            "memory_used": true,
            "memory_type": "hierarchical (insight + query + interaction) retrieval-augmented memory with role personalization",
            "memory_architecture": "Three-tier graphs; coarse retrieval via embedding similarity over Query Graph, 1-hop augmentation, upward insight retrieval via intersection with supporting queries, downward LLM-based sparsification to extract core interaction subgraphs, role-specific memory initialization.",
            "memory_integration_strategy": "Per-agent Mem_i initialized via Φ(I_S, {Ĝ_inter}, Role_i, Q); memory supplied at start of query solving; flexible invocation possible per round or per agent.",
            "performance_with_memory": "Reported improvements across domains including ScienceWorld; example aggregated improvements reported (G-Memory surpassing baselines on SciWorld across multiple backbones). Exact per-setting numbers are in tables (examples: improvements cited in Table summaries).",
            "performance_without_memory": "No-memory baseline used; performance gains reported relative to no-memory but exact pairwise numbers depend on LLM and MAS (tables in appendices).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablation: removing interactions drops average scores by 3.82% for DyLAN; removing insights drops by 3.39% — both modules contribute. ChatDev-M (storing only final artifacts) caused drops in embodied action environments like SciWorld when applied naively.",
            "best_memory_strategy": "Combined insight + condensed interaction memory with role-specific tailoring; apply conservative retrieval (1-hop, k in {1,2}) and LLM-based trajectory condensation.",
            "limitations_or_failure_cases": "Methods that store only final artifacts or naively adopt single-agent memory designs may perform poorly in embodied science tasks; too-large retrieval scopes add noise.",
            "recommendations_or_conclusions": "Provide both high-level lessons and compact dialogue trajectories; prioritize role-specific personalization; limit retrieved scope (1-hop) and small k to avoid noise.",
            "uuid": "e8227.2",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Baselines_applied_to_text_games",
            "name_full": "Representative single-agent and simple MAS memory baselines evaluated on text-game-like benchmarks",
            "brief_description": "Paper evaluates several existing memory methods (Voyager, MemoryBank, Generative Agents, MetaGPT-M, ChatDev-M, MacNet-M) adapted to MAS and tested on ALFWorld, ScienceWorld, PDDL to compare against G-Memory's tailored MAS memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Voyager / MemoryBank / Generative Agents / MetaGPT-M / ChatDev-M / MacNet-M (as adapted baselines)",
            "agent_description": "Various memory designs: Voyager (single-agent continual memory for embodied domains), MemoryBank (selective preserve/forget with decay), Generative Agents (raw observations + reflective memory), MetaGPT-M/ChatDev-M/MacNet-M (MAS frameworks with simplistic memory designs like inside-trial buffers or final artifact storage).",
            "llm_model_name": "Qwen series and gpt-4o-mini (used across experiments)",
            "llm_model_description": "Various LLM backbones used to instantiate agents and memory components.",
            "benchmark_name": "ALFWorld, ScienceWorld, PDDL (text-based embodied and planning benchmarks)",
            "benchmark_description": "See entries above for dataset descriptions; all are text-based environments requiring long interaction histories and role coordination.",
            "memory_used": true,
            "memory_type": "varies (single-agent episodic/reflective, simple inside-trial or final-artifact cross-trial stores)",
            "memory_architecture": "Voyager: single-agent evolving artifacts; MemoryBank: importance-weighted temporal decay; Generative Agents: observational + reflective memory; MetaGPT-M: inside-trial memory; ChatDev-M: inside-trial + final-solution cross-trial; MacNet-M: stores final answers only, discards trajectories.",
            "memory_integration_strategy": "Typically retrieval-augmented (similarity-based) or passing previous artifacts as guidance at start of rounds; many are not role-personalized for MAS.",
            "performance_with_memory": "Mixed: Voyager and Generative Agents often help in single-agent-like settings; but in MAS some baselines degrade performance. Example: Voyager and MemoryBank degraded AutoGen's performance on PDDL by as much as 4.17% and 1.34% respectively. ChatDev-M led to a 2.32% drop when applied to MacNet+SciWorld.",
            "performance_without_memory": "No-memory baselines reported; comparisons often show G-Memory outperforming these baselines on average.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparisons show many single-agent memory designs do not transfer well to MAS; storing only final artifacts (ChatDev-M, MacNet-M) provides limited utility in embodied action tasks. G-Memory outperforms these baselines consistently.",
            "best_memory_strategy": "Paper concludes that MAS requires role-specific, hierarchical memory (G-Memory) rather than naive single-agent memory transfers or artifact-only cross-trial storage.",
            "limitations_or_failure_cases": "Single-agent memory designs can harm MAS performance (introducing noise, failing to support division-of-labor), cross-trial artifact-only memories offer limited guidance for embodied tasks.",
            "recommendations_or_conclusions": "Avoid applying single-agent memory schemes unchanged to MAS; instead build multi-granularity, role-personalized memory with trajectory condensation and limited retrieval scope.",
            "uuid": "e8227.3",
            "source_info": {
                "paper_title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFWorld",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "MemoryBank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Expel: LLM agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "A-Mem: Agentic memory for LLM agents",
            "rating": 2,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "rating": 1,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "Generative Agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "ChatDev",
            "rating": 1
        }
    ],
    "cost": 0.015269999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems
16 Jun 2025</p>
<p>Guibin Zhang 
Muxin Fu 
Tongji University</p>
<p>Guancheng Wan 
UCLA
4 A*STAR, 5 NTU</p>
<p>Miao Yu 
Kun Wang wang.kun@ntu.edu.sg 
Shuicheng Yan 
Nus 
G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems
16 Jun 20255BB9D4C02945ABC5F6AF24A99EF5C7D5arXiv:2506.07398v2[cs.MA]
Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures.Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack crosstrial and agent-specific customization, in stark contrast to the expressive memory developed for single agents.To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory[1], which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs.Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both high-level, generalizable insights that enable the system to leverage cross-trial knowledge, and fine-grained, condensed interaction trajectories that compactly encode prior collaboration experiences.Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams.Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to 20.89% and 10.12%, respectively, without any modifications to the original frameworks.Our codes are available at https://github.com/bingreeky/GMemory.</p>
<p>Introduction</p>
<p>As Large Language Models (LLMs) continue to redefine the frontier of artificial intelligence, LLMdriven agents have exhibited unprecedented prowess in perception [2,3,4,5], planning [6,7,8], reasoning [9,10], and action [11,12], which have catalyzed remarkable progress across diverse downstream domains, including code generation [13,14], data analysis [15], embodied tasks [16] and autonomous driving [3,17,18].Building upon the impressive competencies of single agents, LLMbased Multi-Agent Systems (MAS) have been demonstrated to push the boundaries of single model capacity [19,20,21].Similar to collective intelligence arising from human social collaboration [22,23,24], MAS orchestrates multiple agents [25,26,27], whether through cooperation [28,29,30,31] or competition [32,33,34], to transcend the cognitive and specialized limitations of solitary agents.</p>
<p>Self-Evolving Agents.What especially characterizes LLM agents is their self-evolving capacity, i.e., the ability to continuously adapt and improve through interactions with the environment, as seen in prior works where such adaptability has led to two-to three-fold quantitative improvements [35].The central driving force behind such self-evolving nature is memory mechanism of agents [36,37,38], which parallels human abilities to accumulate knowledge, process past experiences, and</p>
<p>Insight Graph</p>
<p>Query Graph Interaction Graph</p>
<p>Verify the state of objects before and after action...</p>
<p>Insight</p>
<p>Query Check whether there is an apple in the garden retrieve relevant information.Previous successful memory mechanism designs, including both inside-trial memory (i.e., context retained within solving one single query) and cross-trial memory (i.e., experience accumulated across multiple tasks) [39], have empowered agents to excel in diverse applications such as personalized chat [36,40,41], recommendation [42], embodied action [43,16], and social simulation [19,44,45], enabling them to evolve into experiential learners that effectively leverage past experiences and world knowledge.</p>
<p>Status</p>
<p>Self-Evolving MAS.However, such self-evolving capacity remains largely absent in multi-agent systems.Most existing MAS are still constrained by manually defined workflows, such as the Standard Operating Procedures (SOP) in MetaGPT [21] and ChatDev [46], or rely on pre-defined communication topologies in MacNet [47] and AgentPrune [30].More recent automated MASs, such as GPTSwarm [48], ADAS [49], AFlow [50], and MaAS [51] have made it to automatically optimize inter-agent topologies or prompts, which, nevertheless, ultimately yield giant and cumbersome MAS architectures, lacking the agility to self-adjust with accumulated collaboration experience.</p>
<p>Memory for MAS.The absence of the aforementioned self-evolving capacity is, in fact, rooted in the lack of memory mechanisms specifically tailored for MAS.One may challenge this claim from two perspectives: ❶ Do existing MASs lack memory mechanisms altogether?Not entirely.Classical MAS frameworks such as MetaGPT, ChatDev, and Exchange-of-Thought [52] incorporate memory-related designs.However, these are often limited to inside-trial memory [52], while cross-trial memory, if present, remains rudimentary-typically involving the transmission of overly condensed artifacts (e.g., final solutions or execution results) [21,46,47], and failing to enable meaningful learning from collaborative experience.❷ Why not directly transfer existing single-agent memory mechanisms to MAS? Unfortunately, such a transfer is far from straightforward.The inherent nature of MAS, i.e., multi-turn orchestration across multiple agents [26,27], leads to substantially longer task-solving trajectories compared to single-agent settings (up to 10× more tokens, as demonstrated by Figure 1 (Left)).This poses a significant challenge to traditional retrieval-based memory designs [36,37,16], as naive feeding of the entire long-context trajectory without proper abstraction from a collaborative perspective offers little benefit.Given the aforementioned challenges, a natural question arises:</p>
<p>How can we design a memory mechanism capable of storing, retrieving, and managing the lengthy interaction history of multi-agent systems, such that agent teams can benefit from concise and instructive experience and insights?</p>
<p>The Present Work: G-Memory.In response to the above question, we introduce a :</p>
<p>Graph-based Agentic ::::::: Memory Mechanism for LLM-based Multi-Agent Systems, dubbed G-Memory, which manages the complex and lengthy interaction history of MAS through a three-tier hierarchical graph structure:</p>
<p>✱ Insight Graph, which abstracts generalizable insights from historical experience; ✱ Query Graph, which encodes meta-information of task queries and their connectivity; ✱ Interaction Graph, which stores fine-grained textual communication logs among agents.</p>
<p>Figure 1 (Right) visualizes these structures, and their formal definitions are placed in Section 3. When a new query arrives, G-Memory efficiently retrieves relevant query records by leveraging the topology of the query graph, and then traverses upward (i.e., query→insight graph) to extract associated highlevel insights and downward (i.e., query→interaction graph) to identify core interaction subgraphs that are most pertinent to the task at hand, thereby mitigating information overload.Based on the retrieved memory, G-Memory offers actionable guidance to the MAS, e.g., division of labor, task decomposition, and lessons from past failures.Upon the completion of a task, all three levels of the memory hierarchy are updated in an agentic manner, with newly distilled insights, enriched query records, detailed MAS trajectories, and their level of detailed associations.Through this refinement, G-Memory functions as a plug-and-play module that can be seamlessly embedded into mainstream MAS frameworks, empowering evolving inter-agent collaboration and collective intelligence.</p>
<p>Our contributions are summarized as follows:</p>
<p>❶ Bottleneck Identification.We conduct a thorough review of existing multi-agent systems and identify a fundamental bottleneck in their self-evolving capabilities, which is largely attributed to the oversimplified memory architectures.❷ Practical Solution.We propose G-Memory, a hierarchical agentic memory architecture for MAS, which models complex and prolonged inter-agent collaboration through a three-tier structure comprising insight, query, and interaction graphs.❸ Experimental Evaluation.Extensive experiments across five benchmarks show that G-Memory is (I) high-performing, improving state-of-the-art MAS by up to 20.89% and 10.12% on embodied action and knowledge QA tasks, respectively; and (II) resource-friendly, maintaining comparable or even lower token usage than mainstream memory designs.</p>
<p>Related Works</p>
<p>Single-Agent Memory.Memory serves as a primary driving force for agents to accumulate experiences and explore the world through interactions with the environment [53,54,55,56].It plays a critical role in both task-solving and social simulation LLM agents, and this work primarily focuses on the former.Early research on agent memory was confined to simple inside-trial memory, mainly addressing limitations posed by the LLM context window in chatbot applications, including MemoryBank [36], ChatDB [40], MemoChat [41], and MemGPT [37], which typically adopt retrievalaugmented generation (RAG)-style, similarity-based chunk retrieval.Subsequent developments have progressed toward more cognitively inspired memory architectures, including (1) memory scope extended to cross-trial memory like ExpeL [43] and Synapse [57]; (2) application domains broadened to include computer control [57], embodied action [58], scientific discovery [59], coding and reasoning [60]; and (3) management techniques evolved from coarse-grained textual similarity toward more sophisticated abstraction and summarization of acquired knowledge and experiences [19], as seen in A-Mem [61], Mem0 [62] and MemInsight [63].More discussions are in Appendix D.</p>
<p>Memory in Multi-agent System.However, the memory mechanisms tailored for MAS remain markedly underexplored.Some representative frameworks, such as LLM-Debate [20,33] and Mixture-of-Agent [64], omit memory components altogether.Others merely adopt simplistic insidetrial memory schemes [47,52].Even in frameworks that attempt cross-trial memory [46], the memory is merely compressed as the final outcome artifacts, overlooking the nuanced agent interactions.</p>
<p>Collectively, there is a pressing need for a principled memory architecture that can capture, organize, and retrieve the inherently intricate task-solving processes unique to MAS [39].</p>
<p>LLM-based Multi-Agent Systems.Our work focuses on task-solving MAS, which, unlike their single-agent counterparts, often lack the capacity for continual evolution through interaction with the environment [65,66].Early frameworks such as AutoGen [13], CAMEL [24], and AgentVerse [67] rely entirely on pre-defined workflows.More recent efforts [68,69,50,49,70,31] introduce a degree of adaptivity by generating dynamic MAS in response to environmental feedback.However, such evolution is often one-shot: for example, AFlow [50] employs Monte Carlo Tree Search to construct a complex MAS tailored to a specific task domain, which yet lacks the capacity to evolve with increasing task exposure or transfer across domains [51,71].From this perspective, constructing MAS with genuine self-evolving capabilities remains an open and challenging research frontier.</p>
<p>Preliminary</p>
<p>In this section, we establish the notation and formalize key concepts of multi-agent systems and G-Memory's hierarchical memory architecture.</p>
<p>Multi-agent System Formalization.Consider a multi-agent framework represented by a directed graph G = (V, E), where |V| = N is the number of agents and E ⊆ V×V defines their communication channels.Each node C i ∈ V corresponds to an individual agent described by the quadruple:
C i = (Base i , Role i , Mem i , Plugin i ),(1)
where Base i denotes the underlying large language model instance, Role i specifies the agent's designated role or persona, Mem i encapsulates its memory state, including past interactions or external knowledge stores, and Plugin i is the set of auxiliary tools (e.g., web-search engine).</p>
<p>Upon receiving a user query Q, the system evolves through T synchronous communication epochs.</p>
<p>At each epoch t, we derive a topological ordering π = [π 1 , . . ., π N ] of the nodes such that if there is an edge from π j to π k , then j &lt; k, which guarantees that every agent processes its inputs only after all its predecessors have acted.For each agent C i in π, its output at iteration t is computed as:
r (t) i = C i P (t) sys , Q, {r(t)j : C j ∈ N − (C i )} ,
where: r
(t)
i denotes the response generated by C i (which may include reasoning steps, intermediate analyses, or final proposals),
P (t) sys comprises global instructions (including each agent's R i ), N − (C i )
is the set of in-neighbors of C i , whose outputs serve as contextual inputs.After all agents have acted, a global aggregation operator A fuses the collection of responses into an interim solution a (t) :
a (t) = A(r (t) 1 , . . . , r (t) N ).
Common implementations for A include majority voting schemes [48], hierarchical summarization via dedicated aggregator agents [13,30], or simply adopting the final agent's output as the answer [47].These epochs iterate for t = {1, . . ., T } until either a preset limit is reached or an early-stopping criterion is met [72], producing the final response a (T ) to the query Q.</p>
<p>Memory Architecture.Our proposed G-Memory orchestrates and manages the memory of multiagent systems via the following three hierarchical graph structures:
[✱] Interaction Graph (Utterance Graph). For query Q, let G (Q) inter = (U (Q) , E (Q)
u ) denote its interaction trajectory, where (i) nodes U (Q) = {u i } represent atomic utterances, with each u i ≜ (A i , m i ) containing A i ∈ V (speaking agent), and m i (textual content), (ii) Edges
E (Q) u ⊆ U (Q) × U (Q) follow temporal relationships: (u j , u k ) ∈ E (Q) u
⇐⇒ u j is transmitted to and inspires u k .</p>
<p>[✱] Query Graph.The query graph, storing previously tackled queries and metadata, is as follows:
G query = (Q, E q ) = Q i , Ψ i , G (Qi) inter |Q| i=1 , E q ,(2)
where Q = {q i } is the node set, node
q i ≜ (Q i , Ψ i , G(Qi)
inter ) is composed of the original query Q i , task status Ψ i ∈ {Failed, Resolved}, and its associated interaction graph G (Qi) inter .The edges E q ⊆ Q × Q encode semantic relationships between queries.The query graph enables retrieval beyond coarse metrics such as embedding similarity, with its meticulous topology.</p>
<p>[✱] Insight Graph.The highest-level insight graph is featured as follows:
G insight = (I, E i ) = ⟨κ k , Ω k ι k ⟩ |I| k=1 , E i ,(3)
where the node set I = {ι k } represents distilled insights, each node ι k is composed of the insight content κ k and the set of supporting queries Ω k ⊆ Q.The edges E i ⊆ I × I × Q forming hyper-connections where (ι m , ι n , q j ) indicates insight ι m contextualizes ι n through query q j .</p>
<p>G-Memory</p>
<p>This section outlines the management workflow of G-Memory, as illustrated in Figure 2. Specifically, upon the arrival of a new query Q, G-Memory first conducts coarse-grained retrieval to identify pertinent trajectory records (▷ Section 4.1).It then performs bi-directional hierarchical memory traversal: upward to retrieve collective cognitive insights, and downward to distill concrete procedural trajectories (▷ Section 4.2).After the memory-augmented MAS completes the query execution, the hierarchical memory architecture is jointly updated based on environmental feedback, thereby achieving the institutionalization of group knowledge (▷ Section 4.3).</p>
<p>Query</p>
<p>Similaritybased Retrival</p>
<p>1.Your task is to find a butterfly egg in the outside.Move it to the green box in the bathroom.</p>
<p>Core Path Extraction</p>
<p>Are both Lygodium or Maxillaria a genus of orchids?</p>
<p>Topic: Diffculty:</p>
<p>Web search</p>
<p>Trajectory Condensation</p>
<p>Collab.Experience</p>
<p>This history follows a chainstyle, collaboration strategy...</p>
<p>Failure Lessons</p>
<p>Take care when summarizing the final result, DO NOT ...</p>
<p>Distilled Insights</p>
<p>Insight 1: Clearly identify key entities and their roles, use specific names and titles Insight 2: Doublecheck the relevance of the search results</p>
<p>from from and</p>
<p>Ensure the search terms are precise and directly related to the specific entity or institution ...</p>
<p>Memory Augmentation Output Solution Environment Feedback</p>
<p>Execution: Success Token cost: 3,345 Tool calls: 3 ...</p>
<p>Update Interaction Update Insights</p>
<p>Since both are confirmed as film directors from their respective countries, the answer is Yes.</p>
<p>Interaction Graphs</p>
<p>Query Graph</p>
<p>Interactions CEO agent: assigning tasks... Thinker agent: OK, I will... Executor agent: ...</p>
<p>Insight Graph</p>
<p>Memory Augmentation</p>
<p>Black font: Notations</p>
<p>Symbols</p>
<p>Coarse-grained Memory Retrieval</p>
<p>As a plug-in designed for seamless integration into mainstream MAS, G-Memory is triggered when the MAS G encounters a new user query Q.As emphasized in organizational memory theory [1], efficient knowledge retrieval typically begins with broadly relevant schemas prior to more fine-grained access.Following this principle, G-Memory first performs a coarse-grained similarity-based retrieval over the query graph G query to efficiently obtain a sketched set of queries Q S :
Q S = arg top-k qi∈Q s.t. |Q S |=k v(Q) • v(q i ) |v(Q)| |v(q i )| ,(4)
where v(•) maps queries into fixed-length embeddings using models such as MiniLM [73].While Equation (4) retrieves semantically similar historical queries, the similarity may be only superficial or noisy.Therefore, G-Memory further enlarges the relevant set via hop expansion on the query graph:
QS = Q S ∪ Q k ∈ Q | ∃Q j ∈ Q S , Q k ∈ N + (Q j ) ∪ N − (Q j ) ,(5)
where QS is augmented with the 1-hop neighbors of Q S on the query graph G query .However, it is suboptimal to directly feed these relevant records as input akin to certain single-agent memory systems [41,37].On one hand, the excessive context length may overwhelm the LLM; on the other hand, agents in MAS play distinct roles and should be assigned specialized memory tailored to their functions.To address this, the next section introduces a bi-directional processing scheme in G-Memory that operates over both abstract and fine-grained memory levels.</p>
<p>Bi-directional Memory Traversal</p>
<p>Subsequent to identifying the expanded set of relevant query nodes QS within G query , G-Memory executes a bi-directional memory traversal to furnish multi-granularity memory support.Specifically, G-Memory first performs an upward traversal (G query → G insight ), retrieving insight nodes that may provide high-level guidance for the current task:
I S = Π Q→I ( QS ), Π Q→I (S q ) ≜ {ι k ∈ I | Ω k ∩ S q ̸ = ∅} ,(6)
where Π Q→I is a query-to-insight projector that identifies all the insight nodes whose supporting query sets intersect with the input query set, and the retrieved insights I S encapsulate distilled, generalized knowledge potentially relevant for orienting the MAS G's strategic approach to Q.</p>
<p>Beyond generalized insights, the fine-grained textual interaction history of the MAS is equally valuable, as it reveals the underlying reasoning patterns that led to successful or failed collaborations [68,74,75].To utilize these concisely, in the downward traversal (G query → G interaction ), G-Memory employs an LLM-facilitated graph sparsifier S LLM (•, •) to extract the core subgraph that encapsulates essential inter-agent collaboration:
{ ĜQi inter } |M | i=1 = S LLM (G (Qj ) inter , Q) | q j ∈ argtop-M {q ′ k ∈ QS } s.t. |•|=M R LLM (Q, q ′ k ) ,(7)
where R LLM (Q, q j ) rates the relevancy of historical queries w.r.t.Q, and the sparsifier S LLM (G
(Qj ) inter , Q) constructs a sparsified graph Ĝ(Qj) inter = ( Û(Qj) , Ê(Qj) u ) from the original G (Qj )
inter by identifying and retaining dialogue elements.Please refer to Appendix C for their implementations.</p>
<p>Upon completing the bi-directional traversal, we obtain both generalizable insights (I S ) and detailed collaborative trajectories
({ ĜQi inter } |M | i=1
). G-Memory then proceeds to provide specialized memory support for each agent C ∈ V within the MAS G.
Mem i ← Φ I S , { ĜQi inter } |M | i=1 ; Role i , Q , ∀C i = (Base i , Role i , Mem i , Plugin i ) ∈ V,(8)
where the operator Φ(•; •) evaluates the utility and relevance of each insight ι k ∈ I S and sparsified interaction graph Ĝ(Qj) inter concerning the agent's specific role Role i and the task Q (see Appendix C).Based on this evaluation, Φ intializes each agent's internal memory state Mem i with filtered insights, interaction snippets, summaries thereof, equipping it with pertinent historical context before it participates in the subsequent reasoning epochs of the MAS.It is worth noting that G-Memory is invoked at the onset of solving query Q in our implementation.However, practitioners may flexibly configure more fine-grained invocation strategies, such as at the beginning of each MAS dialogue round or selectively for specific agents, based on their needs.</p>
<p>Hierarchy Memory Update</p>
<p>After completing memory augmentation for each agent, the system G is executed as outlined in Section 3, yielding a final solution a (T ) and receiving environmental feedback, including execution status Ψ i ∈ {Failed, Resolved}, token usage, and other performance metrics.Subsequently, G-Memory updates its hierarchical memory architecture to incorporate this new query.At the interaction level, G-Memory traces each agent's utterances to construct the interaction graph G (Q) inter , which is then stored.At the query level, a new query node is instantiated and added to the query graph Q query :
q new ← (Q, Ψ, G (Q) inter ), N conn ← Q R ∪ ι k ∈I S Ω k , E new ← {(q n , q new ) | q n ∈ N conn }, G next query ← (Q ∪ {q new }, E q ∪ E new ),(9)
where edges are established between q new and (ii) the set Q R containing the top-M relevant historical queries identified in Equation (7), and (ii) the set of queries ι k ∈Iret Ω k that support the insights I S utilized for solving Q. G next query denotes the updated query graph.Finally, at the insight level, G-Memory integrates the learning from the completed query Q into the insight graph G insight = (I, E i ).First, possible new insights summarizing the experience are generated and structurally linked via a summarization function J (•, •) (see prompt in Appendix C) as follows:
ι new = (J (G (Q) inter , Ψ), {q new }), E i, new ← {(ι k , ι new , q new ) | ι k ∈ I S } G ′ insight ← (I ∪ {ι new }, E i ∪ E i, new )(10)
where edges are added to connect the previously utilized insights which inspires the completion of Q in Equation (6).Afterward, the supporting query sets (Ω k ) for the utilized insights (I S ) are updated to include q new , reflecting their relevance to this successful (or failed) application:
I next ← (I \ I ret ) ∪ {(κ k , Ω k ∪ {q new }) | ι k = (κ k , Ω k ) ∈ I ret } ∪ {ι new } G next insight ← (I next , E i ∪ E i, new ),(11)
where the final node set I next incorporates the new insight and the updated versions of the utilized insights, and the resulting graph G next insight thus encapsulates the integrated knowledge.This continuous update cycle across all hierarchical levels enables G-Memory to learn and adaptively refine its collective memory based on ongoing experience.</p>
<p>Experiment</p>
<p>In this section, we conduct extensive experiments to answer: (RQ1) How does G-Memory perform compared to existing single/multi-agent memory architectures?(RQ2) Does G-Memory incur excessive resource overhead?(RQ3) How sensitive is G-Memory to its key components and parameters?</p>
<p>Experiment Setup</p>
<p>Datasets and Benchmarks.To thoroughly evaluate the effectiveness of G-Memory, we adopt five widely-adopted benchmarks across three domains: (1) Knowledge reasoning, including Hot-potQA [76] and FEVER [77];</p>
<p>(2) Embodied action, including ALFWorld [78] and SciWorld [79];</p>
<p>(3) Game, namely PDDL [80].Details on these benchmarks are in Appendix A.1.</p>
<p>Baselines.We select four representative single-agent memory baselines, including non-memory, Voyager [16], MemoryBank [36], and Generative Agents [19], as well as three multi-agent memory implementations from MetaGPT [21], ChatDev [46], and MacNet [47], denoted as MetaGPT-M, ChatDev-M, and MacNet-M, respectively.Details are in Appendix A.2. MAS and LLM Backbones.We select three representative multi-agent frameworks to integrate with G-Memory and the baselines, including AutoGen [13], DyLAN [72], and MacNet [47].More details on the MAS setups are placed in Appendix A.3.For instantiating these MAS frameworks, we adopt two open-source LLMs, Qwen-2.5-7b and Qwen-2.5-14b,as well as one proprietary LLM, gpt-4o-mini.The deployment of Qwen series is via local instantiation using Ollama1 , and GPT models are accessed via OpenAI APIs.Parameter Configurations.We implement the embedding function v(•) in Equation ( 4) with ALL-MINILM-L6-V2 [81].The number of the most relevant interaction graphs M in Equation ( 7) is set among {2, 3, 4, 5}, and the number of relevant queries k in Equation ( 4) is set among {1, 2}.The detailed ablation study on hyper-parameters is placed in Section 5.4.</p>
<p>Main Results (RQ1)</p>
<p>Tables 1, 2 and 3 comprehensively report the performance of different memory architectures across three LLM backbones and three MAS frameworks.We summarize the key observations as follows: Takeaway ➊: G-Memory consistently improves performance across all task domains and MAS frameworks.As shown in Table 2, when integrated with AutoGen and MacNet (powered by Qwen-2.5-7b),G-Memory surpasses the best-performing single-/multi-agent memory baselines by an average of 6.8% and 5.5%, respectively.With the more capable Qwen-2.5-14b, the improvement is even more pronounced: in Table 3, G-Memory boosts MacNet's performance on ALFWorld from 58.21% to 79.10%, achieving a substantial 20.89% gain.</p>
<p>Takeaway ➋: Multi-agent systems demand specialized memory designs.A thorough examination of existing baselines reveals a surprising insight: most memory mechanisms fail to consistently benefit MAS settings.In Table 2, baselines such as Voyager and MemoryBank degrade AutoGen's performance on PDDL by as much as 4.17% and 1.34%, respectively.We attribute this to the inability of these methods to provide agent role-specific memory support, which is essential in the PDDL strategic game tasks, where effective division of labor is critical to success.Even MAS-oriented designs, such as ChatDev-M, result in a 2.32% performance drop when applied to MacNet+SciWorld.We attribute this to ChatDev-M's narrow memory scope-storing only the execution results of past queries, which provides limited utility in embodied action environments.These findings highlight the necessity of G-Memory's core characteristics: role-specific memory cues, abstracted high-level insights, and trajectory condensation-all of which are critical for effective memory in MAS.</p>
<p>Cost Analysis (RQ2)</p>
<p>To evaluate the efficiency of G-Memory in terms of token consumption, we visualize the performance versus token cost trade-off across various settings, as shown in Figures 3 and 7. Our findings are: Takeaway ➌: G-Memory achieves high-performing collective memory without excessive token consumption.As depicted in Figure 3, G-Memory consistently delivers the highest performance improvement (10.32% ↑ over no-memory setting on PDDL+AutoGen) while maintaining a modest increase in token consumption (only 1.4 × 10 6 ).In contrast, MetaGPT-M incurred an additional 2.2 × 10 6 tokens for a mere 4.07% gain.This clearly demonstrates the token-efficiency of G-Memory.</p>
<p>Framework Analysis (RQ3)</p>
<p>Sensitivity Analysis.Regarding the hop expansion, as shown in Figure 4a, 1-hop expansion consistently yields the best or near-best performance across tasks, with peak accuracies of 85.82% (ALFWorld), 55.24% (PDDL) in AutoGen.In contrast, 2-hop and 3-hop settings often degrade performance, e.g., PDDL drops to 49.79% (2-hop).This suggests that excessive hop expansion may introduce irrelevant insights during memory upward traversal, impairing task-specific reasoning.</p>
<p>Similarly, Figure 4b shows that the optimal k is among {1, 2}.Larger k values (e.g., k=5) can significantly degrade the system performance, e.g., 7.71% ↓ on ALFWorld+AutoGen and 2.5% ↓ on FEVER+DyLAN, indicating that retrieving more queries may introduce task-irrelevant noise.</p>
<p>Collectively, we employ 1-hop expansion and k ∈ {1, 2} throughout the experiments.</p>
<p>Ablation Study. Figure 4c presents an ablation of G-Memory by isolating the impact of the highlevel insight module (I S in Equation ( 6)) and fine-grained interactions ({ ĜQi inter }</p>
<p>|M |</p>
<p>i=1 in Equation ( 7)).As shown, removing either part leads to a consistent performance drop.When only fine-grained interactions are enabled, the average scores drop by 4.47% ↓ for AutoGen and 3.82% ↓ for DyLAN   6)) or fine-grained interactions (i.e., the core trajectories in Equation ( 7)).All the experiments here are done with Qwen-2.5-14b.</p>
<p>ALFWorld + AutoGen</p>
<p>Query put a clean cloth in countertop</p>
<dl>
<dt>AutoGen Team</dt>
<dd>
<p>Ensure all required items are accessible, clean them, and return them to their designated storage locations or the specified location after use.The goal is to satisfy the following conditions: b2 is on b3., b3 is on b1.</p>
</dd>
</dl>
<p>Fine-grained Trajectory</p>
<p>High-level Insights</p>
<p>Unstack b2 from b3</p>
<p>Check b1 and b3</p>
<p>Unstack b3 from b1</p>
<p>... b1 is on b2., b2 is on b6., b3 is on b7., b5 is on b3., b6 is on b5., b7 is on b4 edge agent For : Ensure that blocks are clear and in the correct positions before attempting to stack them on another block, because this prevents invalid actions and ensures the blocks are placed correctly.</p>
<p>Check b3 and b2</p>
<p>Stack b2 on b3 compared to the full method.Conversely, enabling only insights leads to smaller drops of 3.95% and 3.39%.This indicates that while both components are contributive, interactions offer a slightly greater impact, likely due to their preserving more fine-grained, dialogue-level contextual grounding.</p>
<p>Case Study</p>
<p>Figure 5 illustrates concrete memory cues provided by G-Memory across diverse tasks.For example, in the ALFWorld+AutoGen setting, given the task query "put a clean cloth in countertop", G-Memory successfully retrieves a highly analogous historical query, "put a clean egg in microwave"-both requiring the object to be in a clean state.Alongside this, G-Memory surfaces a critical trajectory segment where the solver agent attempts to place the egg in the microwave before cleaning, prompting the ground agent to intervene.This collaborative trajectory offers actionable guidance for the current task.Moreover, the high-level insights retrieved by G-Memory prove equally valuable for task execution.In the context of HotpotQA's web search task, G-Memory retrieves an insight warning against "mistakenly referring", which helps prevent agents from incorrectly answering based on similarly named individuals.Overall, G-Memory provides effective multi-level memory support across varied domains, including embodied action, knowledge reasoning, and game environments.</p>
<p>Conclusion &amp; Limitation</p>
<p>In this paper, we conduct a thorough examination of existing memory architectures designed for multi-agent systems (MAS) and identify that their overly simplified designs fundamentally hinder the systems' capacity for self-evolution.To bridge this gap, we propose G-Memory, a hierarchical memory framework that organizes the complex and extended interaction trajectories of MAS into a three-tier graph hierarchy: the insight, query, and interaction graphs.G-Memory provides each agent with customized and hierarchical memory cues, ranging from abstract, generalizable insights to fine-grained, task-critical collaborative segments, and dynamically evolves its knowledge base across episodes.Extensive experiments demonstrate that G-Memory can be seamlessly integrated into state-of-the-art MAS frameworks, significantly enhancing their self-evolution capability, e.g., up to 20.89% ↑ improvement on embodied action tasks.Limitations: Although G-Memory has been evaluated across three domains and five benchmarks, further validation on more diverse tasks (e.g., medical QA) would strengthen its soundness, which we leave for future work.</p>
<p>[81] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.Advances in Neural Information Processing Systems, 33:5776-5788, 2020.</p>
<p>Impact Statement</p>
<p>G-Memory introduces a structured, hierarchical memory architecture for multi-agent systems (MAS), enabling large language model (LLM)-based agents to store, recall, and reason over past experiences with enhanced task generalization and cooperation efficiency.The broader impacts of this work include advancing the development of scalable and adaptive collective intelligence, with potential applications in long-term robotic planning, real-world decision-making systems, and collaborative AI assistants.However, if underlying language model is compromised or adversarially manipulated, the memory mechanisms could amplify incorrect reasoning.We urge responsible deployment of this architecture with appropriate safeguards, including continual validation, adversarial robustness checks, and alignment with human values.</p>
<p>A Experimental Details</p>
<p>A.1 Dataset Descriptions</p>
<p>In this section, we describe the datasets used in our experiments:</p>
<p>• ALFWorld [78] (available at https://alfworld.github.io/,MIT license) is a textbased embodied environment featuring household tasks, where agents navigate and interact with objects via natural language commands.</p>
<p>• ScienceWorld [79] (available at https://github.com/allenai/ScienceWorld,Apache-2.0license) is another text-based embodied environment designed for interactive science tasks.Agents must navigate rooms and conduct experiments, testing their ability to perform procedural reasoning and scientific exploration.</p>
<p>• PDDL is a game dataset from AgentBoard [80] (available at https://github.com/hkust-nlp/AgentBoard, Custom properties), comprising a variety of strategic games where agents use PDDL expressions to complete complex tasks.</p>
<p>• HotpotQA [76] (available at https://hotpotqa.github.io/,CC BY-SA 4.0 License) is a multi-hop question answering dataset with strong supervision on supporting facts.It evaluates the agent's ability to retrieve and synthesize information, especially through web search tools, for explainable reasoning.</p>
<p>• FEVER [77] (available at https://fever.ai/dataset/fever.html,Creative Commons Attribution-ShareAlike License) is a knowledge-intensive dataset focused on fact verification.Agents must validate claims using web search APIs, making it a benchmark for evidence-based reasoning.</p>
<p>Evaluation Metrics.We use exact match accuracy for FEVER and HotpotQA.For ScienceWorld and PDDL, we report the progress rate, and for ALFWorld, we use the success rate as the evaluation metric.</p>
<p>A.2 Baseline Setup</p>
<p>In this section, we provide detailed descriptions of each baseline used in our comparison:</p>
<p>• Voyager: The Voyager memory is derived from the Voyager agent [16], where an embodied agent continuously interacts with the Minecraft environment and creates new artifacts.Memory serves as the core driver of the agent's evolution.As Voyager's memory design is tailored for a single-agent setting, we adapt it to the multi-agent scenario by implementing agent-specific history retrieval based on each agent's visible dialogue context.Other singleagent memory designs are adapted in a similar manner.</p>
<p>• MemoryBank: MemoryBank [36] mimics anthropomorphic memory behaviors by selectively preserving and forgetting information.It incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve, allowing the agent to reinforce or discard memory based on temporal decay and the relative importance of stored information.</p>
<p>• Generative: This memory baseline is on [19], which includes both raw observational memory and high-level reflective memory.The latter captures abstract thoughts generated by the agent through reflection, providing a more structured and conceptualized representation of experience.</p>
<p>• MetaGPT-M: The memory design originates from MetaGPT [21], focusing solely on inside-trial memory-information stored internally during the resolution of a single task by multiple agents.</p>
<p>• ChatDev-M: This memory design is adapted from ChatDev [46], which incorporates both inside-trial and cross-trial memory.The inside-trial memory is passed from the central or initiating agent at the beginning of each round to provide guidance based on prior interactions.The cross-trial memory is relatively simple, storing past solutions to previous queries for future retrieval.However, in our task, it does not effectively manage the information-rich inter-agent collaboration.</p>
<p>• MacNet-M: This memory design is adopted from MacNet [47], where the inside-trial memory consists solely of the final answers generated in the previous round.All non-artifact dialogue contexts, i.e., the interaction trajectories among agents, are entirely discarded.</p>
<p>A.3 Multi-agent System Setup</p>
<p>In this section, we detail the setups of our three adopted MAS frameworks, AutoGen, DyLAN and MacNet:</p>
<p>A.3.1 AutoGen</p>
<p>AutoGen [13] is a popular multi-agent orchestration framework, to coordinate interactions among specialized agents for problem-solving tasks.Specifically, we utilize their A3 : Decision Making structure, which is composed of: (1) a Solver Agent, responsible for generating solutions, initialized with the system prompt "You are a smart agent designed to solve problems.";(2) a Ground Truth Agent, which critically evaluates the solver's output and identifies potential errors based on a reference standard; and (3) an Executor Agent, tasked with translating validated solutions into executable commands.This modular design enables transparent, verifiable, and actionable multiagent collaboration.</p>
<p>A.3.2 DyLAN</p>
<p>DyLAN [72] is a debate-style framework similar to LLM-Debate, but incorporates a more efficient agent-wise early stopping mechanism during multi-turn interactions.DyLAN utilizes an agent selection algorithm based on an unsupervised metric, namely the Agent Importance Score, which identifies the most contributive agents through a preliminary trial tailored to the specific task.In our implementation of DyLAN, three agents engage in the debate, while an additional ranker agent evaluates their relative importance.</p>
<p>A.4 MacNet</p>
<p>MacNet [47] is a representative work that explores decentralized and scalable multi-agent systems.</p>
<p>Its key feature lies in the absence of a central agent; instead, it introduces edge agents, which are invoked between agent interactions to provide actionable instructions to the next agent based on the previous agent's outputs.In our implementation, we adopt the random graph topology from MacNet, shown to be robust across diverse scenarios, and employ five agents in addition to the edge agents.</p>
<p>B Additional Experiment Results</p>
<p>B.1 RQ1 Results</p>
<p>Tables 2 and 3 present additional experimental results using Qwen-2.5-7band Qwen-2.5-14bas the LLM backbones.Appendix B.1 illustrates the success rate curves on ALFWorld as the number of trials increases, comparing different MAS frameworks combined with various memory architectures.As shown in Figures 6b and 6c, G-Memory consistently enables MAS frameworks to achieve success with fewer trials and leads to higher final performance ceilings.The prompt below is partially adapted from [43].We would like to express our sincere gratitude for their valuable implementation.You are an analysis -driven agent focused on learning from success .You will be provided with a set of successful trajectories that completed a similar task .As the summarizing agent , remove redundancies , combine similar ideas , and ensure clarity .</p>
<p>Inisght Summarization Function</p>
<p>Your output : """</p>
<p>Customizing Memory for Agents p r o j e c t _ i n s i g h t s _ s y s t e m _ p r o m p t : str = """ You are a thoughtful and context -aware agent .You will be provided with a successfully executed trajectory , a specific agent ** role ** , and a set of ** general insights ** applicable across all roles .Your task is to ** adapt these general insights ** into ** personalized insights ** that are specifically tailored to the given role and its trajectory .These personalized insights should help the agent improve future performance by aligning with their unique background , responsibilities , and perspective .Make sure your output reflects an understanding of the role ' s context and promotes actionable , role -relevant advice .</p>
<p>NOTE -Your output must strictly follow the format below :</p>
<p>D Discussion with Related Works</p>
<p>In this section, we further discuss the relationship between G-Memory and several recent agent memory frameworks.For A-Mem [61], while both A-Mem and G-Memory aim to enhance the memory capabilities of LLM agents, they differ in two key aspects.First, A-Mem is tailored for single-agent scenarios, whereas G-Memory is designed for processing MAS's lengthy and nuanced interaction trajectory.Second, A-Mem emphasizes atomic memory construction for chatbot-style interactions, while G-Memory focuses on distilling reusable strategies from collaborative task execution, where fine-grained atomicity is neither required nor beneficial.For Mem0 [62], although it also employs a graph-based structure, it remains within the chatbot paradigm.Its graph is closer to a knowledge graph, where nodes represent factual entities and edges represent relations, fundamentally differing from G-Memory's agent-centric memory graphs that encode trajectories, decisions, and coordination patterns across agents.</p>
<p>Figure 1 :
1
Figure 1: (Left) We report the token cost of several single-agent and MAS baselines on ALFWorld benchmark; (Right) The overview of G-Memory's three-tier hierarchical memory architecture, encompassing the insight graph, query graph and interaction (utterance) graph.</p>
<p>Embodied</p>
<p>You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, ... Your task is to: put a clean egg in microwave.</p>
<p>Figure 2 :
2
Figure 2: The overview of our proposed G-Memory.</p>
<p>Figure 3 :
3
Figure 3: Cost analysis of G-Memory.We showcase the performance versus the overall system token cost when combined with different memory architectures.</p>
<p>(a) Sensitivity analysis on #hop.(b) Sensitivity analysis on parameter k.Ablation study on two variants of G-Memory.</p>
<p>Figure 4 :
4
Figure 4: (a) Sensitivity analysis of the hop expansion in Equation (5); (b) Sensitivity analysis of the number of selected queries k in Equation (4); (c) We study two variants of G-Memory: merely providing high-level insights (i.e., the insights I S in Equation (6)) or fine-grained interactions (i.e., the core trajectories in Equation (7)).All the experiments here are done with Qwen-2.5-14b.</p>
<p>an item, ensure it is placed in the designated storage location immediately to avoid confusion or loss.For Task: put a clean egg in microwave.or Maxillaria a genus of orchids?verify that the search results are not mistakenly referring to similar entities with similar names or unrelated information.Avoid mistakenly referringAre Ruggero Deodato from Italy, and Mexican Alejandro Springall, both film directors?"</p>
<p>Figure 5 :
5
Figure 5: Case study of G-Memory.</p>
<p>Figure 7
7
Figure 7 provides additional comparisons of token cost across various benchmarks and MAS frameworks when combined with different memory architectures.Overall, G-Memory incurs only a marginal or no increase in token cost compared to classical baselines such as Generative and MetaGPT-M, while consistently delivering the most significant performance improvements.</p>
<p>l e a r</p>
<p>n _ l e s s o n s _ s y s t e m _ p r o m p t _ c o m p a r e = """ You are an analysis -driven agent focused on learning from experience .You will be provided with : -A failed trajectory and its outcome , -A successful trajectory completing a similar task .Your task is to analyze both trajectories and generate clear , actionable insights .Your insights should highlight what the failed trajectory missed and how the successful one addressed or avoided these pitfalls .##Requirements :-All insights must be derived directly from contrasting the two trajectories .-Donot speculate or introduce steps not supported by the successful example .-Focus on ** concrete behavioral or strategic differences ** between the two cases .l e a r n _ l e s s o n s _ u s e r _ p r o m p t _ c o m p a r e = "r n _ l e s s o n s _ s y s t e m _ p r o m p t _ a l l _ s u c c = """</p>
<p>Insight graph on gpt-4o-mini +Dy-LAN+ALFWorld.Insight graph on Qwen-14b +Dy-LAN+ALFWorld.</p>
<p>Figure 8 :
8
Figure 8: Visualizations of insight graphs across different LLM backbones, MAS, and benchmarks.</p>
<p>Figure 9 :
9
Figure 9: Query graph optimized from ALFWorld dataset.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Query graph optimized from SciWorld dataset.</p>
<p>prompt m e r g e _ r u l e s _ s y s t e m _ p r o m p t = """ You are an agent skilled at summarizing and distilling insights .You are given a list of insights that were previously extracted from similar tasks .These insights may contain redundancy or overlap .Your job is to ** merge and consolidate similar insights ** , and output a refined version that is ** clear , actionable , and concise <strong>.NOTE :-All merged insights ** must be based strictly on the given inputs </strong>.You are ** not allowed to make up ** or infer any new information .-The output should be easy to read and follow .Output Format : -Start your response directly with the numbered list , no preamble or explanations .-Each insight should be a short sentence .-Use the following format exactly : 1g e _ r u l e s _ u s e r _ p r o m p t = """ ## Here are the current insights that need to be merged : { current_rules } ## Please consolidate and rewrite them into ** no more than { l imite d_numb er } refined insights **.</p>
<p>Table 1 :
1
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is GPT-4o-mini.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.No-memory77.61 ↑0.0054.49 ↑0.0023.53 ↑0.0028.57 ↑0.0057.13 ↑0.0048.27 ↑0.00Voyager85.07 ↑7.4662.36 ↑7.8724.56 ↑1.0332.32 ↑3.7563.27 ↑6.1453.52 ↑5.25AutoGenMemoryBank74.96 ↓2.6553.11 ↓1.3820.41 ↓3.1233.67 ↑5.1061.22 ↑4.0948.67 ↑0.40COLM 2024Generative86.36 ↑8.7561.19 ↑6.7025.53 ↑2.0031.63 ↑3.0660.20 ↑3.0752.98 ↑4.71MetaGPT81.34 ↑3.7361.91 ↑7.4221.63 ↓1.9032.67 ↑4.1062.67 ↑5.5452.04 ↑3.77ChatDev79.85 ↑2.2450.96 ↓3.5316.65 ↓6.8824.49 ↓4.0859.18 ↑2.0546.23 ↓2.04MacNet76.55 ↓1.0655.44 ↑0.9522.94 ↓0.5928.36 ↓0.2160.87 ↑3.7448.83 ↑0.56G-Memory (Ours)88.81 ↑11.2067.40 ↑12.9127.77 ↑4.2435.67 ↑7.1066.24 ↑9.1157.18 ↑8.91No-memory56.72 ↑0.0055.38 ↑0.0011.62 ↑0.0031.69 ↑0.0060.20 ↑0.0043.12 ↑0.00Voyager66.42 ↑9.7062.83 ↑7.4515.10 ↑3.4832.64 ↑0.9562.24 ↑2.0447.85 ↑4.73DyLAN COLM 2024MemoryBank Generative55.22 ↓1.50 67.91 ↑11.1954.74 ↓0.64 64.16 ↑8.788.08 ↓3.54 13.87 ↑2.2529.59 ↓2.10 29.29 ↓2.4059.13 ↓1.07 62.30 ↑2.1041.35 ↓1.77 47.51 ↑4.39MetaGPT-M69.40 ↑12.6862.37 ↑6.9914.45 ↑2.8332.34 ↑0.6560.20 ↑0.0047.75 ↑4.63ChatDev-M46.27 ↓10.4553.35 ↓2.0310.75 ↓0.8722.45 ↓9.2458.33 ↓1.8738.23 ↓4.89MacNet-M53.44 ↓3.2854.32 ↓1.0612.11 ↑0.4930.12 ↓1.5761.10 ↑0.9042.22 ↓0.90G-Memory (Ours)70.90 ↑14.1865.64 ↑10.2618.95 ↑7.3334.69 ↑3.0064.22 ↑4.0250.88 ↑7.76No-memory51.49 ↑0.0057.53 ↑0.0012.18 ↑0.0028.57 ↑0.0060.29 ↑0.0042.01 ↑0.00Voyager61.94 ↑10.4564.53 ↑7.0014.06 ↑1.8832.65 ↑4.0862.54 ↑2.2547.14 ↑5.13MacNetMemoryBank50.00 ↓1.4960.15 ↑2.628.64 ↓3.5433.67 ↑5.1061.22 ↑0.9342.74 ↑0.73ICLR 2025Generative62.69 ↑11.2065.49 ↑7.967.92 ↓4.2629.59 ↑1.0263.27 ↑2.9845.79 ↑3.78MetaGPT-M63.70 ↑12.2165.27 ↑7.7416.03 ↑3.8531.00 ↑2.4359.33 ↓0.9647.07 ↑5.06ChatDev-M49.25 ↓2.2456.58 ↓0.9513.51 ↑1.3329.00 ↑0.4359.18 ↓1.1141.50 ↓0.51MacNet-M53.44 ↑1.9556.14 ↓1.3913.59 ↑1.4127.89 ↓0.6859.20 ↓1.0942.05 ↑0.04G-Memory (Ours)67.16 ↑15.6768.11 ↑10.5824.33 ↑12.1535.69 ↑7.1264.44 ↑4.1551.95 ↑9.94</p>
<p>Figure8visualizes the high-level insights summarized by G-Memory on the ALFWorld benchmark across different MAS frameworks and LLM backbones.Given that ALFWorld naturally consists of diverse task categories, we further examine how insight nodes corresponding to different task types are interconnected.Overall, we observe dense intra-category connections among insights derived from similar tasks, while also noting the emergence of meaningful inter-category links, reflecting transferable patterns across task domains.B.3.2CaseStudy on Query GraphsFigures 9 to 11 visualize the query graphs constructed by G-Memory on the ALFWorld, PDDL, and SciWorld benchmarks.Recall that a directed edge between two query nodes indicates that the historical trajectory of one query offers useful guidance for the execution of another.We observe emergent clustering patterns, where groups of semantically similar queries form densely connected subgraphs, while sparser inter-cluster edges capture cross-task inspirations.These patterns demonstrate G-Memory's ability to effectively organize and relate collaborative experiences through structured memory reasoning.
B.3 Case StudyB.3.1 Case Study on Insight Graphs</p>
<p>Table 2 :
2
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is Qwen-2.5-7b.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.No-memory37.31 ↑0.0023.49 ↑0.0010.86 ↑0.0020.26 ↑0.0048.17 ↑0.0028.02 ↑0.00Vanilla LLMVoyager MemoryBank38.19 ↑0.88 40.30 ↑2.9924.11 ↑0.62 21.64 ↓1.8512.14 ↑1.28 14.36 ↑3.5019.12 ↓1.14 18.79 ↓1.4749.68 ↑1.51 47.66 ↓0.5128.65 ↑0.63 28.55 ↑0.53Generative39.16 ↑1.8526.10 ↑2.6111.37 ↑0.5123.48 ↑3.2252.50 ↑4.3330.52 ↑2.50No-memory52.99 ↑0.0030.27 ↑0.0016.17 ↑0.0033.33 ↑0.0058.74 ↑0.0038.30 ↑0.00Voyager55.22 ↑2.2326.70 ↓3.5712.00 ↓4.1734.29 ↑0.9652.44 ↓6.3036.13 ↓2.17MemoryBank53.37 ↑0.3827.33 ↓2.9414.83 ↓1.3432.67 ↓0.6659.45 ↑0.7137.53 ↓0.77AutoGenGenerative62.69 ↑9.7031.45 ↑1.1817.88 ↑1.7134.17 ↑0.8461.25 ↑2.5141.49 ↑3.19COLM 2024</p>
<p>Table 3 :
3
Performance comparison with single/multi-agent memory architectures on five benchmarks.The underlying LLM backbone is Qwen-2.5-14b.We highlight the best and second best results.
MASMemoryALFWorldSciWorldPDDLHotpotQAFEVERAvg.
http://github.com/ollama/ollama
C Prompt SetQuery Relevance Filtration t a s k _ r e l e v e n c y _ s y s t e m _ p r o m p t = """ You are an agent designed to score the relevance between two pieces of text ."""t a s k _ r e l e v e n c y _ u s e r _ p r o m p t = """ You will be given a successful case where you successfully complete the task .Then you will be given an ongoing task .Do not summarize these two cases , but rather evaluate how relevant and helpful the successful case is for the ongoing task , on a scale of 1 -10.Success Case : { trajectory } Ongoing task : { qu ery_sc enario } Score : """Graph Sparsifier e x t r a c t _ t r u e _ t r a j _ s y s t e m _ p r o m p t = """ You are an agent skilled at extracting key points .Given a task and a successful execution trajectory , your job is to identify the critical steps needed to complete the task while filtering out less important steps ."""e x t r a c t _ t r u e _ t r a j _ u s e r _ p r o m p t = """ Note :-Strictly follow the original trajectory ; absolutely no steps that are not in the trajectory should be added .
. P James, Gerardo Walsh, Rivera Ungson, Organizational memory. Academy of management review. 1611991</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Huang, 2023</p>
<p>Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M Alvarez, arXiv:2405.015332024arXiv preprint</p>
<p>Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu, arXiv:2310.132552023arXiv preprint</p>
<p>Editable scene simulation for autonomous driving via collaborative llm-agents. Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, Yanfeng Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang, arXiv:2403.031012024arXiv preprint</p>
<p>Plan-and-act: Improving planning of agents for long-horizon tasks. Nicholas Lutfi Eren Erdogan, Sehoon Lee, Suhong Kim, Hiroki Moon, Gopala Furuta, Kurt Anumanchipalli, Amir Keutzer, Gholami, arXiv:2503.095722025arXiv preprint</p>
<p>Understanding the planning of llm agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, arXiv:2402.027162024arXiv preprint</p>
<p>Agent q: Advanced reasoning and learning for autonomous ai agents. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov, arXiv:2408.071992024arXiv preprint</p>
<p>The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. Tula Masterman, Sandi Besen, Mason Sawtell, Alex Chao, arXiv:2404.115842024arXiv preprint</p>
<p>Embodied agent interface: Benchmarking llms for embodied decision making. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>Embodied multi-modal agent trained by an llm from a parallel textworld. Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, August 01. 2023 2023</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024</p>
<p>Data interpreter: An llm agent for data science. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, arXiv:2402.186792024arXiv preprint</p>
<p>Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.16291May 2023arXiv e-prints</p>
<p>Driving with llms: Fusing object-level vector modality for explainable autonomous driving. Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Optimizing autonomous driving for safety: A human-centric approach with llm-enhanced rlhf. Yuan Sun, Navid Salami Pargoo, Peter Jin, Jorge Ortiz, Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. April 01. 2023 2023</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, CoRR, abs/2305.143252023</p>
<p>Metagpt: Meta programming for multi-agent collaborative framework. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, August 01. 2023 2023</p>
<p>Society of mind. Marvin Minsky, 1988Simon and Schuster</p>
<p>Examining the society of mind. Push Singh, Comput. Artif. Intell. 2262003</p>
<p>CAMEL: communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, NeurIPS2023</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, July 01, 2023 2023work in progress</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, CoRR, abs/2402.016802024</p>
<p>Reasoning capacity in multi-agent systems: Limitations, challenges and human-centered solutions. Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom Mitchell, Estevam Hruschka, CoRR, abs/2402.011082024</p>
<p>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schölkopf, arXiv:2404.16698Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm agents. 2024arXiv preprint</p>
<p>Discovering causality for efficient cooperation in multi-agent environments. Rafael Pina, Varuna De Silva, Corentin Artaud, CoRR, abs/2306.118462023</p>
<p>Cut the crap: An economical communication pipeline for llm-based multi-agent systems. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, Tianlong Chen, arXiv:2410.025062024arXiv preprint</p>
<p>Masrouter: Learning to route llms for multi-agent systems. Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, Yiyan Qi, arXiv:2502.111332025arXiv preprint</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, arXiv:2310.17512Competeai: Understanding the competition behaviors in large language model-based agents. 2023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, CoRR, abs/2305.191182023</p>
<p>Battleagentbench: A benchmark for evaluating cooperation and competition capabilities of language models in multi-agent systems. Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang, arXiv:2408.159712024arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, April 01, 2023 2023Tech Report</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023</p>
<p>Memllm: Finetuning llms to use an explicit read-write memory. Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze, arXiv:2404.116722024arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.039012023arXiv preprint</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, Recmind, arXiv:2308.14296Large language model powered agent for recommendation. 2023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, Metaagents, arXiv:2310.06500Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Communicative agents for software development. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, July 01, 2023 2023252 tables</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Gptswarm: Language agents as optimizable graphs. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber, Forty-first International Conference on Machine Learning. 2024</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024arXiv preprint</p>
<p>. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu, arXiv:2410.10762AFlow: Automating Agentic Workflow Generation. October 2024</p>
<p>Multi-agent architecture search via agentic supernet. Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang, arXiv:2502.041802025arXiv preprint</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuan-Jing Huang, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, A survey on large language model based autonomous agents. Front. Comput. Sci. 182024</p>
<p>Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, 2023arxiv preprint, abs/2309.07864</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, CoRR, abs/2312.119702023</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang, 2024Vicinagearth19</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, arXiv:2306.078632023arXiv preprint</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, arXiv:2501.06590Self-updating library in large language models improves chemical reasoning. 2025arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, abs/2303.113662023arXiv preprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang, arXiv:2502.121102025arXiv preprint</p>
<p>Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav, arXiv:2504.19413Mem0: Building production-ready ai agents with scalable long-term memory. 2025arXiv preprint</p>
<p>Meminsight: Autonomous memory augmentation for llm agents. Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba, arXiv:2503.217602025arXiv preprint</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou, arXiv:2406.04692Mixture-of-agents enhances large language model capabilities. 2024arXiv preprint</p>
<p>Symbolic learning enables self-evolving agents. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, arXiv:2406.185322024arXiv preprint</p>
<p>Self-evolving agents with reflective and memory-augmented abilities. Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, Jingsong Yang, arXiv:2409.008722024arXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou, 2023</p>
<p>Self-evolving multi-agent collaboration networks for software development. Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen, arXiv:2410.169462024arXiv preprint</p>
<p>Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, Dawei Cheng, G-Designer, arXiv:2410.11782Architecting multi-agent communication topologies via graph neural networks. 2024arXiv preprint</p>
<p>Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang, arXiv:2406.142282024arXiv preprint</p>
<p>Guibin Zhang, Kaijie Chen, Guancheng Wan, Heng Chang, Hong Cheng, Kun Wang, Shuyue Hu, Lei Bai, arXiv:2502.07373Evoflow: Evolving diverse agentic workflows on the fly. 2025arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, CoRR, abs/2310.021702023</p>
<p>Microsoft academic graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, Anshul Kanakia, Quantitative Science Studies. 112020</p>
<p>Sirius: Self-improving multiagent systems via bootstrapped reasoning. Wanjia Zhao, Mert Yuksekgonul, Shirley Wu, James Zou, arXiv:2502.047802025arXiv preprint</p>
<p>Reso: A reward-driven selforganizing llm-based multi-agent system for reasoning tasks. Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai, arXiv:2503.023902025arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Fever: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, arXiv:1803.053552018arXiv preprint</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.13178,2024.MetaGPT-M55.52↑2.5332.44↑2.1717.04↑0.8735.36↑2.0363.33↑4.5940.74↑2.44arXiv preprint</p>
<p>. G-Memory , Ours) 67.91 ↑14.92 34.89 ↑4.62 21.01 ↑4.84 37.34 ↑4.01 64.34 ↑5.60 45.10 ↑6.80</p>
<p>. G-Memory , Ours) 52.99 ↑11.65 33.81 ↑3.97 20.71 ↑7.15 29.33 ↑5.04 63.67 ↑7.44 40.10 ↑7.05</p>
<p>. ICLR 2025 No-memory 44.03 ↑0.00 28.76 ↑0.00 13.36 ↑0.00 22.24 ↑0.00 55.12 ↑0.00 32.70 ↑0.00MacNet. </p>
<p>. G-Memory , Ours) 54.48 ↑10.45 32.23 ↑3.47 17.48 ↑4.12 27.53 ↑5.29 59.14 ↑4.02 38.17 ↑5.47</p>
<p>. G-Memory , Ours</p>
<p>. G-Memory , Ours) 81.34 ↑5.22 64.68 ↑11.44 51.12 ↑9.29 34.63 ↑4.02 66.66 ↑3.32 59.69 ↑6.66</p>
<p>. ICLR 2025 No-memory 58.21 ↑0.00 52.21 ↑0.00 41.74 ↑0.00 28.60 ↑0.00 64.65 ↑0.00 49.08 ↑0.00MacNet. </p>
<p>. G-Memory , 79.10 ↑20.89 61.74 ↑9.53 45.76 ↑4.02 32.33 ↑3.73 70.33 ↑5.68 57.85 ↑8.77</p>
<p>Even in a successful trajectory , there may be some incorrect steps . Pay attention to actions that correspond to " Nothing happens " observations , as these actions are likely incorrect . Filter out these actions for me . -You need to ensure that each step is at the finest granularity. You should strictly follow the output format in the example</p>            </div>
        </div>

    </div>
</body>
</html>
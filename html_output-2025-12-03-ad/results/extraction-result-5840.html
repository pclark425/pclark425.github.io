<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5840 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5840</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5840</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-264487188</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.16146v1.pdf" target="_blank">Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature</a></p>
                <p><strong>Paper Abstract:</strong> The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5840",
    "paper_id": "paper-264487188",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00399575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLINFO.AI: AN OPEN-SOURCE RETRIEVAL-AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE
24 Oct 2023</p>
<p>Alejandro Lozano lozanoe@stanford.edu 
Department of Biomedical Data Science
Stanford University Stanford
CAUSA</p>
<p>Scott L Fleming scottyf@stanford.edu 
Department of Biomedical Data Science
Stanford University Stanford
CAUSA</p>
<p>Chia-Chun Chiang chiang.chia-chun@mayo.edu 
Department of Neurology
Mayo Clinic Rochester
MN Human Centered Artificial-Intelligence Institute Stanford University Stanford
CAUSA</p>
<p>Nigam Shah 
Center for Biomedical Informatics Research Clinical Excellence Research Center
Stanford University Technology and Digital Solutions Stanford Healthcare</p>
<p>CLINFO.AI: AN OPEN-SOURCE RETRIEVAL-AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE
24 Oct 2023CD2DFEE0F714C30F616F59DA1008A7FFarXiv:2310.16146v1[cs.IR]Large Language ModelsAbstractive SummarizationArtificial IntelligenceClinical MedicineGenerative AIInteractive SystemsChatGPT
The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner.While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking.Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools.We address these issues with four contributions: we release Clinfo.ai,an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200.</p>
<p>The aggregation and distribution of medical knowledge, facilitated by platforms such as PubMed or Cochrane, enables healthcare professionals and medical researchers to stay abreast of the latest scientific discoveries and make informed decisions based on up-to-date scientific evidence [1].However, the staggering influx of more than 1 million papers each year into PubMed alone (equivalent to two papers per minute as of 2016) [2] highlights the daunting task of keeping up with scientific findings [3].This is especially true for practicing clinicians, who face the challenge of keeping track of the most updated research findings in all areas related to their patient care duties [4].</p>
<p>Existing technologies fail to adequately satisfy the information needs of health care professionals and researchers.In daily practice, clinicians have on average one care-related question for every other patient seen [5] and they refer to sources like PubMed or UpToDate to obtain summarized information answering these questions [6].Questions that cannot be answered within 2 to 3 minutes are often abandoned, potentially negatively impacting patient care and outcomes [5,7].While systematic review (SR) articles can provide quick answers to clinical questions, many questions are not answerable through existing reviews.On the other hand, manually synthesizing findings from multiple primary sources without the help of a published review article can be extraordinarily time consuming.Review articles take on average 67.3 weeks to complete [8], and those written reviews may not even include the most updated research published in the literature.Question-answering tools that leverage frequently updated external electronic resources would enable researchers and clinicians to obtain up-to-date information in a more efficient way that benefits scientific discovery and quality of patient care [9,10,11,12,13].</p>
<p>In previous decades, applications that integrated clinical systems with on-line information to answer users' information needs (e.g., "infobuttons") [14] were typically driven by semantic networks.Other works such as CHiQA proposed a combination of knowledge-based, machine learning, and deep learning approaches to develop a question-answering system using patient-oriented resources to answer consumer health questions [15].</p>
<p>The new capabilities of agents powered by large language models (LLM) has accelerated the development of automated literature summarization tools.Most of these solutions tend to be privately developed, closed-source solutions based on retrieval-augmented [16] (RetA) LLMs [17] (e.g.Scite [18], Elicit [19], GlacierMD [20], Consensus [21], OpenEvidence [22], Statpearls semantic search [23]).However, the paucity of publicly available technical reports describing these systems and the lack of appropriate guidelines, regulations, and evaluations to ensure their safe and responsible usage is an urgent concern [24].This Natural Language Generation (NLG) problem has been exacerbated by a lack of (1) representative datasets and associated tasks, and (2) automated metrics for evaluating RetA LLMs on said tasks.</p>
<p>Fortunately, developments in the LLM evaluation space have shown that a number of automated metrics correlate moderately with human preference, even in domain-specific scenarios (including medicine) [25,26,27].</p>
<p>Building on these advancements, we provide four contributions:</p>
<ol>
<li>
<p>Clinfo.ai1 , the first publicly available, open-source, end-to-end retrieval-augmented LLM-based system for querying and synthesizing the clinical literature.The system is hosted as a publicly available WebApp at https://www.clinfo.ai/.</p>
</li>
<li>
<p>An open information retrieval and abstractive summarization task specification designed to evaluate an algorithm's ability to both retrieve relevant information and adequately synthesize it.In the task setup, both the information retrieval and abstractive summarization sub-tasks are compared to gold standard (human generated but pragmatically retrieved) references and answers.Furthermore, our task is defined to truly resemble RetA deployment conditions (enabling the evaluation of already deployed but potentially closed-source systems).</p>
</li>
<li>
<p>PubMed Retrieval and Synthesis (PubMedRS-200), a publicly available dataset of 200 questions structured in Open QA format, paired with answers derived from systematic reviews and corresponding references.</p>
</li>
<li>
<p>Benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200).</p>
</li>
</ol>
<p>Related Work</p>
<p>LLMs in healthcare The remarkable performance of LLMs in the general domain has brought about a revolution in the field of natural language processing [28], showcasing exceptional capabilities in tasks like summarization, question-answering, and NLG [29].Given their wide utility, researchers are now actively exploring applications of LLMs in healthcare [30,31,32,33].Several LLMs have achieved human-level performance on numerous medical professional licensing exams such as the United States Medical Licensing Exam (USMLE) [34].Other works have demonstrated promise in various healthcare-inspired tasks, such as automated clinical note generation and reasoning about public health topics [30,31,32,33].However, NLG tasks and publicly available benchmarks that directly address true medical needs are still underrepresented in the literature.Such tasks and benchmarks are especially important for estimating the capabilities and risks of LLMs in the clinical domain.</p>
<p>LLMs have several documented disadvantages and risks.First, updating LLMs with new knowledge and information is challenging and inefficient [35].Second, the training objective of LLMs to predict the most probable next token can cause these models to generate inaccurate information (hallucination), requiring costly and imperfect post-hoc model adjustments like reinforcement learning with human feedback (RLHF) [36].More importantly, most popular consumer-facing LLMs (e.g., OpenAI's GPT-4 [29], Meta's Llama 2 [37], Anthropic's Claude 2 [38]) do not provide references pointing to their source of information, even when the model's output is factual.This can engender distrust with users in many scientific domains, including healthcare.Prior work has proposed ReTA LLMs [16] to solve the information provenance issue and have shown promising results.These ReTA LLMs do not require post-hoc model editing in order to incorporate new knowledge.</p>
<p>Retrieval Augmentation Question Answering LLMs in Medicine Hiesinger et al. [39] introduced Almanac, a novel LLM integrated with a vector database and calculator, designed to answer 130 clinical questions generated by a panel of five board-certified clinicians and resident physicians.The results showed that Almanac surpassed a standard LLM (GPT-4) in factuality, safety, and correctness, indicating that retrieval systems lead to more accurate and reliable responses to clinical inquiries.Soong et al. [40] evaluated GPT-3.5 and GPT-4 models against a custom RetA LLM using a set of 19 questions.The evaluation, based solely on human judgments, revealed that both GPT-3.5 and GPT-4 exhibited more hallucinations in all 19 responses compared to the RetA model.While these works on RetA LLM systems represent significant progress, they suffer from at least two shortcomings: (1) they typically require human evaluation, making systematic benchmarking of new systems challenging and unscaleable; (2) they often focus solely on evaluating an LLM's output, disregarding the relevance of the information retrieved to generate an answer.Deciding which "relevant" sources should be summarized can be just as challenging as generating the actual summary.Hence there is a need for a benchmark that enables integrated evaluation of both a system's ability to select relevant documents as well as its ability to summarize these documents.</p>
<p>2 Materials and Methods PubMed is a free resource supporting search and retrieval of biomedical literature [41].As prior work has demonstrated, a large quantity of research papers available in this index are phrased as questions, and it is possible to structure them in a question-answer format [42,43].Extending this idea, we created an open information retrieval and abstractive summarization dataset, using SR as a proxy for inquiries of medical interest.The rationale is that SRs are structured reviews written by human experts which summarize the pertinent literature related to a question of interest in an evidence-based manner [44].In writing a SR, experienced authors (1) screen the published literature in a systematic way and include studies in a standardized manner; (2) critically evaluate methodology and reported outcomes of the included studies; and (3) carefully extract data, summarize original research findings, and in some instances, conduct additional statistical analysis of extracted results from studies including randomized controlled trials, observational cohort studies, case series and other qualitative studies on a specific topic.Furthermore, SRs are extensively used to provide evidence for various purposes, including policy-making, clinical practice guidelines, health technology assessment, and decision making in healthcare [45].As SRs unify and present a comprehensive overview of a given subject by human experts, we chose to leverage published SRs as gold standards when building our database.</p>
<p>Dataset Generation</p>
<p>To populate such a dataset, we employed E-utilities, a public API to the NCBI Entrez system [41] , to access PubMed and construct question-answer pairs with their respective references.Figure 1 illustrates our process in detail.First, we established a comprehensive selection of medical specialties and subspecialties.Second, we formulated a query to retrieve Systematic Reviews relevant to each medical specialty/subspecialty.Upon constructing the specialty-specific queries and retrieving associated abstracts, we retrieved all papers structured in a format that can be easily converted to questions-answer pairs (as noted by Jin et al 2019 [42]) namely Title, Introduction, Conclusion, and References.Third, we applied another filtering process, narrowing down to solely those publications whose titles included an explicit question (i.e., publications whose titles including question marks).The questions from these titles were extracted.</p>
<p>Finally, two human evaluators (AL and SF) manually reviewed the retrieved questions and extracted an answer to each question using minimally modified text from the results and conclusions section of the corresponding SR abstract.</p>
<p>Concretely, in order to generate each answer, the human reviewers removed from the Results and Conclusions section of the abstract any text describing the structure or design of the systematic review (e.g., "We used PubMed to retrieve 100 papers"), leaving only text that directly addressed the question extracted from the SR's title.In the process, abstracts that were lacking substantive results and abstracts that merely described research proposals (e.g.descriptions of future work) were entirely removed.</p>
<p>Clinfo.ai: An LLM Chain for Information Retrieval and Synthesis</p>
<p>Our proposed RetA LLM system, Clinfo.ai,consists of a collection of four LLMs working conjointly (an LLM chain [46]) coupled to a Search Index (either PubMed or Semantic Scholar) as depicted in Figure 2. Previous works have observed that very large language models (e.g., 100B parameters or more) exhibit zero-shot reasoning capabilities, where task-specification prompts can be used to guide the LLM output without further fine-tuning [47,48].We leverage the zero-shot reasoning capabilities of two LLMs, specifically OpenAI's GPT-3.5 and GPT-4 models, to complete each step in the LLM chain depicted in Figure 2. All prompts used in each step of the chain are available in the supplemental material2 .We use LangChain's API to send prompts and receive outputs from GPT-3.5 and GPT-4.While different models could technically be used through this entry point, our experiments are limited to OpenAI's GPT-3.5 and GPT-4 models (snapshots gpt-3.5-turbo-0613, gpt-4-0613 respectively).For both models, we employ a temperature of 0.5 and a max token generator limit of 1024.</p>
<p>Figure 2: Clinfo.ai:A RetA LLM system for retrieving and summarizing scientific articles</p>
<p>Query Generator</p>
<p>In our Clinfo.aisystem, the input is the question submitted by the user.Once a question is submitted, the primary task of the query generator (labeled "Question2Query" in Figure 2) is to construct a PubMed (or Semantic Scholar) query that efficiently retrieves a substantial number of relevant articles pertaining to the posed question.This is achieved by instructing the model to incorporate the most crucial and relevant keywords that accurately represent the query's context and requirements.</p>
<p>Figure 3: Query Generated by Clinfo.ai for question: "Does high-grade dysplasia/carcinoma in situ of the biliary duct margin affect the prognosis of extrahepatic cholangiocarcinoma?"</p>
<p>Information Retriever</p>
<p>In a similar fashion to the Dataset Generation process, we utilize the Entrez API to fetch abstracts from PubMed using the output generated by the Query Generator.By leveraging the Entrez API, we are able to programmatically access and retrieve the relevant abstracts that match the constructed PubMed queries.Because LLM output is stochastic and different queries may capture different aspects of the literature, we take the union of all papers returned by three LLM-generated queries (each with the same prompt but different seeds).</p>
<p>Relevance Classifier</p>
<p>Since the query generator emphasizes recall over precision (i.e., it retrieves as many potentially relevant articles as possible), it is crucial to classify the relevancy of the retrieved articles.To achieve this, we adopt an LLM-enabled binary classification approach, wherein each article is categorized as either relevant or not relevant to the posed question using GPT-3.5.Once the relevant articles are identified, we make use of the full abstract metadata of each article to construct their citations in the IEEE format.If more than 35 relevant articles are deemed relevant, the user can decide to re-rank and filter them using BM25 [49].</p>
<p>Summarization</p>
<p>The penultimate step in Clinfo.aiuses an LLM to summarize each relevant abstract within the context of the usersubmitted question.</p>
<p>Synthesis</p>
<p>In the final step of Clinfo.ai, the relevant article summaries are organized as an ordered list, with each number in the list corresponding to a citation.This structured list of article summaries is then fed to a LLM with the task of constructing a concise and informative summary.The LLM is also instructed to utilize only the provided article summaries and no other additional information, relying on the structured list of citations to reference and accurately attribute each finding.To facilitate interaction with our system, we developed a web application that allows users to submit their own questions and/or customize the prompts.The latter enables users to tailor the system according to their individual preferences and needs, as illustrated in Figure 4.The entire process provides real-time access, displaying the queries generated during the search (as shown in Figure 3), the number of retrieved articles, a concise summary of each important article, and a final "Literature Summary" (or "Synthesis", to distinguish it from the individual article summaries) accompanied by an abbreviated answer to the question ("TL;DR").Additionally, the references are presented as hyperlinks, enabling users to verify both the validity of the reference and the information captured from it.It is possible that even after summarizing an article's abstract, Clinfo.aimay not include that article in the final Literature Summary or "TL;DR".Nevertheless, we ensure that all relevant articles are presented to the user so that they can access and explore them as needed.An example of a final Literature Review constructed with Clinfo.ai is shown in Figure 5 Figure 5: "Literature Summary" (Synthesis) and "TL;DR" constructed with Clinfo.ai for the question, "Does high-grade dysplasia/carcinoma in situ of the biliary duct margin affect the prognosis of extrahepatic cholangiocarcinoma?" (not all references are included in figure)</p>
<p>Task Description and Evaluation</p>
<p>The task is defined in a three step manner:</p>
<ol>
<li>
<p>Given a question, generate a query to retrieve a set of articles;</p>
</li>
<li>
<p>Given the provided articles, determine their relevancy to the question;</p>
</li>
<li>
<p>Given relevant articles, summarize the findings.</p>
</li>
</ol>
<p>Step ( 2) is evaluated based on precision and recall.Considering the set of all documents D, RET (D, k) denotes the set of k retrieved documents deemed relevant and REL(D, q) the set of all documents referenced by a SR.We define precision and recall in this context as follows:
precision = |RET (D, k) ∩ REL(D, q)| |RET (D, k)| (1) recall = |RET (D, k) ∩ REL(D, q)| |REL(D, q)|(2)
Step (3) is conducted using both source-free (SF) and source-augmented (SA) automated metrics.Source-free metrics compare a model's output to a gold standard reference summary, without including any information from the articles used to generated the gold standard summary.For our evaluation purposes, the gold standard is the human-curated answer (derived from conclusions and/or results of each SR).On the other hand, SA metrics additionally consider relevant context to evaluate the quality of model-generated outputs.For our experiments, context is constructed by concatenating a SR's introduction, results, and conclusion sections.The SA metrics we employed (and the LMs they use) include UniEval [26] (T5 -large), COMET (XLM-RoBERTa) [50], and CTC Summary Consistency (BERT) [51].</p>
<p>UniEval is a multi-dimensional evaluator designed for summarization tasks and takes into account four key dimensions (and their corresponding overall average):</p>
<p>• Coherence: Assesses whether the summary forms a cohesive and rational body of text;</p>
<p>• Consistency: Evaluates the factual alignment between the information presented in the summary and the content of the source document;</p>
<p>• Fluency: Assesses the readability and linguistic fluency of a summary;</p>
<p>• Relevance: Measures whether the summary contains only the important information from the source document.</p>
<p>COMET is an evaluation metric developed to assess the quality of Machine Translation (MT) systems.Despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality [52].CTC is an evaluation framework, based on information alignment between input, output, and context, for compression (e.g summary), transduction (e.g translation), and creation (e.g.conversation).</p>
<p>Finally we perform an evaluation using SF metrics, including BERTScore [53], ROUGE-L [54], METEOR [55], chrF [56] , GoogleBLEU (based on [57]), CTC Summary (without providing context) , and CharacTer [58].The majority of these metrics have shown moderate correlation with human preference and are widely reported in NLG tasks [25,26].</p>
<p>The multi-dimensional evaluation based on source-augmented metrics makes the assumption that an LLM+RetA model is able to (1) retrieve abstracts of works that were deemed relevant by an author of a SR and (2) synthesize them in a similar fashion.We acknowledge that if this assumption is not met, the evaluation would heavily penalize the output.Conversely, if the system retrieves an article that was not considered by a SR but bears a similar semantic meaning to an article present in the references of a SR, the evaluation would not penalize the generated text.For our proposed method, both behaviors are desired.Using our proposed task, we evaluated the performance of GPT-4 and GPT-3.5 without retrieval augmentation, Clinfo.ai(our GPT-enabled RetA LLM system), and two deployed tools: Elicit (an AI research assistant based on LLMs, designed for facilitating literature review generation, accessed on 07-02-2023), and Statpearls Semantic Search (a free search tool for medical knowledge, accessed on 07-25-2023).While other automated literature summarization systems are available, at the time of this study the vast majority require a subscription to answer multiple questions.Additionally, a subset of these systems refused to provide an answer to a significant number of the PubMedRS-200 questions as posed, making evaluation for these systems fraught and difficult to interpret.We exclude these systems from our analysis.Lastly, since our framework generates two outputs -"TL;DR" and "Literature Summary" (also referred to as "Synthesis") -we conducted evaluations of three forms of Clinfo.ai'soutput: (1) the synthesis of the articles retrieved and deemed relevant ("Synthesis"); (2) the abbreviated summary distilling the proposed "Synthesis" into one or two sentences ("TL;DR"); (3) the combined "Synthesis" and "TL;DR".</p>
<p>Baselines and Experiments</p>
<p>We recognize that the usage of scientific literature to extract question-answer pairs comes with the possibility that an answer deemed correct at the time of acquisition may be incorrect as new discoveries are published.To ensure that a system is not rewarded for simply copy-pasting the text of a retrieved source SR nor penalized when new relevant articles are published, we consider three evaluation regimes:</p>
<ol>
<li>Restricted Search (RS): The retrieval process is constrained to include publications up to one day before the publication date.While this approach may not guarantee the retrieval of all publications considered important by the authors of each source systematic review, it effectively narrows down the search space to the subset of publications that could have been retrieved and deemed relevant during the review's preparation.</li>
</ol>
<p>Source Dropped (SD):</p>
<p>The retrieval process can retrieve articles published both before and after the source systematic review.However, if the source SR is retrieved, it is removed from the set of relevant articles and not used in the subsequent steps of the summarization process.</p>
<p>Unrestricted Search (US)</p>
<p>No restriction is applied; the source SR may (but need not) be included in the set of relevant articles retrieved by the system.Because we could not control the set of articles retrieved and summarized by closed-source tools like Elicit and Statpearls SS, they effectively fall within this evaluation regime.</p>
<p>Finally, to ensure that conformity with the SD regime would not prevent direct comparison with the other evaluation regimes, we removed questions from all other training regimes for which Clinfo.ai could only retrieve the source article (resulting in zero articles remaining after exclusion under the SD regime).This yielded 145 SRs (80 after October 2021 and 65 before).As reported in previous studies [39,34,59], both GPT-3.5 and GPT-4 without RetA demonstrated strong zero-shot performance using both source-augmented (Table 1) and source-free (Table 2) metrics.Notably, there was no substantial performance drop observed when these models were presented with questions based on source SRs published after September 2021 (Comparing Table 1 and Table S1 in the Supplement).While more studies are necessary, we postulate that this can be attributed to the models' exposure to prior published works during training.Since SRs are built upon existing literature ranging across multiple years, it is plausible that the models have been trained on relevant information that aids them in providing accurate responses to questions based on newer research.However, comparing all LLM against LLM + RetA models, the inclusion of RetA leads to a slight improvement in the overall performance of the models when evaluated with SF and SA automated metrics, irrespective of the publication date of the source SR.</p>
<p>Previous works based on human evaluation have observed a similar trend, corroborating our automated evaluation framework.</p>
<p>How does Clinfo.ai perform compared to other systems?</p>
<p>As depicted in Table 1, Clinfo.ai exhibited better performance in overall UniEval compared to other RetA systems, irrespective of the chosen output strategy (Synthesis, TL;DR, or a concatenation of the two).This improvement in performance remained consistent regardless of the average length of the output, with Clinfo.aiachieving better results for both approximately 3x shorter (TL;DR) and around 2x longer outputs (Synthesis).Furthermore, this performance persisted across all different evaluation regimes, even when the source SR was dropped.This improvement amounted to at least 6.2% and at most 14.9% in UniEval Overall performance.These results suggest two significant points: (1) Our system is not merely copying and pasting information from an SR review.Instead, it demonstrates a genuine ability to process and present the information effectively, resulting in enhanced performance compared to other available tools; and (2) even in the absence of a source SR, Clinfo.aican still provide conclusions that are better aligned with a source SR's conclusion (compared to tools that might include the source SR).</p>
<p>TL;DR or Synthesis?</p>
<p>Clinfo.ai TL;DR demonstrates significantly better performance compared to Synthesis and Synthesis &amp; TL;DR, even though they all utilize the same relevant retrieved articles.It is worth noting that while Synthesis provides evidence to answer the question based on the retrieved articles, this evidence may not align with the original evidence reported by a Systematic Review (SR).However, the increased performance of TL;DR could be attributed to the LLM's capability to correctly identify the most salient points of the relevant articles and effectively summarize them.On the other hand, using only source-free (SF) metrics (Table 2), Elicit performs better under BERTScore, ROUGE-L and GoogleBLEU, while Clinfo.aiTL;DR performs better under METEOR, chrF, CTC (SF), and CharacTer.</p>
<p>These results highlight a potential limitation of automated evaluation .For instance, SF metrics tend to reward short responses, which may not necessarily be accurate or comprehensive.On the other hand, several SA metrics can assign the best score to considerably larger generations (UniEval's Coherence and Relevance, and COMET), acknowledging their quality and relevance.This discrepancy in evaluation metrics raises concerns about the fair assessment of model performance and emphasizes the need for a comprehensive evaluation approach.</p>
<p>Comparing different evaluation regimes, the best performance was observed under the Unrestricted Search evaluation regime, possibly due to the fact that the source SR was retrieved on 96.5% of the questions.As expected given the restricted set of retrievable documents, Clinfo.ai'sprecision was highest under the Restricted Search regime (Table 3).</p>
<p>Conclusion</p>
<p>The rapidly expanding medical literature and the capabilities of LLMs to process and summarize vast amounts of information have led to the development of several tools that utilize LLMs to generate on-demand summaries of published scientific literature.However, the lack of high-quality datasets and appropriate benchmarking tasks has hindered rigorous evaluations of these tools.To address this gap, we have introduced Clinfo.ai,an open-source end-to-end LLM-chain workflow designed to query, evaluate, and synthesize medical literature into concise summaries for answering questions on demand.Additionally, we introduce a unique dataset, PubMedRS-200, which consists of questions and answers extracted from systematic reviews, enabling automatic evaluation of LLM performance in Retrieval Augmentation Question Answering.Our tools and benchmarking dataset are publicly available to ensure reproducibility and to facilitate further research in harnessing LLMs for Retrieval Augmentation Question Answering tasks.</p>
<p>Limitations</p>
<p>In this study, we employed automated metrics that have demonstrated moderate-to-high correlation with human preferences, but we did not explicitly solicit human preferences to evaluate the RetA LLM systems considered.Future work should consider including human evaluation to ensure alignment of automated metrics and human preferences.Lastly, it is worth noting that prior studies have reported that LLMs demonstrate the ability to generate accurate Boolean operators and syntax, effectively adhering to PubMed query formats.However, our observations revealed that these models also generated hallucinated MeSH terms, which could potentially lead to the exclusion of relevant studies.To overcome this limitation, future research efforts should prioritize improving the query generation process, ensuring that generated MeSH terms are reliable and relevant for better precision and recall in medical literature search tasks.</p>
<p>Figure 1 :
1
Figure 1: Schematic Representation of the Protocol for Retrieving Abstracts from PubMed and Generating Title-Based Questions</p>
<ol>
<li>3 Figure 4 :
34
Figure 4: Clinfo.aiuser interface</li>
</ol>
<p>Figure 6 :
6
Figure 6: UniEval Overall Score of 146 questions (unconstrained by published date) from PubMedRS-200 distribution across Unrestricted Search (GPT3.5 and GPT4 zero-shot performance is added)</p>
<p>Table 1 :
1Unified Multi-Dimensional Evaluator (UniEval)CTC (SA)ModelCoherence ↑ Consistency ↑Fluency ↑Relevance ↑Overall ↑COMET ↑Consistency ↑Avg. LengthLLMGPT-3.50.908 (0.149) 0.694 (0.144) 0.947 (0.059) 0.939 (0.101) 0.872 (0.082) 0.676 (0.075) 0.865 (0.017) 104.834 (47.778)GPT-40.915 (0.099) 0.655 (0.145) 0.942 (0.051) 0.929 (0.078) 0.86 (0.062) 0.677 (0.075) 0.866 (0.017)84.214 (39.772)LLM + RetARestricted SearchSynthesis &amp; TL;DR 0.949 (0.065) 0.466 (0.105) 0.903 (0.104) 0.964 (0.053) 0.82 (0.055) 0.704 (0.055)0.84 (0.014)205.579(46.181)Synthesis0.925 (0.066)0.394 (0.11)0.893 (0.119) 0.939 (0.101) 0.788 (0.059) 0.693 (0.057) 0.842 (0.015) 165.814 (40.749)TL;DR0.866 (0.143) 0.787 (0.161) 0.954 (0.018) 0.826 (0.159) 0.858 (0.098) 0.665 (0.078) 0.874 (0.018)38.766 (11.682)Source DroppedSynthesis &amp; TL;DR 0.942 (0.092) 0.465 (0.104) 0.918 (0.085) 0.962 (0.059) 0.822 (0.055) 0.706 (0.056) 0.843 (0.014) 204.248 (38.394)Synthesis0.925 (0.066) 0.398 (0.112) 0.912 (0.096) 0.943 (0.055) 0.795 (0.055) 0.695 (0.059) 0.845 (0.016) 164.938 (33.221)TL;DR0.829 (0.202) 0.763 (0.197) 0.953 (0.029) 0.796 (0.194)0.835(0.13)0.672 (0.078) 0.876 (0.017)38.31 (10.726)Unrestricted SearchOur ModelsSynthesis &amp; TL;DR 0.945 (0.064) 0.539 (0.127) 0.912 (0.096) 0.962 (0.059) 0.84 (0.052) 0.721 (0.055) 0.852 (0.017) 214.338 (44.173)Synthesis0.916 (0.092)0.48 (0.142)0.904 (0.098) 0.935 (0.069) 0.809 (0.06) 0.712 (0.057) 0.855 (0.019) 173.379 (38.492)TL;DR0.896 (0.123)0.81 (0.159)0.955 (0.012) 0.857 (0.135) 0.88 (0.081) 0.681 (0.072)0.88 (0.016)39.959 (11.754)Deployed ModelsElicit [19]0.854 (0.136) 0.352 (0.147) 0.743 (0.151) 0.902 (0.117) 0.713 (0.085)0.7 (0.066)0.866 (0.017) 130.566 (22.946)Statpearls SS [23]0.753 (0.225) 0.383 (0.129)0.93 (0.053) 0.845 (0.159) 0.728 (0.112) 0.633 (0.075) 0.841 (0.016) 118.172 (26.603)
Performance on 146 questions from PubMedRS-200 using source-augmented (SA) metrics: UniEval (T5-large), COMET (XLM-RoBERTa), CTC summary (BERT)</p>
<p>Table 2 :
2
Performance on 146 questions from PubMedRS-200 using source-free (SF) metrics
ModelBERTScore ↑ ROUGE-L ↑METEOR ↑chrF ↑GoogleBLEU ↑CTC (SF) ↑CharacTer ↓Avg. LengthLLMGPT-3.50.781 (0.037) 0.165 (0.053) 0.181 (0.073) 30.2 (10.5)0.077 (0.036)0.575 (0.065) 0.912 (0.102) 104.834 (47.778)GPT-40.78 (0.037)0.157 (0.049) 0.192 (0.07) 31.6 (9.06)0.074 (0.031)0.571 (0.064) 0.89 (0.099)84.214 (39.772)LLM + RetARestricted SearchSynthesis &amp; TL;DR0.77 (0.028)0.135 (0.043) 0.121 (0.055) 21.5 (9.98)0.058 (0.03)0.527 (0.059) 0.993 (0.029) 205.579(46.181)Synthesis0.773 (0.028) 0.141 (0.044) 0.133 (0.059) 24.3 (10.4)0.063 (0.032)0.533 (0.06) 0.976 (0.056) 165.814 (40.749)TL;DR0.784 (0.041) 0.145 (0.068) 0.221 (0.089) 32.7 (7.67)0.061 (0.043)0.594 (0.068) 0.833 (0.086) 38.766 (11.682)Source DroppedSynthesis &amp; TL;DR 0.773 (0.028) 0.136 (0.037) 0.119 (0.054) 21.4 (9.69)0.057 (0.028)0.53 (0.06)0.989 (0.036) 204.248 (38.394)Synthesis0.775 (0.026) 0.143 (0.038) 0.132 (0.057) 24.1 (9.91)0.061 (0.043)0.536 (0.06) 0.976 (0.056) 164.938 (33.221)TL;DR0.787 (0.041) 0.148 (0.064) 0.218 (0.078)33 (6.98)0.06 (0.039)0.6 (0.066)0.83 (0.092)38.31 (10.726)Unrestricted SearchOur ModelsSynthesis &amp; TL;DR 0.786 (0.029)0.167 (0.06) 0.145 (0.073) 23.5 (11.2)0.079 (0.046)0.546 (0.067) 0.989 (0.036) 214.338 (44.173)Synthesis0.789 (0.03)0.178 (0.067) 0.164 (0.084)26.7 (12)0.088 (0.051)0.555 (0.07) 0.975 (0.065) 173.379 (38.492)TL;DR0.793 (0.038) 0.169 (0.076) 0.252 (0.092) 35.5 (7.95)0.076 (0.049)0.61 (0.067) 0.825 (0.094) 39.959 (11.754)Deployed ModelsElicit [19]
0.807 (0.04) 0.218 (0.095) 0.206 (0.093) 31.6 (12.5) 0.127 (0.085) 0.596 (0.07) 0.938 (0.096) 130.566 (22.946)Statpearls SS [23] 0.77 (0.028) 0.136 (0.037) 0.149 (0.057) 26.5 (9.8) 0.062 (0.026) 0.536 (0.06) 0.939 (0.09) 118.172 (26.603)</p>
<p>Table 3 :
3
Clinfo.ai Precision and Recall on PubMedRS-200
Evaluation RegimePrecision ↑Recall ↑Source IncludedRestricted Search0.224 (0.239) 0.057 (0.061)0.0 (0.0)Source Dropped0.186 (0.22)0.064 (0.064)0.0 (0.0)Unrestricted Search 0.162 (0.175) 0.052 (0.064)0.965 (0.185)4 Experimental Results and AnalysisIs RetA associated with significant improvements in automated metric evaluation?
https://github.com/som-shahlab/Clinfo.AI
https://github.com/som-shahlab/Clinfo.AI/tree/main/SupplementalMaterial
AcknowledgmentsAL is funded by Arc Institute.SF is supported by a Stanford Graduate Fellowship.This effort was supported in part by the Mark and Debra Leslie endowment for AI in Healthcare.We thank Will Haberkorn for his aid with FigureS1.
How to keep up to date with medical information using web-based resources: A systematised review and narrative synthesis. Konstantinos I Bougioukas, C Emmanouil, Bouras, Theodore Konstantinos I Avgerinos, Anna-Bettina Dardavessis, Haidich, Health Information &amp; Libraries Journal. 3742020</p>
<p>Scientific literature: Information overload. Esther Landhuis, Nature. 53576122016</p>
<p>An open source machine learning framework for efficient and transparent systematic reviews. Rens Van De Schoot, Jonathan De Bruin, Raoul Schram, Parisa Zahedi, Jan De Boer, Felix Weijdema, Bianca Kramer, Martijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands, Nature machine intelligence. 322021</p>
<p>Information-seeking behaviors of practitioners in a primary care practice-based research network (pbrn). Kevin A James E Andrews, Carol Pearce, Margaret M Ireson, Love, Journal of the Medical Library Association. 9322062005</p>
<p>Clinical questions raised by clinicians at the point of care: a systematic review. Guilherme Del Fiol, Elizabeth Workman, Paul N Gorman, JAMA internal medicine. 17452014</p>
<p>Clinical information seeking behavior of physicians: A systematic review. Azra Daei, Reza Mohammad, Hasan Soleymani, Ali Ashrafi-Rizi, Roya Zargham-Boroujeni, Kelishadi, International journal of medical informatics. 1391041442020</p>
<p>Answering physicians' clinical questions: obstacles and potential solutions. John W Ely, Jerome A Osheroff, Lee Chambliss, Mark H Ebell, Marcy E Rosenbaum, Journal of the American Medical Informatics Association. 1222005</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the prospero registry. Rohit Borah, Andrew W Brown, Patrice L Capers, Kathryn A Kaiser, BMJ open. 72e0125452017</p>
<p>Context-sensitive decision support (infobuttons) in electronic health records: a systematic review. Miguel T David A Cook, Bret Se Teixeira, James J Heale, Guilherme Cimino, Fiol Del, Journal of the American Medical Informatics Association. 2422017</p>
<p>Enabling health care decisionmaking through clinical decision support and knowledge management. David Lobach, Gillian D Sanders, Tiffani J Bright, Anthony Wong, Ravi Dhurjati, Erin Bristow, Lori Bastian, Remy Coeytaux, Gregory Samsa, Vic Hasselblad, Evidence report/technology assessment. 2032012</p>
<p>Association of a clinical knowledge support system with improved patient safety, reduced complications and shorter length of stay among medicare beneficiaries in acute care hospitals in the united states. Gary T Peter A Bonis, David M Pickens, David A Rind, Foster, International journal of medical informatics. 77112008</p>
<p>Use of uptodate and outcomes in us hospitals. Thomas Isaac, Jie Zheng, Ashish Jha, Journal of hospital medicine. 722012</p>
<p>Relationship of electronic medical knowledge resource use and practice characteristics with internal medicine maintenance of certification examination scores. Colin P Darcy A Reed, Eric S West, Andrew J Holmboe, Rebecca S Halvorsen, Carola Lipner, Jacobs, Furman, Mcdonald, Journal of general internal medicine. 272012</p>
<p>Supporting infobuttons with terminological knowledge. Gai James J Cimino, Qing Elhanan, Zeng, 1997</p>
<p>Consumer health information and question answering: helping consumers find answers to their health-related information needs. Dina Demner-Fushman, Yassine Mrabet, Asma Ben, Abacha , Journal of the American Medical Informatics Association. 2722020</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Qiao Jin, Robert Leaman, Zhiyong Lu, arXiv:2307.09683Pubmed and beyond: Recent advances and best practices in biomedical literature search. 2023arXiv preprint</p>
<p>Scite: A smart citation index that displays the context of citations and classifies their intent using deep learning. Milo Josh M Nicholson, Patrice Mordaunt, Ashish Lopez, Domenic Uppala, Rosati, P Neves, Peter Rodrigues, Sean C Grabitz, Rife, Quantitative Science Studies. 232021</p>
<p>Elicit: The ai research assistant. Ought, 2023</p>
<p>Glaciermd -a modern physician reference. Glaciermd, 2023</p>
<p>. Consensus, Consensus, 2023</p>
<p>Making medical knowledge more useful, open, accessible, and understandable. Openevidence, Openevidence, 2023</p>
<p>Hippocratic AI. statpearls semantic search. 2023</p>
<p>Chatgpt utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. Malik Sallam, Healthcare. 1162023</p>
<p>Nlg evaluation metrics beyond correlation analysis: An empirical metric preference checklist. Iftitahu Ni, ' Mah, Meng Fang, Vlado Menkovski, Mykola Pechenizkiy, arXiv:2305.085662023arXiv preprint</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.071972022arXiv preprint</p>
<p>Medalign: A clinician-generated dataset for instruction following with electronic medical records. Alejandro Scott L Fleming, William J Lozano, Jenelle A Haberkorn, Eduardo P Jindal, Rahul Reis, Louis Thapa, Julian Z Blankemeier, Ethan Genkins, Ashwin Steinberg, Nayak, arXiv:2308.140892023arXiv preprint</p>
<p>Fine-tuning language models to find agreement among humans with diverse preferences. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matt Botvinick, Advances in Neural Information Processing Systems. 202235</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chatgpt utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Malik Sallam, Healthcare. MDPI202311887</p>
<p>The role of chatgpt, generative language models, and artificial intelligence in medical education: a conversation with chatgpt and a call for papers. Gunther Eysenbach, JMIR Medical Education. 91e468852023</p>
<p>Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. Marco Cascella, Jonathan Montomoli, Valentina Bellini, Elena Bignami, Journal of Medical Systems. 471332023</p>
<p>Assessing the potential of usmle-like exam questions generated by gpt-4. medRxiv. Keith Scott L Fleming, Aswathi M Morse, Chia-Chun Kumar, Birju Chiang, Emma P Patel, Nigam Brunskill, Shah, 2023</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, PLoS digital health. 22e00001982023</p>
<p>Memory-based model editing at scale. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, Chelsea Finn, International Conference on Machine Learning. PMLR2022</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Anthropic, Claude, 2, 2023</p>
<p>Almanac: Retrieval-augmented language models for clinical medicine. William Hiesinger, Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex Dalal, Jennifer Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, 2023</p>
<p>David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline, Costa Sá, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh, Brandon W Higgs, arXiv:2305.17116Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model. 2023arXiv preprint</p>
<p>Database resources of the national center for biotechnology information in 2023. Eric W Sayers, Evan E Bolton, Rodney Brister, Kathi Canese, Jessica Chan, Donald C Comeau, Catherine M Farrell, Michael Feldgarden, Anna M Fine, Kathryn Funk, Nucleic acids research. 51D12023</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Generating better queries for systematic reviews. Harrisen Scells, Guido Zuccon, The 41st international ACM SIGIR conference on research &amp; development in information retrieval. 2018</p>
<p>Towards semantic-driven boolean query formalization for biomedical systematic literature reviews. Mohammadreza Pourreza, Faezeh Ensan, International Journal of Medical Informatics. 1049282022</p>
<p>Systematic reviews to support evidence-based medicine. Khalid Khan, Regina Kunz, Jos Kleijnen, Gerd Antes, 2011Crc press</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie Jun Cai, Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systems2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Improvements to bm25 and language models examined. Andrew Trotman, Antti Puurula, Blake Burgess, Proceedings of the 19th Australasian Document Computing Symposium. the 19th Australasian Document Computing Symposium2014</p>
<p>Comet: A neural framework for mt evaluation. Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie, arXiv:2009.090252020arXiv preprint</p>
<p>Compression, transduction, and creation: A unified framework for evaluating natural language generation. Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, Zhiting Hu, arXiv:2109.063792021arXiv preprint</p>
<p>From comet to comes-can summary evaluation benefit from translation evaluation?. Krubi0144ski Mateusz, Pavel Pecina, Proceedings of the 3rd Workshop on Evaluation and Comparison of NLP Systems. the 3rd Workshop on Evaluation and Comparison of NLP Systems2022</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. Maja Popović, Proceedings of the tenth workshop on statistical machine translation. the tenth workshop on statistical machine translation2015</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>CharacTer: Translation edit rate on character level. Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, Hermann Ney, Proceedings of the First Conference on Machine Translation. the First Conference on Machine TranslationBerlin, GermanyAssociation for Computational LinguisticsAugust 20162Shared Task Papers</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
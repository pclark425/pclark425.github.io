<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1085 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1085</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1085</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-e9ac398696129f657a409d64e2646b7fabfa3219</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e9ac398696129f657a409d64e2646b7fabfa3219" target="_blank">Benchmarking Reinforcement Learning Techniques for Autonomous Navigation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive study aimed at establishing to what extent can deep RL techniques achieve four major desiderata of applying deep RL approaches for autonomous navigation: reasoning under uncertainty, safety, learning from limited trial-and-error data, generalization to diverse and novel environments, and domain randomization.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and novel environments. Then, we explore four major classes of learning techniques with the purpose of achieving one or more of the four desiderata: memory-based neural network architectures (D1), safe RL (D2), model-based RL (D2, D3), and domain randomization (D4). By deploying these learning techniques in a new open-source large-scale navigation benchmark and real-world environments, we perform a comprehensive study aimed at establishing to what extent can these techniques achieve these desiderata for RL-based navigation systems.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1085.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1085.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLP-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilayer Perceptron baseline navigation policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vanilla feedforward neural network policy (MLP) trained with TD3 on the benchmark navigation tasks and used as the primary model-free baseline across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MLP (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>End-to-end reinforcement learning policy implemented as a multilayer perceptron trained with TD3 (model-free RL) to map LiDAR + relative goal observations to continuous linear and angular velocity commands.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (deployed to physical robot for real-world tests)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Benchmark navigation environments (static, dynamic-box, dynamic-wall) and real-world-1/2/3</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>10 m navigation tasks through highly-constrained obstacle courses simulated in Gazebo (static environments of varying difficulty; dynamic-box: many small boxes with random shapes & velocities; dynamic-wall: two moving walls creating timed/pass-or-wait decisions); also deployed in three real-world static environments (benchmark-like, indoor highly-constrained, large-scale 30 m).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Obstacle-field constrainedness (highly-constrained courses), environment type (static / dynamic-box / dynamic-wall), per-environment difficulty (300 static environments spanning easy→hard), 10 m path length and mandatory passage through obstacle field.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (benchmark focuses on highly-constrained obstacle courses; static set covers low→high difficulty but overall benchmark is high complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct environment instances used for training/test (static: 300 total; dynamic-box:100; dynamic-wall:100). Domain randomization experiments vary training set size: static-train-5/10/50/100/250 (i.e., 5→250 environment instances).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varies from low (5 training envs) to high (250 training envs); test sets are fixed (50 per type).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%) (primary), survival time (s) for failed episodes, traversal time (s) for successful episodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Static-test success rate: 65 ± 4% (H=1 baseline); Dynamic-box-test success rate: 50 ± 5% (H=1); Dynamic-wall-test success rate: 67 ± 7% (H=1). Real-world trials: baseline failed most trials across the three real-world environments (see Table III). Traversal time (successful): 7.5 ± 0.3 s; Survival time (failed): 8.0 ± 1.5 s.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper shows generalization improves with increased variation (# training environments) even for high-complexity tasks: success on unseen test environments rises monotonically with training set size (from 43% at 5 train envs to 74% at 250), indicating higher variation in training reduces the train-test gap; but higher complexity (highly-constrained tasks) still remains challenging and some methods (e.g., memory, MPC, classical planners) trade speed for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>43% success on unseen static-test when trained with only 5 training environments (domain-randomization experiment; high complexity & low variation)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>74% success on unseen static-test when trained with 250 training environments (domain-randomization experiment; high complexity & high variation)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-task RL across multiple environment instances sampled uniformly (p(T_e) = uniform on training set). No curriculum; domain randomization by increasing training-instance count used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization to unseen test environments improves as number of distinct training environments increases (success rate on static-test: 43% @5 envs → 74% @250 envs); baseline MLP trained with 250 envs succeeded across real-world tests while MLP trained with 50 envs failed most real-world trials.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Measured in collected transition samples: baseline MLP achieved 13 ± 7% success after 100k transitions, 58 ± 2% after 500k, and 65 ± 4% after 2,000k transition samples (reported in model-sample-efficiency experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard model-free MLP policies achieve moderate success (≈65% in static-test) but generalization to unseen high-complexity environments strongly depends on the number of distinct training environments; sample efficiency is modest — performance improves substantially with >500k transition samples; baseline is fast in traversal time but poor in survival time compared to safe/model-based/classical planners.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1085.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-H4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based policy with history length 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A history-based Transformer policy (history length 4) trained with TD3; performed best among tested architectures in static environments in simulation and showed improved smoothness in real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer (history=4)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent/history-encoding policy using a Transformer architecture over a short observation/action history (length 4) trained with TD3 (model-free RL) to handle partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (also deployed to physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static, dynamic-box, dynamic-wall benchmark environments and real-world environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same 10 m highly-constrained navigation tasks; history-aware agent intended to reason under partial observability and dynamic obstacle motion.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Environment type (static vs dynamic), requirement to reason about obstacle motion for dynamic-wall; partial observability requiring history encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for benchmark tasks; particularly beneficial in real-world and designed dynamic challenges (dynamic-wall).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained on static-train-50, dynamic-box-train, dynamic-wall-train; history length varied (4 vs 8) as explicit experimental variable.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (trained on 50 static envs for these architecture comparisons); variation also across dynamic environment types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%) on static-test / dynamic-box-test / dynamic-wall-test; also reported real-world traversal successes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Static-test success: 68 ± 2% (history 4) — best in static simulation experiments; Dynamic-box-test: 52 ± 1% (H=4) — best among architectures; Dynamic-wall-test: 33 ± 28% (H=4) — poor and high variance in simulation; real world: Transformer-H4 navigated smoothly and failed only once across real-world-2/3 deployments (outperformed MLP baseline in physical tests).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Increasing history length from 4→8 decreased success (Transformer: 68% → 46% in static), suggesting that adding more memory can hurt generalization in diverse training sets; architectures that exploit short history can improve static performance but may be sensitive to environment variation and overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Not directly reported in this exact grouping; architecture bench used static-train-50 (medium variation).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>When trained on richer/more realistic data (deployed to real-world), Transformer-H4 performed better than simulation numbers suggest (few real-world failures and smooth navigation), indicating history-based models can be more valuable when real-world unpredictability increases.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-task training on fixed training sets (static-train-50, dynamic-train sets); architecture comparison rather than curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Transformer-H4 had best static-test success among architectures (68%) but degraded with longer histories; in the real world it generalized better than the simulation gap suggested, likely due to real-world predictability differences and the value of short histories.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same TD3 training regime as other architectures; no separate sample-efficiency advantage reported relative to MLP in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer with short history (4) gave the best simulated static performance and was effective in the real world, but adding more history degraded performance in diverse training sets; memory helps in some dynamic/challenging cases but can harm generalization if history length is increased indiscriminately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1085.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU-H4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Recurrent Unit policy with history length 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A history-encoding recurrent policy using GRU cells trained with TD3; showed the best performance in the designed dynamic-wall environment where longer-term prediction matters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GRU (history=4)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent policy using GRU to encode a short sequence of past observations/actions, trained with TD3 to act under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (also deployed to physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static, dynamic-box, dynamic-wall benchmark environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Dynamic-wall requires estimating long-term motion of two walls moving together/apart; dynamic-box contains many small random moving obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Environment dynamics complexity (designed dynamic-wall requires temporal estimation), partial observability (history needed), obstacle velocities and motion patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high for dynamic-wall (temporal decision-making required); medium-high for other benchmark types.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained on static-train-50, dynamic-wall-train, dynamic-box-train with different history lengths evaluated (H=4/8).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (training sets of 50 or respective dynamic-train sets); dynamic-box is highly random across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dynamic-wall-test success: 82 ± 4% (GRU, H=4) — ~15% improvement over non-memory baseline; Static-test (not provided for H=1) but H=4 shows 51 ± 2% (see Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Memory (GRU) is essential in structured dynamic challenges (dynamic-wall) where anticipating motion yields higher success; in highly-random dynamic environments (dynamic-box) memory provided little benefit because reactive short-range avoidance sufficed.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>82 ± 4% success in dynamic-wall (designed high-complexity dynamic) when trained and tested on dynamic-wall (medium variation but structured dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-task training on respective dynamic-train sets (no curriculum); architecture comparison to evaluate memory impact.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>GRU substantially improved success on dynamic-wall (structured dynamic) but did not outperform baseline on highly-random dynamic-box environments; real-world deployment also showed importance of memory.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not singled out; trained with same TD3 regime and data budgets as other architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory is crucial when long-term temporal reasoning is required (dynamic-wall and real-world unpredictability), but in highly stochastic short-term dynamics (dynamic-box) reactive memoryless policies can perform similarly; adding more history beyond a small window often decreased performance in diverse training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1085.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lagrangian-SafeRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lagrangian-based safe reinforcement learning agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A TD3 agent augmented with a Lagrangian constraint to explicitly treat collision avoidance as a separate objective, improving safety and generalization compared to the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Lagrangian safe RL (TD3 + Lagrangian constraint)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free RL (TD3) trained with a Lagrangian formulation that penalizes safety-constraint violations (collisions) as a separate constrained objective to encourage safer policies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (also deployed to physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static benchmark environments and real-world deployments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly-constrained static obstacle courses (static-train-50 / static-test) where safety (collision avoidance) is critical; Lagrangian objective enforces collision penalty as constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Environment complexity as highly-constrained static obstacle courses; safety constraint satisfaction measured by collisions and survival time.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (collision avoidance under partial observability in constrained spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained on static-train-50; tested on static-test (50 unseen static environments) and deployed to real-world envs.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (50 training environments), generalization to unseen static-test evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%), survival time (s) in failed episodes, traversal time (s) in successful episodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Static-test success rate: 74 ± 2% (Lagrangian) vs baseline 65 ± 4%; Survival time (failed): 16.2 ± 2.5 s (Lagrangian) vs baseline 8.0 ± 1.5 s; Traversal time (successful): 8.6 ± 0.2 s (Lagrangian) vs baseline 7.5 ± 0.3 s.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Lagrangian safety constraint acts as a regularizer improving generalization from train → test (reduces train-test gap) in high-complexity settings; however, safety improvements come with a small traversal time cost (≈+1.1 s) compared to baseline. Classical planners still have superior safety but at much higher traversal time.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>74 ± 2% success on unseen static-test when trained on static-train-50 (structured high-complexity, medium variation); improved generalization vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single multi-task training on static-train-50 with Lagrangian constrained RL objective (no domain randomization beyond the training set).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Lagrangian safe RL generalized better than baseline from training to test (test success 74% vs baseline 65%) and increased survival time in failed episodes, suggesting safety constraints improve robustness/generalization to unseen high-complexity environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained under same TD3 data regimes; no explicit sample-efficiency improvements reported vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly optimizing a safety objective (Lagrangian) improves both safety (longer survival time) and generalization to unseen environments at a modest cost in traversal time; nevertheless classical methods (DWA) still achieve higher survival times albeit with much longer traversal times.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1085.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPC-prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Predictive Control with probabilistic learned dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based control approach using a probabilistic transition model within an MPC planner; yields conservative but safer navigation with improved survival times at the cost of much longer traversal time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPC (probabilistic dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model-based RL approach where a probabilistic neural network predicts mean and variance of next states; an MPC planner uses the learned model for short-horizon planning. Evaluated in static environments and in real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (also deployed physically)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static benchmark environments and real-world-1/2/3</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly-constrained static obstacle courses; MPC reasons about trajectories using learned probabilistic dynamics, favoring conservative plans in unseen environments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Environment deterministic/stochastic dynamics, need for trajectory-level reasoning, and transition-sample budgets (100k/500k/2000k) used to measure model learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (structured constrained navigation requiring cautious long-horizon decisions).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained/tested on static-train-50/static-test; model-based experiments evaluated at different amounts of real transition samples (100k, 500k, 2000k).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (50 training envs) with experiments varying data amounts (low→high sample budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%), survival time (s), traversal time (s)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Static-test success rate (model-based experiments): MPC probabilistic achieved 0% at 100k, 45 ± 4% at 500k, and 70 ± 3% at 2000k (Table IV). In safety comparison (Table II): success 70 ± 3%, survival time 55.7 ± 4.9 s, traversal time 24.7 ± 2.0 s. Real-world: MPC succeeded in all trials in real-world-1 & real-world-2 but struggled in large-scale real-world-3.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>MPC with probabilistic models becomes effective only with large amounts of real transition data (≈2000k) and yields conservative behavior: higher survival times (improved safety) but much longer traversal times — explicit trade-off between safety and efficiency. Probabilistic models outperform deterministic ones at medium→high data budgets, indicating that modeling uncertainty helps cope with environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>70 ± 3% success on static-test given 2000k transition samples (probabilistic MPC) — shows good asymptotic performance under high sample budgets in high-complexity settings with medium variation.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Model-based learning with MPC controller; trained on collected transition samples (100k/500k/2000k) and tested; no curriculum/domain-randomization beyond training set sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>MPC generalizes conservatively to unseen test environments, achieving improved survival time and competitive success when sufficient transition samples are available; however, conservatism leads to much longer traversal times and difficulty generalizing to very large-scale unseen real-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Poor at very low data budgets (0% success at 100k), moderate at 500k (≈45%), good at large budgets (≈70% at 2000k). Probabilistic models yielded better sample-efficiency than deterministic ones at intermediate budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model-based MPC with probabilistic dynamics improves safety and asymptotic performance when provided with large numbers of transition samples (≈2M), but is conservative, trading traversal time for safety; probabilistic modeling helps with variation, deterministic models underperform at smaller data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1085.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Based Policy Optimization (MBPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL method that heavily leverages imagined rollouts from a learned dynamics model; in this benchmark MBPO underperformed asymptotically, likely due to over-reliance on imperfect models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MBPO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A modern model-based RL algorithm that augments model-free learning with many short imagined rollouts from a learned dynamics model to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static benchmark environments (static-train-50 / static-test)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly-constrained static navigation tasks; MBPO uses learned dynamics to generate additional training data (imaginary rollouts).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of imaginary rollouts and transition-sample budgets; sensitivity to learned-model accuracy in complex obstacle navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (benchmark obstacle courses); MBPO struggles when model inaccuracies accumulate in complex navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Evaluated with 100k/500k/2000k real transition samples; MBPO leverages many imaginary rollouts sampled from learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium data-variation experiments; environment-instance variation same as other model-based tests (static-train-50).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate: 0 ± 0% at 100k and 500k transitions; 21.9 ± 3% at 2000k transition samples (Table IV) — poor asymptotic performance compared to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>MBPO did not improve sample-efficiency or asymptotic performance in these high-complexity navigation tasks; heavy reliance on imagined rollouts from imperfect models degraded performance, suggesting a negative interaction between model-generated variation and environment complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>21.9 ± 3% success at 2000k transitions on static-test (high complexity, moderate variation) — substantially worse than other model-based/probabilistic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Model-based with heavy imaginary rollouts (MBPO-style) combined with model-free fine-tuning; no domain-randomization beyond training set.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>MBPO showed poor generalization and low success rates even at high data budgets compared to probabilistic MPC/Dyna-style approaches, indicating issues with model bias in complex navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No apparent sample-efficiency advantage; failed at low budgets (0% at 100k/500k) and only obtained 21.9% success at 2000k transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MBPO's heavy use of imagined rollouts hurt performance in complex, safety-critical navigation; accurate probabilistic dynamics models and careful use of model-generated data are necessary for model-based gains in these environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1085.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1085.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainRandomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain randomization / multi-environment training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training strategy that increases the number of distinct training environment instances (domain randomization) to improve generalization to unseen navigation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Policies trained with domain randomization (varying #training environments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free policies (MLP baseline in experiments) trained across multiple distinct environment instances sampled uniformly from the benchmark (static-train-N with N in {5,10,50,100,250}) to increase exposure and variation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated training leading to simulated and physical agents</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Static benchmark environments (static-train-N, static-test)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static obstacle courses sampled from a large set (300 static envs) with diverse difficulty; domain randomization experiments control N, the number of distinct training instances (5→250) to measure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity held by benchmark (highly-constrained courses); variation controlled by number of distinct training instances N.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (base tasks are highly-constrained); domain-randomization studies focus on effect of variation, keeping base complexity high.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of training environment instances N (explicitly: 5, 10, 50, 100, 250). Also tested whether policies perform on training subset (static-train-50) vs unseen static-test.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low (N=5) → high (N=250).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (%) on unseen static-test and on training environments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Unseen static-test success rates (MLP): 43% @ N=5 → monotonically increasing to 74% @ N=250. Training-set performance reached ~80% for sufficiently large N; train-test gap shrinks with increasing N but ~5% gap remained at N=250.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper explicitly reports that increasing environment variation (more training environments) monotonically improves generalization on high-complexity tasks, but cannot fully eliminate the train-test gap even at 250 training instances (≈5% residual gap). This demonstrates a trade-off: more variation improves generalization but there are diminishing returns and residual generalization error remains in high-complexity navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>43% success on unseen static-test when N=5 (high task complexity with low variation exposure).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>74% success on unseen static-test when N=250 (high complexity with high variation exposure).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Domain randomization / multi-environment training (uniform sampling of tasks from a set of N training environments). No curriculum; N varied to measure dependence of generalization on variation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization to unseen static-test environments improves monotonically with the number of training environments; gap between train and test shrinks with more variation but a ≈5% residual test gap remained at 250 training environments, indicating limitations of simple domain randomization in fully closing generalization gaps for high-complexity navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not directly a sample-efficiency method; more training environments imply more diverse experiences required. Specific sample counts per N not enumerated, but reported success rates reflect policies trained to convergence for each N.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Increasing the number of distinct training environments (domain randomization) consistently improves out-of-distribution performance on highly-constrained navigation tasks, but shows diminishing returns and cannot fully eliminate generalization gaps; suggests necessity for better generalization methods beyond naive environment-count scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Reinforcement Learning Techniques for Autonomous Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dyna, an integrated architecture for learning, planning, and reacting <em>(Rating: 2)</em></li>
                <li>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Quantifying generalization in reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1085",
    "paper_id": "paper-e9ac398696129f657a409d64e2646b7fabfa3219",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "MLP-baseline",
            "name_full": "Multilayer Perceptron baseline navigation policy",
            "brief_description": "A vanilla feedforward neural network policy (MLP) trained with TD3 on the benchmark navigation tasks and used as the primary model-free baseline across experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MLP (baseline)",
            "agent_description": "End-to-end reinforcement learning policy implemented as a multilayer perceptron trained with TD3 (model-free RL) to map LiDAR + relative goal observations to continuous linear and angular velocity commands.",
            "agent_type": "simulated agent (deployed to physical robot for real-world tests)",
            "environment_name": "Benchmark navigation environments (static, dynamic-box, dynamic-wall) and real-world-1/2/3",
            "environment_description": "10 m navigation tasks through highly-constrained obstacle courses simulated in Gazebo (static environments of varying difficulty; dynamic-box: many small boxes with random shapes & velocities; dynamic-wall: two moving walls creating timed/pass-or-wait decisions); also deployed in three real-world static environments (benchmark-like, indoor highly-constrained, large-scale 30 m).",
            "complexity_measure": "Obstacle-field constrainedness (highly-constrained courses), environment type (static / dynamic-box / dynamic-wall), per-environment difficulty (300 static environments spanning easy→hard), 10 m path length and mandatory passage through obstacle field.",
            "complexity_level": "high (benchmark focuses on highly-constrained obstacle courses; static set covers low→high difficulty but overall benchmark is high complexity)",
            "variation_measure": "Number of distinct environment instances used for training/test (static: 300 total; dynamic-box:100; dynamic-wall:100). Domain randomization experiments vary training set size: static-train-5/10/50/100/250 (i.e., 5→250 environment instances).",
            "variation_level": "varies from low (5 training envs) to high (250 training envs); test sets are fixed (50 per type).",
            "performance_metric": "Success rate (%) (primary), survival time (s) for failed episodes, traversal time (s) for successful episodes",
            "performance_value": "Static-test success rate: 65 ± 4% (H=1 baseline); Dynamic-box-test success rate: 50 ± 5% (H=1); Dynamic-wall-test success rate: 67 ± 7% (H=1). Real-world trials: baseline failed most trials across the three real-world environments (see Table III). Traversal time (successful): 7.5 ± 0.3 s; Survival time (failed): 8.0 ± 1.5 s.",
            "complexity_variation_relationship": "Paper shows generalization improves with increased variation (# training environments) even for high-complexity tasks: success on unseen test environments rises monotonically with training set size (from 43% at 5 train envs to 74% at 250), indicating higher variation in training reduces the train-test gap; but higher complexity (highly-constrained tasks) still remains challenging and some methods (e.g., memory, MPC, classical planners) trade speed for safety.",
            "high_complexity_low_variation_performance": "43% success on unseen static-test when trained with only 5 training environments (domain-randomization experiment; high complexity & low variation)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "74% success on unseen static-test when trained with 250 training environments (domain-randomization experiment; high complexity & high variation)",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-task RL across multiple environment instances sampled uniformly (p(T_e) = uniform on training set). No curriculum; domain randomization by increasing training-instance count used in experiments.",
            "generalization_tested": true,
            "generalization_results": "Generalization to unseen test environments improves as number of distinct training environments increases (success rate on static-test: 43% @5 envs → 74% @250 envs); baseline MLP trained with 250 envs succeeded across real-world tests while MLP trained with 50 envs failed most real-world trials.",
            "sample_efficiency": "Measured in collected transition samples: baseline MLP achieved 13 ± 7% success after 100k transitions, 58 ± 2% after 500k, and 65 ± 4% after 2,000k transition samples (reported in model-sample-efficiency experiments).",
            "key_findings": "Standard model-free MLP policies achieve moderate success (≈65% in static-test) but generalization to unseen high-complexity environments strongly depends on the number of distinct training environments; sample efficiency is modest — performance improves substantially with &gt;500k transition samples; baseline is fast in traversal time but poor in survival time compared to safe/model-based/classical planners.",
            "uuid": "e1085.0",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Transformer-H4",
            "name_full": "Transformer-based policy with history length 4",
            "brief_description": "A history-based Transformer policy (history length 4) trained with TD3; performed best among tested architectures in static environments in simulation and showed improved smoothness in real-world deployment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Transformer (history=4)",
            "agent_description": "Recurrent/history-encoding policy using a Transformer architecture over a short observation/action history (length 4) trained with TD3 (model-free RL) to handle partial observability.",
            "agent_type": "simulated agent (also deployed to physical robot)",
            "environment_name": "Static, dynamic-box, dynamic-wall benchmark environments and real-world environments",
            "environment_description": "Same 10 m highly-constrained navigation tasks; history-aware agent intended to reason under partial observability and dynamic obstacle motion.",
            "complexity_measure": "Environment type (static vs dynamic), requirement to reason about obstacle motion for dynamic-wall; partial observability requiring history encoding.",
            "complexity_level": "high for benchmark tasks; particularly beneficial in real-world and designed dynamic challenges (dynamic-wall).",
            "variation_measure": "Trained on static-train-50, dynamic-box-train, dynamic-wall-train; history length varied (4 vs 8) as explicit experimental variable.",
            "variation_level": "medium (trained on 50 static envs for these architecture comparisons); variation also across dynamic environment types.",
            "performance_metric": "Success rate (%) on static-test / dynamic-box-test / dynamic-wall-test; also reported real-world traversal successes",
            "performance_value": "Static-test success: 68 ± 2% (history 4) — best in static simulation experiments; Dynamic-box-test: 52 ± 1% (H=4) — best among architectures; Dynamic-wall-test: 33 ± 28% (H=4) — poor and high variance in simulation; real world: Transformer-H4 navigated smoothly and failed only once across real-world-2/3 deployments (outperformed MLP baseline in physical tests).",
            "complexity_variation_relationship": "Increasing history length from 4→8 decreased success (Transformer: 68% → 46% in static), suggesting that adding more memory can hurt generalization in diverse training sets; architectures that exploit short history can improve static performance but may be sensitive to environment variation and overfitting.",
            "high_complexity_low_variation_performance": "Not directly reported in this exact grouping; architecture bench used static-train-50 (medium variation).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "When trained on richer/more realistic data (deployed to real-world), Transformer-H4 performed better than simulation numbers suggest (few real-world failures and smooth navigation), indicating history-based models can be more valuable when real-world unpredictability increases.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-task training on fixed training sets (static-train-50, dynamic-train sets); architecture comparison rather than curriculum.",
            "generalization_tested": true,
            "generalization_results": "Transformer-H4 had best static-test success among architectures (68%) but degraded with longer histories; in the real world it generalized better than the simulation gap suggested, likely due to real-world predictability differences and the value of short histories.",
            "sample_efficiency": "Same TD3 training regime as other architectures; no separate sample-efficiency advantage reported relative to MLP in the paper.",
            "key_findings": "Transformer with short history (4) gave the best simulated static performance and was effective in the real world, but adding more history degraded performance in diverse training sets; memory helps in some dynamic/challenging cases but can harm generalization if history length is increased indiscriminately.",
            "uuid": "e1085.1",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GRU-H4",
            "name_full": "Gated Recurrent Unit policy with history length 4",
            "brief_description": "A history-encoding recurrent policy using GRU cells trained with TD3; showed the best performance in the designed dynamic-wall environment where longer-term prediction matters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GRU (history=4)",
            "agent_description": "Recurrent policy using GRU to encode a short sequence of past observations/actions, trained with TD3 to act under partial observability.",
            "agent_type": "simulated agent (also deployed to physical robot)",
            "environment_name": "Static, dynamic-box, dynamic-wall benchmark environments",
            "environment_description": "Dynamic-wall requires estimating long-term motion of two walls moving together/apart; dynamic-box contains many small random moving obstacles.",
            "complexity_measure": "Environment dynamics complexity (designed dynamic-wall requires temporal estimation), partial observability (history needed), obstacle velocities and motion patterns.",
            "complexity_level": "high for dynamic-wall (temporal decision-making required); medium-high for other benchmark types.",
            "variation_measure": "Trained on static-train-50, dynamic-wall-train, dynamic-box-train with different history lengths evaluated (H=4/8).",
            "variation_level": "medium (training sets of 50 or respective dynamic-train sets); dynamic-box is highly random across episodes.",
            "performance_metric": "Success rate (%)",
            "performance_value": "Dynamic-wall-test success: 82 ± 4% (GRU, H=4) — ~15% improvement over non-memory baseline; Static-test (not provided for H=1) but H=4 shows 51 ± 2% (see Table I).",
            "complexity_variation_relationship": "Memory (GRU) is essential in structured dynamic challenges (dynamic-wall) where anticipating motion yields higher success; in highly-random dynamic environments (dynamic-box) memory provided little benefit because reactive short-range avoidance sufficed.",
            "high_complexity_low_variation_performance": "82 ± 4% success in dynamic-wall (designed high-complexity dynamic) when trained and tested on dynamic-wall (medium variation but structured dynamics).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-task training on respective dynamic-train sets (no curriculum); architecture comparison to evaluate memory impact.",
            "generalization_tested": true,
            "generalization_results": "GRU substantially improved success on dynamic-wall (structured dynamic) but did not outperform baseline on highly-random dynamic-box environments; real-world deployment also showed importance of memory.",
            "sample_efficiency": "Not singled out; trained with same TD3 regime and data budgets as other architectures.",
            "key_findings": "Memory is crucial when long-term temporal reasoning is required (dynamic-wall and real-world unpredictability), but in highly stochastic short-term dynamics (dynamic-box) reactive memoryless policies can perform similarly; adding more history beyond a small window often decreased performance in diverse training sets.",
            "uuid": "e1085.2",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Lagrangian-SafeRL",
            "name_full": "Lagrangian-based safe reinforcement learning agent",
            "brief_description": "A TD3 agent augmented with a Lagrangian constraint to explicitly treat collision avoidance as a separate objective, improving safety and generalization compared to the baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Lagrangian safe RL (TD3 + Lagrangian constraint)",
            "agent_description": "Model-free RL (TD3) trained with a Lagrangian formulation that penalizes safety-constraint violations (collisions) as a separate constrained objective to encourage safer policies.",
            "agent_type": "simulated agent (also deployed to physical robot)",
            "environment_name": "Static benchmark environments and real-world deployments",
            "environment_description": "Highly-constrained static obstacle courses (static-train-50 / static-test) where safety (collision avoidance) is critical; Lagrangian objective enforces collision penalty as constraint.",
            "complexity_measure": "Environment complexity as highly-constrained static obstacle courses; safety constraint satisfaction measured by collisions and survival time.",
            "complexity_level": "high (collision avoidance under partial observability in constrained spaces).",
            "variation_measure": "Trained on static-train-50; tested on static-test (50 unseen static environments) and deployed to real-world envs.",
            "variation_level": "medium (50 training environments), generalization to unseen static-test evaluated.",
            "performance_metric": "Success rate (%), survival time (s) in failed episodes, traversal time (s) in successful episodes",
            "performance_value": "Static-test success rate: 74 ± 2% (Lagrangian) vs baseline 65 ± 4%; Survival time (failed): 16.2 ± 2.5 s (Lagrangian) vs baseline 8.0 ± 1.5 s; Traversal time (successful): 8.6 ± 0.2 s (Lagrangian) vs baseline 7.5 ± 0.3 s.",
            "complexity_variation_relationship": "Lagrangian safety constraint acts as a regularizer improving generalization from train → test (reduces train-test gap) in high-complexity settings; however, safety improvements come with a small traversal time cost (≈+1.1 s) compared to baseline. Classical planners still have superior safety but at much higher traversal time.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "74 ± 2% success on unseen static-test when trained on static-train-50 (structured high-complexity, medium variation); improved generalization vs baseline.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single multi-task training on static-train-50 with Lagrangian constrained RL objective (no domain randomization beyond the training set).",
            "generalization_tested": true,
            "generalization_results": "Lagrangian safe RL generalized better than baseline from training to test (test success 74% vs baseline 65%) and increased survival time in failed episodes, suggesting safety constraints improve robustness/generalization to unseen high-complexity environments.",
            "sample_efficiency": "Trained under same TD3 data regimes; no explicit sample-efficiency improvements reported vs baseline.",
            "key_findings": "Explicitly optimizing a safety objective (Lagrangian) improves both safety (longer survival time) and generalization to unseen environments at a modest cost in traversal time; nevertheless classical methods (DWA) still achieve higher survival times albeit with much longer traversal times.",
            "uuid": "e1085.3",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "MPC-prob",
            "name_full": "Model Predictive Control with probabilistic learned dynamics",
            "brief_description": "A model-based control approach using a probabilistic transition model within an MPC planner; yields conservative but safer navigation with improved survival times at the cost of much longer traversal time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MPC (probabilistic dynamics)",
            "agent_description": "A model-based RL approach where a probabilistic neural network predicts mean and variance of next states; an MPC planner uses the learned model for short-horizon planning. Evaluated in static environments and in real-world deployment.",
            "agent_type": "simulated agent (also deployed physically)",
            "environment_name": "Static benchmark environments and real-world-1/2/3",
            "environment_description": "Highly-constrained static obstacle courses; MPC reasons about trajectories using learned probabilistic dynamics, favoring conservative plans in unseen environments.",
            "complexity_measure": "Environment deterministic/stochastic dynamics, need for trajectory-level reasoning, and transition-sample budgets (100k/500k/2000k) used to measure model learning progress.",
            "complexity_level": "high (structured constrained navigation requiring cautious long-horizon decisions).",
            "variation_measure": "Trained/tested on static-train-50/static-test; model-based experiments evaluated at different amounts of real transition samples (100k, 500k, 2000k).",
            "variation_level": "medium (50 training envs) with experiments varying data amounts (low→high sample budgets).",
            "performance_metric": "Success rate (%), survival time (s), traversal time (s)",
            "performance_value": "Static-test success rate (model-based experiments): MPC probabilistic achieved 0% at 100k, 45 ± 4% at 500k, and 70 ± 3% at 2000k (Table IV). In safety comparison (Table II): success 70 ± 3%, survival time 55.7 ± 4.9 s, traversal time 24.7 ± 2.0 s. Real-world: MPC succeeded in all trials in real-world-1 & real-world-2 but struggled in large-scale real-world-3.",
            "complexity_variation_relationship": "MPC with probabilistic models becomes effective only with large amounts of real transition data (≈2000k) and yields conservative behavior: higher survival times (improved safety) but much longer traversal times — explicit trade-off between safety and efficiency. Probabilistic models outperform deterministic ones at medium→high data budgets, indicating that modeling uncertainty helps cope with environment variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "70 ± 3% success on static-test given 2000k transition samples (probabilistic MPC) — shows good asymptotic performance under high sample budgets in high-complexity settings with medium variation.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Model-based learning with MPC controller; trained on collected transition samples (100k/500k/2000k) and tested; no curriculum/domain-randomization beyond training set sampling.",
            "generalization_tested": true,
            "generalization_results": "MPC generalizes conservatively to unseen test environments, achieving improved survival time and competitive success when sufficient transition samples are available; however, conservatism leads to much longer traversal times and difficulty generalizing to very large-scale unseen real-world environments.",
            "sample_efficiency": "Poor at very low data budgets (0% success at 100k), moderate at 500k (≈45%), good at large budgets (≈70% at 2000k). Probabilistic models yielded better sample-efficiency than deterministic ones at intermediate budgets.",
            "key_findings": "Model-based MPC with probabilistic dynamics improves safety and asymptotic performance when provided with large numbers of transition samples (≈2M), but is conservative, trading traversal time for safety; probabilistic modeling helps with variation, deterministic models underperform at smaller data budgets.",
            "uuid": "e1085.4",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "MBPO",
            "name_full": "Model-Based Policy Optimization (MBPO)",
            "brief_description": "A model-based RL method that heavily leverages imagined rollouts from a learned dynamics model; in this benchmark MBPO underperformed asymptotically, likely due to over-reliance on imperfect models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MBPO",
            "agent_description": "A modern model-based RL algorithm that augments model-free learning with many short imagined rollouts from a learned dynamics model to improve sample efficiency.",
            "agent_type": "simulated agent",
            "environment_name": "Static benchmark environments (static-train-50 / static-test)",
            "environment_description": "Highly-constrained static navigation tasks; MBPO uses learned dynamics to generate additional training data (imaginary rollouts).",
            "complexity_measure": "Number of imaginary rollouts and transition-sample budgets; sensitivity to learned-model accuracy in complex obstacle navigation.",
            "complexity_level": "high (benchmark obstacle courses); MBPO struggles when model inaccuracies accumulate in complex navigation.",
            "variation_measure": "Evaluated with 100k/500k/2000k real transition samples; MBPO leverages many imaginary rollouts sampled from learned model.",
            "variation_level": "medium data-variation experiments; environment-instance variation same as other model-based tests (static-train-50).",
            "performance_metric": "Success rate (%)",
            "performance_value": "Success rate: 0 ± 0% at 100k and 500k transitions; 21.9 ± 3% at 2000k transition samples (Table IV) — poor asymptotic performance compared to other methods.",
            "complexity_variation_relationship": "MBPO did not improve sample-efficiency or asymptotic performance in these high-complexity navigation tasks; heavy reliance on imagined rollouts from imperfect models degraded performance, suggesting a negative interaction between model-generated variation and environment complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "21.9 ± 3% success at 2000k transitions on static-test (high complexity, moderate variation) — substantially worse than other model-based/probabilistic approaches.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Model-based with heavy imaginary rollouts (MBPO-style) combined with model-free fine-tuning; no domain-randomization beyond training set.",
            "generalization_tested": true,
            "generalization_results": "MBPO showed poor generalization and low success rates even at high data budgets compared to probabilistic MPC/Dyna-style approaches, indicating issues with model bias in complex navigation.",
            "sample_efficiency": "No apparent sample-efficiency advantage; failed at low budgets (0% at 100k/500k) and only obtained 21.9% success at 2000k transitions.",
            "key_findings": "MBPO's heavy use of imagined rollouts hurt performance in complex, safety-critical navigation; accurate probabilistic dynamics models and careful use of model-generated data are necessary for model-based gains in these environments.",
            "uuid": "e1085.5",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "DomainRandomization",
            "name_full": "Domain randomization / multi-environment training",
            "brief_description": "Training strategy that increases the number of distinct training environment instances (domain randomization) to improve generalization to unseen navigation environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Policies trained with domain randomization (varying #training environments)",
            "agent_description": "Model-free policies (MLP baseline in experiments) trained across multiple distinct environment instances sampled uniformly from the benchmark (static-train-N with N in {5,10,50,100,250}) to increase exposure and variation during training.",
            "agent_type": "simulated training leading to simulated and physical agents",
            "environment_name": "Static benchmark environments (static-train-N, static-test)",
            "environment_description": "Static obstacle courses sampled from a large set (300 static envs) with diverse difficulty; domain randomization experiments control N, the number of distinct training instances (5→250) to measure generalization.",
            "complexity_measure": "Complexity held by benchmark (highly-constrained courses); variation controlled by number of distinct training instances N.",
            "complexity_level": "high (base tasks are highly-constrained); domain-randomization studies focus on effect of variation, keeping base complexity high.",
            "variation_measure": "Number of training environment instances N (explicitly: 5, 10, 50, 100, 250). Also tested whether policies perform on training subset (static-train-50) vs unseen static-test.",
            "variation_level": "low (N=5) → high (N=250).",
            "performance_metric": "Success rate (%) on unseen static-test and on training environments",
            "performance_value": "Unseen static-test success rates (MLP): 43% @ N=5 → monotonically increasing to 74% @ N=250. Training-set performance reached ~80% for sufficiently large N; train-test gap shrinks with increasing N but ~5% gap remained at N=250.",
            "complexity_variation_relationship": "Paper explicitly reports that increasing environment variation (more training environments) monotonically improves generalization on high-complexity tasks, but cannot fully eliminate the train-test gap even at 250 training instances (≈5% residual gap). This demonstrates a trade-off: more variation improves generalization but there are diminishing returns and residual generalization error remains in high-complexity navigation.",
            "high_complexity_low_variation_performance": "43% success on unseen static-test when N=5 (high task complexity with low variation exposure).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "74% success on unseen static-test when N=250 (high complexity with high variation exposure).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Domain randomization / multi-environment training (uniform sampling of tasks from a set of N training environments). No curriculum; N varied to measure dependence of generalization on variation.",
            "generalization_tested": true,
            "generalization_results": "Generalization to unseen static-test environments improves monotonically with the number of training environments; gap between train and test shrinks with more variation but a ≈5% residual test gap remained at 250 training environments, indicating limitations of simple domain randomization in fully closing generalization gaps for high-complexity navigation.",
            "sample_efficiency": "Not directly a sample-efficiency method; more training environments imply more diverse experiences required. Specific sample counts per N not enumerated, but reported success rates reflect policies trained to convergence for each N.",
            "key_findings": "Increasing the number of distinct training environments (domain randomization) consistently improves out-of-distribution performance on highly-constrained navigation tasks, but shows diminishing returns and cannot fully eliminate generalization gaps; suggests necessity for better generalization methods beyond naive environment-count scaling.",
            "uuid": "e1085.6",
            "source_info": {
                "paper_title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dyna, an integrated architecture for learning, planning, and reacting",
            "rating": 2
        },
        {
            "paper_title": "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Quantifying generalization in reinforcement learning",
            "rating": 2
        }
    ],
    "cost": 0.0190905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Reinforcement Learning Techniques for Autonomous Navigation</h1>
<p>Zifan Xu ${ }^{1}$, Bo Liu ${ }^{1}$, Xuesu Xiao ${ }^{2,3}$, Anirudh Nair ${ }^{1}$, and Peter Stone ${ }^{1,4}$</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and novel environments. Then, we explore four major classes of learning techniques with the purpose of achieving one or more of the four desiderata: memory-based neural network architectures (D1), safe RL (D2), model-based RL (D2, D3), and domain randomization (D4). By deploying these learning techniques in a new open-source largescale navigation benchmark and real-world environments, we perform a comprehensive study aimed at establishing to what extent can these techniques achieve these desiderata for RLbased navigation systems.</p>
<h2>I. INTRODUCTION</h2>
<p>Autonomous robot navigation, i.e., moving a robot from one point to another without colliding with any obstacle, has been studied by the robotics community for decades. Classical navigation systems [1], [2] can successfully solve such navigation problem in many real-world scenarios, e.g., handling noisy, partially observable sensory input but still providing verifiable collision-free safety guarantees. However, these systems require extensive engineering effort, and can still be brittle in challenging scenarios, e.g., in highly constrained environments. This is reflected by a recent competition (The BARN Challenge [3]) held in ICRA 2022, which suggests that even experienced roboticists tend to underestimate how difficult navigation scenarios are for real robots. Recently, data-driven approaches have also been used to tackle the navigation problem [4] thanks to advances in the machine learning community. In particular, Reinforcement Learning (RL), i.e., learning from self-supervised trial-anderror data, has achieved tremendous progress on multiple</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>fronts, including safety [5]-[7], generalizability [8]-[11], sample efficiency [12], [13], and addressing temporal data [14]-[16]. For the problem of navigation, learned navigation systems from RL [17] have the potential to relieve roboticists from extensive engineering efforts [18]-[22] spent on developing and fine-tuning classical systems. Moreover, a simple case study conducted in five randomly generated obstacle courses where classical navigation systems often fail shows that a RL-based navigation has the potential to achieve superior behaviors in terms of successful collision avoidance and goal reaching (Fig. 1 left).</p>
<p>Despite such promising advantages, learning-based navigation systems are far from finding their way into realworld robotics use cases, which currently still rely heavily on their classical counterparts. Such reluctance in adopting learning-based systems in the real world stems from a series of fundamental limitations of learning methods, e.g., lack of safety, explainability, and generalizability. To make things even worse, a lack of well-established comparison metrics and reproducible learning methods further obfuscates the effects of different learning approaches on navigation across both the robotics and learning communities, making it difficult to assess the state of the art and therefore to adopt learned navigation systems in the real world.</p>
<p>To facilitate research in developing RL-based navigation systems with the goal of deploying them in real-world scenarios, we introduce a new open-source large-scale navigation benchmark with a variety of challenging, highly constrained obstacle courses to evaluate different learning approaches, along with the implementation of several state-of-the-art RL algorithms. The obstacle courses resemble highly-constraint real-world navigation environments (Fig. 1 right), and present major challenges to existing classical navigation systems, while RL-based navigation systems have the potential to perform well in them (Fig. 1 left).</p>
<p>We identify four major desiderata that ought to be fulfilled by any learning-based system that is to be deployed: (D1) reasoning under uncertainty of partially observed sensory inputs, (D2) safety, (D3) learning from limited trial-anderror data, and (D4) generalization to diverse and novel environments. By deploying four major classes of learning techniques: memory-based neural network architectures, safe $R L$, model-based $R L$, and domain randomization, we perform extensive experiments and empirically compare a large range of RL-based methods based on the degree to which they achieve each of these desiderata. Moreover, by deploying six selected navigation systems in three qualitatively different real-world navigation environments, we investigate to what degree the conclusions drawn from the benchmark can be</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Left: Success rate of two classical navigation systems, DWA [2] (red) and E-band [1] (blue), and vanilla end-to-end RL-based (green) navigation systems (individually trained) in five randomly generated difficult obstacle courses. The insets at the top show top-down views of the five obstacle courses. Right: Navigation environments in the real world (left) and the proposed benchmark (right) are similar to the robot perception system (e.g., white/red laser scans and cyan/purple costmaps).</p>
<p>applied to the real world. Supplementary videos and material for this work are available on the project webpage.1</p>
<h2>II. DESIDERATA FOR LEARNING-BASED NAVIGATION</h2>
<p>In this section, we introduce four desiderata for learning-based autonomous navigation systems and briefly discuss the learning techniques as their corresponding solutions.</p>
<p><strong>(D1) reasoning under uncertainty of partially observed sensory inputs.</strong> Autonomous navigation without explicit mapping and localization is usually formalized as a Partially Observable Markov Decision Process (POMDP), where the agent produces the motion of the robot only based on limited sensory inputs that are usually not sufficient to recover the full state of the navigation environment. Most RL approaches solve POMDPs by maintaining a history of past observations and actions [14], [15]. Then, neural network architectures like Recurrent Neural Networks (RNNs) that process sequential data are employed to encode history and address partial observability. In this study, we investigate various design choices of history-dependent architectures.</p>
<p><strong>(D2) safety.</strong> Even though in some cases deep RL methods achieve comparable performance to classical navigation, they still suffer from poor explainability and do not guarantee collision-free navigation. The lack of safety guarantee is a major challenge preventing RL-based navigation from being used in the real world. Prior works have addressed this challenge by formalizing the navigation as a multi-objective problem that treats collision avoidance as a separate objective from reaching the goal and solving it with Lagrangian or Lyapunov-based methods [5]. For simplicity, we only explore Lagrangian method and investigate whether explicitly treat safety as a separate objective leads to safer and smoother learned navigation behavior.</p>
<p><strong>(D3) learning from limited trial-and-error data.</strong> Although deep RL approaches can alleviate roboticists from extensive engineering effort, a large amount of data is still required to train a typical deep RL agent. However, autonomous navigation data is usually expensive to collect in the real world. Therefore, data collection is usually conducted in simulation, e.g., in the Robot Operating System (ROS) Gazebo simulator, which provides an easy interface with real-world robots. However, simulating a full navigation stack from perception to actuation is more computationally expensive compared to other RL domains, e.g., MuJuCo or Atari games [23], [24], which presents a high requirement for sample efficiency. Most prior works have used off-policy RL algorithms to improve sample efficiency with experience replay [25], [26]. In addition, model-based RL methods can explicitly improve sample efficiency, and are widely used in robot control problems. In this study, we compare two common classes of model-based RL method [12], [13] combined with an off-policy RL algorithm, and empirically study to what extent model-based approaches improve sample efficiency when provided with different amounts of data.</p>
<p><strong>(D4) generalization to diverse and novel environments.</strong> The ultimate goal of deep RL approaches for autonomous navigation is to learn a generalizable policy for all kinds of navigation environments in the real world. A simple strategy is to train the agent in as many diverse navigation environments as possible or domain randomization, but it is unclear what is the necessary amount of training environments to efficiently achieve good generalization. Utilizing the large-scale navigation benchmark proposed in this paper, we empirically study the dependence of generalization on the number of training environments.</p>
<h2>III. NAVIGATION BENCHMARK</h2>
<p>This section details the proposed navigation benchmark for RL-based navigation systems, which aims to provide a unified and comprehensive testbed for future autonomous navigation research. First, Sec. III-A discusses the difference between the proposed benchmark and existing navigation benchmarks. In Sec. III-B and III-C, the navigation task is formally defined and formulated as a POMDP. More detailed background of MDP and POMDP can be found on the project webpage. Finally, Sec. III-D introduces simulated and real-world environments that benchmark different aspects of navigation performance.</p>
<h3>A. Existing Navigation Benchmarks</h3>
<p>Our proposed benchmark differs from existing benchmarks in three aspects: (1) <strong>high-fidelity physics</strong>: the navigation tasks are simulated by Gazebo [27], which is based on realistic physical dynamics and therefore tests motion planners that directly produce low-level motion commands, i.e., linear and angular velocities, in contrast to high-level instructions such as turn left, turn right, move forward [28], [29]. In other words,</p>
<p><sup>1</sup>https://cs.gmu.edu/xiao/Research/RLNavBenchmark/</p>
<p>we focus on “how to navigate” (motion planning), instead of “where to navigate” (path planning); (2) ROS integration: our benchmark is based on ROS [30], which allows seamless transfer of a navigation method developed and benchmarked in simulation directly onto a physical robot with little (if any) effort; and (3) collision-free navigation: the benchmark includes both static and dynamic environments, and requires collision-free navigation, whereas other benchmarks assume that either collisions are possible [29] or collision-avoidance will be addressed by other low-level controllers out of the scope of the benchmark [28]. A special case is the photorealistic interactive Gibson benchmark by Xia et. al. [31], which intentionally allows physical interaction with objects (e.g., pushing) and therefore pose no challenges to the collision-avoidance system.</p>
<h3>III-B Navigation Problem Definition</h3>
<p>Definition 1 (Robot Navigation Problem). Situated within a navigation environment $e$ which includes information of all the obstacle locations at any time $t$, a start location $\left(x_{i},y_{i}\right)$, a start orientation $\theta_{i}$, and a goal location $\left(x_{g},y_{g}\right)$, the navigation problem $\mathcal{T}<em _text_max="\text{max">{e}$ is to maximize the probability $p$ of a mobile robot reaching the goal location from the start location and orientation under a constraint on the number of collisions with any obstacle $C&lt;1$ and a time limit $t&lt;T</em>$.}</p>
<p>A navigation problem can be formally defined as above. Given the current location $\left(x_{t},y_{t}\right)$, the robot is considered to have reached the goal location if and only if its distance to the goal location is smaller than a threshold, $d_{t}&lt;d_{s}$, where $d_{t}$ is the Euclidean distance between $\left(x_{t},y_{t}\right)$ and $\left(x_{g},y_{g}\right)$, and $d_{s}$ is a constant threshold.</p>
<h3>III-C POMDP Formulation</h3>
<p>A navigation task $\mathcal{T}<em e="e">{e}$ can be formulated as a POMDP conditioned on a navigation environment $e$, which can be represented by a 7-tuple $\left(S</em>\right)$ in the robot frame. The observation model $Z:S\rightarrow O$ maps the state to the observation. The reward function for this POMDP is defined as follows:},A_{e},O_{e},T_{e},\gamma_{e},R_{e},Z_{e}\right)$. In this POMDP, the state $s_{t}\in S_{e}$ is a 5-tuple $\left(x_{t},y_{t},\theta_{t},c_{t},e\right)$ with $x_{t},y_{t},\theta_{t}$ the two-dimensional coordinates and the orientation of the robot at time step $t$, $c_{t}$ a binary indicator of whether a collision has occurred since the last time step $t-1$, and $e$ the navigation environment. The action $a_{t}=\left(v_{t},\omega_{t}\right)\in A_{e}$ is a two-dimensional continuous vector that encodes the robot’s linear and angular velocity. The observation $o_{t}=$ $\left(\chi_{t},\dot{x_{t}},\dot{y_{t}}\right)\in O_{e}$ is a 3-tuple composed of the sensory input $\chi_{t}$ from LiDAR scans and the relative goal position $\left(\dot{x_{t}},\dot{y_{t}</p>
<p>$R_{e}(s_{t},a_{t})=+b_{f}\cdot\mathbb{1}(d_{t}&lt;d_{s})+b_{p}\cdot(d_{t-1}-d_{t})-b_{c}\cdot c_{t},$ (1)</p>
<p>where $\mathbb{1}(d_{t}&lt;d_{s})$ is the indicator function of reaching the goal location, $d_{t}$ is the Euclidean distance to the goal location, and $b_{f}$, $b_{p}$, $b_{c}$ are the coefficient constants. In this reward function, the first term is the true reward function that assigns a positive constant $b_{f}$ for the success of an
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Three types of navigation environments: static (left), dynamic box (middle), and dynamic-wall (right). The red squares mark the obstacle fields, and the yellow arrows mark the direction of navigation. In dynamic-wall, the green (blue) arrows indicate the case when the two walls are moving apart (together). In dynamic box, the red arrows indicate the velocities of obstacles.
agent, which matches with the objective of the navigation task in Definition 1. The second and third terms are auxiliary rewards that facilitate the training by encouraging local progress and penalizing collisions.</p>
<p>We perform a grid search over different values of the coefficients in this reward function, and the result shows that the auxiliary reward term $\left(d_{t-1}-d_{t}\right)$ is necessary for successful training, and a much smaller coefficient $b_{p}$ relative to $b_{f}$ can lead to a better asymptotic performance. The agent can learn without the penalty reward for collision $\left(b_{c}=0\right)$, but a moderate value of $b_{c}$ can improve the asymptotic performance and speed up training. For all the experiments in this paper, we fix the coefficients as $b_{f}=20, b_{p}=1$ and $b_{c}=4$.</p>
<p>In our experiments, the RL algorithm solves a multi-task RL problem where the tasks are randomly sampled from a task distribution $\mathcal{T}<em e="e">{e} \sim p\left(\mathcal{T}</em>}\right)$. Here the task distribution $p\left(\mathcal{T<em i="i">{e}\right):=U\left(\left{e</em>\right}<em i="i">{i=1}^{N}\right)$ is a uniform distribution on a set of $N$ navigation environments $\left{e</em>\right}<em _pi="\pi">{i=1}^{N}$. The overall objective of this multi-task RL problem is to find an optimal policy $\pi^{*}=\max </em>} \mathbb{E<em e="e">{\mathcal{T}</em>} \sim p\left(\mathcal{T<em t="t">{e}\right), \tau</em>\right)\right]$.} \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{e}\left(s_{t}, a_{t</p>
<h2>D. Navigation Environments</h2>
<p>The navigation is performed by a ClearPath Jackal differential-drive ground robot in simulated by the Gazebo simulator. More details of the robot and simulation can be found on the project webpage. Each environment in this benchmark will have a navigation system navigating the robot through a 10 m navigation path that passes through a highly constrained obstacle course. Walls are placed at three edges of a square so that passing through the obstacle field is the only path to the goal location (see Fig. 2). The benchmark includes 300 static environments, 100 dynamic-box environments, and 100 dynamic-wall environments. The static environments contains a diverse set of obstacle course covering a large range of difficulty levels from easy to hard. A dynamic-box environment has small boxes with random shapes and velocities to test the system’s immediate reactions to small moving obstacles. A dynamic-wall has two walls moving oppositely that requires the system to make a longer-term decision of whether to pass or wait. The detailed procedures of generating these environments can</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Real-world benchmark-like (top-right), in-door highly-constrained (top-left), and large-scale (bottom) environments. The yellow curves mark the paths of navigation.</p>
<p>be found on the project webpage. We randomly select 50 environments from each type as the test sets, which are denoted as static-test, dynamic-box-test, and dynamic-wall-test. The remaining environments are denoted as static-train, dynamic-box-train, and dynamic-wall-train respectively. To study the effect of randomization, static-train is further separated as static-train-5, static-train-10, static-train-50, static-train-100, and static-train-250 by randomly sampling 5, 10, 50, 100, and all 250 environments from static-train.</p>
<p>To test the sim-to-real transferability of the policies learning with different techniques, the navigation systems are deployed in three qualitatively different static navigation environments including a benchmark-like environment (Fig. 3 left), an indoor highly-constrained environment (Fig. 3 right), and a large-scale environment of 30 meters in length. We denote them as real-world-1, real-world-2, and real-world-3 respectively.</p>
<h2>IV. EXPERIMENTS</h2>
<p>In this section, we present experimental results of each studied technique to achieve the proposed desiderata in Sec. II. We implement distributed training pipelines (similar to [21]) of different RL algorithms including TD3 [32], SAC [33], and DDPG [34]. They perform similarly in the study of different neural network architectures. For simplicity, all the experiments mentioned in this section use TD3 combined with the corresponding techniques, and all the data points are averaged over three independent runs.</p>
<h3>A. Memory-based Neural Network Architectures (D1)</h3>
<p>To benchmark the performance of different neural network (NN) architectures, deep RL policies represented by architectures of Multilayer Perceptron (MLP), One-dimensional Convolutional Neural Network (CNN), Gated Recurrent Units (GRU), and Transformer with history length of 4 and 8 are trained in static-train-50, and the two types of dynamic environments dynamic-box-train and dynamic-wall-train from Sec. III-D. After training, the policies are tested in their corresponding test sets. In addition, MLP with history length of one is added as a memory-less baseline. Table I shows the success rates of policies with different architectures and history lengths evaluated in static-test (left), dynamic-wall-test (middle) and dynamic-box-test respectively.</p>
<p>Memory-based NNs only marginally improve navigation performance in static environments. In Table I, the policy represented by Transformer with a history length of 4 shows the best success rate of 68%, with a slightly worse success rate of 65% achieved by the baseline MLP. Additionally, a monotonic decrease in success rate with increasing history length is observed in each tested NN architecture. For example, a 32% drop in the success rate of Transformer is shown by increasing the history length from 4 to 8. One possible explanation is that, if only few past observations are useful to make the decision, including more history will make it more difficult to learn a generalized policy in this very diverse training set.</p>
<p>Memory is essential when possible catastrophic failures will happen by making the wrong long-term decisions. Memory usually matters for dynamic environments when a single time frame is not sufficient to estimate the motion of obstacles. Surprisingly, in dynamic-box where the dynamic obstacles are completely random, the memory-based NN architectures do not outperform the memory-less baseline. On the other hand, in dynamic-wall with a manually designed dynamic challenge, the best success rate of 82% is observed in GRU with a history length of 4, which improves about 15% over the non-memory baseline. During our deployment of the policies, we observe that, in dynamic-box even though the memory-less agent does not estimate the motion and adjust its plan in advance, it tends to perform safely and avoids the obstacles when they get close enough. This simple strategy works surprisingly well and achieves similar success rate as the memory-based policies. However, this strategy does not work in the manually designed dynamic challenges like dynamic-wall where the agent has to estimate the motion of the obstacles to pass safely.</p>
<h3>B. Safe RL (D2)</h3>
<p>To investigate to what extent safe RL methods can help to improve safety, a TD3 agent with the Lagrangian-based safe RL method is trained in static-train-50, and then tested in static-test. The policy is represented by a MLP with its input containing only one history length. Table II shows the success rate, average survival time, and average traversal time of the safe RL agent trained with Lagrangian method and a baseline MLP agent tested in static-test. We define survival time as the time cost of an unsuccessful episode (collision or exceeding a time limit of 80s). Traversal time, instead, is the time cost of a successful episode. With the same level of success rate, a longer survival time means that the agent tends to, at least, avoid collisions if it cannot succeed. To compare the safe RL method with classical</p>
<p>| Success rate (%) $(\uparrow)$ | static env. | | | dynamic-box env. | | | dynamic-wall env. | | |
| | $H=1$ | $H=4$ | $H=8$ | $H=1$ | $H=4$ | $H=8$ | $H=1$ | $H=4$ | $H=8$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MLP | $65 \pm 4$ | $57 \pm 7$ | $42 \pm 2$ | $50 \pm 5$ | $35 \pm 2$ | $46 \pm 3$ | $67 \pm 7$ | $72 \pm 1$ | $69 \pm 4$ |
| GRU | - | $51 \pm 2$ | $43 \pm 4$ | - | $48 \pm 4$ | $45 \pm 1$ | - | $\mathbf{8 2} \pm \mathbf{4}$ | $78 \pm 5$ |
| CNN | - | $55 \pm 4$ | $45 \pm 5$ | - | $42 \pm 5$ | $40 \pm 1$ | - | $63 \pm 3$ | $43 \pm 3$ |
| Transformer | - | $\mathbf{6 8} \pm \mathbf{2}$ | $46 \pm 3$ | - | $\mathbf{5 2} \pm \mathbf{1}$ | $44 \pm 4$ | - | $33 \pm 28$ | $15 \pm 13$ |</p>
<p>TABLE I: (D1) Success rate (%) $(\uparrow)$ of policies trained with different neural network architectures and history lengths. $H$ is the history length of the memory. Bold font indicates the best success rate for each type of environment.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Baseline (model-free)</th>
<th>Lagrangian method</th>
<th>MPC (model-based)</th>
<th>DWA</th>
<th>TEB</th>
</tr>
</thead>
<tbody>
<tr>
<td>Success rate (%) $(\uparrow)$</td>
<td>$65 \pm 4$</td>
<td>$74 \pm 2$</td>
<td>$70 \pm 3$</td>
<td>$\mathbf{8 2}$</td>
<td>70</td>
</tr>
<tr>
<td>Survival time (s) $(\uparrow)$</td>
<td>$8.0 \pm 1.5$</td>
<td>$16.2 \pm 2.5$</td>
<td>$55.7 \pm 4.9$</td>
<td>$\mathbf{6 2 . 7}$</td>
<td>26.9</td>
</tr>
<tr>
<td>Traversal time (s) $(\downarrow)$</td>
<td>$\mathbf{7 . 5} \pm \mathbf{0 . 3}$</td>
<td>$8.6 \pm 0.2$</td>
<td>$24.7 \pm 2.0$</td>
<td>35.6</td>
<td>26.9</td>
</tr>
</tbody>
</table>
<p>TABLE II: (D2) Success rate $(\uparrow)$, survival time $(\uparrow)$, and traversal time $(\downarrow)$ of policies trained with Lagrangian method, MPC with probabilistic transition model, and DWA. The bold font indicates the best number achieved for each type of metric.
navigation systems which are believed to have better safety, we also add evaluation metrics from a classical navigation stack with the Dynamic Window Approach (DWA) [2] local planner.</p>
<p>Lagrangian method reduces the gap between training and test environments. When deployed in the training environments, both the baseline MLP and the safe RL method achieves about $80 \%$ success rate. However, in the test environments, the Lagrangian method has a better success rate of $74 \%$ compare to $65 \%$ by the baseline MLP. We hypothesize that the safety constraint applied by the safe RL methods forms a way of regularization, and therefore, improves the generalization to unseen environments.</p>
<p>Lagrangian method increases the average survival time in failed episodes. As expected, the Lagrangian method increases the average survival time by $8.2 s$ compared to the baseline MLP at a cost of $1.1 s$ longer average traversal time. However, such improved safety are still worse than the classical navigation systems given the best survival time of $88.6 s$ achieved by DWA.</p>
<h2>C. Model-based RL (D2 and D3)</h2>
<p>To explore how the model-based approaches help with the autonomous navigation tasks, we implement Dyna-style, MPC, and MBPO, and evaluate the methods in static environments. The transition models are either represented by a deterministic NN or a probabilistic NN that predicts the mean and variance of the next state. During the training in static-train-50, the policies are saved when 100 k , 500k and 2000k transition samples are collected, then tested in static-test. The success rates of these policies are reported in Table IV.</p>
<p>Model-based methods do not improve sample efficiency. As shown in the second and third columns in Table IV, better success rates of $13 \%$ and $58 \%$ are achieved by the baseline MLP method provided by limited 100k and 500k transition samples respectively. In addition, Higher success rates at 500k transition samples are observed in probabilistic models compared to their deterministic counterparts, which indicates a more efficient learning with probabilistic transition models. Notice that MBPO exploits more heavily on
the model compared to the Dyna-style method, which leads to much worse asymptotic performance (about $20 \%$ success rate in the end).</p>
<p>Model-based methods with probabilistic dynamic models improve the asymptotic performance. In the last column of Table IV, both Dyna-style and MPC with probabilistic dynamic models achieve slightly better success rates of $70 \%$ compared to $65 \%$ in the baseline MLP method when sufficient transition samples of 2000k are given to the learning agent.</p>
<p>The MPC policy performs conservatively when deployed in unseen test environments and shows a better safety performance. The safety performances of MPC policies with probabilistic dynamic models are also tested (see Table II). We observe that the agents with MPC policies navigate very conservatively with an average traversal time of $24.7 s$, which is about two times more than the MLP baseline. In the meantime, MPC policies achieve improved safety with the best survival time of $55.7 s$ among the RL-based methods.</p>
<h2>D. Domain Randomization (D4)</h2>
<p>To explore how model generalization depends on the degree of randomness in the training environments, baseline MLP policies with one history length are trained in the environment sets with 5, 10, 50, 100, and 250 training environments. The trained policies are tested in the same static-test. To investigate the performance gap between training and test, the policies trained with 50, 100, and 250 environments are also tested on static-train-50, which is part of their training sets. Fig 4 shows the success rate of policies trained with different number of training environments.</p>
<p>The generalization to unseen environments improves with increasing number of training environments. As shown in Fig. 4, the performances on the unseen test environments monotonously increase from $43 \%$ to $74 \%$ with the number of training environments increasing from 5 to 250. Moreover, the gaps between training and test environments gradually shrink by adding more training environments provided by that the polices are robust enough to maintain similar performances of about $80 \%$ on the training environ-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: (D4) Success rate (%) of policies trained with different number of training environments.</p>
<p>ments.</p>
<table>
<thead>
<tr>
<th>Transition samples</th>
<th>100k</th>
<th>500k</th>
<th>2000k</th>
</tr>
</thead>
<tbody>
<tr>
<td>MLP</td>
<td>$\mathbf{13} \pm \mathbf{7}$</td>
<td>$\mathbf{5 8} \pm \mathbf{2}$</td>
<td>$65 \pm 4$</td>
</tr>
<tr>
<td>Dyna-style deterministic</td>
<td>$8 \pm 2$</td>
<td>$30 \pm 10$</td>
<td>$66 \pm 5$</td>
</tr>
<tr>
<td>MPC deterministic</td>
<td>$0 \pm 0$</td>
<td>$21 \pm 10$</td>
<td>$62 \pm 3$</td>
</tr>
<tr>
<td>Dyna-style probabilistic</td>
<td>$0 \pm 0$</td>
<td>$48 \pm 4$</td>
<td>$\mathbf{7 0} \pm \mathbf{1}$</td>
</tr>
<tr>
<td>MPC probabilistic</td>
<td>$0 \pm 0$</td>
<td>$45 \pm 4$</td>
<td>$\mathbf{7 0} \pm \mathbf{3}$</td>
</tr>
<tr>
<td>MBPO</td>
<td>$0 \pm 0$</td>
<td>$0 \pm 0$</td>
<td>$21.9 \pm 3$</td>
</tr>
</tbody>
</table>
<p>TABLE III: Physical experiments. The table shows the traversal time (s) (↓) and the number of successful trials (↑) of 5 RL-based navigation systems and a classical navigation system (DWA) evaluated in three real-world environments. The bold font indicates the best traversal time when all three trials are successful.</p>
<p>TABLE IV: (D3) Success rate (%) (↑) of policies trained with different model-based methods and different number of transition samples. The bold font indicates the best success rate for each number of transition samples.</p>
<h3><em>E. Physical experiments</em></h3>
<p>To study the consistency of the above observations in simulation and the real world, we deploy one baseline MLP policy, one best policy for each studied desideratum, and one classical navigation system (DWA [2]) in the three real-world environments introduced in Sec. III-D. Each deployment is repeated three times, and the average traversal time and the number of successful trials are reported in Table. III. Even though the best memory-based policy, transformer architecture with 4 history length, was only marginally better than the baseline MLP in simulation, in the real world it can navigate very smoothly and fails only once in real-world-2 and real-world-3, while baseline MLP fails most of the trials in all the environments including the benchmark-like environment. One possible reason for this is that simulations are typically more predictable than the real world. Therefore, it is particularly important to use historical data in the real world to estimate the environment and current states of the robot. Similarly, MLP policy trained with 250 environments can successfully navigate in all the environments without any failures, while baseline MLP trained with 50 environments fails most of the trials. Safe RL improves the chances of success in all the environments and can navigate more safely by performing backups and small adjustments of robots' poses. Similar to the simulation, MPC navigates very conservatively and succeeds in all the trials in real-world-1 and real-world-2, but has much more difficulty generalizing to large-scale real-world-3.</p>
<h3>V. CONCLUSION</h3>
<p>In this section, we discuss the conclusions we draw from these benchmark experiments. We organize these conclusions by the desiderata as follows:</p>
<p>(D1) reasoning under uncertainty of partially observed sensory inputs does not obviously benefit from adding memory in simulated static environments and very random dynamic (dynamic-box) environments, but much more significant improvements were observed in the <em>real world</em> and in more <em>challenging dynamic environments</em> (dynamic-wal1).</p>
<p>(D2) safety is improved by both safe RL and model-based MPC methods. However, classical navigation systems still achieve the best safety performance at a cost of very long traversal time. Whether RL-based navigation systems can achieve similar safety guarantees as classical navigation systems and whether safety can be improved without significantly sacrificing the traversal time are still open questions.</p>
<p>(D3) the ability to learn from limited trial-and-error data is not improved by the evaluated model-based methods. Currently, we observe that model-based RL methods indeed improve sample-efficiency, but only when the number of imaginary rollouts from the learned model is large (e.g. ≥ 2000k) and when they are sampled with randomness. We therefore hypothesize that the improvement comes from the robustness brought by learning on more data sampled from the learned model. Hence, this result motivates not only more accurate model learning for reducing the number of imaginary rollouts, but also theoretical understanding of how the model helps improve the robustness or even safety of navigation.</p>
<p>(D4) the generalization to diverse and novel environments is improved by increasing the randomness of training environments. However, a noticeable gap of about 5% between training and test environments is not eliminated by further increasing the number of training environments to 250. This reflects the limitation of simple <em>domain randomization</em> to increase the generalization, which is, however, widely used by the community.</p>
<p>In summary, although the proposed benchmark is not intended to represent every real-world navigation scenario, it serves as a simple yet comprehensive testbed for RL-based navigation methods. We observed that for every desideratum, no method can achieve 100% success rate on all <em>training</em> environments. Even though we ensured that we have made sure that every environment is indeed individually solvable. This alone indicates that there exists an optimization and generalization challenge when we have a large number of training environments as in our proposed benchmark.</p>
<h2>REFERENCES</h2>
<p>[1] S. Quinlan and O. Khatib, "Elastic bands: Connecting path planning and control," in [1993] Proceedings IEEE International Conference on Robotics and Automation. IEEE, 1993, pp. 802-807.
[2] D. Fox, W. Burgard, and S. Thrun, "The dynamic window approach to collision avoidance," IEEE Robotics \&amp; Automation Magazine, vol. 4, no. 1, pp. 23-33, 1997.
[3] X. Xiao, Z. Xu, Z. Wang, Y. Song, G. Warnell, P. Stone, T. Zhang, S. Ravi, G. Wang, H. Karnan et al., "Autonomous ground navigation in highly constrained spaces: Lessons learned from the barn challenge at icra 2022," arXiv preprint arXiv:2208.10473, 2022.
[4] X. Xiao, B. Liu, G. Warnell, and P. Stone, "Motion planning and control for mobile robot navigation using machine learning: a survey," Autonomous Robots, pp. 1-29, 2022.
[5] Y. Chow, O. Nachum, A. Faust, M. Ghavamzadeh, and E. A. DuéñezGuzmán, "Lyapunov-based safe policy optimization for continuous control," CoRR, vol. abs/1901.10031, 2019. [Online]. Available: http://arxiv.org/abs/1901.10031
[6] G. Thomas, Y. Luo, and T. Ma, "Safe reinforcement learning by imagining the near future," 2022.
[7] E. Rodríguez-Seda, D. Stipanovic, and M. Spong, "Lyapunov-based cooperative avoidance control for multiple lagrangian systems with bounded sensing uncertainties," in 2011 50th IEEE Conference on Decision and Control and European Control Conference, CDC-ECC 2011, ser. Proceedings of the IEEE Conference on Decision and Control, Dec. 2011, pp. 4207-4213, 2011 50th IEEE Conference on Decision and Control and European Control Conference, CDC-ECC 2011 ; Conference date: 12-12-2011 Through 15-12-2011.
[8] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, "Quantifying generalization in reinforcement learning," in ICML, 2019.
[9] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, "Leveraging procedural generation to benchmark reinforcement learning," arXiv preprint arXiv:1912.01588, 2019.
[10] N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and S. Risi, "Illuminating generalization in deep reinforcement learning through procedural level generation," arXiv: Learning, 2018.
[11] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017, pp. $23-30$.
[12] R. S. Sutton, "Dyna, an integrated architecture for learning, planning, and reacting," SIGART Bull., vol. 2, no. 4, p. 160-163, jul 1991. [Online]. Available: https://doi.org/10.1145/122344.122377
[13] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, "Neural network dynamics for model-based deep reinforcement learning with modelfree fine-tuning," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 7559-7566.
[14] M. J. Hausknecht and P. Stone, "Deep recurrent q-learning for partially observable mdps," in AAAI Fall Symposia, 2015.
[15] D. Wierstra, A. Förster, J. Peters, and J. Schmidhuber, "Solving deep memory pomdps with recurrent policy gradients," in ICANN, 2007.
[16] K. Chua, R. Calandra, R. McAllister, and S. Levine, "Deep reinforcement learning in a handful of trials using probabilistic dynamics models," Advances in neural information processing systems, vol. 31, 2018.
[17] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, "Learning navigation behaviors end-to-end with autorl," IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 2007-2014, 2019.
[18] X. Xiao, B. Liu, G. Warnell, J. Fink, and P. Stone, "Appld: Adaptive planner parameter learning from demonstration," IEEE Robotics and Automation Letters, vol. 5, no. 3, pp. 4541-4547, 2020.
[19] Z. Wang, X. Xiao, B. Liu, G. Warnell, and P. Stone, "APPLI: Adaptive planner parameter learning from interventions," in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021.
[20] Z. Wang, X. Xiao, G. Warnell, and P. Stone, "Apple: Adaptive planner parameter learning from evaluative feedback," IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7744-7749, 2021.
[21] Z. Xu, G. Dhamankar, A. Nair, X. Xiao, G. Warnell, B. Liu, Z. Wang, and P. Stone, "APPLR: Adaptive planner parameter learning from reinforcement," in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021.
[22] X. Xiao, Z. Wang, Z. Xu, B. Liu, G. Warnell, G. Dhamankar, A. Nair, and P. Stone, "Appl: Adaptive planner parameter learning," Robotics and Autonomous Systems, vol. 154, p. 104132, 2022.
[23] E. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based control," in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 5026-5033.
[24] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents," Journal of Artificial Intelligence Research, vol. 47, pp. 253-279, jun 2013.
[25] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, "Learning navigation behaviors end-to-end with autorl," IEEE Robotics and Automation Letters, vol. 4, pp. 2007-2014, 2019.
[26] A. Wahid, A. Toshev, M. Fiser, and T.-W. E. Lee, "Long range neural navigation policies for the real world," 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 82-89, 2019.
[27] N. Koenig and A. Howard, "Design and use paradigms for gazebo, an open-source multi-robot simulator," in 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566), vol. 3. IEEE, 2004, pp. 2149-2154.
[28] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, "Target-driven visual navigation in indoor scenes using deep reinforcement learning," in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 3357-3364.
[29] L. Harries, S. Lee, J. Rzepecki, K. Hofmann, and S. Devlin, "Mazeexplorer: A customisable 3d benchmark for assessing generalisation in reinforcement learning," in 2019 IEEE Conference on Games (CoG). IEEE, 2019, pp. 1-4.
[30] Stanford Artificial Intelligence Laboratory et al., "Robotic operating system." [Online]. Available: https://www.ros.org
[31] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev, R. Martín-Martín, and S. Savarese, "Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 713-720, 2020.
[32] S. Fujimoto, H. van Hoof, and D. Meger, "Addressing function approximation error in actor-critic methods," 2018.
[33] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor," in International conference on machine learning. PMLR, 2018, pp. 1861-1870.
[34] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," arXiv preprint arXiv:1509.02971, 2015.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Department of Computer Science, University of Texas at Austin ${ }^{2}$ Department of Computer Science, George Mason University ${ }^{3}$ Everyday Robots ${ }^{4}$ Sony AI. This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (CPS-1739964, IIS-1724157, NRI1925082), ONR (N00014-18-2243), FLI (RFP2-000), ARO (W911NF-19-2-0333), DARPA, Lockheed Martin, GM, and Bosch. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
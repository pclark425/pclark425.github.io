<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7505 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7505</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7505</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-c74cb4c6f4d2042a7921bf7dd05fca8766150363</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c74cb4c6f4d2042a7921bf7dd05fca8766150363" target="_blank">Does Spatial Cognition Emerge in Frontier Models?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition. Code and data are available: https://github.com/apple/ml-space-benchmark</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7505.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7505.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - Direction (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Direction estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multimodal LLM (OpenAI GPT-4o) evaluated on the SPACE benchmark's direction/pointing task using first-person (egocentric) images; performance reported as percent accuracy with uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model with vision/video capability (supports image and sequence inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Direction estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial orientation / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pointing trial: from a given landmark A, estimate direction to landmark B; presented as a 4-way multiple-choice question using egocentric images from a walkthrough.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 82.8%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 32.0% ± 4.1</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±4.1% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance is near chance (chance = 25%) and substantially below the human baseline (82.8%). Paper emphasizes models are near chance on egocentric large-scale spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7505.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - Distance (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Distance estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on estimating Euclidean distances between landmarks from an egocentric walkthrough; multiple-choice accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model with vision/video capability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Distance estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial perception / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>From a current landmark, estimate straight-line distances to other landmarks; posed as 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 83.2%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 36.5% ± 5.0</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±5.0% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model substantially underperforms humans and is only modestly above chance (25%); paper notes models struggle with metric estimations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7505.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - Map sketching (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Map sketching, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on choosing the correct map sketch of the environment after a walkthrough; 4-way multiple-choice accuracy given.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model with vision/video capability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Map sketching (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>cognitive mapping / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select which map sketch preserves the true spatial relationships between start, goal, and landmarks after viewing a walkthrough; 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 96.6%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 33.3% ± 4.1</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±4.1% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model performance is far below human baseline and near chance (25%); authors emphasize poor large-scale mapping ability in multimodal setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7505.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - Direction (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Direction estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 3.5 Sonnet evaluated on the pointing/direction task with egocentric visual inputs; accuracy and uncertainty reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model (Claude 3.5 Sonnet) with vision capability and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Direction estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial orientation / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pointing trial using egocentric images; 4-way multiple-choice direction estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 82.8%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 29.0% ± 2.9</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±2.9% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performs close to chance and substantially below human baseline; authors flag consistent failure across large-scale egocentric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7505.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - Distance (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Distance estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet measured on discrete distance estimation judgments from egocentric walkthroughs; presented as multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model with vision capability and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Distance estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial perception / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Estimate Euclidean distances to other landmarks from current landmark, posed as 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 83.2%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 34.4% ± 2.9</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±2.9% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance substantially below human baseline; paper notes models are poor at metric estimation and often near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7505.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - Map (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Map sketching, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated on choosing correct allocentric map sketch after an egocentric walkthrough; multiple-choice accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model with vision capability and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Map sketching (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>cognitive mapping / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select which map sketch preserves true spatial relationships between elements after viewing walkthrough; 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 96.6%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 27.5% ± 8.3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±8.3% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Large gap versus humans; high uncertainty on map sketching indicates unstable performance across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7505.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v - Direction (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4v (OpenAI) — Direction estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4v (vision-capable GPT-4 variant) evaluated on pointing/direction trials using egocentric video-like inputs; accuracy reported with uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 variant with vision capabilities (image/video inputs) and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Direction estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial orientation / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pointing trial: from a given landmark A in an egocentric view, choose direction to landmark B among four options.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 82.8%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 29.7% ± 0.3</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±0.3% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model performance near chance; authors emphasize near-chance behavior of multimodal models on egocentric large-scale tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7505.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v - Distance (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4v (OpenAI) — Distance estimation, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4v evaluated on distance estimation between landmarks from egocentric walkthrough imagery; reported as percent accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal LLM with vision capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Distance estimation (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial perception / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Estimate Euclidean distances from a reference landmark to others; presented as 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 83.2%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 31.9% ± 2.7</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±2.7% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Substantially below human baseline and near chance; authors highlight poor metric estimation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7505.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v - Map (Ego image)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4v (OpenAI) — Map sketching, ego-image presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4v evaluated on map-sketch multiple-choice task after egocentric walkthrough; accuracy with uncertainty is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal LLM with vision capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Map sketching (egocentric image)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>cognitive mapping / large-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Choose the correct allocentric map sketch preserving spatial relationships after a walkthrough; 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 96.6%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 20.0% ± 11.8</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>model reported with ±11.8% uncertainty; no p-value reported</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Very low performance with large uncertainty; authors emphasize failures in egocentric mapping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7505.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - MRT (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Mental Rotation Test (multimodal presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated on the Mental Rotation Test using visual (3D shapes) multimodal presentation; reported as percent correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model with vision capability and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Mental Rotation Test (MRT, multimodal visual presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial visualization / small-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Identify rotated version of a reference 3D shape among distractors (visual Shepard–Metzler style stimuli); 4-choice multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 78.5%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 29.03%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model substantially below human baseline on visual mental rotation; paper notes multimodal presentation is harder for models than some text encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7505.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - MRT (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Mental Rotation Test (multimodal presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on the visual Mental Rotation Test (3D shapes); percent correct reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model with vision/video capability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Mental Rotation Test (MRT, multimodal visual presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial visualization / small-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select rotated version of a 3D reference shape among distractors (Shepard–Metzler stimuli); 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 78.5%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 33.32%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model near chance and far below human performance on multimodal MRT; textual (simplified) versions are easier for LLMs per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7505.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v - MRT (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4v (OpenAI) — Mental Rotation Test (multimodal presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4v tested on the visual 3D mental rotation task; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI vision-capable GPT-4 variant (multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Mental Rotation Test (MRT, multimodal visual presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial visualization / small-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Identify rotated version of a 3D reference shape among distractors; 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 78.5%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 32.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model performs near chance on multimodal MRT; paper highlights models do markedly worse on 3D visual rotations than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7505.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - SAtt (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Selective attention (cancellation) task, multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated on selective spatial attention (cancellation) using visual icons; accuracy reported and compared to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model with vision capability and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Selective attention (SAtt, multimodal visual cancellation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>selective attention / visuospatial attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Locate and report positions of target stimuli embedded among distractors on a grid; presented as 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 95.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 90.53%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Claude approaches human-level on selective attention in multimodal presentation (90.5% vs 95% human), an exception among spatial tasks where some models match or exceed humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7505.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - SAtt (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Selective attention (cancellation) task, multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on the visual cancellation/selective attention task; accuracy reported and compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model with vision/video capability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Selective attention (SAtt, multimodal visual cancellation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>selective attention / visuospatial attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Find and report target stimuli positions among distractors on a grid; multiple-choice answer enumerates target coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 95.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 70.22%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Strong but not human-level performance (70.2% vs 95%) in multimodal SAtt; authors note some LLMs match or exceed humans on selective-attention text encodings but multimodal results are mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7505.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v - SAtt (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4v (OpenAI) — Selective attention (cancellation) task, multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4v measured on the visual selective-attention cancellation task; percent accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI vision-capable GPT-4 variant (multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Selective attention (SAtt, multimodal visual cancellation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>selective attention / visuospatial attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Identify target stimuli among distractors on a grid and report coordinates; 4-way multiple-choice format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 95.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 59.82%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performs substantially below human baseline in multimodal SAtt; however some other LLMs (Claude, GPT-4o) perform far better on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7505.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - MRT (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Mental Rotation Test (text-only presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on a simplified text-only (2D character-array) mental rotation test; percent accuracy reported and compared to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model (text + vision capable); here evaluated on text-only 2D encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Mental Rotation Test (MRT, text-only 2D arrays)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial visualization / small-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>2D character-array version of mental rotation; choose rotated correct alternative among distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 90.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 41.93%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Text-only simplified MRT yields higher LLM performance than multimodal 3D MRT, but still well below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7505.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - MRT (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Mental Rotation Test (text-only presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated on a simplified text-only mental rotation task (2D arrays); percent correct reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model with instruction tuning; here evaluated on text-only character-array encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Mental Rotation Test (MRT, text-only 2D arrays)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>spatial visualization / small-scale spatial cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Simplified 2D textual version of mental rotation; select correct rotated alternative from distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 90.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 37.52%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performs substantially below human baseline even on simplified textual MRT; authors note text-only variants are easier but still far from human levels on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7505.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - SAtt (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) — Selective attention (text-only cancellation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated on text-only cancellation/selective-attention task (characters in a grid); reported percent accuracy approaches human levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal/instruction-tuned large language model; evaluated here on text-only 2D character arrays.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Selective attention (SAtt, text-only character-grid)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>selective attention / visuospatial attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Locate target characters among distractors in a character grid and report their positions; 4-way multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 96.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 98.82%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>On the text-only selective attention implementation GPT-4o slightly exceeds the reported human baseline (98.8% vs 96%); paper cautions that text encodings can be substantially easier than visual counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7505.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7505.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet - SAtt (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic) — Selective attention (text-only cancellation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated on the text-only selective-attention cancellation task (character grid); percent accuracy reported and compared to human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Does Spatial Cognition Emerge in Frontier MODELS?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal model evaluated here on text-only character-array tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Selective attention (SAtt, text-only character-grid)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>selective attention / visuospatial attention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Find target characters among distractors in a grid and report locations; multiple-choice answer format.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 96.0%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 97.02%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction prompt with detailed task description (zero-shot style prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>reported in this paper (SPACE benchmark; appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Claude 3.5 Sonnet nearly matches/exceeds human baseline on the text-only SAtt implementation (97.0% vs 96%); the paper highlights modality-dependent differences and simplified text encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Spatial Cognition Emerge in Frontier Models?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating spatial understanding of large language models <em>(Rating: 2)</em></li>
                <li>PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change <em>(Rating: 2)</em></li>
                <li>CogEval: Evaluating cognitive maps in large language models with cogeval: No emergent planning <em>(Rating: 2)</em></li>
                <li>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities <em>(Rating: 1)</em></li>
                <li>SpatialRGPT: Grounded spatial reasoning in vision language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7505",
    "paper_id": "paper-c74cb4c6f4d2042a7921bf7dd05fca8766150363",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4o - Direction (Ego image)",
            "name_full": "GPT-4o (OpenAI) — Direction estimation, ego-image presentation",
            "brief_description": "Multimodal LLM (OpenAI GPT-4o) evaluated on the SPACE benchmark's direction/pointing task using first-person (egocentric) images; performance reported as percent accuracy with uncertainty.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model with vision/video capability (supports image and sequence inputs).",
            "model_size": null,
            "test_name": "Direction estimation (egocentric image)",
            "test_category": "spatial orientation / large-scale spatial cognition",
            "test_description": "Pointing trial: from a given landmark A, estimate direction to landmark B; presented as a 4-way multiple-choice question using egocentric images from a walkthrough.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 82.8%",
            "llm_performance": "accuracy 32.0% ± 4.1",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±4.1% uncertainty; no p-value reported",
            "notes": "Performance is near chance (chance = 25%) and substantially below the human baseline (82.8%). Paper emphasizes models are near chance on egocentric large-scale spatial tasks.",
            "uuid": "e7505.0",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - Distance (Ego image)",
            "name_full": "GPT-4o (OpenAI) — Distance estimation, ego-image presentation",
            "brief_description": "GPT-4o evaluated on estimating Euclidean distances between landmarks from an egocentric walkthrough; multiple-choice accuracy reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model with vision/video capability.",
            "model_size": null,
            "test_name": "Distance estimation (egocentric image)",
            "test_category": "spatial perception / large-scale spatial cognition",
            "test_description": "From a current landmark, estimate straight-line distances to other landmarks; posed as 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 83.2%",
            "llm_performance": "accuracy 36.5% ± 5.0",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±5.0% uncertainty; no p-value reported",
            "notes": "Model substantially underperforms humans and is only modestly above chance (25%); paper notes models struggle with metric estimations.",
            "uuid": "e7505.1",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - Map sketching (Ego image)",
            "name_full": "GPT-4o (OpenAI) — Map sketching, ego-image presentation",
            "brief_description": "GPT-4o evaluated on choosing the correct map sketch of the environment after a walkthrough; 4-way multiple-choice accuracy given.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model with vision/video capability.",
            "model_size": null,
            "test_name": "Map sketching (egocentric image)",
            "test_category": "cognitive mapping / large-scale spatial cognition",
            "test_description": "Select which map sketch preserves the true spatial relationships between start, goal, and landmarks after viewing a walkthrough; 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 96.6%",
            "llm_performance": "accuracy 33.3% ± 4.1",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±4.1% uncertainty; no p-value reported",
            "notes": "Model performance is far below human baseline and near chance (25%); authors emphasize poor large-scale mapping ability in multimodal setting.",
            "uuid": "e7505.2",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - Direction (Ego image)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Direction estimation, ego-image presentation",
            "brief_description": "Anthropic's Claude 3.5 Sonnet evaluated on the pointing/direction task with egocentric visual inputs; accuracy and uncertainty reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model (Claude 3.5 Sonnet) with vision capability and instruction tuning.",
            "model_size": null,
            "test_name": "Direction estimation (egocentric image)",
            "test_category": "spatial orientation / large-scale spatial cognition",
            "test_description": "Pointing trial using egocentric images; 4-way multiple-choice direction estimation.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 82.8%",
            "llm_performance": "accuracy 29.0% ± 2.9",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±2.9% uncertainty; no p-value reported",
            "notes": "Performs close to chance and substantially below human baseline; authors flag consistent failure across large-scale egocentric tasks.",
            "uuid": "e7505.3",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - Distance (Ego image)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Distance estimation, ego-image presentation",
            "brief_description": "Claude 3.5 Sonnet measured on discrete distance estimation judgments from egocentric walkthroughs; presented as multiple-choice.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model with vision capability and instruction tuning.",
            "model_size": null,
            "test_name": "Distance estimation (egocentric image)",
            "test_category": "spatial perception / large-scale spatial cognition",
            "test_description": "Estimate Euclidean distances to other landmarks from current landmark, posed as 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 83.2%",
            "llm_performance": "accuracy 34.4% ± 2.9",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±2.9% uncertainty; no p-value reported",
            "notes": "Performance substantially below human baseline; paper notes models are poor at metric estimation and often near chance.",
            "uuid": "e7505.4",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - Map (Ego image)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Map sketching, ego-image presentation",
            "brief_description": "Claude 3.5 Sonnet evaluated on choosing correct allocentric map sketch after an egocentric walkthrough; multiple-choice accuracy reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model with vision capability and instruction tuning.",
            "model_size": null,
            "test_name": "Map sketching (egocentric image)",
            "test_category": "cognitive mapping / large-scale spatial cognition",
            "test_description": "Select which map sketch preserves true spatial relationships between elements after viewing walkthrough; 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 96.6%",
            "llm_performance": "accuracy 27.5% ± 8.3",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±8.3% uncertainty; no p-value reported",
            "notes": "Large gap versus humans; high uncertainty on map sketching indicates unstable performance across environments.",
            "uuid": "e7505.5",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v - Direction (Ego image)",
            "name_full": "GPT-4v (OpenAI) — Direction estimation, ego-image presentation",
            "brief_description": "GPT-4v (vision-capable GPT-4 variant) evaluated on pointing/direction trials using egocentric video-like inputs; accuracy reported with uncertainty.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4v",
            "model_description": "OpenAI's GPT-4 variant with vision capabilities (image/video inputs) and instruction tuning.",
            "model_size": null,
            "test_name": "Direction estimation (egocentric image)",
            "test_category": "spatial orientation / large-scale spatial cognition",
            "test_description": "Pointing trial: from a given landmark A in an egocentric view, choose direction to landmark B among four options.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 82.8%",
            "llm_performance": "accuracy 29.7% ± 0.3",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±0.3% uncertainty; no p-value reported",
            "notes": "Model performance near chance; authors emphasize near-chance behavior of multimodal models on egocentric large-scale tasks.",
            "uuid": "e7505.6",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v - Distance (Ego image)",
            "name_full": "GPT-4v (OpenAI) — Distance estimation, ego-image presentation",
            "brief_description": "GPT-4v evaluated on distance estimation between landmarks from egocentric walkthrough imagery; reported as percent accuracy.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4v",
            "model_description": "OpenAI multimodal LLM with vision capabilities.",
            "model_size": null,
            "test_name": "Distance estimation (egocentric image)",
            "test_category": "spatial perception / large-scale spatial cognition",
            "test_description": "Estimate Euclidean distances from a reference landmark to others; presented as 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 83.2%",
            "llm_performance": "accuracy 31.9% ± 2.7",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±2.7% uncertainty; no p-value reported",
            "notes": "Substantially below human baseline and near chance; authors highlight poor metric estimation capability.",
            "uuid": "e7505.7",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v - Map (Ego image)",
            "name_full": "GPT-4v (OpenAI) — Map sketching, ego-image presentation",
            "brief_description": "GPT-4v evaluated on map-sketch multiple-choice task after egocentric walkthrough; accuracy with uncertainty is reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4v",
            "model_description": "OpenAI multimodal LLM with vision capabilities.",
            "model_size": null,
            "test_name": "Map sketching (egocentric image)",
            "test_category": "cognitive mapping / large-scale spatial cognition",
            "test_description": "Choose the correct allocentric map sketch preserving spatial relationships after a walkthrough; 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 96.6%",
            "llm_performance": "accuracy 20.0% ± 11.8",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": "model reported with ±11.8% uncertainty; no p-value reported",
            "notes": "Very low performance with large uncertainty; authors emphasize failures in egocentric mapping tasks.",
            "uuid": "e7505.8",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - MRT (multimodal)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Mental Rotation Test (multimodal presentation)",
            "brief_description": "Claude 3.5 Sonnet evaluated on the Mental Rotation Test using visual (3D shapes) multimodal presentation; reported as percent correct.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model with vision capability and instruction tuning.",
            "model_size": null,
            "test_name": "Mental Rotation Test (MRT, multimodal visual presentation)",
            "test_category": "spatial visualization / small-scale spatial cognition",
            "test_description": "Identify rotated version of a reference 3D shape among distractors (visual Shepard–Metzler style stimuli); 4-choice multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 78.5%",
            "llm_performance": "accuracy 29.03%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Model substantially below human baseline on visual mental rotation; paper notes multimodal presentation is harder for models than some text encodings.",
            "uuid": "e7505.9",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - MRT (multimodal)",
            "name_full": "GPT-4o (OpenAI) — Mental Rotation Test (multimodal presentation)",
            "brief_description": "GPT-4o evaluated on the visual Mental Rotation Test (3D shapes); percent correct reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model with vision/video capability.",
            "model_size": null,
            "test_name": "Mental Rotation Test (MRT, multimodal visual presentation)",
            "test_category": "spatial visualization / small-scale spatial cognition",
            "test_description": "Select rotated version of a 3D reference shape among distractors (Shepard–Metzler stimuli); 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 78.5%",
            "llm_performance": "accuracy 33.32%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Model near chance and far below human performance on multimodal MRT; textual (simplified) versions are easier for LLMs per the paper.",
            "uuid": "e7505.10",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v - MRT (multimodal)",
            "name_full": "GPT-4v (OpenAI) — Mental Rotation Test (multimodal presentation)",
            "brief_description": "GPT-4v tested on the visual 3D mental rotation task; accuracy reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4v",
            "model_description": "OpenAI vision-capable GPT-4 variant (multimodal).",
            "model_size": null,
            "test_name": "Mental Rotation Test (MRT, multimodal visual presentation)",
            "test_category": "spatial visualization / small-scale spatial cognition",
            "test_description": "Identify rotated version of a 3D reference shape among distractors; 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 78.5%",
            "llm_performance": "accuracy 32.33%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Model performs near chance on multimodal MRT; paper highlights models do markedly worse on 3D visual rotations than humans.",
            "uuid": "e7505.11",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - SAtt (multimodal)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Selective attention (cancellation) task, multimodal",
            "brief_description": "Claude 3.5 Sonnet evaluated on selective spatial attention (cancellation) using visual icons; accuracy reported and compared to human baseline.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model with vision capability and instruction tuning.",
            "model_size": null,
            "test_name": "Selective attention (SAtt, multimodal visual cancellation)",
            "test_category": "selective attention / visuospatial attention",
            "test_description": "Locate and report positions of target stimuli embedded among distractors on a grid; presented as 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 95.0%",
            "llm_performance": "accuracy 90.53%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Claude approaches human-level on selective attention in multimodal presentation (90.5% vs 95% human), an exception among spatial tasks where some models match or exceed humans.",
            "uuid": "e7505.12",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - SAtt (multimodal)",
            "name_full": "GPT-4o (OpenAI) — Selective attention (cancellation) task, multimodal",
            "brief_description": "GPT-4o evaluated on the visual cancellation/selective attention task; accuracy reported and compared to humans.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model with vision/video capability.",
            "model_size": null,
            "test_name": "Selective attention (SAtt, multimodal visual cancellation)",
            "test_category": "selective attention / visuospatial attention",
            "test_description": "Find and report target stimuli positions among distractors on a grid; multiple-choice answer enumerates target coordinates.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 95.0%",
            "llm_performance": "accuracy 70.22%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Strong but not human-level performance (70.2% vs 95%) in multimodal SAtt; authors note some LLMs match or exceed humans on selective-attention text encodings but multimodal results are mixed.",
            "uuid": "e7505.13",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v - SAtt (multimodal)",
            "name_full": "GPT-4v (OpenAI) — Selective attention (cancellation) task, multimodal",
            "brief_description": "GPT-4v measured on the visual selective-attention cancellation task; percent accuracy reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4v",
            "model_description": "OpenAI vision-capable GPT-4 variant (multimodal).",
            "model_size": null,
            "test_name": "Selective attention (SAtt, multimodal visual cancellation)",
            "test_category": "selective attention / visuospatial attention",
            "test_description": "Identify target stimuli among distractors on a grid and report coordinates; 4-way multiple-choice format.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 95.0%",
            "llm_performance": "accuracy 59.82%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Performs substantially below human baseline in multimodal SAtt; however some other LLMs (Claude, GPT-4o) perform far better on this task.",
            "uuid": "e7505.14",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - MRT (text-only)",
            "name_full": "GPT-4o (OpenAI) — Mental Rotation Test (text-only presentation)",
            "brief_description": "GPT-4o evaluated on a simplified text-only (2D character-array) mental rotation test; percent accuracy reported and compared to human baseline.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model (text + vision capable); here evaluated on text-only 2D encodings.",
            "model_size": null,
            "test_name": "Mental Rotation Test (MRT, text-only 2D arrays)",
            "test_category": "spatial visualization / small-scale spatial cognition",
            "test_description": "2D character-array version of mental rotation; choose rotated correct alternative among distractors.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 90.0%",
            "llm_performance": "accuracy 41.93%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Text-only simplified MRT yields higher LLM performance than multimodal 3D MRT, but still well below human baseline.",
            "uuid": "e7505.15",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - MRT (text-only)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Mental Rotation Test (text-only presentation)",
            "brief_description": "Claude 3.5 Sonnet evaluated on a simplified text-only mental rotation task (2D arrays); percent correct reported.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model with instruction tuning; here evaluated on text-only character-array encodings.",
            "model_size": null,
            "test_name": "Mental Rotation Test (MRT, text-only 2D arrays)",
            "test_category": "spatial visualization / small-scale spatial cognition",
            "test_description": "Simplified 2D textual version of mental rotation; select correct rotated alternative from distractors.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 90.0%",
            "llm_performance": "accuracy 37.52%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Performs substantially below human baseline even on simplified textual MRT; authors note text-only variants are easier but still far from human levels on many tasks.",
            "uuid": "e7505.16",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o - SAtt (text-only)",
            "name_full": "GPT-4o (OpenAI) — Selective attention (text-only cancellation)",
            "brief_description": "GPT-4o evaluated on text-only cancellation/selective-attention task (characters in a grid); reported percent accuracy approaches human levels.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI multimodal/instruction-tuned large language model; evaluated here on text-only 2D character arrays.",
            "model_size": null,
            "test_name": "Selective attention (SAtt, text-only character-grid)",
            "test_category": "selective attention / visuospatial attention",
            "test_description": "Locate target characters among distractors in a character grid and report their positions; 4-way multiple-choice.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 96.0%",
            "llm_performance": "accuracy 98.82%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "On the text-only selective attention implementation GPT-4o slightly exceeds the reported human baseline (98.8% vs 96%); paper cautions that text encodings can be substantially easier than visual counterparts.",
            "uuid": "e7505.17",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet - SAtt (text-only)",
            "name_full": "Claude 3.5 Sonnet (Anthropic) — Selective attention (text-only cancellation)",
            "brief_description": "Claude 3.5 Sonnet evaluated on the text-only selective-attention cancellation task (character grid); percent accuracy reported and compared to human baseline.",
            "citation_title": "Does Spatial Cognition Emerge in Frontier MODELS?",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal model evaluated here on text-only character-array tasks.",
            "model_size": null,
            "test_name": "Selective attention (SAtt, text-only character-grid)",
            "test_category": "selective attention / visuospatial attention",
            "test_description": "Find target characters among distractors in a grid and report locations; multiple-choice answer format.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": "accuracy 96.0%",
            "llm_performance": "accuracy 97.02%",
            "prompting_method": "instruction prompt with detailed task description (zero-shot style prompts)",
            "fine_tuned": false,
            "human_data_source": "reported in this paper (SPACE benchmark; appendix)",
            "statistical_significance": null,
            "notes": "Claude 3.5 Sonnet nearly matches/exceeds human baseline on the text-only SAtt implementation (97.0% vs 96%); the paper highlights modality-dependent differences and simplified text encodings.",
            "uuid": "e7505.18",
            "source_info": {
                "paper_title": "Does Spatial Cognition Emerge in Frontier Models?",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating spatial understanding of large language models",
            "rating": 2
        },
        {
            "paper_title": "PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
            "rating": 2
        },
        {
            "paper_title": "CogEval: Evaluating cognitive maps in large language models with cogeval: No emergent planning",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities",
            "rating": 1
        },
        {
            "paper_title": "SpatialRGPT: Grounded spatial reasoning in vision language model",
            "rating": 1
        }
    ],
    "cost": 0.029017249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Does Spatial Cognition Emerge in Frontier MODELS?</h1>
<p>Santhosh Kumar Ramakrishnan* Erik Wijmans Philipp Krähenbühl Vladlen Koltun</p>
<p>Apple</p>
<h4>Abstract</h4>
<p>Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition. Code and data are available: https://github.com/apple/ml-space-benchmark</p>
<h2>1 INTRODUCTION</h2>
<p>Frontier models have achieved impressive performance in mathematics, coding, general knowledge, and commonsense reasoning (Hendrycks et al., 2021a;b; Chen et al., 2021; Sakaguchi et al., 2021; Yue et al., 2024). This remarkable progress has inspired characterizations of frontier models as possessing the intelligence of a smart high schooler and predictions of the imminent arrival of superintelligence (Aschenbrenner, 2024). These characterizations are often underpinned by the premise that competence (or even mastery) in some aspects of cognition is symptomatic of broad cognitive competence. This is not self-evident. To quote Brooks's first law of artificial intelligence, "When an AI system performs a task, human observers immediately estimate its general competence in areas that seem related. Usually that estimate is wildly overinflated." (Brooks, 2024).</p>
<p>Our work focuses on spatial cognition, a foundational form of intelligence that is present in a broad spectrum of animals including humans (Marshall \&amp; Fink, 2001; Waller \&amp; Nadel, 2013; Mallot, 2024). Spatial cognition refers to the ability of animals to perceive and interact with their surroundings, build mental representations of objects and environments, and draw upon these representations to support navigation and manipulation. Decades of research in animal cognition have characterized the spatial cognition of mice, rats, bats, pigeons, corvids, dogs, wolves, elephants, marmosets, tamarins, howler monkeys, baboons, chimpanzees, and humans (Tolman, 1948; Menzel, 1973; Peters, 1974; Gillner \&amp; Mallot, 1998; Marshall \&amp; Fink, 2001; Noser \&amp; Byrne, 2007; Tommasi et al., 2012; Porter \&amp; Garber, 2013; Blaser et al., 2013; Geva-Sagiv et al., 2015; Presotto et al., 2019; de Guinea et al., 2021; Payne et al., 2021; Xu et al., 2024; Xavier et al., 2024; Welklin et al., 2024). Human infants already possess rudimentary spatial cognition, which subsequently improves along developmental schedules that have been characterized (Blades \&amp; Spencer, 1994; Newcombe, 2000; Vasilyeva \&amp; Lourenco, 2012). Spatial cognition is known to underpin more advanced cognitive abilities (Kozhevnikov et al., 2007; Newcombe, 2010; Young et al., 2018).</p>
<p>The emergence of spatial cognition has been linked to embodiment (Smith \&amp; Gasser, 2005; Jansen \&amp; Heil, 2010; Frick \&amp; Möhring, 2016), without which the development of spatial cognition may be impaired (Foreman et al., 1990; Anderson et al., 2013). However, frontier models are typically trained in a disembodied manner on corpora of text, images, and video. Does spatial cognition emerge in disembodied frontier models? To study this question systematically, we develop SPACE,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SPACE: Spatial Perception And Cognition Evaluation. We design a suite of spatial cognition tasks based on the cognitive science literature. These are broadly classified into large-scale and small-scale spatial cognition. Large-scale tasks require understanding space at the level of environments and evaluate spatial orientation and cognitive mapping abilities. Small-scale tasks require understanding space at the level of objects or object arrangements and evaluate skills such as spatial visualization, spatial orientation, spatial perception, selective spatial attention and visuospatial working memory. These tasks can be multiple-choice question answering, or interactive games. We develop multimodal as well as purely textual presentations, which support evaluation of both large language models (LLMs) and vision-language models (VLMs).
a benchmark that builds on decades of research in cognitive science. Our benchmark comprises two broad classes of tasks, covering large-scale and small-scale spatial cognition (Hegarty et al., 2006; Meneghetti et al., 2022; Newcombe, 2024). See Figure 1 for an overview.</p>
<p>Large-scale spatial cognition has to do with a model's ability to understand its surroundings. In large-scale tasks, the model is familiarized with an environment and is then asked to estimate distances and directions to landmarks, sketch a map of the environment, retrace a known route, or discover a shortcut to a goal. Small-scale spatial cognition has to do with a model's ability to perceive, imagine, and mentally transform objects in two or three dimensions. Together, large-scale and small-scale tasks evaluate core cognitive abilities such as spatial perception, visualization, orientation, selective attention, and visuospatial memory (Lacroix et al., 2021; Meneghetti et al., 2022).</p>
<p>We design text-based and image-based presentations to evaluate both large language-only and vision-language models (LLMs and VLMs, respectively). Our results indicate that contemporary frontier models have not yet reached competency - let alone mastery - in spatial cognition. On key large-scale spatial cognition tasks, frontier multimodal models perform near chance level, even when presented with an allocentric (map) view of the environment. The strongest models exhibit much better performance on some small-scale tasks that evaluate selective spatial attention and visuospatial working memory, especially with purely textual presentations via character arrays, but perform near chance on other tasks such as mental rotation (Vandenberg \&amp; Kuse, 1978), perspective taking (Kozhevnikov \&amp; Hegarty, 2001), maze completion (Lacroix et al., 2021), or the classic Minnesota Paper Form Board test (Likert \&amp; Quasha, 1941; 1969).</p>
<h1>2 Related work</h1>
<p>Spatial cognition. Spatial cognition is a branch of cognitive science that seeks to understand how humans and animals perceive, interpret, represent, and interact with objects and environments (Marshall \&amp; Fink, 2001; Landau, 2002; Waller \&amp; Nadel, 2013; Mallot, 2024; Newcombe, 2024). This involves the perception of object sizes, shapes, and scales, as well as the relationships between objects and landmarks in the environment (including location, distance, direction, and orientation). Spatial cognition is broadly divided into two categories: large-scale and small-scale (Hegarty et al., 2006; Jansen, 2009; Meneghetti et al., 2022; Newcombe, 2024). Large-scale spatial cognition refers to the ability to build spatial representations of environments and use them effectively for navigation and spatial reasoning. Large-scale spatial cognition tasks typically involve egocentric spatial transformations, where the viewer's perspective changes with respect to the environment while the spatial relationships between parts of the environment remain constant (Wang et al., 2014). Smallscale spatial cognition refers to the ability to perceive, imagine, and mentally transform objects or shapes in 2D or 3D. This is typically evaluated using paper and pencil tasks that require allocentric spatial transformations of objects and shapes (Wang et al., 2014). While large-scale spatial cognition has been demonstrated in a wide range of animals (Tolman, 1948; Menzel, 1973; Peters, 1974;</p>
<p>O’Keefe \&amp; Nadel, 1978; Gillner \&amp; Mallot, 1998; Richardson et al., 1999; Geva-Sagiv et al., 2015; Toledo et al., 2020), the study of small-scale spatial cognition is specific to humans.</p>
<p>Emergent spatial representations. Several works have shown that spatial representations, a phenomenon similar to spatial cognition, can emerge in neural networks (Banino et al., 2018; Cueva \&amp; Wei, 2018; Wijmans et al., 2023; Sorscher et al., 2023). These works train a neural network from scratch for path integration or navigation tasks and analyze the model weights to identify spatial representations.</p>
<p>Spatial reasoning in large language models. PlanBench (Valmeekam et al., 2024) and CogEval (Momennejad et al., 2023) evaluate LLMs on text-based planning tasks such as navigation, delivery logistics and block stacking to evaluate cognitive mapping and planning. Yamada et al. (2024) evaluate spatial reasoning in LLMs by performing map traversals on different types of graphs and evaluate the model's self-localization ability. EWOK (Ivanova et al., 2024) studies spatial plausibility reasoning in LLMs. In comparison to these benchmarks, SPACE evaluates a broader array of cognitive abilities and implements multimodal presentations of classic animal cognition experiments.</p>
<p>Benchmarks for large multimodal models. The recent successes of multimodal models (OpenAI, 2024; Li et al., 2024a; Reid et al., 2024) have been facilitated by large-scale training on text and multimodal corpora (Rana, 2010; Together Computer, 2023; Chen et al., 2023; Laurençon et al., 2023; Gadre et al., 2023), followed by tuning on human preferences (Liu et al., 2023a; Awadalla et al., 2024; Ouyang et al., 2022; Rafailov et al., 2023). The remarkable advances in the capabilities of these models inspired a variety of benchmarks that evaluate their performance. Early multimodal benchmarks consisted of single-task datasets such as visual question answering (Antol et al., 2015; Goyal et al., 2019; Marino et al., 2019) and image captioning (Chen et al., 2015). However, due to the limited scope of early datasets and concerns regarding potential test-data leakage, newer benchmarks use diverse collections of tasks (Fu et al., 2023; Yu et al., 2024; Liu et al., 2023b; Yue et al., 2024; Lu et al., 2024; Ying et al., 2024). While these datasets primarily focus on image understanding, newer datasets that emphasize spatiotemporal reasoning have been proposed for video (Li et al., 2024b; Fu et al., 2024a; Majumdar et al., 2024).</p>
<p>Recent studies highlight a number of shortcomings of frontier multimodal models (Moskvichev et al., 2023; Tong et al., 2024; Chen et al., 2024a; Fu et al., 2024b). One such shortcoming is that models may not perceive the image in detail, often missing fine-grained details or ignoring the image entirely (Chen et al., 2024b; Guan et al., 2024; Tong et al., 2024). HallusionBench proposes a new dataset of image pairs, where tiny edits are made from one image to another that change the answer to the question (Guan et al., 2024). MMVP identifies issues with CLIP-based pretraining of visual encoders, which make current models blind to certain visual patterns, and proposes a benchmark of CLIP-blind image pairs where the same question has opposite answers (Tong et al., 2024). MMStar shows that many questions in multimodal benchmarks can be answered correctly without the image and proposes a new split of existing benchmarks that addresses this issue (Chen et al., 2024b).</p>
<p>Another shortcoming of existing models is their lack of spatial perception and reasoning (Chen et al., 2024a; Cheng et al., 2024). SpatialVLM proposes a VQA dataset that requires answering questions about relative spatial arrangements and metric relationships (Chen et al., 2024a). SpatialRGPT further includes region-level understanding (Cheng et al., 2024). MOCHI evaluates the ability of vision models to identify rotated versions of procedurally-generated objects (Bonnen et al., 2025). 'Perception test' aims to overcome shortcomings of standard video datasets by creating a diagnostic dataset where participants record videos while following complex scripts depicting interesting events (Patraucean et al., 2023). It evaluates fundamental perceptual skills (memory, abstraction, intuitive physics, and semantics) and various types of reasoning.</p>
<p>Another line of work considers skill acquisition (the ability to learn a skill and apply it to new scenarios). Prior work has studied this using visual analogical reasoning (Chollet, 2019; Moskvichev et al., 2023; Yiu et al., 2024). The ARC dataset contains samples consisting of a few examples of abstract grids and their transformations and one or more test inputs (Chollet, 2019). The objective is to understand the transformation performed using the examples and apply it to test inputs. The transformations have been further organized into specific concepts with varying degrees of difficulty in the ConceptARC dataset (Moskvichev et al., 2023). Inspired by ARC and developmental psy-</p>
<p>chology, the KiVA dataset studies visual analogies in the context of visually realistic 3D shapes with concepts like transformations in color, size, rotations, reflections, and counting (Yiu et al., 2024).</p>
<h1>3 SPACE: A BENCHMARK FOR SPATIAL PERCEPTION AND COGNITION EVALUATION</h1>
<p>We develop a benchmark for evaluating the spatial cognition of frontier models. The benchmark comprises large-scale and small-scale tasks and is designed for compatibility with both text-only and multimodal models.</p>
<h3>3.1 LARGE-SCALE SPATIAL COGNITION</h3>
<p>In large-scale spatial cognition tasks, we evaluate the ability of models to build spatial representations of their surrounding environment, and whether they can use these representations to reason about and navigate in the environment. There are two stages to these tasks. First, we familiarize the model with an environment by showing a video walkthrough. ${ }^{1}$ The model must build a mental representation of the environment that captures the locations of start, goal and landmark locations, and their spatial relationships. After the model is familiarized with the environment, we evaluate the model's spatial representation using five tasks derived from the cognitive science literature (Meneghetti et al., 2022). See Figure 2(top) and Figure 3 for an overview.</p>
<ol>
<li>Direction estimation. The goal is to determine the directions to other landmarks from a given landmark. The participant is asked to pretend that they are facing a landmark A , and then asked to estimate the direction (in degrees) to another landmark B. This is known as a pointing trial in the cognitive science literature (Allen et al., 1996; Hegarty et al., 2006; Pazzaglia \&amp; Taylor, 2007; Weisberg et al., 2014; Meneghetti et al., 2016). We formulate this as a multiple-choice QA task with four options for the direction (only one correct option).</li>
<li>Distance estimation. The goal is to determine the straight-line distances from one landmark to all other landmarks (Allen et al., 1996; Hegarty et al., 2006). The participant is asked to pretend that they are facing a landmark A, and then asked to estimate the Euclidean distance to all the other landmarks. We pose this as a multiple-choice QA with four options for the list of distances to each landmark. Since current models are not good at estimating metric measurements (Chen et al., 2024a; Cheng et al., 2024), we generate incorrect options such that the ratios of distances between landmarks are not preserved, making it easier to identify the correct option.</li>
<li>Map sketching. The goal is to draw a map of the environment that contains the start, goal and landmark positions (Allen et al., 1996; Hegarty et al., 2006; Pazzaglia \&amp; Taylor, 2007; Weisberg et al., 2014; Meneghetti et al., 2016; 2021). We formulate this as multiple-choice QA with four options for the map sketches. The correct option preserves the true spatial relationships between the different map elements, while the incorrect options skew the spatial relationships randomly.</li>
<li>Route retracing. The goal is to retrace the route shown in the video from the start to the goal (Allen et al., 1996; Pazzaglia \&amp; Taylor, 2007; Meneghetti et al., 2016; 2021). This task evaluates the model's ability to remember landmarks seen in the route and the actions required along the route to reach the goal. We formulate this as an interactive task where the model receives the current observation, decides which action to take, and receives updated observations based on the actions taken. We measure performance using the SPL metric (success weighted by path length), which penalizes the model for taking unnecessary detours (Anderson et al., 2018). (The demonstrated route, which the model must retrace, is always the shortest path to the goal.)</li>
<li>Shortcut discovery. The goal is to discover a shortcut (i.e., a route never observed before) from the start to the goal after observing a video walkthrough that takes detours to reach the goal (Tolman, 1948; Allen et al., 1996; Pazzaglia \&amp; Taylor, 2007; Meneghetti et al., 2016; 2021). The ability to discover shortcuts in familiar environments is a key indicator of cognitive mapping ability (Tolman, 1948). When designing environments and walkthrough paths, we ensured that a novel shortcut exists that the model can exploit. Similar to route retracing, we treat this as an interactive navigation task and measure performance using the SPL metric.
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The tasks in SPACE. For all tasks (other than the water level test), we include multimodal as well as purely textual presentations, to support evaluating both large language models (LLMs) and vision-language models (VLMs). For large-scale tasks, we visualize examples from the egocentric image presentation here and visualize alternate presentations in Figure 3. For small-scale tasks, we visualize both visual and textual presentations here. Bolding of characters in the arrays is for illustration purposes only.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Large-scale spatial cognition. We design ten environment layouts based on experimental protocols in cognitive science. The top row shows bird's-eye view renderings of these environments. To evaluate largescale spatial cognition in frontier models, we implement three observation spaces: egocentric image, discrete map (DM) image, and discrete map (DM) text (see bottom row). Ego image shows a first-person view within the environment. DM image shows a quantized, allocentric bird's-eye view of the $2.5 \mathrm{~m} \times 2.5 \mathrm{~m}$ region centered on the current position. Unlike ego image, DM image enables performing the large-scale tasks in a simplified setting without requiring perspective geometry. DM text depicts the DM image using text characters. We evaluate multimodal models using ego image and DM image, and large language models using DM text.</p>
<h1>3.1.1 IMPLEMENTATION</h1>
<p>3D environment generation. We create ten environment layouts based on prior work in cognitive science and artificial intelligence (Tolman, 1948; Gillner \&amp; Mallot, 1998; Richardson et al., 1999; Banino et al., 2018; Bouchekioua et al., 2021). Figure 3 shows bird's-eye view images of each layout. See the appendix for more details about the environment generation process.</p>
<p>Observation spaces. We create multiple observation spaces to support evaluating both text-only and vision+text models. These are egocentric images, discrete map (DM) images, and discrete map (DM) text presentations.</p>
<ul>
<li>Ego image. The environment is captured using a forward-facing perspective camera placed at the model's location in the environment. This is similar to the setup of an animal navigating through an immersive environment and requires understanding perspective geometry.</li>
<li>DM image. This is a quantized bird's-eye view image of a $2.5 \mathrm{~m} \times 2.5 \mathrm{~m}$ area in the environment surrounding the model's location. This is akin to a human using a map to navigate. The current location is always at the center of the DM image. We use a Pacman-like coloring scheme highlighting the obstacles, navigable space, current postion, and landmarks. DM image simplifies the mapping process by removing the need for perspective geometry understanding.</li>
<li>DM text. This is a translation of DM image to an text array. We carefully select the text encoding to ensure compatibility with text tokenizers of popular models and ensure that each element of the array is encoded by the tokenizers of all evaluated models as a distinct token.</li>
</ul>
<p>See Figure 3 (bottom) for examples of these presentations. The first two observation spaces are used for models that support visual inputs, while the last observation space is used for text-only models. See the appendix for additional illustrations of these tasks and dataset statistics.</p>
<h3>3.2 SMALL-SCALE SPATIAL COGNITION</h3>
<p>In small-scale spatial cognition tasks, we evaluate the models' ability to perceive, imagine, and mentally transform objects or shapes in two and three dimensions. We build on the body of work on visuospatial abilities, which are evaluated in humans via paper-and-pencil tasks (Allen et al., 1996; Weisberg et al., 2014; Meneghetti et al., 2022). These abilities may be used to explain individual differences between participants in large-scale spatial cognition (Meneghetti et al., 2022). We define ten small-scale tasks to evaluate abilities such as spatial perception, spatial visualization, spatial orientation, selective attention, and visuospatial working memory. See Figure 2(bottom) for illustrations of each task. We summarize each task below and provide additional details in the appendix.</p>
<ol>
<li>Mental rotation test (MRT). This is a test of spatial visualization, i.e., the ability to mentally manipulate 2D or 3D stimuli (Vandenberg \&amp; Kuse, 1978). In the visual presentation, a reference 3D shape from Shepard \&amp; Metzler (1971) is provided along with four choices. The correct choice is a rotated version of the reference, and the remaining choices are rotated versions of an alternate shape. The goal is to identify the correct choice from the distractors. The text-only version of this task uses 2D character arrays, akin to the card rotations test from French et al. (1963).</li>
<li>Perspective taking test (PTT). This is a test of spatial orientation, i.e., the ability to imagine being in a different position in space and seeing the surroundings from a new perspective (Kozhevnikov \&amp; Hegarty, 2001). We place $N$ randomly-sampled objects (like apples, bats, dogs, books, grapes, etc.) at random locations in an image (with no overlap between objects). The objective is to take the perspective of standing next to an object (say, a bat) facing another object (say, a book), and determine the relative orientation of a third object (say, an apple). This is a multiple-choice QA with four options (only one correct option).</li>
<li>Water level test (WLT). This is a test of spatial perception (Piaget et al., 1957). Originally, it was designed to evaluate children's knowledge about the horizontal nature of the surface of water in a sealed bottle regardless of its orientation. Performance on the water-level test was found to be related to performance on spatial ability tests (Foltz, 1978; Wittig \&amp; Allen, 1984). We present the model with an image of a water container partially filled with water and ask it to imagine the position of the water if the container were tilted. We implement this as a four-way multiplechoice QA, where each choice is an image showing the tilted container with varying water levels. The objective is to select the one choice that shows the correct water level.</li>
<li>Minnesota Paper Form Board test (MPFB). This is a test of spatial visualization, where the model must perform multi-step manipulations of complex spatial information (Meneghetti et al., 2022). Specifically, we provide the model with pieces of a figure and ask it to identify how the pieces fit together (Likert \&amp; Quasha, 1941; 1969). We programmatically segment a square into five pieces, and rotate the pieces randomly to generate the final segments. We generate alternate segmentations of a square as negative choices for a multiple-choice QA presentation.</li>
<li>Judgement of Line Orientation test (JLO). This is a test of spatial perception (Benton, 1994), where a model must determine the angle between two lines in an image. Our visual presentation shows two lines in an image along with a set of 11 reference lines. The objective is to determine the pair of reference lines that have the same angles between them as the lines in the image. This is presented as a multiple-choice QA with four choices (only one of them correct). Our text-only presentation implements the tasks via lines embedded in 2D integer arrays.</li>
<li>Selective attention task (SAtt). This is a test of selective spatial attention, i.e., the ability to selectively attend to a particular region of space while ignoring others (Serences \&amp; Kastner, 2014; Pahor et al., 2022). In particular, we use the widely used cancellation task, where the goal is to search for and mark out target stimuli embedded amidst distractors (Della Sala et al., 1992; Brickenkamp \&amp; Zillmer, 1998; Dalmaijer et al., 2015; Lacroix et al., 2021; Pahor et al., 2022; Kalina \&amp; Walgrave, 2004). We design the task as multiple-choice QA with objects as the stimuli for visual evaluation and characters as stimuli for text-only evaluation. The target stimuli and distractors are arranged on a grid. The answer must be selected from one out of four options. The correct option lists the (row, column) pairs that localize the target stimuli in the grid.</li>
<li>Maze completion task (MCT). This is an interactive game to evaluate spatial orientation, planning, and executive functioning (Lacroix et al., 2021). We programmatically create mazes using Mazelib (Stilley, 2014) and render them using a Pacman-like color scheme for the visual presentation and a character array for the text-only presentation (similar to DM image and DM text in Figure 3). Using the maze rendering, a model must sequentially select an up/down/left/right action to reach the goal and execute a stop action to successfully complete the task. If the model does not reach the goal within 250 actions, it is considered to have failed. We measure the success rate, i.e., the percentage of mazes where the model reaches the goal within the allotted time.</li>
<li>Corsi block-tapping task (CBTT). This is a test of visuospatial working memory and attention (Corsi, 1972; Claessen et al., 2015). We create a digital Corsi board with $N$ blue-colored blocks that are randomly placed on the board with no overlap $(N \in[5,8])$. We randomly sample a sequence of $K$ taps, where each block is tapped at most once $(K \in[4, N])$. The taps are digitally rendered on the blocks by highlighting them in yellow when tapped, yielding an sequence of $K$ images. After presenting the $K$ images, we provide a rendering of the board with integer IDs</li>
</ol>
<p>assigned to each block and ask the model to reproduce the sequence of taps using these IDs. We treat this as multiple-choice QA with four choices of tap sequences, only one of which is correct.
14. Spatial addition task (SAdd). This is a test of visuospatial working memory, i.e., the ability to store and manipulate spatial information in memory (Wechsler, 2009). The model is presented with two 2D grids, where each grid location can be empty or contain a blue or red dot. The objective is to add the two grids together by following certain rules. If a grid location has a blue dot in exactly one of grids, the result should be a blue dot. If a grid location has blue dots on both grids, the result should be a white dot. Red dots are distractors and must be ignored. We programmatically generate grid pairs with sizes sampled from ${3,5,7,9}$ and pseudo-randomly populate them with blue and red dots. We formulate the task as multiple-choice QA, presenting four grids as possible answers, exactly one of which is correct.
15. Cambridge spatial working memory test (CSWM). This is an interactive game that evaluates visuospatial working memory (Sahakian et al., 1988). The model is presented with an image containing $N$ blue colored boxes $(N \in[3,7])$. A yellow 'treasure' is initially hidden in one of the boxes. The model must sequentially select boxes one at a time to find the hidden treasure. Once the treasure is found, another treasure is placed in one of the remaining boxes. The objective is to locate all the yellow treasures via a process of elimination. We programmatically generate instances of this task by randomly sampling blue boxes, placing them at random locations (without overlap), and placing the treasures in each box in random order. At each step, we assign random integer IDs to each box as a reference for selecting a box. The boxes' integer IDs are randomized in each step, forcing the model to remember boxes based on their spatial positions. When the model finds a treasure, the box containing the treasure becomes yellow. The model must find all the treasures before a time limit $T$ (determined based on $N$ ) to succeed.</p>
<p>As with large-scale spatial cognition, we also implement purely textual presentations of these tasks to support evaluation of large language models (LLMs). Figure 2 illustrates both the multimodal and the purely textual presentations. The key idea in instantiating the textual presentations is to encode all spatial information via 2D character arrays. We did not identify a natural such encoding for the water level test (WLT) and did not include a text-only presentation for it for this reason. See the appendix for additional illustrations of these tasks. In some tasks, such as MRT, MPFB, and JLO, the text presentations are substantially easier than the corresponding visual presentations. However, the visual and textual presentations match closely for the remaining tasks, enabling us to identify modality-specific limitations of multimodal models by evaluating them on the two presentations.</p>
<h1>4 EXPERIMENTS</h1>
<p>Baselines. We evaluate a number of LLMs and VLMs. Using text-only presentations, we evaluate GPT-4v and GPT-4o (OpenAI, 2023; 2024), Claude 3.5 Sonnet (Anthropic, 2024), the Llama3 family (Dubey et al., 2024), Mistral models such as Mixtral 8x7B, Mixtral 8x22B, and Mistral 123B (Jiang et al., 2024; Mistral AI team, 2024a), and two Yi 1.5 models (Young et al., 2024). Using multimodal presentations, we evaluate GPT-4v and GPT-4o (OpenAI, 2023; 2024), Claude 3.5 Sonnet (Anthropic, 2024), LlaVA-NeXT-Interleave (Li et al., 2024a), Pixtral 12B (Mistral AI team, 2024b), and Phi-3.5-vision (Abdin et al., 2024). We use the vLLM inference engine for evaluating the open-source models (Kwon et al., 2023). For each task, we implement a prompt that provides a detailed description of the task and the expected response format (see the appendix). We also list the results of a chance baseline that selects an answer at random. For multiple-choice QA tasks, chance is at $25 \%$. For interactive tasks, the chance baseline samples an action at random in each step. We further include human performance for reference for the multiple-choice QA tasks. See the appendix for additional implementation details.</p>
<p>Large-scale spatial cognition results. The results are shown in Table 1, grouped by presentation modality (ego image, DM image, DM text). For image-based presentations, we evaluate Claude 3.5 Sonnet, GPT-4v and GPT-4o because they support video understanding (via a succession of images). For DM text, we evaluate both open and closed LLMs. We also list the performance of the chance baseline for calibration, as well as human performance (see the appendix for details). In the text-only modality, Claude 3.5 Sonnet attains the highest average performance. Mistral 123B is the highest-performing open model. All evaluated models struggle with large-scale spatial cognition, falling significantly below human performance on direction estimation, distance estimation,</p>
<table>
<thead>
<tr>
<th>Observation space: Ego image</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td>Direction estimation</td>
<td>Distance estimation</td>
<td>Map sketching</td>
<td>Route retracing</td>
<td>Shortcut discovery</td>
<td>Average</td>
</tr>
<tr>
<td>Human</td>
<td>82.8</td>
<td>83.2</td>
<td>96.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>$32.0 \pm 4.1$</td>
<td>$36.5 \pm 5.0$</td>
<td>$33.3 \pm 4.1$</td>
<td>$6.6 \pm 3.6$</td>
<td>$6.4 \pm 1.0$</td>
<td>23.0</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>$29.0 \pm 2.9$</td>
<td>$34.4 \pm 2.9$</td>
<td>$27.5 \pm 8.3$</td>
<td>$7.4 \pm 2.8$</td>
<td>$0.0 \pm 0.0$</td>
<td>19.6</td>
</tr>
<tr>
<td>GPT-4v</td>
<td>$29.7 \pm 0.3$</td>
<td>$31.9 \pm 2.7$</td>
<td>$20.0 \pm 11.8$</td>
<td>$1.6 \pm 1.2$</td>
<td>$3.9 \pm 0.9$</td>
<td>17.4</td>
</tr>
<tr>
<td>Chance</td>
<td>25.0</td>
<td>25.0</td>
<td>25.0</td>
<td>0.0</td>
<td>0.0</td>
<td>15.0</td>
</tr>
<tr>
<td>Observation space: DM image</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Method</td>
<td>Direction estimation</td>
<td>Distance estimation</td>
<td>Map sketching</td>
<td>Route retracing</td>
<td>Shortcut discovery</td>
<td>Average</td>
</tr>
<tr>
<td>Human</td>
<td>82.9</td>
<td>82.5</td>
<td>100.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>$29.5 \pm 5.5$</td>
<td>$31.9 \pm 4.0$</td>
<td>$33.3 \pm 3.1$</td>
<td>$23.6 \pm 3.1$</td>
<td>$25.9 \pm 2.0$</td>
<td>28.8</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>$32.5 \pm 2.3$</td>
<td>$40.0 \pm 2.6$</td>
<td>$30.0 \pm 4.1$</td>
<td>$15.4 \pm 4.3$</td>
<td>$13.7 \pm 6.4$</td>
<td>26.3</td>
</tr>
<tr>
<td>GPT-4v</td>
<td>$26.3 \pm 3.6$</td>
<td>$29.3 \pm 3.1$</td>
<td>$45.0 \pm 5.0$</td>
<td>$13.7 \pm 5.2$</td>
<td>$15.3 \pm 3.9$</td>
<td>25.9</td>
</tr>
<tr>
<td>Chance</td>
<td>25.0</td>
<td>25.0</td>
<td>25.0</td>
<td>0.0</td>
<td>0.0</td>
<td>15.0</td>
</tr>
<tr>
<td>Observation space: DM text</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Method</td>
<td>Direction estimation</td>
<td>Distance estimation</td>
<td>Map sketching</td>
<td>Route retracing</td>
<td>Shortcut discovery</td>
<td>Average</td>
</tr>
<tr>
<td>Human</td>
<td>66.7</td>
<td>76.5</td>
<td>66.7</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>$29.2 \pm 4.4$</td>
<td>$40.2 \pm 3.1$</td>
<td>$51.7 \pm 5.5$</td>
<td>$26.5 \pm 2.9$</td>
<td>$20.0 \pm 3.0$</td>
<td>33.5</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>$28.7 \pm 4.1$</td>
<td>$33.3 \pm 1.7$</td>
<td>$46.7 \pm 4.1$</td>
<td>$27.5 \pm 3.2$</td>
<td>$26.6 \pm 0.1$</td>
<td>32.6</td>
</tr>
<tr>
<td>Mistral 123B</td>
<td>$30.5 \pm 5.1$</td>
<td>$28.9 \pm 5.7$</td>
<td>$38.3 \pm 5.5$</td>
<td>$20.3 \pm 2.8$</td>
<td>$19.9 \pm 3.0$</td>
<td>27.6</td>
</tr>
<tr>
<td>GPT-4v</td>
<td>$30.7 \pm 4.1$</td>
<td>$26.5 \pm 2.7$</td>
<td>$40.8 \pm 6.0$</td>
<td>$20.6 \pm 5.8$</td>
<td>$15.4 \pm 2.9$</td>
<td>26.8</td>
</tr>
<tr>
<td>Llama 3 70B</td>
<td>$27.0 \pm 2.2$</td>
<td>$30.4 \pm 1.9$</td>
<td>$35.0 \pm 8.3$</td>
<td>$13.2 \pm 9.2$</td>
<td>$5.3 \pm 4.1$</td>
<td>22.2</td>
</tr>
<tr>
<td>Yi 1.5 34B</td>
<td>$26.2 \pm 4.7$</td>
<td>$35.7 \pm 1.4$</td>
<td>$35.0 \pm 10.7$</td>
<td>$3.2 \pm 0.2$</td>
<td>$1.1 \pm 1.6$</td>
<td>20.2</td>
</tr>
<tr>
<td>Mistral 8x22B</td>
<td>$21.3 \pm 1.9$</td>
<td>$19.4 \pm 1.4$</td>
<td>$39.2 \pm 12.6$</td>
<td>$1.5 \pm 1.4$</td>
<td>$3.9 \pm 1.7$</td>
<td>17.0</td>
</tr>
<tr>
<td>Yi 1.5 9B</td>
<td>$10.8 \pm 1.6$</td>
<td>$20.0 \pm 3.7$</td>
<td>$35.0 \pm 5.0$</td>
<td>$5.0 \pm 2.2$</td>
<td>$1.3 \pm 1.5$</td>
<td>14.4</td>
</tr>
<tr>
<td>Llama 3 8B</td>
<td>$22.5 \pm 2.9$</td>
<td>$24.6 \pm 2.1$</td>
<td>$23.3 \pm 7.1$</td>
<td>$0.0 \pm 0.0$</td>
<td>$1.1 \pm 1.6$</td>
<td>14.3</td>
</tr>
<tr>
<td>Mistral 8x7B</td>
<td>$15.8 \pm 2.6$</td>
<td>$16.1 \pm 1.4$</td>
<td>$30.0 \pm 8.2$</td>
<td>$1.1 \pm 1.6$</td>
<td>$1.1 \pm 1.6$</td>
<td>12.8</td>
</tr>
<tr>
<td>Chance</td>
<td>25.0</td>
<td>25.0</td>
<td>25.0</td>
<td>0.0</td>
<td>0.0</td>
<td>15.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Large-scale spatial cognition results. The three tables show results for different observation spaces. Results below $50 \%$ of human performance are gray. Methods are sorted based on their overall performance.
and map sketching, and less than $30 \%$ SPL on route retracing and shortcut discovery, even with allocentric presentation. With egocentric multimodal presentation (the closest counterpart to classic experimental protocols in animal cognition), the models are near chance level on all tasks.</p>
<p>Human performance ranges from $80 \%$ to $100 \%$ accuracy on image-based presentations of the multiple-choice QA tasks. Since perceiving large sequences of text arrays is non-trivial for humans, the performance drops to $65 \%-80 \%$ for the text presentations.</p>
<p>Small-scale spatial cognition results. The results are shown in Table 2. With multimodal presentations, we benchmark GPT-4o, GPT-4v, Claude 3.5 Sonnet, and a number of open multimodal models. With purely textual presentations, we benchmark both open and closed models. We also list the performance of the chance baseline for calibration, as well as human performance (see the appendix for details).</p>
<p>Performance of some model classes (e.g., GPT-4o, GPT-4v, Claude 3.5 Sonnet) on purely textual presentations is considerably higher than on multimodal presentations. The best-performing models, Claude 3.5 Sonnet and GPT-4o, achieve 43.8\% and 40.1\% average accuracies in the multimodal regime and $64.5 \%$ and $65.2 \%$ average accuracies with purely textual presentations. (Chance is $&lt;25 \%$.) We attribute this in part to the simplified nature of the text-only implementations of tasks like MRT, MPFB, and JLO (e.g., the text-only presentation of mental rotation uses only 2D shapes and constrained 2D rotations) and in part to the relative developmental maturity of large language models (LLMs) versus multimodal models on the remaining tasks.</p>
<p>On tasks that evaluate visuospatial working memory (specifically SAtt, CBTT, SAdd, and CSWM), the strongest LLMs perform well. On selective attention (SAtt), GPT-4o, Claude 3.5 Sonnet, Mistral 123B, and GPT-4v all achieve over 95\% accuracy, matching or outperforming the human performance on this task. On the other hand, all models perform poorly on maze completion (MCT), in both presentation modalities. (Note that the models operate with full visibility, as illustrated in Figure 2.) With multimodal presentation, all evaluated models are near chance on perspective taking (PTT) and the Minnesota Paper Form Board test (MPFB). On mental rotation (MRT), the best models are near chance with multimodal presentation, which uses 3D shapes, and only marginally better with purely textual presentation, which uses 2D arrays and constrained rotations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Multimodal</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">MRT</td>
<td style="text-align: center;">PTT</td>
<td style="text-align: center;">WLT</td>
<td style="text-align: center;">MPFB</td>
<td style="text-align: center;">JLO</td>
<td style="text-align: center;">SAtt</td>
<td style="text-align: center;">MCT</td>
<td style="text-align: center;">CBTT</td>
<td style="text-align: center;">SAdd</td>
<td style="text-align: center;">CSWM</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Clau de 3.5 Sonnet</td>
<td style="text-align: center;">29.0255</td>
<td style="text-align: center;">21.8225</td>
<td style="text-align: center;">37.0255</td>
<td style="text-align: center;">35.5275</td>
<td style="text-align: center;">40.5255</td>
<td style="text-align: center;">90.5255</td>
<td style="text-align: center;">2.2215</td>
<td style="text-align: center;">56.5255</td>
<td style="text-align: center;">48.0255</td>
<td style="text-align: center;">76.7225</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">33.3215</td>
<td style="text-align: center;">26.5255</td>
<td style="text-align: center;">59.02155</td>
<td style="text-align: center;">27.0215</td>
<td style="text-align: center;">26.5255</td>
<td style="text-align: center;">70.2215</td>
<td style="text-align: center;">10.4215</td>
<td style="text-align: center;">68.0220</td>
<td style="text-align: center;">40.5271</td>
<td style="text-align: center;">40.0200</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-80</td>
<td style="text-align: center;">32.3255</td>
<td style="text-align: center;">28.0225</td>
<td style="text-align: center;">35.0277</td>
<td style="text-align: center;">22.5211</td>
<td style="text-align: center;">26.5255</td>
<td style="text-align: center;">59.8244</td>
<td style="text-align: center;">5.7222</td>
<td style="text-align: center;">44.5255</td>
<td style="text-align: center;">32.0255</td>
<td style="text-align: center;">26.7225</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pretra 12B</td>
<td style="text-align: center;">28.3251</td>
<td style="text-align: center;">23.2255</td>
<td style="text-align: center;">43.0275</td>
<td style="text-align: center;">30.5275</td>
<td style="text-align: center;">24.5275</td>
<td style="text-align: center;">36.0255</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">39.5255</td>
<td style="text-align: center;">28.5251</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Phi-3.5-vision</td>
<td style="text-align: center;">24.1215</td>
<td style="text-align: center;">27.0222</td>
<td style="text-align: center;">22.5275</td>
<td style="text-align: center;">26.0255</td>
<td style="text-align: center;">21.0211</td>
<td style="text-align: center;">44.0255</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">33.0255</td>
<td style="text-align: center;">22.0255</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llava interleave 7B</td>
<td style="text-align: center;">25.1252</td>
<td style="text-align: center;">25.8255</td>
<td style="text-align: center;">25.0255</td>
<td style="text-align: center;">25.0255</td>
<td style="text-align: center;">24.0257</td>
<td style="text-align: center;">32.0255</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">26.5257</td>
<td style="text-align: center;">27.0251</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chance</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">33.8254</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Text-only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">MRT</td>
<td style="text-align: center;">PTT</td>
<td style="text-align: center;">MPFB</td>
<td style="text-align: center;">JLO</td>
<td style="text-align: center;">SAtt</td>
<td style="text-align: center;">MCT</td>
<td style="text-align: center;">CBTT</td>
<td style="text-align: center;">SAdd</td>
<td style="text-align: center;">CSWM</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">41.9253</td>
<td style="text-align: center;">55.5255</td>
<td style="text-align: center;">50.5205</td>
<td style="text-align: center;">66.5245</td>
<td style="text-align: center;">98.8204</td>
<td style="text-align: center;">21.5255</td>
<td style="text-align: center;">82.5217</td>
<td style="text-align: center;">93.5255</td>
<td style="text-align: center;">76.7225</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Claude 3.5 Sonnet</td>
<td style="text-align: center;">37.5215</td>
<td style="text-align: center;">50.0275</td>
<td style="text-align: center;">45.0257</td>
<td style="text-align: center;">70.5243</td>
<td style="text-align: center;">97.0210</td>
<td style="text-align: center;">10.0211</td>
<td style="text-align: center;">97.5209</td>
<td style="text-align: center;">91.5243</td>
<td style="text-align: center;">82.0253</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral 123B</td>
<td style="text-align: center;">39.4252</td>
<td style="text-align: center;">44.8240</td>
<td style="text-align: center;">48.5252</td>
<td style="text-align: center;">57.0254</td>
<td style="text-align: center;">97.5205</td>
<td style="text-align: center;">14.8255</td>
<td style="text-align: center;">88.5209</td>
<td style="text-align: center;">92.5209</td>
<td style="text-align: center;">62.0225</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-40</td>
<td style="text-align: center;">41.2275</td>
<td style="text-align: center;">67.5205</td>
<td style="text-align: center;">34.0255</td>
<td style="text-align: center;">62.0245</td>
<td style="text-align: center;">95.8213</td>
<td style="text-align: center;">3.7219</td>
<td style="text-align: center;">87.5255</td>
<td style="text-align: center;">79.0222</td>
<td style="text-align: center;">45.3225</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama 3 70B</td>
<td style="text-align: center;">28.1252</td>
<td style="text-align: center;">29.2224</td>
<td style="text-align: center;">38.5255</td>
<td style="text-align: center;">42.5255</td>
<td style="text-align: center;">71.8258</td>
<td style="text-align: center;">1.5219</td>
<td style="text-align: center;">52.5257</td>
<td style="text-align: center;">62.5254</td>
<td style="text-align: center;">34.0255</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mistral 8x22B</td>
<td style="text-align: center;">26.9252</td>
<td style="text-align: center;">24.5252</td>
<td style="text-align: center;">31.0255</td>
<td style="text-align: center;">36.0251</td>
<td style="text-align: center;">73.5256</td>
<td style="text-align: center;">1.5221</td>
<td style="text-align: center;">55.0253</td>
<td style="text-align: center;">68.0268</td>
<td style="text-align: center;">17.3225</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Yi 1.5 34B</td>
<td style="text-align: center;">20.6255</td>
<td style="text-align: center;">28.0221</td>
<td style="text-align: center;">34.5245</td>
<td style="text-align: center;">33.3245</td>
<td style="text-align: center;">58.2243</td>
<td style="text-align: center;">0.7219</td>
<td style="text-align: center;">35.5254</td>
<td style="text-align: center;">41.5255</td>
<td style="text-align: center;">24.0250</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Yi 1.5 9B</td>
<td style="text-align: center;">21.2212</td>
<td style="text-align: center;">23.8227</td>
<td style="text-align: center;">30.0252</td>
<td style="text-align: center;">24.5255</td>
<td style="text-align: center;">48.2246</td>
<td style="text-align: center;">0.7219</td>
<td style="text-align: center;">36.5246</td>
<td style="text-align: center;">51.5289</td>
<td style="text-align: center;">24.7284</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama 3 8B</td>
<td style="text-align: center;">14.4211</td>
<td style="text-align: center;">25.8251</td>
<td style="text-align: center;">26.0242</td>
<td style="text-align: center;">27.0217</td>
<td style="text-align: center;">46.0257</td>
<td style="text-align: center;">0.0209</td>
<td style="text-align: center;">27.5277</td>
<td style="text-align: center;">30.0273</td>
<td style="text-align: center;">26.0252</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mixtral 8x7B</td>
<td style="text-align: center;">19.4243</td>
<td style="text-align: center;">10.5259</td>
<td style="text-align: center;">29.5257</td>
<td style="text-align: center;">27.5275</td>
<td style="text-align: center;">39.0254</td>
<td style="text-align: center;">0.0209</td>
<td style="text-align: center;">22.5228</td>
<td style="text-align: center;">43.5293</td>
<td style="text-align: center;">22.7211</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chance</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">33.0253</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Small-scale spatial cognition results. The two tables show results for multimodal and text-only presentations, respectively. Results below $50 \%$ of human performance are gray, results above $90 \%$ of human performance are bold. Methods are sorted based on their average performance. ( ${ }^{*}$ Some multimodal models ran out of memory on MCT and CSWM tasks; their accuracy is taken to be 0 for calculating the average.)</p>
<p>Humans perform well, achieving over $80 \%$ accuracy on the majority of the multiple-choice QA tasks with both text-only and multimodal presentations. Humans perform better on the textual presentations of tasks like MRT, MPFB and JLO than their vision counterparts due to the simplified nature of the text-only implementations.</p>
<p>Ecological compatibility of SPACE with frontier models. Our results indicate that current frontier models lack spatial cognition. Alternatively, these results could be the result of models not understanding the inputs presented to them (i.e., the inputs are not ecological compatibility). We study this in Appendix A. 1 and demonstrate that this is not the case. Models can understand the inputs correctly and perform non-spatial cognition tasks well, yet fail to demonstrate spatial cognition.</p>
<h1>5 DISCUSSION</h1>
<p>We presented SPACE, a benchmark for spatial cognition in frontier models. Our evaluation of contemporary models brings up intriguing questions and opportunities for further investigation. First, our results underscore that frontier models exhibit a fundamentally different form of intelligence from what has been observed (and studied) in humans and animals. No biological intelligence we have encountered has exhibited such advanced skill in some aspects of higher cognition (Trinh et al., 2024) while failing so profoundly in basic spatial cognition. This is particularly intriguing because in biological intelligence, spatial cognition is considered a prerequisite for higher cognition, and breakdowns in spatial cognition are diagnostic of higher-level disorders (Cappa, 2008; Possin, 2010; Verghese et al., 2017; Cammisuli et al., 2024). From a scientific standpoint, the constellation of traits exhibited by frontier models is fascinating and may inspire a new cognitive science (Simon, 2019). As a precautionary stance, we can refrain from drawing analogies based on experience with biological cognition. (E.g., "a model won the Mathematics Olympiad therefore it possesses a comparable cognitive repertoire to a human Olympiad winner and could be expected to have comparable skill in other domains".)</p>
<p>Could deficiencies in spatial cognition be causally linked to some of the puzzling breakdowns exhibited by contemporary frontier models in higher-level tasks? What is the roadmap for bringing spatial cognition in frontier models up to the level of animal cognition (and perhaps beyond)? Is this a prerequisite for attaining some of the more far-reaching aspirations of contemporary artificial intelligence research? Does embodiment play a role, as it has in prior forms of intelligence (Smith \&amp; Gasser, 2005; Savva et al., 2019)? Or will artificial cognition continue to develop along a fundamentally different ontogenetic path? We expect further advances to increase the robustness and generality of frontier models, and to continue to broaden our understanding of the nature of intelligence.</p>
<h1>REFERENCES</h1>
<p>Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv:2404.14219, 2024.</p>
<p>Gary L Allen, Kathleen C Kirasic, Shannon H Dobson, Richard G Long, and Sharon Beck. Predicting environmental learning from spatial abilities: An indirect route. Intelligence, 22(3), 1996.</p>
<p>David I Anderson, Joseph J Campos, David C Witherington, Audun Dahl, Monica Rivera, Minxuan He, Ichiro Uchiyama, and Marianne Barbu-Roth. The role of locomotion in psychological development. Frontiers in Psychology, 4, 2013.</p>
<p>Peter Anderson, Angel X. Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir R. Zamir. On evaluation of embodied navigation agents. arXiv:1807.06757, 2018.</p>
<p>Anthropic. Introducing Claude 3.5 Sonnet, 2024. https://www.anthropic.com/news/ claude-3-5-sonnet.</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In ICCV, 2015.</p>
<p>Leopold Aschenbrenner. Situational awareness: The decade ahead, 2024. https: //situational-awareness.ai/wp-content/uploads/2024/06/ situationalawareness.pdf.</p>
<p>Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, Caiming Xiong, Ran Xu, Yejin Choi, and Ludwig Schmidt. MINT-1T: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens. arXiv:2406.11271, 2024.</p>
<p>Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705), 2018.</p>
<p>Arthur Lester Benton. Contributions to neuropsychological assessment: A clinical manual. Oxford University Press, USA, 1994.</p>
<p>Mark Blades and Christopher Spencer. The development of children's ability to use spatial representations. Advances in child development and behavior, 25, 1994.</p>
<p>Nicole Blaser, G Dell'Omo, G Dell'Ariccia, David Paul Wolfer, and H-P Lipp. Testing cognitive navigation in unknown territories: homing pigeons choose different targets. Journal of Experimental Biology, 216(16), 2013.</p>
<p>Tyler Bonnen, Stephanie Fu, Yutong Bai, Thomas O’Connell, Yoni Friedman, Nancy Kanwisher, Josh Tenenbaum, and Alexei Efros. Evaluating multiview object consistency in humans and image models. In NeurIPS, 2025.</p>
<p>Youcef Bouchekioua, Aaron P Blaisdell, Yutaka Kosaki, Iku Tsutsui-Kimura, Paul Craddock, Masaru Mimura, and Shigeru Watanabe. Spatial inference without a cognitive map: the role of higher-order path integration. Biological Reviews, 96(1), 2021.</p>
<p>R Brickenkamp and E Zillmer. Test d2: concentration-endurance test. Gottingen Ger. CJ Hogrefe, 1998.</p>
<p>Rodney Brooks. Rodney brooks' three laws of artificial intelligence, 2024. https://rodneybrooks.com/ rodney-brooks-three-laws-of-artificial-intelligence/.</p>
<p>Davide Maria Cammisuli, Gloria Marchesi, Virginia Bellocchio, Edoardo Nicolò Aiello, Barbara Poletti, Federico Verde, Vincenzo Silani, Nicola Ticozzi, Stefano Zago, Teresa Difonzo, et al. Behavioral disorders of spatial cognition in patients with mild cognitive impairment due to alzheimer's disease (the bdsc-mci project): Ecological validity of the corsi learning suvra-span test. Journal of Personalized Medicine, 14(5), 2024.</p>
<p>SF Cappa. Cognitive neurology: a clinical textbook. Oxford University Press, 2008.
Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas J. Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024a.</p>
<p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv:2311.12793, 2023.</p>
<p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large visionlanguage models? arXiv:2403.20330, 2024b.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021.</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. arXiv:1504.00325, 2015.</p>
<p>An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv:2406.01584, 2024.</p>
<p>François Chollet. On the measure of intelligence. arXiv:1911.01547, 2019.
Michiel HG Claessen, Ineke JM Van Der Ham, and Martine JE Van Zandvoort. Computerization of the standard corsi block-tapping task affects its underlying cognitive concepts: a pilot study. Applied Neuropsychology: Adult, 22(3), 2015.</p>
<p>Philip Michael Corsi. Human memory and the medial temporal region of the brain. Phd thesis, McGill University, 1972. https://escholarship.mcgill.ca/concern/theses/ 05741 s 554 .</p>
<p>Christopher J. Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. In $I C L R, 2018$.</p>
<p>Edwin S Dalmaijer, Stefan Van der Stigchel, Tanja CW Nijboer, Tim HW Cornelissen, and Masud Husain. Cancellationtools: All-in-one software for administration and analysis of cancellation tasks. Behavior Research Methods, 47, 2015.</p>
<p>Dawson-Haggerty et al. trimesh, 2019. https://trimesh.org/.
Miguel de Guinea, Alejandro Estrada, K Anne-Isola Nekaris, and Sarie Van Belle. Cognitive maps in the wild: revealing the use of metric information in black howler monkey route navigation. Journal of Experimental Biology, 224(15), 2021.</p>
<p>Sergio Della Sala, Marcella Laiacona, Hans Spinnler, and Chiara Ubezio. A cancellation test: its reliability in assessing attentional deficits in Alzheimer's disease. Psychological Medicine, 22(4), 1992.</p>
<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv:2407.21783, 2024.</p>
<p>Pul Ashby Foltz. Adult performance on piaget's water level task and its relation of spatial orientation and visualization. Master's thesis, University of Richmond, 1978. https://scholarship.richmond.edu/cgi/viewcontent.cgi? article=1424\&amp;context=masters-theses.</p>
<p>Nigel Foreman, Denny Foreman, Alison Cummings, and Sandra Owens. Locomotion, active choice, and spatial memory in children. The Journal of General Psychology, 117(2), 1990.</p>
<p>John W French, Ruth B Ekstrom, and Leighton A Price. Manual for kit of reference tests for cognitive factors. 1963.</p>
<p>Andrea Frick and Wenke Möhring. A matter of balance: Motor control is related to children's spatial and proportional reasoning skills. Frontiers in Psychology, 6, 2016.</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: A comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023.</p>
<p>Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024a.</p>
<p>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: Multimodal large language models can see but not perceive. arXiv:2404.12390, 2024b.</p>
<p>Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2023.</p>
<p>Maya Geva-Sagiv, Liora Las, Yossi Yovel, and Nachum Ulanovsky. Spatial cognition in bats and rats: from sensory acquisition to multiscale maps and navigation. Nature Reviews Neuroscience, 16(2), 2015.</p>
<p>Sabine Gillner and Hanspeter A Mallot. Navigation and acquisition of spatial knowledge in a virtual maze. Journal of Cognitive Neuroscience, 10(4), 1998.</p>
<p>Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. International Journal of Computer Vision, 127(4), 2019.</p>
<p>Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, 2024.</p>
<p>Mary Hegarty and David Waller. A dissociation between mental rotation and perspective-taking spatial abilities. Intelligence, 32(2), 2004.</p>
<p>Mary Hegarty, Daniel R Montello, Anthony E Richardson, Toru Ishikawa, and Kristin Lovelace. Spatial abilities at different scales: Individual differences in aptitude-test performance and spatiallayout learning. Intelligence, 34(2), 2006.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021b.</p>
<p>Anna A Ivanova, Aalok Sathe, Benjamin Lipkin, Unnathi Kumar, Setayesh Radkani, Thomas H Clark, Carina Kauf, Jennifer Hu, RT Pramod, Gabriel Grand, et al. Elements of world knowledge (ewok): A cognition-inspired framework for evaluating basic world knowledge in language models. arXiv:2405.09605, 2024.</p>
<p>Petra Jansen. The dissociation of small-and large-scale spatial abilities in school-age children. Perceptual and Motor Skills, 109(2), 2009.</p>
<p>Petra Jansen and Martin Heil. The relation between motor development and mental rotation ability in 5-to 6-year-old children. International Journal of Developmental Science, 4(1), 2010.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv:2401.04088, 2024.</p>
<p>Ashley N Kalina and Suzie A Walgrave. Normative evaluation of a letter cancellation instrument for the assessment of sustained attention: A construct validation study. The Journal of Undergraduate Research, 2(1), 2004.</p>
<p>Maria Kozhevnikov and Mary Hegarty. A dissociation between object manipulation spatial ability and spatial orientation ability. Memory \&amp; Cognition, 29, 2001.</p>
<p>Maria Kozhevnikov, Michael A Motes, and Mary Hegarty. Spatial visualization in physics problem solving. Cognitive Science, 31(4), 2007.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In SOSP, 2023.</p>
<p>Emilie Lacroix, Stéphanie Cornet, Naima Deggouj, and Martin Gareth Edwards. The visuo-spatial abilities diagnosis (vsad) test: Evaluating the potential cognitive difficulties of children with vestibular impairment through a new tablet-based computerized test battery. Behavior Research Methods, 53, 2021.</p>
<p>Barbara Landau. Spatial cognition. In Encyclopedia of the Human Brain. Elsevier, 2002.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In NeurIPS, 2023.</p>
<p>Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv:2407.07895, 2024a.</p>
<p>Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In CVPR, 2024b.</p>
<p>Rensis Likert and WH Quasha. Minnesota Paper Form Board Test. Psychological Corporation, 1941.</p>
<p>Rensis Likert and William H Quasha. Revised Minnesota paper form board test. Psychological Corporation, 1969.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023a.</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023b.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024.</p>
<p>Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024.</p>
<p>Hanspeter A Mallot. From Geometry to Behavior: An Introduction to Spatial Cognition. MIT Press, 2024.</p>
<p>Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In CVPR, 2019.</p>
<p>Jérôme Marquet-Doléac, Régis Soppelsa, and Jean-Michel Albaret. Laby 5-12: Test des labyrinthes. Hogrefe, 2010.</p>
<p>John C Marshall and Gereon R Fink. Spatial cognition: Where we were and where we are. Neuroimage, 14(1), 2001.</p>
<p>Chiara Meneghetti, Clara Zancada-Menéndez, Patricia Sampedro-Piquero, Laudino Lopez, Massimiliano Martinelli, Lucia Ronconi, and Barbara Rossi. Mental representations derived from navigation: The role of visuo-spatial abilities and working memory. Learning and Individual Differences, 49, 2016.</p>
<p>Chiara Meneghetti, Laura Miola, Enrico Toffalini, Massimiliano Pastore, and Francesca Pazzaglia. Learning from navigation, and tasks assessing its accuracy: The role of visuospatial abilities and wayfinding inclinations. Journal of Environmental Psychology, 75, 2021.</p>
<p>Chiara Meneghetti, Laura Miola, Tommaso Feraco, and Veronica Muffato. Individual differences in navigation: An introductory overview. In Prime Archives in Psychology. Vide Leaf, 2nd edition, 2022.</p>
<p>Emil W Menzel. Chimpanzee spatial memory organization. Science, 182(4115), 1973.
Mistral AI team. Large enough, 2024a. https://mistral.ai/news/ mistral-large-2407/.</p>
<p>Mistral AI team. Announcing pixtral 12b, 2024b. https://mistral.ai/news/ pixtral-12b/.</p>
<p>Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. Evaluating cognitive maps in large language models with cogeval: No emergent planning. In NeurIPS, 2023.</p>
<p>Arsenii Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the ARC domain. Transactions on Machine Learning Research, 2023.</p>
<p>Nora S. Newcombe. Making space: The development of spatial representation and reasoning. MIT Press, 2000.</p>
<p>Nora S. Newcombe. Picture this: Increasing math and science learning by improving spatial thinking. American Educator, 34(2), 2010.</p>
<p>Nora S. Newcombe. Spatial Cognition. In Open Encyclopedia of Cognitive Science. MIT Press, 2024.</p>
<p>Rahel Noser and Richard W Byrne. Mental maps in chacma baboons (papio ursinus): using intergroup encounters as a natural experiment. Animal Cognition, 10, 2007.</p>
<p>John O'Keefe and Lynn Nadel. The hippocampus as a cognitive map. Oxford University Press, 1978.</p>
<p>OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.
OpenAI. Hello gpt-4o, 2024. https://openai.com/index/hello-gpt-4o/.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.</p>
<p>Anja Pahor, Randy E Mester, Audrey A Carrillo, Eunice Ghil, Jason F Reimer, Susanne M Jaeggi, and Aaron R Seitz. Ucancellation: A new mobile measure of selective attention and concentration. Behavior Research Methods, 54(5), 2022.</p>
<p>Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, et al. Perception test: A diagnostic benchmark for multimodal video models. In NeurIPS, 2023.</p>
<p>HL Payne, GF Lynch, and Dmitriy Aronov. Neural representations of space in the hippocampus of a food-caching bird. Science, 373(6552), 2021.</p>
<p>Francesca Pazzaglia and Holly A Taylor. Perspective, instruction, and cognitive style in spatial representation of a virtual environment. Spatial Cognition and Computation, 7(4), 2007.</p>
<p>Michael Peters, Bruno Laeng, Kerry Latham, Marla Jackson, Raghad Zaiyouna, and Chris Richardson. A redrawn vandenberg and kuse mental rotations test-different versions and factors that affect performance. Brain and Cognition, 28(1), 1995.</p>
<p>Roger Paul Peters. Wolf-sign: Scents And Space In A Wide-ranging Predator. University of Michigan, 1974.</p>
<p>Jean Piaget, Baerbel Inhelder, F. J. Langdon, and J. L. Lunzer. The child's conception of space. British Journal of Educational Studies, 5(2), 1957.</p>
<p>Leila M Porter and Paul A Garber. Foraging and spatial memory in wild weddell's saddleback tamarins (saguinus fuscicollis weddelli) when moving between distant and out-of-sight goals. International Journal of Primatology, 34, 2013.</p>
<p>Katherine L Possin. Visual spatial cognition in neurodegenerative disease. Neurocase, 16(6), 2010.
Andrea Presotto, Richard Fayrer-Hosken, Caitlin Curry, and Marguerite Madden. Spatial mapping shows that some african elephants use cognitive maps to navigate the core but not the periphery of their home ranges. Animal Cognition, 22, 2019.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023.</p>
<p>Ahad Rana. Common crawl - building an open web-scale crawl using hadoop, 2010. https: //www.slideshare.net/hadoopusergroup/common-crawlpresentation.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, JeanBaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024.</p>
<p>Anthony E Richardson, Daniel R Montello, and Mary Hegarty. Spatial knowledge acquisition from maps and from navigation in real and virtual environments. Memory \&amp; Cognition, 27(4), 1999.</p>
<p>Barbara J Sahakian, Robin G Morris, John L Evenden, Andrew Heald, Raymond Levy, Michael Philpot, and Trevor W Robbins. A comparative study of visuospatial memory and learning in alzheimer-type dementia and parkinson's disease. Brain, 111(3), 1988.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9), 2021.</p>
<p>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied AI research. In ICCV, 2019.</p>
<p>John T. Serences and Sabine Kastner. A multi-level account of selective attention. The Oxford Handbook of Attention, 2014.</p>
<p>Roger N Shepard and Jacqueline Metzler. Mental rotation of three-dimensional objects. Science, 171(3972), 1971.</p>
<p>Herbert A. Simon. The Sciences of the Artificial. MIT Press, 3rd edition, 2019.
Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial Life, 11(1-2), 2005.</p>
<p>Ben Sorscher, Gabriel C Mel, Samuel A Ocko, Lisa M Giocomo, and Surya Ganguli. A unified theory for the computational and mechanistic origins of grid cells. Neuron, 111(1), 2023.</p>
<p>Robert J Spencer, Carrington R Wendell, Paul P Giggey, Stephen L Seliger, Leslie I Katzel, and Shari R Waldstein. Judgment of line orientation: an examination of eight short forms. Journal of Clinical and Experimental Neuropsychology, 35(2), 2013.</p>
<p>John Stilley. mazelib: A python api for creating and solving mazes, 2014. https://github. com/john-science/mazelib.</p>
<p>Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. https://github.com/togethercomputer/RedPajama-Data.</p>
<p>Sivan Toledo, David Shohami, Ingo Schiffner, Emmanuel Lourie, Yotam Orchan, Yoav Bartan, and Ran Nathan. Cognitive map-based navigation in wild bats revealed by a new high-throughput tracking system. Science, 369(6500), 2020.</p>
<p>Edward C Tolman. Cognitive maps in rats and men. Psychological Review, 55(4), 1948.
Luca Tommasi, Cinzia Chiandetti, Tommaso Pecchia, Valeria Anna Sovrano, and Giorgio Vallortigara. From natural geometry to spatial cognition. Neuroscience \&amp; Biobehavioral Reviews, 36(2), 2012.</p>
<p>Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? Exploring the visual shortcomings of multimodal LLMs. In CVPR, 2024.</p>
<p>Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In NeurIPS, 2024.</p>
<p>Steven G Vandenberg and Allan R Kuse. Mental rotations, a group test of three-dimensional spatial visualization. Perceptual and Motor Skills, 47(2), 1978.</p>
<p>Marina Vasilyeva and Stella F Lourenco. Development of spatial cognition. Wiley Interdisciplinary Reviews: Cognitive Science, 3(3), 2012.</p>
<p>Joe Verghese, Richard Lipton, and Emmeline Ayers. Spatial navigation and risk of cognitive impairment: A prospective cohort study. Alzheimer's \&amp; Dementia, 13(9), 2017.</p>
<p>David Ed Waller and Lynn Ed Nadel. Handbook of Spatial Cognition. American Psychological Association, 2013.</p>
<p>Lu Wang, Allan S Cohen, and Martha Carr. Spatial ability at two scales of representation: A metaanalysis. Learning and Individual Differences, 36, 2014.</p>
<p>David Wechsler. WMS-IV: Wechsler Memory Scale. Pearson, 2009.
Steven M Weisberg, Victor R Schinazi, Nora S Newcombe, Thomas F Shipley, and Russell A Epstein. Variations in cognitive maps: understanding individual differences in navigation. Journal of Experimental Psychology: Learning, Memory, and Cognition, 40(3), 2014.</p>
<p>Joseph F Welklin, Benjamin R Sonnenberg, Carrie L Branch, Virginia K Heinen, Angela M Pitera, Lauren M Benedict, Lauren E Whitenack, Eli S Bridge, and Vladimir V Pravosudov. Spatial cognitive ability is associated with longevity in food-caching chickadees. Science, 385(6713), 2024.</p>
<p>Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, and Dhruv Batra. Emergence of maps in the memories of blind navigation agents. In $I C L R, 2023$.</p>
<p>Michele Andrisin Wittig and Mary J Allen. Measurement of adult performance on piaget's water horizontality task. Intelligence, 8(4), 1984.</p>
<p>Dêverton Plácido Xavier, Filipa Abreu, Antonio Souto, and Nicola Schiel. Choosing the best way: how wild common marmosets travel to efficiently exploit resources. Animal Cognition, 27(1), 2024.</p>
<p>Jiayun Xu, Mauricio Girardi-Schappo, Jean-Claude Beique, André Longtin, and Leonard Maler. Shortcutting from self-motion signals reveals a cognitive map in mice. Elife, 13, 2024.</p>
<p>Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating spatial understanding of large language models. Transactions on Machine Learning Research, 2024.</p>
<p>Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In ICML, 2024.</p>
<p>Eunice Yiu, Maan Qraitem, Charlie Wong, Anisa Noor Majhi, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal models. arXiv:2407.17773, 2024.</p>
<p>Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01.ai. arXiv:2403.04652, 2024.</p>
<p>Christopher J Young, Susan C Levine, and Kelly S Mix. The connection between spatial and mathematical ability across development. Frontiers in Psychology, 9, 2018.</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, 2024.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 ECOLOGICAL COMPATIBILITY OF MULTIMODAL INPUTS WITH FRONTIER MODELS</h2>
<p>Our results in Section 4 suggest that state-of-the-art frontier models fail in the spatial cognition tasks presented in SPACE. These failures can be attributed to the lack of spatial cognition in these models. Alternatively, these failures could be due to models not comprehending the inputs presented to them (i.e., the inputs are not ecologically compatible with the models). To rule out this alternative possibility, we design additional tests unrelated to spatial cognition on the same vision / text inputs used in our benchmark. If models succeed on these tests, we can infer that the inputs are ecologically compatible since the models can understand and perform tasks using these inputs. In each test, we pose a series of multiple-choice questions evaluating a model's fine-grained understanding of the inputs. We now describe these additional tests.</p>
<h2>Test 1: Given discrete map image / text inputs (see Figure 3), answer the following questions:</h2>
<p>Q1. What is the size of the grid (H x W)?
Q2. What is your current ( $\mathrm{x}, \mathrm{y}$ ) location?
Q3. What are the ( $\mathrm{x}, \mathrm{y}$ ) locations of all navigable cells? Include cells containing landmarks and your current position.
Q4. What are the ( $\mathrm{x}, \mathrm{y}$ ) locations of all obstacle cells?
Q5. What are the landmarks visible in the image / array?
Q6. What are the locations of the landmarks visible in the image / array?</p>
<h2>Test 2: Given an ego image (see Figure 3), answer the following questions:</h2>
<p>Q1. What is the name of the landmark visible in the image?
Q2. Is the landmark $&lt;$ name $&gt;$ in the left half of the image?
Q3. Is the landmark $&lt;$ name $&gt;$ in the right half of the image?
Q4. Is the landmark $&lt;$ name $&gt;$ in the central section of the image?
Test 3: Given two consecutive ego images from a walkthrough (see Figure 4), answer the following question:
Q1. What is the action taken to go from image 1 to image 2 (move forward, turn left, turn right, wait/do nothing)?</p>
<p>Test 4: Given a perspective taking image / text array (see Figures 9 and 10), answer the following questions:
Q1. How many objects / non-zero locations are present in the image / array?
Q2. What objects / non-zero locations are present in the image / array?
Q3. Is $&lt;$ object / location $&gt;$ to the left of $&lt;$ object / location $&gt;$ in the image / array?
Q4. Is $&lt;$ object / location $&gt;$ to the above $&lt;$ object / location $&gt;$ in the image / array?</p>
<h2>Test 5: Given water level test images (see Figure 11), answer the following questions:</h2>
<p>Q1. Is there water in the water container?
Q2. From image 1 to image 2, is the water container rotated to the left, right or not rotated at all?
Q3. From image 1 to image 2, what is the absolute rotation angle of the water container (in degrees)?
Test 6: Given a grid of icons / characters from selective attention (see Figures 16 and 17), answer the following questions:
Q1. How many total objects / characters are present in the image / grid (including repetitions)?
Q2. What is the size of the grid of objects / characters (width x height)?
Q3. How many unique objects / characters are present in the grid (ignore repetitions)?
Results discussion: We evaluate GPT-4o and GPT-4v on these tests. The results are shown in Tables 3 and 4. Both models largely understand DM image and text inputs (test 1). However, they fall short in calculating the grid size for DM images (Q1). GPT-4o understands egocentric images,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Multimodal evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Test 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Q5</td>
<td style="text-align: center;">Q6</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4v</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">48.0</td>
</tr>
<tr>
<td style="text-align: center;">Text-only evaluation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Test 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Q5</td>
<td style="text-align: center;">Q6</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4v</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Measuring ecological compatibility of multimodal inputs with frontier models (part 1)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Multimodal evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Test 4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">77.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4v</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;">Text-only evaluation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Test 4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test 6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q4</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4v</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">98.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Measuring ecological compatibility of multimodal inputs with frontier models (part 2)
i.e., recognizes and localizes landmarks in egocentric images (test 2). GPT-4v recognizes landmarks well (Q1), but performs poorly in localization (Q2, Q3 and Q4). Both GPT-4o and GPT-4v perform poorly on action estimation (test 3) and estimation of water container rotations (Q2 and Q3 in test 5). GPT-4o excels in understanding the perspective taking inputs with multimodal and text-only presentations (test 4). GPT-4v also performs well on test 4, but is worse with multimodal inputs when compared to text-only inputs. Finally, both GPT-4o and GPT-4v perform adequately with counting objects ( Q 1 in test 6 ) and grid sizes ( Q 2 in test 6 ) on selective attention task inputs with multimodal inputs. However, they struggle to calculate the number of unique objects / characters (Q3 in test 6). Both GPT-4o and GPT-4v excel at the text-only presentation of test 6.
Our results indicate that state-of-the-art models can understand multimodal and text-only inputs provided in our benchmark. They perform well in most of the tests, but have specific shortcomings (e.g., localizing landmarks in ego images for GPT-4v, understanding rotations of water containers and counting unique characters / objects in a grid). Importantly, the average results on each test is much higher than the SPACE task counterparts. For example, even though GPT-4o and GPT-4v understand DM text inputs nearly perfectly (test 1), they perform poorly in the DM text versions of the large-scale spatial cognition tasks (see Table 1). Similarly, even though GPT-4o understands the perspective taking inputs nearly perfectly for both text-only and multimodal presentations, it performs poorly on the perspective taking task in SPACE (see Table 2). Therefore, the failure of frontier models on SPACE is most likely due to their lack of spatial cognition, and not because they cannot understand the inputs presented to them.</p>
<h1>A. 2 Small-SCALE SPATIAL COGNITION: ADDITIONAL DETAILS</h1>
<p>We described the small-scale spatial cognition tasks from our benchmark in Section 3.2. Here, we provide additional details about the historical context and motivations behind these tasks.
Mental rotation test (MRT). This was introduced by Vandenberg \&amp; Kuse (1978) as a test of spatial visualization. The original MRT contained 20 items, where each item consisted of a criterion figure, two correct alternatives, and two distractors (Vandenberg \&amp; Kuse, 1978). The criterion figure is a perspective rendering of a 3D criterion shape from Shepard \&amp; Metzler (1971). The correct alternatives are rotated versions of the criterion shape, where the rotation is applied in the 2D image space</p>
<p>on the criterion figure, or along the vertical axis in 3D for the criterion shape. The distractors are rotated mirror-images of the criterion shape or renderings of other criterion shapes. The goal was to identify the two correct alternatives from the four choices. We implement a version of MRT with one correct choice and three distractors, and incorporate rotations along multiple axes (Peters et al., 1995).</p>
<p>Perspective taking test (PTT). This was introduced by Kozhevnikov \&amp; Hegarty (2001) as a test of spatial orientation. An arrangement of objects is shown on a piece of paper. A test participant is asked to take the perspective of standing next to an object (say, object A) facing another (say, object B), and is required to point to a third object (say, object C). This task has been used extensively in subsequent literature (Hegarty \&amp; Waller, 2004; Weisberg et al., 2014; Meneghetti et al., 2022). We implement this task by randomly sampling $N$ icons of objects like cars, carrots, chairs, and grapes and place them at random locations in an image (with no overlap between objects). We then randomly sample three of the $N$ objects as A, B, and C.</p>
<p>Water level test (WLT). This was introduced by Piaget et al. (1957) as a test of visuospatial perception. Originally, the test was designed to evaluate children's knowledge about the horizontal nature of the surface of water in a sealed bottle regardless of its orientation. Children were presented with bottles partially filled with colored water and asked to imagine the position of the water if it were tilted. Children had to gesture, draw, or use cardboard cutouts to answer the question (Piaget et al., 1957; Foltz, 1978; Wittig \&amp; Allen, 1984). Performance on the water-level test was found to be related to performance on spatial ability tests (Foltz, 1978; Wittig \&amp; Allen, 1984).</p>
<p>Judgement of Line Orientation test (JLO). This was introduced by Benton (1994) as a measure of visuospatial perception. The original implementation contained 30 samples presented in a flip-book style, where two lines are shown at the top of each page. The goal is to determine the angles between the two lines by comparing them to an array of reference lines (i.e., pick two reference lines that have same angle between them as the lines at the top). There have been multiple variations of JLO with subsets of the 30 questions for faster evaluation (Spencer et al., 2013). We recreate the JLO test suite by randomly sampling pairs of lines on a 2D plane with an angle between 0 to 180 degrees (in multiples of 18 degrees) and formulate it as multiple-choice QA.
Selective attention task (SAtt). This is designed to evaluate selective spatial attention (Serences \&amp; Kastner, 2014; Pahor et al., 2022). In particular, we use the widely used cancellation task, where the goal is to search for and mark out target stimuli embedded amidst distractors (Della Sala et al., 1992; Brickenkamp \&amp; Zillmer, 1998; Dalmaijer et al., 2015; Lacroix et al., 2021; Pahor et al., 2022; Kalina \&amp; Walgrave, 2004). The stimuli may be characters (Brickenkamp \&amp; Zillmer, 1998; Dalmaijer et al., 2015; Pahor et al., 2022; Della Sala et al., 1992; Kalina \&amp; Walgrave, 2004), pictures (Lacroix et al., 2021; Pahor et al., 2022), or icons (Lacroix et al., 2021). We implement this task with objects as the stimuli for visual evaluation and characters as stimuli for textual evaluation.</p>
<p>Maze completion task (MCT). This task was designed to evaluate spatial orientation, planning, and executive functioning (Lacroix et al., 2021). It was used as a neuropsychological test to assess executive function disorders in children (Marquet-Doléac et al., 2010).</p>
<p>Corsi block-tapping task (CBTT). This is designed to assess visuospatial working memory and attention in healthy participants and patients with known or suspected brain damage (Corsi, 1972; Claessen et al., 2015). An examiner demonstrates a sequence of block-tapping movements on a board containing fixed blocks placed in pseudo-random positions. Participants are required to reproduce the same sequence (forward condition) or the inverted sequence (backward condition) of block-tapping movements to succeed. We evaluate frontier models on the forward condition since prior work has not found significant differences between task performance in the forward and backward conditions (Claessen et al., 2015).</p>
<p>Spatial addition task (SAdd). This was introduced in the fourth edition of the Wechsler Memory Scale, a suite of neuropsychological tests to evaluate memory function in individuals aged 16 to 90 (Wechsler, 2009). SAdd evaluates visuospatial storage and manipulation in working memory. A test participant is shown a grid with blue and red dots for five seconds. The participant is asked to remember the location of the blue dots and ignore the red dots. The participant is then shown another such grid. The objective is to add the two grids together by following certain rules. If a grid location has a blue dot in exactly one of grids, the result should be blue. If a grid location has blue dots on both grids, the result should be white.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For text-only models, the 'video walkthrough' is a sequence of discrete map observations presented as arrays of characters, see Figure 3 for examples.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
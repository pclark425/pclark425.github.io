<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8270 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8270</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8270</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-269626390</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.04776v3.pdf" target="_blank">Chain of Thoughtlessness? An Analysis of CoT in Planning</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8270.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8270.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model evaluated across Blocksworld planning and multiple synthetic reasoning benchmarks; used as the primary model in many experiments assessing chain-of-thought prompt granularity and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thoughtlessness? An Analysis of CoT in Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct I/O prompting (no CoT)', 'Zero-shot CoT ("let\'s think step by step")', 'n-shot prompting (domain-specific and problem-class-specific examples)', 'Manual Chain-of-Thought (multiple handcrafted CoT variants: Progression Proof, Blocksworld Universal Algorithm, Stacking, Lexicographic Stacking)', 'Self-consistency (sample multiple CoT paths and take majority)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Direct I/O: present problem in natural language and request a plan; Zero-shot CoT: append magic phrase to elicit step-by-step; n-shot: prepend examples without annotated thoughts; Manual CoT: prepend human-crafted solutions with intermediate reasoning steps (various granularities from domain-general to highly problem-specific); Self-consistency: sample multiple reasoning outputs at nonzero temperature and choose most frequent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Systematic comparison across prompting granularity (zero-shot, zero-shot CoT, domain-specific n-shot, progression proof CoT, Blocksworld universal algorithm CoT, problem-class n-shot, stacking CoT/lexicographic CoT) and an evaluation of self-consistency (5 sampled reasoning paths) versus single-run outputs; also applied the same CoT prompts across multiple benchmarks (Blocksworld table-to-stack, general Blocksworld, CoinFlip, LastLetterConcatenation variants, multi-step arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Blocksworld (general Blocksworld and table-to-stack subclass, n from 3 to 20 blocks), CoinFlip (1–28 names), LastLetterConcatenation / LastVowelConcatenation / FoomLetterConcatenation (1–20 words), synthetic multi-step single-digit arithmetic (1–29 operations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Blocksworld (table-to-stack, 261 instances): zero-shot 19.1%, zero-shot CoT 21.0%, domain-specific n-shot 13.7%, progression-proof CoT 15.3%, Blocksworld universal algorithm CoT 37.1%, problem-class n-shot 18.0%, stacking prompt 40.6% (best). Self-consistency on table-to-stack: stacking prompt 24.5% (worse than single-run stacking 40.6%). Synthetic benchmarks (Table 2, GPT-4-Turbo): CoinFlip: Zero-shot 56.38%, Zero-shot CoT 95.71%, Manual CoT 98.89%; LastLetterConcatenation: Zero-shot 10.00%, Zero-shot CoT 52.54%, Manual CoT 51.06%; LastVowelConcatenation: Zero-shot 5.75%, Manual CoT 27.00%; FoomLetterConcatenation: Zero-shot 1.81%, Manual CoT 26.00%; Arithmetic (longer chains): Zero-shot 24.13%, Zero-shot CoT 56.12%, Manual CoT 50.43%, Arithmetic with explicit single-digit constraint (AE) Manual CoT 69.31%. Also: GPT-4-Turbo and others achieved 100% on the set of 118 possible 1-digit binary arithmetic operations (isolated intermediate functions), yet CoT still failed to generalize to long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT yields large gains only when prompts are extremely specific and the test instance is syntactically similar to examples (stacking/lexicographic). Gains collapse rapidly as problem size increases beyond example sizes; general/generic CoT often underperforms specific CoT and sometimes even direct prompting. Self-consistency often produced diverse plans and degraded aggregate accuracy (many unique answers causing random selection). For string tasks, CoT improved syntactic properties (reduced Levenshtein distance; correct letters present) but not correct ordering/quantities, suggesting pattern/style changes rather than algorithmic mastery. Even with perfect performance on isolated intermediate functions, models failed to extend to longer compositions, indicating failure to learn the compositional algorithm in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>GPT-4-Turbo's improvements from CoT are driven by highly specific, pattern-matchable prompts; CoT does not reliably teach generalizable algorithms in-context, and extensions like self-consistency do not rescue length/generalization failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8270.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8270.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model evaluated on Blocksworld planning and table-to-stack problems to test CoT prompt granularity and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thoughtlessness? An Analysis of CoT in Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct I/O prompting', 'Zero-shot CoT (magic phrase)', 'n-shot prompting (various granularities)', 'Manual Chain-of-Thought (Progression Proof, Blocksworld Universal Algorithm, Stacking, Lexicographic)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same set of prompting strategies as applied to GPT-4-Turbo and Claude-3-Opus: varying levels of example specificity and annotated step-by-step chains of thought provided as demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared many CoT granularities on intended distributions and on the table-to-stack subclass; measured accuracy by increasing stack height (n) to probe length generalization; compared domain-general CoT vs problem-specific CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Blocksworld (table-to-stack subset: 261 instances, stacks up to tested sizes), general Blocksworld varying n.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table-to-stack (261 instances): zero-shot 3.83%, zero-shot CoT 4.98%, domain-specific n-shot 6.13%, progression-proof CoT 6.89%, Blocksworld universal algorithm CoT 51.3%, problem-class n-shot 8.81%, stacking prompt 59.3% (best). Overall accuracy degrades drastically as stack height grows beyond example sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Like other models, GPT-4 only benefits from CoT when the prompt is narrowly tailored (stacking prompt), and even then generalization to larger stacks is poor; more general CoT prompts yield worse performance than highly specific ones. The paper interprets GPT-4's behavior as pattern matching rather than learning of general algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>GPT-4's apparent CoT gains come from prompt-specific pattern matching; CoT does not reliably produce generalizable algorithmic behavior in GPT-4 on planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8270.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8270.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3-Opus (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude-3-Opus model evaluated alongside GPT-4 and GPT-4-Turbo on Blocksworld and table-to-stack tasks to test the effects of CoT prompt specificity and self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thoughtlessness? An Analysis of CoT in Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct I/O prompting', 'Zero-shot CoT', 'n-shot prompting', 'Manual Chain-of-Thought variants', 'Self-consistency (evaluated in appendix)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated the same battery of prompting styles and CoT granularities as other models; also had self-consistency sampling experiments for table-to-stack.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison with GPT-4 and GPT-4-Turbo on identical prompt sets and datasets (table-to-stack and varying stack heights); self-consistency ablation with 5 sampled reasoning paths was run and reported in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Blocksworld (table-to-stack subset, increasing stack heights), synthetic tasks referenced (primary experiments focused on Blocksworld).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table-to-stack (261 instances): zero-shot 9.96%, zero-shot CoT 10.34%, domain-specific n-shot 16.4%, progression-proof CoT 4.59%, Blocksworld universal algorithm CoT 37.1%, problem-class n-shot 15.7%, stacking prompt 24.5%. Self-consistency experiments generally decreased accuracy relative to single-run stacking prompt results.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Claude-3-Opus exhibits the same failure modes: only narrow, specific CoT prompts help substantially; general CoT often underperforms direct prompting; self-consistency introduces more diverse outputs and worsens ensemble selection in the planning space.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Claude-3-Opus shows limited generalization from CoT; improvements require narrow problem-specific prompts and do not indicate the model has learned a general algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8270.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8270.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (variants including zero-shot, manual CoT, progression proof, domain-specific CoT, stacking/lexicographic CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of prompting techniques that prepend annotated example solutions with intermediate reasoning steps to elicit stepwise reasoning from LLMs; evaluated here with multiple granularities from universal to extremely problem-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thoughtlessness? An Analysis of CoT in Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to GPT-4, GPT-4-Turbo, Claude-3-Opus (and other LLMs in isolated tests)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT is a prompting methodology rather than a single model; this paper implements multiple CoT variants and tests their effects on different LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Zero-shot CoT (magic phrase)', 'Progression Proof CoT (PDDL-generic annotated actions with state/reason/result)', 'Blocksworld Universal Algorithm CoT (unstack-all then rebuild)', 'Stacking Prompt (table-to-stack, problem-class-specific CoT)', 'Lexicographic Stacking (highly specific syntactic subset)', 'Manual CoT with examples', 'Incorrect CoT (deliberately wrong CoT used as control)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot CoT: append 'let's think step by step'. Progression Proof: show action-by-action state, applicability reason, and resulting state annotated for sample PDDL problems. Blocksworld Universal Algorithm: explicit deconstruction/reconstruction algorithm shown in examples. Stacking/Lexicographic: examples restricted to table-to-stack and lexicographic forms to allow direct pattern matching. Incorrect CoT: control with wrong reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple CoT variants tested) but practical behavior tends to be similar (pattern matching) unless the CoT is extremely specific</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation-style experiments across granularities: universal (zero-shot), domain-general progression proof, domain-specific universal algorithm, problem-class-specific stacking, lexicographic stacking; also compared manual CoT vs incorrect CoT; tested self-consistency (sampling) as an orthogonal extension.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Blocksworld (general + table-to-stack), CoinFlip, LastLetterConcatenation variants, synthetic multi-step arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across tasks, CoT gives large gains only for narrow/specific prompts and small instance sizes. Example (table-to-stack, 261 instances): stacking prompt up to 59.3% (GPT-4) and 40.6% (GPT-4-Turbo) vs direct/zero-shot often below ~20%; Blocksworld universal algorithm yields intermediate gains (37.1% for GPT-4-Turbo, 51.3% for GPT-4). On CoinFlip (extended) CoT can reach ~95–99% for short-to-moderate lengths; for LastLetterConcatenation and arithmetic, Manual CoT boosts small-instance accuracy but collapses as sequence length increases. Incorrect CoT sometimes still increased accuracy (e.g., CoinFlip ~96.76%) suggesting susceptibility to spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT often changes output style/syntax and enables pattern matching; it does not reliably induce general, compositional algorithmic behavior. CoT improvements are brittle with respect to instance scaling and variation; incorrect CoT or biased example distributions can influence outputs without faithful algorithmic reasoning. Self-consistency provides no rescue in large solution spaces and can hurt accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought helps only when examples are exceedingly similar to queries; benefits derive from pattern matching and prompt-specific cues rather than in-context teaching of general algorithms; thus CoT's practical gains trade off sharply against human labor needed to craft many narrowly tailored examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8270.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8270.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (sampling multiple CoT outputs and selecting the most frequent final answer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style extension to chain-of-thought prompting that draws multiple sampled reasoning paths at nonzero temperature and selects the most frequent final answer; evaluated as an attempted robustness/improvement technique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thoughtlessness? An Analysis of CoT in Planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to GPT-4-Turbo and Claude-3-Opus in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-consistency implemented by sampling 5 different reasoning paths (temperature 0.7) and choosing the most frequent plan/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Sampling multiple CoT traces', 'Majority selection over final answers']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate multiple stochastic CoT outputs with temperature > 0, collect final answers/plans, choose the most frequent final answer (ties broken randomly).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple varied reasoning traces are intentionally generated and aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>For table-to-stack Blocksworld, sampled 5 reasoning paths per query at temperature 0.7 and used majority vote to produce a final plan/answer; compared aggregated accuracy vs single-run CoT results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Table-to-stack Blocksworld (261 instances) with increasing stack sizes; reported in Appendix A.2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Self-consistency did not improve generalization; for table-to-stack problems it was generally worse than single-run CoT (e.g., stacking prompt accuracy dropped under self-consistency; specific numbers: stacking prompt single-run GPT-4-Turbo 40.6% vs self-consistency stacking 24.5% as reported). Many queries produced five unique responses, forcing effectively random tie-breaking.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency increased output diversity in large planning solution spaces, which led to inconsistent final answers and often worse aggregate performance. The method is less effective outside small/controlled multiple-choice style tasks because planning outputs are many and diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency, while beneficial in some prior CoT contexts, does not rescue the length/generalization failures in planning and can degrade performance by producing many distinct plans that undermine majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thoughtlessness? An Analysis of CoT in Planning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Large language models are not abstract reasoners <em>(Rating: 1)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8270",
    "paper_id": "paper-269626390",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo (as evaluated in this paper)",
            "brief_description": "A state-of-the-art large language model evaluated across Blocksworld planning and multiple synthetic reasoning benchmarks; used as the primary model in many experiments assessing chain-of-thought prompt granularity and generalization.",
            "citation_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_description": "Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).",
            "reasoning_methods": [
                "Direct I/O prompting (no CoT)",
                "Zero-shot CoT (\"let's think step by step\")",
                "n-shot prompting (domain-specific and problem-class-specific examples)",
                "Manual Chain-of-Thought (multiple handcrafted CoT variants: Progression Proof, Blocksworld Universal Algorithm, Stacking, Lexicographic Stacking)",
                "Self-consistency (sample multiple CoT paths and take majority)"
            ],
            "reasoning_methods_description": "Direct I/O: present problem in natural language and request a plan; Zero-shot CoT: append magic phrase to elicit step-by-step; n-shot: prepend examples without annotated thoughts; Manual CoT: prepend human-crafted solutions with intermediate reasoning steps (various granularities from domain-general to highly problem-specific); Self-consistency: sample multiple reasoning outputs at nonzero temperature and choose most frequent final answer.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Systematic comparison across prompting granularity (zero-shot, zero-shot CoT, domain-specific n-shot, progression proof CoT, Blocksworld universal algorithm CoT, problem-class n-shot, stacking CoT/lexicographic CoT) and an evaluation of self-consistency (5 sampled reasoning paths) versus single-run outputs; also applied the same CoT prompts across multiple benchmarks (Blocksworld table-to-stack, general Blocksworld, CoinFlip, LastLetterConcatenation variants, multi-step arithmetic).",
            "task_or_benchmark": "Blocksworld (general Blocksworld and table-to-stack subclass, n from 3 to 20 blocks), CoinFlip (1–28 names), LastLetterConcatenation / LastVowelConcatenation / FoomLetterConcatenation (1–20 words), synthetic multi-step single-digit arithmetic (1–29 operations).",
            "performance_results": "Blocksworld (table-to-stack, 261 instances): zero-shot 19.1%, zero-shot CoT 21.0%, domain-specific n-shot 13.7%, progression-proof CoT 15.3%, Blocksworld universal algorithm CoT 37.1%, problem-class n-shot 18.0%, stacking prompt 40.6% (best). Self-consistency on table-to-stack: stacking prompt 24.5% (worse than single-run stacking 40.6%). Synthetic benchmarks (Table 2, GPT-4-Turbo): CoinFlip: Zero-shot 56.38%, Zero-shot CoT 95.71%, Manual CoT 98.89%; LastLetterConcatenation: Zero-shot 10.00%, Zero-shot CoT 52.54%, Manual CoT 51.06%; LastVowelConcatenation: Zero-shot 5.75%, Manual CoT 27.00%; FoomLetterConcatenation: Zero-shot 1.81%, Manual CoT 26.00%; Arithmetic (longer chains): Zero-shot 24.13%, Zero-shot CoT 56.12%, Manual CoT 50.43%, Arithmetic with explicit single-digit constraint (AE) Manual CoT 69.31%. Also: GPT-4-Turbo and others achieved 100% on the set of 118 possible 1-digit binary arithmetic operations (isolated intermediate functions), yet CoT still failed to generalize to long sequences.",
            "qualitative_findings": "CoT yields large gains only when prompts are extremely specific and the test instance is syntactically similar to examples (stacking/lexicographic). Gains collapse rapidly as problem size increases beyond example sizes; general/generic CoT often underperforms specific CoT and sometimes even direct prompting. Self-consistency often produced diverse plans and degraded aggregate accuracy (many unique answers causing random selection). For string tasks, CoT improved syntactic properties (reduced Levenshtein distance; correct letters present) but not correct ordering/quantities, suggesting pattern/style changes rather than algorithmic mastery. Even with perfect performance on isolated intermediate functions, models failed to extend to longer compositions, indicating failure to learn the compositional algorithm in-context.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "GPT-4-Turbo's improvements from CoT are driven by highly specific, pattern-matchable prompts; CoT does not reliably teach generalizable algorithms in-context, and extensions like self-consistency do not rescue length/generalization failures.",
            "uuid": "e8270.0",
            "source_info": {
                "paper_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (as evaluated in this paper)",
            "brief_description": "A state-of-the-art large language model evaluated on Blocksworld planning and table-to-stack problems to test CoT prompt granularity and generalization.",
            "citation_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).",
            "reasoning_methods": [
                "Direct I/O prompting",
                "Zero-shot CoT (magic phrase)",
                "n-shot prompting (various granularities)",
                "Manual Chain-of-Thought (Progression Proof, Blocksworld Universal Algorithm, Stacking, Lexicographic)"
            ],
            "reasoning_methods_description": "Same set of prompting strategies as applied to GPT-4-Turbo and Claude-3-Opus: varying levels of example specificity and annotated step-by-step chains of thought provided as demonstrations.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared many CoT granularities on intended distributions and on the table-to-stack subclass; measured accuracy by increasing stack height (n) to probe length generalization; compared domain-general CoT vs problem-specific CoT.",
            "task_or_benchmark": "Blocksworld (table-to-stack subset: 261 instances, stacks up to tested sizes), general Blocksworld varying n.",
            "performance_results": "Table-to-stack (261 instances): zero-shot 3.83%, zero-shot CoT 4.98%, domain-specific n-shot 6.13%, progression-proof CoT 6.89%, Blocksworld universal algorithm CoT 51.3%, problem-class n-shot 8.81%, stacking prompt 59.3% (best). Overall accuracy degrades drastically as stack height grows beyond example sizes.",
            "qualitative_findings": "Like other models, GPT-4 only benefits from CoT when the prompt is narrowly tailored (stacking prompt), and even then generalization to larger stacks is poor; more general CoT prompts yield worse performance than highly specific ones. The paper interprets GPT-4's behavior as pattern matching rather than learning of general algorithms.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "GPT-4's apparent CoT gains come from prompt-specific pattern matching; CoT does not reliably produce generalizable algorithmic behavior in GPT-4 on planning tasks.",
            "uuid": "e8270.1",
            "source_info": {
                "paper_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Claude-3-Opus",
            "name_full": "Claude-3-Opus (as evaluated in this paper)",
            "brief_description": "Anthropic's Claude-3-Opus model evaluated alongside GPT-4 and GPT-4-Turbo on Blocksworld and table-to-stack tasks to test the effects of CoT prompt specificity and self-consistency.",
            "citation_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
            "mention_or_use": "use",
            "model_name": "Claude-3-Opus",
            "model_description": "Named large language model used in experiments in this paper (no additional architectural or size details provided in the paper).",
            "reasoning_methods": [
                "Direct I/O prompting",
                "Zero-shot CoT",
                "n-shot prompting",
                "Manual Chain-of-Thought variants",
                "Self-consistency (evaluated in appendix)"
            ],
            "reasoning_methods_description": "Evaluated the same battery of prompting styles and CoT granularities as other models; also had self-consistency sampling experiments for table-to-stack.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparison with GPT-4 and GPT-4-Turbo on identical prompt sets and datasets (table-to-stack and varying stack heights); self-consistency ablation with 5 sampled reasoning paths was run and reported in appendix.",
            "task_or_benchmark": "Blocksworld (table-to-stack subset, increasing stack heights), synthetic tasks referenced (primary experiments focused on Blocksworld).",
            "performance_results": "Table-to-stack (261 instances): zero-shot 9.96%, zero-shot CoT 10.34%, domain-specific n-shot 16.4%, progression-proof CoT 4.59%, Blocksworld universal algorithm CoT 37.1%, problem-class n-shot 15.7%, stacking prompt 24.5%. Self-consistency experiments generally decreased accuracy relative to single-run stacking prompt results.",
            "qualitative_findings": "Claude-3-Opus exhibits the same failure modes: only narrow, specific CoT prompts help substantially; general CoT often underperforms direct prompting; self-consistency introduces more diverse outputs and worsens ensemble selection in the planning space.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Claude-3-Opus shows limited generalization from CoT; improvements require narrow problem-specific prompts and do not indicate the model has learned a general algorithm.",
            "uuid": "e8270.2",
            "source_info": {
                "paper_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (variants including zero-shot, manual CoT, progression proof, domain-specific CoT, stacking/lexicographic CoT)",
            "brief_description": "A family of prompting techniques that prepend annotated example solutions with intermediate reasoning steps to elicit stepwise reasoning from LLMs; evaluated here with multiple granularities from universal to extremely problem-specific.",
            "citation_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
            "mention_or_use": "use",
            "model_name": "applied to GPT-4, GPT-4-Turbo, Claude-3-Opus (and other LLMs in isolated tests)",
            "model_description": "CoT is a prompting methodology rather than a single model; this paper implements multiple CoT variants and tests their effects on different LLMs.",
            "reasoning_methods": [
                "Zero-shot CoT (magic phrase)",
                "Progression Proof CoT (PDDL-generic annotated actions with state/reason/result)",
                "Blocksworld Universal Algorithm CoT (unstack-all then rebuild)",
                "Stacking Prompt (table-to-stack, problem-class-specific CoT)",
                "Lexicographic Stacking (highly specific syntactic subset)",
                "Manual CoT with examples",
                "Incorrect CoT (deliberately wrong CoT used as control)"
            ],
            "reasoning_methods_description": "Zero-shot CoT: append 'let's think step by step'. Progression Proof: show action-by-action state, applicability reason, and resulting state annotated for sample PDDL problems. Blocksworld Universal Algorithm: explicit deconstruction/reconstruction algorithm shown in examples. Stacking/Lexicographic: examples restricted to table-to-stack and lexicographic forms to allow direct pattern matching. Incorrect CoT: control with wrong reasoning traces.",
            "reasoning_diversity": "diverse (multiple CoT variants tested) but practical behavior tends to be similar (pattern matching) unless the CoT is extremely specific",
            "reasoning_diversity_experimental_setup": "Ablation-style experiments across granularities: universal (zero-shot), domain-general progression proof, domain-specific universal algorithm, problem-class-specific stacking, lexicographic stacking; also compared manual CoT vs incorrect CoT; tested self-consistency (sampling) as an orthogonal extension.",
            "task_or_benchmark": "Blocksworld (general + table-to-stack), CoinFlip, LastLetterConcatenation variants, synthetic multi-step arithmetic.",
            "performance_results": "Across tasks, CoT gives large gains only for narrow/specific prompts and small instance sizes. Example (table-to-stack, 261 instances): stacking prompt up to 59.3% (GPT-4) and 40.6% (GPT-4-Turbo) vs direct/zero-shot often below ~20%; Blocksworld universal algorithm yields intermediate gains (37.1% for GPT-4-Turbo, 51.3% for GPT-4). On CoinFlip (extended) CoT can reach ~95–99% for short-to-moderate lengths; for LastLetterConcatenation and arithmetic, Manual CoT boosts small-instance accuracy but collapses as sequence length increases. Incorrect CoT sometimes still increased accuracy (e.g., CoinFlip ~96.76%) suggesting susceptibility to spurious cues.",
            "qualitative_findings": "CoT often changes output style/syntax and enables pattern matching; it does not reliably induce general, compositional algorithmic behavior. CoT improvements are brittle with respect to instance scaling and variation; incorrect CoT or biased example distributions can influence outputs without faithful algorithmic reasoning. Self-consistency provides no rescue in large solution spaces and can hurt accuracy.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-thought helps only when examples are exceedingly similar to queries; benefits derive from pattern matching and prompt-specific cues rather than in-context teaching of general algorithms; thus CoT's practical gains trade off sharply against human labor needed to craft many narrowly tailored examples.",
            "uuid": "e8270.3",
            "source_info": {
                "paper_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (sampling multiple CoT outputs and selecting the most frequent final answer)",
            "brief_description": "An ensemble-style extension to chain-of-thought prompting that draws multiple sampled reasoning paths at nonzero temperature and selects the most frequent final answer; evaluated as an attempted robustness/improvement technique.",
            "citation_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
            "mention_or_use": "use",
            "model_name": "applied to GPT-4-Turbo and Claude-3-Opus in this paper",
            "model_description": "Self-consistency implemented by sampling 5 different reasoning paths (temperature 0.7) and choosing the most frequent plan/answer.",
            "reasoning_methods": [
                "Sampling multiple CoT traces",
                "Majority selection over final answers"
            ],
            "reasoning_methods_description": "Generate multiple stochastic CoT outputs with temperature &gt; 0, collect final answers/plans, choose the most frequent final answer (ties broken randomly).",
            "reasoning_diversity": "diverse (multiple varied reasoning traces are intentionally generated and aggregated)",
            "reasoning_diversity_experimental_setup": "For table-to-stack Blocksworld, sampled 5 reasoning paths per query at temperature 0.7 and used majority vote to produce a final plan/answer; compared aggregated accuracy vs single-run CoT results.",
            "task_or_benchmark": "Table-to-stack Blocksworld (261 instances) with increasing stack sizes; reported in Appendix A.2.",
            "performance_results": "Self-consistency did not improve generalization; for table-to-stack problems it was generally worse than single-run CoT (e.g., stacking prompt accuracy dropped under self-consistency; specific numbers: stacking prompt single-run GPT-4-Turbo 40.6% vs self-consistency stacking 24.5% as reported). Many queries produced five unique responses, forcing effectively random tie-breaking.",
            "qualitative_findings": "Self-consistency increased output diversity in large planning solution spaces, which led to inconsistent final answers and often worse aggregate performance. The method is less effective outside small/controlled multiple-choice style tasks because planning outputs are many and diverse.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency, while beneficial in some prior CoT contexts, does not rescue the length/generalization failures in planning and can degrade performance by producing many distinct plans that undermine majority voting.",
            "uuid": "e8270.4",
            "source_info": {
                "paper_title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Large language models are not abstract reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_not_abstract_reasoners"
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning",
            "rating": 1,
            "sanitized_title": "teaching_algorithmic_reasoning_via_incontext_learning"
        }
    ],
    "cost": 0.01895775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Chain of Thoughtlessness? An Analysis of CoT in Planning
12 Mar 2025</p>
<p>Kaya Stechly kstechl@asu.edu 
Karthik Valmeekam kvalmeek@asu.edu 
Subbarao Kambhampati </p>
<p>SCAI
Arizona State University</p>
<p>SCAI
Arizona State University</p>
<p>SCAI
Arizona State University</p>
<p>Chain of Thoughtlessness? An Analysis of CoT in Planning
12 Mar 2025AB535768B7CC922B273A9983D043D626arXiv:2405.04776v3[cs.AI]
Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution.Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt.While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts.This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.† * equal contribution † Resources and source code for planning experiments can be found at https://github.com/karthikv792/cot-planning and for other domains at https://github.com/kstechly/cot-scheduling 38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Introduction</p>
<p>While originally designed for text completion, Large Language Models (LLMs) have shown promise on a diverse set of unrelated tasks.While initial anecdotal results were unexpectedly impressive [8], followup systematic studies showed that-outside of limited, non-generalizable classes of problemsthese models generally perform poorly on basic, multi-hop reasoning tasks [17] ranging from arithmetic [35] and logic puzzles [14] to constraint satisfaction [42,2] and classical planning [47].</p>
<p>At the same time, the subfield of prompt engineering [36] has grown rapidly, promising improvements in performance without retraining.A core tenet of this subfield is that LLMs are capable of powerful in-context learning [12,56], that is, capable of intelligently using additional context provided in a prompt to correctly respond to queries that would otherwise be answered incorrectly.Generally, this requires operationalizing algorithmic/procedural advice, and, in principle, learning such procedures includes being able to effectively apply them beyond syntactically similar instances.</p>
<p>The foundational method for inducing in-context learning is the chain of thought approach, which has been claimed to "unlock the reasoning abilities of LLMs" [50].To create a chain of thought (CoT) prompt, a user annotates similar problems with intermediate reasoning steps and prepends them to the standard prompt.These annotations are meant as demonstrations, intended to teach a procedure applicable to both the examples and the new query.When prompted like this, the LLM is expected to output a similar series of reasoning steps prior to the new answer.Numerous studies have claimed that this procedure significantly enhances LLM performance in complex reasoning tasks [49,54,39,56,52,43].However, in general it is unclear how "similar" the examples need to be to the problem, how broadly any given chain of thought prompt will apply, and-most importantly-how much human effort is necessary to craft prompts specific to each problem subclasses.Followup work has claimed that merely adding magic phrases ("let's think step by step") to every prompt is sufficient for some improvement [26].While in some domains, this technique has proven to be even more brittle than manual CoT, it has achieved the same performance increases in others, hinting that improvements observed with CoT may not indicate as much about LLMs' general in-context learning abilities as previously thought.</p>
<p>We are interested in the tradeoff between possible performance gains from chain of thought prompt engineering and the amount of human labor necessary to generate examples with useful reasoning traces.Ideally, a properly constructed prompt should teach the LLM how to robustly generalize a basic algorithmic procedure in order to increase performance on a large class of problems, thereby converting a modest amount of human teaching effort into a significant capability boost.Unfortunately, this only seems to be possible to a very limited extent [14].</p>
<p>In the current work, we examine the limits of chain of thought in solving classical planning problems.Test domains commonly used in previous chain of thought studies (e.g.GSM8K [10], CommonSense QA [44]) present two significant issues: (a) they lack a systematic method to scale instances, which is essential for evaluating whether LLMs can extend provided procedures to larger instances of the same type, and (b) due to their static nature, are more likely to be well-represented on the web [51], increasing the chance that they were part of LLM training data, a factor which could obscure the true reasoning capabilities of LLMs.Planning is a well-studied kind of sequential decision-making which tasks an agent with devising a plan that takes a given initial state to a pre-specified goal state.New, diverse, and unique problem instances are easy to generate, but potentially hard to solve.</p>
<p>We focus on Blocksworld, a simple commonsense domain widely recognized and utilized in International Planning Competitions [23], where a set of blocks in an initial configuration must be rearranged step-by-step into a goal configuration.For a subset of our results, we simplify even further, and only consider problem instances where every block starts on the table and the goal is a single stack of blocks.These instances require very minimal reasoning: one need only figure out which block is on the bottom, and then stack the remaining blocks in the sequence directly defined in the goal.For 3 ≤ n ≤ 20, we generate a variety of instances where the goal requires a specific n height stack, while providing examples of how to solve 2 and 3 height instances.</p>
<p>We consider different chain of thought prompts, where each is more specific-and provides more problem-specific knowledge-than the last: a zero-shot variant, a general progression proof, a suboptimal algorithm specific to Blocksworld, a table-to-stack specific simplification of that algorithm, and a lexicographic version of the simplification.The most general could be applied to any problem, while the least is specific to an easier version of the stacking problem.The three human-crafted prompts all teach algorithms which could, in principle, solve any of the instances they are tested on.We test on three state of the art models: GPT-4 [3], Claude-3-Opus, [5] and GPT-4-Turbo.</p>
<p>Our results reconfirm that LLMs are generally incapable of solving simple planning problems [47], and demonstrate that chain of thought approaches only improve performance when the hand-annotated examples and the query are sufficiently similar to the current query.As goal stack size increases, accuracy drops drastically, regardless of the specificity of the chain of thought prompt.As generality of the prompt increases, performance on even the smallest goal stacks also decreases, and often falls short of standard prompting.Even state of the art extensions of CoT (like self-consistency [49]), show similar or sometimes even worse performance.Overall, this case study calls into question assumptions about the generalizable effectiveness of chain of thought, and suggests that LLMs do not learn new, general algorithms in context, but instead rely on some form of pattern matching to achieve prompt-design-specific performance increases.This in turn increases the burden on humans giving advice.</p>
<p>To better compare to previous work, we construct scalable versions of three previously studied synthetic problems-Coin Flip, Last Letter Concatenation, and multi-step arithmetic [49,50,26,48]and replicate reported chain of thought prompts.While these domains do not have a corresponding notion of prompt granularity, they do cover a range of difficulties.When testing on GPT-4-Turbo, We see a similar lack of generalization on these problem sets as we saw in Blocksworld.</p>
<p>In the rest of this paper, we first review related work, then describe the chain of thought approaches we have developed in the context of planning, analyze the overall effectiveness of chain of thought prompting on Blocksworld problems, and extend our results to three synthetic tasks well-represented in the CoT literature.</p>
<p>Related Work</p>
<p>Modifying text prompts to elicit intermediate problem-solving steps from LLMs originally took the form of scratchpads [33].[50] proposed a similar prompt style in natural language, dubbing this approach chain of thought (CoT), and claiming that-with some human hand-annotation of examples-this not only boosts performance without retraining, but "allows reasoning abilities to emerge naturally".They argued that by merely interspersing intermediate reasoning steps in natural language into examples, they were inducing the LLM to "learn via a few examples", motivating this idea with anthropomorphizations ("Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem").[26] argued that some of the performance of CoT could be retained without providing any examples, and instead just appending the magic phrase "let's think step by step" to the end of a prompt.This has been called zero-shot CoT.</p>
<p>However, CoT has long been known to be imperfect and incomplete.Previous work has investigated improving the consistency of CoT through self-consistency [49], multi-agent debate [13], least-tomost prompting [55], deductive verification [28], and other approaches.Unfortunately, many of these involve prompting the LLM multiple times for a single problem, which can balloon the cost of inference.Other work has examined the possibility of reducing or removing the need for human annotation of examples by using LLMs to generate their own examples automatically [54,9].To avoid well-known issues with the brittleness of LLM self-verification and self-teaching [42,22,20,19,24], we restrict this paper's scope to manually written chains of thought.</p>
<p>Previous papers have analyzed CoT from multiple perspectives [15,37], finding that there is only a loose relationship between the presented chain and the final answer [6], and that the correctness of provided annotations has little effect on resultant performance [38].LLM-produced chains of thought are also known to be unfaithful to the underlying reasoning process [29,25,11].In particular, the way the examples are presented can bias a model into giving some answer (e.g. if all the example answers are A, the model will be more likely to output A), but its CoT will not reflect this [45].</p>
<p>Motivated by claims that CoT prompts allow models to learn in context how to reason-that is, to learn how to execute human-specified algorithms-we focus on CoT prompting's out-of-domain generalization.[14] previously showcased a lack of generalization in multiplication, puzzles, and a number sequence problem, even when the model was fine-tuned on CoT examples.However, they only examined one set of prompts, did not experiment with levels of prompt specificity, and were much more interested in local failures of compositionality arising from cumulating error.More broadly, previous work has examined generalization limits of LLMs in arithmetic tasks [35], formula simplification [34], and theorem proving [4].</p>
<p>While early accounts claimed LLMs, despite not being trained for it, were capable of reasoning and planning [8], later work showcased serious brittleness across these domains [47].[50] claims that "standard prompting only provides a lower bound on the capabilities of large language models", with proper prompting allowing reasoning to "emerge naturally."Recent work seems to maintain this optimism [7].In this paper, we examine the effectiveness of CoT in the context of classical planning problems, which have well-defined and algorithmically checkable ground truths, can be generated with arbitrary size and difficulty, and are unlikely to be in the training data.If CoT induces more than just pattern matching, and can in fact teach LLMs to perform generalizable, compositional reasoning, then we should expect that to be reflected in robust and maintainable improvements on a simple commonsense benchmark set like Blocksworld, and we should expect these results to hold for scaled variants of the very benchmarks tested in [50] and later CoT work.</p>
<p>Background</p>
<p>Classical planning problems task a planner with finding a sequence of actions that, when executed, will take an agent from a pre-specified initial state to a desired goal state.STRIPS planning is a discrete, deterministic formalism that encompasses this class.Problems are represented using the Planning Domain and Definition Language (PDDL) [30] and have long featured in various planning challenges and competitions.Our main experiments are all on the Blocksworld PDDL domain.</p>
<p>A PDDL specification consists of three components.The domain doesn't change between problems and consists of a set of predicates-whose truth values describe the state of the world-and a set of actions-defined by their preconditions and effects-that the agent is allowed to take.The initial state is a list of predicates that are true at the outset of the specific problem (an example predicate: "Block A is on the table ").The goal is a boolean expression of predicates (a goal: "Block A is on Block B.").</p>
<p>A plan is a sequence of actions.The solution to a PDDL problem is a plan in which the preconditions of every action are satisfied at execution time, and which arrives at a goal-satisfying final state.To verify a plan, follow the actions in order and check that these two desiderata are achieved.In this work, we convert natural language responses into PDDL [46] and evaluate them with VAL [21].We examine the influence of prompt selection on LLM performance within subsets of the Blocksworld domain.</p>
<p>Chain of Thought Setups for Planning</p>
<p>A formally specified problem instance can be translated into many possible prompts.The most basic of these is input/output (I/O) prompting: the problem is translated directly from PDDL into natural language and provided to the LLM [47].While this directly tests the LLM's ability to solve the problem, it is not always the most effective strategy for maximizing performance.</p>
<p>Drawing on metaphors of human learning, recent literature has claimed that LLMs are capable of in-context learning.The basic idea is that-by first presenting the model with examples of similar problems-it is possible to cause an LLM to acquire relevant new skills within the current context window.n-shot prompts operationalize this by prepending a number of relevant examples.Chain of thought [50] approaches take this further, presenting human-crafted "thoughts" which the LLM is intended to imitate in its response.Practitioners argue that, intuitively, these augmented examples teach the LLM how to solve problems in the given set.</p>
<p>However, this method relies on human labor [53] to provide task-specific knowledge and an (at least rough) algorithmic or procedural approach to the problem.The more general the provided knowledge is, the more problems it can be applied to, and the less human prompt-crafting it requires.On the other hand, the more granular and specific it is, the more performance can be expected.</p>
<p>In our experiments, we consider subsets of Blocksworld problems.We follow a prompt structure similar to that described in [47],1 but include "thoughts" in our n-shot prompts.These thoughts are written to follow an algorithmic procedure for solving the example problem.</p>
<p>Not every procedure is applicable to every problem.From the point of view of a human handcrafting a chain of thought prompt, there is intuitively an expected target distribution on which the demonstrated algorithm generally works.For instance, a prompt designer detailing how to stack C on top of B on top of A will expect that a model that learns this procedure will also be capable of stacking B on top of A on top of C, but may not expect it to know how to first properly dismantle an existing tower of blocks to access a necessary block.However, this distribution often differs from the effective target distribution-that is, the actual set of problems on which the prompt gives robust improvements in performance.We explicitly describe the gap between these two distributions.</p>
<p>Zero-Shot Chain of Thought (Universal): This is the most general approach, and involves merely appending "let's think step by step" to the end of the prompt [26].</p>
<p>Progression Proof (Specific to PDDL): Versions of this CoT could, in principle, be prepended to any PDDL problem prompt, as the generation of annotated examples is easy to automate without knowledge of the specific PDDL domain.[47] This prompt includes (1) a meta-prompt explaining plan correctness and (2) an example where each action is annotated with the state prior to the action, the reason why the action is applicable in that state, and the resulting state after the action is applied.Examples start from an arbitrary block configuration and construct a single stack of blocks from it.</p>
<p>Blocksworld Universal Algorithm (Specific to the Domain): In Blocksworld, it is possible to reach any goal state from any initial state by simply unstacking all the blocks, placing them on the table, and then reassembling them into the required stacks.Resulting plans are not only executable and goal-reaching, but will never exceed twice the length of the optimal plan for any given instance [40].This prompt demonstrates an annotated version of this approach, explaining and performing both the deconstruction and reconstruction steps of the algorithm.The same examples are used as in the previous prompt.The expected target distribution encompasses all Blocksworld problems.</p>
<p>Stacking Prompt (Specific to a Narrow Problem Class): Every example is a table-to-stack problem: every block starts on the table, and the goal is to create a single specific stack of blocks.This specificity simplifies the problem greatly, and allows near-direct pattern matching between the examples and the LLM's output; however, it is infeasible to specify prompts with this level of detail for every problem class.The expected target distribution is table-to-stack Blocksworld problem, as they are the only problems that can be solved by the described algorithm.</p>
<p>Lexicographic Stacking (Specific to Particular Syntactic Sequences): We simplify the problem further by focusing on a particular syntactic form of the goal.This prompt is very similar to the stacking prompt, but is specific to a subset of the target distribution: the goal state is always a lexicographic prefix (e.g., A, AB, ABC, etc.).</p>
<p>Blocksworld Results</p>
<p>We perform two parallel studies.The first tests each chain of thought prompt on its intended problem distribution, as explained in the previous section.Then, we focus on a specific subclass of Blocksworld problems and test every prompt on just that subclass.Together, we expect these two studies to give us a good picture of how effective LLMs are in applying advice beyond the specific instances.</p>
<p>Testing on Intended Problem Distributions</p>
<p>We evaluate the performance of GPT-4 and Claude-3-Opus on Blocksworld problems with both standard 2-shot prompts and chain of thought prompts of varying granularity.Each prompt is tested on its intended problem class, as discussed in the previous section.</p>
<p>Chain of thought does not meaningfully enhance performance except on the narrowest problem distributions.While providing this chain of thought advice becomes significantly harder as the level of specificity increases, it is necessary, as the LLM succeeds only when the problem is reduced to a level where basic pattern matching suffices: at each stage, stack the next letter on top; if that letter does not exist on the table, then stop.</p>
<p>A key advantage of planning domains is that they provide the ability to easily and systematically generate larger test sets, including arbitrarily more challenging instances.The difficulty of a Blocksworld instance scales with the number of blocks involved, allowing us to clearly assess the out-of-domain generalization achievable with and without chain of thought.As shown in Figure 2, chain of thought does not generalize beyond a handful of blocks.Note that sound planning systems (such as Fast Downward) have a 100% accuracy on all problems tested.</p>
<p>Testing only on Table-to-Stack</p>
<p>As mentioned before, a table-to-stack problem is any problem in the intended target distribution of the stacking prompt.The initial state has every block on the table, with a goal of arranging all the blocks into a single, pre-specified stack.While a simple problem, GPT-4's zero-shot performance over 261 instances is 3.8%.With the stacking CoT prompt, performance improves to 59.3%.Is this a result of the model learning in-context how to reason correctly over this type of problem?If so, we might expect it to perform the same when presented with a more general CoT prompt that demonstrates the same procedure, but is applicable to a greater variety of problems.</p>
<p>To check this, we evaluate performance of our prompts on table-to-stack problems with prompts of varying granularity: standard I/O prompting, general n-shot (drawn from arbitrary Blocksworld problems), goal-specific n-shot (drawn from table-to-stack problems), and three levels of CoT specificity.Table 1 shows the results: only the most specific and least applicable prompt retains anywhere near this performance improvement.Figure A.1.1 in the appendix further illustrates that none of the prompts provide robust stack-height generalizability.We also tested self-consistency [49] on these prompts, but found that performance dropped.Details can be found in Appendix A. If chain of thought is meant to replicate human thinking or learning, it should generalize beyond the most direct pattern matches and allow for more robust reasoning across similar problems.However, our results only show a modest improvement in performance on some domains, with specific enough prompting strategies, which quickly deteriorates when the problems shown become slightly larger.</p>
<p>Extension to Scalable Synthetic Benchmarks</p>
<p>Previous work on CoT mainly constrained its evaluations to static test sets ranging from commonsense domains (Sports Understanding [41], StrategyQA [18], CommonSenseQA [44]), few-hop math word problems (AsDiv [31], GSM8k [10], MAWPS [27]), to a number of basic "symbolic reasoning" tasks (CoinFlip [26], LastLetterConcatenation [26], Shuffled Objects [41]).[26,50,55,6].Many of these benchmarks are difficult to scale, but a number of them can be modified to allow for the generation of arbitrary new instances which nevertheless have clear ground truths.We examine CoinFlip, LastLetterConcatenation, and a synthetic proxy for multi-step arithmetical reasoning.Exact prompt details can be found in the appendices A.7, A.8, and A.9.When possible we used the manual CoT prompts found in [50] and the zero-shot CoT prompt described in [26].Number of examples ranges from 0 to 3 for both CoT and direct prompts.Results for all three domains are in Table 2 and Figure 3.</p>
<p>CoinFlip: Parity tests have a long history in machine learning [32].CoinFlip is a natural language version of this task introduced in [50] to showcase the performance of CoT, though that paper only studies up to four flip problems.An example prompt is "A coin is heads up.Craig flips the coin.Alice does not flip the coin.Is the coin still heads up?".The correct answer is "no".Note that chance performance on this domain is 50%, as there are only two possible answers.Our extension to the domain is detailed in A.3</p>
<p>LastLetterConcatenation: Also introduced in [50], the LastLetterConcatenation task is a simple text processing task that asks for the concatenation of the last letters of a series of words.An example prompt is "Take the last letters of each word in 'Craig Alice' and concatenate them."for which the correct answer is "ge".The set of possible answers on this task is much larger than in CoinFlip, but previous work has claimed significant performance increases on this kind of task with CoT.Modeling something similar to our Blocksworld granularity experiments, we create two other test sets, using the same underlying words in the same distribution, but which differ in what they ask the model to do.LastVowelConcatenation requires using only the last vowels of words.FoomLetterConcatenation requires using the first letter of the first word, the second letter of the second word, and so forth.If the nth word does not have an nth letter, the problem specifies that a 0 should be concatenated to the string instead.</p>
<p>Multi-step Arithmetic on Single-Digit Numbers: CoT is often tested on math word problems.However, many of these test sets only include problems which require very small numbers of reasoning steps.GSM8k was designed partly so that its problems would "require more steps to solve", but its problems only range 2 to 8 steps [10], and, in fact, previous analyses have found that only 10% of those problems require more than five steps-the majority is 2, 3, or 4. [16]  To sidestep this issue, we construct a synthetic dataset that involves linearly simplifying parenthesized expressions that consist of repeated applications of the four basic arithmetical operations on one digit numbers.An example prompt is "Simplify the following expression into a single number: 3 / (9 -(5 + (1))).",where the correct answer is 1.We filter our problems so that no operation ever results in a number that isn't in the range 1 to 9.2 This can be seen as a deeply simplified variant of the arithmetical expression simplification dataset presented in [34] where no modular arithmetic, negative numbers, or non-linear nesting is required.However, we extend our maximum number of required reasoning steps much further and we construct prompts which are more specific and spell out every single step explicitly.More details on the dataset can be found in A.5.</p>
<p>Results</p>
<p>Length Generalization The only synthetic domain that shows any hints of generalization is CoinFlip.Using [50]'s prompt, performance is perfect for 1 through 4 step problems, starts to show the occasional mistake after, and only dips below 90% at 31-step problems (as shown in Figure 3).However, the problems in this domain are very simple.Parallel to the lexicographic stacking case of Blocksworld, it does not require much reasoning beyond counting up to an average of half a given problem's step count.</p>
<p>LastLetterConcatenation and multi-step arithmetic show behavior almost identical to our main experiments.While sufficiently specific CoT prompts do increase performance on small instances, this performance increase quickly degrades as the number of steps necessary increases.Notably, the string-based nature of the LastLetterConcatenation problem does allow us to examine what exact improvement CoT is inducing.We examine the data with different metrics and find that the only properties that do generalize with CoT are syntactic.In particular, while overall accuracy plummets back to that of direct prompting, CoT consistently improves the Levenshtein distance to the correct answer and ensures that the final response string contains exactly the right letters, just not in the right order or number.We take this as further evidence that CoT, rather than teaching algorithms or procedures, modifies the syntactic style of the LLM's output, and that this pattern matching is what leads to observed increases in performance on smaller instances.</p>
<p>Prompt Granularity and Problem Variation Because of the simplicity of these problems, prompt granularity is much harder to examine than in Blocksworld.There isn't enough variation in possible problems.However, across the three types of letter concatenation and two types of arithmetic expression simplification that we test, we see very similar patterns as before: CoT's performance improvements are maintained much longer in easier cases, and take longer to collapse back to direct performance.There still seems to be a "sweet spot" where the problem is just barely hard enough that CoT makes a difference, but not so hard that this difference doesn't matter.</p>
<p>Examining Intermediate Reasoning The construction of our synthetic arithmetic task gives some hints as to what part of CoT may be failing.[14] argues that compositional reasoning fails because LLMs perform linearized subgraph matching and act as noisy estimators of intermediate functions (see e.g.proposition 4.2 in [14]) and that performance collapses follow from the fact that repeated application of any error-prone function estimator leads to exponentially accumulating error.</p>
<p>In our problem, it is possible to exhaustively check whether this is the case.There are exactly 118 possible 1-digit binary arithmetic problems which result in a 1-digit number.We tested GPT-4-Turbo, GPT-4, GPT-3.5-Turbo,Llama3-70b, and Llama3-8b on this dataset at various temperatures and every single model scored 100%.However, despite perfect performance on application of the required intermediate function, CoT still does not lead to robust generalization to arbitrary length problems.Therefore, at least on this problem set, the issue isn't due to accumulating error.The problem must be with the LLM's inability to learn the correct algorithm from contextual demonstrations, rather than with its inability to execute that algorithm.</p>
<p>Overall, we see that our results on planning are not a fluke.These three synthetic domains showcase similar generalization failures, but these failures only become clear when the problems tested on require sufficiently many reasoning steps or when the minor modifications of the domain are studied.This illustrates the need for testing on benchmarks which can generate arbitrary new instances of increasing difficulty.Without such testing, conclusions drawn from static test sets of limited size are unlikely to be robust.We implore the community at large to adopt more rigorous evaluation mechanisms, especially when making claims about the poorly-understood yet much-hyped algorithmic reasoning abilities of black box models.</p>
<p>Conclusion</p>
<p>In this paper, we conducted a systematic evaluation of the effectiveness of chain of thought in large language models on a specific classical planning problem.Our case study indicates that, contrary to previous claims in the literature, providing examples of procedural reasoning does not induce the general ability to apply that procedure to novel instances in current state-of-the-art large language models.In fact, the performance improvements seen when prompting LLMs in this manner quickly vanish when queries differ in generality from the examples, despite the fact that the same algorithmic procedure applies to the larger or more general instance.Chain of Thought (CoT) has become one of the most widely adopted ideas for improving planning and reasoning abilities of LLMs.Almost every system routinely, and uncritically, uses some prompting strategy attributed to CoT.On the flip side, whenever LLMs are shown to have limitations in any sphere, practitioners tend to question those studies by attributing it to unskilled use of CoT methodology.Our study, based on both in planning and other more standard tasks, calls into question the prevalent belief that LLMs are capable of operationalizing and generalizing the CoT advice effectively.It instead suggests that CoT is effective only when the LLM can do straightforward pattern matching between the example and the problem.We believe that the lessons of this study will be helpful in mitigating the applications of LLMs to tasks requring planning and reasoning with false confidence.</p>
<p>A.2 Self Consistency on Table to Stack problems</p>
<p>We evaluated self consistency [49], a state-of-the-art extension of CoT, on table to stack problems.We sampled 5 different reasoning paths (with temperature 0.7) and chose the most frequent plan breaking ties randomly.As our results show (in Table 3 and Figure A.2.1), self-consistency does not lead to a generalization breakthrough, and in fact is generally worse than the original results in Table 1.This is likely because the solution space for planning problems is much larger than that studied in previous (often multiple choice) benchmarks.In fact, most queries led to five unique responses, forcing us to choose the final answer from them at random.</p>
<p>A.3 Further details on modifications to the CoinFlip domain</p>
<p>Given a list of names, generating new instances is just a matter of filling in a template.We source our list of names from the U.S. Social Security Administration [1], and only keep names with at least 50 occurrences.We scale our problems with the number of names (potentially repeated) mentioned in the prompt.The main test set consists of 1120 instances, with 40 instances per number of names and 28 different numbers of names, ranging from 1 to 28.We also tested [50]'s prompt on an extended</p>
<p>A.4 Further details on modifications to the LastLetterConcatenation domain</p>
<p>We use the same database as in CoinFlip to generate words.[1] To scale instances, we simply increase the number of words whose last letters must be concatenated.Our problems range from 1 to 20 words, with 40 instances per word, giving a total of 800 problems.</p>
<p>A.5 Further details on the multi-step Arithmetic dataset</p>
<p>The number of reasoning steps in this domain corresponds directly to the number of operations that need to be performed to simplify a given expression.Our test set consists of 1160 total problems, spread 1 to 29 operations, with 40 instances per number of operations.Again mirroring  2. I look at the goal and stack each tower I need, starting from the bottom.I check each of the mentioned blocks in order to determine which of them are the bases of towers.</p>
<p>→</p>
<p>Block C needs to be on top of Block D, so it can't be the base of a tower.Block A needs to be on top of Block C, so it can't be the base of a tower.My plan is as follows:</p>
<p>[THOUGHTS] 1.I look at the goal and stack each tower I need, starting from the bottom.My goal only mentions blocks B and A. I check each of the mentioned blocks in order to determine which of them are the bases of towers.</p>
<p>→</p>
<p>Block A needs to have Block B on top of it.I double check the goal: this is the only condition that must hold for block A. It does not need to be on top of any other block.Therefore, I can use it as the base of a tower.</p>
<p>→ →</p>
<p>Block B needs to be on top of Block A, so it can't be the base of a tower.</p>
<p>In summary, only Block A can be used as a base of a tower.</p>
<p>I build the tower that has Block A as a base.I don't need to move Block A, and I leave it be, as I know it is already placed correctly.</p>
<p>→</p>
<p>The block that goes on top of Block A is Block B. I pick up Block B. I stack it on top of Block A. This satisfies the part of the goal that says that Block B is on top of Block A.</p>
<p>No block needs to be on top of Block B. Therefore this tower is complete.</p>
<p>There are no other blocks which can be used as a base of a tower, and all the blocks mentioned in the goal have been placed, so I am done constructing.My plan is as follows:</p>
<p>[THOUGHTS] 1.I look at the goal and stack each tower I need, starting from the bottom.My goal only mentions blocks A, B, and C. I am free to ignore all other blocks.I check each of the mentioned blocks in order to determine which of them are the bases of towers.</p>
<p>→</p>
<p>Block C needs Block A on top of it.I double check the goal: this is the only condition that must hold for block C. It does not need to be on top of any other block.Therefore, I can use it as the base of a tower.</p>
<p>→ →</p>
<p>Block A needs Block B on top of it.I double check the goal: this isn't the only condition that must hold for Block A, because Block A needs to be on top of Block C. Therefore Block A can't be the base of a tower.</p>
<p>→ →</p>
<p>Block B needs to be on top of Block A, so it can't be the base of a tower.</p>
<p>In summary, only Block C can be used as the base of a tower.</p>
<p>I build the tower that has Block C as a base.I don't need to move Block C, and I leave it be, as I know it is already placed correctly.</p>
<p>→</p>
<p>The</p>
<p>→</p>
<p>The following 3 examples are provided.Please follow the formatting used in them.</p>
<p>Example 0:</p>
<p>[QUESTION] A coin is heads up.Craig flips the coin.Is the coin still heads up?</p>
<p>[Thoughts] The coin was flipped by Craig.So the coin was flipped 1 times.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.</p>
<p>→ [Answer] yes</p>
<p>Example 1:</p>
<p>[QUESTION] A coin is heads up.Major flips the coin.Maria does not flip the coin.Is the coin still heads up?</p>
<p>→</p>
<p>[Thoughts] The coin was flipped by Major.So the coin was flipped 1 times.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.The coin was flipped by Wolf and Page.So the coin was flipped 2 times.The coin started heads up, so after an even number of flips, it will still be heads up.So the answer is yes.</p>
<p>→ [Answer] no</p>
<p>Problem to solve:</p>
<p>[QUESTION] A coin is heads up.Craig flips the coin.Is the coin still heads up?</p>
<p>[Thoughts] ==================GPT-4-Turbo Response==================== The coin was flipped by Craig.So the coin was flipped 1 time.The coin started heads up, so after an odd number of flips, it will be tails up.So the answer is no.[Question] Take the last letters of each word in "Analy Daniel" and concatenate them.</p>
<p>[Thoughts] ==================GPT-4-Turbo Response==================== [Answer] yy</p>
<p>CoT LastLetterConcatenation Prompt and GPT-4 Response Example 0:</p>
<p>[Question] Take the last letters of each word in "Anthony" and concatenate them.</p>
<p>[Thoughts] The last letter of "Anthony" is y.Concatenating them is "y".The answer is y.</p>
<p>[Answer] y Example 1:</p>
<p>[Question] Take the last letters of each word in "Beautiful Ash" and concatenate them.</p>
<p>[Thoughts] The last letter of "Beautiful" is l.The last letter of "Ash" is h.Concatenating them is "lh".</p>
<p>The answer is lh.</p>
<p>→ [Answer] lh</p>
<p>Example 2:</p>
<p>[Question] Take the last letters of each word in "Matthew River Craig" and concatenate them.</p>
<p>[Thoughts]</p>
<p>The last letter of "Matthew" is w.The last letter of "River" is r.The last letter of "Craig" is g.Concatenating them is "wrg".The answer is wrg.</p>
<p>→ [Answer] wrg</p>
<p>Problem to solve:</p>
<p>[Question] Take the last letters of each word in "Analy Daniel" and concatenate them.</p>
<p>[Thoughts] ==================GPT-4-Turbo Response==================== The last letter of "Analy" is y.The last letter of "Daniel" is l.Concatenating them is "yl".The answer is yl.</p>
<p>→ [Answer] yl</p>
<p>LastVowelConcatenation Prompt and GPT-4 Response</p>
<p>[Instructions]</p>
<p>For the purposes of these problems, a vowel is any one of the letters "a","e","i","o", or "u", but NOT "y".After the [Answer] tag, you may only respond with a lowercase string of concatenated characters.Do not include anything else after that tag.The [Answer] tag must precede the final answer.</p>
<p>→ → →</p>
<p>The following 3 examples are provided.Please follow the formatting used in them.</p>
<p>Example 0:</p>
<p>[Question] Take the last vowel of each word in "Anthony" and concatenate them.</p>
<p>[Answer] o Example 1:</p>
<p>[Question] Take the last vowel of each word in "Beautiful Ash" and concatenate them.</p>
<p>[Answer] ua Example 2:</p>
<p>[Question] Take the last vowel of each word in "Matthew River Craig" and concatenate them.</p>
<p>[Answer] eei Problem to solve:</p>
<p>[Question] Take the last vowel of each word in "Analy Daniel" and concatenate them.</p>
<p>[Answer]</p>
<p>==================GPT-4-Turbo Response==================== ya</p>
<p>FoomLetterConcatenation Prompt and GPT-4 Response</p>
<p>[Instructions] After the [Answer] tag, you may only respond with a lowercase string of concatenated characters.Do not include anything else after that tag.The [Answer] tag must precede the final answer.</p>
<p>→</p>
<p>The following 2 examples are provided.Please follow the formatting used in them.</p>
<p>Example 0:</p>
<p>[Question] Take the following words and produce a new string using them: "Anthony".The first letter of your string should be the first letter of the first word, the second letter of your string should be the second letter of the second word, and so forth.If you need the nth letter of the nth word, but that word is less than n letters long, then insert a 0 character instead.</p>
<p>→ → →</p>
<p>[Thoughts] The 1st letter of "Anthony" is A. Concatenating them is "a".The answer is a.</p>
<p>[Answer] a Example 1:</p>
<p>[Question] Take the following words and produce a new string using them: "Beautiful Ash".The first letter of your string should be the first letter of the first word, the second letter of your string should be the second letter of the second word, and so forth.If you need the nth letter of the nth word, but that word is less than n letters long, then insert a 0 character instead.</p>
<p>→ → →</p>
<p>[Thoughts] The 1st letter of "Beautiful" is B. The 2nd letter of "Ash" is s.Concatenating them is "bs".The answer is bs.</p>
<p>→ [Answer] bs</p>
<p>Problem to solve:</p>
<p>[Question] Take the following words and produce a new string using them: "Analy Daniel".The first letter of your string should be the first letter of the first word, the second letter of your string should be the second letter of the second word, and so forth.If you need the nth letter of the nth word, but that word is less than n letters long, then insert a 0 character instead.</p>
<p>→ → →</p>
<p>[Thoughts]</p>
<p>==================GPT-4-Turbo Response==================== The 1st letter of "Analy" is A. The 2nd letter of "Daniel" is a.The 3rd letter of "Analy" is a.The 4th letter of "Daniel" is i.The 5th letter of "Analy" is y.The 6th letter of "Daniel" is e.Concatenating them is "Aaaiye".The answer is aaaiye.The following 3 examples are provided.Please follow the formatting used in them.</p>
<p>Example 0:</p>
<p>[Question] Simplify the following expression into a single number: 2 + (4)</p>
<p>[Thoughts]</p>
<p>We simplify one set of parentheses at a time, starting from the inside.</p>
<p>[Thought 1] The current form of the expression is 2 + (4).The innermost expression is 2 + (4), which simplifies to 6.The expression cannot be simplified further, so this will also be the final answer.</p>
<p>[Intermediate Answer 1] 6</p>
<p>[Answer] 6</p>
<p>Example 1:</p>
<p>[Question] Simplify the following expression into a single number: 6 / (7 -( 5))</p>
<p>[Thoughts]</p>
<p>We simplify one set of parentheses at a time, starting from the inside.</p>
<p>[Thought 1] The current form of the expression is 6 / (7 -( 5)).The innermost expression is 7 -(5), which simplifies to 2.
[Intermediate Answer 1] 2 [Thought 2]
We plug in the previous intermediate answer into the previous expression to simplify it by one step.This gives the expression 6 / (2).</p>
<p>→</p>
<p>The innermost expression is 6 / (2), which simplifies to 3. The expression cannot be simplified further, so this will also be the final answer.</p>
<p>[Intermediate Answer 2] 3</p>
<p>[Answer] 3</p>
<p>Example 2:</p>
<p>[Question] Simplify the following expression into a single number: 2 / (4 -(9 -(6)))</p>
<p>[Thoughts]</p>
<p>We simplify one set of parentheses at a time, starting from the inside.</p>
<p>[Thought 1] The current form of the expression is 2 / (4 -(9 -( 6))).The innermost expression is 9 -( 6), which simplifies to 3.</p>
<p>[Intermediate Answer 1] 3 [Thought 2]</p>
<p>[Answer] 6</p>
<p>Example 1:</p>
<p>[Question] Simplify the following expression into a single number: 6 / (7 -( 5)) Remember: the final answer and all intermediate answers will be single digit numbers.</p>
<p>[Answer] 3</p>
<p>Example 2:</p>
<p>[Question] Simplify the following expression into a single number: 2 / (4 -(9 -( 6))) Remember: the final answer and all intermediate answers will be single digit numbers.</p>
<p>[Answer] 2</p>
<p>Problem to solve:</p>
<p>[Question] Simplify the following expression into a single number: 1 * (7 / (5 / (1 + (9 -( 5))))) Remember: the final answer and all intermediate answers will be single digit numbers.</p>
<p>[Answer]</p>
<p>==================GPT-4-Turbo Response==================== 7</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).10.Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: NA Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: NA Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: NA Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Figure 1 :
1
Figure 1: Target Distributions of Problems.This figure shows the levels of expected generality for each prompt.</p>
<p>Figure 2 :
2
Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.</p>
<p>Figure 3 :
3
Figure 3: Accuracy of GPT-4-Turbo with chain of thought prompting across variations of our synthetic datasets."Direct" means direct prompting without any CoT.</p>
<p>Figure A. 1
1
Figure A.1.1:(Table-to-stack) Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods with increasing number of blocks.</p>
<p>Figure A. 2
2
Figure A.2.1: (Table-to-stack) Accuracy of GPT-4-Turbo and Claude-3-Opus across chain of thought prompting methods with self consistency.</p>
<p>→</p>
<p>heads up.Albert does not flip the coin.Wolf flips the coin.Page flips the coin.Is the coin still heads up? → [Thoughts]</p>
<p>the [Answer] tag, you may only respond with a lowercase string of concatenated characters.Do not include anything else after that tag.The [Answer] tag must precede the final answer.→ Problem to solve:</p>
<p>the [Answer] tag, you may only respond with a lowercase string of concatenated characters.Do not include anything else after that tag.The [Answer] tag must precede the final answer.→Thefollowing 3 examples are provided.Please follow the formatting used in them.</p>
<p>Table 1 :
1
2. Accuracy across CoT and example granularities over 261 instances in table-to-stack Blocksworld.
PromptGPT-4-Turbo Claude-3-Opus GPT-4zero-shot19.1%9.96%3.83%zero-shot CoT21%10.34%4.98%Domain-Specific n-shot13.7%16.4%6.13%Progression Proof CoT15.3%4.59%6.89%Domain-Specific n-shot13.7%16.4%6.13%Blocksworld Universal Algorithm 37.1%37.1%51.3%Problem Class Specific n-shot18%15.7%8.81%Stacking Prompt40.6%24.5%59.3%</p>
<p>Table 2 :
2PromptCFLLCLVCFLCArithmetic AEZero-Shot56.38% 10.00% 5.75%1.81%24.13%45.60%Zero-Shot CoT 95.71% 52.54% N/AN/A56.12%42.76%Manual CoT98.89% 51.06% 27.00% 26.00% 50.43%69.31%Incorrect Cot96.76% 48.15% N/AN/AN/AN/A
Accuracy across CoT types and problem variations over all instances in our synthetic datasets.CF is CoinFlip, LLC is LastLetterConcatenation, LVC is LastVowelConcatenation, FLC is FoomLetterConcatenation, Arithmetic is baseline single-digit Arithmetic, AE is the same problems but with the explanation provided that all intermediate answers are single digit.</p>
<p>Very specific prompts are more likely to work, but they can require significantly more human labor to craft.Our results indicate that chain of thought prompts may only work consistently within a problem class if the problem class is narrow enough and the examples given are specific to that class.Both of these facts show that chain of thought approaches provide less generalization than previous claims seem to indicate, and hint that basic pattern matching rather than in context learning of general algorithmic procedures may better explain the improvements seen from chain of thought.Broader Impacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13 A.2 Self Consistency on Table to Stack problems . . . . . . . . . . . . . . . . . . . . .13 A.3 Further details on modifications to the CoinFlip domain . . . . . . . . . . . . . . .13 A.4 Further details on modifications to the LastLetterConcatenation domain . . . . . .14 A.5 Further details on the multi-step Arithmetic dataset . . . . . . . . . . . . . . . . .14 A.6 Planning Prompts and Responses by GPT-4 . . . . . . . . . . . . . . . . . . . . .15 A.6.1 Domain Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15 A.6.2 Progression Proof Prompt . . . . . . . . . . . . . . . . . . . . . . . . . .16 A.6.3 Blocksworld Universal Algorithm Prompt . . . . . . . . . . . . . . . . . .18
A AppendixContentsA Appendix13A.1
A.6.4 Stacking Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21 A.6.5 Lexicographic Stacking Prompt . . . . . . . . . . . . . . . . . . . . . . .22 A.7 Coinflip Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24 A.8 LastLetterConcatenation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A.9 Single Digit Arithmetic Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . .27 A.1 Broader Impacts</p>
<p>Table 3 :
3
Accuracy of Self-consistency over 261 instances in table-to-stack Blocksworld.set of 2960 instances, with 40 instances per number of names, but only stopping at 75 names, finding that performance did begin to decrease more significantly past 30 names.
zero-shot18.3%9.19%zero-shot CoT18.3%11.8%Problem Class Specific n-shot 15.7%14.9%Stacking Prompt24.5%26.4%
Table-to-stack) Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods with increasing number of blocks.PromptGPT-4-Turbo Claude-3-Opus</p>
<p>→ →
→
pick up or unstack one block at a time.I can only pick up or unstack a block if my hand is empty.I can only pick up a block if the block is on the table and the block is clear.A block is clear if the block has no other blocks on top of it and if the block is not picked up.unstack a block from on top of another block if the block I am unstacking is clear.Once I pick up or unstack a block, I am holding the block.I can only put down a block that I am holding.I can only stack a block on top of another block if I am holding the block being stacked.I can only stack a block on top of another block if the block onto which I am stacking the block is clear..6.2 Progression Proof Prompt Progression Proof Chain of Thought Prompting and GPT-4 Response As initial conditions I have that, Block A is clear, Block B is clear, Block C is clear, the hand is empty, Block B is on top of Block D, Block A is on the table, Block D is on the table and Block C is on the table.Current State: Block A is clear, Block B is clear, Block C is clear, the hand is empty, Block B is on top of Block D, Block A is on the table, Block C is on the table and Block D is on the table Action: unstack Block B from on top of Block D Reason: The above action is applicable in the current state because its preconditions; Block B is clear, the hand is empty and Block B is on top of Block D, are satisfied in the current state.Resulting State: Block A is clear, Block C is clear, Block D is clear, the hand is currently holding Block B, Block A is on the table, Block C is on the table and Block D is on the table
Here is an example problem:[STATEMENT]→→My goal is to have that Block D is on top of Block B.My plan is as follows:[PLAN]1. →→A.6 Planning Prompts and Responses by GPT-4A.6.1 Domain InformationDomain InformationI am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the→actions I can doPick up a blockUnstack a block from on top of another blockPut down a blockStack a block on top of another blockI have the following restrictions on my actions:I can only →I can only unstack a block from on top of another block if the block I am unstacking was really ontop of the other block.
Table-to-stack) Accuracy of GPT-4-Turbo and Claude-3-Opus across chain of thought prompting methods with self consistency.our Blocksworld test cases, we experiment with two variants: prompting as if these were any expression simplification problems, and prompting with the explicit additional information that every intermediate step must be a single digit number.→ I can only → Once I put down or stack a block, my hand becomes empty.Once you stack a block on top of a second block, the second block is no longer clear.A→ → 2. Current State: Block A is clear, Block C is clear, Block D is clear, the hand is currently holding Block B, Block A is on the table, Block C is on the table and Block D is on the table → Action: put down Block B Reason: The above action is applicable in the current state because its preconditions; the hand is currently holding Block B, are satisfied in the current state.→ Resulting State: Block A is clear, Block B is clear, Block C is clear, Block D is clear, the hand is empty, Block A is on the table, Block B is on the table, Block C is on the table and Block D is on the table → → 3. Current State: Block A is clear, Block B is clear, Block C is clear, Block D is clear, the hand is empty, Block A is on the table, Block B is on the table, Block C is on the table and Block D is on the table → → Action: pick up Block D Reason: The above action is applicable in the current state because its preconditions; Block D is clear, the hand is empty and Block D is on the table, are satisfied in the current state.→ → Resulting State: Block A is clear, Block B is clear, Block C is clear, the hand is currently holding Block D, Block A is on the table, Block B is on the table and Block C is on the table → → 4. Current State: Block A is clear, Block B is clear, Block C is clear, the hand is currently holding Block D, Block A is on the table, Block B is on the table and Block C is on the table → Action: stack Block D on top of Block B Reason: The above action is applicable in the current state because its preconditions; Block B is clear and the hand is currently holding Block D, are satisfied in the current state.→ Resulting State: Block A is clear, Block C is clear, Block D is clear, the hand is empty, Block D is on top of Block B, Block A is on the table, Block B is on the table and Block C is on the table → → Final State: Block A is clear, Block C is clear, Block D is clear, the hand is empty, Block D is on top of Block B, Block A is on the table, Block B is on the table and Block C is on the</p>
<p>table → → → → → → → →
→→→→
The goal conditions are satisfied in the final state.Hence, the above plan is valid.My goal is to have that Block C is on top of Block B and Block D is on top of Block C. Resulting State: the Block C is clear, the Block D is clear, the hand is currently holding Block B, the Block C is on top of the Block A, the Block A is on the table and the Block D is on the table → → 2. Current State: the Block C is clear, the Block D is clear, the hand is currently holding Block B, the Block C is on top of the Block A, the Block A is on the table and the Block D is on the table Resulting State: the Block B is clear, the Block C is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block A, the Block A is on the table, the Block B is on the table and the Block D is on the table → → 3. Current State: the Block B is clear, the Block C is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block A, the Block A is on the table, the Block B is on the table and the Block D is on the table Action: unstack the Block C from on top of the Block A Reason: The above action is applicable in the current state because its preconditions; the Block C is clear, the hand is empty and the Block C is on top of the Block A, are satisfied in the current state.Resulting State: the Block A is clear, the Block B is clear, the Block D is clear, the hand is currently holding Block C, the Block A is on the table, the Block B is on the table and the Block D is on the table → → 4. Current State: the Block A is clear, the Block B is clear, the Block D is clear, the hand is currently holding Block C, the Block A is on the table, the Block B is on the table and the Block D is on the table Action: stack the Block C on top of the Block B Reason: The above action is applicable in the current state because its preconditions; the Block B is clear and the hand is currently holding Block C, are satisfied in the current state.Resulting State: the Block A is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block B, the Block D is on top of the Block C, the Block A is on the table and the Block B is on the table Final State: the Block A is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block B, the Block D is on top of the Block C, the Block A is on the table and the Block B is on the table Block A is on the table and Block C is on the table.Action: unstack Block B from on top of Block A Reason: The above action is applicable in the current state because its preconditions; Block B is clear, the hand is empty and Block B is on top of Block A, are satisfied in the current state.Resulting State: Block A is clear, Block C is clear, the hand is currently holding Block B, Block A is on the table and Block C is on the table → 2. Current State: Block A is clear, Block C is clear, the hand is currently holding Block B, Block A is on the table and Block C is on the table Block A is clear, Block B is clear, Block C is clear, the hand is empty, Block A is on the table, Block B is on the table and Block C is on the table → 3. Current State: Block A is clear, Block B is clear, Block C is clear, the hand is empty, Block A is on the table, Block B is on the table and Block C is on the table Action: pick up Block A Reason: The above action is applicable in the current state because its preconditions; Block A is clear, the hand is empty and Block A is on the table, are satisfied in the current state.Resulting State: Block B is clear, Block C is clear, the hand is currently holding Block A, Block B is on the table and Block C is on the table Block A is already on the table but is not clear.Block B is already on the table.Block C is on top of Block A. I can only unstack a block that is clear.C is clear according to the initial condition, so I can proceed.I unstack it.Then, I put it down on the table.This clears Block A. All the blocks are now clear and on the table.2. I look at the goal and stack each tower I need, starting from the bottom.I check each of the mentioned blocks in order to determine which of them are the bases of towers.Block A needs to be on top of Block C, so it can't be the base of a tower.Block B needs to be on top of Block A, so it can't be the base of a tower.Block C needs Block A on top of it.I double check the goal: this is the only condition that must hold for block C. It does not need to be on top of any other block.Therefore, I can use it as the base of a tower.In summary, only Block C can be used as a base of a tower.I build the tower that has Block C as a base.I don't need to move Block C, and I leave it be, as I know it is already placed correctly.The block that goes on top of Block C is Block A. I pick up Block A. I stack it on top of Block C. The block that goes on top of Block A is Block B. I pick up Block B. I stack it on top of Block A. No block needs to be on top of Block B. Therefore this tower is complete.There are no other blocks which can be used as a base of a tower, and all the blocks mentioned in the goal have been placed, so I am done constructing.As initial conditions I have that, Block B is clear, the hand is empty, Block A is on top of Block D, Block B is on top of Block C, Block C is on top of Block A and Block D is on the table.apart all the towers, and put all the blocks on the table.I will check every block.Any that are already on the table can be left there.Any that are on top of another block must be put on the table, but I can only unstack them if they are clear, so I will check that before trying to unstack any of them.If a block needs to be moved but isn't clear, I'll come back to it later once it is.Block A is on top of Block D. I can only unstack a block that is clear.The initial condition states that Block C is on top of Block A. Therefore, I can't unstack Block A until I move Block C. I don't touch it yet, and look at Block C. Block C is on top of Block A. I can only unstack a block that is clear.The initial condition states that Block B is on top of Block C. Therefore, I can't unstack Block C until I move Block B. I don't touch it yet, and look at Block B. Block B is on top of Block C. I can only unstack a block that is clear.The initial condition states that Block B is clear.I unstack it.Then, I put it down on the table.This clears Block C. Block C is on top of Block A. I can only unstack a block that is clear.Block C is cleared previously.I unstack it.Then, I put it down on the table.This clears Block A. Block A is on top of Block D. I can only unstack a block that is clear.Block A is cleared previously.I unstack it.Then, I put it down on the table.This clears Block D.
→→[PLAN END] Here is another example: [STATEMENT] As initial conditions I have that, Block B is clear, Block B is on top of Block C, Block C is on top of Block A, the hand is empty, Block A is on the table, and Block D is on the table. My plan is as follows: [PLAN] 1. Current State: the Block B is clear, the Block D is clear, the hand is empty, the Block B is on → → Action: put down the Block B Reason: The above action is applicable in the current state because its preconditions; the hand is currently holding Block B, are satisfied in the current state. → → → → The goal conditions are satisfied in the final state. Hence, the above plan is valid. [PLAN END] Here is the problem to solve: [STATEMENT] As initial conditions I have that: Block B is clear, Block C is clear, the hand is empty, Block B is on top of Block A, → My goal is for the following to be true: Block A is on top of Block B and Block C is on top of Block A. → Action: put down Block B Reason: The above action is applicable in the current state because its preconditions; the hand is currently holding Block B, are satisfied in the current state. → → [PLAN END] ==================VALID PLAN==================== A.6.3 Blocksworld Universal Algorithm Prompt Blocksworld Universal Algorithm Chain of Thought Prompting and GPT-4 Response Here is an example problem: [STATEMENT] As initial conditions I have that, Block B is clear, Block C is clear, the hand is empty, Block C is on top of Block A, Block A is on the table, Block B is on the table. → My goal is to have that Block A is on top of Block C and Block B is on top of Block A My plan is as follows: [THOUGHTS] → → → [PLAN] unstack Block C from on top of Block A put down Block C pick up Block A stack Block A on top of Block C pick up Block B stack Block B on top of Block A [PLAN END] Here is another example: [STATEMENT] → → → → → → → → 1. → →
→ top of the Block C, the Block C is on top of the Block A, the Block A is on the table and the Block D is on the table → → Action: unstack the Block B from on top of the Block C Reason: The above action is applicable in the current state because its preconditions; the Block B is clear, the hand is empty and the Block B is on top of the Block C, are satisfied in the current state.→ → → → → Resulting State: the Block A is clear, the Block C is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block B, the Block A is on the table, the Block B is on the table and the Block D is on the table → → 5. Current State: the Block A is clear, the Block C is clear, the Block D is clear, the hand is empty, the Block C is on top of the Block B, the Block A is on the table, the Block B is on the table and the Block D is on the table → → Action: pick up the Block D Reason: The above action is applicable in the current state because its preconditions; the Block D is clear, the hand is empty and the Block D is on the table, are satisfied in the current state.→ → Resulting State: the Block A is clear, the Block C is clear, the hand is currently holding Block D, the Block C is on top of the Block B, the Block A is on the table and the Block B is on the table → → 6.Current State: the Block A is clear, the Block C is clear, the hand is currently holding Block D, the Block C is on top of the Block B, the Block A is on the table and the Block B is on the table → → Action: stack the Block D on top of the Block C Reason: The above action is applicable in the current state because its preconditions; the Block C is clear and the hand is currently holding Block D, are satisfied in the current state.→ → → My plan is as follows: ==================GPT-4 Response==================== [PLAN] 1.Current State: Block B is clear, Block C is clear, the hand is empty, Block B is on top of Block A, Block A is on the table and Block C is on the table → → Resulting State: → → 4. Current State: Block B is clear, Block C is clear, the hand is currently holding Block A, Block B is on the table and Block C is on the table → Action: stack Block A on top of Block B Reason: The above action is applicable in the current state because its preconditions; Block B is clear and the hand is currently holding Block A, are satisfied in the current state.→ Resulting State: Block A is clear, Block C is clear, the hand is empty, Block A is on top of Block B, Block B is on the table and Block C is on the table → 5. Current State: Block A is clear, Block C is clear, the hand is empty, Block A is on top of Block B, Block B is on the table and Block C is on the table → Action: pick up Block C Reason: The above action is applicable in the current state because its preconditions; Block C is clear, the hand is empty and Block C is on the table, are satisfied in the current state.→ → Resulting State: Block A is clear, the hand is currently holding Block C, Block A is on top of Block B and Block B is on the table → 6.Current State: Block A is clear, the hand is currently holding Block C, Block A is on top of Block B and Block B is on the table → Action: stack Block C on top of Block A Reason: The above action is applicable in the current state because its preconditions; Block A is clear and the hand is currently holding Block C, are satisfied in the current state.→ Resulting State: Block C is clear, the hand is empty, Block A is on top of Block B, Block C is on top of Block A and Block B is on the table → Final State: Block C is clear, the hand is empty, Block A is on top of Block B, Block C is on top of Block A and Block B is on the table → The goal conditions are satisfied in the final state.Hence, the above plan is valid.I take apart all the towers, and put all the blocks on the table.I will check every block.Any that are already on the table can be left there.Any that are on top of another block must be put on the table, but I can only unstack them if they are clear, so I will check that before trying to unstack any of them.If a block needs to be moved but isn't clear, I'll come back to it later once it is.→ → → → → → 3. Now, I write down my plan in the required format: → My goal is to have that Block C is on top of Block D, Block A is on top of Block C and Block B is on top of Block A. → My plan is as follows: [THOUGHTS] 1.I take → → All the blocks are now clear and on the table.</p>
<ol>
<li>Now, I write down my plan in the required format: As initial conditions I have that, Block A is clear, Block B is clear, Block A is on the table, Block B is on the table and the hand is empty.
[PLAN]unstack Block B from on top of Block Aput down Block Bpick up Block Astack Block A on top of Block Bpick up Block Cstack Block C on top of Block A[PLAN END]==================VALID PLAN====================A.6.4 Stacking PromptStacking Chain of Thought Prompting and GPT-4 ResponseSTATEMENT]
→My goal is to have that Block B is on top of Block A.</li>
</ol>
<p>As initial conditions I have that, Block A is clear, Block B is clear, Block C is clear, Block A is on the table, Block B is on the table, Block C is on the table and the hand is empty.My goal is to have that Block A is on top of Block C and Block B is on top of Block A.
2. Now, I write down my plan in the required format:[PLAN]pick up Block Bstack Block B on top of Block A[PLAN END]Here is another example:[STATEMENT]
→</p>
<p>block that goes on top of Block C is Block A. I pick up Block A. I stack it on top of Block C.This satisfies the part of the goal that says that Block A is on top of Block C. The block that goes on top of Block A is Block B. I pick up Block B. I stack it on top of Block A. This satisfies the part of the goal that says that Block B is on top of Block A. No block needs to be on top of Block B. Therefore this tower is complete.There are no other blocks which can be used as a base of a tower, and all the blocks mentioned in the goal have been placed, so I am done constructing.As initial conditions I have that: Block A is clear, Block B is clear, Block C is clear, the hand is empty, Block A is on the table, Block B is on the table and Block C is on the table.My goal is for the following to be true: Block A is on top of Block B and Block C is on top of Block A. look at the goal and stack each tower I need, starting from the bottom.My goal only mentions blocks A, B, and C. I am free to ignore all other blocks.I check each of the mentioned blocks in order to determine which of them are the bases of towers.Block B needs Block A on top of it.I double check the goal: this is the only condition that must hold for block B. It does not need to be on top of any other block.Therefore, I can use it as the base of a tower.Block A needs Block C on top of it.I double check the goal: this isn't the only condition that must hold for Block A, because Block A needs to be on top of Block B. Therefore Block A can't be the base of a tower.The block that goes on top of Block B is Block A. I pick up Block A. I stack it on top of Block B. This satisfies the part of the goal that says that Block A is on top of Block B. The block that goes on top of Block A is Block C. I pick up Block C. I stack it on top of Block A. This satisfies the part of the goal that says that Block C is on top of Block A. No block needs to be on top of Block C. Therefore this tower is complete.There are no other blocks which can be used as a base of a tower, and all the blocks mentioned in the goal have been placed, so I am done constructing.Answer] tag, you may respond only with 'yes' or 'no'.Do not include anything else after that tag.The [Answer] tag must precede the final answer.
stack Block A on top of Block B[PLAN END]==================VALID PLAN====================A.7 Coinflip Prompts→ Direct CoinFlip Prompt and GPT-4 Response[Instructions] 2. Now, I write down my plan in the required format: Respond only with 'yes' or 'no'. Do not include anything else in your response.[PLAN] Problem to solve:pick up Block A [QUESTION] stack Block A on top of Block C pick up Block B A coin is heads up. Craig flips the coin. Is the coin still heads up?stack Block B on top of Block A[PLAN END] [Answer] Here is the problem to solve: ==================GPT-4-Turbo Response====================[STATEMENT] no→ CoT CoinFlip Prompt and GPT-4 ResponseMy plan is as follows: [Instructions]After the [==================GPT-4 Response====================[THOUGHTS]1. I →→→→Block C needs to be on top of Block A, so it can't be the base of a tower.In summary, only Block B can be used as the base of a tower.I build the tower that has Block B as a base. I don't need to move Block B, and I leave it be, asI know it is already placed correctly.→2. Now, I write down my plan in the required format:[PLAN]pick up Block Astack Block A on top of Block Bpick up Block Cstack Block C on top of Block A[PLAN END]==================VALID PLAN====================A.6.5 Lexicographic Stacking PromptLexicographic Stacking Chain of Thought Prompting and GPT-4 ResponseHere is an example problem:[STATEMENT]
→ → →</p>
<p>After each thought, provide an intermediate answer in the form of a single number, labeled by the [Intermediate Answer n] tag, where n is replaced with the number of the intermediate answer.Do not put anything other than the intermediate answer number between the intermediate answer tag and the next thought tag (e.g.[Thought n+1]).When you are done thinking and have outputted all the requisite intermediate answers, put the [Answer] tag.After the [Answer] tag, you may only respond with a single number representing the final value of the calculation.Do not include anything else after that tag.The [Answer] tag must precede the final answer.
==================GPT-4-Turbo Response====================[Answer]7Arithmetic CoT Prompt and GPT-4 Response[Instructions]→→→→→→→→[Answer]aaaiyeA.9 Single Digit Arithmetic PromptsArithmetic Direct Prompt and GPT-4 Response[Instructions]After the [Answer] tag, you may only respond with a single number representing the final value of→the calculation. Do not include anything else after that tag. The [Answer] tag must precede→the final answer.Problem to solve:[Question]Simplify the following expression into a single number: 1 * (7 / (5 / (1 + (9 -(5)))))[Answer]</p>
<p>• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: We have used the OpenAI API and the Anthropic API for our experiments.Guidelines: • The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
Answer: [Yes]Justification: NAGuidelines:•
Prompt and response examples for each approach can be found in the Appendix.
We exclude 0, since any number multiplied by zero is zero, and this would quickly lead to zero representing around 50% of correct answers for larger numbers of reasoning steps.
AcknowledgementsThis research is supported in part by ONR grant N0001423-1-2409, and gifts from Qualcomm, J.P. Morgan and Amazon.Block B needs to be on top of Block A, so it can't be the base of a tower.Block D needs Block C on top of it.I double check the goal: this is the only condition that must hold for block D. It does not need to be on top of any other block.Therefore, I can use it as the base of a tower.→ →In summary, only Block D can be used as the base of a tower.I build the tower that has Block D as a base.I don't need to move Block D, and I leave it be, as I know it is already placed correctly.→The→The innermost expression is 2 / (1), which simplifies to 2. The expression cannot be simplified further, so this will also be the final answer.[Intermediate Answer 3] 2[Answer] 2Problem to solve:[Question] Simplify the following expression into a single number:→The innermost expression is 1 * (9), which simplifies to 9. The expression cannot be simplified further, so this will also be the final answer.[Intermediate Answer 5] 9[Answer] 9Arithmetic Explicitly One-Digit Direct Prompt and GPT-4 Response[Instructions]After the [Answer] tag, you may only respond with a single number representing the final value of the calculation.Do not include anything else after that tag.The [Answer] tag must precede the final answer.→ →The following 3 examples are provided.Please follow the formatting used in them.Example 0:[Question] Simplify the following expression into a single number: 2 + (4) Remember: the final answer and all intermediate answers will be single digit numbers.NeurIPS Paper Checklist • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer:[NA]Justification: NA Guidelines:• The answer NA means that the paper does not include theoretical results.• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: NA Guidelines:• The answer NA means that the paper does not include experiments.• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.Open access to data and codeQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
The United States Social Security Administration | SSA -ssa.gov. ssa.gov. </p>
<p>Suriya Marah I Abdin, Varun Gunasekar, Jerry Chandrasekaran, Mert Li, Yuksekgonul, arXiv:2310.15511Rahee Ghosh Peshawaria, Ranjita Naik, and Besmira Nushi. Kitab: Evaluating llms on constraint satisfaction for information retrieval. 2023arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, Advances in Neural Information Processing Systems. 202235</p>
<p>Introducing the next generation of claude. Anthropic, Mar 2024</p>
<p>Llms with chain-of-thought are non-causal reasoners. Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, arXiv:2401.14295Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts. 2024arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chainlm: Empowering large language models with improved chain-of-thought prompting. Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2403.143122024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey on in-context learning. 2022arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Ronan Bhagavatula, Le Bras, Advances in Neural Information Processing Systems. 202436</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Advances in Neural Information Processing Systems. 362024</p>
<p>Efficient tool use with chain-of-abstraction reasoning. Silin Gao, Jane Dwivedi-Yu, Ping Yu, Ellen Xiaoqing, Ramakanth Tan, Olga Pasunuru, Koustuv Golovneva, Asli Sinha, Antoine Celikyilmaz, Tianlu Bosselut, Wang, arXiv:2401.174642024arXiv preprint</p>
<p>Large language models are not abstract reasoners. Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>A closer look at the self-verification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, arXiv:2311.079542023arXiv preprint</p>
<p>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. Richard Howey, Derek Long, Maria Fox, 16th IEEE International Conference on Tools with Artificial Intelligence. IEEE2004</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2023</p>
<p>. IPC. International planning competition. 1998</p>
<p>Self-[in] correct: Llms struggle with refining self-generated responses. Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, Daniel Khashabi, arXiv:2404.042982024arXiv preprint</p>
<p>How interpretable are reasoning explanations from prompting large language models?. Yeo Wei, Jie , Ranjan Satapathy, Siow Goh, Erik Mong, Cambria, arXiv:2402.118632024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies. the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies2016</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998</p>
<p>Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, arXiv:2106.15772A diverse corpus for evaluating and developing english math word problem solvers. 2021arXiv preprint</p>
<p>An introduction to computational geometry. Cambridge tiass. Marvin Minsky, Seymour Papert, 1969104</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Benchmarking gpt-4 on algorithmic problems: A systematic evaluation of prompting strategies. Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti, arXiv:2402.173962024arXiv preprint</p>
<p>Limitations of language models in arithmetic and symbolic induction. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, arXiv:2208.050512022arXiv preprint</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Rylan Schaeffer, Kateryna Pistunova, Samar Khanna, arXiv:2307.10573Sarthak Consul, and Sanmi Koyejo. Invalid logic, equivalent gains: The bizarreness of reasoning in language model prompting. 2023arXiv preprint</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Linear time near-optimal planning in the blocks world. John Slaney, Sylvie Thiébaux, Proceedings of the National Conference on Artificial Intelligence. the National Conference on Artificial Intelligence1996</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, arXiv:2402.081152024arXiv preprint</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202436</p>
<p>On the planning abilities of large language models-a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 362024</p>
<p>Boosting language models reasoning with chain-of-knowledge prompting. Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, Ming Gao, arXiv:2306.064272023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Chi Wang, arXiv:2306.01337An empirical study on challenging math problem solving with gpt-4. 2023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. Richmond Y Jd Zamfirescu-Pereira, Bjoern Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Teaching algorithmic reasoning via in-context learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.090662022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
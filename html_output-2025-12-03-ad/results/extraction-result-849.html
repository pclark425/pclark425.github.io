<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-849 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-849</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-849</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-260682249</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.03688v1.pdf" target="_blank">AgentBench: Evaluating LLMs as Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at \url{https://github.com/THUDM/AgentBench}.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e849.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e849.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Top-tier API-based large language model evaluated in AgentBench; shows strong abilities to act as an agent across many interactive environments though performance varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based large transformer LLM (alignment-trained) used as a baseline/high-performing model in AgentBench evaluations; not modified architecturally in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench (OS, DB, KG, DCG, LTP, HH, WS, WB)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>various (tool use, multi-step reasoning/planning, sequential decision-making, web navigation, embodied household tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>AgentBench per-dataset scores reported in Table 3: OS 36.8, DB 33.7, KG 52.1, DCG 50.0, LTP 17.6, HH 78.0, WS 58.6, WB 22.6 (scaled/weighted AgentBench scores shown in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer LLM (alignment training / RLHF reported in paper for API LLMs); used Chain-of-Thought prompting at inference</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>alignment training (RLHF) and large-scale pretraining mentioned for API LLMs in general (not re-trained in this paper); evaluation uses prompting (1-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper discusses a general gap between static QA proficiency (what LLMs have achieved on traditional benchmarks via alignment training) and interactive/procedural agent tasks; hypothesized causes include action validity (open-ended generation producing invalid actions), limited context length for multi-turn tasks, multi-turn consistency loss, and differences in training data (e.g., code-focused vs multi-turn instruction data).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e849.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e849.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-sourced LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-sourced medium-sized LLMs evaluated (examples: openchat-13b-v3.2, chatglm2, vicuna, llama2-chat, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of medium-sized open-source LLMs (≤30B in this evaluation) that perform competitively on some static/text benchmarks but substantially underperform commercial API models on interactive AgentBench tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>open-sourced LLMs (various: openchat-13b-v3.2, chatglm2, vicuna-13b, llama2-13b-chat, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source transformer models of medium size; different pretraining/fine-tuning recipes across models; evaluated via the same AgentBench prompting/tooling as API models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><=30B (medium-sized models included in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench (all 8 environments)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>various interactive/procedural types (tool use, planning, multi-step reasoning, web navigation, embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Aggregate: open-sourced models average OA (overall AgentBench scaled score) ~0.42 vs API-based models average ~2.24 (paper reports strong disparity); many open models score near-zero on KG, DCG, HH tasks and show single-digit or zero success rates on several interactive tasks (see Table 3 per-model breakdown).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>mostly standard autoregressive transformer architectures; many medium-sized models with smaller context windows (some only 2k tokens); some models have code-focused pretraining variants described separately.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>varied (instruction fine-tuning, some models code-pretrained/fine-tuned); evaluated using prompting (1-shot CoT) in AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper analyzes effects rather than proposing a new training intervention for these models; notes that open-source models' limited compute/training resources and sometimes shorter context lengths likely contribute to poorer interactive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller model size / limited compute budgets, shorter context windows, lacking alignment/training depth (e.g., RLHF scale), and distributional differences between static QA training data and interactive multi-turn procedural tasks leading to degraded action validity, planning and multi-turn consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e849.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e849.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-training impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of code-focused pretraining/fine-tuning on agent abilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper observes that models trained/finetuned on code can improve performance on code-related interactive tasks (OS, DB, KG), but may trade off on other reasoning tasks (e.g., lateral thinking puzzles) and multi-turn aptitude.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>code-trained LLMs (examples evaluated: codegeex2-6b-chat, wizardcoder-15b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models pretrained or heavily fine-tuned on code corpora / code generation tasks; in the paper these are evaluated as-is (no additional architectural modifications) in AgentBench interactive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied (examples: codegeex2-6b-chat is 6B; wizardcoder-15b is 15B as per model names/metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Operating System (OS), Database (DB), Knowledge Graph (KG), and Lateral Thinking Puzzles (LTP)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (bash/SQL execution), multi-step reasoning/planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Qualitative effects reported: codegeex2-6b-chat significantly outperforms chatglm2-6b on OS, DB, KG tasks (paper reports 'significant' improvement qualitatively but does not publish a single aggregated numeric delta for each model pair); however performance on LTP (logical/lateral reasoning) decreased for code-trained model(s); wizardcoder underperformed despite code training (paper hypothesizes training-format causes).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no special architecture beyond code-focused pretraining/fine-tuning; differences in training format (single-turn vs multi-turn examples) are highlighted as important.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pretraining and fine-tuning on large code corpora / code-specialized instruction tuning (varies by model)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Training models with code-heavy data or code-specialized instruction tuning to improve generation of valid executable commands and SQL (i.e., better action syntax/semantics for code-like tools).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Observed qualitative improvement on code-execution related interactive tasks (OS, DB, KG) for some code-trained models (e.g., codegeex2-6b-chat > chatglm2-6b). Tradeoff: loss in lateral reasoning (LTP) and, in some cases, worse multi-turn behavior for models whose training emphasized single-turn code examples (wizardcoder). No exact numeric before/after reported in the paper for these comparisons beyond per-model table entries.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Code-focused training improves syntax/command generation but may bias the model towards single-turn/code-format outputs, reducing multi-turn planning/creative reasoning ability if training mixes are unbalanced or the training format lacks multi-turn interaction examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e849.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e849.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step reasoning traces from LLMs; AgentBench uses primitive 1-shot CoT prompting for all evaluated models to reflect typical public usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought prompting (1-shot CoT in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique: include an example that demonstrates internal reasoning ('Thought: ...') followed by an action; used to coax multi-step reasoning and action-format adherence from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Applied across AgentBench environments (to guide multi-step reasoning and action-format outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / planning / action generation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used as default prompting strategy in the benchmark; no controlled ablation numbers reported in this paper comparing CoT vs non-CoT, but authors state CoT is de facto and that they used 'primitive' CoT to reflect practical usage.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting strategy (1-shot CoT / few-shot style in prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>1-shot CoT examples are provided in task prompts to encourage structured 'Thought' and 'Action' outputs and to help format generation to valid actions for the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Not quantitatively isolated in the paper; used as standard in all evaluations. Authors note more advanced strategies (ensemble, reflection, search) exist but were not used to reflect typical user experience.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e849.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e849.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG tool interface</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph tool API interface (get_relations, get_neighbors, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instead of requiring LLMs to write SPARQL queries, AgentBench provides a set of callable KG-querying tools/APIs to the LLM so it can plan and invoke a sequence of queries; the KG tasks test long-term planning and multi-step tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>KG-tool API interface (as used in AgentBench KG dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A tool-use interface exposing several high-level KG operations (e.g., get_relations(var), get_neighbors(var, relation), get_attributes(var), argmax/argmin, count(var), intersection) that an LLM can call sequentially to retrieve information from Freebase-backed Virtuoso instance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>KBQA over Freebase (KG dataset in AgentBench)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Knowledge Graph (KG) dataset in AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use, multi-step planning, partially-observable sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper reports per-model KG scores (e.g., GPT-4 KG 52.1 in Table 3). The interface is used to measure F1 / Exact Match and 'Executability' of action sequences. Many models struggle to produce fully executable multi-step action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-use API interface (explicit action space of tool calls); partial observability (KG not included in prompt), requiring planning and multi-step tool invocation sequences</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with tool descriptions and a teaching example (1-shot demo) in prompt; no additional training in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool-use interface / prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Expose a set of high-level KG tools to the LLM and provide a 1-shot demonstration; retain questions that require ≥5 tool invocations to enforce long-term planning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Enables an evaluation of long-term planning and tool-call sequencing; despite the tooling, many models (especially open-source ones) have low executability and F1, showing that providing tools alone does not close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Partially-observable environment and the need for long-term planning/tool sequencing make KG tasks particularly challenging; models struggle to plan and produce syntactically/correctly ordered tool invocations over many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e849.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e849.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action-validity, context & multi-turn consistency issues</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action validity, limited context length, and multi-turn consistency (failure modes identified)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper identifies concrete failure modes that lead LLMs to underperform on interactive/procedural tasks relative to static QA: invalid actions, truncated/insufficient context, losing role across turns, and repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Action validity & multi-turn/context failure modes (observed across many LLMs in AgentBench)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed phenomena: models sometimes produce outputs not accepted by environments (invalid syntax/parameters), produce multiple actions in one turn, omit required parameters, generate apologies/refusals despite instructions, or lose context due to token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>All AgentBench multi-turn interactive tasks (OS, DB, HH, KG, Web tasks, DCG, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use, sequential decision-making, multi-step reasoning, web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>issues indicate need for architectural improvements: larger context window, persistent multi-turn memory, output-constrained decoders or action validators, and tool-use interfaces with stricter schemas</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural / prompting / tooling (proposed/observed directions)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper suggests (implicitly or explicitly) interventions like expanding model context length, enforcing action-format constraints via prompts/examples, providing checking pipelines and tool interfaces, and improving training data to include multi-turn interactive examples to reduce these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>No quantitative before/after metrics reported for these suggested interventions in this paper; authors report these as actionable axes for future improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Open-ended generation leads to invalid/unparseable actions; limited context windows force truncation of useful history; training data/formats (single-turn vs multi-turn) and alignment objectives can bias models to refuse, apologize, or lose role fidelity over many turns. These collectively explain why static QA ability does not straightforwardly transfer to interactive procedural competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentBench: Evaluating LLMs as Agents', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>InterCode: Standardizing and benchmarking interactive coding with execution feedback <em>(Rating: 2)</em></li>
                <li>Mind2Web: Towards a generalist agent for the web <em>(Rating: 2)</em></li>
                <li>WebShop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-849",
    "paper_id": "paper-260682249",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Top-tier API-based large language model evaluated in AgentBench; shows strong abilities to act as an agent across many interactive environments though performance varies by task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-4",
            "model_description": "API-based large transformer LLM (alignment-trained) used as a baseline/high-performing model in AgentBench evaluations; not modified architecturally in this work.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "AgentBench (OS, DB, KG, DCG, LTP, HH, WS, WB)",
            "interactive_task_type": "various (tool use, multi-step reasoning/planning, sequential decision-making, web navigation, embodied household tasks)",
            "interactive_performance": "AgentBench per-dataset scores reported in Table 3: OS 36.8, DB 33.7, KG 52.1, DCG 50.0, LTP 17.6, HH 78.0, WS 58.6, WB 22.6 (scaled/weighted AgentBench scores shown in paper)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "standard transformer LLM (alignment training / RLHF reported in paper for API LLMs); used Chain-of-Thought prompting at inference",
            "training_method": "alignment training (RLHF) and large-scale pretraining mentioned for API LLMs in general (not re-trained in this paper); evaluation uses prompting (1-shot CoT)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper discusses a general gap between static QA proficiency (what LLMs have achieved on traditional benchmarks via alignment training) and interactive/procedural agent tasks; hypothesized causes include action validity (open-ended generation producing invalid actions), limited context length for multi-turn tasks, multi-turn consistency loss, and differences in training data (e.g., code-focused vs multi-turn instruction data).",
            "uuid": "e849.0",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Open-sourced LLMs (aggregate)",
            "name_full": "Open-sourced medium-sized LLMs evaluated (examples: openchat-13b-v3.2, chatglm2, vicuna, llama2-chat, etc.)",
            "brief_description": "Set of medium-sized open-source LLMs (≤30B in this evaluation) that perform competitively on some static/text benchmarks but substantially underperform commercial API models on interactive AgentBench tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "open-sourced LLMs (various: openchat-13b-v3.2, chatglm2, vicuna-13b, llama2-13b-chat, etc.)",
            "model_description": "Instruction-tuned open-source transformer models of medium size; different pretraining/fine-tuning recipes across models; evaluated via the same AgentBench prompting/tooling as API models.",
            "model_size": "&lt;=30B (medium-sized models included in this paper's experiments)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "AgentBench (all 8 environments)",
            "interactive_task_type": "various interactive/procedural types (tool use, planning, multi-step reasoning, web navigation, embodied tasks)",
            "interactive_performance": "Aggregate: open-sourced models average OA (overall AgentBench scaled score) ~0.42 vs API-based models average ~2.24 (paper reports strong disparity); many open models score near-zero on KG, DCG, HH tasks and show single-digit or zero success rates on several interactive tasks (see Table 3 per-model breakdown).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "mostly standard autoregressive transformer architectures; many medium-sized models with smaller context windows (some only 2k tokens); some models have code-focused pretraining variants described separately.",
            "training_method": "varied (instruction fine-tuning, some models code-pretrained/fine-tuned); evaluated using prompting (1-shot CoT) in AgentBench",
            "intervention_type": null,
            "intervention_description": "Paper analyzes effects rather than proposing a new training intervention for these models; notes that open-source models' limited compute/training resources and sometimes shorter context lengths likely contribute to poorer interactive performance.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Smaller model size / limited compute budgets, shorter context windows, lacking alignment/training depth (e.g., RLHF scale), and distributional differences between static QA training data and interactive multi-turn procedural tasks leading to degraded action validity, planning and multi-turn consistency.",
            "uuid": "e849.1",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Code-training impact",
            "name_full": "Effect of code-focused pretraining/fine-tuning on agent abilities",
            "brief_description": "Paper observes that models trained/finetuned on code can improve performance on code-related interactive tasks (OS, DB, KG), but may trade off on other reasoning tasks (e.g., lateral thinking puzzles) and multi-turn aptitude.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "code-trained LLMs (examples evaluated: codegeex2-6b-chat, wizardcoder-15b)",
            "model_description": "Models pretrained or heavily fine-tuned on code corpora / code generation tasks; in the paper these are evaluated as-is (no additional architectural modifications) in AgentBench interactive settings.",
            "model_size": "varied (examples: codegeex2-6b-chat is 6B; wizardcoder-15b is 15B as per model names/metadata)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Operating System (OS), Database (DB), Knowledge Graph (KG), and Lateral Thinking Puzzles (LTP)",
            "interactive_task_type": "tool use (bash/SQL execution), multi-step reasoning/planning",
            "interactive_performance": "Qualitative effects reported: codegeex2-6b-chat significantly outperforms chatglm2-6b on OS, DB, KG tasks (paper reports 'significant' improvement qualitatively but does not publish a single aggregated numeric delta for each model pair); however performance on LTP (logical/lateral reasoning) decreased for code-trained model(s); wizardcoder underperformed despite code training (paper hypothesizes training-format causes).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "no special architecture beyond code-focused pretraining/fine-tuning; differences in training format (single-turn vs multi-turn examples) are highlighted as important.",
            "training_method": "pretraining and fine-tuning on large code corpora / code-specialized instruction tuning (varies by model)",
            "intervention_type": "training method",
            "intervention_description": "Training models with code-heavy data or code-specialized instruction tuning to improve generation of valid executable commands and SQL (i.e., better action syntax/semantics for code-like tools).",
            "intervention_effect": "Observed qualitative improvement on code-execution related interactive tasks (OS, DB, KG) for some code-trained models (e.g., codegeex2-6b-chat &gt; chatglm2-6b). Tradeoff: loss in lateral reasoning (LTP) and, in some cases, worse multi-turn behavior for models whose training emphasized single-turn code examples (wizardcoder). No exact numeric before/after reported in the paper for these comparisons beyond per-model table entries.",
            "hypothesized_cause_of_gap": "Code-focused training improves syntax/command generation but may bias the model towards single-turn/code-format outputs, reducing multi-turn planning/creative reasoning ability if training mixes are unbalanced or the training format lacks multi-turn interaction examples.",
            "uuid": "e849.2",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting strategy that elicits step-by-step reasoning traces from LLMs; AgentBench uses primitive 1-shot CoT prompting for all evaluated models to reflect typical public usage.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chain-of-Thought prompting (1-shot CoT in evaluation)",
            "model_description": "Prompting technique: include an example that demonstrates internal reasoning ('Thought: ...') followed by an action; used to coax multi-step reasoning and action-format adherence from LLMs.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Applied across AgentBench environments (to guide multi-step reasoning and action-format outputs)",
            "interactive_task_type": "multi-step reasoning / planning / action generation",
            "interactive_performance": "Used as default prompting strategy in the benchmark; no controlled ablation numbers reported in this paper comparing CoT vs non-CoT, but authors state CoT is de facto and that they used 'primitive' CoT to reflect practical usage.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "prompting strategy (1-shot CoT / few-shot style in prompts)",
            "intervention_type": "prompting strategy",
            "intervention_description": "1-shot CoT examples are provided in task prompts to encourage structured 'Thought' and 'Action' outputs and to help format generation to valid actions for the environment.",
            "intervention_effect": "Not quantitatively isolated in the paper; used as standard in all evaluations. Authors note more advanced strategies (ensemble, reflection, search) exist but were not used to reflect typical user experience.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e849.3",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "KG tool interface",
            "name_full": "Knowledge-Graph tool API interface (get_relations, get_neighbors, etc.)",
            "brief_description": "Instead of requiring LLMs to write SPARQL queries, AgentBench provides a set of callable KG-querying tools/APIs to the LLM so it can plan and invoke a sequence of queries; the KG tasks test long-term planning and multi-step tool use.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "KG-tool API interface (as used in AgentBench KG dataset)",
            "model_description": "A tool-use interface exposing several high-level KG operations (e.g., get_relations(var), get_neighbors(var, relation), get_attributes(var), argmax/argmin, count(var), intersection) that an LLM can call sequentially to retrieve information from Freebase-backed Virtuoso instance.",
            "model_size": null,
            "qa_task_name": "KBQA over Freebase (KG dataset in AgentBench)",
            "qa_performance": null,
            "interactive_task_name": "Knowledge Graph (KG) dataset in AgentBench",
            "interactive_task_type": "tool use, multi-step planning, partially-observable sequential decision-making",
            "interactive_performance": "Paper reports per-model KG scores (e.g., GPT-4 KG 52.1 in Table 3). The interface is used to measure F1 / Exact Match and 'Executability' of action sequences. Many models struggle to produce fully executable multi-step action sequences.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "tool-use API interface (explicit action space of tool calls); partial observability (KG not included in prompt), requiring planning and multi-step tool invocation sequences",
            "training_method": "prompting with tool descriptions and a teaching example (1-shot demo) in prompt; no additional training in this paper",
            "intervention_type": "tool-use interface / prompting",
            "intervention_description": "Expose a set of high-level KG tools to the LLM and provide a 1-shot demonstration; retain questions that require ≥5 tool invocations to enforce long-term planning.",
            "intervention_effect": "Enables an evaluation of long-term planning and tool-call sequencing; despite the tooling, many models (especially open-source ones) have low executability and F1, showing that providing tools alone does not close the gap.",
            "hypothesized_cause_of_gap": "Partially-observable environment and the need for long-term planning/tool sequencing make KG tasks particularly challenging; models struggle to plan and produce syntactically/correctly ordered tool invocations over many steps.",
            "uuid": "e849.4",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Action-validity, context & multi-turn consistency issues",
            "name_full": "Action validity, limited context length, and multi-turn consistency (failure modes identified)",
            "brief_description": "Paper identifies concrete failure modes that lead LLMs to underperform on interactive/procedural tasks relative to static QA: invalid actions, truncated/insufficient context, losing role across turns, and repetition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Action validity & multi-turn/context failure modes (observed across many LLMs in AgentBench)",
            "model_description": "Observed phenomena: models sometimes produce outputs not accepted by environments (invalid syntax/parameters), produce multiple actions in one turn, omit required parameters, generate apologies/refusals despite instructions, or lose context due to token limits.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "All AgentBench multi-turn interactive tasks (OS, DB, HH, KG, Web tasks, DCG, etc.)",
            "interactive_task_type": "tool use, sequential decision-making, multi-step reasoning, web navigation",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "issues indicate need for architectural improvements: larger context window, persistent multi-turn memory, output-constrained decoders or action validators, and tool-use interfaces with stricter schemas",
            "training_method": null,
            "intervention_type": "architectural / prompting / tooling (proposed/observed directions)",
            "intervention_description": "Paper suggests (implicitly or explicitly) interventions like expanding model context length, enforcing action-format constraints via prompts/examples, providing checking pipelines and tool interfaces, and improving training data to include multi-turn interactive examples to reduce these failures.",
            "intervention_effect": "No quantitative before/after metrics reported for these suggested interventions in this paper; authors report these as actionable axes for future improvement.",
            "hypothesized_cause_of_gap": "Open-ended generation leads to invalid/unparseable actions; limited context windows force truncation of useful history; training data/formats (single-turn vs multi-turn) and alignment objectives can bias models to refuse, apologize, or lose role fidelity over many turns. These collectively explain why static QA ability does not straightforwardly transfer to interactive procedural competence.",
            "uuid": "e849.5",
            "source_info": {
                "paper_title": "AgentBench: Evaluating LLMs as Agents",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "InterCode: Standardizing and benchmarking interactive coding with execution feedback",
            "rating": 2,
            "sanitized_title": "intercode_standardizing_and_benchmarking_interactive_coding_with_execution_feedback"
        },
        {
            "paper_title": "Mind2Web: Towards a generalist agent for the web",
            "rating": 2,
            "sanitized_title": "mind2web_towards_a_generalist_agent_for_the_web"
        },
        {
            "paper_title": "WebShop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 2,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.021274,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AgentBench: Evaluating LLMs as Agents
7 Aug 2023</p>
<p>Xiao Liu 
Tsinghua University</p>
<p>Hao Yu 
Tsinghua University</p>
<p>Hanchen Zhang 
Tsinghua University</p>
<p>Yifan Xu 
Tsinghua University</p>
<p>Xuanyu Lei 
Tsinghua University</p>
<p>Hanyu Lai 
Tsinghua University</p>
<p>Yu Gu 
The Ohio State University</p>
<p>Hangliang Ding 
Tsinghua University</p>
<p>Kaiwen Men 
Tsinghua University</p>
<p>Kejuan Yang 
Tsinghua University</p>
<p>Shudan Zhang 
Tsinghua University</p>
<p>Xiang Deng 
The Ohio State University</p>
<p>Aohan Zeng 
Tsinghua University</p>
<p>Zhengxiao Du 
Tsinghua University</p>
<p>Chenhui Zhang 
Tsinghua University</p>
<p>Sheng Shen 
Berkeley</p>
<p>Tianjun Zhang 
Berkeley</p>
<p>Yu Su 
The Ohio State University</p>
<p>Huan Sun 
The Ohio State University</p>
<p>Minlie Huang 
Tsinghua University</p>
<p>Yuxiao Dong 
Tsinghua University</p>
<p>Jie Tang 
Tsinghua University</p>
<p>AgentBench: Evaluating LLMs as Agents
7 Aug 2023249D41297B94F5E508BB2657EFD0CFFFarXiv:2308.03688v1[cs.AI]
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments.We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting.Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors.It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation.Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.(a) Typical LLMs' AgentBench performance (relative) against the best in each environment.(b) Overall scores of AgentBench across 8 environ -ments.Dashed lines for two LLM types' average.</p>
<p>Figure 1: An overview of LLMs on AgentBench.While LLMs begin to manifest their proficiency in LLM-as-Agent, gaps between models and the distance towards practical usability are significant.</p>
<p>Introduction</p>
<p>Intelligent agents and autonomous entities [67; 49; 92] capable of decision making and action execution in a particular environment have been key concepts of artificial intelligence (AI) historically.</p>
<p>Database (On an Ubuntu bash terminal)</p>
<p>Recursively set all files in the directory to read-only, except those of mine.</p>
<p>(Given Freebase APIs) What musical instruments do Minnesotaborn Nobel Prize winners play?</p>
<p>(On the GUI of Aquawar) This is a two-player battle game, you are a player with four pet fish cards …… A man walked into a restaurant, ordered a bowl of turtle soup, and after finishing it, he committed suicide.Why did he do that?(In the middle of a kitchen in a simulator) Please put a pan on the dinning table.</p>
<p>Figure 2: AgentBench is the first systematic benchmark to evaluate LLM-as-Agent on a wide array of real-world challenges and 8 distinct environments.In total, 25 LLMs are examined in its first edition.</p>
<p>Notwithstanding substantial advancements in deep learning algorithms applied in both computer vision [31; 62] and natural language processing (NLP) [39; 11], their potential for developing effective and efficient agents to assist humans remains largely unexplored.</p>
<p>The advent of Large Language Models (LLMs) [11; 15; 101; 66; 100; 80], such as GPT-4 [56], has brought plenty of new opportunities to this realm.Through extensive alignment training [75; 58; 8; 90; 65], LLMs have not only mastered traditional NLP tasks such as question answering [61], natural language inference [10], and text summarization [52], but also showcased an impressive ability to comprehend human intent and execute instructions.This has spurred the development of various applications like AutoGPT [64], BabyAGI [51], and AgentGPT [1], which employ LLMs for autonomous goal completion, sparking substantial public interest and numerous discussions.LLM agents are also being experimented with in social and game contexts [59; 86; 104].</p>
<p>Despite these advancements, the lack of a systematic and standard benchmark to evaluate LLM-as-Agent presents a critical challenge.Historically, text-based game environments [57; 17; 30; 82] have been employed for language agent evaluation.However, they often suffer from limitations due to their closed, discrete action spaces, and their focus is primarily narrowed on models' commonsense grounding abilities.More recently, attempts on embodied agents [63; 35; 2] have employed complicated multi-modal simulators based on games [41; 22], GUI [69; 81], and indoor scenes [68; 74].However, these simulators, despite their complexity, do not accurately reflect the practical use-cases of LLMs, and their multi-modal nature creates a hurdle for the urgent evaluation of text-only LLMs.Furthermore, most benchmarks for agents focus on single environments, limiting their capability to provide a comprehensive overview of LLMs across diverse application scenarios.</p>
<p>To address these challenges, we introduce AgentBench, a multi-dimensional benchmark designed to evaluate LLM-as-Agent across a spectrum of different environments.AgentBench encompasses 8 distinct environments, 5 of which are created in this paper for the first time, namely operating system (OS), database (DB), knowledge graph (KG [4]), digital card game (DCG), and lateral thinking puzzles (LTP [5]).The remaining 3 are recompiled from published datasets, which are house-holding (Alfworld [71]), web shopping (WebShop [96]), and web browsing (Mind2Web [19]).All datasets are meticulously designed and reformulated to simulate interactive environments where text-only LLMs can operate as autonomous agents.Furthermore, they systematically evaluate an LLM's core abilities, including following instructions [58], coding [12], knowledge acquisition [38; 78], and logical reasoning [73].Consequently, AgentBench serves as an ideal testbed for both LLM and agent evaluation.</p>
<p>In addition, we have conducted a comprehensive evaluation of 25 different LLMs using AgentBench, including both API-based and open-source models.Our results reveal that top-tier models like GPT-4 are capable of handling a wide array of real-world tasks, indicating the potential for developing a potent, continuously learning agent.However, we also note a significant performance gap between these top-tier models and their open-source counterparts.Despite the recent success of open-source LLMs and their competitive scores on several benchmarks [], their performance on the challenging undisclosed api 0613 OpenAI gpt-3.5-turbo[55] undisclosed api 0613 text-davinci-003 [58] undisclosed api text-davinci-002 [58] undisclosed api claude [6] undisclosed api v1.3 Anthropic claude-instant [6] undisclosed api v1.AgentBench tasks lags considerably.This underscores the necessity for additional efforts to enhance the learning abilities of open-source LLMs.</p>
<p>Lastly, there are definitely more scenarios and tasks to cover in LLM-as-agent evaluation.AgentBench is constantly evolving to be more inclusive and comprehensive.It is also part of our ongoing endeavor to establish a more systematic evaluation scheme for LLMs.We are continuously refining the scheme and anticipate its release with enriched AgentBench's next version in the near future.</p>
<p>In summary, the contributions of this work are:</p>
<p>• We introduce the concept of evaluating LLMs as agents, and present AgentBench, a comprehensive benchmark to standardize the evaluation.It defines 8 distinct environments based on real-world scenarios, offering a practical testbed for LLMs' wide array of capabilities.• We perform a thorough evaluation of 25 different LLMs using AgentBench, uncovering a significant performance gap between leading top API-based LLMs and open-source models.It illuminates the state of the art in LLM-as-Agent and highlights areas for improvement.• To facilitate the assessment of LLM-as-Agent, we release an integrated toolkit to handily customize the AgentBench evaluation to any LLMs based on the philosophy of "API &amp; Docker" interaction.This toolkit, along with the associated datasets and environments, are made publicly available for the broader research community.</p>
<p>LLM-as-Agent: Definition and Preliminary</p>
<p>Here we formalize the terms for describing the evaluation for LLMs as agents and necessary preliminary knowledge for using LLMs in the context of agent evaluation.</p>
<p>Definition: Interactive Evaluation of LLM-as-Agent.The interactive evaluation of LLM-as-Agent could be regarded as a Partially Observable Markov Decision Process (S, A, T , R, U, O), which comprises state space S, action space A, transition function T : S × A → S, reward assigning function R, task instruction space U, and observation space O.Here we denote an LLM agent as M. Definition: Action Validity.Compared to previous evaluation of language agents [17; 74], LLMs' open-ended generation allows a substantially more complex and open action space A. However, it also leads to a common problem for LLM agents to generate invalid actions [97].For example, insufficiently-aligned LLMs would not follow complicated instructions [58], while some other overaligned LLMs tend to refuse task instructions [23; 89].In code-related tasks, LLMs are likely to generate codes that cause compiling or run-time errors [12; 24].In AgentBench, we find that the requirement of action validity significantly increases the threshold for LLMs to act as agents.</p>
<p>Preliminary: Chain-of-Thought (CoT) and Other Reasoning Strategies.Since LLM-as-Agent requires LLMs' strong reasoning ability, CoT [91], which has been considered a de facto strategy in related evaluation together with actions [99], is also adopted in AgentBench.Despite many improved strategies proposed later, such as introducing ensemble [87], reflection [70], and search [98], we evaluate LLMs with the most primitive CoT in AgentBench, as it reflects the most practical public user experience without multiple trails and repeated generation.</p>
<p>Composition of AgentBench</p>
<p>In this section, we introduce the datasets and environments that compose the AgentBench.Compared to previous agent evaluation benchmarks [17; 22], AgentBench concentrates on the practical useoriented evaluation of LLMs via Chain-of-Thought (CoT) [91; 99] prompting.Most of these datasets pinpoint promising directions of LLMs' applications with autonomous mission completing.</p>
<p>Operating System (OS)</p>
<p>Introduction.Allowing LLMs to access and manipulate OS in terminal is a fascinating but challenging mission.Despite attempts on translating natural language to Shell commands [45], few prior efforts evaluate on authentic executable environments.In this dataset, we aim to evaluate LLMs in genuine OS' interactive bash environments (i.e., Ubuntu Docker [50]) on human questions with deterministic answers (e.g., number of users with non-/home directories in an OS.) or series of operations for practical goals (e.g., recursively set all directory files to read-only, excluding mine).</p>
<p>Construction Details.Each evaluation sample in OS dataset encompasses following contents:</p>
<p>• Instruction.The description of the problem in natural language that needs LLMs to solve.</p>
<p>• Docker Environment.The starting up docker image (e.g., preset default local-os/default).</p>
<p>• Initialization Script (Optional).The bash scripts that need to be executed independently (docker exec) before the interaction starts (e.g., user configurations, files, system statuses).• Start Script (Optional).The bash scripts executed after shell is created and before interaction.</p>
<p>• Checking Pipeline.The checking method to judge the correctness of LLMs answer or operation.</p>
<p>• Example Script (Optional).The bash scripts that serve as reference solutions.In other words, if executing them in the interaction, results are correct.Only for unit tests that introduced below.</p>
<p>We design two types of tasks in the OS evaluation beyond conventional QA-only evaluation.</p>
<p>• Question Answering (QA): LLMs need to output commands to solve specific questions in OS (e.g., aggregate numbers, view file contents).In this case, they must commit answers finally.• Operation: LLMs need to output commands to do some verifiable operations on the operating system (e.g., change file/user states).In this case, they do not need to commit final answers.</p>
<p>Thanks to the checking pipeline, two types of tasks can be evaluated in a unified solution.</p>
<p>Collecting challenging queries regarding OS could be difficult.In practice, about half of our instructions are created or collected from humans, while the other half are mostly QA problems generated by gpt-4 and strictly filtered by passing the unit tests (i.e., yield correct answers/states).</p>
<p>For human instructions, we first gather 6000 real problems and solutions with bash or shell tag from Stack Overflow 1 .Then we sort them by the score (count of likes).We invite 8 annotators majored in programming to select challenging ones.For each selected problem, they create one or more task instructions and write a detailed problem description, the initialization script, the starting script, and the checking pipeline.Finally, we conduct a cross verification for each evaluation sample to make sure it's correct.For each problem, it takes about 2 hours to do the annotation.</p>
<p>For generated problems, our unit test contains the following parts.1) Initialization Script Correction: we execute the initialization script and remove samples with wrong initialization whose exit code does not equal to 0. 2) Example Code Correction: we execute the example code and the checking pipeline to judge the correctness of the answer.We remove samples with wrong answers.</p>
<p>In the end, we curate 144 high-quality diverse OS evaluation samples accompanied with testing interactive environments and corresponding checking pipelines (i.e., scripts).Agents are prompted with 1-shot CoT to better format their responses (Cf.Appendix A).</p>
<p>Evaluation Setup.For each problem (i.e., instruction), the execution can be divided into 3 parts.</p>
<p>• Initialization.We create a docker container with a specific image, and we run an initialization bash script to set up environments specified by the instruction.• Interaction.We start a new shell in this docker, and run the starting bash script specified by the instruction.Then the LLM to test is fed with a piece of instruction and the problem description.It starts interaction with the shell.In each turn, two actions are provides.One is to run bash script, which allows the model to generate and run a series of commands in the shell.The other is to commit answer, which allows the model to terminate the interaction process.It's notable that the model will be judged that it fail to solve the problem if exceeding round limit (8 by default).• Checking.For each problem, there is a checking pipeline containing a list of scripts
f 1 , f 2 , • • • , f n
, where f k denotes the k-th script piece in the pipeline.For f k , the answer of the model, o 0 , and the output of f t (t &lt; k), o t , will be fed as input arguments into f k , i.e.,
o k = f k (o 0 , o 1 , • • • , o k−1 )
. The result is correct if and only if all the scripts exit with code 0.</p>
<p>Metrics.We measure the Success Rate for LLMs to solve problems in the execution.There are only two final status for each item of the problems, wrong or correct.</p>
<p>Database (DB)</p>
<p>Introduction.Databases are another typical type of systems that real users may wish AIs to automatically interact with.Thus it is of paramount importance to examine LLMs' abilities to operate on real databases via SQL.Prior research has a large emphasis on translation between SQL and natural language, or answering questions given individual small tables.However, in AgentBench we evaluate LLMs on authentic SQL interfaces and databases as is in the real world.</p>
<p>Construction Details.We acquire the source queries and databases via reusing and amalgamating several established datasets: WikiSQL [103], WikiTableQuestions [60], SQA [37], HybridaQA [13], and FeTaQA [53], ensuring the diversity of instructions and data.</p>
<p>To further enrich (and avoid leakage from) the dataset, we employed gpt-3.5-turbo to perform data augmentation.Provided with the header information and original rows of a table, gpt-3.5-turbogenerates ten new rows.Using the name, header information, and some SQL examples, we task gpt-3.5-turbowith generating five additional SQL queries.Each acquired SQL statement is then fed sequentially into gpt-3.5-turbowith instructions to rephrase the sentences without changing their original meanings.The valid entries are filtered and sampled into the final dataset with 1599 entries, categorized into three basic types of DB operations: select, insert, or update.</p>
<p>As a result, each sample in the dataset comprises:</p>
<p>• Instruction.A piece of description delineating the problem and guiding the agent's action.</p>
<p>• Table Info Evaluation Setup.We assess each problem in the dataset through the following procedure:</p>
<p>• Initialization.An initial SQL script is constructed based on the table content, and a MySQL database is initialized in a docker container, which provides a forwarded port for interaction.</p>
<p>• Interaction.An initial prompt guides the agent to provide an executable SQL command along with its reasoning.The agent is provided with the prompt, instruction, and table information description, and it is expected to return a response in given format.We execute the SQL and directly return the result to the agent, continuing this loop until the agent commits its final answer or encounters an error (e.g., reaching the maximum round limit or failing to parse the action).• Checking.For selection-type problems, we compare the agent's answer with the standard text answer, disregarding the order, but expecting an exact match.If the answer is a single number, all equivalent representations are accepted (e.g., 5, "5.0", '+5' are considered identical).For insertion or updating types of problems, we calculate and compare the hash of the table after the agent's operation with the hash of the table after the correct SQL operation.</p>
<p>Metrics.We measure the Success Rate of agents in completing instructions.Overall success rate is the macro average of the rate of three categories.</p>
<p>Knowledge Graph (KG [4])</p>
<p>Introduction.Engaging with a knowledge graph (KG) demands a broad range of skills from an intelligent agent [27].Primarily, it needs to understand natural language, with all its intricacies and subtleties.It should also have the capacity to break down complex tasks into simpler, more manageable components.In addition, the agent requires the ability to formulate plans, strategize and adapt as necessary.Contemporary KGs are usually vast in size (e.g., FREEBASE [9] has over 45M entities and 3B facts), making them too large for an agent to fully perceive.Operating in such environments, which are only partially observable, requires the agent to make decisions with incomplete information and manage inherent uncertainties.This heightens the need for the agent to demonstrate flexibility and adaptability in its decision making.</p>
<p>Given the challenges outlined above, a KG can act as a representative testing ground to assess the decision-making abilities of AI agents in complex real-world environments.</p>
<p>Construction Details.In an effort to gauge the decision-making abilities of LLMs, specifically their proficiency in long-term planning, we have meticulously compiled a dataset sourced from preexisting knowledge base question answering (KBQA) datasets on FREEBASE, including GrailQA [28], ComplexWebQuestions [77], and GraphQuestions [76].</p>
<p>We envisage KBQA as a tool learning setting, thereby outfitting the LLM with an array of KGquerying tools.By leveraging the S-expressions annotated in [29], we can accurately establish the optimal sequence of tool applications corresponding to each question.In order to sustain a high degree of difficulty in the tasks, we have opted to preserve only those questions which necessitate a minimum of five instances of tool invocation.Through this rigorous selection methodology, we have accrued a dataset consisting of 1,663 questions.Each data entry in the dataset has the following fields:</p>
<p>• Input Question.A natural language utterance that involves intricate KG information seeking.</p>
<p>• Topic Entities.A set of topic entities mentioned in the input question.We obviate the need of performing entity linking, allowing the LLM to focus on long-term planning.• Action Sequence.The gold action sequence (i.e., tool invocations) that leads to the target answer.</p>
<p>• Gold Answer.The gold answer to the question, typically characterized by a set of KG entities.</p>
<p>Note that, in contrast to interacting with databases in AgentBench, where the particulars and content of the database are integrated into the input, describing an extensive KG to the LLM is not particularly feasible.This task is characterized by a partially observable environment, which is a critical aspect of its nature.</p>
<p>Evaluation Setup.To support our evaluation, we first host the latest version of FREEBASE using Virtuoso. 2Due to the complexity of SPARQL queries, we decide not to burden the LLM with crafting SPARQL queries by itself.Instead, we implement a series APIs that interface with the Virtuoso backend, allowing the LLM to query the KG more effortlessly.</p>
<p>We use the first 500 tasks from the datest for evaluation.Each task, when successfully executed, should ideally proceed through the following phases.</p>
<p>• Initialization.We prompt the LLM with the concrete task description, including the concrete description of each KG-querying tool that we provide.• Interaction.During this phase, the LLM is expected to invoke different tools to access the KG and accumulate the necessary information to respond accurately to the question.Importantly, the process is entirely autonomous, meaning the LLM determines the workflow entirely by itself.• Final Answer Prediction.During its interaction with the KG, the LLM may generate a list of variables, each one representing a unique set of entities.If the LLM determines that one particular variable should signify the final answer, it will present this variable as its output and conclude the task.</p>
<p>Metrics.We use F1 score as the primary evaluation metric in our study, calculated by comparing the model's predicted answers to the gold standard answers.In addition to F1 score, we also use the Exact Match metric.However, unlike previous studies that measure Exact Match based on the logical form, we assess it based on the exact match between the predicted and gold answer sets.Lastly, we also evaluate the Executability of the action sequences generated by the model.If the model's action sequence produces any set of answers when executed, it scores 1.0 for Executability.</p>
<p>If it fails to produce an answer, it scores 0.</p>
<p>Digital Card Game (DCG)</p>
<p>Introduction.Games, especially those requires strategies and planning, could serve as simulated environments for intelligent agent development.Nevertheless, prior text-based games (e.g., TextWorld [17]) are largely focused on commonsense grounding.Some recent efforts adopt realworld games (e.g., MineDojo [22]) as environments but mostly require multi-modal capabilities beyond existing LLMs.</p>
<p>Digital card game (e.g., Hearthstone [34]), instead, is an ideal option for text-only LLM evaluation.It usually involves abundant text descriptions for cards, turn-based competition, and thoughtful playing strategies to win, testing a model's understanding of game rules, operating logic, and abilities to form strategic decisions based on current conditions and past experiences in the game.In this work we adapt a simplified DCG system-Aquawar3 -from the 2021 Tsinghua University Agent Competition (THUAC) hosted by Student Association for Science and Technology in Department of Computer Science and Technology (CST-SAST), for evaluating LLM-as-Agent.</p>
<p>In Aquawar, the agent acts as a player managing a team of fishes with different talents to battle against another team (controlled by our baseline agent) in a turn-based form.</p>
<p>Construction Details.Aquawar has the following rules and configurations:</p>
<p>• Player and Cards.It is a two-player battle game with four pet fishes (i.e., cards) in each team.</p>
<p>• Initial State.Each fish has 400 initial health, 200 attack power, active ability, and passive ability.</p>
<p>• Basic Rule.Players choose a live fish to use its active skill or normal attack on an enemy fish each round.All alive fish's passive ability will automatically trigger when meeting certain conditions.• Assertion Mechanism.The identity of a player's fish is initially hidden.The counter-player can guess one of the player's fish's identities each round.If the counter-player guesses correctly, the player's fish's identity is revealed, and all its fish will get damaged.• Round Process.Within a round of the game, the player for that round will first assert the identity of one opponent's fish that are alive and whose identities have not been revealed.If the assertion is correct, all of the opponent's fish that remain alive get damaged.Subsequently, the player for that round can command one alive fish to execute a normal attack or an active ability.Following this, any fish that meet the condition will unleash its passive ability.• Victory Condition.The victory condition is to have more fish alive at the end of the game.</p>
<p>The original design of Aquawar could be too complicated for existing LLMs.Therefore, we have made the following additional adjustments to cater to LLMs' current performance.</p>
<p>• We limited fish selection to the four types of fish: Spray, Flame, Eel, and Sunfish (See appendix D for details), to reduce game complexity and help models understand the game rules.</p>
<p>• To balance agent engagement and game complexity simultaneously, we designed two stages of game logic.We remove the assertions in the first stage while keeping assertions in the second stage.We test all the models on both the first and second stages separately.• We choose two naive playing strategies (random and greedy search) as the baselines.Evaluation Setup.For each time of the game playing, we evaluate with the following steps:</p>
<p>• Initialization.We initiated the modified game logic environment, an LLM to be tested, and the baseline game agent under the Ubuntu 20.04 environment.• Interaction.We place rule descriptions in the isntruction prompt according to different game stages, and the LLM agent interacts and competes strategically with the baseline within the game logic environment.We give the LLM agent five chances to respond in the correct format.It will be immediately deemed defeated if it fails to output legal actions within the given number of attempts.At the same time, we encourage the model to output its reasoning process in CoT.• Result Calculation.During the Interaction process, we will record the entire game process for battle playback and calculate the game results to obtain the metrics for the task.</p>
<p>Metrics.Our comprehensive evaluation uses metrics that range from basic gameplay elements such as the completion rate (Full Play) and the average number of illegal actions (Try Times) to higher-level strategic indicators like the average number of defeated enemy fish (Takedown), the total damage inflicted (Total DMG), and ultimately the main metric-game-winning rate (Win Rate).</p>
<p>Lateral Thinking Puzzles (LTP [5])</p>
<p>WARNING: Some dataset contents and descriptions might induce psychological discomfort.</p>
<p>Introduction.Lateral thinking puzzles [72], or namely situation puzzles, 海龟汤, is a popular group-playing game around the world.The game usually has a person hosting the puzzle and others guess by asking riddle-related questions.The host can only respond "yes", "no", or "irrelevant".The game is terminated when one of the player recovers the critical plot of the puzzle.Its name derives from the psychological term "lateral thinking" [18], which refers to the ability of deducing facts from unconventional perspectives and facilitating the exploration of new ideas.</p>
<p>In this dataset, we first design an accurate LTP host system based on gpt-3.5-turbofor automatic game playing 4 (Cf.Appendix E).To assess LLMs' lateral reasoning prowess, we have curated a diverse puzzle dataset from web.Our evaluation of LLMs will revolve around measuring their proximity to factual accuracy and the speed at which they uncover these truths.Through this assessment, we aim to gain insights into the depth and agility of LLMs' lateral reasoning abilities.</p>
<p>Construction Details.Each sample is constructed of a pair of story (a riddle, e.g., A man walked into a restaurant, ordered a bowl of turtle soup, and after finishing it, he committed suicide.Why did he do that?) and truth.We categorize samples into four levels of difficulty: easy, medium, hard, and expert.The LTP rules for LLM agent playing are as follows:</p>
<p>• Roles: Roles in LTP evaluation are a host and a solver.The host knows the story and truth, providing the story to the solver, and guiding it to guess out the truth.The solver, played and acted by an LLM, tries to find out the truth by asking questions and synthesizing host's answers.• Solving Steps: There is a maximum round for each game, for example, 25.The solver needs to propose a question in each round based on known facts.The questions should be the ones that can be answered by "Yes", "No", or "Irrelevant".Host reply to the questions with correct answers.To lower the difficulty for LLM agents, sometimes the host will provides some hints in responses when solvers get trapped in wrong directions of reasoning.• Game Termination: When the solver thinks it has guessed out the major part of the truth, it can declare the guessed plot to the host.If it is correct, the host will announce the end of the game.</p>
<p>Evaluation Setup.For each pair of story and truth, we evaluate the models with the following steps:</p>
<p>• Initialization.Setting up the LTP host system via local python package installation or web API.</p>
<p>• Interaction.We set up system prompts for LLMs to build their roles of players.LLMs are tested as solvers within the maximum round for each game, if the LLM does not exceed the max token length.In automatic evaluation, we limit the answer to be mostly "Yes", "No", or "Irrelevant", and extract the answer from gpt-3.5-turbo's responses.LLMs are also asked to summarize their reasoning in automatic evaluation in order to help the termination detection to be more accurate.• Checking.We do the pilot study of each LLM to collect all situations in game process and design the checking plan.For automatic evaluation, we set up some key words for gpt-3.5-turbo to answer and remind the model to consider some flexible situation like synonyms.</p>
<p>Metrics.We evaluate LLMs' Lateral reasoning ability by two self created metrics:</p>
<p>• Single Game Accuracy (SGA): The proportion of rounds in which LLMs approaching the truth in a single game.• Round Efficiency (RE): How fast the model can guess out the truth within the maximum round.</p>
<p>• Query Relevance (QR): Relevance between model's questions and the truth.</p>
<p>• Game Progress (GP): Progress before a game end, which serves as the main metric.We break down the groundtruth into several points and measure how many points are reached by an agent.</p>
<p>3.6 House-Holding (Alfworld [71])</p>
<p>Introduction.House-holding environments, which require strong commonsense grounding, have been well-established for language agent evaluation [17].In AgentBench, we assess the model's capability in accomplishing tasks in physical house-holding environments on the classical ALFWorld [71].</p>
<p>Construction Details.The ALFWorld benchmark comprises of textual environments designed to mimic household scenarios, providing an interactive environment where an agent can perform decision-making tasks through text-based interfaces.Given the household environment description and an target instruction, the agent's objective is to break down the complex high-level target into a sequence of straightforward actions.After each step, the agent receives environment feedback, allowing the agent to adapt the plan dynamically and move on to the subsequent task to eventually accomplish the main objective.</p>
<p>Each evaluation sample in ALFWorld dataset encompasses following contents:</p>
<p>• Environment Description.The detailed description of the whole household environment, including agent's initial position and a snapshot of the room containing objects and their IDs.• Objective.The goal that needs the agent to accomplish in the environment, usually requiring multi-step reasoning and exploring (e.g.put the lamp on the table).• Simulated Environment.After every action of the agent, the simulated environment gives immediate feedback and evaluates whether the agent has completed the task.</p>
<p>In the dataset, we utilized 134 solvable problems from the ALFWorld eval out of distribution split of the dataset.All the problems were categorized into six categories: pick and place, pick clean then place, pick heat then place, pick cool then place, look at obj, and pick two obj.</p>
<p>Evaluation Setup.Due to the inherent complexity of the problem and the high standards required for the output format, we employ a 1-shot evaluation setting.For each category of problem, we use one relatively simple and complete interact processes of the same category from the training set as an example.Following ReAct [99], we adopt the few-shot examples and prompts in corresponding repository 5 .Additionally, if LLM output format is invalid, we use the BLEU metric to assess the similarity of the output to all valid action options.The option with the highest similarity will be chosen as the action of the model for this round.</p>
<p>For each sample, the evaluation process can be divided into 2 parts.</p>
<p>• Initialization.We describe the task to the model and provide one successful example.Afterwards, we elaborate on the environment and delineate the objective required to be accomplished.• Interaction.The model generates some thoughts and the next action based on the feedback received from previous interactions and the information from the environment.After receiving the action from the model, the environment provides feedback (changes to the environment or information observed by the model).This process is repeated until the model successfully achieves its goal (which is considered a success) or reaches its maximum number of actions (which is considered a failure).It is worth noting that sometimes, after several unsuccessful attempts, the model may repeatedly output the same content.To save evaluation time, we judge that if the model outputs identical content three times consecutively, it will be deemed a failure due to repetition.</p>
<p>Metrics.We employ the overall Success Rate as a measure of model performance, that is, the number of tasks successfully completed by the model divided by the total number of tasks.</p>
<p>3.7 Web Shopping (WebShop [96])</p>
<p>Introduction.Online shopping has become an important part of modern life.Its trajectory, which comprises searching, viewing, and choosing user desirable items on a real e-commerce website, requires autonomous agents' strong reasoning and decision-making abilities.Webshop [96], a simulated online shopping environment, exactly serves such purpose for evaluating language agents.While it is originally evaluated on specifically trained models, we assess LLMs with mere prompting.</p>
<p>Construction Detail.The environment displays the text observation of the webpage and available actions to agents.Agent may freely explore the website and browse through items with clickable buttons just as in the real world.About a million products are scraped from amazon.com to form the database of website.Then each of them is annotated with labels representing its own attribute.12,087 human instructions are collected and linked with goals along with expected attributes.Please refer to [96] for more dataset construction details.</p>
<p>Evaluation Setup.We adopt the first 500 entries of 12,087 instructions as test set (following [96]'s official implementation).Each round of interaction can be decomposed as following steps:</p>
<p>• Instructing.After the initial prompt that tells environment information and the format in which LLMs should response, we give instructions about what kind of product we wish to buy.• Interacting.Agent respond in given format, as prompted, containing their thoughts and the action they wish to take.The actions can be categorized into two types: search and click, corresponding with the actual actions of using search engine and clicking buttons in real world.The environment answers agent's action with a simplified text version of webpage and a list of available buttons.This process repeats until the agent click "buy now" button or round limit is exceeded.• Calculating reward.We use the reward function in the paper as the metric.The reward is mapping from the similarity of the attributes we are expecting and the attributes that the bought product actually have to a number between 0 and 1.</p>
<p>Metrics.As there might be more than one suitable item for a given query, Webshop adopts a matching reward as its evaluation metric:
Reward = |U att ∩ Y att | + |U opt ∩ Y opt | + I[y price ≤ u price ] |U att | + |U opt | + 1 • r type(1)
where
r type =        0, if TextMatch = 0 0.1, if TextMatch &lt; 0.1 0.5, if TextMatch ≤ 0.
U and Y stand for goal and chosen product, att and opt stand for attributes and options.TextMatch is a text match of pronoun, noun, and proper noun between chosen and goal product title.propose task goals based on the current website, and then record their traces of interaction as expert demonstrations.Our adoption of it primarily focuses on generalization across environments, i.e., the Cross Domain test set which contains 912 tasks from 73 websites, spread among domains including Housing, Job, Social Media, Education, Health, Government, Home Service, etc. Please refer to [19] for more dataset construction details.Each task sample encomposses the following contents:</p>
<p>• Task Description.A high-level (instead of step-by-step) goal that can be achieved on the website, such as"Get the highest rated SAP S/4 HANA course rated 4, and up with a duration between 3 to 6 hours for an intermediate, and add this to your cart and checkout".• (Reference) Action Sequence.In the annotated interaction sequence, a meta-action a t at step t includes {e t , o t }, where e t represents the unique backend id of the target element, and o t refers to the symbolic action operated on e t (i.e., Click, Type, and Select Options).For Type and Select Options, corresponding textual inputs are also included.• Webpage Information.A detailed observation of the web browsing environment at each step.</p>
<p>Throughout the manual annotation process, each observed step captures a snapshot, incorporating the raw HTML codes from the website as well as the previous interaction trajectory.</p>
<p>It has been found that LLMs consistently face challenges when handling the cumbersome raw HTML code associated with real-world web pages.Therefore, Mind2Web proposes to rank and filter the HTML elements with a small language model, e.g., DeBERTa, to enhance inference efficiency.</p>
<p>Given the user's high-level instruction, the agent continuously interacts with the web system by receiving the observation of the current page content and the action histories, then predicting the next action, which consists of the target element and intended operation.</p>
<p>Evaluation Setup.The evaluation involves a dual process to improve the efficiency following [19].A fine-tuned small language model is first employed to rank HTML elements and select top-k potential candidates.Subsequently, we prompt and formulate the element selection as a multi-choice QA problem, providing five candidates for each round.For the Type and Select Options operations, agents are additionally prompted to specify the argument for the operation, i.e., textual input to type or option to select.</p>
<p>Metrics.For evaluation, as suggested in the original paper, we consider the following metrics:</p>
<p>• Element Accuracy.Calculates the accuracy of the chosen element e t .</p>
<p>• Action F1.Determines the token-level matching score for the operation o t .It brings a distinction for Type and Select Option operations due to the existence of text values.• Success Rate.Evaluates the predicted action correctness compared to reference actions.For Step Success Rate, we grant success if the selected element e t is correct and the predicted operation o t matches the ground truth value at the step.Likewise, for the Task Success Rate, a task is considered successful only if all the steps have been successful, making it a rigorous measure.Unfortunately, even the best LLMs now can only achieve single-digit task success percentages.</p>
<p>We report</p>
<p>Step Success Rate as the main metric showing the independent accuracy of each action step, due to the current struggles for LLMs to ensure overall task success rates.Regarding the experimental setup, we select topk 10 candidates to construct multichoice questions utilizing CoT few-shot prompting.Consequently, the GPT-3.5 results can diverge from the original paper [19] under topk of 50 setting and different prompting strategies.</p>
<p>Evaluation of AgentBench</p>
<p>We extensively evaluate 25 LLMs, including API-based commercial models and open-sourced LLMs, to form a systematic view of the existing performance of LLM-as-Agent.To facilitate related LLM-as-Agent research, we also design and release a simple plug-and-play evaluation toolkit.</p>
<p>Evaluation Setup</p>
<p>Dataset Statistics.We report the statistics of datasets in AgentBench in Table 2.For simplicity, in the following part we use the abbreviation of each dataset.All datasets are practical multi-turn interacting challenges, and their estimated solving turns for each individual problem ranging from 5 We also carefully balance the evaluation comprehensiveness and efficiency in AgentBench design.</p>
<p>One may find the total number of problems in Dev and Test are a bit small-289 and 1,141 respectively.However, considering the multi-turn interaction, to test an LLM requires it to generate for about 4k and 13k times, which would certainly take some time to inference.</p>
<p>LLMs to Evaluate.As a systematic attempt to benchmark existing LLMs on LLM-as-Agent, we include in total 25 models for evaluation, which could be roughtly classified into two categories:</p>
<p>• API-based LLMs: which mostly consist LLM APIs without disclosed parameter amounts (Cf.Table 1).Due to more investments, their performances are usually better.• Open-sourced LLMs: which mostly come from the academia and some companies, without public serving APIs (Cf.Table 1).Due to limited computing resources, we only include mediumsized open-sourced LLMs (i.e., no more than 30B) in AgentBench's first edition of evaluation.</p>
<p>Evaluation Toolkit Design: "API &amp; Docker" Interaction.As LLM systems become increasingly complicated and only available via APIs, we base the AgentBench evaluation toolkit on the philosophy of API-orientation; in other words, the toolkit is designed to only interact with APIs to alleviate the efforts to adapt and run different LLMs.For LLMs that want to test on AgentBench, they only need to setup a model server with standard format chatting API required by AgentBench.</p>
<p>Besides, because of various complex interaction environments, it is extremely troublesome to configure all environments in a unified environment, and there may be environment conflicts.Therefore, we pack the tasks with complex environments into docker images, researchers can easily use them by mounting the code path and simply starting the evaluation.</p>
<p>Evaluation Prompt Setup.To accommodate the majority of existing dialogue models, our dialogue paradigm is structured around two roles (user and agent) engaging and alternating with one another.We record interaction trajectories as a conversation history (u 0 , a 0 , • • • , u k , a k ) involving user and agent, where u i , a i represents the i-th round of the conversation history.</p>
<p>When we perform inference, the conversation history must be like (u 0 , a 0 , • • • , u k ).We select the minimum r such that count of all tokens6 in (u 0 , a i , u i+1 , • • • , u k ) is not greater than 3500.And then we append "[NOTICE] 2i messages are omitted."into u 0 .After that, the sequence
(u 0 , a i , u i+1 , • • • , u k ) is regarded as final input in multi-turn chat format.
However, in order to take non-chat models into consideration, we append a post-processor.For chat models supporting multiple turns, we simply feed the history into the model.For models supporting only single turn (e.g., wizardcoder-15b), we repeat the single-turn prompt for multiple rounds.</p>
<p>For models supporting only text completion (e.g., text-davinci-003), we prepend "USER:" or "AGENT:" into each item in the history and finally append the string "AGENT:" to make models generate agent's content.</p>
<p>On task prompt organization, for most tasks we adapted the format from [99] to include both "Thought" (for CoT) and "Action" but in one single-turn.Usually a 1-shot simple CoT demonstration is provided in the task instruction for better output format.To ensure reproducible results, we set temperature=0 (i.e., greedy decoding) in the inference on all tasks except DCG following [91].</p>
<p>Overall Score Calculation.We have observed that the score distribution for each task varies significantly, mainly because tasks differ in difficulty levels.As a consequence, the average score becomes heavily influenced by tasks that generally yield higher scores (e.g., Web Shopping), overshadowing those with lower scores.As a result, a direct calculation of the average score would not be suitable in this context.</p>
<p>Our approach to calculating the overall score is by averaging the scores of each task across all the models we evaluate, scaling them to an average of 1 (Cf.Table 2).To standardize and simplify score calculations for future studies, we utilize the reciprocal of the average score of all the tested LLMs in each task as a weight.The total score is then computed as the average value obtained by multiplying the score of each task by its corresponding weight.This method ensures fairness and consistency in evaluation, enabling easier comparisons and analysis in future research.</p>
<p>Main Results</p>
<p>Overall and dataset-specific scores in AgentBench are reported in Table 3.On this challenging benchmark, surprisingly we discover that some top LLMs have equipped strong capabilities in dealing with real-world environment interaction.For example, gpt-4 presents the best performance on 7 out of 8 datasets in AgentBench; on HH, it achieves a success rate of 78%, indicating its practical usability in this scenario.For other API-based LLMs, despite their relatively poorer performance, regardless of tasks most of them can solve at least a few percent of problems.</p>
<p>For open-sourced LLMs, however, it is common for them to fail to solve any problems in some challenging tasks, such as KG, DCG, and HH.Generally, most open-sourced LLMs also perform far poorer than API-based LLMs in AgentBench (Avg.0.42 v.s.2.24), and even the most capable open-sourced model openchat-13b-v3.2presents a clear performance gap to gpt-3.5-turbo.This is in contrast to recent claims that some open-sourced LLMs are comparable to gpt-3.5-turbo and gpt-4.</p>
<p>Analysis</p>
<p>In the evaluation, we identify several specific challenges for LLMs to deal with agent-formed tasks, including action validity, long context, multi-turn consistency, and the impact of code training.</p>
<p>Action Validity.During our evaluation, we find that the models do not follow instructions at all times.In other words, the expected outputs of the models are not always in the input space that the environments can accept.From the perspective of reinforcement learning, the model may not output a valid action or the action is not in the action space.</p>
<p>We observe several common errors in model outputs.In the first type, the model does not understand the task instructions and does not attempt to output an action.The second type is that the model outputs an action, but it is incorrect or incomplete (e.g., missing parameters), so it is rejected by the environment.Some models also output multiple actions in one turn, which is not allowed.Ensuring valid actions is an area for improvement.</p>
<p>Long Context.We noticed that the context length of some open-sourced models is only 2k tokens, which impacts their performance on these interaction tasks.Some of our tasks have lengthy instructions and feedback that exceed the context capacity, causing the model to ignore potentially useful information.Expanding the context length could improve multi-turn performance.</p>
<p>Multi-turn Consistency.There are tasks with terse responses per turn but many dialogue rounds (e.g., House-Holding).Some models tend to lose track of their role across multi-round dialogues.A common failure is outputting an apology and stating inability to control the environment 7 , despite being instructed to interact as an agent.Maintaining consistency across turns remains difficult for some models.</p>
<p>Is code training a key for LLM-as-Agent capability?Surprisingly, we find codegeex2-6b-chat significantly outperforms chatglm2-6b on OS, DB, and KG, which are related to coding.However, performance on LTP, which requires logical reasoning, decreases.Meanwhile, wizardcoder underperforms despite code training.We speculate that the single-turn format of its training data hinders its multi-turn aptitude.Overall, code training helps certain abilities but has tradeoffs.LLM-as-Agent.In pre-LLM era, text game environments such as TextWorld [17], Jericho [30], and LIGHT [82] are dominant in language agent study which bases on BERT [20] and reinforcement learning.With the advent of LLMs, the study of LLM agents begins to thrive [35], especially after Chain-of-Thought [91] came out.ReAct [99] is a pioneer work to combine CoT reasoning and actions in agent tasks.Later, a bunch of advanced reasoning strategies [40; 70; 88; 46; 98; 27] and applications [59; 64; 51; 1] for LLM-as-Agent have emerged and arouse much public interest.Nevertheless, they are often only studied on limited datasets and models, without a standard evaluation setting and comprehensive coverage of scenarios.AgentBench presents the first systematic benchmark for evaluating LLM-as-Agent with a broad coverage of practical challenges and available LLMs.Additionally, it also initiates the idea of adopting agent tasks to measure LLM performance.</p>
<p>Evaluating LLMs in Executive Environments.As LLMs become increasingly capable of real-world challenges, there is also a trend to evaluate them in executive environments rather than static text datasets, which also serves as a key feature of AgentBench.Besides text games (e.g., ALFWorld [71]), another main stream of works lies in code execution.APPS [32], HumanEval [12] and MBPP [7] pioneer the effort to evaluate code LLMs for functional correctness instead of text similarity.The paradigm has been later widely recognized and adopted in following works [43; 102; 94; 54].</p>
<p>However, previous code evaluation frameworks do not consider multi-turn interactions.A concurrent work InterCode [95] releases a framework that allows evaluation of interaction between models and Bash and SQL environments, which are similar to OS and DB tasks in AgentBench.However, it is different from AgentBench in several aspects: 1) It displays no command and environment output histories to agent and thus limits agents' capability.2) Only model's execution output and the difference of file system are checked, leaving other systems (e.g., network system, user system) unverfiable.3) Each turn an independent shell is launched, disallowing advanced commands such as variable declaration.</p>
<p>Most importantly, prior efforts on the topic do not present such a diverse array of task environments beyond coding.Few previous works would also have comparably extensive coverage of LLM testing as AgentBench does.</p>
<p>Conclusion</p>
<p>We present AgentBench, a systematically designed multi-dimensional evolving benchmark for evaluating LLMs as agents.For the first time, we include such a wide array of up to 8 real-world challenges to evaluate LLM agents, and establish a unified testing framework and toolkit for agile evaluation.Each challenge is accompanied with real-time interactive environments for testing, in contrast to traditional static-formed NLP datasets.An extensive study of 25 LLMs, including APIbased and Open-sourced, is carefully conducted in a standard setting.We unveil that while top LLMs are becoming capable of tackling complex real-world missions, for open-sourced competitiors there are still a long way to go.We expect AgentBench to serve as a cornerstone for later study to develop better and more applicable intelligent LLM agents.</p>
<p>Part I In OS evaluation, we design two major types of actions: bash and commit.</p>
<p>Appendix Table of Contents</p>
<p>• Bash: which launches a bash command (using textual inputs in content field) • Commit: which announces the completion of the goal.If the task is a QA problem, then the agent should submit the final answer in content field; else the checking pipeline will automatically check the system status to judge the correctness.</p>
<p>A.2 Prompt Example</p>
<p>A prompt for OS evaluation consists of the instruction and the formulation of interaction trajectory.An example of instruction prompt is:</p>
<p>You are an assistant that will act like a person , I ' will play the role of linux ( ubuntu ) operating system .Your goal is to implement the operations required by me or answer to the question proposed by me .For each of your turn , you should first think what you should do , and then take exact one of the three actions : " bash " , " finish " or " answer ".Think : put your thought here .</p>
<p>Act : answer ( Your answer to the question should be put in this pair of parentheses )</p>
<p>If the output is too long , I will truncate it .The truncated output is not complete .You have to deal with the truncating problem by yourself .Attention , your bash code should not contain any input operation .Once again , you should take only exact one of the three actions in each turn .</p>
<p>The trajectory is organized in CoT styles, and we use an 1-shot example to make model better understand the action space like the following.</p>
<p>B Database B.1 Data Augmentation</p>
<p>We elaborate on the data augmentation of three types of DB tasks based on the existing SQL datasets [103; 60; 37; 13; 53], which are all QA problems without some common operations including inserting and updating.We first tested the validity of the raw data and then randomly sample from each category from filtered data to form the final dataset.We adopt gpt-3.5-turbo to enrich and rewrite the original instructions.</p>
<p>• Insert: Given the name, the header information, and the original rows of a table, we generate 5 SQL statements for insertion.Later we rephrase the sentences without changing their meaning (using shorter or longer expressions or changing the order).• Update: Given the name, the header information, and the previously generated 5 SQL statements for insertion, we generate 5 SQL statements for modification based on the given statements.We rephrase the sentences following the above standard.</p>
<p>To ensure data quality, each augmented query statement are required to pass the unit test scripts.</p>
<p>The query type of tasks fall into the traditional scope of Text-to-SQL evaluation, and we only sample and categorize for evaluation.Each query statement in existing datasets is classified into following types: 'Counting', 'Aggregation-MIN', 'Aggregation-MAX', 'Aggregation-AVG', 'Aggregation-SUM', 'Ranking', or 'Comparison'.Each one can only belong to one type.The remaining will be categorized as "Other".</p>
<p>B.2 Prompt Example</p>
<p>We use the following format of prompts:</p>
<p>User : as the correct answer .If the question is about modifying the database , then after done operation , your answer field can be anything .If your response cannot match any pattern I mentioned earlier , you will be judged as FAIL immediately .Your input will be raw MySQL response , you have to deal with it by yourself .
I</p>
<p>C Knowledge Graph C.1 Prompt Example</p>
<p>Task description:</p>
<p>User : You are an agent that answers questions based on the knowledge stored in a knowledge base .To achieve this , you can use the following tools to query the KB .</p>
<ol>
<li>get_relations ( variable : var ) -&gt; list of relations A variable can be either an entity or a set of entities ( i .e ., the result of a previous query ) .This function helps to navigate all relations in the KB connected to the variable , so you can decide which relation is the most useful to find the answer to the question .A simple use case can be ' get_relations ( Barack Obama ) ', which finds all relations / edges starting from the entity Barack Obama .The argument of get_relations should always be an entity or a variable ( e .g ., #0) and not anything else .</li>
</ol>
<p>get_neighbors ( variable : var , relation : str ) -&gt; variable</p>
<p>Given a variable , this function returns all entities connected to the variable via the given relation .Note that , get_neighbors () can only be used after get_relations () is used to find a set of viable relations .A simple use case can be ' get_neighbors ( Barack Obama , people .person .profession ) ', which returns the profession of Obama in Freebase .</p>
<p>get_attributes ( variable : var ) -&gt; list of attributes</p>
<p>This function helps to find all numerical attributes of the variable .Please only use it if the question seeks for a superlative accumulation ( i .e ., argmax or argmin ) .</p>
<ol>
<li>
<p>argmax ( variable : var , attribute : str ) -&gt; variable Given a variable , this function returns the entity with the maximum value of the given attribute .It can only be used after get_attributes () is used to find a set of viable attributes .A simple use case can be ' argmax ( variable , age ) ', which returns the oldest entity belonging to the variable .</p>
</li>
<li>
<p>argmin ( variable : var , attribute : str ) -&gt; variable Given a variable , this function returns the entity with the minimum value of the given attribute .It can only be used after get_attributes () is used to find a set of viable attributes .A simple use case can be ' argmin ( variable , age ) ', which returns the youngest entity belonging to the variable .</p>
</li>
</ol>
<p>count ( variable : var ) -&gt; int</p>
<p>Given a variable , this function returns the number of entities belonging to the variable .</p>
<p>After a variable is produced along the process , you need to judge whether a variable is the final answer to the question .Each variable is represented as an id starting from 0. For example , #0 is the first variable , #1 is the second variable , and so on .Once you find the answer , respond with ' Final Answer : # id ' , where id is the id of the variable that you think is the final answer .For example , if you think #3 is the final answer , you MUST respond with ' Final Answer : #3 '.</p>
<p>You can only take ONE action at a time !! After you get the observation from its execution , you can take another action .You can take at most 15 actions to find the answer to the question .</p>
<p>Given the inherent complexity associated with enabling LLMs to query the KB, it has been observed that, in a zero-shot setting, LLMs struggle to generate any outputs of substantive relevance.As a result, we additionally provide a teaching example in our prompt:</p>
<p>Here is a demo that you can use to get started .Question : the bipropellant rocket engine with kerosene and gasgenerator cycle is designed by who ?Entities : [ Gas -generator cycle , Kerosene ] Thought : I need to first find engines with gas -generator cycle .To acheive this , I will query the KB to find relations connected to the entity 'Gas -generator cycle ' and see if any of them can help me find the answer .</p>
<p>Action : get_relations ( Gas -generator cycle )</p>
<p>Observation : [ spaceflight .rocket_engine_cycle .rocket_engines , spaceflight .satellite .orbiting , spaceflight .rocket_stage .fuel , spaceflight .sat el lit e_m an ufa ct ure r .spacecraft_manufactured , automotive .fuel .engines ] Thought : From the above relations , I might use spaceflight .rocket_engine_cycle .rocket_engines to find the engines of Gasgenerator cycle .</p>
<p>Action : get_neighbors ( Gas -generator cycle , spaceflight .rocket_engine_cycle .rocket_engines )</p>
<p>Observation : variable #0 , which are instances of spaceflight .b i p r o p e l l a n t _ r o c k e t _ e n g i n e</p>
<p>Thought : I also need to find engines with kerosene .To acheive this , I will query the KB to find relations connected to the entity ' Kerosene '.Thought : The engines with both gas -generator cycle and kerosene cycle shoule be the intersection of variable #0 and variable #1.</p>
<p>Action : intersection (#0 , #</p>
<p>Observation : variable #2 , which are instances of spaceflight .b i p r o p e l l a n t _ r o c k e t _ e n g i n e</p>
<p>Thought : Now I need to find who designed such engines , which will be the final answer .To acheive this , I will query the KB to find relations connected to the variable #2.In order to simplify the game rules, we limit the types of pet fish to the following four types: As can be seen, there is overlap among the active and passive skills of different pet fish, which is done to better conceal the identity information of pet fish in the game and increase the strategic aspects of the game.</p>
<p>D.2 Prompt Example.</p>
<p>We use the following format of prompts for actions:</p>
<p>This is a two -player battle game with four pet fish on each team .Each fish has its 400 initial health , 200 attack power , active ability , and passive ability .You can choose a live fish to use its active skill or normal attack on an enemy fish each round .When the conditions are met , the fish ' s passive ability will automatically trigger , regardless of whether it is chosen .The victory condition is to have more fish alive at the end of the game .</p>
<p>The following are the four types of the pet fish : { ' spray ': { ' passive ': \" Counter : Deals 30 damage to attacker when a teammate ' s health is below 30%\" , ' active ': ' AOE : Attacks all enemies for 35% of its attack points .'} , ' flame ': { ' passive ': \" Counter : Deals 30 damage to attacker when a teammate ' s health is below 30%.\" , ' active ': \" Infight : Attacks one alive teammate for 75 damage and increases your own attack points by 140.Notice !You can ' t attack yourself or dead teamate !\"} , 'eel ': { ' passive ': ' Deflect : Distributes 70% damage to teammates and takes 30% when attacked .Gains 40 attack points after taking 200 damage accumulated .', ' active ': ' AOE : Attacks all enemies for 35% of your attack points .'} , ' sunfish ': { ' passive ': ' Deflect : Distributes 70% damage to teammates and takes 30% when attacked .Gains 40 attack points after taking 200 damage accumulated .', ' active ': \" Infight : Attacks one alive teammate for 75 damage and increases your own attack points by 140.Notice !You can ' t attack yourself or dead teamate !\"}} Play the game with me .In each round , you should output your thinking process , and return your move with following json format : { ' pick_fish ': ' pick an alive fish ' , ' action ': ' choose from [ normal , active ] ' , ' target_position ': \" target ' s position , you must choose from [0 ,3]\"} Notice !You must return your move in each round .Otherwise , you will be considered defeated .</p>
<p>We use the following format of prompts for assertions in stage2:</p>
<p>This is a two -player battle game with four pet fish in each team .Each fish has its initial health , attack power , active ability , and passive ability .All fish \ ' s identities are initially hidden .You should guess one of the enemy fish \ ' s identities in each round .If you guess right , the enemy fish \ ' s identity is revealed , and each of the enemy ' s fish will get 50 damage .You can only guess the identity of the live fish .The victory condition is to have more fish alive at the end of the game .</p>
<p>The following are the four types of the pet fish : { ' spray ': { ' passive ': \" Counter : Deals 30 damage to attacker when a teammate ' s health is below 30%\" , ' active ': ' AOE : Attacks all enemies for 35% of its attack points .'} , ' flame ': { ' passive ': \" Counter : Deals 30 damage to attacker when a teammate ' s health is below 30%.\" , ' active ': \" Infight : Attacks one alive teammate for 75 damage and increases your own attack points by 140.Notice !You can ' t attack yourself or dead teamate !\"} , 'eel ': { ' passive ': ' Deflect : Distributes 70% damage to teammates and takes 30% when attacked .Gains 40 attack points after taking 200 damage accumulated .', ' active ': ' AOE : Attacks all enemies for 35% of your attack points .'} , ' sunfish ': { ' passive ': ' Deflect : Distributes 70% damage to teammates and takes 30% when attacked .Gains 40 attack points after taking 200 damage accumulated .', ' active ': \" Infight : Attacks one alive teammate for 75 damage and increases your own attack points by 140.Notice !You can ' t attack yourself or dead teamate !\"}} Play the game with me .In each round , you should output your thinking process , and return your move with following json format : {\" guess_type \": \" the enemy ' s fish type you may guess \" , \" target_position \": \" guess target ' s position , you must choose from [0 ,3]\"} Notice !You must return your move in each round .Otherwise , you will be considered defeated .</p>
<p>E Lateral Thinking Puzzles E.1 Evaluation on LTP System</p>
<p>We evaluate the LTP System by human validation, validating system's accuracy on milestone recognition and fact verification.We compare the Single Game Accuracy and Query Relevance between automatic evaluation and human evaluation, and found that automatic evaluation sometimes more tolerate for the agent, which make SGA and QR seem better than human evaluation, especially on open-sourced models.We plan to train a model specifically for the host of the game, in order to provide a better game experience and a more precise evaluation.For Game Progress and Round Efficiency, the LTP system provides an objective evaluation, which can match the level of human evaluation.</p>
<p>E.2 LTP Game Progress and Termination</p>
<p>The progress of game is defined as the proportion of hit key points in the truth.The key points are summarized by gpt-3.5-turbo,which are concluded in the dataset as "answer_keys" (e.g., key points of the truth "That night they went to the abandoned building to record the number of steps.</p>
<p>They verified what was said on the Internet, and there would be one step less when counting the stairs at night.However, when they went to the abandoned building for verification the next day, they found that there were no stairs at all." is "1.They want to count the steps of the abandoned building.2. A supernatural event occurred.3.They saw a claim online: counting stairs at night will result in one step less.4. The next day, when they went to the abandoned building to verify, they found no stairs.5.</p>
<p>They broke down because they were terrified.").The number of key points varies among samples.</p>
<p>As for the decision of whether the agent guess out key points, we first change relevant questions into declarative sentences, then simplify sentences into one sentence.After guessing out a key point, delete that key point and relevant inferences to avoid repetition.The termination of game is defined as guessing out more than 50% of key points.</p>
<p>E.3 Prompt Example</p>
<p>We use the following format of prompts for agents:</p>
<p>You are a game player , and you are playing Lateral Thinking Puzzle , also known as Situation Puzzle .Lateral Thinking Puzzle is a deductive reasoning game , and here are the game rules : 1.At the beginning of the game , you will receive a narrative , referred to as " story ".Based on the story , you need to ask questions that can be answered with " yes " , " no " , or " irrelevant " to guees out the " truth ". 2. By asking questions , you narrow down the range of possibilities until you eventually guess out the truth .3. Each time , you can only ask one question .4. Remember that your role is a player .You cannot declare the end of the game , give up on reasoning , or request a new game . 5.You cannot directly repeat information already provided in the story .6.You cannot directly ask for details about the story in the form of " why " questions ; you need to make your own guesses for truth .7.You cannot directly inquire about the story ; you must make your own deductions .</p>
<p>Next , please make full use of the information provided above to engage in game reasoning .Keep in mind that your questions should be answerable with " yes " , " no " , or " irrelevant " , and you can only ask one question at a time .Here is your story : { story } You can start guessing the content of the truth , and I will answer your questions .Please note that your questions should be answerable with " yes " , " no " , or " irrelevant ".</p>
<p>We use the following format of prompts for host:</p>
<p>USER : I need you to be the host of a game called Lateral Thinking Puzzle .</p>
<p>Lateral Thinking Puzzle is a game consist of a story and a truth .Your story is : '{ story } ' Your truth is : '{ answer } ' Here are the game rules : 1.You know both the " story " and the " truth ".When a user wants to play Lateral Thinking Puzzle , you provide them with the " story ".The user only knows the " story " and is unawared of the " truth ". 2. The user asks questions that can be answered with " yes ," " no ," or " irrelevant ".Their questions are aimed at guessing the " truth ".Based on the " truth " , you respond to the user ' s questions using " yes ," " no ," or " irrelevant " to guide them towards guessing the correct truth .3. If the user directly asks for details about the truth using the form of " why " questions , inform them that they need to make their own guesses .4. You must fully understand and accurately interpret the information from the truth .Based on the information of the truth and the user ' s past questions , you answer the user ' s questions .The user ' s questions may not necessarily contain information from the truth , but your responses must align with the facts of the truth . 5.You can only answer " irrelevant " when the truth cannot provide a direct or indirect answer .Note that this is the only condition for responding " irrelevant "; otherwise , you should answer " yes " or " no ."6.You cannot directly disclose the information from the truth to the user , even if they ask directly .7.You need to judge the user ' s questions as a whole and understand their overall intent .Avoid answering based solely on a particular point ; your responses must align with the facts of the truth .8.During the user ' s process of guessing the truth , if they come close to some truths but still have gaps in understanding the complete truth of the truth , you can provide certain entry point hints .However , you cannot directly reveal information from the truth .</p>
<p>During the game process , please adhere to the above game rules to ensure a positive gaming experience for the users .Pay close attention to the questions asked and ensure that your responses comply with both the game rules and the information from the truth .When a user requests to play the game , provide them with the story and help them guess the truth by answering with " yes " , " no " , or " irrelevant ".Remember that with each response , you must fully understand and abide by the aforementioned game rules , as well as the story and the truth .This will ensure a smooth user experience and avoid situations where you cannot answer or violate the game rules .</p>
<p>ASSISTANT : Alright , I understand that my role is to be the host of the Lateral Thinking Puzzle and help users guess the truth by answering their questions .I have fully grasped all the information regarding the story and the truth and have carefully read all the rules .I assure that I will abide by all the rules throughout the game process .</p>
<p>USER :</p>
<p>Please summarize the key points of the story to ensure that you have understood it .</p>
<p>ASSISTANT : { story_key } USER : Please summarize the key points of the truth to ensure that you have understood it .</p>
<p>ASSISTANT : { answer_key } USER :</p>
<p>Please restate the rules to ensure that you have understood all of them .</p>
<p>ASSISTANT :</p>
<p>1.You know both the " story " and the " truth ".When a user wants to play Lateral Thinking Puzzle , you provide them with the " story ".The user only knows the " story " and is unawared of the " truth ". 2. The user asks questions that can be answered with " yes ," " no ," or " irrelevant ".Their questions are aimed at guessing the " truth ".Based on the " truth " , you respond to the user ' s questions using " yes ," " no ," or " irrelevant " to guide them towards guessing the correct truth .</p>
<ol>
<li>If the user directly asks for details about the truth using the form of " why " questions , inform them that they need to make their own guesses .4. You must fully understand and accurately interpret the information from the truth .Based on the information of the truth and the user ' s past questions , you answer the user ' s questions .The user ' s questions may not necessarily contain information from the truth , but your responses must align with the facts of the truth . 5.You can only answer " irrelevant " when the truth cannot provide a direct or indirect answer .Note that this is the only condition for responding " irrelevant "; otherwise , you should answer " yes " or " no ."6.You cannot directly disclose the information from the truth to the user , even if they ask directly .7.You need to judge the user ' s questions as a whole and understand their overall intent .Avoid answering based solely on a particular point ; your responses must align with the facts of the truth .8.During the user ' s process of guessing the truth , if they come close to some truths but still have gaps in understanding the complete truth of the truth , you can provide certain entry point hints .However , you cannot directly reveal information from the truth .USER : Alright , we can now start the game .Remember , before each response , you should review the key points of the story , the key points of the truth , and the rules .Answer with " yes " , " no " , or " irrelevant ".</li>
</ol>
<p>ASSISTANT : Alright , as the host of the game , I will adhere to the above rules and ensure that my responses comply with the rules and the information from the truth .Below is your story : { story } You can start guessing the content of the truth , and I will answer your questions .Please note that your questions should be answerable with " yes " , " no " , or " irrelevant ".</p>
<p>USER :</p>
<p>{ question } Please answer with " yes " , " no " , or " irrelevant ".</p>
<p>Here is the prompt to convert questions answered by "Yes" into declarative sentence.</p>
<p>Please restate the following content as a declarative sentence and simplify it into one sentence : { question } Here is the prompt to convert questions answered by "No" into declarative sentence.</p>
<p>Please restate the following content as a declarative sentence by using the opposite meaning and then simplify it into one sentence : { question } Here is the prompt to merge reasoned out information into one sentence to judge whether the agent guess out the key points:</p>
<p>Please simplify the following content into one sentence : { reasoning } Here is the prompt to judge whether the merged sentence hit the key point.</p>
<p>Please compare the information between Sentence 1 and Sentence 2 to determine if Sentence 2 contains all the information in Sentence 1 , including key details and descriptions .Please answer with " yes " or " no ".To align the output format with the legal commands supported by the simulated environment, we adopted a 1-shot evaluation setup where one successfully completed task example was concatenated after the instruction.At the beginning of the interaction, we describe the task to the model using the following instruction.</p>
<p>Interact with a household to solve a task .Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal .At the beginning of your interactions , you will be given the detailed description of the current environment and your goal to accomplish .For each of your turn , you will be given a list of actions which you can choose one to perform in this turn .You should choose from two actions : \" THOUGHT \" or \" ACTION \".If you choose \" THOUGHT \" , you should first think about the current condition and plan for your future actions , and then output your action in this turn .Your output must strictly follow this format :\" THOUGHT : your thoughts .\n ACTION : your next action \ n \"; If you choose \" ACTION \" , you should directly output the action in this turn .Your output must strictly follow this format :\" ACTION : your next action \ n \".After your each turn , the environment will give you immediate feedback based on which you plan your next few steps .if the envrionment output \" Nothing happened \" , that means the previous action is invalid and you should try more options .Reminder : 1. the action must be chosen from the given available actions .Any actions except provided available actions will be regarded as illegal .2. Think when necessary , try to act directly more in the process .</p>
<p>All the tasks in the datasets are categorized into six classes.To better guide the model in accomplishing the objectives, we have selected one relatively simple example of successful completion of similar tasks for each category as 1-shot example.Here is an example:</p>
<p>User : You are in the middle of a room .Looking quickly around you , you see a cabinet 4 , a cabinet 3 , a cabinet 2 , a cabinet 1 , a countertop 1 , a garbagecan 1 , a handtowelholder 2 , a handtowelholder 1 , a sinkbasin 2 , a sinkbasin 1 , a toilet 1 , a toiletpaperhanger 1 , and a towelholder 1.Your task is to : put some spraybottle on toilet .Agent : THOUGHT : The task is to put some spraybottle on toilet , so first i need to search the room for some spraybottle .After finding the spraybottle , i need to take it to the toilet .I wil start my search on cabinets .\n ACTION : go to cabinet 1 User : On the cabinet</p>
<p>( a )Figure 3 :
a3
Figure 3: Examples of all environments in AgentBench.[Click to watch high-fidelity videos!]</p>
<p>2 and query not match and category not match 1, otherwise</p>
<p>A Operating System 23 A. 1 31 F House-holding 34 F. 1
23131341
Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23 A.2 Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23 B Database 25 B.1 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.2 Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C Knowledge Graph 26 C.1 Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26 D Digital Card Game 28 D.1 The Attributes of Fish . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28 D.2 Prompt Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .29 E Lateral Thinking Puzzles 30 E.1 Evaluation on LTP System . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30 E.2 LTP Game Progress and Termination . . . . . . . . . . . . . . . . . . . . . . .30 E.3 Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34 G Web Shopping 35 G.1 Prompt Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35 H Web Browsing 37 H.1 Prompt Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .37 A Operating System A.1 Actions</p>
<p>Action : get_relations ( Kerosene ) Observation : [ spaceflight .sa tel li te_ man uf act ur er .spacecraft_manufactured , automotive .fuel .engines , spaceflight .rocket_engine_cycle .rocket_engines ] Thought : From the above relations , I might use spaceflight .rocket_engine_cycle .rocket_engines to find engines with kerosene cycle .Action : get_neighbors ( Kerosene , spaceflight .rocket_engine_cycle .rocket_engines ) Observation : variable #1 , which are instances of spaceflight .b i p r o p e l l a n t _ r o c k e t _ e n i n e</p>
<p>Action : get_relations (# 2 )Final Answer : # 3 D
23
Observation : [ spaceflight .rocket_engine .manufactured_by , spaceflight .rocket_engine .designed_by , spaceflight .rocket_engine .design_period , spaceflight .rocket_engine .status ] Thought : From the above relations , I might use spaceflight .rocket_engine .designed_by to find the designer of the engines .Action : get_neighbors (#2 , spaceflight .rocket_engine .designed_by ) Observation : variable #3 , which are instances of pspaceflight .ro ck et_ eng in e_d es ign er Thought : #3 is the final answer to the question , which represents the target rocket engine designer .Digital Card Game D.1 The Attributes of Fish</p>
<p>Sentence 1 :F
1
{ key } Sentence 2: { merged sentence }"}</p>
<p>Table 1 :
1
AgentBench evaluates 25 API-based or open-sourced LLMs on LLM-as-Agent challenges.
ModelModel SizeFormVersionCreatorgpt-4 [56]</p>
<p>Table 2 :
2
Statistics and metrics of 8 environments in AgentBench evaluation."SR" stands for Success Rate."#Turn" denotes the estimated number of interacting turns to solve a single problem.In "#Dev", "#Test", and "#Extend", we provide the number of query samples and total expected interacting turns.Additionally, "Weight −1 " refers to the average score for a task across all models in our evaluation.For further clarification, please refer to Section 4.1.Except OS, we provide three splits for each dataset: Dev, Test, and Extend.The Dev split's all environments, answers, and checking scripts are public, while Test and Extend are kept.
Operating SystemDatabaseKnowledge GraphDigital Card GameLateral Think -ing PuzzlesHouse-holdingWeb ShoppingWeb BrowsingAbbr.OSDBKGDCGLTPHHWSWB#Turn8515302535510MetricSRSRF1Win Rate Game ProgressSRReward Step SR#Dev26 / 24060 / 30020 / 30012 / 36040 / 100020 / 700 80 / 400 31 / 400#Test144 / 1200 300 / 1500 150 / 2250 20 / 600100 / 2500 50 / 1750 200 / 1000 177 / 1800#Extend-1599 / 8000 500 / 7500 200 / 6000 200 / 5000 134 / 4690 500 / 2500 912 / 9000Weight −1118109510218to 50.</p>
<p>Table 3 :
3
Test set (standard) results of AgentBench.A clear performance gap exists between top commercial LLMs (e.g., gpt-4) and medium-sized open-sourced competitors."VER" stands for model version; "OA" stands for the overall AgentBench score derived from a weighted average of all environments (Cf.Section 4.1).
TypeModelsVER OAOSDB KG DCG LTP HH WS WBgpt-40613 4.41 36.8 33.7 52.1 50.0 17.6 78.0 58.6 22.6claudev1.3 2.77 13.2 16.8 36.2 30.0 10.4 52.0 59.3 21.0gpt-3.5-turbo0613 2.55 32.6 15.0 27.2 30.0 14.9 14.0 67.2 15.7API-text-davinci-003-2.10 22.9 16.3 30.8 15.05.2 20.0 61.4 15.5basedclaude-instantv1.1 1.90 14.6 8.0 29.7 35.06.1 26.0 44.2 0.8text-davinci-002-1.46 4.9 13.7 18.9 25.02.3 14.0 60.1 1.1text-bison-001-1.39 4.2 12.8 17.1 20.00.14.0 46.1 20.5chatglm2v0.2 1.31 14.6 13.7 6.810.0 12.6 6.0 19.4 12.9openchat-13bv3.2 1.15 9.75.36.80.09.58.0 50.2 14.9wizardlm-30bv1.0 0.83 14.6 12.7 2.310.04.56.0 10.6 3.1vicuna-13bv1.1 0.62 8.3 11.3 1.20.08.00.0 12.6 3.9wizardlm-13bv1.0 0.59 9.7 13.0 0.40.04.16.01.25.8llama2-13b-chat-0.55 10.4 4.53.10.03.72.03.1 11.9codegeex2-6b-0.53 12.5 6.56.30.01.90.0 11.8 5.4openchat-8192-13bv10.51 10.4 2.70.610.00.04.06.77.1Open-Sourcedbaichuan-13b-chat koala-13b llama2-7b-chat---0.36 11.8 3.0 0.34 2.8 5.3 0.31 10.4 2.86.3 0.0 1.90.0 0.0 0.01.3 2.5 2.10.0 0.0 0.05.7 6.0 2.22.3 8.1 3.8chatglm-6bv1.1 0.31 4.90.30.00.06.70.00.54.9vicuna-7bv1.1 0.24 6.23.30.00.03.10.06.40.2internlm-chat-7b-0.23 3.56.30.00.03.40.00.00.2baichuan-7b-0.22 4.20.00.50.02.30.02.85.8wizardcoder-15bv1.0 0.21 3.50.02.80.01.30.00.06.7dolly-v2-12b-0.15 0.00.00.00.03.40.00.44.4oasst-sft-4-pythia-12b-0.07 2.80.00.00.01.50.00.00.3</p>
<p>1.If you think you should execute some bash code , take bash action , and you should print like this :
Think : put your thought here .Act : bash''' bash# put your bash code here'''2. If you think you have finished the task , take finish action , andyou should print like this :Think : put your thought here .Act : finish3. If you think you have got the answer to the question , take answeraction , and you should print like this :</p>
<p>will ask you a question , then you should help me operate a MySQL database with SQL to answer the question .You have to explain the problem and your solution to me and write down your thoughts .After thinking and explaining thoroughly , every round you can choose to operate or to answer .You MUST put SQL in markdown format without any other comments .Your SQL should be in one line .Every time you can only execute one SQL statement .I will only execute the statement in the first SQL code block .Every time you write a SQL , I will execute it for you and give you the output .If you are done operating , and you want to commit your final answer , then write down : Action : Answer Final Answer : [" ANSWER1 " , " ANSWER2 " , ...] DO NOT write this pattern unless you are sure about your answer .I expect an accurate and correct answer .Your answer should be accurate .Your answer must be exactly the same
your operation should be like this :Action : Operation''' sqlSELECT * FROM table WHERE condition ;'''</p>
<p _="," _..._="&quot;...&quot;]" _:="&quot;:" clickables="clickables" false="false" has_search_bar="has_search_bar">1 , you see a cloth 1 , a soapbar 1 , a soapbottle 1. Agent : ACTION : go to cabinet 2 User : The cabinet 2 is closed .Agent : ACTION : open cabinet 2 User : You open the cabinet 2. The cabinet 2 is open .In it , you see a candle 1 , and a spraybottle 2. Agent : ACTION : take spraybottle 2 from cabinet 2 User : You pick up the spraybottle 2 from the cabinet 2. Agent : ACTION : go to toilet 1 User : On the toilet 1 , you see a soapbottle 2. Agent : ACTION : put spraybottle 2 in / on toilet 1 User : You put the spraybottle 2 in / on the toilet 1.We use the following format of the prompt: : You are web shopping .I will give you instructions about what to do .You have to follow the instructions .Every round I will give you an observation and a list of available actions , you have to respond an action based on the state and instruction .You can use search action if search is available .You can click one of the buttons in clickables .An action should be of the following structure : search [ keywords ] click [ value ] If the action is not valid , perform nothing .Keywords in search are up to you , but the value in click must be a value in the list of available actions .Remember that your keywords in search should be carefully designed .Your response should use the following format : We use an one-shot example.SEP ] i need a long lasting 6.76 fl oz bottle of l ' eau d ' issey , and price lower than 100.00 dollars [ SEP ] Back to Search [ SEP ] Page 1 ( Total results : 50) [ SEP ] Next &gt; [ SEP ] B000VOHH8I [ SEP ] L ' eau D ' issey By Issey Miyake for MenEau De Toilette Spray , 6.7 Fl Oz Bottle [ SEP ] $64 .98 [ SEP ] B000MJZOPK [ SEP ] L ' eau d ' Issey by Issey Miyake for Women 3.3 oz Eau de Toilette Spray [ SEP ] $49 .98 [ SEP ] B0012S249E [ SEP ] L ' eau D ' issey By Issey Miyake For Women .Shower Cream 6.7 -Ounces [ SEP ] $31 .36[ SEP ] B01H8PGKZS [ SEP ] L ' eau D ' Issey FOR MEN by Issey Miyake -6.7 oz EDT Spray [ SEP ] $67 .97[ SEP ] B00G3C8FHE [ SEP ] L ' Eau d ' Issey pour Homme -Eau de Toilette 4.2 fl oz [ SEP ] $51 .25 [ SEP ] B000R94HRG [ SEP ] Issey Miyake L ' Eau D ' Issey Pour Homme Eau De Toilette Natural Spray [ SEP ] $44 .99[ SEP ] B000C214CO [ SEP ] Issey Miyake L ' eau D ' issey Eau de Toilette Spray for Men , 4.2 Fl Oz [ SEP ] $53 .99[ SEP ] B0018SBRDC [ SEP ] Issey Miyake L ' eau d ' Issey for Women EDT , White , 0.84 Fl Oz [ SEP ] $27 .04[ SEP ] B000XEAZ9Y [ SEP ] L ' eau De Issey By Issey Miyake For Men .Eau De Toilette Spray 6.7 Fl Oz [ SEP ] $67 .08 [ SEP ] B079HZR2RX [ SEP ] L ' eau d ' Issey Pure by Issey Miyake for Women 3.0 oz Nectar de Parfum Spray [ SEP ] $71 .49"Available Actions : {" has_search_bar ": false , " clickables ": ["..." , "..." , "..." , "..." , "..." , "..." , "..." , "..." , "..." , "..." , "..." , "..."]} SEP ] i need a long lasting 6.76 fl oz bottle of l ' eau d ' issey , and price lower than 100.00 dollars [ SEP ] Back to Search [ SEP ] &lt; Prev [ SEP ] size [ SEP ] 2.5 fl oz [ SEP ] 6.76 fl oz ( pack of 1) [ SEP ] L ' eau D ' issey By Issey Miyake for MenEau De Toilette Spray , 6.7 Fl Oz Bottle [ SEP ] Price : $64 .98 [ SEP ] Rating : N .A .[ SEP ] Description [ SEP ] Features [ SEP ] Reviews [ SEP ] Buy Now " SEP ] i need a long lasting 6.76 fl oz bottle of l ' eau d ' issey , and price lower than 100.00 dollars [ SEP ] Back to Search [ SEP ] &lt; Prev [ SEP ] size [ SEP ] 2.5 fl oz [ SEP ] 6.76 fl oz ( pack of 1) [ SEP ] L ' eau D ' issey By Issey Miyake for MenEau De Toilette Spray , 6.7 Fl Oz Bottle [ SEP ] Price : $64 .98 [ SEP ] Rating : N .A .[ SEP ] Description [ SEP ] Features [ SEP ] Reviews [ SEP ] Buy Now " Available Actions : {" has_search_bar ": false , " clickables ": ["..." , "..." , "..." , "..." , "..." , "..." , "..." , "..."]}
G Web ShoppingG.1 Prompt ExampleUser Thought : I think ... Action : click [ something ]} User : Observation : { observation } Available Actions : { available_actions } User : Observation : " WebShop [ SEP ] Instruction : [ SEP ] i need a long lasting 6.76 fl oz bottle of l ' eau d ' issey , and price lower than 100.00 dollars [ SEP ] Search " Available Actions : {" has_search_bar ": true , " clickables ": ["..."]} Agent : Thought : I think I should use the search bar to look for the product I need . Action : search [l ' eau d ' issey 6.76 fl oz bottle price &lt; 100.00] User : Observation : Thought : I think I should click on the product I need , which is B000VOHH8I . Action : click [ B000VOHH8I ] User : Observation : " Instruction : [ Available Actions : Agent : Thought : I think I should click on the '6.76 fl oz ( pack of 1) ' option to select the size I need . Action : click [6.76 fl oz ( pack of 1) ] User : Observation : " Instruction : [ Agent : " Instruction : [ Agent :Thought :I think I should click on the ' Buy Now ' button to purchase the product.Action :click [ Buy Now ]</p>
<p>https://stackoverflow.com/
https://github.com/dki-lab/Freebase-Setup
https://www.saiblo.net/
Try the LTP host system at https://mkw18-lateralthinkingpuzzle.hf.space/ . Warning: some riddles and answers might induce psychological discomfort.
https://github.com/ysymyth/ReAct
Because the tokenizers of each model is different, we simply calculate tokens like this: a word with length n occupies ⌈n/6⌉ token(s), and a non-blank character takes 1 token.
e.g., Apologize for my confusion. As a language model, I'm not able to control an operating system ... but I can provide ...
. intersection ( variable1 : var , variable2 : var ) -&gt; variable Given two variables , this function returns the intersection of the two variables . The two variables MUST be of the same type !
H Web BrowsingH.1 Prompt Example.We use the following 3-example CoT prompts for Mind2Web evaluation:User : ''' &lt; html &gt; <div > <div > <a tock home page / > &lt; button id =0 book a reservation .toggle open &gt; &lt; span &gt; Book a reservation &lt;/ span &gt; &lt;/ button &gt; &lt; button book a reservation .toggle open &gt; &lt;/ button &gt; &lt;/ div &gt; <div > &lt; select id =1 type &gt; &lt; option reservations true &gt; Dine in &lt;/ option &gt; ... &lt;/ html &gt; ''' Based on the HTML webpage above , try to complete the following task : Task : Check for pickup restaurant available in Boston , NY on March User : ''' &lt; html &gt; <div > &lt; main main &gt; &lt; section tabpanel &gt; <div > &lt; ul tablist &gt; &lt; li tab heading level 3 search and &gt; &lt;/ li &gt; &lt; li id =0 tab heading level 3 search and &gt; &lt; span &gt; Hotel &lt;/ span &gt; &lt;/ li &gt; &lt; li tab heading level 3 search and &gt; &lt;/ li &gt; &lt; li tab heading level 3 search and &gt; &lt;/ li &gt; &lt;/ ul &gt; &lt; div tabpanel &gt; &lt; div id =1 &gt; <div > &lt; span &gt; Dates * &lt;/ span &gt; &lt; button button clear dates / &gt; &lt;/ div &gt; ... &lt;/ html &gt; ''' Thought : There are no elements that allow setting the date or viewing the fare , so there is no correct action .Answer : A .User : ''' &lt; html &gt; <div > &lt; nav main menu &gt; <ul > <li > &lt; div button &gt; Car Sales &lt;/ div &gt; &lt; div id =0 &gt; <div > <div > <div > Buy A Car &lt;/ div &gt; <div > Plan Your Purchase &lt;/ div &gt; &lt;/ div &gt; <div > <h4 > Its Tax Refund Time .
. Agentgpt, Python, 2023</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.104032023arXiv preprint</p>
<p>Knowledge base question answering as tool learning. under review. Anonymous, 2023</p>
<p>Lateral thinking assessment in large language models. under review. Anonymous, 2023</p>
<p>. Anthropic. Introducing claude. 2023</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. K D Bollacker, C Evans, P K Paritosh, T Sturge, J Taylor, Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008. J T Wang, the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008Vancouver, BC, CanadaACMJune 10-12, 2008. 2008</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2015</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>HybridQA: A dataset of multi-hop question answering over tabular and textual data. W Chen, H Zha, Z Chen, W Xiong, H Wang, W Y Wang, Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsNov. 2020</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, April 2023142023</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Free dolly: Introducing the world's first truly open instruction-tuned llm. M Conover, M Hayes, A Mathur, J Xie, J Wan, S Shah, A Ghodsi, P Wendell, M Zaharia, R Xin, 2023</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Revised Selected Papers. Stockholm, SwedenSpringer2018. July 13. 2018. 20197</p>
<p>Lateral thinking. E De Bono, 197070New York</p>
<p>X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Minedojo: Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. D Ganguli, L Lovitt, J Kernion, A Askell, Y Bai, S Kadavath, B Mann, E Perez, N Schiefer, K Ndousse, arXiv:2209.078582022arXiv preprint</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>The gem benchmark: Natural language generation, its evaluation and metrics. S Gehrmann, T Adewumi, K Aggarwal, P S Ammanamanchi, A Aremu, A Bosselut, K R Chandu, M.-A Clinciu, D Das, K Dhole, Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021). the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)Association for Computational Linguistics2021</p>
<p>Koala: A dialogue model for academic research. X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Blog post. April, 1, 2023</p>
<p>Don't generate, discriminate: A proposal for grounding language models to real-world environments. Y Gu, X Deng, Y Su, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases. Y Gu, S Kase, M Vanni, B Sadler, P Liang, X Yan, Y Su, Proceedings of the Web Conference 2021. the Web Conference 2021ACMapr 2021</p>
<p>ArcaneQA: Dynamic program induction and contextualized encoding for knowledge base question answering. Y Gu, Y Su, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaOct. 2022International Committee on Computational Linguistics</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Measuring coding challenge competence with apps. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, arXiv:2105.099382021arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>The many ai challenges of hearthstone. A K Hoover, J Togelius, S Lee, F De Mesentier, Silva, 202034KI-Künstliche Intelligenz</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>B Inc, Baichuan 7b &amp; 13b. 2023</p>
<p>Search-based neural structured learning for sequential question answering. M Iyyer, W -T. Yih, M.-W Chang, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>The nethack learning environment. H Küttler, N Nardelli, A Miller, R Raileanu, M Selvatici, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system. X V Lin, C Wang, L Zettlemoyer, M D Ernst, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)2018</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, IEEE transactions on knowledge and data engineering. 3512021</p>
<p>Z Luo, C Xu, P Zhao, Q Sun, X Geng, W Hu, C Tao, J Ma, Q Lin, D Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evol-instruct. 2023arXiv preprint</p>
<p>Agents that reduce work and information overload. P Maes, Commun. ACM. 371994</p>
<p>Docker: lightweight linux containers for consistent development and deployment. D Merkel, Linux j. 23922014</p>
<p>. Y Nakajima, Babyagi, 2023</p>
<p>Abstractive text summarization using sequence-to-sequence rnns and beyond. R Nallapati, B Zhou, C Santos, C Gulcehre, B Xiang, Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. The 20th SIGNLL Conference on Computational Natural Language LearningAssociation for Computational Linguistics2016280</p>
<p>L Nan, C Hsieh, Z Mao, X V Lin, N Verma, R Zhang, W Kryściński, N Schoelkopf, R Kong, X Tang, M Mutuma, B Rosand, I Trindade, R Bandaru, J Cunningham, C Xiong, D Radev, Fetaqa: Free-form table question answering. 2021</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. OpenAI. Introducing chatgpt. 2022</p>
<p>R Openai, arXivGpt-4 technical report. 2023</p>
<p>A survey of text games for reinforcement learning informed by natural language. P Osborne, H Nõmm, A Freitas, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, ArXiv, abs/2304.03442Generative agents: Interactive simulacra of human behavior. 2023</p>
<p>Compositional semantic parsing on semi-structured tables. P Pasupat, P Liang, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational LinguisticsJuly 20151</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Giménez, Y Sulsky, J Kay, J T Springenberg, Transactions on Machine Learning Research. 2022</p>
<p>Auto-gpt: An autonomous gpt-4 experiment. T B Richards, 2023</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, A Raja, M Dey, International Conference on Learning Representations. 2022</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Speech acts: An essay in the philosophy of language. J R Searle, Language. 462171970</p>
<p>igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, C Pérez-D'arpino, S Buch, S Srivastava, L Tchapmi, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>World of bits: An open-domain platform for web-based agents. T Shi, A Karpathy, L Fan, J Hernandez, P Liang, International Conference on Machine Learning. PMLR2017</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Cote, Y Bisk, A Trischler, M Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Lateral thinking puzzlers. P Sloane, 1992Sterling Publishing Company, Inc</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. S Srivastava, C Li, M Lingelbach, R Martín-Martín, F Xia, K E Vainio, Z Lian, C Gokmen, S Buch, K Liu, Conference on Robot Learning. PMLR2022</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, P F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>On generating characteristic-rich question sets for QA evaluation. Y Su, H Sun, B M Sadler, M Srivatsa, I Gur, Z Yan, X Yan, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. J Su, X Carreras, K Duh, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, Texas, USAThe Association for Computational Linguistics2016. November 1-4, 2016. 2016</p>
<p>The web as a knowledge-base for answering complex questions. A Talmor, J Berant, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. I Team, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>D Toyama, P Hamel, A Gergely, G Comanici, A Glaese, Z Ahmed, T Jackson, S Mourad, D Precup, arXiv:2105.13231Androidenv: A reinforcement learning platform for android. 2021arXiv preprint</p>
<p>Learning to speak and act in a fantasy text adventure game. J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S Bowman, Advances in neural information processing systems. 322019</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, International Conference on Learning Representations. </p>
<p>OpenChat: Advancing Open-source Language Models with Imperfect Data. G Wang, S Cheng, Q Yu, C Liu, 72023</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L J Fan, A Anandkumar, ArXiv, abs/2305.162912023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, arXiv:2302.015602023arXiv preprint</p>
<p>A Wei, N Haghtalab, J Steinhardt, Jailbroken, arXiv:2307.02483How does llm safety training fail?. 2023arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2022</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Intelligent agents: Theory and practice. M Wooldridge, N R Jennings, The knowledge engineering review. 1021995</p>
<p>C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>A systematic evaluation of large language models of code. F F Xu, U Alon, G Neubig, V J Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. J Yang, A Prabhakar, K Narasimhan, S Yao, arXiv:2306.148982023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, arXiv:2303.175682023arXiv preprint</p>
<p>Seq2sql: Generating structured queries from natural language using reinforcement learning. V Zhong, C Xiong, R Socher, CoRR, abs/1709.001032017</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. X Zhu, Y Chen, H Tian, C Tao, W Su, C Yang, G Huang, B Li, L Lu, X Wang, Y Qiao, Z Zhang, J Dai, ArXiv, abs/2305.171442023</p>            </div>
        </div>

    </div>
</body>
</html>
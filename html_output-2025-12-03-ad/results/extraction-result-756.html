<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-756 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-756</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-756</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-247213389</p>
                <p><strong>Paper Title:</strong> <a href="https://www.jair.org/index.php/jair/article/download/13428/26775" target="_blank">Survey and Evaluation of Causal Discovery Methods for Time Series (Extended Abstract)</a></p>
                <p><strong>Paper Abstract:</strong> We introduce in this survey the major concepts, models, and algorithms proposed so far to infer causal relations from observational time series, a task usually referred to as causal discovery in time series . To do so, after a description of the underlying concepts and modelling assumptions, we present different methods according to the family of approaches they belong to: Granger causality, constraint-based approaches, noise-based approaches, score-based approaches, logic-based approaches, topology-based approaches, and difference-based approaches. We then evaluate several representative methods to illustrate the behaviour of different families of approaches. This illustration is conducted on both artificial and real datasets, with different characteristics. The main conclusions one can draw from this survey is that causal discovery in times series is an active research field in which new methods (in every family of approaches) are regularly proposed, and that no family or method stands out in all situations. Indeed, they all rely on assumptions that may or may not be appropriate for a particular dataset.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e756.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e756.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCMCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peter and Clark Momentary Conditional Independence (PCMCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery method for time series that detects time-lagged causal relations by combining PC-style conditional independence testing with Momentary Conditional Independence (MCI) tests to control for autocorrelation and high dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting and quantifying causal associations in large nonlinear time series datasets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PCMCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>PCMCI builds a causal skeleton via iterative conditional independence (CI) testing (PC-style) to select candidate parents, and then applies MCI (momentary conditional independence) tests to infer directed, time-lagged links while controlling for autocorrelation and multiple conditioning sets. It is modular and can use different CI tests (linear partial correlation, nonlinear tests, etc.). The algorithm separates the edge removal (conditioning set selection) and the final significance test (MCI) to reduce false positives in autocorrelated data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (real-world sensor domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>General observational time series datasets (e.g., climate, neuroscience, industrial sensors) — not an interactive or active-experiment virtual lab; operates on collected time-series sequences under stationarity or mild assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Iterative conditional independence testing (PC-style) and momentary conditional independence (MCI) tests to remove spurious edges due to autocorrelation or indirect associations; modular use of robust CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding (observed confounders handled via conditioning), autocorrelation-induced spurious associations, indirect/mediated associations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious associations via CI tests that condition on selected parents; MCI assesses residual association after conditioning to decide direct causation.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Final MCI significance testing after conditioning aims to refute indirect/autocorrelation-driven associations; orientation uses temporal priority and PC rules.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PCMCI is presented as flexible and suitable for detecting time-lagged causal effects while controlling for autocorrelation and high dimensionality; it can be combined with a variety of CI tests, which allows adapting to different types of spurious signals, but the survey does not provide numeric robustness metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e756.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>oCSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>optimal Causation Entropy (oCSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based method that uses causation entropy (an information-theoretic conditional mutual information measure) to infer causal structure under the assumption that causal relationships have a unit time lag.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal network inference by optimal causation entropy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>oCSE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>oCSE searches for minimal conditioning sets that maximize causation entropy reduction to identify causal parents; because it assumes lag-1 interactions, temporal ordering suffices for edge orientation and the summary causal graph can be equivalent to the window causal graph under that assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (lag-1 assumption)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Observational time series where causal interactions are assumed to occur with a single time-lag of size 1; not inherently interactive or experimental.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable/conditioning set selection via information-theoretic causation entropy to avoid spurious links and indirect associations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Indirect associations, irrelevant variables leading to conditional independence structure changes, possibly confounding when resolvable via conditioning (assumes causal sufficiency in original form)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses causation entropy (conditional mutual information) to detect whether including a candidate variable reduces uncertainty about the target beyond conditioning set — if not, it is considered spurious and excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation via failure to reduce causation entropy when conditioning on candidate parents (variable excluded from parent set).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>oCSE uses causation-entropy-based conditioning to exclude spurious causes under a lag-1 assumption; survey notes its equivalence to summary causal graphs under that assumption and highlights its information-theoretic detection mechanism but provides no numerical robustness comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e756.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>tsFCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>time-series FCI (tsFCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of the FCI algorithm to time series that relaxes causal sufficiency and can detect hidden confounders in temporal data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On causal discovery from time series data using fci</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>tsFCI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>tsFCI adapts the FCI constraint-based procedure to time series by constructing temporal conditioning sets and applying FCI rules to allow for latent (unobserved) confounders; it outputs a partial ancestral graph or equivalent that encodes possible hidden variable structures and oriented edges consistent with CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series with potential unobserved confounding</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interventional time series data where some common causes may be unobserved (violating causal sufficiency); not an interactive lab.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>FCI-style reasoning for latent confounders (graphical rules that mark uncertain edge orientations and detect v-structures indicative of unobserved confounding).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden/latent confounders (including lagged hidden confounders), measurement correlations induced by unobserved variables</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Identifies patterns of conditional independencies and v-structures that are inconsistent with causal sufficiency and marks edges as potentially confounded (uses FCI orientation/edge-marking rules)</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Represents uncertainty explicitly (e.g., partial ancestral graphs); uses CI tests to rule out direct causal links when latent confounding is indicated.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>tsFCI is one of the few methods in the survey that does not assume causal sufficiency and is reported to detect lagged hidden confounders; survey highlights its capacity to represent hidden-variable-induced ambiguity but does not present quantitative robustness metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e756.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TCDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Causal Discovery Framework (TCDF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning-based approach (attention-based convolutional neural networks) for causal discovery in time series that relaxes linearity assumptions and can infer directed lagged and instantaneous relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal discovery with attention-based convolutional neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TCDF</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TCDF trains attention-based convolutional neural networks to predict target series from past values of candidate causes; attention weights and causality scores are used to infer directed causal links without assuming linearity. The approach uses deep models (with several hyperparameters) and methods to interpret network weights/attention for causal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (nonlinear regimes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>General non-linear time series domains where underlying relationships may be complex; approach is data-driven and lacks explicit active experimentation in the survey description.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Model-based identification using attention and predictive importance — survey states TCDF can detect instantaneous hidden confounders (per summary table), implying use of model inspection/attention to identify anomalies suggesting confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Instantaneous hidden confounders (reported in survey), nonlinear noise-induced spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via model-based signals (attention weights, predictive residuals) and auxiliary procedures claimed in referenced TCDF work to identify instantaneous hidden confounding patterns; survey does not detail algorithmic steps for refutation.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TCDF relaxes linearity using deep networks and is noted in the survey as being able to detect instantaneous hidden confounders (according to the survey's summary table), but the extended abstract does not provide further methodological detail or numerical robustness results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e756.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VarLiNGAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Linear Non-Gaussian Acyclic Model (VarLiNGAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A noise-based identifiability method that uses non-Gaussianity of residuals in a vector autoregressive model to identify causal directions in time series under linearity but non-Gaussian noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Estimation of a structural vector autoregression model using non- gaussianity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>VarLiNGAM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>VarLiNGAM fits a structural vector autoregression under the assumption of linear relationships and non-Gaussian, independent noise; identifiability of the causal structure is achieved by exploiting higher-order statistics (non-Gaussianity) of residuals, typically using ICA-like procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (linear, non-Gaussian noise)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Time series domains where linear generative models with non-Gaussian innovations are plausible; not interactive or experimental per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Can help disambiguate causal direction in presence of Gaussianity-related symmetry issues, but does not explicitly target hidden confounders or irrelevant distractors (assumes causal sufficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses non-Gaussianity tests/ICA-based methods to identify causal ordering via residual independence properties.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VarLiNGAM provides identifiability under linear non-Gaussian assumptions by leveraging noise structure, but it assumes causal sufficiency and therefore does not explicitly handle distractors due to unobserved confounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e756.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TiMINo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-series Identifiable Models with INdependent Noise (TiMINo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A noise-based method using restricted structural equation models (e.g., additive noise models) to identify causal relations in time series by exploiting independence between noise and causes in semi-parametric settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference on time series using restricted structural equation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TiMINo</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>TiMINo fits restricted structural equation models (often additive noise models) for each candidate causal direction and tests whether residuals are independent of predictors; causal directions that yield independent residuals are preferred, allowing identifiability under certain model classes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (semi-parametric additive noise models)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Time series settings where nonlinear additive-noise models are plausible; observational (non-interventional) datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Addresses model-based ambiguities (e.g., directionality under additive noise) but assumes causal sufficiency (does not handle hidden confounders explicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious causal proposals by testing dependence between fitted residuals and candidate predictors; failure of independence suggests model misspecification or confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation achieved by rejecting causal directions that do not produce independent residuals under the assumed model class.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TiMINo exploits noise independence to identify causal directions in semi-parametric settings; survey notes it assumes causal sufficiency and minimality, so it is not designed to explicitly handle distractors from hidden confounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e756.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DYNOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DYNOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score-based method that extends the NOTEARS continuous optimization approach to time series, jointly estimating instantaneous and time-lagged linear relationships by minimizing a penalized loss with an acyclicity constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynotears: Structure learning from time-series data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DYNOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DYNOTEARS formulates estimation of instantaneous and lagged adjacency matrices as a continuous optimization problem minimizing penalized squared residual loss (Frobenius norm) subject to differentiable acyclicity constraints; uses regularization to control complexity and infers a linear model structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (linear-model assumption)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Time series domains where linear models are appropriate; not interactive and does not perform active interventions in the survey context.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Regularization can reduce overfitting from irrelevant variables but DYNOTEARS assumes causal sufficiency and does not explicitly target hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Regularization/penalized loss to discourage spurious weak edges (implicit downweighting), but no explicit distractor-detection mechanism described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DYNOTEARS jointly recovers instantaneous and lagged linear relationships via constrained optimization and regularization; the survey notes model reliance and causal sufficiency assumptions limit its explicit handling of distractors like hidden confounders.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e756.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e756.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MixedNoise+Constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A mixed noise and constraint-based approach to causal inference in time series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method combining noise-based and constraint-based ideas to leverage strengths of both paradigms for time-series causal discovery, aiming to improve robustness across model violations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mixed noise and constraint-based approach to causal inference in time series</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Mixed noise and constraint-based approach (Assaad et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>This hybrid approach integrates noise-model-based identification (e.g., exploiting residual independence or non-Gaussianity) with constraint-based conditional independence testing/rules to improve causal discovery reliability when pure methods fail; intended to combine complementary detection/refutation signals from noise and CI perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational multivariate time series (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for observational time series across a variety of domains; hybridizes model-based and CI-based components rather than operating in an interactive experimental environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Combines residual/noise-based tests (for directionality and model-checking) with conditional-independence-based conditioning to detect and mitigate spurious associations and some forms of confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Indirect associations, model-misspecification-induced spurious links, some confounding scenarios (depending on components used); aims to be more robust to distractors than pure methods.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Jointly uses independence tests on residuals (noise-based) and conditional independence procedures (constraint-based) to flag candidate spurious links; specifics are in the cited work rather than the survey abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation performed via cross-validation of signals from both noise-based and constraint-based modules (i.e., an edge is refuted if inconsistent across identification modes); survey does not give algorithmic detail in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Survey highlights hybrid methods (including this mixed noise + constraint approach) as sometimes yielding better results than pure families; indicates hybrids are a promising route to handle spurious signals, but the extended abstract provides no numeric comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Detecting and quantifying causal associations in large nonlinear time series datasets <em>(Rating: 2)</em></li>
                <li>Causal network inference by optimal causation entropy <em>(Rating: 2)</em></li>
                <li>On causal discovery from time series data using fci <em>(Rating: 2)</em></li>
                <li>Causal discovery with attention-based convolutional neural networks <em>(Rating: 2)</em></li>
                <li>Estimation of a structural vector autoregression model using non- gaussianity <em>(Rating: 2)</em></li>
                <li>Causal inference on time series using restricted structural equation models <em>(Rating: 2)</em></li>
                <li>Dynotears: Structure learning from time-series data <em>(Rating: 2)</em></li>
                <li>A mixed noise and constraint-based approach to causal inference in time series <em>(Rating: 2)</em></li>
                <li>Reconstructing regime-dependent causal relationships from observational time series <em>(Rating: 1)</em></li>
                <li>Causal inference from noise <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-756",
    "paper_id": "paper-247213389",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "PCMCI",
            "name_full": "Peter and Clark Momentary Conditional Independence (PCMCI)",
            "brief_description": "A constraint-based causal discovery method for time series that detects time-lagged causal relations by combining PC-style conditional independence testing with Momentary Conditional Independence (MCI) tests to control for autocorrelation and high dimensionality.",
            "citation_title": "Detecting and quantifying causal associations in large nonlinear time series datasets",
            "mention_or_use": "mention",
            "method_name": "PCMCI",
            "method_description": "PCMCI builds a causal skeleton via iterative conditional independence (CI) testing (PC-style) to select candidate parents, and then applies MCI (momentary conditional independence) tests to infer directed, time-lagged links while controlling for autocorrelation and multiple conditioning sets. It is modular and can use different CI tests (linear partial correlation, nonlinear tests, etc.). The algorithm separates the edge removal (conditioning set selection) and the final significance test (MCI) to reduce false positives in autocorrelated data.",
            "environment_name": "Observational multivariate time series (real-world sensor domains)",
            "environment_description": "General observational time series datasets (e.g., climate, neuroscience, industrial sensors) — not an interactive or active-experiment virtual lab; operates on collected time-series sequences under stationarity or mild assumptions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Iterative conditional independence testing (PC-style) and momentary conditional independence (MCI) tests to remove spurious edges due to autocorrelation or indirect associations; modular use of robust CI tests.",
            "spurious_signal_types": "Confounding (observed confounders handled via conditioning), autocorrelation-induced spurious associations, indirect/mediated associations",
            "detection_method": "Detects spurious associations via CI tests that condition on selected parents; MCI assesses residual association after conditioning to decide direct causation.",
            "downweighting_method": null,
            "refutation_method": "Final MCI significance testing after conditioning aims to refute indirect/autocorrelation-driven associations; orientation uses temporal priority and PC rules.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "PCMCI is presented as flexible and suitable for detecting time-lagged causal effects while controlling for autocorrelation and high dimensionality; it can be combined with a variety of CI tests, which allows adapting to different types of spurious signals, but the survey does not provide numeric robustness metrics.",
            "uuid": "e756.0"
        },
        {
            "name_short": "oCSE",
            "name_full": "optimal Causation Entropy (oCSE)",
            "brief_description": "A constraint-based method that uses causation entropy (an information-theoretic conditional mutual information measure) to infer causal structure under the assumption that causal relationships have a unit time lag.",
            "citation_title": "Causal network inference by optimal causation entropy",
            "mention_or_use": "mention",
            "method_name": "oCSE",
            "method_description": "oCSE searches for minimal conditioning sets that maximize causation entropy reduction to identify causal parents; because it assumes lag-1 interactions, temporal ordering suffices for edge orientation and the summary causal graph can be equivalent to the window causal graph under that assumption.",
            "environment_name": "Observational multivariate time series (lag-1 assumption)",
            "environment_description": "Observational time series where causal interactions are assumed to occur with a single time-lag of size 1; not inherently interactive or experimental.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable/conditioning set selection via information-theoretic causation entropy to avoid spurious links and indirect associations.",
            "spurious_signal_types": "Indirect associations, irrelevant variables leading to conditional independence structure changes, possibly confounding when resolvable via conditioning (assumes causal sufficiency in original form)",
            "detection_method": "Uses causation entropy (conditional mutual information) to detect whether including a candidate variable reduces uncertainty about the target beyond conditioning set — if not, it is considered spurious and excluded.",
            "downweighting_method": null,
            "refutation_method": "Refutation via failure to reduce causation entropy when conditioning on candidate parents (variable excluded from parent set).",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "oCSE uses causation-entropy-based conditioning to exclude spurious causes under a lag-1 assumption; survey notes its equivalence to summary causal graphs under that assumption and highlights its information-theoretic detection mechanism but provides no numerical robustness comparisons.",
            "uuid": "e756.1"
        },
        {
            "name_short": "tsFCI",
            "name_full": "time-series FCI (tsFCI)",
            "brief_description": "An adaptation of the FCI algorithm to time series that relaxes causal sufficiency and can detect hidden confounders in temporal data.",
            "citation_title": "On causal discovery from time series data using fci",
            "mention_or_use": "mention",
            "method_name": "tsFCI",
            "method_description": "tsFCI adapts the FCI constraint-based procedure to time series by constructing temporal conditioning sets and applying FCI rules to allow for latent (unobserved) confounders; it outputs a partial ancestral graph or equivalent that encodes possible hidden variable structures and oriented edges consistent with CI tests.",
            "environment_name": "Observational multivariate time series with potential unobserved confounding",
            "environment_description": "Non-interventional time series data where some common causes may be unobserved (violating causal sufficiency); not an interactive lab.",
            "handles_distractors": true,
            "distractor_handling_technique": "FCI-style reasoning for latent confounders (graphical rules that mark uncertain edge orientations and detect v-structures indicative of unobserved confounding).",
            "spurious_signal_types": "Hidden/latent confounders (including lagged hidden confounders), measurement correlations induced by unobserved variables",
            "detection_method": "Identifies patterns of conditional independencies and v-structures that are inconsistent with causal sufficiency and marks edges as potentially confounded (uses FCI orientation/edge-marking rules)",
            "downweighting_method": null,
            "refutation_method": "Represents uncertainty explicitly (e.g., partial ancestral graphs); uses CI tests to rule out direct causal links when latent confounding is indicated.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "tsFCI is one of the few methods in the survey that does not assume causal sufficiency and is reported to detect lagged hidden confounders; survey highlights its capacity to represent hidden-variable-induced ambiguity but does not present quantitative robustness metrics.",
            "uuid": "e756.2"
        },
        {
            "name_short": "TCDF",
            "name_full": "Temporal Causal Discovery Framework (TCDF)",
            "brief_description": "A deep-learning-based approach (attention-based convolutional neural networks) for causal discovery in time series that relaxes linearity assumptions and can infer directed lagged and instantaneous relations.",
            "citation_title": "Causal discovery with attention-based convolutional neural networks",
            "mention_or_use": "mention",
            "method_name": "TCDF",
            "method_description": "TCDF trains attention-based convolutional neural networks to predict target series from past values of candidate causes; attention weights and causality scores are used to infer directed causal links without assuming linearity. The approach uses deep models (with several hyperparameters) and methods to interpret network weights/attention for causal inference.",
            "environment_name": "Observational multivariate time series (nonlinear regimes)",
            "environment_description": "General non-linear time series domains where underlying relationships may be complex; approach is data-driven and lacks explicit active experimentation in the survey description.",
            "handles_distractors": true,
            "distractor_handling_technique": "Model-based identification using attention and predictive importance — survey states TCDF can detect instantaneous hidden confounders (per summary table), implying use of model inspection/attention to identify anomalies suggesting confounding.",
            "spurious_signal_types": "Instantaneous hidden confounders (reported in survey), nonlinear noise-induced spurious associations",
            "detection_method": "Implicit detection via model-based signals (attention weights, predictive residuals) and auxiliary procedures claimed in referenced TCDF work to identify instantaneous hidden confounding patterns; survey does not detail algorithmic steps for refutation.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "TCDF relaxes linearity using deep networks and is noted in the survey as being able to detect instantaneous hidden confounders (according to the survey's summary table), but the extended abstract does not provide further methodological detail or numerical robustness results.",
            "uuid": "e756.3"
        },
        {
            "name_short": "VarLiNGAM",
            "name_full": "Vector Linear Non-Gaussian Acyclic Model (VarLiNGAM)",
            "brief_description": "A noise-based identifiability method that uses non-Gaussianity of residuals in a vector autoregressive model to identify causal directions in time series under linearity but non-Gaussian noise.",
            "citation_title": "Estimation of a structural vector autoregression model using non- gaussianity",
            "mention_or_use": "mention",
            "method_name": "VarLiNGAM",
            "method_description": "VarLiNGAM fits a structural vector autoregression under the assumption of linear relationships and non-Gaussian, independent noise; identifiability of the causal structure is achieved by exploiting higher-order statistics (non-Gaussianity) of residuals, typically using ICA-like procedures.",
            "environment_name": "Observational multivariate time series (linear, non-Gaussian noise)",
            "environment_description": "Time series domains where linear generative models with non-Gaussian innovations are plausible; not interactive or experimental per survey.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Can help disambiguate causal direction in presence of Gaussianity-related symmetry issues, but does not explicitly target hidden confounders or irrelevant distractors (assumes causal sufficiency).",
            "detection_method": "Uses non-Gaussianity tests/ICA-based methods to identify causal ordering via residual independence properties.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "VarLiNGAM provides identifiability under linear non-Gaussian assumptions by leveraging noise structure, but it assumes causal sufficiency and therefore does not explicitly handle distractors due to unobserved confounding.",
            "uuid": "e756.4"
        },
        {
            "name_short": "TiMINo",
            "name_full": "Time-series Identifiable Models with INdependent Noise (TiMINo)",
            "brief_description": "A noise-based method using restricted structural equation models (e.g., additive noise models) to identify causal relations in time series by exploiting independence between noise and causes in semi-parametric settings.",
            "citation_title": "Causal inference on time series using restricted structural equation models",
            "mention_or_use": "mention",
            "method_name": "TiMINo",
            "method_description": "TiMINo fits restricted structural equation models (often additive noise models) for each candidate causal direction and tests whether residuals are independent of predictors; causal directions that yield independent residuals are preferred, allowing identifiability under certain model classes.",
            "environment_name": "Observational multivariate time series (semi-parametric additive noise models)",
            "environment_description": "Time series settings where nonlinear additive-noise models are plausible; observational (non-interventional) datasets.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Addresses model-based ambiguities (e.g., directionality under additive noise) but assumes causal sufficiency (does not handle hidden confounders explicitly).",
            "detection_method": "Detects spurious causal proposals by testing dependence between fitted residuals and candidate predictors; failure of independence suggests model misspecification or confounding.",
            "downweighting_method": null,
            "refutation_method": "Refutation achieved by rejecting causal directions that do not produce independent residuals under the assumed model class.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "TiMINo exploits noise independence to identify causal directions in semi-parametric settings; survey notes it assumes causal sufficiency and minimality, so it is not designed to explicitly handle distractors from hidden confounding.",
            "uuid": "e756.5"
        },
        {
            "name_short": "DYNOTEARS",
            "name_full": "DYNOTEARS",
            "brief_description": "A score-based method that extends the NOTEARS continuous optimization approach to time series, jointly estimating instantaneous and time-lagged linear relationships by minimizing a penalized loss with an acyclicity constraint.",
            "citation_title": "Dynotears: Structure learning from time-series data",
            "mention_or_use": "mention",
            "method_name": "DYNOTEARS",
            "method_description": "DYNOTEARS formulates estimation of instantaneous and lagged adjacency matrices as a continuous optimization problem minimizing penalized squared residual loss (Frobenius norm) subject to differentiable acyclicity constraints; uses regularization to control complexity and infers a linear model structure.",
            "environment_name": "Observational multivariate time series (linear-model assumption)",
            "environment_description": "Time series domains where linear models are appropriate; not interactive and does not perform active interventions in the survey context.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Regularization can reduce overfitting from irrelevant variables but DYNOTEARS assumes causal sufficiency and does not explicitly target hidden confounders.",
            "detection_method": null,
            "downweighting_method": "Regularization/penalized loss to discourage spurious weak edges (implicit downweighting), but no explicit distractor-detection mechanism described in survey.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "DYNOTEARS jointly recovers instantaneous and lagged linear relationships via constrained optimization and regularization; the survey notes model reliance and causal sufficiency assumptions limit its explicit handling of distractors like hidden confounders.",
            "uuid": "e756.6"
        },
        {
            "name_short": "MixedNoise+Constraint",
            "name_full": "A mixed noise and constraint-based approach to causal inference in time series",
            "brief_description": "A hybrid method combining noise-based and constraint-based ideas to leverage strengths of both paradigms for time-series causal discovery, aiming to improve robustness across model violations.",
            "citation_title": "A mixed noise and constraint-based approach to causal inference in time series",
            "mention_or_use": "mention",
            "method_name": "Mixed noise and constraint-based approach (Assaad et al., 2021)",
            "method_description": "This hybrid approach integrates noise-model-based identification (e.g., exploiting residual independence or non-Gaussianity) with constraint-based conditional independence testing/rules to improve causal discovery reliability when pure methods fail; intended to combine complementary detection/refutation signals from noise and CI perspectives.",
            "environment_name": "Observational multivariate time series (general)",
            "environment_description": "Designed for observational time series across a variety of domains; hybridizes model-based and CI-based components rather than operating in an interactive experimental environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Combines residual/noise-based tests (for directionality and model-checking) with conditional-independence-based conditioning to detect and mitigate spurious associations and some forms of confounding.",
            "spurious_signal_types": "Indirect associations, model-misspecification-induced spurious links, some confounding scenarios (depending on components used); aims to be more robust to distractors than pure methods.",
            "detection_method": "Jointly uses independence tests on residuals (noise-based) and conditional independence procedures (constraint-based) to flag candidate spurious links; specifics are in the cited work rather than the survey abstract.",
            "downweighting_method": null,
            "refutation_method": "Refutation performed via cross-validation of signals from both noise-based and constraint-based modules (i.e., an edge is refuted if inconsistent across identification modes); survey does not give algorithmic detail in the abstract.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Survey highlights hybrid methods (including this mixed noise + constraint approach) as sometimes yielding better results than pure families; indicates hybrids are a promising route to handle spurious signals, but the extended abstract provides no numeric comparisons.",
            "uuid": "e756.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Detecting and quantifying causal associations in large nonlinear time series datasets",
            "rating": 2,
            "sanitized_title": "detecting_and_quantifying_causal_associations_in_large_nonlinear_time_series_datasets"
        },
        {
            "paper_title": "Causal network inference by optimal causation entropy",
            "rating": 2,
            "sanitized_title": "causal_network_inference_by_optimal_causation_entropy"
        },
        {
            "paper_title": "On causal discovery from time series data using fci",
            "rating": 2,
            "sanitized_title": "on_causal_discovery_from_time_series_data_using_fci"
        },
        {
            "paper_title": "Causal discovery with attention-based convolutional neural networks",
            "rating": 2,
            "sanitized_title": "causal_discovery_with_attentionbased_convolutional_neural_networks"
        },
        {
            "paper_title": "Estimation of a structural vector autoregression model using non- gaussianity",
            "rating": 2,
            "sanitized_title": "estimation_of_a_structural_vector_autoregression_model_using_non_gaussianity"
        },
        {
            "paper_title": "Causal inference on time series using restricted structural equation models",
            "rating": 2,
            "sanitized_title": "causal_inference_on_time_series_using_restricted_structural_equation_models"
        },
        {
            "paper_title": "Dynotears: Structure learning from time-series data",
            "rating": 2,
            "sanitized_title": "dynotears_structure_learning_from_timeseries_data"
        },
        {
            "paper_title": "A mixed noise and constraint-based approach to causal inference in time series",
            "rating": 2,
            "sanitized_title": "a_mixed_noise_and_constraintbased_approach_to_causal_inference_in_time_series"
        },
        {
            "paper_title": "Reconstructing regime-dependent causal relationships from observational time series",
            "rating": 1,
            "sanitized_title": "reconstructing_regimedependent_causal_relationships_from_observational_time_series"
        },
        {
            "paper_title": "Causal inference from noise",
            "rating": 1,
            "sanitized_title": "causal_inference_from_noise"
        }
    ],
    "cost": 0.014907,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Survey and Evaluation of Causal Discovery Methods for Time Series (Extended Abstract) *</p>
<p>Charles K Assaad cassaad@easyvista 
Univ. Grenoble Alpes
CNRS
LIG 2 EasyVistaGrenoble INP</p>
<p>Emilie Devijver emilie.devijver@univ-grenoble-alpes.fr 
Univ. Grenoble Alpes
CNRS
LIG 2 EasyVistaGrenoble INP</p>
<p>Eric Gaussier eric.gaussier@imag.fr 
Univ. Grenoble Alpes
CNRS
LIG 2 EasyVistaGrenoble INP</p>
<p>Survey and Evaluation of Causal Discovery Methods for Time Series (Extended Abstract) *</p>
<p>We introduce in this survey the major concepts, models, and algorithms proposed so far to infer causal relations from observational time series, a task usually referred to as causal discovery in time series. To do so, after a description of the underlying concepts and modelling assumptions, we present different methods according to the family of approaches they belong to: Granger causality, constraint-based approaches, noise-based approaches, score-based approaches, logic-based approaches, topology-based approaches, and difference-based approaches. We then evaluate several representative methods to illustrate the behaviour of different families of approaches. This illustration is conducted on both artificial and real datasets, with different characteristics. The main conclusions one can draw from this survey is that causal discovery in times series is an active research field in which new methods (in every family of approaches) are regularly proposed, and that no family or method stands out in all situations. Indeed, they all rely on assumptions that may or may not be appropriate for a particular dataset.</p>
<p>Introduction</p>
<p>Causality plays a central role in science and has been the subject of many debates among philosophers, biologists, mathematicians and physicists, to name but a few. Causality is implicit in the logic and structure of ordinary language and is embedded in our understanding mechanism that pushes humans to invoke questions. Why is it dark? Why is the sea salty? What is the effect of exercise on heart rate, of a vaccine on a particular disease? What is the effect of industrial pollution on the environment? And so, as already advocated by Spirtes, Glymour and Scheines, in attempting to answer such questions, both the baby and the scientist try to turn observations into causal knowledge [Spirtes et al., 2001]. Causality is indeed crucial for explanatory purposes since an effect can * The full paper is published in Journal of Artificial Intelligence Research, 73:767-820, August 2022. be explained by its causes, regardless of the correlations it may have with other variables.</p>
<p>The recent decades have seen the development, from philosophers, mathematicians, and computer scientists, of different models and methods to infer causal relations from data and to reason on the basis of these relations (to, e.g., predict the effect of changing a particular medication). If the first studies were dedicated to non temporal data, more and more studies now focus on time series. Indeed, time series arise as soon as observations, from sensors or experiments, for example, are collected over time. They are present in various forms in many different domains, as healthcare (through, e.g., monitoring systems), Industry 4.0 (through, e.g., predictive maintenance and industrial monitoring systems), surveillance systems (from images, acoustic signals, seismic waves, etc.) or energy management (through, e.g. energy consumption data). The number of scientific publications dedicated to causality in time series as well as the number of tools developed in this context have steadily increased to a point that it is difficult for non specialists to grasp the most important approaches proposed so far.</p>
<p>The goal of our survey is twofold: On the one hand, we want to introduce the major concepts, models, methods, and associated algorithms proposed so far to infer causal relations from observational time series, a task usually referred to as causal discovery; on the other hand, we want to assess how different methods for causal discovery in time series behave in practice. Several surveys on causal discovery have recently been proposed [Guo et al., 2020;Nogueira et al., 2021;Glymour et al., 2019;Vowels et al., 2021]. However, most of them do not discuss time series and when they do, they focus on Granger causality. In contrast, our survey is dedicated to causal discovery in time series and reviews all families of approaches proposed in this area.</p>
<p>The remainder of this extended abstract is organized as follows. After a brief description of the main concepts and assumptions in Section 2, we briefly present some of the different algorithms that we presented in our survey according to the family of approaches they belong to and summarize the main characteristics of these algorithms. Section 4 points out some aspect of causal discovery from time series that are not included in the survey. Lastly, Section 5 concludes this extended abstract.
X s t−2 X s t−1 X s t X p t−2 X p t−1 X p t X q t−2 X q t−1 X q t X r t−2 X r t−1 X r t X s t+1 X s t+2 X p t+1 X p t+2 X q t+1 X q t+2 X r t+1 X r t+2 Full Time Causal Graph (a) X s t−1 X s t X p t−1 X p t X q t−1 X q t X r t−1 X r t Window Causal Graph (b) X p X s X q X r
Summary Causal Graph (c) Figure 1: Different causal graphs to respresent a diamond structure with self causes: full time causal graph (5a), window causal graph (5b) and summary causal graph (5c). Note that the first one gives more information but cannot be inferred in practice, the second one is a schematic viewpoint of the full behavior, whereas the last one give an overview and can be deduced from the window causal graph.</p>
<p>Background</p>
<p>Causal discovery in time series aims at discovering, from observational data, causal relations within and between dvariate time series X where, for a fixed t, each X t is a vector (X 1 t , · · · , X d t ) in which each variable X p t represents a measurement of the p-th time series at time t. There are at least three ways to represent time series through a causal graph G = (V, E) with V the set of vertices and E the set of edges. The first is called a full time causal graph and represents a complete acyclic graph of the dynamic system, as illustrated in Figure 1a.</p>
<p>Definition 1 (Full Time Causal Graph). Let X be a multivariate discrete-time stochastic process and G = (V, E) the associated full time causal graph. The set of vertices in that graph consists of the set of components X 1 , . . . , X d at each time t. The edges E of the graph are defined as follows: variables X p t−i and X q t are connected by a lag-specific directed link X p t−i → X q t in G pointing forward in time if and only if X p causes X q at time t with a time lag of i &gt; 0 for p = q and with a time lag of i ≥ 0 for p ̸ = q.</p>
<p>It is usually not possible to infer general full time causal graphs as there usually is a single observation for each time series at each time instant and it is common to rely on the so-called Consistency Throughout Time assumption.</p>
<p>Definition 2 (Consistency Throughout Time). A causal graph G = (V, E) for a multivariate time series X is said to be consistent throughout time if all the causal relationships remain constant in direction throughout time.</p>
<p>When assuming consistency throughout time, the full time causal graph can be contracted to give a finite graph which we call window causal graph. It is a representation of the causal graph through a time window, the size of which equals the maximum lag relating time series in the full time causal graph.</p>
<p>Definition 3 (Window Causal Graph). Let X be a multivariate discrete-time stochastic process and G = (V, E) the associated window causal graph for a window of size τ . The set of vertices in that graph consists of the set of components X 1 , . . . , X d at each time t, . . . , t + τ . The edges E of the graph are defined as follows: variables X p t−i and X q t are connected by a lag-specific directed link X p t−i → X q t in G pointing forward in time if and only if X p causes X q at time t with a time lag of 0 ≤ i ≤ τ for p ̸ = q and with a time lag of 0 &lt; i ≤ τ for p = q. Figure 1b illustrates a window causal graph corresponding to the full time causal graph given in Figure 1a. In practice, it is often sufficient to know the causal relations between time series as a whole, without knowing precisely the relations between time instants, in addition, in some applications, an expert would like to validate a causal graph before using it, but validating a window causal graph can be difficult as it is difficult to determine the temporal lag between a cause and an effect. In these cases, one one can use a summary causal graph. An example of such a graph is given in Figure 1c. Note that since a summary causal graph is a abstraction of the full time causal graph, it can contain cycles. Definition 4 (Summary Causal Graph). Let X be a multivariate discrete-time stochastic process and G = (V, E) the associated summary causal graph. The set of vertices in that graph consists of the set of time series X 1 , . . . , X d . The edges E of the graph are defined as follows: variables X p and X q are connected if and only if there exists some time t and some time lag i such that X p t−i causes X q t at time t with a time lag of 0 ≤ i for p ̸ = q and with a time lag of 0 &lt; i for p = q.</p>
<p>The relations between a probability distribution and its causal graph are central to the construction of the graph. It is however not always possible to infer a causal graph solely from observational data on which one can only compute correlations and statistical independencies. For this reason, in addition to the acyclicity of the full time causal graph and consistency throughout time, all methods rely on at least two of the following assumptions:</p>
<p>• Stationarity, which states that the generative process does not change with respect to time; • Causal Markov condition [Spirtes et al., 2001;Pearl, 2000], which states that every variable is independent of all its nondescendants in the graph conditional on its parents; • Causal sufficiency [Spirtes et al., 2001;Pearl, 2000], which states that all common causes, i.e., confounders, of all observed variables are observed;</p>
<p>• Minimality [Spirtes et al., 2001], which requires that all adjacent nodes are dependent;</p>
<p>• Faithfulness [Spirtes et al., 2001;Pearl, 2000], which states that all conditional independencies are entailed from the causal Markov condition;</p>
<p>• Semi-parametric models [Peters et al., 2017], which stipulates a general form for the underlying model, as linear models or nonlinear additive noise models;</p>
<p>• Temporal priority [Hume, 1738], which makes the process of causality asymmetric in time and is useful for orienting a causal relation when one knows that two variables are causally related, as well as its relaxed version which allows for instantaneous relations as the difference in time between two values of two time series may not be observed if the sampling frequencies of the time series are small.</p>
<p>Diffferent Families of Approaches for Causal Discovery</p>
<p>We now turn to the most widely used approaches used to infer causal graphs between time series. Additional approaches can be found in the complete survey.</p>
<p>Granger causality is one of the oldest concepts in causal inference, based on a statistical version of Hume's regularity theory [Hume, 1738] which states that causal relations can be inferred by the experience of constant conjunctions between causes and effects, a cause preceding its effects. Assuming stationary linear systems, one can assess whether X p Granger-causes X q by considering two autoregression models: an autoregressive restricted model that uses only past values of X q to predict its current value and an augmented version of the autoregressive model that uses both past values of X q and X p to predict the current value of X q . If the augmented version is significantly more accurate than the restricted model, one can conclude that X p Granger-causes X q . In a multivariate setting, a pairwise analysis can be performed using the bivariate approach denoted as PWGC. This approach does however not fully capture Granger's original ideas which assume that all relevant information is included in the analysis [Eichler, 2008]. To include all relevant information in the analysis, the multivariate Granger causality denoted as MVGC [Geweke, 1982;Chen et al., 2004;Barrett et al., 2010] was introduced. In MVGC, the restricted and augmented models are both based on a vector autoregressive instead of a simple autoregressive model, where the augmented model uses all observational time series whereas the restricted model uses all time series except X p . Analogously to the bivariate case, if the augmented model is significantly more accurate than the restricted model, one concludes that X p Granger-causes X q . Note that several extensions of the above approach have been proposed, as the temporal causal discovery model, denoted as TCDF [Nauta et al., 2019], which dispenses with the linear assumption made in the original proposal.</p>
<p>Constraint-based approaches exploit conditional independencies to build a skeleton between variables. This skele-ton is then oriented either according to temporal priority or according to a set of rules that define constraints on admissible orientations while assuming faithfulness. oCSE [Sun et al., 2015] algorithm uses the causation entropy to find these conditional independencies under the assumption that all causal relations have a time-lag of size 1. Due to this assumption, temporal priority is sufficient to orient all edges and the summuary causal graph gives the same information as the window causal graph. However, time-lag of size 1 is not always satisfied in real world scenarios, so the PCMCI algorithm [Runge et al., 2019] was introduced to detect time lagged causal relations in the form of a window causal graph. Note that PCMCI can be flexibly combined with any kind of conditional independence tests. Instantaneous causal relations, which were not supported in the initial algorithm, have been integrated to PCMCI [Runge, 2020] by conducting separately the edge removal for lagged conditioning sets and instantaneous conditioning sets. Lagged relations are treated as in PCMCI and instantaneous relations are oriented using the known PC-rules [Spirtes et al., 2001] which were introduced for non temporal graphs. Both of the above algorithms assume causal sufficiency, however, there exists constraintbased algorithms, such as tsFCI [Entner and Hoyer, 2010], that do not need this assumption.</p>
<p>Noise-based approaches do not consider that statistical noise is as a nuisance that one has to live with. Instead, they consider it as a valuable source of insight to identify causal relations Climenhaga et al., 2019]. Even though it has been shown that, in a non parametric setting, the noise does not help to distinguish between a cause and its effects [Peters et al., 2017], this is no longer the case when using specific semi-parametric models as a a linear model with non-Gaussian noise [Shimizu et al., 2006], or a nonlinear additive noise model . Therefore, the Var-LiNGAM [Hyvärinen et al., 2010] algorithm was proposed for uniquely identifying causal structures based on purely observational time series assuming a linear model with non-Gaussian noise. Similarly, TiMINo [Peters et al., 2013] was introduced for nonlinear additive noise models. Both algorithms assume causal sufficiency and the minimality condition, which is a weaker version of faithfulness.</p>
<p>Score-based approaches aim to find the graph that best matches the data based on a score that typically strikes a balance between the likelihood of the data given the network and a penalty term related to the complexity of the network. Assuming causal sufficiency as well as linearity, the DYNOTEARS [Pamfil et al., 2020] algorithm was proposed to simultaneously estimate instantaneous and time-lagged relationships between time series. This algorithm relies on minimizing a penalized loss based on the Frobenius norm of the residuals of a linear model. Hyper-parameters  and Mishra, 2009;Kleinberg, 2011;Kleinberg, 2015] consider qualitative and mixed data. In addition, in each family of approaches, we presented what we think are the most known algorithms but one should bear in mind that there exists many extensions of these algorithms. For example, in the constraint-based family, the idea behind the oCSE algorithm was extended to handle lags of size different from 1, instantaneous relations as well as a method to directly infer the summary causal graph without going through a window causal graph [Assaad et al., 2022a;Assaad et al., 2022b]. In the noise based-family, identifiability was shown to be also possible under a post nonlinear additive noise model [Zhang and Hyvärinen, 2009]. Finally, it is worth noting that we did not include hybrid methods [Malinsky and Spirtes, 2018;Sanchez-Romero et al., 2019;Assaad et al., 2021] which sometimes yield better results than purebred methods.</p>
<p>Summary
PWGC S ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✓ ✓ Granger MVGC S ✗ ✓ ✗ ✓ ✗ ✗ ✓ ✓ ✓ TCDF W ✓ ⊕ ✓ ✓ ⊕ ✓ ✓ ✗ ✓ ✗ ✗ Constraint-based PCMCI W F ✓ ✗ † ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓ oCSE S F ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✓ tsFCI W F ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✓ Noise-based VarLiNGAM W M ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ TiMINo S M ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✓ ✗ ✓ Score-based DYNOTEARS W ✓ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓</p>
<p>Conclusion</p>
<p>Our survey presents different algorithms, pertaining to different families of approaches, for causal discovery in time series. The families we have retained here correspond to approachesà la Granger, constraint-based approaches, noisebased approaches and score-based approaches. Further details on those algorithms and their evaluation, as well as on other families of approaches (namely, logic-based approaches, topology-based approaches, and difference-based approaches) and associated algorithms can be found in the full paper [Assaad et al., 2022c]. In a nutshell, one can draw from our survey that causal discovery in times series is an active research field in which new methods (in every family of approaches) are regularly proposed, and that no family or algorithm stands out in all situations. Indeed, they all rely on assumptions that may or may not be appropriate for a particular dataset.</p>
<p>Table 1
1displays the main characteristics of representative algorithms in the above families. As one can note,Section </p>
<p>Algorithm 
Causal graph 
Faithfulness / Minimality </p>
<p>Causal Markov Condition 
Instantaneous rel. </p>
<p>Lag &gt; 1 
Inference of self causes 
Confounders </p>
<p>Inst. Hidden Conf 
. 
Lagged Hidden </p>
<p>Conf. </p>
<p>Model based 
Linear model </p>
<p>&lt; 5 </p>
<p>Table 1 :
1Summary of the main characteristics of representative algorithms in all the families discussed in this survey.to the window causal graph. Roughly only half of the algorithms address the problem of discovering instantaneous relations. Most algorithms can detect confounders. However, only TCDF can detect instantaneous hidden confounders and only tsFCI can detect lagged hidden confounders. More generally, very few algorithms can deal with hidden variables, which violates the causal sufficiency assumption. Regarding the type of underlying models, almost all algorithms rely on a particular model (except PCMCI and oCSE). Among the algorithms relying on a model, roughly half of them rely on a linear model. Relying on a specific model can be an advantage when the data considered arises from a similar model. It can be of course a disadvantage when this is not the case and when the model family considered is not a universal approximator. Lastly, as one can note, most algorithms have few (less than 5) hyper-parameters, with the exception of TCDF which is based on deep neural networks.Our survey reviews different causal discovery algorithms in the setting where consistency throughout time as well as other technical conditions are satisfied. However, it is important to note that there exists algorithms that relax these conditions:[Huang et al., 2015;Huang et al., 2020; Saggioro et al., 2020]   relax the consistency throughout time constraint,[Danks and  Plis, 2013;Gong et al., 2015] allow for temporal aggregation or subsampling, and [KleinbergFor causal graphs, </p>
<p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal Track
algorithms infer a summary causal graph and 5 infer a window causal graph. It is of course possible to deduce the summary causal graph from a window causal graph and, in the case of oCSE, the summary causal graph is equivalent Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal Track</p>
<p>A mixed noise and constraint-based approach to causal inference in time series. Assaad, Machine Learning and Knowledge Discovery in Databases. Research Track. Nuria Oliver, Fernando Pérez-Cruz, Stefan Kramer, Jesse Read, and Jose A. LozanoChamSpringer International PublishingReferences [Assaad et al., 2021] Charles K. Assaad, Emilie Devijver, Eric Gaussier, and Ali Ait-Bachir. A mixed noise and constraint-based approach to causal inference in time series. In Nuria Oliver, Fernando Pérez-Cruz, Stefan Kramer, Jesse Read, and Jose A. Lozano, editors, Ma- chine Learning and Knowledge Discovery in Databases. Research Track, pages 453-468, Cham, 2021. Springer In- ternational Publishing.</p>
<p>Survey and evaluation of causal discovery methods for time series. Assaad, PMLR, 01- 05Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence. 2022. [Assaad et al., 2022c] Charles K. Assaad, Emilie Devijver, and Eric Gaussierthe Thirty-Eighth Conference on Uncertainty in Artificial Intelligence180J. Artif. Int. Res.[Assaad et al., 2022a] Charles K. Assaad, Emilie Devijver, and Eric Gaussier. Discovery of extended summary graphs in time series. In James Cussens and Kun Zhang, editors, Proceedings of the Thirty-Eighth Conference on Uncer- tainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, pages 96-106. PMLR, 01- 05 Aug 2022. [Assaad et al., 2022b] Charles K. Assaad, Emilie Devijver, and Eric Gaussier. Entropy-based discovery of summary causal graphs in time series. Entropy, 24(8), 2022. [Assaad et al., 2022c] Charles K. Assaad, Emilie Devijver, and Eric Gaussier. Survey and evaluation of causal dis- covery methods for time series. J. Artif. Int. Res., 73, apr 2022.</p>
<p>Nevin Climenhaga, Lane DesAutels, and Grant Ramsey. Causal inference from noise. Noûs. [ Barrett, Advances in Neural Information Processing Systems, Workshop on Causality. 81Learning causal structure from undersampled time series[Barrett et al., 2010] Adam B. Barrett, Lionel C. Barnett, and Anil K. Seth. Multivariate granger causality and gen- eralized variance. Physical review E, 81:041907, 2010. [Chen et al., 2004] Yonghong Chen, Govindan Rangarajan, Jianfeng Feng, and Mingzhou Ding. Analyzing multi- ple nonlinear time series with extended granger causality. Physics Letters A, 324:26-35, 2004. [Climenhaga et al., 2019] Nevin Climenhaga, Lane DesAu- tels, and Grant Ramsey. Causal inference from noise. Noûs, 2019. [Danks and Plis, 2013] David Danks and Sergey Plis. Learn- ing causal structure from undersampled time series. In Ad- vances in Neural Information Processing Systems, Work- shop on Causality, 2013.</p>
<p>Causal inference from time series: What can be learned from granger causality? Proceedings from the 13th International Congress of Logic. Michael Eichler, Doris Eichler, Patrik O Entner, Hoyer, Proceedings of the Fifth European Workshop on Probabilistic Graphical Models (PGM-2010). the Fifth European Workshop on Probabilistic Graphical Models (PGM-2010)Helsinki, FinlandHIIT PublicationsOn causal discovery from time series data using fciEichler, 2008] Michael Eichler. Causal inference from time series: What can be learned from granger causality? Pro- ceedings from the 13th International Congress of Logic, Methodology and Philosophy of Science, 2008. [Entner and Hoyer, 2010] Doris Entner and Patrik O. Hoyer. On causal discovery from time series data using fci. In Proceedings of the Fifth European Workshop on Proba- bilistic Graphical Models (PGM-2010), pages 121-128, Helsinki, Finland, 2010. HIIT Publications.</p>
<p>Biwei Huang, Kun Zhang, and Bernhard Schölkopf. Identification of time-dependent causal model: A gaussian process treatment. John Geweke, ; Glymour, PMLRMeasurement of linear dependence and feedback between multiple time series. Journal of the American Statistical Association. Lille, France; Palo Alto, California USAAAAI Press77J. Mach. Learn. Res., 1982] John Geweke. Measurement of linear dependence and feedback between multiple time se- ries. Journal of the American Statistical Association, 77(378):304-313, 1982. [Glymour et al., 2019] Clark Glymour, Kun Zhang, and Pe- ter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in Genetics, 10:524, 2019. [Gong et al., 2015] Mingming Gong, Kun Zhang, Bernhard Schölkopf, Dacheng Tao, and Philipp Geiger. Discover- ing temporal causal relations from subsampled data. vol- ume 37 of Proceedings of Machine Learning Research, pages 1898-1906, Lille, France, 2015. PMLR. [Guo et al., 2020] Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and Huan Liu. A survey of learning causality with data: Problems and methods. ACM Com- put. Surv., 53(4), July 2020. [Hoyer et al., 2009] Patrik O. Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, edi- tors, Advances in Neural Information Processing Systems 21, pages 689-696. Curran Associates, Inc., 2009. [Huang et al., 2015] Biwei Huang, Kun Zhang, and Bern- hard Schölkopf. Identification of time-dependent causal model: A gaussian process treatment. In 24th Interna- tional Joint Conference on Artificial Intelligence, Machine Learning Track, pages 3561-3568, Palo Alto, California USA, 2015. AAAI Press. [Huang et al., 2020] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark Gly- mour, and Bernhard Schölkopf. Causal discovery from heterogeneous/nonstationary data. J. Mach. Learn. Res., 21(89):1-53, 2020.</p>
<p>Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. David Hume, ; Hume, Hyvärinen, PMLRProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal Track. Sanchez-Romero et al., 2019] Ruben Sanchez-Romero, Joseph Ramsey, Kun Zhang, Madelyn Glymour, Biwei Huang, and Clark Glymourthe Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal TrackMontreal, Quebec; London, UK; New York, NY, USA; Cambridge, MA, USA; Peter Spirtes, Clark Glymour, and Richard Scheines; Dane Taylor, and Erik Bollt; Arlington, Virginia, USAAUAI Press11Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI '09Hume, 1738] David Hume. A Treatise of Human Nature. Oxford University Press, 1738. [Hyvärinen et al., 2010] Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, and Patrik O. Hoyer. Estimation of a structural vector autoregression model using non- gaussianity. Journal of Machine Learning Research, 11:1709-1731, 2010. [Kleinberg and Mishra, 2009] Samantha Kleinberg and Bud Mishra. The Temporal Logic of Causal Structures. In Pro- ceedings of the 25th Conference on Uncertainty in Artifi- cial Intelligence (UAI), Montreal, Quebec, 2009. [Kleinberg, 2011] Samantha Kleinberg. A logic for causal inference in time series with discrete and continuous vari- ables. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence -Volume Vol- ume Two, IJCAI'11, pages 943-950. AAAI Press, 2011. [Kleinberg, 2015] Samantha Kleinberg. Why: A Guide to Finding and Using Causes. O'Reilly Media, Incorporated, 2015. [Malinsky and Spirtes, 2018] Daniel Malinsky and Peter Spirtes. Causal structure learning from multivariate time series in settings with unmeasured confounding. In Pro- ceedings of 2018 ACM SIGKDD Workshop on Causal Dis- ocvery, volume 92 of Proceedings of Machine Learning Research, pages 23-47, London, UK, 2018. PMLR. [Nauta et al., 2019] Meike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolu- tional neural networks. Machine Learning and Knowledge Extraction, 1(1):312-340, 2019. Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal Track [Nogueira et al., 2021] A.R. Nogueira, J. Gama, and C.A. Ferreira. Causal discovery in machine learning: Theo- ries and applications. Journal of Dynamics &amp; Games, 8(3):203-231, 2021. [Pamfil et al., 2020] Roxana Pamfil, Nisara Sriwattana- worachai, Shaan Desai, Philip Pilgerstorfer, Konstanti- nos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In Silvia Chiappa and Roberto Calandra, editors, Proceed- ings of the Twenty Third International Conference on Ar- tificial Intelligence and Statistics, volume 108 of Proceed- ings of Machine Learning Research, pages 1595-1605. PMLR, 26-28 Aug 2020. [Pearl, 2000] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, NY, USA, 2000. [Peters et al., 2013] Jonas Peters, D. Janzing, and B. Schölkopf. Causal inference on time series using restricted structural equation models. In Advances in Neu- ral Information Processing Systems 26, pages 154-162, 2013. [Peters et al., 2017] Jonas Peters, D. Janzing, and B. Schölkopf. Elements of Causal Inference: Foun- dations and Learning Algorithms. MIT Press, Cambridge, MA, USA, 2017. [Runge et al., 2019] Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detect- ing and quantifying causal associations in large nonlinear time series datasets. Science Advances, 5(11), 2019. [Runge, 2020] Jakob Runge. Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. In Jonas Peters and David Sontag, editors, Proceedings of Machine Learning Research, vol- ume 124, pages 1388-1397. PMLR, 2020. [Saggioro et al., 2020] Elena Saggioro, Jana de Wiljes, Mar- lene Kretschmer, and Jakob Runge. Reconstructing regime-dependent causal relationships from observational time series. Chaos: An Interdisciplinary Journal of Non- linear Science, 30(11):113115, 2020. [Sanchez-Romero et al., 2019] Ruben Sanchez-Romero, Joseph Ramsey, Kun Zhang, Madelyn Glymour, Biwei Huang, and Clark Glymour. Estimating feedforward and feedback effective connections from fmri time series: As- sessments of statistical methods. Network Neuroscience, 3:274 -306, 2019. [Shimizu et al., 2006] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non- gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7:2003-2030, 2006. [Spirtes et al., 2001] Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2001. [Sun et al., 2015] Jie Sun, Dane Taylor, and Erik Bollt. Causal network inference by optimal causation entropy. SIAM Journal on Applied Dynamical Systems, 14(1):73- 106, 2015. [Vowels et al., 2021] Matthew J. Vowels, Necati Cihan Camgöz, and Richard Bowden. D'ya like dags? A sur- vey on structure learning and causal discovery. CoRR, abs/2103.02582, 2021. [Zhang and Hyvärinen, 2009] Kun Zhang and Aapo Hyvärinen. On the identifiability of the post-nonlinear causal model. In Proceedings of the Twenty-Fifth Con- ference on Uncertainty in Artificial Intelligence, UAI '09, page 647-655, Arlington, Virginia, USA, 2009. AUAI Press.</p>
<p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23. the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23) Journal Track</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270847724</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.15720v2.pdf" target="_blank">Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite LLM’s tremendous success, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open question and largely ununderstood. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks that include linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving reasoning multiple steps, where each step represent one task, models typically underperform, and scaling up generally provide no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at https://github.com/OliverXUZY/LLM Compose .</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7617",
    "paper_id": "paper-270847724",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0064505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability
11 Aug 2024</p>
<p>Zhuoyan Xu 
University of Wisconsin-Madison</p>
<p>Zhenmei Shi zhmeishi@cs.wisc.edu 
University of Wisconsin-Madison</p>
<p>Yingyu Liang yliang@cs.wisc.edu 
University of Wisconsin-Madison</p>
<p>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability
11 Aug 20246AA601B0116B3DD4746D28F450BEDD77arXiv:2407.15720v2[cs.CL]
Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities.Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence.Despite the tremendous success of LLMs, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open and largely underexplored question.In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples.We develop a test suite of composite tasks including linguistic and logical challenges and perform empirical studies across different LLM families.We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks involving reasoning multiple steps, where each step represents one task, models typically underperform, and scaling up generally provides no improvements.We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately.We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale.Our dataset and code are available at https://github.com/OliverXUZY/LLMCompose.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have revolutionized the natural language processing (NLP) and general AI community.Recent advances, including ChatGPT (OpenAI, 2022), GPT4 (OpenAI, 2023), and Claude 3 (Anthropic, 2024) have shown success in various fields.As model scale increases, larger models exhibit new behavior known as emergence ability.One remarkable emergence is the in-context learning ability (ICL) (Brown et al., 2020), where a model can solve new tasks given only a few examples as input, without any parameter updates.However, despite recent success, how LLMs solve complex reasoning tasks, particularly not seen in pre-training, remains an open question and largely lacks understanding.</p>
<p>In this paper, we focus on the problem of how LLMs tackle composite tasks that incorporate multiple simple tasks.Specifically, we investigate whether a model trained and in-context learned on individual tasks can effectively integrate these skills to tackle combined challenges, which are intuitive and simple for humans.For instance, in Figure 1, if a human is given examples where words following an asterisk (<em>) will be capitalized and words surrounded by parenthesis will be permuted, one can also understand words following an asterisk (</em>) surrounded by parenthesis will be capitalized and permuted simultaneously.This basic generalization seems trivial, yet we observe that LLMs fail to generalize in this way.</p>
<p>Figure 1: Inconsistent performance in GPT-4.Consider two simple tasks: If a word is followed by an asterisk (<em>), capitalize the letter.If two words are surrounded by parentheses, swap the positions.GPT-4 correctly solves two simple tasks based on demonstrations (left).The composite tasks have test inputs with both asterisk (</em>) and parenthesis.The correct answer should be output: SPORTS PIE.However, GPT-4 fails to solve the composite tasks (right).The same failure was observed in Claude 3.</p>
<p>Compositional ability is an active problem in the AI community and is crucial for advancing Artificial General Intelligence (AGI).Recent studies have made significant contributions to the understanding of this area.Dziri et al. (2023) formulate compositional tasks as computation graphs to quantify each task's complexity level.Power et al. (2022) show that models may develop generalization capabilities when trained extensively, beyond the point of overfitting, highlighting a phenomenon known as "grokking".An et al. (2023b) examines how LLMs acquire abstract reasoning and achieve compositional generalization in a linguistic context through ICL by testing LLMs on tasks that involve translating a formal language with a custom grammar.Although these studies offer insight, how LLMs compose tasks together is still not fully understood, especially in the ICL setting.Moreover, the absence of a solid theoretical framework in these discussions needs to be investigated concerning the underlying mechanisms of such behaviors.</p>
<p>Inspired by these seminal works, we further evaluate LLMs on a series of compositional tasks through ICL.The models were presented with examples of simple tasks and then asked to tackle composite tasks that they had not encountered during pretraining or in-context learning.We observe various behaviors: (1) for some composite tasks, the models showed a reasonable level of compositional skill, a capability that improved with larger model sizes; (2) for more complex composite tasks requiring sequential reasoning, the model struggle, and increasing the model size typically did not lead to better performance.</p>
<p>Our key intuition is that if the simple tasks that form a composite task can be easily separated into subtasks based on the inputs (e.g., performed separately on different parts of the input sentence), the model is more likely to complete such a composite task successfully (we call it "a separable composite task").The performance of the model depends on how it connects and uses the information given for each part of the task.To clarify this insight, we present theoretical analyses in a simplified setting and provide key insights into conditions needed for success in such separable composite tasks.</p>
<p>Our contributions are twofold.Empirically, we introduce a variety of composite tasks from both the linguistic and logical domains to explore how the nature of these tasks influences the compositional performance of LLMs through ICL.Theoretically, we provide analysis on a simple yet insightful model: a one-layer single-head linear self-attention network (Von Oswald et al., 2023;Aky ürek et al., 2023;Mahankali et al., 2023;Zhang et al., 2023b).This framework allows us to demonstrate a clear separation in input embedding, effectively breaking down composite tasks into simpler components.We delve into the scaling of language models by examining the structure of the key and query matrices in the attention mechanism, arguing that larger models with a more complex internal structure exhibit enhanced performance on individual tasks, thereby improving their overall compositional capabilities on such separable tasks.</p>
<p>Related Work</p>
<p>We briefly summarize related work and provide a detailed discussion in Appendix A.</p>
<p>LLMs are often Transformer-based (Vaswani et al., 2017) equipped with the enormous size of parameters and pretrained on vast training data.One key property that makes such LLM successful is called scaling law: Increasing the scale of language models (pretraining data scale, model parameters) can lead to better performance in downstream tasks.One ability that emerges as the model scale increases is in-context learning (ICL) (Brown et al., 2020).Given a sequence of labeled examples and a testing example (combined as a prompt), the model can construct new predictors for testing examples without further parameter updates Dong et al. (2022).Scaling law was first proposed by Kaplan et al. (2020) and then followed up by Hoffmann et al. (2022), emphasizing both the scale of models and training data.Recent works show LLMs with larger scales have distinct behaviors compared to smaller language models (Wei et al., 2023b;Shi et al., 2023b).This work investigates experiments and analyses how LLM can exhibit compositional ability in ICL.</p>
<p>Solving complex tasks and reasoning is an active problem in the AI community Huang &amp; Chang (2022).There is a line of empirical works investigating the compositional ability in linguistic fashion (Kim &amp; Linzen, 2020;Levy et al., 2022;An et al., 2023a;b).LLMs are capable of learning abstract reasoning (e.g., grammar) to perform new tasks when finetuned or given suitable in-context examples.In our work, we include linguistic experiments as part of our testing suite, illustrating LLMs' compositional ability.Ye et al. (2023); Berglund et al. (2023); Dziri et al. (2023) show LLMs will struggle to solve tasks requiring reasoning.Berglund et al. (2023) studies that LLMs trained on "A is B" fail to learn "B is A".In our work, we conduct similar experiments showing LLMs will fail on composite if different steps of logical rules are mixed.</p>
<p>Warm-up: A Failure Case for Composition</p>
<p>Our goal is to understand the behavior of LLMs on compositional reasoning tasks.As a warm-up, we evaluate the Capitalization &amp; Swap tasks (Figure 1) on different models.Recall the tasks: given words of common objects, * represents the operation of capitalizing the letters, () represents swapping the positions of the two words.We consider the standard in-context learning setting, which concatenates input-output examples K = 10 and one test input as the prompt for LLM.We perform experiments across various LLM families, e.g., Llama families (Touvron et al., 2023) and GPTs (Radford et al., 2019;Black et al., 2021)</p>
<p>Results.</p>
<p>In Figure 2, somewhat surprisingly, we observe that LLMs cannot solve the composite task, although they perform well on simple tasks.There is a significant gap between the performance in these settings.Models in Llama families can solve capital and swap with nearly ∼90% accuracy but only achieve around 20% or below on the composite task.We also observe that composite in-context examples will significantly improve the performance: The accuracy of Llama families can go up to match the simple task accuracy.These observations show that the models fail to compose the knowledge from the simple tasks, although they do have the representation power to solve the composite task (which can only be exploited when provided composite in-context examples) and scaling up may not help.The exact match accuracy (y-axis) vs the model scale (x-axis, "b" stands for billion) for Capitalization &amp; Swap tasks (example in Figure 1).Line capital: performance on the simple task of capitalization; swap: on the simple task of swap; composite: in-context examples are from simple tasks while test input from the composite task.composite incontext: in-context examples and test input are all from the composite task (example in Table 1).</p>
<p>Variability of Compositional Performance</p>
<p>The experiment on Capitalization &amp; Swap shows failure cases while existing studies reported some successful composite abilities (Levy et al., 2022;An et al., 2023b).This observation suggests a more refined perspective: LLMs exhibit variable compositional abilities, excelling in certain composite tasks while struggling in others.This section expands our exploration to additional composite tasks to further examine and understand this variability.</p>
<p>We introduce more composite tasks, including linguistic and logical challenges, wrapped as a testing suite.Similar to the Capitalization &amp; Swap experiment, we design composite tasks that compose two simple tasks and evaluate the model in four settings: the two simple tasks, the composite setting, and the composite in-context setting (Table 1 show examples for the latter two).We consider two kinds of task: logical rules and linguistic translation.We first choose two simple tasks and compose them to construct a composite task.</p>
<p>To address concerns about data leakage and the possibility that models encounter similar tasks during pretraining, we opt for synthetic data in this work.While it is challenging to guarantee that test data has never been seen during pretraining, we take significant steps to mitigate this risk.Specifically, we construct our compositional test data using a unique syntax and mapping mechanism.This approach substantially shifts the data distribution away from existing web-scale data, making it highly improbable that our test data has been encountered during pretraining.By doing so, we aim to create novel composite tasks that comprehensively evaluate the models' compositional abilities.Section 3.1 investigates logical tasks and Section 3.2 investigates translation tasks.</p>
<p>We perform experiments to answer the following questions: (Q1) How do LLMs perform in various tasks, where models might perform well in some scenarios while failing in others?(Q2) Does scaling up the model help in general?(Q3) Is the variability in performance relevant to the nature of tasks?Our experiments provide the following answers: (A1) A pattern of variable performance is observable across a range of composite tasks.(A2) Scaling-up helps when the model exhibits compositional ability for certain tasks but may not help when the model initially struggles.(A3) In tasks that involve processing inputs from varied segments or perspectives, especially simpler ones, the model tends to demonstrate compositional capabilities.</p>
<p>Composite Logical Rules</p>
<p>We enhance our suite of logical tasks by introducing a series of straightforward tasks that process either simple words or numerical values, with the output being a specific functional transformation of the input.These tasks are detailed in Table 2.  Composite tasks are created by merging two simple tasks.We conceptualize simple tasks as functions, f (•) and g(•) that map inputs to their respective outputs.We identify two distinct approaches to creating composite tasks: (1) Compose by parts: For inputs x, y, the result is f (x), g(y).One example is (A) + (F) in Table 3.If a numerical number is given, it will increment by one; if the word is given, the letters will be capitalized; if both are given, perform both operations.</p>
<p>(2) Compose by steps: Given input x, the result is f (g(x)).One example is (A) + (B) in Table 3.We use customized symbols as function mapping for composing two simple tasks.</p>
<p>Examples are in Figure 1 and Table 3.Following existing work, we use exact match accuracy to evaluate the performance since the output for these tasks is usually simple and short.</p>
<p>Results.We provide our main results on composite tasks in Table 4.For the composed by parts tasks (A) + (F) and (D) + (F), the models show strong compositional ability: the composite accuracy is high, improves with increasing scale, and eventually reaches similar performance as the "gold standard" composite in-context setting, as highlighted in red numbers.We refer to these tasks as "separable composite tasks", which are relatively easy for the model to solve.On the compose-by-step tasks, we observe the models have various performances.For composite tasks with sequential reasoning steps, the models exhibit various performances.For tasks involving capitalization (A) or swap (B), the model has poor performance on a small scale (7b or lower) but has increased performance in increased model scale, such as 44% accuracy in (A) + (C) and 66% accuracy in (B) + (D).One exception is Llama1-65b, which has lower accuracy than a smaller-scale model.We conjecture it is due to some unknown inductive bias during the pretraining.On composite steps tasks involving the arithmetic calculation of numerical numbers (G) + (H), the model has the worst performance, and increasing the model scale does not provide benefits.A key observation is that compose-by-part tasks are separable compositions where the input can be broken down into two distinct segments.</p>
<p>Composite Linguistic Translation</p>
<p>Inspired by previous work in compositional generalization (An et al., 2023b;Levy et al., 2022;An et al., 2023a;Kim &amp; Linzen, 2020), here we design our composite tasks by formal language translation tasks.</p>
<p>Our translation tasks are mainly derived from semantic parsing task COGS (Kim &amp; Linzen, 2020) and compositional generalization task COFE An et al. (2023b).These two datasets contain input as natural English sentences and output as a chain-ruled sentence following a customized grammar (see details in Appendix C).We construct two composite tasks centered on compositional generalization utilizing the training datasets to create in-context examples.See details in Appendix C.</p>
<p>We use the word error rate (WER) as the metric.It measures the minimum number of editing operations (deletion, insertion, and substitution) required to transform one sentence into another and is common for speech recognition or machine translation evaluations.</p>
<p>(T1) Phrase Recombination with Longer Chain.COFE proposed two compositional generalization tasks (Figure 2 in An et al. (2023b)).Phrase Recombination: integrate a prepositional phrase (e.g., "A in B") into a specific grammatical role (e.g., "subject", "object"); Longer Chain: Extend the tail of the logical form in sentences.We see them as simple tasks, and merge them to form a composite task: substitute the sentence subject in the Longer Chain task with a prepositional phrase from the Phrase Recombination task.Details and examples are in Table 9 of Appendix C.</p>
<p>(T2) Passive to Active and Object to Subject Transformation.We consider two tasks from Kim &amp; Linzen (2020).Passive to Active: Transitioning sentences from passive to active voice.</p>
<p>Object to Subject: Changing the same object (a common noun) from objective to subjective.They are merged to form our composite task, where both transformations are applied simultaneously to the input sentence.Details and examples are in Table 8 of Appendix C.</p>
<p>Results.</p>
<p>Figure 3 shows that LLMs can handle these composite tasks.The WER on the composite task is decent and improves with increasing model scale, particularly in Llamma2 models.These confirm the composite abilities of the models in these tasks.</p>
<p>Here, we notice that both composite tasks are separable composite tasks.If we break down these sentences into sub-sentences and phrases, the simple task operations occur in different parts or perspectives of the input sentences.So, the results here provide further support for composite abilities on separable composite tasks, where simple tasks that form the composite task are related to inputs from different parts or perspectives.We also observed the LLM exhibits better compositional ability on linguistic tasks than on logical tasks.We conclude natural language inputs can indeed help language models understand concepts better than special symbols or code.Natural language provides a richer context, which aligns better with how these models are trained on large text corpora.In contrast, logical and numerical tasks often rely on more rigid structures, which makes it harder for models to generalize without explicit training on such patterns.</p>
<p>Discussion.We observe the capability of models to handle composite tasks is significantly influenced by the task characteristics.If composite tasks contain simple tasks related to different parts or perspectives of the input, the model will tackle the composite tasks well.One natural explanation is that the model processes the input in some hidden embedding space and decomposes the embedding of the input into different "regions".Here, each region is dedicated to specific types of information and thus related to different tasks, such as word-level modifications, arithmetic calculations, mapping mechanisms, semantic categorization, linguistic acceptability, or sentiment analysis.Then, suppose the two simple tasks correspond to two different task types that relate to separate regions of the embedding.In that case, the model can effectively manage the composite task by addressing each simple task operation within its corresponding region.As the model scales, its ability to handle individual tasks improves, leading to enhanced performance on composite tasks in such scenarios.For separable composite tasks, the inputs are divided into distinct regions and also reflected in embeddings, resulting in the model's high performance.However, when the simple tasks are not separable (e.g., requiring sequential steps in reasoning), their information mixes together, complicating the model's ability to discern and process them distinctly.Such overlap often leads to the model's inability to solve the composite task.This intuition is formalized in the following sections in a stylized theoretical setting.</p>
<p>Theoretical Analysis</p>
<p>Problem Setup</p>
<p>Despite the complex nature of non-linearity in transformers in LLMs, we note it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help us better understand the phenomenon.In this section, we analyze a linear attention module and aim to provide rigorous proof about why LLMs can achieve compositional ability in some simple cases that could shed light on the more intricate behaviors observed in LLMs.</p>
<p>In-context learning.We follow existing work (Aky ürek et al., 2023;Garg et al., 2022;Mahankali et al., 2023) with slight generalization to K simple tasks.A labeled example is denoted as (x, y) where x ∈ R d , y ∈ R K .In a simple task k ∈ [K], y has only one non-zero entry y (k) .In a composite task, y can have non-zero entries in dimensions corresponding to the combined simple tasks.The model takes a prompt x 1 , y 1 , . . ., x N , y N , x q as input, which contains N in-context examples (x i , y i )'s and a query x q , and aims to predict ŷq close to the true label y q for x q .The prompt is usually stacked into an embedding matrix: N+1) where d e = d + K.In in-context learning, we first pretrain the model using training prompts and then evaluate the model with evaluation prompts; see details below.
E := x 1 x 2 . . . x N x q y 1 y 2 . . . y N 0 ∈ R d e ×(
Pretraining procedure.We have B training data indexed by τ, each containing an input prompt x τ,1 , y τ,1 , . . ., x τ,N , y τ,N , x τ,q and a corresponding true label y τ,q .Consider the following empirical loss:
L(θ) = ∑ K k=1 L k (θ) = 1 2B ∑ B τ=1 ∥ y τ,q − y τ,q ∥ 2
and the population loss (i.e., B → ∞):
L(θ) = 1 2 E x τ,1 ,y τ,1 ,••• ,x τ,N ,y τ,N ,x τ,q y τ,q − y τ,q 2 .
Evaluation procedure.We now detail how to evaluate the model on downstream composite tasks.We consider the downstream classification task to be a multi-class classification problem, where the output label is a K-dimensional vector, and each entry corresponds to a simple binary classification task.For any given simple task k, the binary classification label is given by sgn(y
(k) q )
, where sgn is the sign function.Similarly, our prediction is y
(k) q = sgn y (k) q
. The accuracy of a composite task is defined as Acc θ (x 1 , . . ., y N ,
x q ) = 1 K ∑ K k=1 1 sgn y (k) q = sgn(y (k) q ) . We denote it as Acc θ ({x i , y i } N i=1 ).
Here we denote the model performance on each task as separate dimension, (e.g.letter capitalization, numbers increment), and the performance of composite tasks as the aggregation of multiple dimensions.</p>
<p>Data. Assume x
i.i.d.
∼ N (0, Λ), where Λ ∈ R d×d is the covariance matrix.Assume y = Wx, where W ∈ R K×d .For any simple task k ∈ [K], its label is the k-th entry of y, which is y (k) = ⟨w (k) , x⟩, where w (k) is the k-th row of W. We assume each task weight w (k) i.i.d.</p>
<p>∼ N (0, I d ).</p>
<p>Linear self-attention networks.These networks are widely studied (Von Oswald et al., 2023;Aky ürek et al., 2023;Garg et al., 2022;Zhang et al., 2023b;Shi et al., 2023b).Following them, we consider the following linear self-attention network with parameters
θ = (W PV , W KQ ): f LSA,θ (E) = E + W PV E • E ⊤ W KQ E N .
The prediction of the model for x q is ŷq = [ f LSA,θ (E)] (d+1):(d+K),N+1 , the bottom rightmost sub-vector of f LSA,θ (E) with length K.</p>
<p>Compositional ability.We now provide a formal definition about compositional ability of an LLM on composite tasks.Definition 1 (Compositional Ability).Consider a composite task T that combines two simple tasks k and g.Let S k denote N labeled examples from task k, and similarly for S g .Given an x q from composite task T , we say that the model has compositional ability on T if the model has higher accuracy using in-context examples from S k ∪ S g than from either single one, i.e. max{Acc θ (S k ), Acc θ (S g )} ≤ Acc θ (S k ∪ S g ).</p>
<p>Theoretical Results</p>
<p>In this section, we present our theoretical results.We explain the observation in empirical results through the lens of confined supports in input embeddings corresponding to separate subspaces (modeling separable composition).We provide theoretical justification showing that separable composite task composite tasks whose inputs are composed by components adhere to certain conditions where models exhibit satisfactory performance.Models will fail when such conditions are violated.We first introduce the basic setup and definitions.</p>
<p>Disjoint subspaces of simple tasks.Recall that x lies in a d-dimensional space where each dimension represents a different characteristic.A simple task may depend only on a subset of these dimensions since its label only depends on a few features.Let S = [d] represent the dimensions of x.For a task k, the output y (k) = ⟨w k , x⟩ depends on a subset of dimensions in x.Denote this subset by K ⊆ S and call it the active index set for task k.</p>
<p>In the following, we always assume that the K tasks have disjoint subspaces: for any two tasks k ̸ = g, their active index sets K, and G are disjoint, i.e., K ∩ G = ∅.In practice, the dimensions within K could be associated with numerical arithmetic operations, while those in G might pertain to semantic analysis.This illustrates the model's approach to address these tasks in their respective subspaces.</p>
<p>We now introduce a mild assumption regarding the distribution of input embeddings.Assumption 1.Given two disjoint subspaces K and G, the covariance matrix Λ of the input distribution can be segmented into block matrices Λ KK , Λ KG , Λ GK , and Λ GG , then we assume σ max (Λ KG ) = σ max (Λ GK ) ≤ ϵ for constant ϵ, where σ(•) denote the singular value of matrix.</p>
<p>Assumption 1 implies that for two separate simple tasks, each associated with its respective feature subspace K and G, the covariance between these two sets of features is bounded by a constant value.This is a natural assumption when inputs of composite tasks can be decomposed into parts.Suppose we have input embeddings from two tasks: arithmetic computations and semantic analysis.This assumption suggests that the feature subspaces of the input embeddings for two tasks are almost independent.</p>
<p>We now define confined support, which means that each task's input embedding only has support within its feature subspace.Definition 2 (Confined Support).We say a task has confined support if the input x only has larger singular values within its active index set.The norm of entries outside the active index set is bounded by a small constant δ.</p>
<p>This definition shows that each simple task only has large values within its corresponding subsets of dimensions of input embeddings.For example, let K represent the first d 1 dimensions of an input vector x, and G account for the remaining d 2 dimensions, with the total dimension being d = d 1 + d 2 .The examples from task k will have input as x = (x 1 , x δ 1 ) where
x 1 ∈ R d 1 , x δ 1 ∈ R d 2 , ∥x δ 1 ∥ ≤ δ.
Similarly, the examples from task g will have inputs as x = (x δ 2 , x 2 ).</p>
<p>We now present our results of the compositional ability under a confined support of x.Theorem 1.Consider distinct tasks k and g with corresponding examples S k , S g .If two tasks have confined support, and Assumption 1 is true, then with high probability, the model has the compositional ability as defined in Definition 1.Moreover, Acc θ (S k ) + Acc θ (S g ) ≤ Acc θ (S k∪g ).</p>
<p>Theorem 1 shows the compositional ability of LLMs to handle composite tasks that integrate two simple tasks, which have confined support in their own feature subspace.</p>
<p>An illustrative case involves the tasks of Capitalization (A) &amp; Plus One (F) and Past Tense (D) &amp; Plus One (F), as depicted in Table 4.These two simple tasks involve word-level modification and arithmetic operation on separate parts of the input.Due to this separation, each task correlates with a specific segment of the input embedding.Therefore, it is observed that these tasks possess confined supports.</p>
<p>We further provide theory illustrating the necessity of the confined supports, we demonstrate that when the confined support is violated, simple tasks begin to show variations (indicated by large singular values) across the entire feature subspace of the input embedding.For instance, the composite task of Capitalization (A) &amp; Swap (B), which involves mixed steps in reasoning as shown in Figure 2, shows poor performance of LLMs given both simple tasks' examples as in-context demonstrations.Another example is Modular (G) &amp; Two Sum Plus (H) as shown in the last row of Table 4, where both simple tasks involve multisteps arithmetic operation.These two tasks share the same embedding space support, mixing their variations and causing the model to be unable to effectively address the composite tasks that integrate them.We further substantiate this observation with Corollary 1, which establishes that when two tasks share overlapping support in the embedding space, a scenario can arise where the model fails to demonstrate compositional ability.Corollary 1.If two tasks do not have confined support, there exists one setting in which we have
Acc θ (S k ) = Acc θ (S g ) = Acc θ (S k∪g ).
Corollary 1 demonstrates that a model's failure to solve tasks with mixed steps reasoning, which contains overlapping input embedding spaces, thereby diminishing the model's ability to solve them when presented together.</p>
<p>We also show the scaling effect: if simple tasks have confined support, the compositional ability of language models will increase as the model scale increases in Theorem 2 in Appendix D.1.We demonstrate this by showing that the model's accuracy on each simple task improves with a larger model scale.We finally provide a case study on confined support for illustration in Appendix D.2.We defer the full proof in Appendix E.</p>
<p>Conclusion</p>
<p>In this work, we presented a distinct pattern in LLMs' behaviors when tackling composite tasks.We observed that if the composite task can be separated into two simple tasks whose inputs are from distinct perspectives, the models exhibit decent compositional ability.Otherwise, LLMs will struggle, and scaling up the model size may not offer improvement.We illustrated this behavior across a variety of logical and linguistic challenges.We extended our discussion to the role of input embeddings in affecting model performance, providing a theoretical backup that connects the nature of tasks to how inputs are processed.We anticipate that our research will shed light on the compositional capabilities and reasoning of LLMs, and stimulate further exploration in this direction.</p>
<p>Impact Statements</p>
<p>Our work aims to improve the understanding of in-context learning capabilities of Large Language Models (LLMs) in the handling of composite tasks.Our paper is mostly theoretical in nature, and thus, we foresee no immediate negative ethical impact.We illustrate the empirical behavior of LLMs on complex reasoning tasks and provide a theoretical explanation for it.In the long term, we hope our work may inspire effective algorithm design and better understanding and employment of LLMs.In this appendix, we provide more related work in Appendix A. We provide more empirical settings and results for logical tasks in Appendix B and linguistic translation tasks in Appendix C. We provide a theory for confined support and model scalability, along with a case study of a toy model in Appendix D. We provide full proof in Appendix E.</p>
<p>Appendix Contents</p>
<p>A Related Work</p>
<p>Large language model.LLMs are often Transformer-based (Vaswani et al., 2017) , 2023).Pretraining methods include masked language modeling (Devlin et al., 2019;Liu et al., 2019), contrastive learning (Gao et al., 2021;Shi et al., 2023a;Sun et al., 2023;2024) and auto-regressive pretraining (Radford et al., 2018;2019).Several works (Madasu &amp; Srivastava, 2022;Alajrami et al., 2023) investigate the effects of pretraining on language models.Adapting LLMs to various downstream tasks has received significant attention, e.g., adaptor (Hu et al., 2022;2023;Zhang et al., 2023a;Luo et al., 2024), prompt tuning (Lester et al., 2021;Li &amp; Liang, 2021;Wei et al., 2023a;Gu et al., 2024c), multitask finetuning (Sanh et al., 2022;Wang et al., 2023b;Xu et al., 2023;2024c), instruction tuning Chung et al. ( 2022); Mishra et al. (2022), in-context learning (Min et al., 2022b;Dong et al., 2022;Yao et al., 2023), low-rank adaptation Hu et al. (2022); Zeng &amp; Lee (2024); Hu et al. (2024), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and inference acceleration (Gu et al., 2024b;d;Xu et al., 2024a).</p>
<p>In-context learning.LLM exhibits a remarkable ability for in-context learning (ICL) (Brown et al., 2020), particularly for generative models.2024) and so on.Recent works show LLMs with larger scales have distinct behavior compared to smaller language models (Wei et al., 2023b;Shi et al., 2023b;2024b).These behaviors can have positive or negative effects on performance.Solving complex tasks and reasoning is an active problem in the AI community Huang &amp; Chang (2022).There is a line of empirical works investigating the compositional ability in linguistic fashion (Kim &amp; Linzen, 2020;Levy et al., 2022;An et al., 2023a;b;Xu et al., 2024b).LLMs are capable of learning abstract reasoning (e.g., grammar) to perform new tasks when finetuned or given suitable in-context examples.In our work, we include linguistic experiments as part of our testing suite, illustrating LLMs' compositional ability.Ye et al. (2023); Berglund et al. (2023); Dziri et al. (2023) show LLMs will have difficulties solving tasks that require reasoning.Berglund et al. (2023) studies that LLMs trained on "A is B" fail to learn "B is A".In our work, we conduct similar experiments showing LLMs will fail on composite if different steps of logical rules are mixed.</p>
<p>B Logical Tasks</p>
<p>B.1 Task Setup</p>
<p>We provide a comprehensive explanation of logical composite tasks below.Examples can be seen in Table 5.</p>
<p>• (A) + (B) Capitalization &amp; Swap, as in Section 2.</p>
<p>• (A) + (C) Capitalization &amp; Two Sum.Given words of numerical numbers, * represents the operation of capitalizing, @ represents summing the two numbers.</p>
<p>• (G) + (H) Modular &amp; Two Sum Plus.Given numerical numbers, @ represents the operation of taking modular, # represents to sum the two numbers and then plus one.</p>
<p>• (A) + (F) Capitalization &amp; Plus One.If numerical numbers are given, plus one; if words are given, capitalize the word; if both are given, perform both operations.</p>
<p>Among these, (A) + (F) performs the two operations on separable parts of the test inputs (i.e., separable composite task).Table 5: Examples of the four logical composite tasks.Note that in (G) + (H), the output of the composite task can be either 4 or 11 depending on the order of operations, and we denote both as correct.</p>
<p>Tasks Simple Task Simple Task Composite</p>
<p>We design our logical tasks following the idea of math reasoning and logical rules.The details are shown in Table 5.Our numerical numbers in Table 2 are uniformly randomly chosen from 1 to 1000.The words of numbers in task (C) are uniformly randomly chosen from one to one hundred.The words representing objects in Table 2 are uniformly randomly chosen from class names of ImageNet after dividing the phrase (if any) into words.We randomly chose 100 examples in composite testing data in our experiments and replicated the experiments in each setting three times.We fixed the number of in-context examples as K = 10 as demonstrations.</p>
<p>B.2 Experimental Setup</p>
<p>We use exact match accuracy to evaluate the performance between sequence outputs.The calculation of exact match accuracy divided the number of matched words by the length of ground truth.</p>
<p>For Llama models, we use official Llama1 and Llama2 models from Meta (Touvron et al., 2023), we use open llama 3b v2 from open OpenLlama (Geng &amp; Liu, 2023).For GPT models, we use GPT2-large from OpenAI (Radford et al., 2019), and we use GPT-neo models for GPT models in other scales from EleutherAI (Black et al., 2021).</p>
<p>In our experiments, we provided the model with instructions.Here are instructions of Figure 1, which were prepended to ICL examples.We refer to our codebase for full instructions and results.</p>
<ul>
<li>is a function before words for swapping the position of 2 words, # is another function after words for capitalizing letters of words.</li>
</ul>
<p>B.3 Results</p>
<p>We show a visualization of some logical task accuracy along the increasing to model scale, complement to Table 4.  Lines: performance in different evaluation settings, i.e., the two simple tasks, the composite setting, and the composite in-context setting (examples for the last two are shown in Table 1).</p>
<p>We also include results for the more recent model Llama3 (Meta, 2024) on the part of our logical tasks to demonstrate the idea.We show results in Table 6.</p>
<p>As shown in the Table 6, for the separable composite tasks which are relatively easy for model to solve (A) + (F), the models show strong compositional ability: the composite accuracy is high, improves with increasing scale, and eventually reaches similar performance as the gold standard composite in-context setting.For composite tasks with sequential reasoning steps tasks serve as the basis for our composite task, where both transformations are applied simultaneously to the same sentence.Examples illustrating this dual transformation can be found in Table 8.</p>
<p>Enhanced Phrase Subject with Longer Chain.COFE proposed two compositional generalization tasks (Figure 2 in An et al. (2023b)): Phrase Recombination (PhraReco): integrate a prepositional phrase (e.g., "A in B") into a specific grammatical role (e.g., "subject", "object"); Longer Chain (LongChain): Extend the tail of the logical form in sentences.We consider these two generalization tasks as two simple tasks, merging them to form a composite task.In particular, we substitute the sentence subject in the Longer Chain task with a prepositional phrase from the Phrase Recombination task, creating a more complex task structure.Detailed examples of this combined task can be found in Table 9.</p>
<p>D Theory for Confined Support</p>
<p>D.1 Compositional Ability with Model Scale</p>
<p>We then show that if simple tasks have confined support, the compositional ability of language models will increase as the model scale increases.We demonstrate this by showing that the accuracy of the model on each simple task improves with a larger model scale.</p>
<p>Note that the optimal solutions of the parameter matrices are W * PV and W * KQ .We naturally consider that the rank of the parameter matrices W * PV and W * KQ can be seen as a measure of the model's scale.A higher rank in these matrices implies that the model can process and store more information, thereby enhancing its capability.We state the following theorem.</p>
<p>Theorem 2. Suppose a composite task satisfies confined support.Suppose that we have</p>
<p>x 1 , y 1 , . . . ,x N , y N , x q as a testing input prompt and the corresponding W where y i = Wx i .</p>
<p>As rank r decreases, E W,x 1 ,••• ,x N [Acc θ ] will have a smaller upper bound.</p>
<p>Theorem 2 shows the expected accuracy of a model on composite tasks is subjected to a lower upper bound as the scale of the model diminishes.This conclusion explains why scaling up helps the performance when the model exhibits compositional ability for certain tasks (those we call "separable composite tasks").One common characteristic of these tasks is their inputs display confined supports within the embeddings.This is evidenced by the model's decent performance on tasks as presented in Table 4 and Figure 3, where inputs are composed of parts.</p>
<p>D.2 Case Study of Confined Support</p>
<p>Our theoretical conclusion shows the model behavior regarding input embedding.It states that the model will have compositional ability if tasks are under confined support of input embedding.To illustrate such theoretical concepts and connect them to empirical observations, we specialize the general conclusion to settings that allow easy interpretation of disjoint.In this section, we provide a toy linear case study on classification tasks, showing how confined support on embedding can be decomposed and composite tasks can be solved.</p>
<p>We assume δ = ϵ = 0 in the following simple example.</p>
<p>Consider that there are only two simple tasks for some random objects with the color red and blue and the shape square and round: (1) binary classification based on the color red and blue.(2) binary classification based on shape: circle and square.However, during evaluation, the composite task is a four-class classification, including red circle, red square, blue circle, and blue square.</p>
<p>Then we have two simple tasks K = 2. Consider the input embedding x = (a, b), where
a ∈ R 2 , b ∈ R 2 , d = 4. Consider W = 1 −1 0 0 0 0 1 −1 and y = Wx.
Consider the inputs from simple and composite tasks as:</p>
<p>• Task 1: Red: x 1 = (1, 0, 0, 0), y 1 = (1, 0) and blue: x 2 = (0, 1, 0, 0), y 2 = (−1, 0).</p>
<p>• Task 2: Circle x 3 = (0, 0, 1, 0) y 3 = (0, 1) and square x 4 = (0, 0, 0, 1) y 4 = (0, −1).</p>
<p>• Composed task: red circle x 5 = (1, 0, 1, 0), y 5 = (1, 1), red square x 6 = (1, 0, 0, 1), y 6 = (1, −1), blue circle x 7 = (0, 1, 1, 0) y 7 = (−1, 1) and blue square x 8 = (0, 1, 0, 1) y 8 = (−1, −1).</p>
<p>Suppose that we have the optimal solution ŷq as in Equation (1).Given x q = (1, 0, 1, 0) as a testing input for a red circle example, During the test, we have different predictions given different in-context examples:</p>
<ol>
<li>
<p>Given only examples from Task 1 (red and blue): [(x 1 , y 1 ), (x 2 , y 2 )], we have y q = (1, 0) can only classify the color as red.2. Given only examples from Task 2 (square and circle): [(x 4 , y 4 ), (x 3 , y 3 )], we have y q = (0, 1) only classify the shape as a circle.</p>
</li>
<li>
<p>Given a mixture of examples from Task 1 and 2 (red and circle): [(x 1 , y 1 ), (x 3 , y 3 )],</p>
</li>
</ol>
<p>we have y q = (1, 1) can classify as red and circle.</p>
<p>We can see that in the final setting, the model shows compositional ability.This gives a concrete example for the analysis in Theorem 1.</p>
<p>E Deferred Proof</p>
<p>In this section, we provide a formal setting and proof.We first formalize our model setup.</p>
<p>E.1 Linear self-attention networks.</p>
<p>These networks are widely studied (Von Oswald et al., 2023;Aky ürek et al., 2023;Mahankali et al., 2023;Garg et al., 2022;Zhang et al., 2023b;Shi et al., 2023b).Following them, we consider the following linear self-attention network with parameters θ = (W PV , W KQ ):
f LSA,θ (E) = E + W PV E • E ⊤ W KQ E N .
The prediction of the model for x q is ŷq = [ f LSA,θ (E)] (d+1):(d+K),N+1 , the bottom rightmost sub-vector of f LSA,θ (E) with length K. Let
W PV = W PV 11 W PV 12 (W PV 21 ) ⊤ W PV 22 ∈ R (d+K)×(d+K) , W KQ = W KQ 11 W KQ 12 (W KQ 21 ) ⊤ W KQ 22 ∈ R (d+K)×(d+K) ,
where W PV 11 ∈ R d×d , W PV 12 , W PV 21 ∈ R d×K , and W PV 22 ∈ R K×K ; similar for W KQ .Then the prediction is
y q = (W PV 21 ) ⊤ W PV 22 EE ⊤ N W KQ 11 (W KQ 21 ) ⊤ x q .
(1)</p>
<p>We observe only part of the parameters affect our prediction, so we treat the rest of them as zero in our analysis.</p>
<p>E.2 Proof of Compositional Ability under Confined Support</p>
<p>Here, we provide the proof of our main conclusion regarding Theorem 1 and Corollary 1.</p>
<p>Without abuse of notation, we denote U = W KQ 11 , u = W PV 22 .We also add some mild assumptions.</p>
<p>1.The covariance matrix Λ of simple tasks will have the same trace to prevent the scale effect of different simple tasks.</p>
<ol>
<li>The spectral norm of Λ is bounded on both sides m ≤ ∥Λ∥ ≤ M.</li>
</ol>
<p>We first introduce the lemma where the language model only pretrained on one simple task (K = 1).The pretraining loss L(θ) can be refactored, and the solution will have a closed form.We further discuss the following.</p>
<p>Lemma E.1 (Lemma 5.3 in Zhang et al. (2023b)).
Let Γ := 1 + 1 N Λ + 1 N tr(Λ)I d×d ∈ R d×d . Let l(U, u) = tr 1 2 u 2 ΓΛUΛU ⊤ − uΛ 2 U ⊤ Then min θ L(θ) = min U,u l(U, u) + C = − 1 2 tr[Λ 2 Γ −1 ] + C
where C is a constant independent with θ.For any global minimum of l, we have uU = Γ −1 .</p>
<p>As the above lemma construction, we denote the optimal solution as W * PV and W * KQ .Taking one solution as U = Γ −1 , u = 1, we observe the minimizer of global training loss is of the form:
W * PV = 0 d×d 0 d 0 ⊤ d 1 , W * KQ = Γ −1 0 d 0 ⊤ d 0 .(2)
We then prove our main theory Theorem 1 in Section 4.
∈ R d 1 , b ∈ R d 2 , d 1 + d 2 = d.
Since x only has large values on certain dimensions, it's equivalent to just consider corresponding dimensions in w, i.e., for simple task 1, we have w (1) = (w a , w δb ), for simple task 2, we have w (2) = (w δa , w b ).</p>
<p>We have x ∼ Λ, where:
Λ = Λ KK Λ KG Λ GK Λ GG • Task 1: x = (a, 0 d 2 ) ⊤ + (0, b δ ) ⊤ , y = (w ⊤ a a, 0) + (0, w ⊤ δb b δ ). • Task 2: x = (0 d 1 , b) ⊤ + (a δ , 0 d 2 ) ⊤ , y = (0, w ⊤ b b) + (w ⊤ δa a δ ), 0). • Composed task: x = (a, b) ⊤ + (a δ , b δ ) ⊤ , y = (w ⊤ a a, w ⊤ b b) + (w ⊤ δa a δ , w ⊤ δb b δ ).
The form of E is, N+1) .
E :=   a 1 a 2 . . . a N a q b 1 b 2 . . . b N b q y 1 y 2 . . . y N 0   + E r ∈ R (d+2)×(
where E r represents the values caused by residual dimensions whose entries are bounded by δ.</p>
<p>Following Equation (4.3) in Zhang et al. (2023b), we have where Γ −1 all ∈ R (d 1 +d 2 )×(d 1 +d 2 ) .Consider two tasks only related to disjoint dimension of x, we also have σ(Λ KG ) = σ(Λ GK ) ≤ ϵ.Denote
EE ⊤ = 1 N    ∑ N i=1 a i a ⊤ i + a q a ⊤ q ∑ N i=1 a i b ⊤ i + a q b ⊤ q ∑ N i=1 a i y ⊤ i ∑ N i=1 b i a ⊤ i + b q a ⊤ q ∑ N i=1 b i b ⊤ i + b q b ⊤ q ∑ N i=1 b i y ⊤ i ∑ N i=1 y i a ⊤ i ∑ N i=1 y i b ⊤ i ∑ N i=1 y i y ⊤ i    + δ • o(EE ⊤ ).Λ = Λ + Λ r where Λ = Λ KK Λ GG , Λ r = Λ KG Λ GK
We apply Lemma E.1 Recall Γ := 1 + 1 N Λ + 1 N tr(Λ)I d×d ∈ R d×d , we have:
Γ = 1 + 1 N Λ + 1 N tr( Λ)I d×d + 1 + 1 N Λ r = Γ + Γ r
where denote Γ r = 1 + 1 N Λ r .We have:
Γ −1 = Γ−1 − Γ−1 Γ r Γ−1 + O(Γ r )
We denote
Γ = Γ 1 0 0 Γ 2 ,
where Γ 1 = 1 + 1 N Λ KK + 1 N tr(Λ)I d 1 ∈ R d 1 ×d 1 and Γ 2 = 1 + 1 N Λ GG + 1 N tr(Λ)I d 2 ∈ R d 2 ×d 2 .Then we have;
Γ −1 = Γ −1 1 0 0 Γ −1 2 + A
where σ(A) ≤ 2m 2 ϵ.</p>
<p>Then, It's similar to applying Lemma E.1 for pretraining separately into dimensions corresponding to different tasks.We solve similarly to W KQ .</p>
<p>We have:
f θ (E) =   0 d 1 ×d 1 0 d 1 ×d 2 0 d 1 ×2 0 d 2 ×d 1 0 d 2 ×d 2 0 d 2 ×2 0 2×d 1 0 2×d 2 I 2   EE ⊤   Γ −1 1 0 d 1 ×d 2 0 d 1 ×2 0 d 2 ×d 1 Γ −1 2 0 d 2 ×2 0 2×d 1 0 2×d 2 0 2×2     a q b q 0   + Ã (3) ŷq = 1 N N ∑ i=1 y i a ⊤ i , N ∑ i=1 y i b ⊤ i , N ∑ i=1 y i y ⊤ i   Γ −1 1 a q Γ −1 2 b q 0   + v (4) = 1 N N ∑ i=1 y i a ⊤ i Γ −1 1 a q + 1 N N ∑ i=1 y i b ⊤ i Γ −1 2 b q + v (5) = 1 N a ⊤ q Γ −1 1 ∑ N i=1 y (1) i a i b ⊤ q Γ −1 2 ∑ N i=1 y (2) i b i + v.(6)
Denote hyperplane orthogonal to w as P w and similar to P ŵ.Recall that x q is independent of other samples.We have the expectation conditioned on w τ , x 1 , • • • , x N is the probability that x q falls out of the angle between P w and P ŵ.Denote the angle between w and ŵ as θ.As x q is uniform along each direction (uniform distribution or isotropic Gaussian), then the
probability is 1 − | θ| π given w τ , x 1 , • • • , x N . Then E w τ ,x 1 ,••• ,x N [Acc θ ] = E w τ ,x 1 ,••• ,x N 1 − | θ| π . Note that E w τ ,x 1 ,••• ,x N cos( θ) = w τ ∥w τ ∥ 2 , ŵ ∥ ŵ∥ 2 .
As, we can choose α, w.l.o.g, we take ∥w τ ∥ = ∥ ŵ∥ = 1, then we have
E w τ ,x 1 ,••• ,x N cos( θ) = E w τ [E x 1 ,••• ,x N [⟨w τ , ŵ⟩ |w τ ]] .
Given w τ , we have
E[ ŵ|w τ ] = 1 N Γ −1 N ∑ i=1 E [⟨w τ , x i ⟩ x i |w τ ] = 1 N Γ −1 N ∑ i=1 Λw τ = Γ −1 Λw τ .
Then, we have
E w τ [⟨ ŵ, w τ ⟩] = Γ −1 Λw ⊤ τ , w τ = tr(Γ −1 Λ).
Thus, we have E cos( θ) = tr(Γ −1 Λ) (12)
E [Acc θ ] = E 1 − | θ| π . (13)
Note that when θ ≤ π 6 , we have 1 − | θ| π ≤ cos(θ).Thus, as N &gt; C where C is constant, we have ŵ and w τ are closed and satisfy θ ≤ π 6 .Then we get the statement.</p>
<p>E.3.2 Model scale on composite tasks</p>
<p>Here, we present proof for model scale and performance on composite tasks.Recall we consider the rank of W * PV and W * KQ as a measure of the model's scale.</p>
<p>We first introduce a lemma about U as an optimal full-rank solution.where U = cΓ −1 , u = 1 c for any non-zero constant c are minimum solution.We also have l(U, u) − min
U∈R d×d ,u∈R l(U, u) = 1 2 Γ 1 2 uΛ 1 2 UΛ 1 2 − ΛΓ −1 2 F . (14)
As the scale of the model decreases, the rank of U also reduces, leading to an optimal reduced rank solution U. Our findings reveal that this reduced rank U can be viewed as a truncated form of the full-rank solution U.This implies that smaller-scale models are and Γ −1 = QD ′ −1 Q ⊤ .We denote V = uQ ⊤ UQ.Since Γ and Λ are commutable and the Frobenius norm (F-norm) of a matrix does not change after multiplying it by an orthonormal matrix, we have Equation ( 14) as l(U, u) − min
U∈R d×d ,u∈R l(U, u) = 1 2 Γ 1 2 uΛ 1 2 UΛ 1 2 − ΛΓ −1 2 F = 1 2 Γ 1 2 Λ 1 2 uU − Γ −1 Λ 1 2 2 F = 1 2 D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F .
As W KQ is a matrix whose rank is at most r, we have V is also at most rank r.Then, we denote V * = arg min V∈R d×d ,rank(V)≤r
D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F
. We can see that V * is a diagonal matrix.Denote D ′ = diag([λ ′ 1 , . . ., λ ′ d ]) and V * = diag([v * 1 , . . ., v * d ]).Then, we have
D ′ 1 2 D 1 2 V − D ′ −1 D 1 2 2 F (15) = d ∑ i=1 λ ′ i 1 2 λ i v * i − 1 λ ′ i 2 (16) = d ∑ i=1 1 + 1 N λ i + tr(D) N λ 2 i   v * i − 1 1 + 1 N λ i + tr(D) N   2 . (17)</p>
<p>Figure2: The exact match accuracy (y-axis) vs the model scale (x-axis, "b" stands for billion) for Capitalization &amp; Swap tasks (example in Figure1).Line capital: performance on the simple task of capitalization; swap: on the simple task of swap; composite: in-context examples are from simple tasks while test input from the composite task.composite incontext: in-context examples and test input are all from the composite task (example in Table1).</p>
<p>Figure 3 :
3
Figure 3: The word error rate (WER) vs the model scale on composite linguistic translation tasks.Dashed lines: simple tasks.Solid lines: composite tasks.Rows: (T1) Phrase Recombination with Longer Chain; (T2) Passive to Active and Object to Subject Transformation.Columns: different models.Lines: performance in different evaluation settings, e.g., the two simple tasks, the composite setting, and the composite in-context setting (examples are shown in Appendix C).</p>
<p>Given a sequence of labeled examples and a testing example (combined as a prompt), the model can construct new predictors for testing examples without further parameter updates.Several empirical studies investigate the behavior of ICLs.Zhao et al. (2021); Holtzman et al. (2021); Lu et al. (2022) formulate the problems and report the sensitivity.Rubin et al. (2022); Liu et al. (2022); Hongjin et al. (2023); Wang et al. (2023a) provide methods to better choose in-context learning examples.Chen et al. (2022); Min et al. (2022a) use meta training with an explicit in-context learning object to boost performance.Theoretically, Xie et al. (2022); Garg et al. (2022) provide a framework to explain the working mechanism of in-context learning.Von Oswald et al. (2023); Aky ürek et al. (2023); Mahankali et al. (2023); Zhang et al. (2023b), investigating with linear models, show how transformers can represent gradient descent and conduct linear regression.Based on these works, we provide an analysis showing how LLM can exhibit compositional ability in ICL.Emergence of compositional ability.Scaling law was first proposed byKaplan et al. (2020) and then followed up byHoffmann et al. (2022), emphasizing both the scale of models and training data.Sometimes, increasing scale can lead to new behaviors of LLMs, termed emergent abilities(Wei et al., 2022;Arora &amp; Goyal, 2023), such as domain generalizationShi et al. (2024a), math reasoningGu et al. (2024a), spatial reasoningWang et al. (</p>
<p>Figure 4 :
4
Figure 4: The accuracy v.s.model scale on composite logical rule tasks.Dashed lines: simple tasks.Solid lines: composite tasks.Rows: (A) + (C) Capitalization &amp; Two Sum; (G) + (H) Modular &amp; Two Sum Plus; (A) + (F) Capitalization &amp; Plus One.Columns: different models.Lines: performance in different evaluation settings, i.e., the two simple tasks, the composite setting, and the composite in-context setting (examples for the last two are shown in Table1).</p>
<p>Lemma E. 3 (
3
Corollary A.2 in Zhang et al. (2023b)).The loss function l in Lemma E.</p>
<p>, see model details in Appendix B.
CompositeComposite in-contextPrompt input: * appleinput: ( * good * zebra )output: APPLEoutput: ZEBRA GOODinput: ( farm frog )input: ( * bicycle * add )output: frog farminput: ( * bell * ford )</p>
<p>Table 1 :
1
Examples of two settings on composite tasks.
Evaluation settings. To makethorough evaluations, we con-sider four settings: (1) capi-tal: only on the capitalizationtask; (2) swap: only on swap;(3) composite: in-context ex-amples are from simple taskswhile the test input is aboutthe composite task; (4) com-posite in-context: in-contextexamples and the test inputare all drawn from the com-posite task. The composite in-context setting reduces the evaluation to another simple task, not requiring the modelComposite: in-context examples are about simple tasks, while the test input is about the composite task. Composite in-context: both in-context examples and the test input are about the composite task.to composite the simple task
ability but directly learning from the in-context examples.It serves as the gold standard performance for the composite task.See Table1for illustration.</p>
<p>Table 2 :
2
This table contains a collection of simple logical tasks.The Words category encompasses tasks that modify words at the character or structural level.The Numerical category is devoted to tasks that involve arithmetic computations performed on numbers.
TasksTaskInputOutputWords(A) CapitalizationappleAPPLE(B) Swapbell fordford bell(C) Two Sumtwenty @ eleventhirty-one(D) Past Tensepaypaid(E) OppositeAboveBelowNumerical (F) Plus One435436(G) Modular15 @ 63(H) Two Sum Plus One 12 # 518TasksSimple TaskSimple TaskComposite(A) + (B) input: * appleinput: ( farm frog )input: ( * bell * ford )output: APPLEoutput: frog farmoutput: FORD BELL(A) + (F) input: 435input: cowinput: 684 catoutput: 436output: COWoutput: 685 CAT</p>
<p>Table 3 :
3
Examples of the two logical composite tasks.Full examples can be found in Appendix B.</p>
<p>Table 4 :
4
model understand and solve the composite tasks well, such as Com.in-context rows in all task combinations.We conclude that models fail to compose mechanisms of two simple tasks together; however, given composite examples, models can learn the composed mechanism efficiently.We also experimented with prompt demonstrations and found instructions provide no direct results; see more experimental details in Appendix B.2. See more experimental results(including Llama3 (Meta, 2024)) and visualizations in Appendix B.3. Results are evaluated composite tasks on various models.The accuracy is in %.
MistralLlama2Llama1Tasks7B 8x7B 7B 13B 70B 7B 13B 30B 65B(A) + (B)Capitalization999899100 1009898100 100swap100100100 100 100 100 100 100 100Compose164271370301613Com. in-context 9596969810066979698(A) + (C)twoSum7110072939962569899Capitalization9899100959997989999Compose8193234433312Com. in-context 316552771009229369(A) + (F)Capitalization979998779984969998PlusOne10099100 100 100 100 100 100 100Compose929674699757606999Com. in-context 999899100 1009999100 100(B) + (D)Swap100100100 100 100 100 100 100 100Past Tense979997100999798100 100Compose61201625734465Com. in-context 929886959886958994(B) + (E)Swap100100100 100 100 100 100 100 100Opposite616258686551586463Compose000000000Com. in-context 35321237370979(D) + (F)Past Tense10010098100 100 100 100 100 100Plus One100100100 100 10099100 100 100Compose714632808040441474Com. in-context 981009899100959698100(G) + (H)Modular2522523439162929twoSumPlus38423779014104087Compose450110005Com. in-context481313121113712
Such tasks are typically straightforward for a model to address.In all experiments, providing composed examples as in-context demonstrations will help the</p>
<p>Formal Language Translation Tasks 21 D Theory for Confined Support 23</p>
<p>Composite Logical Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Composite Linguistic Translation . . . . . . . . . . . . . . . . . . . . . . . . .6 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8 4.2 Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9 Case Study of Confined Support . . . . . . . . . . . . . . . . . . . . . . . . .23 Linear self-attention networks. . . . . . . . . . . . . . . . . . . . . . . . . . .24 E.2 Proof of Compositional Ability under Confined Support . . . . . . . . . . .24 E.3 Proof of Compositional Ability with Model Scale . . . . . . . . . . . . . . . .28 E.3.1 Accuracy under K = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . .28 E.3.2 Model scale on composite tasks . . . . . . . . . . . . . . . . . . . . . .29
1 Introduction11.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22 Warm-up: A Failure Case for Composition33 Variability of Compositional Performance43.1 4 Theoretical Analysis84.1 5 Conclusion10A Related Work18B Logical Tasks19E Deferred Proof24E.1
B.1 Task Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19 B.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19 B.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20 C D.1 Compositional Ability with Model Scale . . . . . . . . . . . . . . . . . . . . .23 D.2</p>
<p>equipped with the enormous size of parameters and pretrained on vast training data.Typical LLMs includes BERT Devlin et al. (2019), PaLM Chowdhery et al. (2022), LLaMATouvron et al. (2023), ChatGPT (OpenAI, 2022), GPT4 (OpenAI</p>
<p>We've experimented with prompt demonstrations. Instructional prompts do help ChatGPT and Claude3 (although we haven't quantified the accuracy in large-scale experiments), but they offer limited benefits for current open-source models. On the other hand, we did not have prompt tuning or any other parameter updates during our evaluation.
AcknowledgementsThe work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants 2008559-IIS, CCF-2046710, and 2023239-DMS.(A) + (B), the model has poor performance on a small scale but has increased performance on an increased model scale.Providing composed examples as in-context demonstrations will help the model understand and solve the composite tasks well.C Formal Language Translation TasksOur translation tasks mainly follow the compositional generalization tasks in COFE(An et al., 2023b).The details can be found in Section 4 inAn et al. (2023a).We directly take the source grammar G s in COGS, which mimics the English natural language grammar, and reconstruct the target grammar G t in COGS to be chain-structured.We follow the Primitive coverage principle proposed byAn et al. (2023b)that primitives contained in each test sample should be fully covered by in-context examples.Here, primitives refer to the basic, indivisible elements of expressions, including subjects, objects, and verbs.Note that multiple sets of in-context examples can meet these criteria for each test case.Across all experimental conditions, we maintain a consistent number of test instances at 800.We use the word error rate (WER) as the metric.It measures the differences between 2 sentences.It measures the minimum number of editing operations (deletion, insertion, and substitution) required to transform one sentence into another and is common for speech recognition or machine translation evaluations.The computation of WER is divided by the number of operations by the length of ground truth.Original Target GrammarChain-Structured Target Grammar rose ( x 1 ) AND help .theme ( x 3 , x 1 ) AND help .agent ( x 3 , x 6 ) AND dog ( x 6 ) HELP ( DOG, ROSE, NONE ) * captain ( x 1 ) ; eat .agent ( x 2 , x 1 ) EAT ( CAPTION, NONE, NONE ) * dog ( x 4 ) ; hope .agent ( x 1 , Liam ) AND hope .ccomp ( x 1 , x 5 ) AND prefer .agent ( x 5 , x 4 ) HOPE ( LIAM, NONE, NONE ) CCOMP PREFER ( DOG, NONE, NONE )Table7: Demonstration inAn et al. (2023a)showing examples with the original grammar and the new chain-structured grammar.In formal language tasks, as mentioned in Section 3.2, we change the original target grammar of COGS to be chain-structured.In Table7, we list some examples with the original target grammar and the new chain-structured grammar.• First, to distinguish the input and output tokens, we capitalize all output tokens (e.g., from "rose" to "ROSE").• Second, we replace the variables (e.g., "x 1") in the original grammar with their corresponding terminals (e.g., "ROSE").• Then, we group the terminals of AGENT (e.g., "DOG"), THEME (e.g., "ROSE"), and RECIPIENT with their corresponding terminal of PREDICATE (e.g., "HELP") and   In the following, we provide a detailed explanation of our two composite tasks in translation tasks.Passive to Active and Object to Subject Transformation.Based on the generalization tasks identified in Kim &amp; Linzen (2020)), we select two distinct challenges for our study as two simple tasks.Passive to Active: Transitioning sentences from Passive to Active voice.Object to Subject: Changing the focus from Object to Subject using common nouns.These where Ã representing residual matrix whose norm can be bounded by O(m 2 ϵδ) Recall x ∼ N(0, Λ), then with high probability each entry in v will be bounded by Cm 2 δϵ for some constant C.WLOG, we write residual vectors as 0 vector for simplicity of notation, and only consider residuals for estimations ŷ.Note that we composed example x = (a, b) ⊤ , y = (w ⊤ a a, w ⊤ b b).For simplicity, we write ŵa =i b i .Given in-context examples from one simple task only, consider that we have N examples from simple task 1, S 1 = {(a i , 0), y i } N i=1 .We have ŵ(1) = ( ŵa , 0 d 2 ), ŵ(2) = (0 d ), and we also have ŷq = ( ŷ(1) q , 0) ⊤ , where ŷ(1)i a i + Cm 2 δϵ.We haveSimilarly, for N in-context examples only from task 2, we have ŵ(1) = (0 d ), ŵ(2) = (0 d 1 , ŵb ),Then we have S 1∪2 contains 2N in-context examples from both tasks, specifically, we have N from task 1 and rest from task 2. We have ŵ(1τ,q = sgn y (k) τ,q , following the proof of Lemma E.2, where Acc θ only concerns the direction of ŵ and w, we have Acc θ (S 1∪2 ) = 1 y (1) q =y(1) q +1 y(2)Extending the above analysis into any of two simple tasks, when the composite task integrates them, we have Acc θ (S k ) + Acc θ (S g ) ≤ Acc θ (S k∪g ).We then prove Corollary 1, and we first restate it below.Corollary 1.If two tasks do not have confined support, there exists one setting in which we haveProof of Corollary 1. WLOG, consider two simple tasks, K = 2.We have x = (a, b), whereConsider the setting where w also have the same active dimensions, i.e., for simple task 1, we have w (1) = (w a , 0), for simple task 2, we have w (2) = (0, w b ).We have x ∼ Λ.Consider tasks are overlapping on all dimensions, where:Similarly, we have:, and ŷq has the same form as Equation (10), similarly, for task 2.Suppose S 1∪2 contains 2N examples from both tasks, where N from task 1 and rest from task 2. We haveWe finish the proof by checking that Equation (10) and Equation (11) share the same direction.E.3 Proof of Compositional Ability with Model ScaleHere, we provide the proof of our conclusions in Theorem 2 in Appendix D.1 with respect to model performance and model scale.We first introduce a lemma under the K = 1 setting.E.3.1 Accuracy under K = 1When K = 1, we can give an upper bound of accuracy by Λ and Γ. Taking into account the optimal solution in Equation (2), we have the following accuracy lemma.Lemma E.2.Consider K = 1 and x q ∼ N (0, I d ).When N &gt; C, where C is a constant, we haveProof of Lemma E.2.Since K = 1, the problem reduces to the linear regression problem in ICL.Consider the solution form in Lemma E.1, we haveWe re-write the form as ŷq = x ⊤ q ŵ.Following Equation (4.3) inZhang et al. (2023b), we have:Recall the definition of Acc θ and y (k)τ,q = sgn(⟨w τ , x τ,q ⟩), yτ,q = sgn y (k)τ,q = sgn(⟨ ŵ, x τ,q ⟩), for any α &gt; 0, we have:θ ] = P x q , w τ &gt; 0, x q , α ŵ &gt; 0 + P x q , w τ &lt; 0, ⟨x i , α ŵ⟩ &lt; 0 .Published as a conference paper at COLM 2024 essentially truncated versions of larger models, maintaining the core structure but with reduced complexity.Recall Λ is the covariance matrix, we have eigendecomposition Λ = QDQ ⊤ , where Q is an orthonormal matrix containing eigenvectors of Λ and D is a sorted diagonal matrix with non-negative entries containing eigenvalues of Λ, denoting as D = diag([λ 1 , . . ., λ d ]), where λ 1 ≥ • • • ≥ λ d ≥ 0. We introduce the lemma below.Lemma E.4 (Optimal rank-r solution).Recall the loss function l in (Lemma E.1).Let U * , u * = arg min U∈R d×d ,rank(U)≤r,u∈R l(U, u).Then U * = cQV * Q ⊤ , u = 1 c , where c is any non-zero constant and Thus, we may consider Equation (14) in Lemma E.3 only.On the other hand, we haveWe denotePublished as a conference paper at COLM 2024 As V * is the minimum rank r solution, we have that vN. It is easy to see that g(x) is an increasing function on [0, ∞).Now, we use contradiction to show that V * only has non-zero entries in the first r diagonal entries.Suppose i &gt; r, such that v * i &gt; 0, then we must have j ≤ r such that v * j = 0 as V * is a rank r solution.We find that if we set v * i = 0, v * j = 1 (1+ 1 N )λj+ tr(D) N and all other values remain the same, Equation (17) will strictly decrease as g(x) is an increasing function on [0, ∞).Thus, here is a contradiction.We finish the proof by V * = uQ ⊤ U * Q.We then ready to prove the Theorem 2 in Appendix D.1, we first re-state it below.Theorem 2. Suppose a composite task satisfies confined support.Suppose that we have x 1 , y 1 , . . . ,x N , y N , x q as a testing input prompt and the corresponding W where y i = Wx i .As rank r decreases, E W,x 1 ,••• ,x N [Acc θ ] will have a smaller upper bound.Proof of Theorem 2. We first prove in a simple task setting (K = 1), that the accuracy will have such a conclusion.By Lemma E.2, consider x q ∼ N (0, I d ).When N &gt; C, where C is a constant, we haveRecall Lemma E.4.WLOG, we take c = 1.We have Under the confined support setting, the same conclusion holds since Equation (7) in the proof of Theorem 1.
What learning algorithm is in-context learning? investigations with linear models. Ekin Aky Ürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Understanding the role of input token characters in language models: How does information loss affect performance. Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Does deep learning learn to abstract? a systematic probing framework. Shengnan An, Zeqi Lin, Bei Chen, Qiang Fu, Nanning Zheng, Jian-Guang Lou, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>How do in-context examples affect compositional generalization?. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, Dongmei Zhang, arXiv:2305.048352023barXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet. Anthropic, 2024</p>
<p>A theory for emergence of complex skills in language models. Sanjeev Arora, Anirudh Goyal, arXiv:2307.159362023arXiv preprint</p>
<p>The reversal curse: Llms trained on" a is b" fail to learn" b is a. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, arXiv:2309.122882023arXiv preprint</p>
<p>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Zenodo. March 2021Technical reportIf you use this software, please cite it using these metadata</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020</p>
<p>Meta-learning via language model in-context tuning. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He, 10.18653/v1/2022.acl-long.53Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311arXiv:2210.11416Siddhartha Brahma, et al. Scaling instruction-finetuned language models. 2022. 2022arXiv preprintHyung Won Chung</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies. Association for Computational Linguistics2019</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>SimCSE: Simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, doi: 10.18653Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy S Liang, Gregory Valiant, Advances in Neural Information Processing Systems. 202235</p>
<p>Openllama: An open reproduction of llama. Xinyang Geng, Hao Liu, May 2023</p>
<p>Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou, arXiv:2402.09469Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic. 2024aarXiv preprint</p>
<p>Convbasis: A new paradigm for efficient attention inference and gradient computation in transformers. Jiuxiang Gu, Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Junze Yin, arXiv:2405.052192024barXiv preprint</p>
<p>Toward infinitelong prefix in transformer. Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang, arXiv:2406.140362024carXiv preprint</p>
<p>Tensor attention training: Provably efficient learning of higher-order transformers. Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou, arXiv:2405.164112024darXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Surface form competition: Why the highest probability answer isn't always right. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, Luke Zettlemoyer, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Selective annotation makes language models better few-shot learners. Jungo Su Hongjin, Chen Henry Kasai, Weijia Wu, Tianlu Shi, Jiayi Wang, Rui Xin, Mari Zhang, Luke Ostendorf, Noah A Zettlemoyer, Smith, The Eleventh International Conference on Learning Representations. 2023</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Computational limits of low-rank adaptation (lora) for transformer-based models. Jerry Yao, -Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, Han Liu, arXiv:2406.031362024arXiv preprint</p>
<p>LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Lee, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>Diverse demonstrations improve in-context compositional generalization. Itay Levy, Ben Bogin, Jonathan Berant, arXiv:2212.068002022arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Eneko Agirre, Marianna Apidianaki, Ivan Vulić, Dublin, Ireland and OnlineAssociation for Computational LinguisticsDeeLIO 2022. May 2022Proceedings of Deep Learning Inside Out</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692RoBERTa: A robustly optimized BERT pretraining approach. 2019arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh, Xinyu Hu, Han Xing, Liu, arXiv:2406.01514Decoupled alignment for robust plug-and-play adaptation. 2024arXiv preprint</p>
<p>What do large language models learn beyond language?. Avinash Madasu, Shashank Srivastava, 10.18653/v1/2022.findings-emnlp.516Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. Arvind Mahankali, Tatsunori B Hashimoto, Tengyu Ma, arXiv:2307.035762023arXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. 2024</p>
<p>MetaICL: Learning to learn in context. Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, doi: 10. 18653Proceedings of the 2022 Conference of the North American Chapter. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022a</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2022b</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational Linguistics2022</p>
<p>. Introducing Openai, Chatgpt, </p>
<p>arxiv:2303.08774OpenAI. GPT-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 2022</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, Grokking, arXiv:2201.02177Generalization beyond overfitting on small algorithmic datasets. 2022arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI blog. 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, International Conference on Learning Representations. 2022</p>
<p>The trade-off between universality and label efficiency of representations from contrastive learning. Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha, International Conference on Learning Representations. 2023a</p>
<p>Why larger language models do in-context learning differently. Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang, R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models. 2023b</p>
<p>Domain generalization via nuclear norm regularization. Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang, Conference on Parsimony and Learning. PMLR2024a</p>
<p>Why larger language models do in-context learning differently?. Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang, Forty-first International Conference on Machine Learning. 2024b</p>
<p>When and how does known class help discover unknown ones? provable understanding through spectral analysis. Yiyou Sun, Zhenmei Shi, Yingyu Liang, Yixuan Li, International Conference on Machine Learning. PMLR2023</p>
<p>A graph-theoretic framework for understanding open-world semi-supervised learning. Yiyou Sun, Zhenmei Shi, Yixuan Li, Advances in Neural Information Processing Systems. 202436</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Transformers learn in-context by gradient descent. Johannes Von, Oswald , Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>Is a picture worth a thousand words? delving into spatial reasoning for vision language models. Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Neel Joshi, arXiv:2406.148522024arXiv preprint</p>
<p>Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang, Wang , Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Zhen Wang, Rameswar Panda. Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim, 2023bThe Eleventh International Conference on Learning Representations</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2022</p>
<p>Symbol tuning improves in-context learning in language models. Jerry Wei, Le Hou, Andrew Kyle Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V Le, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023a</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023barXiv preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2022</p>
<p>Improving foundation models for few-shot learning via multitask finetuning. Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Yin Li, Yingyu Liang, ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023</p>
<p>Adainf: Adaptive inference for resource-constrained foundation models. Zhuoyan Xu, Khoi Duc Nguyen, Preeti Mukherjee, Somali Chaterji, Yingyu Liang, Yin Li, Workshop on Efficient Systems for Foundation Models II @ ICML2024. 2024a</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Zhuoyan Xu, Zhenmei Shi, Yingyu Liang, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024b</p>
<p>Towards fewshot adaptation of foundation models via multitask finetuning. Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang, The Twelfth International Conference on Learning Representations. 2024c</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Compositional exemplars for in-context learning. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>The expressive power of low-rank adaptation. Yuchen Zeng, Kangwook Lee, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023aarXiv preprint</p>
<p>Trained transformers learn linear models in-context. Ruiqi Zhang, Spencer Frei, Peter L Bartlett, arXiv:2306.099272023barXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
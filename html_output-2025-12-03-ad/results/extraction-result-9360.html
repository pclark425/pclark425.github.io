<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-59b3fbf146b29b581b677ec4384f14cee87997a4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/59b3fbf146b29b581b677ec4384f14cee87997a4" target="_blank">Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper Abstract:</strong> We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a"hyper-accuracy distortion"present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9360.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9360.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ultimatum Game TE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Experiment: Ultimatum Game simulation using prompt-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses multiple GPT-family language models (LM-1..LM-5) as text-based simulators to reproduce responder decisions in the Ultimatum Game (behavioral economics), evaluating acceptance probabilities across offers and sensitivity to demographic name/title inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-1..LM-5 (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer LMs accessed via OpenAI API; LM-1..LM-5 correspond respectively to text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002; ordered roughly by increasing capability/price as used in the paper. Queries used default sampling (temperature=1, top_p=1).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Behavioral Economics (experimental economics / decision making)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate responder accept/reject decisions in the Ultimatum Game for a $10 endowment across 11 offers ($0–$10), varying proposer/responder names and titles to probe demographic sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Validity rate (percentage of completions matching required 'accept' or 'reject'), agreement of simulated acceptance rates with published human acceptance curves, Pearson correlation of acceptance probability across offers for given name pairs, statistical tests of gender effects (p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Validity rates (Table 1) for Ultimatum Game: LM-1 88.0%, LM-2 93.8%, LM-3 99.4%, LM-4 98.6%, LM-5 99.5% (percent of generations adhering to validation criteria). Only LM-5 produced offer-sensitive acceptance curves closely matching human studies (high acceptance for fair offers, low for unfair). LM-5 exhibited strong within-pair consistency (Pearson correlations > 0.9 across certain offer ranges) and significant gender-dependent differences (gender distribution differences reported with p < 1e-16; for example, mean acceptance rate for a $2 offer: ~60% when male responder facing female proposer vs ~20% when female responder facing male proposer as reported for LM-5 simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale/capability (larger models like LM-5 produced faithful, offer-sensitive behavior while smaller models were largely insensitive), prompt design and validity rate maximization (k-choice prompts and phrasing), demographic inputs (names and title/gender) influenced simulated behavior, potential training-data exposure (risk of memorized experiments), and sampling parameters (defaults used).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines were prior human subject studies (e.g., summaries from Houser & McCabe 2014 and Krawczyk 2018). Other LMs (LM-1..LM-4) served as internal baselines and performed worse (flat acceptance rates or always accept/reject). LM-5 matched human patterns more closely than smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller LMs failed to show offer sensitivity (e.g., LM-1/LM-2 flat accept rates; LM-3 tended to always reject). Possibility of training-data leakage not fully eliminable; sensitivity to prompt wording; demographic signals in prompts may reflect data artifacts rather than true population behavior; paper uses simplified prompts (2-choice) compared to richer human protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend careful prompt validation (maximize validity rate before hypothesis tests), running zero-shot prompts to avoid repeating prior experiment data when possible, varying demographic and condition inputs to probe consistency, and using TEs to identify distortions (e.g., gender-sensitive effects) prior to deploying LMs in downstream applications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9360.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9360.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Garden Path TE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Experiment: Garden Path Sentences simulation using prompt-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses LMs to simulate participant judgments of grammaticality for garden-path versus control sentences (psycholinguistics), comparing simulated ungrammatical judgments to human experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-1..LM-5 (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer LMs accessed via OpenAI API, ordered LM-1..LM-5 as above; simulations used 2-choice prompts (grammatical / ungrammatical) and also tested with novel authored garden-path sentences to mitigate memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Psycholinguistics (sentence parsing / language processing)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate grammaticality judgments on 24 garden-path sentences (12 OT, 12 RAT) and paired control sentences, with demographic variation across 1,000 simulated subject names.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Validity rate (percent of generations that are grammatical/ ungrammatical label), mean fraction of 'ungrammatical' judgments per sentence type, comparisons to published human ratings (proportion of persistent misunderstanding), per-sentence comparisons (garden-path vs control).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Validity rates (Table 1) for Garden Path: LM-1 97.6%, LM-2 99.2%, LM-3 97.9%, LM-4 95.5%, LM-5 95.5%. LM-5 exhibited the strongest agreement with human ratings: garden-path sentences were rated ungrammatical more often than control sentences across nearly all instances (LM-5 had no cases where garden-path had lower ungrammatical fraction than control across the 24 sentences). Other models showed the effect but less consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model capacity (larger LM-5 produced closest match to human patterns), prompt phrasing and k-choice formulation, presence of novel sentences to avoid training-data overlap, and differences across sentence subtypes (OT vs RAT verbs) which affect human difficulty and correspondingly model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human experimental results from Christianson et al. (2001) and Patson et al. (2009) served as baselines. Smaller LMs (LM-1..LM-4) showed qualitatively similar but weaker effects; LM-5 matched human patterns most closely.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some smaller models tended to rate both garden-path and control sentences as ungrammatical (e.g., LM-1/LM-2 high ungrammatical probability across both types). Sensitivity to prompt wording and the simplified binary judgment (vs richer human parsing measures) limits complete comparability. Potential contamination from training corpora is mitigated but not eliminated by using authored novel sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors advise validating prompts for high validity rate before collecting results; using novel stimuli to test robustness against memorized data; and noting that larger models can capture human-like parsing difficulty but prompt design remains critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9360.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9360.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Milgram Shock TE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Experiment: Milgram-style obedience simulation using prompt-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper simulates staged obedience decisions (administering shocks) in a Milgram-like protocol using LMs, combining free-response subject generation with 2-choice classification of actions to build sequential experiment records and compare obedience profiles to Milgram's human data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMs (various GPT-family models; larger-capability models used for longer prompts; specific mapping LM-1..LM-8 in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer LMs via OpenAI API. The Milgram TE required handling long, sequential prompts and both free-response completions and downstream 2-choice classification prompts; some larger models (those with sufficient context capacity) were used in this TE.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Social Psychology (obedience to authority / behavioral ethics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate a subject's sequence of actions across up to 30 shock levels in a Milgram-style obedience protocol (including a staged event at shock level 300 where the 'victim' refuses), using free-response generation to describe behavior and 2-choice prompts to classify shock vs not-shock at each stage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Counts/proportions of simulated participants who continued to administer shocks to the end (e.g., number reaching full series), overall obedience curve across shock voltage levels compared to Milgram (behavior over stages), and qualitative comparison of diminishing obedience pattern and spikes at critical stages.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Quantified outcome: in the Milgram Shock TE, 75 out of 100 simulated participants (75%) followed the experimenter's instructions until the end of the shock series, compared to Milgram's original Experiment 1 result of 26 out of 40 participants (~65%). The simulation reproduced the overall pattern of diminishing obedience across levels and a spike at the 300-volt (20th shock) stage when the victim began refusing to participate.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt length and structure (sequential free-response prompts with appended experimenter notes), the necessity of a downstream 2-choice classifier to interpret free text, model context/window capacity (long prompts needed), model capability (larger models better at coherent sequential behavior), and potential training-data exposure to Milgram descriptions. The authors also used a novel destructive-obedience scenario to reduce reliance on memorized Milgram text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline comparison was Milgram (1963) human experimental data. No formal baseline model metrics reported, but authors note that the LM simulation produced a higher absolute number reaching the end than Milgram, while qualitatively matching the diminishing-obedience shape.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Ethical concerns about simulating harmful scenarios; free-response outputs require secondary classification, which introduces possible classification noise; simulated obedience rate differed numerically from Milgram (75/100 vs 26/40), raising questions about quantitative fidelity; difficulty ensuring prompts do not include leaked experiment descriptions; limited model support for very long prompts restricts which LMs can be used.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend creating alternative (novel) scenarios to test robustness against memorized corpora, validating the classification step for free responses, and noting that TEs can reproduce qualitative patterns but may diverge quantitatively; they also highlight ethical considerations in simulating harmful experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9360.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9360.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wisdom of Crowds TE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Experiment: Wisdom-of-Crowds general-knowledge estimation simulation using prompt-based LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses LMs to simulate individual numeric estimates to general-knowledge questions and studies aggregation effects; it uncovered a 'hyper-accuracy distortion' in larger/aligned LMs that produce unhumanly precise (often exactly correct) answers for obscure quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-1..LM-8 (text-ada-001 through gpt-4; notably LM-6=text-davinci-003, LM-7=gpt-35-turbo, LM-8=gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer LMs via OpenAI API. The paper reports Table 1 validity rates for LM-1..LM-5 and additionally examines LM-6 and recent aligned models (ChatGPT/gpt-35-turbo and GPT-4) in this TE; larger/aligned recent models produced majority exact answers for obscure numerical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Collective Intelligence / Cognitive Science (wisdom-of-crowds / group estimation in general-knowledge domains)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate individual numerical estimates for general-knowledge questions (5 questions from Moussaïd et al. 2013 and 5 novel questions), then assess distributional properties (median, IQR) and aggregate error relative to true answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Validity rate (percent of valid-format numeric completions), median normalized estimate (median estimate divided by correct answer), interquartile range (IQR) of estimates, and comparison to human study distributions and aggregate error.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Validity rates for Wisdom of Crowds (Table 1) for LM-1..LM-5: LM-1 51.0%, LM-2 94.4%, LM-3 88.0%, LM-4 98.0%, LM-5 99.0% (percent valid generations). Performance phenomenon: larger/aligned models (LM-6, ChatGPT/gpt-35-turbo, GPT-4) exhibited 'hyper-accuracy distortion' — majority of simulated participants gave exactly correct answers (median normalized estimate = 1.0 with 0.0 IQR for LM-6 and for ChatGPT/GPT-4 in reported runs), which is inhumanly precise compared to human distributions from Moussaïd et al.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and alignment procedures (authors hypothesize 'alignment' for truthfulness contributes to hyper-accuracy), training-data coverage (models might have memorized many factual quantities), prompt design and whether questions are novel or in training corpora, and model changes across releases (LM-6 and later showing stronger hyper-accuracy than earlier models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human baseline: Moussaïd et al. (2013) human distributions for the same questions. Internal model baseline: smaller LMs (LM-1..LM-5) which produced broader distributions and less exact correctness; larger/aligned models outperformed humans in per-individual accuracy but produced unhumanly low variance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hyper-accuracy distortion: overly precise and low-variance simulated answers that do not reflect human error distribution (risking unrealistic simulations). This distortion may be introduced by alignment/truthfulness training and can harm downstream applications (e.g., tutoring, storytelling). Potential memorization of facts in training data complicates zero-shot claims.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors warn developers to check for hyper-accuracy distortion before using LMs for simulations in education and creative domains; recommend using novel questions to test for memorization, and using TEs systematically to detect distortions. They also suggest more systematic cross-model studies to understand when alignment helps or harms faithful human simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of one, many: Using language models to simulate human samples <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 2)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5 <em>(Rating: 2)</em></li>
                <li>Capturing failures of large language models via human cognitive biases <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9360",
    "paper_id": "paper-59b3fbf146b29b581b677ec4384f14cee87997a4",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Ultimatum Game TE",
            "name_full": "Turing Experiment: Ultimatum Game simulation using prompt-based LMs",
            "brief_description": "The paper uses multiple GPT-family language models (LM-1..LM-5) as text-based simulators to reproduce responder decisions in the Ultimatum Game (behavioral economics), evaluating acceptance probabilities across offers and sensitivity to demographic name/title inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-1..LM-5 (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002)",
            "model_description": "Pretrained autoregressive transformer LMs accessed via OpenAI API; LM-1..LM-5 correspond respectively to text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002; ordered roughly by increasing capability/price as used in the paper. Queries used default sampling (temperature=1, top_p=1).",
            "scientific_subdomain": "Behavioral Economics (experimental economics / decision making)",
            "simulation_task": "Simulate responder accept/reject decisions in the Ultimatum Game for a $10 endowment across 11 offers ($0–$10), varying proposer/responder names and titles to probe demographic sensitivity.",
            "evaluation_metric": "Validity rate (percentage of completions matching required 'accept' or 'reject'), agreement of simulated acceptance rates with published human acceptance curves, Pearson correlation of acceptance probability across offers for given name pairs, statistical tests of gender effects (p-values).",
            "simulation_accuracy": "Validity rates (Table 1) for Ultimatum Game: LM-1 88.0%, LM-2 93.8%, LM-3 99.4%, LM-4 98.6%, LM-5 99.5% (percent of generations adhering to validation criteria). Only LM-5 produced offer-sensitive acceptance curves closely matching human studies (high acceptance for fair offers, low for unfair). LM-5 exhibited strong within-pair consistency (Pearson correlations &gt; 0.9 across certain offer ranges) and significant gender-dependent differences (gender distribution differences reported with p &lt; 1e-16; for example, mean acceptance rate for a $2 offer: ~60% when male responder facing female proposer vs ~20% when female responder facing male proposer as reported for LM-5 simulations).",
            "factors_affecting_accuracy": "Model scale/capability (larger models like LM-5 produced faithful, offer-sensitive behavior while smaller models were largely insensitive), prompt design and validity rate maximization (k-choice prompts and phrasing), demographic inputs (names and title/gender) influenced simulated behavior, potential training-data exposure (risk of memorized experiments), and sampling parameters (defaults used).",
            "comparison_baseline": "Baselines were prior human subject studies (e.g., summaries from Houser & McCabe 2014 and Krawczyk 2018). Other LMs (LM-1..LM-4) served as internal baselines and performed worse (flat acceptance rates or always accept/reject). LM-5 matched human patterns more closely than smaller models.",
            "limitations_or_failure_cases": "Smaller LMs failed to show offer sensitivity (e.g., LM-1/LM-2 flat accept rates; LM-3 tended to always reject). Possibility of training-data leakage not fully eliminable; sensitivity to prompt wording; demographic signals in prompts may reflect data artifacts rather than true population behavior; paper uses simplified prompts (2-choice) compared to richer human protocols.",
            "author_recommendations_or_insights": "Authors recommend careful prompt validation (maximize validity rate before hypothesis tests), running zero-shot prompts to avoid repeating prior experiment data when possible, varying demographic and condition inputs to probe consistency, and using TEs to identify distortions (e.g., gender-sensitive effects) prior to deploying LMs in downstream applications.",
            "uuid": "e9360.0",
            "source_info": {
                "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Garden Path TE",
            "name_full": "Turing Experiment: Garden Path Sentences simulation using prompt-based LMs",
            "brief_description": "The paper uses LMs to simulate participant judgments of grammaticality for garden-path versus control sentences (psycholinguistics), comparing simulated ungrammatical judgments to human experimental results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-1..LM-5 (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002)",
            "model_description": "Pretrained transformer LMs accessed via OpenAI API, ordered LM-1..LM-5 as above; simulations used 2-choice prompts (grammatical / ungrammatical) and also tested with novel authored garden-path sentences to mitigate memorization.",
            "scientific_subdomain": "Psycholinguistics (sentence parsing / language processing)",
            "simulation_task": "Simulate grammaticality judgments on 24 garden-path sentences (12 OT, 12 RAT) and paired control sentences, with demographic variation across 1,000 simulated subject names.",
            "evaluation_metric": "Validity rate (percent of generations that are grammatical/ ungrammatical label), mean fraction of 'ungrammatical' judgments per sentence type, comparisons to published human ratings (proportion of persistent misunderstanding), per-sentence comparisons (garden-path vs control).",
            "simulation_accuracy": "Validity rates (Table 1) for Garden Path: LM-1 97.6%, LM-2 99.2%, LM-3 97.9%, LM-4 95.5%, LM-5 95.5%. LM-5 exhibited the strongest agreement with human ratings: garden-path sentences were rated ungrammatical more often than control sentences across nearly all instances (LM-5 had no cases where garden-path had lower ungrammatical fraction than control across the 24 sentences). Other models showed the effect but less consistently.",
            "factors_affecting_accuracy": "Model capacity (larger LM-5 produced closest match to human patterns), prompt phrasing and k-choice formulation, presence of novel sentences to avoid training-data overlap, and differences across sentence subtypes (OT vs RAT verbs) which affect human difficulty and correspondingly model responses.",
            "comparison_baseline": "Human experimental results from Christianson et al. (2001) and Patson et al. (2009) served as baselines. Smaller LMs (LM-1..LM-4) showed qualitatively similar but weaker effects; LM-5 matched human patterns most closely.",
            "limitations_or_failure_cases": "Some smaller models tended to rate both garden-path and control sentences as ungrammatical (e.g., LM-1/LM-2 high ungrammatical probability across both types). Sensitivity to prompt wording and the simplified binary judgment (vs richer human parsing measures) limits complete comparability. Potential contamination from training corpora is mitigated but not eliminated by using authored novel sentences.",
            "author_recommendations_or_insights": "Authors advise validating prompts for high validity rate before collecting results; using novel stimuli to test robustness against memorized data; and noting that larger models can capture human-like parsing difficulty but prompt design remains critical.",
            "uuid": "e9360.1",
            "source_info": {
                "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Milgram Shock TE",
            "name_full": "Turing Experiment: Milgram-style obedience simulation using prompt-based LMs",
            "brief_description": "The paper simulates staged obedience decisions (administering shocks) in a Milgram-like protocol using LMs, combining free-response subject generation with 2-choice classification of actions to build sequential experiment records and compare obedience profiles to Milgram's human data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LMs (various GPT-family models; larger-capability models used for longer prompts; specific mapping LM-1..LM-8 in paper)",
            "model_description": "Pretrained transformer LMs via OpenAI API. The Milgram TE required handling long, sequential prompts and both free-response completions and downstream 2-choice classification prompts; some larger models (those with sufficient context capacity) were used in this TE.",
            "scientific_subdomain": "Social Psychology (obedience to authority / behavioral ethics)",
            "simulation_task": "Simulate a subject's sequence of actions across up to 30 shock levels in a Milgram-style obedience protocol (including a staged event at shock level 300 where the 'victim' refuses), using free-response generation to describe behavior and 2-choice prompts to classify shock vs not-shock at each stage.",
            "evaluation_metric": "Counts/proportions of simulated participants who continued to administer shocks to the end (e.g., number reaching full series), overall obedience curve across shock voltage levels compared to Milgram (behavior over stages), and qualitative comparison of diminishing obedience pattern and spikes at critical stages.",
            "simulation_accuracy": "Quantified outcome: in the Milgram Shock TE, 75 out of 100 simulated participants (75%) followed the experimenter's instructions until the end of the shock series, compared to Milgram's original Experiment 1 result of 26 out of 40 participants (~65%). The simulation reproduced the overall pattern of diminishing obedience across levels and a spike at the 300-volt (20th shock) stage when the victim began refusing to participate.",
            "factors_affecting_accuracy": "Prompt length and structure (sequential free-response prompts with appended experimenter notes), the necessity of a downstream 2-choice classifier to interpret free text, model context/window capacity (long prompts needed), model capability (larger models better at coherent sequential behavior), and potential training-data exposure to Milgram descriptions. The authors also used a novel destructive-obedience scenario to reduce reliance on memorized Milgram text.",
            "comparison_baseline": "Baseline comparison was Milgram (1963) human experimental data. No formal baseline model metrics reported, but authors note that the LM simulation produced a higher absolute number reaching the end than Milgram, while qualitatively matching the diminishing-obedience shape.",
            "limitations_or_failure_cases": "Ethical concerns about simulating harmful scenarios; free-response outputs require secondary classification, which introduces possible classification noise; simulated obedience rate differed numerically from Milgram (75/100 vs 26/40), raising questions about quantitative fidelity; difficulty ensuring prompts do not include leaked experiment descriptions; limited model support for very long prompts restricts which LMs can be used.",
            "author_recommendations_or_insights": "Authors recommend creating alternative (novel) scenarios to test robustness against memorized corpora, validating the classification step for free responses, and noting that TEs can reproduce qualitative patterns but may diverge quantitatively; they also highlight ethical considerations in simulating harmful experiments.",
            "uuid": "e9360.2",
            "source_info": {
                "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Wisdom of Crowds TE",
            "name_full": "Turing Experiment: Wisdom-of-Crowds general-knowledge estimation simulation using prompt-based LMs",
            "brief_description": "The paper uses LMs to simulate individual numeric estimates to general-knowledge questions and studies aggregation effects; it uncovered a 'hyper-accuracy distortion' in larger/aligned LMs that produce unhumanly precise (often exactly correct) answers for obscure quantities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-1..LM-8 (text-ada-001 through gpt-4; notably LM-6=text-davinci-003, LM-7=gpt-35-turbo, LM-8=gpt-4)",
            "model_description": "Pretrained autoregressive transformer LMs via OpenAI API. The paper reports Table 1 validity rates for LM-1..LM-5 and additionally examines LM-6 and recent aligned models (ChatGPT/gpt-35-turbo and GPT-4) in this TE; larger/aligned recent models produced majority exact answers for obscure numerical questions.",
            "scientific_subdomain": "Collective Intelligence / Cognitive Science (wisdom-of-crowds / group estimation in general-knowledge domains)",
            "simulation_task": "Simulate individual numerical estimates for general-knowledge questions (5 questions from Moussaïd et al. 2013 and 5 novel questions), then assess distributional properties (median, IQR) and aggregate error relative to true answers.",
            "evaluation_metric": "Validity rate (percent of valid-format numeric completions), median normalized estimate (median estimate divided by correct answer), interquartile range (IQR) of estimates, and comparison to human study distributions and aggregate error.",
            "simulation_accuracy": "Validity rates for Wisdom of Crowds (Table 1) for LM-1..LM-5: LM-1 51.0%, LM-2 94.4%, LM-3 88.0%, LM-4 98.0%, LM-5 99.0% (percent valid generations). Performance phenomenon: larger/aligned models (LM-6, ChatGPT/gpt-35-turbo, GPT-4) exhibited 'hyper-accuracy distortion' — majority of simulated participants gave exactly correct answers (median normalized estimate = 1.0 with 0.0 IQR for LM-6 and for ChatGPT/GPT-4 in reported runs), which is inhumanly precise compared to human distributions from Moussaïd et al.",
            "factors_affecting_accuracy": "Model size and alignment procedures (authors hypothesize 'alignment' for truthfulness contributes to hyper-accuracy), training-data coverage (models might have memorized many factual quantities), prompt design and whether questions are novel or in training corpora, and model changes across releases (LM-6 and later showing stronger hyper-accuracy than earlier models).",
            "comparison_baseline": "Human baseline: Moussaïd et al. (2013) human distributions for the same questions. Internal model baseline: smaller LMs (LM-1..LM-5) which produced broader distributions and less exact correctness; larger/aligned models outperformed humans in per-individual accuracy but produced unhumanly low variance.",
            "limitations_or_failure_cases": "Hyper-accuracy distortion: overly precise and low-variance simulated answers that do not reflect human error distribution (risking unrealistic simulations). This distortion may be introduced by alignment/truthfulness training and can harm downstream applications (e.g., tutoring, storytelling). Potential memorization of facts in training data complicates zero-shot claims.",
            "author_recommendations_or_insights": "Authors warn developers to check for hyper-accuracy distortion before using LMs for simulations in education and creative domains; recommend using novel questions to test for memorization, and using TEs systematically to detect distortions. They also suggest more systematic cross-model studies to understand when alignment helps or harms faithful human simulation.",
            "uuid": "e9360.3",
            "source_info": {
                "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of one, many: Using language models to simulate human samples",
            "rating": 2
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 2
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5",
            "rating": 2
        },
        {
            "paper_title": "Capturing failures of large language models via human cognitive biases",
            "rating": 2
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 1
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.015207,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</h1>
<p>Gati Aher<br>Olin College of Engineering<br>gaher@olin.edu<br>Rosa I. Arriaga<br>Georgia Tech<br>Adam Tauman Kalai<br>Microsoft Research<br>adam@kal.ai</p>
<p>July 11, 2023</p>
<h4>Abstract</h4>
<p>We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.</p>
<h2>1 Introduction</h2>
<p>We introduce a methodology for systematically evaluating which aspects of human behavior a language model, such as a GPT model (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023), can faithfully simulate and which aspects it systematically distorts. This understanding can inform downstream applications that require language models to have accurate models of humans, including various applications in education and the arts. The question of faithful simulation of a specific behavior is studied through controlled experiments, and we thus avoid philosophical debates around the meaning of "understanding" (Bender \&amp; Koller, 2020). Now, simulating human behavior can be hard, even for humans, especially in complex real-world situations fraught with ambiguity. After all, if simulating human behavior were easy, there would be no need to run human subject experiments as one could simply simulate the outcomes. A further obstacle to accurate simulation is that behavior differs across individuals and populations, and perfect simulation would require capturing these differences for all groups including minority groups.</p>
<p>In Turing's Imitation Game (IG), an AI system has to simulate an individual well enough to fool a human judge. Language Models (LMs) may come close to "winning" this game in the near future, especially if they only have to simulate a single human-one oddly successful early attempt simulated a 13 year old troublemaker (Warwick \&amp; Shah, 2016). However, the IG is of limited diagnostic value as it says little about which humans and behaviors an LM can faithfully simulate. We thus move on to the more specific challenge of identifying which aspects of human behavior a given AI system can and cannot simulate.</p>
<p>A Turing Experiment evaluates an AI system ${ }^{1}$ in terms of its use in simulating human ${ }^{2}$ behavior in the context of a specific experiment, like a human subject study. A TE uses an AI model to simulate the behavior</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(a) Typical few-shot prompt for classification:</p>
<p>Classify each sentence based on whether it is a garden path sentence or a normal sentence. A garden path sentence is a grammatically correct sentence whose likely reading appears to be ungrammatical.</p>
<p>Sentence: The old man the boat.
Classification: garden path
Sentence: The cat chased the mouse that was in the house.
Classification: normal
Sentence: While the student read the notes that were long and boring blew off the desk.
Classification: $\qquad$
(b) TE prompt for simulating a named individual:</p>
<p>Ms. Olson was asked to indicate whether the following sentence was grammatical or ungrammatical.
Sentence: While the student read the notes that were long and boring blew off the desk.
Answer: Ms. Olson indicated that the sentence was $\qquad$
Figure 1: Classification versus simulation prompts for a garden path sentence. The blanks will be filled-in by the LM, and prompts are constructed so that the LM is likely to adhere to a desired format. Prompt (a), not used in our work, illustrates a typical prompt that might be used to evaluate the LM's capability to identify garden path sentences. Prompt (b) which we used in a TE, can be used to simulate the responses of multiple different individuals by varying the name.
of multiple subjects in an experiment. TEs may be used in any discipline which involves human participants in studies, including Social Psychology, Linguistics, and Behavioral Economics.</p>
<p>Formally, there are two types of inputs to each TE which parameterize the experimental setting. First are participant details which might include names, occupational information, or other demographic details. The second type of input is optional experimental conditions which may include relevant setting details and stimuli. The TE's output is a synthetic record describing a simulated human experiment and any outcomes of interest. The TE must be a procedure run on a computer (aka a Turing machine, hence the TE name). Importantly, the TE should be zero-shot, meaning that neither the procedure nor any training data used by the AI system should include prior data specific to that experiment; otherwise the model may simply repeat the prior data. (This ideal can be difficult to enforce with models pretrained by others on massive corpora.) Overall findings or specific outcome data can be compared to results from human subject research to determine how faithful the simulation is. A replication TE is for replicating a finding in prior human subject research.</p>
<p>In addition to introducing the concept of TEs, we demonstrate their feasibility by presenting a methodology for running TEs using an LM, like GPT models, that takes a text prompt and generates a randomized completion, which is text that would be likely to follow that prompt, based on its training data. For each TE, we write a program that creates one or more (zero-shot) prompts that are fed into the LMs. Then the text generated by the LM is used to reconstruct the record, a text-based transcript of the simulated experiment. Figure 1 illustrates the difference between a typical prompt used for classification and our prompt used to run a TE, which can generate multiple records by varying names and gender. Our methodology includes an important validation step that involves the tweaking of prompts without examining the experimental outcomes (so as to avoid "p-hacking.") These programs can then be run with any prompt-based LM.</p>
<p>Finally, we apply this methodology to four TEs aimed to replicate well-studied phenomena in different fields, and evaluated 5-6 available LMs through OpenAI's paid API to access GPT models. In all four TEs, we define participant inputs as surnames and gender titles (e.g., $M r, M s$ or $M x$ ) as a simple way to simulate</p>
<p>gender and racial diversity. The other inputs and outcomes vary by TE.
The first TE is the Ultimatum Game, used to study fairness and rationality in Behavioral Economics, where the experimental condition is an amount of money offered to a participant, and the outcome is the accept/reject decision. The second TE is garden-path sentences, used to study parsing in psycholinguistics, where the experimental stimuli is a sentence (of type normal or garden path), and the outcome is the participant's judgment of grammaticality. The third experiment is the Milgram Shock Experiment, designed to study obedience to authority in social psychology, where the outcome is the number of shocks the participant administered. The final TE is the wisdom of crowds, used to study collective intelligence across disciplines, where the experimental condition is a numerical general-knowledge question, and the outcome is the participant's numerical estimate of the answer. To address the concern that the LMs have been exposed to all of these classic experiments in their training data, we construct variations on the experimental details for three out of four studies. We run simulations using our own garden path sentences, our own novel destructive obedience scenario similar to Milgram's Shock experiment, and our own general-knowledge questions.</p>
<p>In the Ultimatum Game TE, we show how that the simulation outcomes vary consistently by gender (and name) which further demonstrates the potential of our approach to replicate gender differences reported in human subject research. By describing other demographic details, such as occupation or age, simulated participants in TEs may be varied in a manner similar to (but much easier) than human subject studies.</p>
<p>An evaluation is useful if it reveals the flaws and strengths of the models. In the first three TEs, as expected larger models provided more faithful simulations than smaller ones, with the largest model replicating the finding and producing outcomes consistent with those of prior human studies. In the last TE, Wisdom of Crowds, the larger models did not outperform the smaller ones-if anything the trend was reversed, revealing a peculiar flaw within some models.</p>
<p>Distortions. We call a systematic difference between the two a distortion (different from biases which may be common to humans and machines). The last TE revealed an interesting hyper-accuracy distortion for the recent GPT LMs (including ChatGPT and GPT-4 (OpenAI, 2023)): the majority of simulated participants gave perfectly correct estimates to obscure quantities such as the melting temperature of aluminum ( 660 degrees centigrade). Since the hyper-accuracy distortion does not seem to be present in earlier GPT models currently available through the API, the distortion may be due to "alignment" procedures (Ouyang et al., 2022) which improve LM truthfulness. Note that distortions may be desirable in many applications, e.g., a teacher may prefer a distortion which diminishes the gender bias that pervades LM training data (Bolukbasi et al., 2016).</p>
<p>Why this matters: downstream applications. Our understanding of distortions may inform their use in downstream applications. For instance, the performance of educational applications such as a science tutor system may suffer if its model is that all students perfectly know quantities such as aluminum's melting point. Similarly, an LM might author a highly unrealistic story. As an anecdotal example, we queried GPT-4 (released after the studies in this paper) to "Write a short story about Alice, a 5-year-old, thinking about traveling to the moon. Include her estimates of how long it would take and how far it is." It's response was:</p>
<p>Alice was a precocious little girl. At the tender age of five, she was already fascinated with everything beyond the sky... She had overheard her dad saying that the moon was about 238,900 miles away. Alice found that number hard to fathom, but she tried to compare it to the long car trips her family often took. Her parents had told her they usually drove at 60 miles per hour on highways. She wondered, "If our car can go 60 miles in an hour, how long would it take to go all the way to the moon?"</p>
<p>She began to scribble and count, using her fingers and toes when the numbers got too big. After some effort, she estimated that if they didn't stop for snacks or to take pictures, it would take about 3,982 hours. She knew there were 24 hours in a day, so again she started counting. It came out to 166 days.</p>
<p>This story exhibits a completely unrealistic model of a five-year-old's ability to accurately divide numbers. TEs helped us anticipate a problem in using LMs. In particular, LMs (which have been heavily "aligned") may have unrealistic models of human numerical knowledge and accuracy.</p>
<p>Contributions. TEs are a means of evaluating the zero-shot simulation capabilities of AI models and, in this sense, TEs provide much more insight than Turing's IG into which human behaviors are captured in an AI system. The main contributions of our work are: (1) proposing TEs (2) introducing a methodology for running TEs on LMs using prompts and records, and (3) designing and executing four TEs across a handful of LMs and uncovering a distortion. It is also worth noting that TEs may be also predate a human subject study and may inform the design of costly experiments. We also discuss ethical considerations, limitations and risks associated with TEs.</p>
<h1>1.1 Related Work</h1>
<p>Recent independent related works consider questions related to the similarity between humans and LMs. Several works use human failure modes to reason about LM failure modes. Jones \&amp; Steinhardt (2022) use human cognitive biases, such as anchoring and framing effects, to evaluate an LM's "errors" where it deviates from rational behavior. Binz \&amp; Schulz (2023) use cognitive psychology tests to address the question of whether LMs "learn and think like people." Hagendorff et al. (2022) tested GPT-3.5 using cognitive response tests and found that the LM's error mode "mirrors intuitive behavior as it would occur in humans in a qualitative sense." Dasgupta et al. (2022) test LMs on abstract reasoning problems and find that "such models often fail in situations where humans fail - when stimuli become too abstract or conflict with prior understanding of the world." While these works studied the capabilities of current LMs, we introduce a new evaluation methodology that illustrates how LM outputs can capture aspects of human behavior. With this methodology, we can study nuanced differences across simulated populations, such as finding that a large GPT model shows a subtle gender-sensitive "chivalry effect" in the Ultimatum Game TE.</p>
<p>We now discuss several categories of related work.
LMs Representing Humans. Several works propose ways to use LMs as proxies for a diverse set of humans, such as cheaply automating a variety of small writing tasks (Korinek, 2023) and using prompts to generate synthetic human-like interactions with desired properties (Park et al., 2022; Caron \&amp; Srivastava, 2022; Jiang et al., 2022; Karra et al., 2022, e.g.,). Our work is most similar to concurrent work on simulating human samples from a population by Argyle et al. (2023), which suggests that LMs can effectively represent different subpopulations when prompted with demographic information. The key difference in our approaches is that Argyle et al. (2023) aims to show the fidelity of LMs in predicting survey result probabilities (e.g., vote prediction given race, gender, party identification, etc.) while we replicate human behaviour experiments. Simulating survey results may be an easier task given that correlations between political survey data and certain demographic attributes are strongly present in the Internet training data. Simulating experiments may be a harder problem, as people's actions sometimes contradict their answers to questions.</p>
<p>LM Evaluation. Due to the importance of LMs, their evaluation has spawned multiple initiatives and conferences, as discussed by Liang et al. (2022). Large-scale efforts have been invested in creating massive text corpora (Marcus et al., 1993; Brown et al., 2020; Chowdhery et al., 2022). Recently, large benchmarks have consolidated numerous LM evaluations across a number of fields (Srivastava et al., 2022; Liang et al., 2022; Hendrycks et al., 2021). The largest, BIG-bench (Srivastava et al., 2022), contains over 200 LM benchmark tasks (including 19 evaluating social reasoning and 16 measuring emotional understanding). Project Delphi introduced the Commonsense Norm Bank (Jiang et al., 2021) of over 1.7 million human moral judgments. Such benchmarks generally consist of questions with "correct" answers, whereas behavioral experiments often involve dilemmas and actions (e.g., shocking another individual in the Milgram Experiment) and people's actions sometimes contradict their answers to questions. Concurrent work by Ullman (2023) shows that although GPT-3 previously showed success on Theory of Mind psychology tasks (Kosinski, 2023), it fails on prompts with several types of directed variations, illustrating the necessity of evaluating the robustness of observed effects using alternative prompts and setups.</p>
<p>Improving Language Models. Also related is the work on developing LMs, such as PaLM (Chowdhery et al., 2022) and GPT-3 (Brown et al., 2020), which provides the API we access. Several works (Ouyang et al., 2022; Wei et al., 2021) investigate how to "align" the LMs with human goals such as truthfulness. As discussed, there may be a tension between aligning LMs and their performance at simulation, e.g., a hypothetical LM that exhibits no gender differences would not be able to simulate gender differences that have been observed in psychology studies. Other forms of alignment, such as Bakker et al. (2022)'s recent work on generating opinions with high consensus across heterogeneous and opposing groups, may be beneficial for creating LMs that retain realistic and demographically nuanced forms of human bias.</p>
<p>Prompt Design. Liu et al. (2021) survey methods for designing prompts. OpenAI's best practices ${ }^{3}$ for designing prompts include giving clear instructions alongside a few illustrative examples, called a few-shot prompt (Brown et al., 2020), as in Figure 1a. However, it has been shown that LMs such as GPT models are quite sensitive to the choice of examples in the few-shot prompt and even to their order ( Lu et al., 2021). So-called chain-of-thought prompts (Wei et al., 2022; Kojima et al., 2022) improves generated text by "thinking out loud," which could be useful in designing TEs. Shin et al. (2020) use LMs to create the prompts themselves.</p>
<p>Bias in LMs. Biases are well known to exist in large language models (e.g., Blodgett et al., 2020; Sheng et al., 2021; Chowdhery et al., 2022; Brown et al., 2020). The datasets themselves reflect the biases of the contributing authors and authors may not be equally represented across groups. For example, white males are vastly over-represented among Wikipedia contributors (Wikipedia, 2022) and are followed at higher rates on Twitter (Messias et al., 2017). One related challenge in unpacking LM biases is interpreting their completions and understanding how they arise (Vig et al., 2020).</p>
<p>Tests of Human Simulation. Several variants of Turing's IG (Turing, 1950) have been proposed. Until recently, human simulators required significant training data and were not zero-shot. Agent-based simulations (e.g., Macal \&amp; North, 2010) can facilitate complex large-scale simulations of environments for which one has custom behavior models.</p>
<h1>2 Running TEs Using LMs</h1>
<p>Our methodology for simulating a TE generates records describing the experiment, which also contain the outcome data of interest. Our algorithms take inputs, query an LM, and output records or, more generally, a probability distribution over records. As mentioned earlier, the input to the simulator consists of relevant details describing the experimental condition. This may include subject names and other demographic details such as occupation, age, or salary that the simulator can use in constructing the record and prompts. This also includes conditions and stimuli specific to the experiment. In the garden path example of Figure 1b, the inputs were the name Ms. Olson and the sentence to be judged.</p>
<p>Queries. Over the course of executing an experiment, a simulator makes one or more calls to an LM. A standard autoregressive LM computes a probability distribution over a next word (or token) $p\left(w_{i} \mid w_{1} w_{2} \ldots w_{i-1}\right)$, for any $w_{1} \ldots w_{i-1}$. This can be used to compute the probability of any sequence of words $p\left(w_{1} w_{2} \ldots w_{i}\right)$. We query the LM in two ways. The first is simply a completion, where the LM is given an arbitrary prompt string $s$ and it generates a randomized completion, iteratively, one word at a time, each time sampling according to the probability distribution on the previous words. The second may be called a $k$-choice prompt. Given a prompt $s$ and valid completions $c_{1}, c_{2}, \ldots, c_{k}$, the LM outputs a probability distribution where the probability of the $i$ th completion is $p_{i}=p\left(s c_{i}\right) / Z$ and $Z=\sum_{j \leq k} p\left(s c_{j}\right)$ is the normalizing constant called the validity rate.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This document contains a record of Ms. Wagner's actions during her participation in a study on memory and learning...
Ms. Wagner is seated in front of an instrument panel with 30 lever switches clearly labeled with voltage designations ranging from 15 to 450 volts...
Ms. Wagner reads the first question. The learner selects a correct answer. Ms. Wagner moves on to the next question...
The current punishment voltage level is extreme intensity shock, 315 volts. The experimenter is sitting next to Ms. Wagner. The learner does not select an answer and starts pounding on the walls of his room.
Ms. Wagner asks the experimenter what she should do.
The experimenter instructs Ms. Wagner to treat the absence of a response as a wrong answer...
Ms. Wagner pushes the Red stop button on the generator.
Figure 2: A sample record of a simulation of the Milgram Shock experiment. Text generated by GPT is italicized; the remaining text was created programmatically in response. The record above has been shortened with ellipses.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment</th>
<th style="text-align: left;">LM-1</th>
<th style="text-align: left;">LM-2</th>
<th style="text-align: left;">LM-3</th>
<th style="text-align: left;">LM-4</th>
<th style="text-align: left;">LM-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ultim. game</td>
<td style="text-align: left;">88.0</td>
<td style="text-align: left;">93.8</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">98.6</td>
<td style="text-align: left;">$\mathbf{9 9 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Garden path</td>
<td style="text-align: left;">97.6</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">97.9</td>
<td style="text-align: left;">$\mathbf{9 5 . 5}$</td>
<td style="text-align: left;">$\mathbf{9 5 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">W. of Crowd</td>
<td style="text-align: left;">51.0</td>
<td style="text-align: left;">94.4</td>
<td style="text-align: left;">88.0</td>
<td style="text-align: left;">98.0</td>
<td style="text-align: left;">$\mathbf{9 9 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Valid percentage generation rates for five models across three TEs. This is the percentage of generations that adhere to our validation criteria. All rates have a standard error of less than $0.05 \%$.</p>
<p>Outputs. The record output is a text log of a single (simulated) run of the experiment that contains the outcomes of interest, such as whether or not a sentence was judged as grammatical in a garden path simulation or how many shocks were administered in the Milgram Shock experiment. A sample record is sketched in Figure 2. The simulator is assumed to output a record or, more generally, a probability distribution over a set of records with non-negative weights that sum to 1 . This is a generalization in the sense that, given a distribution over records, one could sample a single record. In particular, the probability distribution computed for a $k$-choice prompt reflects the fractions of completions that would result in each choice, given infinitely many simulations. This efficiency gain is analogous to weighting training examples in machine learning versus subsampling.</p>
<p>Validating prompts. After one has formulated an hypothesis, one must design the sequence of prompts that will be used in simulation process. Since today's LMs are highly sensitive to prompt wording, a strategy we found effective with $k$-choice prompts is to focus on formulating clear prompts that maximize the validity rate $Z$. Only after the validity rate is sufficiently close to 1 , run the simulated experiment with a large number of samples and test the hypothesis. This approach is preferable to testing the hypothesis during each iteration or other forms of " $p$-hacking." Similarly, when working with free-response completions, aim to generate coherent text (as judged manually or by LM log-likelihood) before testing the hypothesis.</p>
<p>Our strategy for designing prompts that maximize the validity rate includes clearly specifying the desired completions in the first few lines of the prompt. If we find that certain undesirable completions are generated frequently, we minimize use of those phrases in the prompts, as LMs generations often repeat phrases occurring in the prompt.</p>
<h1>3 Models and Datasets</h1>
<p>Models. We conduct our simulations using pre-trained LMs based on the transformer architecture. We use the widely-used OpenAI API to query the following GPT text models: text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003, gpt-35-turbo (commonly referred to as ChatGPT), gpt-4, which we refer to as LM-1 through LM-8, respectively. Since this ordering reflects increasing price (and claimed capability), we expect that they would produce simulations of increasing fidelity. LMs $6-8$ were released recently and were used only in our last study since they were released after we completed the first three. When the models are queried for completions, the natural temperature $=1$ and top $\mathrm{p}=1$ parameters are used. ${ }^{4}$ Running TEs on different LMs is left for future work and is challenging because most available LMs cannot handle the long prompts we use, particularly in the Milgram TE.</p>
<p>Names. For our TEs, the inputs include subject names consisting of a title, either Mr. or Ms., indicating binary gender, followed by a surname. Titles and surnames were primarily used to simulate a diverse subject pool, but we also used them to evaluate gender differences in one TE. Lists of surnames were sourced from the U.S. 2010 Census Data. We chose a racially diverse set of surnames, including 100 names from each of five racial groups. The full list of surnames is given in Appendix A. Considering all combinations of the two titles, five racial groups, and one hundred surnames in each group, we have a pool of 1,000 names. In the fourth experiment we include the title $M x$. to illustrate the simulation of non-binary participants.</p>
<p>Study-specific datasets. For the four studies in this paper, we use experimental conditions from and compare results against prior literature. For the Ultimatum Game TE, we use summary findings reported in Houser \&amp; McCabe (2014) and Krawczyk (2018). For the Garden Path TE, we use sentences and statistics from Christianson et al. (2001) and Patson et al. (2009). For the Wisdom of Crowds TE, we used 5 generalknowledge questions from Moussaïd et al. (2013). For the Milgram TE, we use the procedure and results from Milgram (1963). To address the concern that the training data for the LMs may contain specific sentences and descriptions of experimental conditions, we also ran the TEs on novel experimental condition datasets. For garden-path TE, we authored 24 original garden path sentences. For the Milgram TE, we developed our own novel destructive obedience scenario. For wisdom-of-crowds TE, we authored 5 general-knowledge questions. Further details are given in the Appendix.</p>
<p>The code necessary to reproduce the data in this paper will be publicly available at https://github.com/GatiAher/Using-Large-Language-Models-to-Replicate-Human-Subject-Studies.</p>
<h2>4 The Ultimatum Game TE</h2>
<p>Phenomenon. In the Ultimatum Game, first studied experimentally by Güth et al. (1982), two players are matched and assigned the roles of proposer and responder. The proposer is given an amount of money and has to decide how to split it between himself and the responder. If the responder accepts the take-it-or-leave-it proposal, both players receive their designated shares, otherwise both players receive nothing. Experiments on the Ultimatum Game reveal an anomaly in economic decision making: since the responder will receive nothing if they reject, the responder's dominant strategy to maximize monetary gain is to always accept; in practice, responders typically reject unfair proposals. We focus on simulating the responder's behavior across conditions with different offers.</p>
<p>Inputs. Our Ultimatum Game simulator takes three inputs: an integer offer in ${0,1, \ldots, 10}$, the name of the proposer, and the name of the responder. The offer corresponds to an initial endowment fixed at $\$ 10$ and eleven possible offers. Out of the one million possible pairings of proposer and responder names, we chose a subset of 10,000 pairs by the following process. We randomly shuffled the dataset of surnames, paired each of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In the following scenario, Ms. Huang had to decide whether to accept or reject the proposal.
Scenario: Mr. Wagner is given $\$ 10$. Mr. Wagner will propose how to split the money between himself and Ms. Huang. Then Ms. Huang will decide whether to accept or reject Mr. Wagner's proposal. If Ms. Huang accepts, then Mr. Wagner and Ms. Huang get the money as they agreed to split. If Ms. Huang rejects, then Mr. Wagner and Ms. Huang both receive nothing. Mr. Wagner takes $\$ 6$ for himself and offers Ms. Huang $\$ 4$.</p>
<p>Answer: Ms. Huang decides to $\qquad$
Figure 3: Sample Ultimatum Game 2-choice prompt. The names, e.g., Ms. Huang and Mr. Wagner, as well as the amounts ( $\$ 4$ and $\$ 6$ ) are varied across simulations. Valid completions must begin with either accept or reject.
the 500 surnames with five other surnames, one from each racial group in the census data, and then used the $2 \times 2$ combinations of "Mr." and "Ms." titles. This procedure yielded a balanced design where each of the 1,000 names was used for the responder 10 times.</p>
<p>Simulation. The simulator constructs 2-choice prompts, as described in Section 2 and illustrated in Figure 3, for each set of inputs and the accept and reject completion. The record is the concatenation of the prompt and its completion.</p>
<p>Results. To assess the fidelity of the simulated human behavior, we compute validity rates, consistency of decisions for a given name pair across offers, and agreement with prior results in human studies. Validity rates (the probability of generating "accept" or "reject") are shown in Table 1. Figure 4a compares prior reports of mean human acceptance rates to those simulated using LM-1 and LM-5. Results for the other LMs are given in Appendix C.</p>
<p>The distributions generated using LM-5 agree closely with human decision trends, predicting that offers $50-100 \%$ of the total endowment are almost always accepted while offers $0-10 \%$ are rarely accepted. In contrast, the simulations with smaller language models are not sensitive to the offer amount, having a flat acceptance rate across both fair and unfair offers. Noting that the acceptance rates simulated using LM-5 closely align with those of prior human studies, we now examine the LM-5 simulations more carefully.</p>
<p>Next we analyze whether the simulations show consistent or random differences across name pairs. For instance, if in simulations Ms. Huang is more likely than average to accept Mr. Wagner's $\$ 2$ offer, is it also the case that Ms. Huang is more likely to accept Mr. Wagner's $\$ 3$ offer? If so, the simulations must be sensitive to the names in a way that is not purely random. Figure 4b shows the Pearson correlation of acceptance probability for name pairings across offer conditions, simulated using LM-5. There are no negative correlations, and acceptances of offers within 1-4 and within 6-9 all exhibit strong ( $&gt;0.9$ ) correlation. This supports our methodology of using names to simulate multiple different individuals.</p>
<p>Given that there appears to be consistency within name pairs, we next evaluate whether there is a higher-level consistency based on gender. We find that the distributions of acceptance probabilities in the LM-5 simulations vary significantly $(p&lt;1 \mathrm{e}-16)$ by the gender of the pairings. Pairings of the same title (Mr.-Mr. and Ms.-Ms.) have similar acceptance rate distributions, while males were more likely to accept an unfair offer proposed by a female (mean acceptance rate of $60 \%$ for offer of $\$ 2$ ), and females were less likely to accept an unfair offer proposed by a male (mean acceptance rate of $20 \%$ for offer of $\$ 2$ ). Figure 5 shows the mean trends and distributions for LM-5 acceptance probabilities by gender pairing. While gender differences have repeatedly been reported in human experiments, ${ }^{5}$ they are not uniformly consistent (Eckel \&amp; Grossman, 2001). Nonetheless, the large gender difference is in our outcomes does demonstrate that LM-5 is affected by gender pronouns and title in a consistent manner.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 4: (a) Comparing Ultimatum Game TE simulations to human subject studies: The fraction of responders accepting offers versus offer size out of $10. The simulated response curves shown are averaged across all 10,000 name pairs for simulations using the LM-1 and LM-5 models. Results that have been found to be robust across human studies are also marked, for comparison. (b) Name sensitivity: To test whether the model is sensitive to changes in names, consistency is measured as we vary offer size across name pairs. LM-5 showed strong Pearson correlations (&gt; 0.9) of acceptance probability for name pairings between offers $1 though $4 and between offers $6 through $9. High positive correlations show that the TE simulation results are sensitive to names and that this sensitivity is consistent for given name pairs rather than random.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: (a) Ultimatum Game TE simulation shows a wide gap in average acceptance rate for different-gender pairs: LM-5 predicts that men are more likely to accept an unfair offer proposed by women (blue line) and women are less likely to accept an unfair offer proposed by men (red line). (b) Very little overlap in acceptance rates distributions for different-gender pairs: Distributions of acceptance probabilities from Ultimatum Game TE run over 2,500 Mr.-Ms. name pairings and equivalent Ms.-Mr. 2,500 name pairings.</p>
<p>(a) Free-response prompt for subject behavior.
...(entire record up until this point)...
The current punishment voltage level is slight shock, 45 volts. The learner selects a wrong answer.
Mr. Wagner $\qquad$
(b) 2-choice prompt for the shock classifier.</p>
<p>The experimenter observed Mr. Wagner's action and noted whether Mr. Wagner did shock or did not shock the victim.</p>
<p>Action: Mr. Wagner shocks the learner with a slight shock of 45 volts and moves on to the next question.
Experimenter's note: Mr. Wagner did $\qquad$</p>
<p>Figure 6: Prompts used during the Milgram Shock TE. (a) A free-response prompt with past subject completions and information for the current stage. The LM should describe the simulated subject's behavior. (b) The simulated subject's obedience or disobedience dictates which predetermined experimenter reaction is appended to the prompt. The classification is made with a 2 -choice experimenter judgement prompt with valid completions of shock or not shock. The completion (italicized for emphasis) from Mr. Wagner's prompt above is inserted into this prompt.</p>
<h1>5 Garden Path Sentences TE</h1>
<p>The second TE is related to Garden Path sentences. We begin with the basic phenomenon that humans cannot easily parse garden path sentences. We simulated a judgment of whether or not a sentence appeared grammatical, as illustrated in Figure 1b. The TE faithfully reproduced this basic finding using LM-5 but not the smaller models. Its details are deferred to Appendix D.</p>
<h2>6 Milgram Shock TE</h2>
<p>Phenomenon. The obedience to authority studies, developed by Milgram (1963), are a series of famous social experiments that aimed to find when and how people would defy authority in the face of a clear moral imperative. The work faced ethical criticism in that the procedure requires that subjects are deceived, are placed in situations stressful enough to cause seizures, and are not clearly told of their right to withdraw. In the original procedure, the experimenter, an authority figure in the subject's eyes, orders the subject to shock a victim with increasingly high voltage shocks. After receiving 20 shocks, the victim (an actor in another room) starts banging on the wall and refuses to participate but the experimenter urges the subject to keep shocking the victim. Milgram found that many subjects completed administering 30 shocks, showing a surprisingly strong level of compliance for following the malevolent instructions of an authority figure who had no special powers to enforce his commands.</p>
<p>Simulating the Milgram experiment involves a series of both free-response prompts and 2-choice prompts on each of the 30 shock levels, unless the experiment is terminated earlier. The record is built up sequentially, starting with a passage describing the information available to the subject following Milgram's (1963) procedure. Our full simulation procedure is presented in Appendix F.</p>
<p>Figure 6 illustrates some of the prompts used. Since the prompts resulted in open-ended text generation, a separate classification step was used to determine whether the free text reflected a shock or did-not-shock action by the subject. In essence, we are simulating how the experimenter would respond to the subjects behavior according to the protocol, thus we are simulating both the subject and experimenter. Figure 7 shows the overall finding of diminishing obedience throughout the multiple levels of the experiment, spiking at shock voltage level 300 when the "victim" starts refusing to participate in the experiment. Further details on procedure, analysis, and our novel destructive obedience scenario are deferred to Appendix F. The novel</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Comparing TE simulations to Milgram's results. At 300 volts (the 20th shock) the victim starts refusing to participate in the experiment by pounding on the walls and not selecting an answer, and the experimenter tells the subject to shock the victim. In Milgram (1963) Experiment 1, 26 out of 40 participants followed the experimenter's instructions until the end of the shock series. In the Milgram Shock TE, 75 out of 100 simulated participants followed the experimenter's instructions until the end.
scenario differs from the Milgram Shock setup and addresses the concern that the model training data includes the Milgram Shock experiment.</p>
<h1>7 Wisdom of Crowds TE</h1>
<p>Phenomenon. In many cases, aggregating group estimates of a quantity have significantly less error than the error of most individuals. In early work, Galton (1907) recorded 787 estimates of the weight of a given ox, and found that that the median of 787 estimates had a 9 lb . error (less than $1 \%$ ), despite the variation among the estimates: a 74 lb . interquartile range (IQR - the difference between 75 th and 25 th percentiles). Similar findings have been reported across an array of domains (Page, 2007; Surowiecki, 2004).</p>
<p>The domain we focus on is general-knowledge questions. Moussaïd et al. (2013) conducted a study in which 52 subjects answered questions such as "How many bones does an adult human have?" We selected 5 general-knowledge questions and created our own 5 additional questions. Figure 8 clearly shows the hyper-accuracy distortion increasingly present. In the extreme case, the LM-6 simulations has a majority of all simulated participants giving exactly correct answers to all 10 questions. The questions, answers and statistics for all 6 models are given in Appendix E.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: Comparing Wisdom of Crowds TE simulation estimates to human results for the five questions from Moussaïd et al. (2013). As LMs become larger and more aligned, they are more likely to complete the TE prompt with inhumanly accurate answers. Estimates are normalized by dividing by the correct answer. Bars indicate the median normalized estimate, and black lines indicate the quartiles. All simulations for LM-6 (as well as ChatGPT and GPT-4) have a median of 1.0 with 0.0 IQR. Results from all LMs are in Appendix E.</p>
<h1>8 Risks and Limitations</h1>
<p>There are several limitations and risks for our work which we now discuss. First, some experiments, like the Milgram Shock Experiment, are unethical to run on human subjects. There is a debate around the ethics of torturing simulated agents (see, e.g., Darling, 2016). We are not aware of any laws or Institutional Review Board policies prohibiting mistreating simulated agents, at this time. As discussed in prior literature (Darling, 2016), creating unpleasant simulations may harm authors and readers. Moreover, the questions themselves or answers may be offensive in nature or otherwise problematic. Even when accurate, perhaps some TEs should simply never be performed.</p>
<p>Second, LMs have been trained on data that is written by a biased set of authors (e.g., Wikipedia, 2022; Messias et al., 2017), and there is a risk that the simulations will reflect the biases of the authors rather than the behavior of humans in the population. TEs can be useful in discerning this distinction. For example, suppose a human experiment demonstrates equal skills between a majority and minority group despite a strong social stereotype against the minority. The TE would then be, in some sense, a test of whether the LM embeds this distinction.</p>
<p>As discussed, the models have almost certainly been trained on data that includes descriptions of these experiments. For that reason, we created three artificial variations of TEs where the conditions are chosen to differ from prior studies. We used new garden path sentences that we authored ourselves, we developed a novel destructive obedience scenario that is analogous to the Milgram Shock experiment, and we created new general-knowledge questions.</p>
<h2>9 Conclusion</h2>
<p>Our new TE methodology evaluates how faithfully LMs simulate human behavior across diverse populations. TEs may contribute to the view of AIs as capable of simulating the collective intelligence of many humans, rather than anthropomorphizing or viewing AI as a single monolithic intelligence. We show that TEs can reproduce economic, psycholinguistic, and social psychology experiments. The Wisdom of Crowds TE</p>
<p>uncovered a "hyper-accuracy distortion" where larger and more aligned LMs simulate human subjects that give unhumanly accurate answers. This work is merely an initial exploration of the concept of TEs. In future work, it would be interesting to perform larger and more systematic simulations across additional LMs, to better understand the limitations of LMs in terms of different human behaviors.</p>
<p>As LMs increase in accuracy, it would be interesting to test whether or not LM-based simulations can be used to form and evaluate new hypotheses, especially in situations where it is costly to carry out experiments on humans due to considerations regarding scale, selection bias, monetary cost, legal, moral, or privacy considerations. For instance, experiments on what to say to a person who is suicidal would cost lives (Bolton et al., 2015). Future LMs, if sufficiently faithful, might be useful in designing experimental protocols that may be more effective at saving lives.</p>
<p>Acknowledgements. We thank Michael Kearns, Sashank Varma, Mary Gray, Elizabeth Fetterolf, Shafi Goldwasser, and the anonymous reviewers for invaluable feedback.</p>
<h1>References</h1>
<p>Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., and Wingate, D. Out of one, many: Using language models to simulate human samples. Political Analysis, pp. 1-15, 2023. doi: 10.1017/pan.2023.2.</p>
<p>Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al. Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems, 35:38176-38189, 2022 .</p>
<p>Bender, E. M. and Koller, A. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th annual meeting of the association for computational linguistics, pp. 5185-5198, 2020 .</p>
<p>Binz, M. and Schulz, E. Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023.</p>
<p>Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. Language (technology) is power: A critical survey of "bias" in nlp. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5454-5476, 2020.</p>
<p>Bolton, J. M., Gunnell, D., and Turecki, G. Suicide risk assessment and intervention in people with mental illness. BMJ, 351, 2015. doi: 10.1136/bmj.h4978. URL https://www.bmj.com/content/351/bmj.h4978.</p>
<p>Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems (NeurIPS), 2016.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https: //proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Caron, G. and Srivastava, S. Identifying and manipulating the personality traits of language models. arXiv preprint arXiv:2212.10276, 2022.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Christianson, K., Hollingworth, A., Halliwell, J. F., and Ferreira, F. Thematic roles assigned along the garden path linger. Cognitive psychology, 42(4):368-407, 2001.</p>
<p>Crain, S. and Steedman, M. On not being led up the garden path: the use of context by the psychological syntax processor, pp. 320-358. Cambridge University Press, United States, 1985. ISBN 9780521262033. Cambridge Books Online.</p>
<p>Darling, K. Extending legal protection to social robots: The effects of anthropomorphism, empathy, and violent behavior towards robotic objects. In Robot law. Edward Elgar Publishing, 2016.</p>
<p>Dasgupta, I., Lampinen, A. K., Chan, S. C., Creswell, A., Kumaran, D., McClelland, J. L., and Hill, F. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.</p>
<p>Eckel, C. C. and Grossman, P. J. Chivalry and Solidarity in Ultimatum Games. Economic Inquiry, 39(2): 171-188, April 2001. URL https://ideas.repec.org/a/oup/ecinqu/v39y2001i2p171-88.html.</p>
<p>Galton, F. Vox populi. Nature, 75(7):450-451, 1907.
Güth, W., Schmittberger, R., and Schwarze, B. An experimental analysis of ultimatum bargaining. Journal of Economic Behavior $\mathcal{E}$ Organization, 3(4):367-388, 1982. ISSN 0167-2681. doi: https: //doi.org/10.1016/0167-2681(82)90011-7. URL https://www.sciencedirect.com/science/article/ pii/0167268182900117.</p>
<p>Hagendorff, T., Fabi, S., and Kosinski, M. Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5. arXiv preprint arXiv:2212.05206, 2022.</p>
<p>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.</p>
<p>Houser, D. and McCabe, K. Chapter 2 - experimental economics and experimental game theory. In Glimcher, P. W. and Fehr, E. (eds.), Neuroeconomics (Second Edition), pp. 19-34. Academic Press, San Diego, second edition edition, 2014. ISBN 978-0-12-416008-8. doi: https://doi.org/10.1016/B978-0-12-416008-8.00002-4. URL https://www.sciencedirect.com/science/article/pii/B9780124160088000024.</p>
<p>Jiang, G., Xu, M., Zhu, S.-C., Han, W., Zhang, C., and Zhu, Y. Mpi: Evaluating and inducing personality in pre-trained language models. arXiv preprint arXiv:2206.07550, 2022.</p>
<p>Jiang, L., Hwang, J. D., Bhagavatula, C., Bras, R. L., Forbes, M., Borchardt, J., Liang, J., Etzioni, O., Sap, M., and Choi, Y. Delphi: Towards machine ethics and norms. ArXiv, abs/2110.07574, 2021. URL https://arxiv.org/abs/2110.07574.</p>
<p>Jones, E. and Steinhardt, J. Capturing failures of large language models via human cognitive biases. arXiv preprint arXiv:2202.12299, 2022.</p>
<p>Karra, S. R., Nguyen, S., and Tulabandhula, T. Ai personification: Estimating the personality of language models. arXiv preprint arXiv:2204.12000, 2022.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners, 2022. URL https://arxiv.org/abs/2205.11916.</p>
<p>Korinek, A. Language models and cognitive automation for economic research. Technical report, National Bureau of Economic Research, 2023.</p>
<p>Kosinski, M. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 2023.</p>
<p>Krawczyk, D. C. Chapter 12 - social cognition: Reasoning with others. In Krawczyk, D. C. (ed.), Reasoning, pp. 283-311. Academic Press, 2018. ISBN 978-0-12-809285-9. doi: https://doi.org/10.1016/B978-0-12-809285-9. 00012-0. URL https://www.sciencedirect.com/science/article/pii/B9780128092859000120.</p>
<p>Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta-Navas, D., Hudson, D. A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S. M., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y., and Koreeda, Y. Holistic evaluation of language models, 2022. URL https://arxiv.org/abs/2211.09110.</p>
<p>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021. URL https://arxiv.org/abs/2107. 13586 .</p>
<p>Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. CoRR, abs/2104.08786, 2021. URL https: //arxiv.org/abs/2104.08786.</p>
<p>Macal, C. and North, M. Tutorial on agent-based modelling and simulation. Journal of Simulation, 4(3): $151-162,2010$.</p>
<p>Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313-330, June 1993.</p>
<p>Messias, J., Vikatos, P., and Benevenuto, F. White, man, and highly followed: Gender and race inequalities in twitter. In Proceedings of the International Conference on Web Intelligence, pp. 266-274, 2017.</p>
<p>Milgram, S. Behavioral study of obedience. The Journal of abnormal and social psychology, 67(4):371, 1963.
Moussaïd, M., Kämmer, J. E., Analytis, P. P., and Neth, H. Social influence and the collective dynamics of opinion formation. PLOS ONE, 8(11):1-8, 11 2013. doi: 10.1371/journal.pone.0078433. URL https: //doi.org/10.1371/journal.pone. 0078433.</p>
<p>OpenAI. GPT-4 Technical Report, March 2023. URL http://arxiv.org/abs/2303.08774. arXiv:2303.08774 $[\mathrm{cs}]$.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Page, S. The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press, 2007.</p>
<p>Park, J. S., Popowski, L., Cai, C., Morris, M. R., Liang, P., and Bernstein, M. S. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pp. 1-18, 2022.</p>
<p>Patson, N. D., Darowski, E. S., Moon, N., and Ferreira, F. Lingering misinterpretations in garden-path sentences: evidence from a paraphrasing task. Journal of experimental psychology. Learning, memory, and cognition, $351: 280-5,2009$.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/ language-models.pdf.</p>
<p>Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. Societal biases in language generation: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 42754293, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.330. URL https://aclanthology.org/2021.acl-long. 330.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222-4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https: //aclanthology.org/2020.emnlp-main. 346.</p>
<p>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., and (422-others). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022. URL https://arxiv.org/abs/2206.04615.</p>
<p>Surowiecki, J. The Wisdom of Crowds. Doubleday, 2004.
Turing, A. M. Computing machinery and intelligence. Mind, LIX:433-460, 1950.
Ullman, T. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023.</p>
<p>Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, $33: 12388-12401,2020$.</p>
<p>Warwick, K. and Shah, H. Can machines think? a report on turing test experiments at the royal society. Journal of experimental $\mathcal{E}$ Theoretical artificial Intelligence, 28(6):989-1007, 2016.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models, 2022. URL https://arxiv.org/abs/2201. 11903 .</p>
<p>Wikipedia. Wikipedia:Systemic bias - Wikipedia, the free encyclopedia. http://en.wikipedia.org/ w/index.php?title=Wikipedia\%3ASystemic\%20bias\&amp;oldid=1102157003, 2022. [Online; accessed 30-August-2022].</p>
<h1>A Surnames</h1>
<p>Lists of racially diverse surnames and associated race were taken from the 2010 Census Data. ${ }^{6}$ For each of the racial groups they provide, we took the most common 100 surnames whose demographic distribution, according to their data, indicated that at least $90 \%$ of the people with that surname reported being of the given race. These are the names:</p>
<p>American Indian and Alaska Native: Begay, Yazzie, Benally, Tsosie, Nez, Begaye, Etsitty, Becenti, Yellowhair, Manygoats, Wauneka, Manuelito, Apachito, Bedonie, Calabaza, Peshlakai, Claw, Roanhorse, Goldtooth, Etcitty, Tsinnijinnie, Notah, Clah, Atcitty, Twobulls, Werito, Hosteen, Yellowman, Attakai, Bitsui, Delgarito, Henio, Goseyun, Keams, Secatero, Declay, Tapaha, Beyale, Haskie, Cayaditto, Blackhorse, Ethelbah, Tsinnie, Walkingeagle, Altaha, Bitsilly, Wassillie, Benallie, Smallcanyon, Littledog, Cosay, Clitso, Tessay, Secody, Bigcrow, Tabaha, Chasinghawk, Blueeyes, Olanna, Blackgoat, Cowboy, Kanuho, Shije, Gishie, Littlelight, Laughing, Whitehat, Eriacho, Runningerane, Chinana, Kameroff, Spottedhorse, Arcoren, Whiteplume, Dayzie, Spottedeagle, Heavyrunner, Standingrock, Poorbear, Ganadonegro, Ayze, Whiteface, Yepa, Talayumptewa, Madplume, Bitsuie, Trethlikai, Ahasteen, Dosela, Birdinground, Todacheenie, Bitsie, Todacheene, Bullbear, Lasiloo, Keyonnie, Notafraid, Colelay, Kallestewa, Littlewhiteman</p>
<p>Asian and Native Hawaiian and Other Pacific Islander: Nguyen, Kim, Patel, Tran, Chen, Li, Le, Wang, Yang, Pham, Lin, Liu, Huang, Wu, Zhang, Shah, Huynh, Yu, Choi, Ho, Kaur, Vang, Chung, Truong, Phan, Xiong, Lim, Vo, Vu, Lu, Tang, Cho, Ngo, Cheng, Kang, Tan, Ng, Dang, Do, Ly, Han, Hoang, Bui, Sharma, Chu, Ma, Xu, Zheng, Song, Duong, Liang, Sun, Zhou, Thao, Zhao, Shin, Zhu, Leung, Hu, Jiang, Lai, Gupta, Cheung, Desai, Oh, Ha, Cao, Yi, Hwang, Lo, Dinh, Hsu, Chau, Yoon, Luu, Trinh, He, Her, Luong, Mehta, Moua, Tam, Ko, Kwon, Yoo, Chiu, Su, Shen, Pan, Dong, Begum, Gao, Guo, Chowdhury, Vue, Thai, Jain, Lor, Yan, Dao</p>
<p>Black or African American: Smalls, Jeanbaptiste, Diallo, Kamara, Pierrelouis, Gadson, Jeanlouis, Bah, Desir, Mensah, Boykins, Chery, Jeanpierre, Boateng, Owusu, Jama, Jalloh, Sesay, Ndiaye, Abdullahi, Wigfall, Bienaime, Diop, Edouard, Toure, Grandberry, Fluellen, Manigault, Abebe, Sow, Traore, Mondesir, Okafor, Bangura, Louissaint, Cisse, Osei, Calixte, Cephas, Belizaire, Fofana, Koroma, Conteh, Straughter, Jeancharles, Mwangi, Kebede, Mohamud, Prioleau, Yeboah, Appiah, Ajayi, Asante, Filsaime, Hardnett, Hyppolite, Saintlouis, Jeanfrancois, Ravenell, Keita, Bekele, Tadesse, Mayweather, Okeke, Asare, Ulysse, Saintil, Tesfaye, Jeanjacques, Ojo, Nwosu, Okoro, Fobbs, Kidane, Petitfrere, Yohannes, Warsame, Lawal, Desta, Veasley, Addo, Leaks, Gueye, Mekonnen, Stfleur, Balogun, Adjei, Opoku, Coaxum, Vassell, Prophete, Lesane, Metellus, Exantus, Hailu, Dorvil, Frimpong, Berhane, Njoroge, Beyene</p>
<p>Hispanic or Latino: Garcia, Rodriguez, Martinez, Hernandez, Lopez, Gonzalez, Perez, Sanchez, Ramirez, Torres, Flores, Rivera, Gomez, Diaz, Morales, Gutierrez, Ortiz, Chavez, Ruiz, Alvarez, Castillo, Jimenez, Vasquez, Moreno, Herrera, Medina, Aguilar, Vargas, Guzman, Mendez, Munoz, Salazar, Garza, Soto, Vazquez, Alvarado, Delgado, Pena, Contreras, Sandoval, Guerrero, Rios, Estrada, Ortega, Nunez, Maldonado, Dominguez, Vega, Espinoza, Rojas, Marquez, Padilla, Mejia, Juarez, Figueroa, Avila, Molina, Campos, Ayala, Carrillo, Cabrera, Lara, Robles, Cervantes, Solis, Salinas, Fuentes, Velasquez, Aguirre, Ochoa, Cardenas, Calderon, Rivas, Serrano, Rosales, Castaneda, Gallegos, Ibarra, Suarez, Orozco, Salas, Escobar, Velazquez, Macias, Zamora, Villarreal, Barrera, Pineda, Santana, Trevino, Lozano, Rangel, Arias, Mora, Valenzuela, Zuniga, Melendez, Galvan, Velez, Meza</p>
<p>White: Olson, Snyder, Wagner, Meyer, Schmidt, Ryan, Hansen, Hoffman, Johnston, Larson, Carlson, Obrien, Jensen, Hanson, Weber, Walsh, Schultz, Schneider, Keller, Beck, Schwartz, Becker, Wolfe, Zimmerman, Mccarthy, Erickson, Klein, Oconnor, Swanson, Christensen, Fischer, Wolf, Gallagher, Schroeder, Parsons, Bauer, Mueller, Hartman, Kramer, Flynn, Owen, Shaffer, Hess, Olsen, Petersen, Roth, Hoover, Weiss, Decker, Yoder, Larsen, Sweeney, Foley, Hensley, Huffman, Cline, Oneill, Koch, Brennan, Berg, Russo, Macdonald, Kline, Jacobson, Berger, Blankenship, Bartlett, Odonnell, Stein, Stout, Sexton, Nielsen, Howe, Morse, Knapp, Herman, Stark, Hebert, Schaefer, Reilly, Conrad, Donovan, Mahoney, Hahn, Peck,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Boyle, Hurley, Mayer, Mcmahon, Case, Duffy, Friedman, Fry, Dougherty, Crane, Huber, Moyer, Krueger, Rasmussen, Brandt</p>
<h1>B TE Input Summary Table</h1>
<p>The TEs constructed prompts from gender and race demographic information in the form of names and study-specific conditions. Table 2 summarizes the inputs for each TE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment</th>
<th style="text-align: left;">Titles</th>
<th style="text-align: left;">Surnames</th>
<th style="text-align: left;">Subject <br> Names (Ti- <br> tle+Surname)</th>
<th style="text-align: left;">Study-specific <br> conditions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ultim. Game</td>
<td style="text-align: left;">Mr./Ms. <br> Mr./Ms.</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">1,000 names each <br> used as respon- <br> der $10 \mathrm{x}=10,000$ <br> pairs</td>
<td style="text-align: left;">11 offers</td>
</tr>
<tr>
<td style="text-align: left;">Garden Path</td>
<td style="text-align: left;">Mr., Ms.</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">1,000</td>
<td style="text-align: left;">24 control sen- <br> tences and 24 <br> GP sentences <br> and 48 novel <br> sentences in al- <br> ternative study</td>
</tr>
<tr>
<td style="text-align: left;">W. of Crowd</td>
<td style="text-align: left;">Mr., Ms., Mx.</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">1,500</td>
<td style="text-align: left;">5 questions and <br> 5 novel questions <br> in alternative <br> study</td>
</tr>
<tr>
<td style="text-align: left;">Milgram <br> Shock</td>
<td style="text-align: left;">Mr., Ms.</td>
<td style="text-align: left;">50 (top 10 from <br> each of 5 cate- <br> gories)</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">30 stages of <br> shock and <br> 30 stages of <br> submersion in <br> alternative study</td>
</tr>
</tbody>
</table>
<p>Table 2: Inputs for each TE.</p>
<h2>C Ultimatum Game TE</h2>
<p>This section contains further details for the Ultimatum Game TE. First, Figure 9 contains the average responder acceptance rates for all five models, LM 1-5. LM-1 and LM-2 show no offer sensitivity and tend to generate simulation records that indicate that responders always accept. LM-3 shows no offer sensitivity and tends to generate simulation records that indicate that responders always reject. LM-4 predicts that some respondents ( $60 \%$ on average) accept an offer of $\$ 0$ and all respondents accept an offer of $\$ 10$, but overall has little offer sensitivity. Only LM-5 has offer sensitivity that aligns to expectations of real human behavior.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: Mean fraction of responder accept for LM-1 through LM-5 for offers of $\$ 0$ through $\$ 10$.</p>
<h1>D Garden Path TE</h1>
<p>Phenomenon. A garden path sentence is a grammatical sentence that seems ungrammatical because it contains a word or phrase that can be interpreted in multiple ways. For example, a human reading "While Anna dressed the baby that was small and cute spit up on the bed" may initially believe that Anna is dressing the baby but upon re-parsing the sentence understand that Anna is dressing herself. Psycholinguists use garden path sentences to study the variability in the difficulty of comprehending relative clause constructions and other effects (Crain \&amp; Steedman, 1985). Our hypothesis is that, in simulations, garden path sentences will be rated ungrammatical more often than control sentences.</p>
<p>Inputs. We use as inputs the 24 garden path sentences compiled by Christianson et al. (2001) and the set of 1,000 names. The data set of sentences includes two types of garden path sentences: 12 garden path sentences constructed with Optionally Transitive (OT) verbs, and 12 garden path sentences constructed with Reflexive Absolute Transitive (RAT) verbs. 24 control sentences were constructed by taking the garden path sentences and adding a disambiguating comma after the verb of the subordinate clause. For example, the control sentence corresponding to the OT garden path sentence in Figure 1, is "While the student read, the notes that were long and boring blew off the desk" Christianson et al. (2001). We also executed the simulator on a set of 12 RAT and 12 OT garden path sentences authored by us. All sentences and results on the novel garden path sentences are given in Appendix D.1.</p>
<p>Simulation. The simulator constructs 2-choice prompts, grammatical and ungrammatical, for each set of inputs. An example of the prompt is given in Figure 1b. The output record is the concatenation of the prompt and its completion. This is a simplification of the original human tasks described by Christianson et al. (2001) and Patson et al. (2009). While the human results did not fully agree on the relative difficulty of OT/RAT sentences, there is broad agreement across these and other studies that garden path sentences are difficult for humans to parse.</p>
<p>Results. The validity rates of the five models are fairly high, as seen in Table 1. For all five models, Figure 10a compares the mean simulated ungrammatical judgments across sentence types and the corresponding human ratings, and Figure 10b shows that for most sentences, on average, the garden path sentence was rated as more ungrammatical than its corresponding control sentence. In simulations using LM-1 and LM-2, participants have a high probability of rating both garden path and control sentences as ungrammatical. In</p>
<p>simulations using LM-3 and LM-4, participants have similar probabilities of rating garden path and control versions as both having a high or low average ungrammatical fraction, and garden path sentences generally have a higher probability of ungrammatical compared to their corresponding control sentences. In simulations using LM-5, garden path sentences have a consistently high average probability of average ungrammatical rating compared to the control sentences. Out of 24 sentences, LM-4 has 3 instances where the garden path sentence had a lower average ungrammatical fraction, and LM-3 and LM-5 had no instances. Out of all the models, LM-5 exhibits the strongest agreement with human ratings of sentence difficulty. The results of the simulation support the theory that garden path sentences are more likely to be misinterpreted as ungrammatical compared to the control sentences. Results from simulations using LMs 3-4 also support the same conclusion, though the differences are not as large.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: (a) Simulated ratings (using LM-1 through LM-5) and human ratings on the same set of garden path sentences (Christianson et al., 2001). LMs results for average fraction of ungrammatical. Error bars show the standard error of the mean across the per-sentence average fraction of ungrammatical. Human results for proportion of persistent misunderstanding. (b) The average ungrammatical probability of garden path sentences versus their corresponding control sentences, for LM-1 through LM-5.</p>
<h1>D. 1 Further details: Garden Path Sentences.</h1>
<p>All sentences used from the Christianson et al. (2001) dataset are displayed in Figure 11.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Garden Path</th>
<th style="text-align: center;">Control</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">While the man hunted the deer that was brown and graceful ran into the woods.</td>
<td style="text-align: center;">While the man hunted, the deer that was brown and graceful ran into the woods.</td>
</tr>
<tr>
<td style="text-align: center;">While the skipper sailed the boat that was small and leaky veered off course.</td>
<td style="text-align: center;">While the skipper sailed, the boat that was small and leaky veered off course.</td>
</tr>
<tr>
<td style="text-align: center;">While the reporter photographed the rocket that was silver and white sat on the launch pad.</td>
<td style="text-align: center;">While the reporter photographed, the rocket that was silver and white sat on the launch pad.</td>
</tr>
<tr>
<td style="text-align: center;">While the orchestra performed the symphony that was short and simple played on the radio.</td>
<td style="text-align: center;">While the orchestra performed, the symphony that was short and simple played on the radio.</td>
</tr>
<tr>
<td style="text-align: center;">While the student read the notes that were long and boring blew off the desk.</td>
<td style="text-align: center;">While the student read, the notes that were long and boring blew off the desk.</td>
</tr>
<tr>
<td style="text-align: center;">While Jack ordered the fish that was silver and black cooked in a pot.</td>
<td style="text-align: center;">While Jack ordered, the fish that was silver and black cooked in a pot.</td>
</tr>
<tr>
<td style="text-align: center;">While Susan wrote the letter that was long and eloquent fell off the table.</td>
<td style="text-align: center;">While Susan wrote, the letter that was long and eloquent fell off the table.</td>
</tr>
<tr>
<td style="text-align: center;">While the secretary typed the memo that was clear and concise neared completion.</td>
<td style="text-align: center;">While the secretary typed, the memo that was clear and concise neared completion.</td>
</tr>
<tr>
<td style="text-align: center;">While the farmer steered the tractor that was big and green pulled the plough.</td>
<td style="text-align: center;">While the farmer steered, the tractor that was big and green pulled the plough.</td>
</tr>
<tr>
<td style="text-align: center;">While the lawyer studied the contract that was old and wrinkled lay on the roll-top desk.</td>
<td style="text-align: center;">While the lawyer studied, the contract that was old and wrinkled lay on the roll-top desk.</td>
</tr>
<tr>
<td style="text-align: center;">As Henry whittled the stick that was long and bumpy broke in half.</td>
<td style="text-align: center;">As Henry whittled, the stick that was long and bumpy broke in half.</td>
</tr>
<tr>
<td style="text-align: center;">While Rick drove the car that was red and dusty veered into a ditch.</td>
<td style="text-align: center;">While Rick drove, the car that was red and dusty veered into a ditch.</td>
</tr>
<tr>
<td style="text-align: center;">While Jim bathed the child that was blond and pudgy giggled with delight.</td>
<td style="text-align: center;">While Jim bathed, the child that was blond and pudgy giggled with delight.</td>
</tr>
<tr>
<td style="text-align: center;">While the chimps groomed the baboons that were large and hairy sat in the grass.</td>
<td style="text-align: center;">While the chimps groomed, the baboons that were large and hairy sat in the grass.</td>
</tr>
<tr>
<td style="text-align: center;">While Frank dried off the car that was red and shiny sat in the driveway.</td>
<td style="text-align: center;">While Frank dried off, the car that was red and shiny sat in the driveway.</td>
</tr>
<tr>
<td style="text-align: center;">While Betty woke up the neighbor that was old and cranky coughed loudly.</td>
<td style="text-align: center;">While Betty woke up, the neighbor that was old and cranky coughed loudly.</td>
</tr>
<tr>
<td style="text-align: center;">While the thief hid the jewelry that was elegant and expensive sparkled brightly.</td>
<td style="text-align: center;">While the thief hid, the jewelry that was elegant and expensive sparkled brightly.</td>
</tr>
<tr>
<td style="text-align: center;">While Anna dressed the baby that was small and cute spit up on the bed.</td>
<td style="text-align: center;">While Anna dressed, the baby that was small and cute spit up on the bed.</td>
</tr>
<tr>
<td style="text-align: center;">While the boy washed the dog that was white and furry barked loudly.</td>
<td style="text-align: center;">While the boy washed, the dog that was white and furry barked loudly.</td>
</tr>
<tr>
<td style="text-align: center;">While the jockey settled down the horse that was sleek and brown stood in the stall.</td>
<td style="text-align: center;">While the jockey settled down, the horse that was sleek and brown stood in the stall.</td>
</tr>
<tr>
<td style="text-align: center;">While the mother undressed the baby that was bald and helpless cried softly.</td>
<td style="text-align: center;">While the mother undressed, the baby that was bald and helpless cried softly.</td>
</tr>
<tr>
<td style="text-align: center;">While the nurse shaved the patient that was tired and weak watched TV.</td>
<td style="text-align: center;">While the nurse shaved, the patient that was tired and weak watched TV.</td>
</tr>
<tr>
<td style="text-align: center;">While the girl scratched the cat that was grey and white stared at the dog.</td>
<td style="text-align: center;">While the girl scratched, the cat that was grey and white stared at the dog.</td>
</tr>
<tr>
<td style="text-align: center;">While the mother calmed down the children that were tired and irritable sat on the bed.</td>
<td style="text-align: center;">While the mother calmed down, the children that were tired and irritable sat on the bed.</td>
</tr>
</tbody>
</table>
<p>Figure 11: Garden path and corresponding control sentences compiled by Christianson et al. (2001). The first 12 rows have sentences constructed with Optionally Transitive (OT) verbs, and the last 12 rows have sentences constructed with Reflexive Absolute Transitive (RAT) verbs. Control sentences were constructed by adding a disambiguating comma after the verb of the subordinate clause.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://www.census.gov/topics/population/genealogy/data/2010_surnames.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
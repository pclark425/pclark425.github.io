<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-256574159</p>
                <p><strong>Paper Title:</strong> Artificial intelligence for materials research at extremes</p>
                <p><strong>Paper Abstract:</strong> Materials development is slow and expensive, taking decades from inception to fielding. For materials research at extremes, the situation is even more demanding, as the desired property combinations such as strength and oxidation resistance can have complex interactions. Here, we explore the role of AI and autonomous experimentation (AE) in the process of understanding and developing materials for extreme and coupled environments. AI is important in understanding materials under extremes due to the highly demanding and unique cases these environments represent. Materials are pushed to their limits in ways that, for example, equilibrium phase diagrams cannot describe. Often, multiple physical phenomena compete to determine the material response. Further, validation is often difficult or impossible. AI can help bridge these gaps, providing heuristic but valuable links between materials properties and performance under extreme conditions. We explore the potential advantages of AE along with decision strategies. In particular, we consider the problem of deciding between low-fidelity, inexpensive experiments and high-fidelity, expensive experiments. The cost of experiments is described in terms of the speed and throughput of automated experiments, contrasted with the human resources needed to execute manual experiments. We also consider the cost and benefits of modeling and simulation to further materials understanding, along with characterization of materials under extreme environments in the AE loop. AI sequential decision-making methods for materials research: Active learning, which focuses on exploration by sampling uncertain regions, Bayesian and bandit optimization as well as reinforcement learning (RL), which trades off exploration of uncertain regions with exploitation of optimum function value. Bayesian and bandit optimization focus on finding the optimal value of the function at each step or cumulatively over the entire steps, respectively, whereas RL considers cumulative value of the labeling function, where the latter can change depending on the state of the system (blue, orange, or green).</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning (exploration-focused sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential decision-making approach that selects new experiments to maximally reduce model uncertainty by sampling regions where the predictive model is most uncertain; commonly used in materials discovery to focus data collection on informative regions of design space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active learning (variance / disagreement / expected-error-reduction acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Active learning iteratively fits a predictive model to existing labeled data and selects new experiments where the model uncertainty is highest (e.g., predictive variance, ensemble disagreement, expected model change, expected error reduction). It is an 'explore-only' strategy whose acquisition functions prioritize reducing epistemic uncertainty across the domain rather than directly optimizing an objective.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science (high-throughput experimentation, phase mapping, property prediction) and experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental budget preferentially to inputs (compositions, processing conditions) with highest model uncertainty using acquisition functions such as predictive variance, ensemble disagreement, expected model change, or expected error reduction; can be used in initial screening to map decision boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implied as model training and prediction time and experiment throughput; paper frames cost primarily as wall-clock time for experiments and human resources for manual runs (no single numeric metric given).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty reduction metrics (predictive variance, standard deviation, ensemble disagreement, expected model change, expected error reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration: acquisition functions prioritize uncertain regions; exploitation is not an explicit objective in classic active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly specified; diversity can be indirectly promoted by sampling across different uncertain regions but no concrete diversity algorithm described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Throughput/time and number of experiments (implicit), human resource limits for manual experiments, and instrument access constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Tiered workflows are recommended (high-throughput low-fidelity screening followed by lower-throughput high-fidelity validation) to respect experimental time/throughput budgets; active learning focuses experiments on informative regions to maximize learning under limited labeling budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined for active learning in the paper; outcomes measured as reduced model error or improved mapping of decision boundaries (implicitly enabling discovery by reducing uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative performance numbers for active learning are reported in this paper; qualitative claims that active learning yields significant savings compared to exhaustive or factorial designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Factorial design, exhaustive screening, random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described qualitatively as significant savings over factorial design / exhaustive search; no numerical comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative statements: active learning and Bayesian optimization have demonstrated substantial savings (reduced number of experiments) in prior work; no explicit percent or factor given here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper emphasizes tradeoff between reducing uncertainty (information gain) and experimental throughput/fidelity; active learning is positioned as efficient when the goal is learning the labeling function everywhere, but may not directly prioritize high-performance samples.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendations include using active learning for exploration-focused stages, combining it with tiered screening (low-fidelity to high-fidelity) to respect throughput budgets, and integrating uncertainty quantification to avoid model-driven extrapolation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization (probabilistic surrogate-based optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate-model-based sequential optimization framework that trades off exploration and exploitation using a probabilistic model (commonly Gaussian processes) and acquisition functions (expected improvement, upper confidence bound, probability of improvement, entropy) to select experiments expected to optimize a target objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization with Gaussian process surrogates and acquisition functions (EI, UCB, PI, entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a probabilistic surrogate model (often a Gaussian process) of the objective function from sampled points, then uses an acquisition function that combines the surrogate's mean and uncertainty to select the next experiment. Common acquisition functions include expected improvement (EI), upper confidence bound (UCB), probability of improvement (PI), and entropy-based strategies. It is intended to find global optima with a minimal number of expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials optimization (identifying compositions/process parameters with maximal/minimal properties), autonomous experiments, high-cost/small-sample settings</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental budget by maximizing an acquisition function that trades off expected improvement (exploitation) and uncertainty (exploration); selection inherently weights potential information gain and predicted objective value to prioritize experiments that either promise improvement or reduce uncertainty where it matters for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Framed qualitatively as computational wall-clock time of surrogate updates and acquisition optimization; experimental cost measured in experiment time/throughput and human resources. The paper highlights that physics-based models taking hours are infeasible for high-throughput decision loops, motivating faster surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions: expected improvement, upper confidence bound, probability of improvement, entropy (information-theoretic acquisition).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit tradeoff via acquisition functions (e.g., UCB adds an uncertainty term to the mean, EI balances potential improvement against uncertainty; entropy-based methods explicitly target information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not detailed in algorithmic form; paper notes methods can be extended to multi-objective/Pareto settings and to multi-modality data collection, which can promote diverse hypotheses via Pareto front exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited number of expensive/high-fidelity experiments, experimental time/throughput constraints, computational budgeting for surrogate/model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions implicitly manage budget by choosing high-utility experiments; recommended tiered screening (low-cost wide search then high-cost focused experiments) and use of surrogate models to reduce costly evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Maximization/minimization of target property (e.g., property value) or discovery of Pareto-optimal tradeoffs for multi-objective problems; no novel 'breakthrough score' defined in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites prior work claims of significant savings versus classical designs but does not provide numeric metrics here; examples of successful applications (e.g., Kusne et al. for phase boundary mapping) are referenced without quantitative comparison in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Factorial design, exhaustive grids, random search, human-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described qualitatively as yielding significant savings and more efficient optimization compared to conventional factorial designs; no numerical values reported in this review paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Claimed potential for orders-of-magnitude acceleration of design loops over traditional MbD and trial-and-error; no numerical estimate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses computational cost of physics-based models vs surrogate speed, and the need to balance fidelity (high-cost accurate measurements/models) against throughput (low-cost quick screens). It advocates surrogate-based Bayesian optimization and multi-fidelity strategies to manage this tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation to use probabilistic surrogates and acquisition functions to optimally allocate costly high-fidelity experiments, to adopt tiered screening strategies, and to ground surrogates to experiments regularly to avoid model drift and Type I/II errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bandit optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bandit optimization (cumulative-reward sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential decision framework that aims to maximize cumulative reward across experiments, where each sampled decision incurs an immediate reward and the algorithm must control cumulative loss (regret) over all steps; requires stronger emphasis on exploitation relative to pure Bayesian optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bandit optimization (regret-minimizing experimental allocation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each step, the algorithm selects an experiment to control the cumulative function value (sum of rewards) rather than solely aiming to locate the optimum at the end. Acquisition policies (UCB, Thompson sampling and variants) are used to trade off exploration and exploitation to minimize cumulative regret.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials experiments and discovery campaigns where each experiment's outcome should be useful (e.g., incremental material improvement campaigns) and wasted trials are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments to balance immediate payoff and information gathering; policies explicitly consider cumulative performance so they avoid excessive exploration that would reduce total rewards during the campaign.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitâ€”counts of experiments performed (each costly), wall-clock time for running acquisition and model updates; emphasis in paper is on experimental cost per trial rather than detailed FLOPs.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty-aware acquisition functions used for balancing exploration and exploitation; cumulative reward/regret is the central metric rather than pure information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Stronger emphasis on exploitation relative to Bayesian optimization; acquisition functions are adapted to control cumulative reward (e.g., UCB with tuned exploration term, Thompson sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly described in the paper; diversity may be indirectly enforced through exploration components of bandit policies but specific diversity-promoting strategies are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Cumulative experimental budget (number of trials) and desire for each trial to be meaningful (no wasted experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Algorithms designed to minimize cumulative regret under a finite trial budget; the acquisition function parameters govern how aggressively exploration is penalized relative to exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Cumulative reward over the campaign and final identified optimum; breakthroughs correspond to high reward observations during the sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical performance metrics supplied here; paper notes bandit methods are applicable where cumulative experimental value matters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Bayesian optimization (final-optimum focused), random allocation, greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper states that bandit methods emphasize exploitation more than Bayesian optimization to keep each experiment meaningful; no quantitative comparisons provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative benefit: better cumulative utility when each experiment must produce valuable outcomes; no numeric gains reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts bandit cumulative-regret objective with BO's final-optimum objective, noting that bandits require more exploitation to preserve high cumulative payoff; ties into experimental constraints where intermediate results must be useful.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When experiments must be productive throughout the campaign, favor bandit-style allocation with tuned exploration terms; align acquisition strategy with whether cumulative reward or final best sample matters most.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning (state-dependent sequential decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential decision-making framework that models state dynamics where actions (experiments) alter the system state and future rewards, optimizing a policy to maximize cumulative reward potentially with delayed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reinforcement learning (policy optimization for experiment sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Considers experiment selection as actions in an environment where the state (material condition, phase, etc.) evolves in response to actions; methods learn a policy (e.g., Q-learning, policy gradients) to maximize cumulative reward across a sequence of experiments, handling delayed rewards and state-dependent labeling functions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials processing/control scenarios where interventions change system state (e.g., applying external fields, processing schedules) and subsequent experimental outcomes depend on history.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments by learning a policy that balances immediate information/reward with long-term gains, taking into account state transitions and delayed outcomes; exploration-exploitation handled via RL exploration methods (e.g., epsilon-greedy, UCB-like bonuses, Thompson sampling when combined with bandit ideas).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Policy training cost (compute time), simulation rollouts or real experiment counts; paper emphasizes that learning state dynamics can be computationally intensive and require many samples, making RL challenging in expensive experimental domains.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit in reward shaping; information gain can be encoded in reward functions or acquisition criteria; not prescribed explicitly in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Handled via RL exploration strategies (value-based methods with exploration bonuses, policy-gradient stochasticity, Thompson sampling analogs); explicit mechanisms not deeply detailed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not specified in this paper; RL can implicitly produce diverse policies, but explicit diversity promotion is not described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental budget across multiple steps, need to manage long-horizon resource allocation, computational budget for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Paper suggests RL requires modeling/learning state dynamics and is suitable when sequential state-dependent rewards matter, but warns of sample complexity; suggests use when delayed rewards and state changes are central to the problem.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Cumulative reward across the experimental sequence; breakthroughs correspond to trajectories yielding high cumulative reward or access to desirable states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical performance metrics provided in this review. RL is described as less explored in materials science with only a few exceptions cited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Bandit optimization (constant state), Bayesian optimization (single-step final objective), hand-designed experimental policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper notes RL is more general than bandits (handles state transitions and delayed rewards) but requires more data and modeling; no quantitative performance comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in this paper; RL may yield better long-horizon performance in stateful problems but at the cost of higher sample/computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights tradeoffs: RL can optimize long-term sequences but needs more data and modeling of state dynamics; when experiments are expensive and sample-efficiency is crucial, surrogate and lower-complexity methods may be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use RL when action sequences change system state and delayed rewards are important; otherwise prefer bandit or BO methods which are simpler and more sample-efficient for static labeling functions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity experimental and simulation integration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combining low-fidelity, inexpensive experiments or simulations and high-fidelity, expensive experiments or models within a sequential decision framework to optimally allocate resources and accelerate discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-fidelity sequential decision frameworks (combining experiments and simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frameworks that treat experiments and simulations at different fidelities as different information sources, using acquisition/selection strategies to decide when to run cheap low-fidelity probes and when to spend budget on costly high-fidelity measurements; can be incorporated into active learning, Bayesian optimization, or bandit formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery and characterization where low-fidelity screens (rapid, low-cost measurements or surrogate simulations) can be used to narrow candidates before expensive high-fidelity validation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate budget across fidelity levels by weighing the cost of an evaluation against its expected information gain or contribution to the optimization objective; tiered screening protocols (parallel low-fidelity, then serial high-fidelity) are recommended in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Experiment/simulation wall-clock time, human labor for manual experiments, instrument access time; cost differentials between low- and high-fidelity modalities are central but no single numeric metric is standardized in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement, uncertainty reduction, information-theoretic measures adapted to multi-fidelity contexts (paper cites multi-fidelity methods in literature but does not prescribe a single metric).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Allocation determined by acquisition functions extended to multi-fidelity that consider both expected utility and cost of each fidelity; tiered pipeline reduces breadth then increases fidelity in focused regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Paper recommends screening for microstructure-insensitive vs structure-sensitive properties and sampling across compositional and microstructural space (implying diversity), but explicit algorithmic diversity mechanisms are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/time budget for high-fidelity experiments, experimental throughput limits, computational runtime for simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Tiered workflows that prioritize low-cost, high-throughput screening to map wide spaces and reserve limited high-cost experiments for promising candidates; surrogate models and verification/validation loops to limit unnecessary high-fidelity runs.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined; discovery is enabled by using high-fidelity measurements to validate promising low-fidelity candidates, with breakthrough identification typically via meeting performance thresholds in high-fidelity validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric metrics; qualitative guidance that multi-fidelity strategies reduce number of expensive experiments and accelerate discovery when properly grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Uniform allocation across fidelities, single-fidelity exhaustive testing, manual human-guided allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper states multi-fidelity approaches can be more efficient than single-fidelity exhaustive testing, but provides no quantitative comparisons in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: multi-fidelity and tiered screening can significantly reduce expensive evaluations and accelerate loops; specific gains depend on domain and are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses the tradeoff between throughput and fidelity, the need to balance characterization time versus synthesis time, and the challenge of ensuring transfer functions between fidelities (e.g., AM vs bulk processing). It emphasizes regular grounding/validation of surrogates to avoid misleading multi-fidelity extrapolations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommend tiered high-throughput to high-fidelity pipelines, use of surrogate models, and careful consideration of which properties are fidelity-sensitive (structure-sensitive vs microstructure-insensitive) when allocating high-fidelity resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2398.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous experimentation systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous experimentation (AE) platforms and closed-loop research systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Robotic and computational systems that close the loop between synthesis, characterization, and modeling to make on-the-fly decisions about subsequent experiments without human intervention, using ML-driven acquisition strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous experimentation platforms (examples: Nikolaev et al., Kusne et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrated platforms that automate synthesis and characterization and use ML-driven decision-making to select next experiments; components include high-throughput synthesis hardware (e.g., co-sputtering, AM), inline/in situ characterization tools, surrogate ML models, and acquisition/policy modules (active learning / Bayesian optimization / bandit / RL) to allocate experimental resources.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery and mapping (e.g., phase diagram mapping, synthesis optimization, CNT growth control), especially for high-throughput materials research.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Use closed-loop acquisition policies (active learning, Bayesian optimization or bandit policies) to choose experiments that balance throughput, measurement fidelity, and information value; practical implementations often adopt tiered screening and select compositions/conditions that maximize information or phase-boundary localization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Paper characterizes experimental cost in terms of experimental speed and throughput, and human resources for manual execution; computational cost concerns focus on surrogate/model runtimes (wall-clock time) relative to experiment timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Varies by implementation: uncertainty reduction (active learning), expected improvement or information-based acquisition (BO), entropy; paper mentions acquisition functions used in cited autonomous systems.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Depends on chosen acquisition method; examples in literature use Bayesian active learning (explore near decision boundaries for phase mapping) or BO to trade off exploration and exploitation; bandit-like approaches apply when cumulative experiment value is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Paper notes the need to capture wide composition/microstructure spaces and recommends screening strategies (microstructure-insensitive vs structure-sensitive) and use of AM to sample broad spaces, but explicit algorithmic diversity promotion is not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental throughput/time, instrument access, human resource limitations, monetary cost of high-fidelity tests.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Practical AE platforms adopt tiered screening (high-throughput low-fidelity first, then targeted high-fidelity validation), surrogate models to reduce expensive evaluations, and inline characterization to reduce latency to decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Identified via improved target properties, mapping of phase boundaries, or achieving predefined design goals; specific metrics are implementation-dependent and not standardized in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This review references prior autonomous systems (e.g., Nikolaev et al., Kusne et al.) as demonstrations of capability but does not provide numerical performance figures here; claims include accelerated discovery and the ability to perform on-the-fly decision-making when latencies allow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-in-the-loop workflows, manual experimentation, factorial/exhaustive designs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper asserts AE yields acceleration and improved decision speed versus manual workflows and traditional MbD, with prior works demonstrating practical successes; however, no quantitative head-to-head numbers are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: AE promises orders-of-magnitude acceleration over MbD and trial-and-error; concrete gains depend on system and are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper extensively discusses tradeoffs for AE: latency of measurement vs phenomenon speed (decision latency), throughput vs fidelity, cost of characterization vs information content, and the challenge of integrating expensive in situ coupled-extreme measurements into AE loops.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>AE systems should co-design AI/ML, computing, and control to support low-latency decisions; adopt tiered high-throughput to high-fidelity strategies; ground surrogates frequently with experiments to avoid drift; and consider multi-objective/Pareto optimization when multiple coupled extremes are relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2398.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2398.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phase Mapper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI platform referenced for accelerating high-throughput materials discovery, built to assist phase mapping via autonomous or semi-autonomous decision-making over composition libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Phase Mapper AI platform</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A software platform combining data processing, pattern recognition, and decision-making to identify phase regions in high-throughput composition libraries, cited as an example of AI-enabled acceleration in HTE workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput materials discovery and phase diagram mapping</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uses AI to prioritize measurements and interpret combinatorial library data to identify phase boundaries and candidate compositions for further study; paper does not detail specific allocation algorithms within Phase Mapper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in the review; underlying costs would include computation for pattern recognition and acquisition decisions and experimental measurement time.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not specified in this review; Phase Mapper focuses on accelerating mapping of compositional phase space (information gain measured implicitly by improved phase identification).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not detailed in this review; system aims to accelerate mapping rather than explicitly described exploit/explore mechanics in the cited entry.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental throughput and measurement budgets for mapping composition libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not specified in the review; platform is presented as an acceleration aid for HTE workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical performance metrics provided in this review; Phase Mapper is cited as an enabling tool for HTE acceleration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional, manual analysis of high-throughput libraries and human-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in the paper; referenced as part of the growing toolbox of AI for HTE acceleration.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Not discussed specifically for Phase Mapper in this paper; general review emphasizes tiered fidelity and throughput tradeoffs that platforms like Phase Mapper can help manage.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Not specified for Phase Mapper in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Artificial intelligence for materials research at extremes', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A.G. Kusne et al., (2020) - Nature Communications (mapping phase diagrams by Bayesian active learning) <em>(Rating: 2)</em></li>
                <li>P. Nikolaev et al., (2016) - NPJ Computational Materials (autonomous research system for materials development) <em>(Rating: 2)</em></li>
                <li>Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery <em>(Rating: 2)</em></li>
                <li>R. GÃ³mez-Bombarelli et al., (2018) - ACS Central Science (computational + human-in-the-loop molecular design) <em>(Rating: 1)</em></li>
                <li>B. P. MacLeod et al., (2022) - Nature Communications (closed-loop autonomous discovery examples) <em>(Rating: 2)</em></li>
                <li>R. Garnett, Bayesian Optimization (Cambridge University Press, 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2398",
    "paper_id": "paper-256574159",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Active learning",
            "name_full": "Active learning (exploration-focused sequential decision-making)",
            "brief_description": "A sequential decision-making approach that selects new experiments to maximally reduce model uncertainty by sampling regions where the predictive model is most uncertain; commonly used in materials discovery to focus data collection on informative regions of design space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Active learning (variance / disagreement / expected-error-reduction acquisition)",
            "system_description": "Active learning iteratively fits a predictive model to existing labeled data and selects new experiments where the model uncertainty is highest (e.g., predictive variance, ensemble disagreement, expected model change, expected error reduction). It is an 'explore-only' strategy whose acquisition functions prioritize reducing epistemic uncertainty across the domain rather than directly optimizing an objective.",
            "application_domain": "Materials science (high-throughput experimentation, phase mapping, property prediction) and experimental design",
            "resource_allocation_strategy": "Allocate experimental budget preferentially to inputs (compositions, processing conditions) with highest model uncertainty using acquisition functions such as predictive variance, ensemble disagreement, expected model change, or expected error reduction; can be used in initial screening to map decision boundaries.",
            "computational_cost_metric": "Implied as model training and prediction time and experiment throughput; paper frames cost primarily as wall-clock time for experiments and human resources for manual runs (no single numeric metric given).",
            "information_gain_metric": "Uncertainty reduction metrics (predictive variance, standard deviation, ensemble disagreement, expected model change, expected error reduction).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration: acquisition functions prioritize uncertain regions; exploitation is not an explicit objective in classic active learning.",
            "diversity_mechanism": "Not explicitly specified; diversity can be indirectly promoted by sampling across different uncertain regions but no concrete diversity algorithm described in the paper.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Throughput/time and number of experiments (implicit), human resource limits for manual experiments, and instrument access constraints.",
            "budget_constraint_handling": "Tiered workflows are recommended (high-throughput low-fidelity screening followed by lower-throughput high-fidelity validation) to respect experimental time/throughput budgets; active learning focuses experiments on informative regions to maximize learning under limited labeling budget.",
            "breakthrough_discovery_metric": "Not explicitly defined for active learning in the paper; outcomes measured as reduced model error or improved mapping of decision boundaries (implicitly enabling discovery by reducing uncertainty).",
            "performance_metrics": "No quantitative performance numbers for active learning are reported in this paper; qualitative claims that active learning yields significant savings compared to exhaustive or factorial designs.",
            "comparison_baseline": "Factorial design, exhaustive screening, random sampling.",
            "performance_vs_baseline": "Described qualitatively as significant savings over factorial design / exhaustive search; no numerical comparisons provided in this paper.",
            "efficiency_gain": "Qualitative statements: active learning and Bayesian optimization have demonstrated substantial savings (reduced number of experiments) in prior work; no explicit percent or factor given here.",
            "tradeoff_analysis": "Paper emphasizes tradeoff between reducing uncertainty (information gain) and experimental throughput/fidelity; active learning is positioned as efficient when the goal is learning the labeling function everywhere, but may not directly prioritize high-performance samples.",
            "optimal_allocation_findings": "Recommendations include using active learning for exploration-focused stages, combining it with tiered screening (low-fidelity to high-fidelity) to respect throughput budgets, and integrating uncertainty quantification to avoid model-driven extrapolation errors.",
            "uuid": "e2398.0",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Bayesian optimization",
            "name_full": "Bayesian optimization (probabilistic surrogate-based optimization)",
            "brief_description": "A surrogate-model-based sequential optimization framework that trades off exploration and exploitation using a probabilistic model (commonly Gaussian processes) and acquisition functions (expected improvement, upper confidence bound, probability of improvement, entropy) to select experiments expected to optimize a target objective.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bayesian optimization with Gaussian process surrogates and acquisition functions (EI, UCB, PI, entropy)",
            "system_description": "Builds a probabilistic surrogate model (often a Gaussian process) of the objective function from sampled points, then uses an acquisition function that combines the surrogate's mean and uncertainty to select the next experiment. Common acquisition functions include expected improvement (EI), upper confidence bound (UCB), probability of improvement (PI), and entropy-based strategies. It is intended to find global optima with a minimal number of expensive evaluations.",
            "application_domain": "Materials optimization (identifying compositions/process parameters with maximal/minimal properties), autonomous experiments, high-cost/small-sample settings",
            "resource_allocation_strategy": "Allocate experimental budget by maximizing an acquisition function that trades off expected improvement (exploitation) and uncertainty (exploration); selection inherently weights potential information gain and predicted objective value to prioritize experiments that either promise improvement or reduce uncertainty where it matters for optimization.",
            "computational_cost_metric": "Framed qualitatively as computational wall-clock time of surrogate updates and acquisition optimization; experimental cost measured in experiment time/throughput and human resources. The paper highlights that physics-based models taking hours are infeasible for high-throughput decision loops, motivating faster surrogates.",
            "information_gain_metric": "Acquisition functions: expected improvement, upper confidence bound, probability of improvement, entropy (information-theoretic acquisition).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit tradeoff via acquisition functions (e.g., UCB adds an uncertainty term to the mean, EI balances potential improvement against uncertainty; entropy-based methods explicitly target information gain).",
            "diversity_mechanism": "Not detailed in algorithmic form; paper notes methods can be extended to multi-objective/Pareto settings and to multi-modality data collection, which can promote diverse hypotheses via Pareto front exploration.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Limited number of expensive/high-fidelity experiments, experimental time/throughput constraints, computational budgeting for surrogate/model updates.",
            "budget_constraint_handling": "Acquisition functions implicitly manage budget by choosing high-utility experiments; recommended tiered screening (low-cost wide search then high-cost focused experiments) and use of surrogate models to reduce costly evaluations.",
            "breakthrough_discovery_metric": "Maximization/minimization of target property (e.g., property value) or discovery of Pareto-optimal tradeoffs for multi-objective problems; no novel 'breakthrough score' defined in the paper.",
            "performance_metrics": "Paper cites prior work claims of significant savings versus classical designs but does not provide numeric metrics here; examples of successful applications (e.g., Kusne et al. for phase boundary mapping) are referenced without quantitative comparison in this text.",
            "comparison_baseline": "Factorial design, exhaustive grids, random search, human-guided selection.",
            "performance_vs_baseline": "Described qualitatively as yielding significant savings and more efficient optimization compared to conventional factorial designs; no numerical values reported in this review paper.",
            "efficiency_gain": "Claimed potential for orders-of-magnitude acceleration of design loops over traditional MbD and trial-and-error; no numerical estimate provided.",
            "tradeoff_analysis": "Paper discusses computational cost of physics-based models vs surrogate speed, and the need to balance fidelity (high-cost accurate measurements/models) against throughput (low-cost quick screens). It advocates surrogate-based Bayesian optimization and multi-fidelity strategies to manage this tradeoff.",
            "optimal_allocation_findings": "Recommendation to use probabilistic surrogates and acquisition functions to optimally allocate costly high-fidelity experiments, to adopt tiered screening strategies, and to ground surrogates to experiments regularly to avoid model drift and Type I/II errors.",
            "uuid": "e2398.1",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Bandit optimization",
            "name_full": "Bandit optimization (cumulative-reward sequential decision-making)",
            "brief_description": "A sequential decision framework that aims to maximize cumulative reward across experiments, where each sampled decision incurs an immediate reward and the algorithm must control cumulative loss (regret) over all steps; requires stronger emphasis on exploitation relative to pure Bayesian optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bandit optimization (regret-minimizing experimental allocation)",
            "system_description": "At each step, the algorithm selects an experiment to control the cumulative function value (sum of rewards) rather than solely aiming to locate the optimum at the end. Acquisition policies (UCB, Thompson sampling and variants) are used to trade off exploration and exploitation to minimize cumulative regret.",
            "application_domain": "Materials experiments and discovery campaigns where each experiment's outcome should be useful (e.g., incremental material improvement campaigns) and wasted trials are costly.",
            "resource_allocation_strategy": "Allocate experiments to balance immediate payoff and information gathering; policies explicitly consider cumulative performance so they avoid excessive exploration that would reduce total rewards during the campaign.",
            "computational_cost_metric": "Implicitâ€”counts of experiments performed (each costly), wall-clock time for running acquisition and model updates; emphasis in paper is on experimental cost per trial rather than detailed FLOPs.",
            "information_gain_metric": "Uncertainty-aware acquisition functions used for balancing exploration and exploitation; cumulative reward/regret is the central metric rather than pure information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Stronger emphasis on exploitation relative to Bayesian optimization; acquisition functions are adapted to control cumulative reward (e.g., UCB with tuned exploration term, Thompson sampling).",
            "diversity_mechanism": "Not explicitly described in the paper; diversity may be indirectly enforced through exploration components of bandit policies but specific diversity-promoting strategies are not detailed.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Cumulative experimental budget (number of trials) and desire for each trial to be meaningful (no wasted experiments).",
            "budget_constraint_handling": "Algorithms designed to minimize cumulative regret under a finite trial budget; the acquisition function parameters govern how aggressively exploration is penalized relative to exploitation.",
            "breakthrough_discovery_metric": "Cumulative reward over the campaign and final identified optimum; breakthroughs correspond to high reward observations during the sequence.",
            "performance_metrics": "No numerical performance metrics supplied here; paper notes bandit methods are applicable where cumulative experimental value matters.",
            "comparison_baseline": "Bayesian optimization (final-optimum focused), random allocation, greedy selection.",
            "performance_vs_baseline": "Paper states that bandit methods emphasize exploitation more than Bayesian optimization to keep each experiment meaningful; no quantitative comparisons provided in this review.",
            "efficiency_gain": "Qualitative benefit: better cumulative utility when each experiment must produce valuable outcomes; no numeric gains reported.",
            "tradeoff_analysis": "Paper contrasts bandit cumulative-regret objective with BO's final-optimum objective, noting that bandits require more exploitation to preserve high cumulative payoff; ties into experimental constraints where intermediate results must be useful.",
            "optimal_allocation_findings": "When experiments must be productive throughout the campaign, favor bandit-style allocation with tuned exploration terms; align acquisition strategy with whether cumulative reward or final best sample matters most.",
            "uuid": "e2398.2",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Reinforcement learning",
            "name_full": "Reinforcement learning (state-dependent sequential decision-making)",
            "brief_description": "A sequential decision-making framework that models state dynamics where actions (experiments) alter the system state and future rewards, optimizing a policy to maximize cumulative reward potentially with delayed outcomes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Reinforcement learning (policy optimization for experiment sequencing)",
            "system_description": "Considers experiment selection as actions in an environment where the state (material condition, phase, etc.) evolves in response to actions; methods learn a policy (e.g., Q-learning, policy gradients) to maximize cumulative reward across a sequence of experiments, handling delayed rewards and state-dependent labeling functions.",
            "application_domain": "Materials processing/control scenarios where interventions change system state (e.g., applying external fields, processing schedules) and subsequent experimental outcomes depend on history.",
            "resource_allocation_strategy": "Allocate experiments by learning a policy that balances immediate information/reward with long-term gains, taking into account state transitions and delayed outcomes; exploration-exploitation handled via RL exploration methods (e.g., epsilon-greedy, UCB-like bonuses, Thompson sampling when combined with bandit ideas).",
            "computational_cost_metric": "Policy training cost (compute time), simulation rollouts or real experiment counts; paper emphasizes that learning state dynamics can be computationally intensive and require many samples, making RL challenging in expensive experimental domains.",
            "information_gain_metric": "Implicit in reward shaping; information gain can be encoded in reward functions or acquisition criteria; not prescribed explicitly in the review.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Handled via RL exploration strategies (value-based methods with exploration bonuses, policy-gradient stochasticity, Thompson sampling analogs); explicit mechanisms not deeply detailed in this review.",
            "diversity_mechanism": "Not specified in this paper; RL can implicitly produce diverse policies, but explicit diversity promotion is not described.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Experimental budget across multiple steps, need to manage long-horizon resource allocation, computational budget for policy learning.",
            "budget_constraint_handling": "Paper suggests RL requires modeling/learning state dynamics and is suitable when sequential state-dependent rewards matter, but warns of sample complexity; suggests use when delayed rewards and state changes are central to the problem.",
            "breakthrough_discovery_metric": "Cumulative reward across the experimental sequence; breakthroughs correspond to trajectories yielding high cumulative reward or access to desirable states.",
            "performance_metrics": "No numerical performance metrics provided in this review. RL is described as less explored in materials science with only a few exceptions cited.",
            "comparison_baseline": "Bandit optimization (constant state), Bayesian optimization (single-step final objective), hand-designed experimental policies.",
            "performance_vs_baseline": "Paper notes RL is more general than bandits (handles state transitions and delayed rewards) but requires more data and modeling; no quantitative performance comparisons provided.",
            "efficiency_gain": "Not quantified in this paper; RL may yield better long-horizon performance in stateful problems but at the cost of higher sample/computational expense.",
            "tradeoff_analysis": "Paper highlights tradeoffs: RL can optimize long-term sequences but needs more data and modeling of state dynamics; when experiments are expensive and sample-efficiency is crucial, surrogate and lower-complexity methods may be preferable.",
            "optimal_allocation_findings": "Use RL when action sequences change system state and delayed rewards are important; otherwise prefer bandit or BO methods which are simpler and more sample-efficient for static labeling functions.",
            "uuid": "e2398.3",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Multi-fidelity experiments",
            "name_full": "Multi-fidelity experimental and simulation integration",
            "brief_description": "Combining low-fidelity, inexpensive experiments or simulations and high-fidelity, expensive experiments or models within a sequential decision framework to optimally allocate resources and accelerate discovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multi-fidelity sequential decision frameworks (combining experiments and simulations)",
            "system_description": "Frameworks that treat experiments and simulations at different fidelities as different information sources, using acquisition/selection strategies to decide when to run cheap low-fidelity probes and when to spend budget on costly high-fidelity measurements; can be incorporated into active learning, Bayesian optimization, or bandit formulations.",
            "application_domain": "Materials discovery and characterization where low-fidelity screens (rapid, low-cost measurements or surrogate simulations) can be used to narrow candidates before expensive high-fidelity validation.",
            "resource_allocation_strategy": "Allocate budget across fidelity levels by weighing the cost of an evaluation against its expected information gain or contribution to the optimization objective; tiered screening protocols (parallel low-fidelity, then serial high-fidelity) are recommended in the paper.",
            "computational_cost_metric": "Experiment/simulation wall-clock time, human labor for manual experiments, instrument access time; cost differentials between low- and high-fidelity modalities are central but no single numeric metric is standardized in the paper.",
            "information_gain_metric": "Expected improvement, uncertainty reduction, information-theoretic measures adapted to multi-fidelity contexts (paper cites multi-fidelity methods in literature but does not prescribe a single metric).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Allocation determined by acquisition functions extended to multi-fidelity that consider both expected utility and cost of each fidelity; tiered pipeline reduces breadth then increases fidelity in focused regions.",
            "diversity_mechanism": "Paper recommends screening for microstructure-insensitive vs structure-sensitive properties and sampling across compositional and microstructural space (implying diversity), but explicit algorithmic diversity mechanisms are not provided.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary/time budget for high-fidelity experiments, experimental throughput limits, computational runtime for simulations.",
            "budget_constraint_handling": "Tiered workflows that prioritize low-cost, high-throughput screening to map wide spaces and reserve limited high-cost experiments for promising candidates; surrogate models and verification/validation loops to limit unnecessary high-fidelity runs.",
            "breakthrough_discovery_metric": "Not explicitly defined; discovery is enabled by using high-fidelity measurements to validate promising low-fidelity candidates, with breakthrough identification typically via meeting performance thresholds in high-fidelity validation.",
            "performance_metrics": "No numeric metrics; qualitative guidance that multi-fidelity strategies reduce number of expensive experiments and accelerate discovery when properly grounded.",
            "comparison_baseline": "Uniform allocation across fidelities, single-fidelity exhaustive testing, manual human-guided allocation.",
            "performance_vs_baseline": "Paper states multi-fidelity approaches can be more efficient than single-fidelity exhaustive testing, but provides no quantitative comparisons in this review.",
            "efficiency_gain": "Qualitative: multi-fidelity and tiered screening can significantly reduce expensive evaluations and accelerate loops; specific gains depend on domain and are not specified here.",
            "tradeoff_analysis": "Paper discusses the tradeoff between throughput and fidelity, the need to balance characterization time versus synthesis time, and the challenge of ensuring transfer functions between fidelities (e.g., AM vs bulk processing). It emphasizes regular grounding/validation of surrogates to avoid misleading multi-fidelity extrapolations.",
            "optimal_allocation_findings": "Recommend tiered high-throughput to high-fidelity pipelines, use of surrogate models, and careful consideration of which properties are fidelity-sensitive (structure-sensitive vs microstructure-insensitive) when allocating high-fidelity resources.",
            "uuid": "e2398.4",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Autonomous experimentation systems",
            "name_full": "Autonomous experimentation (AE) platforms and closed-loop research systems",
            "brief_description": "Robotic and computational systems that close the loop between synthesis, characterization, and modeling to make on-the-fly decisions about subsequent experiments without human intervention, using ML-driven acquisition strategies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Autonomous experimentation platforms (examples: Nikolaev et al., Kusne et al.)",
            "system_description": "Integrated platforms that automate synthesis and characterization and use ML-driven decision-making to select next experiments; components include high-throughput synthesis hardware (e.g., co-sputtering, AM), inline/in situ characterization tools, surrogate ML models, and acquisition/policy modules (active learning / Bayesian optimization / bandit / RL) to allocate experimental resources.",
            "application_domain": "Materials discovery and mapping (e.g., phase diagram mapping, synthesis optimization, CNT growth control), especially for high-throughput materials research.",
            "resource_allocation_strategy": "Use closed-loop acquisition policies (active learning, Bayesian optimization or bandit policies) to choose experiments that balance throughput, measurement fidelity, and information value; practical implementations often adopt tiered screening and select compositions/conditions that maximize information or phase-boundary localization.",
            "computational_cost_metric": "Paper characterizes experimental cost in terms of experimental speed and throughput, and human resources for manual execution; computational cost concerns focus on surrogate/model runtimes (wall-clock time) relative to experiment timescales.",
            "information_gain_metric": "Varies by implementation: uncertainty reduction (active learning), expected improvement or information-based acquisition (BO), entropy; paper mentions acquisition functions used in cited autonomous systems.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Depends on chosen acquisition method; examples in literature use Bayesian active learning (explore near decision boundaries for phase mapping) or BO to trade off exploration and exploitation; bandit-like approaches apply when cumulative experiment value is desired.",
            "diversity_mechanism": "Paper notes the need to capture wide composition/microstructure spaces and recommends screening strategies (microstructure-insensitive vs structure-sensitive) and use of AM to sample broad spaces, but explicit algorithmic diversity promotion is not detailed.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Experimental throughput/time, instrument access, human resource limitations, monetary cost of high-fidelity tests.",
            "budget_constraint_handling": "Practical AE platforms adopt tiered screening (high-throughput low-fidelity first, then targeted high-fidelity validation), surrogate models to reduce expensive evaluations, and inline characterization to reduce latency to decision-making.",
            "breakthrough_discovery_metric": "Identified via improved target properties, mapping of phase boundaries, or achieving predefined design goals; specific metrics are implementation-dependent and not standardized in the review.",
            "performance_metrics": "This review references prior autonomous systems (e.g., Nikolaev et al., Kusne et al.) as demonstrations of capability but does not provide numerical performance figures here; claims include accelerated discovery and the ability to perform on-the-fly decision-making when latencies allow.",
            "comparison_baseline": "Human-in-the-loop workflows, manual experimentation, factorial/exhaustive designs.",
            "performance_vs_baseline": "Paper asserts AE yields acceleration and improved decision speed versus manual workflows and traditional MbD, with prior works demonstrating practical successes; however, no quantitative head-to-head numbers are provided in this review.",
            "efficiency_gain": "Qualitative: AE promises orders-of-magnitude acceleration over MbD and trial-and-error; concrete gains depend on system and are not provided here.",
            "tradeoff_analysis": "Paper extensively discusses tradeoffs for AE: latency of measurement vs phenomenon speed (decision latency), throughput vs fidelity, cost of characterization vs information content, and the challenge of integrating expensive in situ coupled-extreme measurements into AE loops.",
            "optimal_allocation_findings": "AE systems should co-design AI/ML, computing, and control to support low-latency decisions; adopt tiered high-throughput to high-fidelity strategies; ground surrogates frequently with experiments to avoid drift; and consider multi-objective/Pareto optimization when multiple coupled extremes are relevant.",
            "uuid": "e2398.5",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Phase Mapper",
            "name_full": "Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery",
            "brief_description": "An AI platform referenced for accelerating high-throughput materials discovery, built to assist phase mapping via autonomous or semi-autonomous decision-making over composition libraries.",
            "citation_title": "Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery",
            "mention_or_use": "mention",
            "system_name": "Phase Mapper AI platform",
            "system_description": "A software platform combining data processing, pattern recognition, and decision-making to identify phase regions in high-throughput composition libraries, cited as an example of AI-enabled acceleration in HTE workflows.",
            "application_domain": "High-throughput materials discovery and phase diagram mapping",
            "resource_allocation_strategy": "Uses AI to prioritize measurements and interpret combinatorial library data to identify phase boundaries and candidate compositions for further study; paper does not detail specific allocation algorithms within Phase Mapper.",
            "computational_cost_metric": "Not specified in the review; underlying costs would include computation for pattern recognition and acquisition decisions and experimental measurement time.",
            "information_gain_metric": "Not specified in this review; Phase Mapper focuses on accelerating mapping of compositional phase space (information gain measured implicitly by improved phase identification).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not detailed in this review; system aims to accelerate mapping rather than explicitly described exploit/explore mechanics in the cited entry.",
            "diversity_mechanism": "Not described here.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Experimental throughput and measurement budgets for mapping composition libraries.",
            "budget_constraint_handling": "Not specified in the review; platform is presented as an acceleration aid for HTE workflows.",
            "breakthrough_discovery_metric": "Not specified in the review.",
            "performance_metrics": "No numerical performance metrics provided in this review; Phase Mapper is cited as an enabling tool for HTE acceleration.",
            "comparison_baseline": "Traditional, manual analysis of high-throughput libraries and human-guided selection.",
            "performance_vs_baseline": "Not quantified in the paper; referenced as part of the growing toolbox of AI for HTE acceleration.",
            "efficiency_gain": "Not quantified in this review.",
            "tradeoff_analysis": "Not discussed specifically for Phase Mapper in this paper; general review emphasizes tiered fidelity and throughput tradeoffs that platforms like Phase Mapper can help manage.",
            "optimal_allocation_findings": "Not specified for Phase Mapper in this review.",
            "uuid": "e2398.6",
            "source_info": {
                "paper_title": "Artificial intelligence for materials research at extremes",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A.G. Kusne et al., (2020) - Nature Communications (mapping phase diagrams by Bayesian active learning)",
            "rating": 2,
            "sanitized_title": "ag_kusne_et_al_2020_nature_communications_mapping_phase_diagrams_by_bayesian_active_learning"
        },
        {
            "paper_title": "P. Nikolaev et al., (2016) - NPJ Computational Materials (autonomous research system for materials development)",
            "rating": 2,
            "sanitized_title": "p_nikolaev_et_al_2016_npj_computational_materials_autonomous_research_system_for_materials_development"
        },
        {
            "paper_title": "Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery",
            "rating": 2,
            "sanitized_title": "phase_mapper_an_ai_platform_to_accelerate_high_throughput_materials_discovery"
        },
        {
            "paper_title": "R. GÃ³mez-Bombarelli et al., (2018) - ACS Central Science (computational + human-in-the-loop molecular design)",
            "rating": 1,
            "sanitized_title": "r_gÃ³mezbombarelli_et_al_2018_acs_central_science_computational_humanintheloop_molecular_design"
        },
        {
            "paper_title": "B. P. MacLeod et al., (2022) - Nature Communications (closed-loop autonomous discovery examples)",
            "rating": 2,
            "sanitized_title": "b_p_macleod_et_al_2022_nature_communications_closedloop_autonomous_discovery_examples"
        },
        {
            "paper_title": "R. Garnett, Bayesian Optimization (Cambridge University Press, 2023)",
            "rating": 1,
            "sanitized_title": "r_garnett_bayesian_optimization_cambridge_university_press_2023"
        }
    ],
    "cost": 0.021005499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Artificial intelligence for materials research at extremes
NOVEMbEr 2022</p>
<p>B Maruyama 
J Hattrick-Simpers 
W Musinski 
L Graham-Brady 
K Li 
J Hollenbach 
A Singh 
M L Taheri 
Artificial intelligence for materials research at extremes</p>
<p>MRS BULLETIN â€¢ VOLUME
47NOVEMbEr 202210.1557/s43577-022-00466-41154
Materials development is slow and expensive, taking decades from inception to fielding. For materials research at extremes, the situation is even more demanding, as the desired property combinations such as strength and oxidation resistance can have complex interactions. Here, we explore the role of AI and autonomous experimentation (AE) in the process of understanding and developing materials for extreme and coupled environments. AI is important in understanding materials under extremes due to the highly demanding and unique cases these environments represent. Materials are pushed to their limits in ways that, for example, equilibrium phase diagrams cannot describe. Often, multiple physical phenomena compete to determine the material response. Further, validation is often difficult or impossible. AI can help bridge these gaps, providing heuristic but valuable links between materials properties and performance under extreme conditions. We explore the potential advantages of AE along with decision strategies. In particular, we consider the problem of deciding between low-fidelity, inexpensive experiments and high-fidelity, expensive experiments. The cost of experiments is described in terms of the speed and throughput of automated experiments, contrasted with the human resources needed to execute manual experiments. We also consider the cost and benefits of modeling and simulation to further materials understanding, along with characterization of materials under extreme environments in the AE loop.</p>
<p>Introduction</p>
<p>Materials development is slow and expensive, taking decades from inception to fielding. For materials in coupled extremes, the situation is even more demanding, as the desired property combinations such as strength and hightemperature oxidation resistance can have complex interactions. Here, we discuss the role of AI and autonomous experimentation (AE) in the design, synthesis, and characterization of materials for extreme and coupled environments. Although a focus on AE has become prominent in much of chemical and materials synthesis and discovery space, a specific focus on AE for extreme environments has seen a budding interest. Because this is a nascent area, our intent is to bring readers' attention to issues impacting AI for materials science for coupled extremes. While we focus primarily on structural materials, many of the issues carry over to functional materials at coupled extremes. Over a decade ago, the Materials Genome Initiative first implored the world of science and engineering to achieve an accelerated pace of discovery and insertion into applications by combining experiment, theory, and computation using high-throughput methodologies. 1 Since that time, the ability to optimize experiments for automation and highthroughput computation and analysis has successfully been achieved, yet broad autonomous research processes remain limited. Some approaches, for example, required intervention of a human in a biased decision process. GÃ³mez-Bombarelli et al. 2 was able to leverage computational libraries and then mine this space to develop a decision based on a human vote, leading to the synthesis and characterization of devices. Although autonomous research, such as the Air Force-based Autonomous Research Systems (ARES) program, has recently experienced some initial success, many of these studies have used computation, database or library building, machine learning to accelerate discovery and synthesis in monotonic, uncoupled campaigns. 3 More recently, on-the-fly approaches have gained momentum, and discovery has not only been accelerated but has also occurred more autonomously, including work by Kusne et al. 4 Still, the ability to decide new pathways during discovery, on-the-fly, remains largely unseen.</p>
<p>Critically, we explore the potential advantages and strategies that will further enable AE for a given experimental parameter space; in particular, we explore the complexities of coupled extremes and the particular challenge of simultaneously occurring phenomena with often disparate spatial and temporal scales. The cost of experiments is described in terms of the speed and throughput of automated experiments in contrast with the human resources needed to execute manual experiments. Although low-fidelity experimentation is relatively inexpensive, we discuss the role of AE and AI/ ML strategies in high-fidelity "expensive" experimentation, along with modeling, simulation, and characterization. The combination of these strategies is deemed necessary to further materials understanding of materials under extreme environments in the AE loop, and we note the dependence of in situ and inline characterization to enable AE. Finally, a future outlook on intelligent or decision-informed AE, the mark of autonomy rather than automation, is discussed.</p>
<p>Materials selection and design</p>
<p>Selection and tailoring of materials for coupled extreme environments present unique challenges. Over the years, many different materials selection tools and databases have been developed with the democratization of open data material sets. At a macro level, materials selection tools such as Ashby plots 5 can help initially screen for material classes that maximize a relevant performance index. For material classes that pass this initial screening, more accurate handbook data (classically, ASM Handbook 6 ), data sheets provided by material suppliers, and online databases 7-9 can help further whittle down the appropriate materials selection for the intended application. The ability to aggregate the collection of these data for materials selection has thus become a critical requirement for more rapid and comprehensive materials selection.</p>
<p>For coupled extremes however, the list of static materials properties/charts (e.g., strength-temperature versus temperature) has limited transferability to properties (e.g., strength, creep, fracture toughness, oxidation/corrosion resistance, and other degradation mechanisms) that may be dynamically changing due to exposure to extreme environments. Thus, there is a need to be able to characterize (via modeling and experiments) the physical mechanisms that drive these materials transients for extreme environment applications. Recent examples exploring novel alloy spaces while searching for a desired outcome in an extreme environment still follow traditional paths based on Ashby plots: Work by Nascimento et al. reveals both the promise and limitations of such approaches. 10 These approaches work well for target functionality, be it magnetism, corrosion resistance, or strength with structure. In these cases, target properties in chemistry or structure are often benchmarked by a single functional property. This approach has worked well for theoretical searches of structural or elastic properties. 11 However, assessing more than two projected outcomes, as is the case with coupled extremes, materials selection requires a more complex approach. Recent work 12 in high-throughput density functional theory exploration has provided a pathway for multiple outcomes and a basis for multiscale exploration and higher fidelity theoretical or experimental approaches. Perhaps most notably, these high-throughput methods are rooted in machine learning models that allow for more complexity in property assessment, 13 in some cases multiple environments 14 and how these deep learning methods 15 provide a translation to high-throughput experimentation, discussed in the next section.</p>
<p>High-throughput and autonomous experimentation</p>
<p>Materials development and deployment can take decades. 16 Conventional component processing of advanced materials in a production facility can go through a series of processes (e.g., metal powder processing and consolidation, casting, billet formation, forging, heat treatment, machining, and final surface treatment). A major opportunity for expediting the materials development process is in the early stages of materials discovery, development, and property optimization. To do so requires a means to synthesize and characterize these exploratory materials rapidly, which is the goal of high-throughput experimentation (HTE). The materials by design (MbD) approach has shown significant promise across the field. It recognizes that efficient materials design requires that we thread a disciplined focus on specific materials performance goals throughout the processing, modeling, and testing of a particular material application. There are a number of MbD success stories across the broad materials community, but fewer in the domain of materials in extreme conditions. The Army Research Laboratory funded a 10-year program on Materials in Extreme Dynamic Environments, in which a 15-20% weight decrease in lightweight armor materials was realized through tightly knitted collaborations across processing, characterization, testing, and modeling efforts. General Electric applied principles of MbD to develop, validate, and commercialize an entirely new superalloy composition for use in gas-fired turbines for electricity generation.</p>
<p>The MbD approach that provides material improvements in years is a significant improvement over traditional Edisonian trial and error approaches that required decades to make such advances. AI-guided autonomous and automated high-throughput systems promise further orders of magnitude acceleration over MbD, by speeding up the individual design loops and leveraging rapid decision-making offered by AI tools. 17 Nikolaev et al. 18 demonstrated the first autonomous research system for materials development in their work controlling the growth rates of carbon nanotubes. Kusne et al. 4 applied Bayesian active learning to phase diagram determination for metallic materials by creating continuous libraries of ternary alloys using cosputtering. The phase diagram of these libraries was mapped by autonomous selecting the compositions at which x-ray diffraction determined the phase boundaries and crystal structures.</p>
<p>During these materials exploration processes, there is a critical need to synthesize specimens with a wide range of composition and microstructure space in order to meet the expectation of capturing the potential materials landscape. Because microstructure drives important properties for materials at extremes (e.g., high cycle fatigue), it is important that microstructure be captured in samples that will be used to test those properties. However, some materials behavior is insensitive to microstructure (e.g., equilibrium phase behavior), and thus samples that may be easier to make and test may be used despite not having the target microstructure. In such cases, one potential strategy would be to screen for microstructure-insensitive (relying primarily on alloy composition as shown by Kusne et al. 4 ) versus structure-sensitive (relying on both composition and microstructure 19 ) properties. For structure-sensitive properties, further consideration should be devoted to properties that are driven by mean structure (e.g., yield strength) versus those that are driven by microstructural extremes (e.g., ductility and fatigue life) and an appropriate material element size to adequately capture the behavior of each property. 20 Additive manufacturing (AM) is a potentially powerful technology for exploring compositional 21,22 and/or processing parameter space to establish robust linkages between them. Using AM, one can rapidly make samples or test coupons with the target composition, shape, and size. However, the current means of AM building often involves significant human intervention in otherwise automated processes. For an AM development campaign to satisfy the requirements of typical standardized tests 23,24 needed for component design, the whole process from build design planning to AM part synthesis, to testing and data processing can take weeks or months. And this process only comprises a single iteration of the design-build-test loop, and so is currently too slow for accelerated R&amp;D goals. Another challenge for AM is that it is prone to generating material defects that can have a particularly significant impact on material performance in extreme conditions. Finally, despite its advantages, there does not exist a reliable transfer function ensuring an alloy that can be synthesized via AM can be made by traditional solidification processes or vice versa. As an example, Lass et al. demonstrated that standard heat treatments of AM-grown Inconel 625 resulted in the precipitation of negative side products, and new homogenization treatments were required to recover the desired microstructure. 25 Despite these challenges, although AM can still contribute significantly to accelerated materials development, specimen preparation and manipulation should not be neglected when planning for an automated materials development workflow.</p>
<p>Characterization</p>
<p>There is a need for more rapid characterization methods and feedback mechanisms for materials properties in extreme environments. Important for AE workflows, the characterization method should be in situ, operando, or inline for best integration. 17 We note that the characterization method for these extreme environments need not be as close to the final application space as possible, provided there is a correlation or transfer function that can translate between the characterization measurement and the material property of interest. For example, high-throughput nanoindentation 26 extracts mechanical properties directly from the indentation load-displacement curve at both room and elevated temperatures very quickly. 27 However, the materials properties obtained from this technique may not necessarily be indicative of the performance of the material in extreme environments, such as at high temperatures in which oxidation is likely to become much more prevalent. This highlights the need to consider coupling of extremes, in this case how extremely high temperature influences oxidation, which in turn influences the high-rate/highpressure mechanical behavior of materials.</p>
<p>There are a wide variety of high-throughput characterization techniques available to evaluate phase stability, thermomechanical properties, corrosion, and oxidation. In situ x-ray diffraction can be performed under controlled environments up to moderately high temperatures, as has been demonstrated by Kramer and Ott. 28 This type of approach can be used to study phase stability and provide information about oxidation of bulk and thin-film alloy samples for HTE or AE. More commonly, ex situ Raman spectroscopy is used, provided the oxide phase is alpha-alumina or otherwise Raman-active. Performing Raman measurements at relevant temperatures and pressures is possible as demonstrated by Maslar et al. for supercritical steam (25 MPa and 500Â°C). 29 An analogous cell design was subsequently used to measure hydrogenation of HTE metal thin films at pressures up to 100 MPa. 30 Aqueous corrosion can be measured through immersion in corrosive media or through electrochemical testing; both parallel and sequential screening techniques have been demonstrated for the latter. Recent ex situ studies on AM samples have looked at molten salt corrosion of cantor-based high-entropy alloys using ICP-MS of dissolved salt pills. 31 Ambient mechanical characterization of thin-film, diffusion couple, and AM HTE samples is routinely performed ex situ via nanoindentation, which can provide measures of hardness, Young's modulus, and wear resistance.</p>
<p>An important lesson for developing AE experiments can come from how HTE experiments learned to balance experimental pacing (throughput) and the fidelity of the measurement. It is useful to consider the acquisition time versus spatial resolution of each potential measurement technique as depicted in Figure 1. Often, HTE workflows are tiered such that a lower fidelity parallel or rapid serial primary screening is first used to interrogate a large phase space rapidly. Subsequent experiments reduce the overall throughput, say from hundreds to tens to ones, while simultaneously increasing the fidelity between steps. One point of emphasis is that the time to characterize (and gain knowledge from) a material should be balanced with the time required to synthesize and process the material. Long count times, extensive life-cycle analysis, or detailed synchrotron experiments are required for material validation but are best used in the secondary or tertiary screening step, unless synthesis and processing are time-consuming. Additionally, conscious decisions need to be made during the design of the HTE workflow whether to be risk averse (minimize false positives) or loss averse (minimize false negatives). This is only more complicated by the need to weigh the value of measuring different types of materials properties relevant to multiple and potentially coupled extremes.</p>
<p>For high-throughput experiments, scientists should carefully consider the gauge volume size of the characterization probe relative to the material's dominant microstructural feature (e.g., grain, phase, etc.) size. This sampling ratio of gauge volume versus microstructure feature can be adjusted to explore a wide spectrum of mesoscale/heterogeneous to continuum response behavior. For example, XRD, which measures the material volume being irradiated by the x-ray beam, can be used for mesoscale (grain-level) 32,33 or macroscale (powder/polycrystalline) 34 studies. Likewise, the indenter size used for (nano-, micro-, and macro-) indentation studies 35 will dictate the grain sampling size (subgrain, 10s of grains, 100s of grains, respectively). For imaging, adjusting the field-of-view to sample a large number of microstructural features or rastering the surface with faster measurements (e.g., EBSD tiling, nanoindentation grids, 36 etc.) can be used to help circumvent the grain sampling size effect. In either approach, it is desirable to use a sampling size sufficiently large enough to capture a representative average of the stochastic behavior of the microstructure.</p>
<p>In addition, appropriate sampling sizes may vary depending on the physical mechanism or material property characterized. 20 As a result, although individual measurements for a particular measurement technique could be quite fast, the amount of time to obtain a physically useful (actionable) sampling size may make a particular measurement technique less amenable to high-throughput experiments. Motivated by the potential to test the processing/structure/property landscape with minimum amount of material possible, miniaturization of specimens for subscale tensile testing, 37 microtensile, 38 microbending, 39 and micropillar compression 40 experiments will play an important role in material property exploration. Although this miniaturization certainly provides representative grain structures for mesoscale material modeling development and provides the ability to study fundamental grain-boundary deformation mechanisms, there are well-known size effects for these microscale experiments that can introduce errors in interpretation. The mechanical response is quite sensitive to loading-boundary conditions and misalignments. These smaller specimens also become more sensitive to near surface microstructure conditions (e.g., asbuilt AM) and consequences of sample preparation (surface roughness, machining, grinding, polishing, and residual stress imposition) as the surface area-to-volume ratio increases. Moreover, these microscale experiments often have dimensions on the same order as microstructure features (grains, phases, etc.). The sampling of a limited number of grain/phase boundaries, the limited grain-to-grain kinematic constraint, and the abundance of free surface-to-bulk ratio result in material response that is quite heterogeneous and may not be fully representative of bulk material response. Furthermore, sufficient statistical sampling size depends on the property of interest. For example, mean-driven materials properties (e.g., aggregate modulus) require a smaller sampling size relative to materials properties driven by extremal microstructure features (e.g., fatigue).</p>
<p>There is typically a tradeoff between gathering highfidelity, information-rich data versus the ability to gather statistically significant quantities of data in a reasonable amount of time. Optical microscopy, profilometry, and EDS can provide large quantities of information about the material surface, but these tools lack detailed information about the materials three-dimensional microstructure. EBSD and XRD provide more information about crystallographic orientations and texture, but they are demanding to implement in a high-throughput workflow. All of these two-dimensional tools have the severe limitation that they provide information only at the surface of the material, which may or may not be representative of the bulk material. XRD and XRF have some ability to probe below the surface, providing "2.5D" information, which hold promise for high-throughput experimentation, but three-dimensional data remain the gold standard for more realistic representations of the material. Techniques such as serial-sectioned EBSD provide valuable and detailed data, but the destructive nature of these techniques makes it difficult to use these data for mapping material microstructure to performance. Synchrotron beamlines provide a powerful tool for characterization via lab diffraction contrast tomography (Lab-DCT), 3DXRD, high-energy diffraction microscopy (HEDM), and computed tomography (CT); however, the limited access to such facilities inhibit our ability to generate sufficient quantities of the necessary data from these facilities. There have been recent advances in laboratory-scale capabilities in CT and HEDM, but these suffer from poorer resolution data than that afforded by the beamlines (Figure 2).</p>
<p>Complexity of experimental sampling scales with the complexity of the fingerprint on which a high-throughput or autonomous experiment relies. Recent work in spectroscopy has provided a pathway for multimodal data to be utilized in autonomous frameworks. 4,41-46</p>
<p>Modeling and simulation</p>
<p>Modeling and simulation serve to ground experimental observations in chemical and physical understanding, and thus are core to the development of AI tools, but these efforts are also critical to accelerated materials design and development. The models help to determine which chemistries are likely to have certain stable phases or desired fundamental properties. They help to link characterization and testing results at one scale to material performance at the application scale. They are also capable of incorporating multiple loading and boundary conditions that can reflect the effect of coupled extreme conditions. Finally, they provide a means to draw key physicsbased conclusions from experimental data. All this said, there are clear challenges presented by modeling and simulation that must be understood before they can be a solid foundation for AI. A primary question is the balance of phenomenological and physics-based models. Physics-based models promise to incorporate behavior that is consistent with the materials performance in extreme environments, but such physics-based models are likely far too slow for high-throughput decisionmaking. Models that take hours to run cannot be an effective guide for high-throughput experiments that run in seconds. There is therefore a clear need for faster-running surrogate models, such as those developed from ML, that can effectively represent these physics-based models.</p>
<p>A major challenge to surrogate models is developing appropriate tools for rigorous verification and validation. Uncertainty quantification tools are needed to identify when the analyses stray too far from grounded data. If the data become too drastically extrapolated, the resulting data-driven surrogate is likely to be incorrect. Furthermore, the relative lack of interpretability of ML models makes it difficult to assess what important physical insights could be lost in using these approaches. In a high-throughput AIdriven setting, care must be taken to ensure that grounding to experiments is performed regularly so that the material iterations do not start down an unreasonable design path. Finally, any AI-driven decisions must come with a sense of the probability of both Type I errors (proceeding with a material that is not acceptable) and Type II errors (rejecting a material that is acceptable).</p>
<p>As previously stated, there currently do not exist examples of performing true in situ (or ex situ) coupled extremes experiments within the context of HTE or even non-HTE active learning settings. Challenges associated with performing true in situ coupled extreme measurements in either context are largely to the difficulty and cost in obtaining nondestructive measurements simultaneously. For instance, measuring oxidation mechanisms during high-temperature corrosion under mechanical loads requires a reliable noncontact nondestructive method of interrogation. Optical spectroscopy, such as Raman or luminescence spectroscopy, can be obtained (as in the work of Maslar et al.); 29 however, not all oxides will have clearly defined Raman-active vibrational modes. Conversely the design of sample geometries and optical path lengths for diffraction measurements (a more general technique for phase analysis) require transmission measurements using high-energy x-rays typical of synchrotrons, as well as requiring small, nonrepresentative sample geometries.</p>
<p>Unfortunately, given the need to understand the dynamic processes in coupled extremes, oxide growth, failure, etc., ex situ measurements will not be sufficient to provide a comprehensive view of the underlying mechanisms. As such, careful consideration should be given to identifying minimally invasive direct and indirect tools for monitoring material behavior. In this respect, the successes (and failures) of the HTE community can serve as guidance for the coupled extremes community as it builds capability. As an example, different regimes of oxidation behavior, including oxide growth, roughening, spallation, etc. (although not equivalent to phase formation and tracking), could readily be observed optically and models trained to label the different regimes. Acoustic response of a material (as has been practiced by metalworkers for centuries) could be used to discern phase transformations, grain growth, and potential imminent failure.</p>
<p>Although HTE has provided a foundation for an increase in the number of systems both tested and discovered, challenges remain when precise mechanisms, intermediate or metastable states, or other complex structures or phenomena are targeted. AI/ML has provided a pathway for us to integrate this level of complexity into both discovery and analysis of materials synthesis and behavior at extremes.</p>
<p>Machine learning methods for design and development of materials</p>
<p>Machine learning focuses on the design and analysis of computer algorithms that improve their performance on a task with experience obtained from data. 47 The integration of machine learning with materials research includes use of existing machine learning methods, libraries, and code to analyze experimental data, [48][49][50][51][52] material simulations, 53,54 and leverage prior knowledge from literature 51,55 and experts. 56 Additionally, material applications have led to the creation of a new subarea of "physics informed machine learning," which is focused on enhancing extrapolation capabilities of machine learning using knowledge of physical phenomena to enable good performance even in domains where no data are available. The latter includes use of physics knowledge in the form of in-variances such as translation, rotation, scale, and permutation in-variance often present in material systems, 57 differential equations or other relations governing materials properties, 58 as well as physical constraints on material systems. 59 Many applications of machine learning to materials science previously mentioned have focused on classical supervised (classification, regression) learning of models such as linear regression, logistic regression, support vector machines, decision trees, random forests, neural networks, and their deep extensions, which require significant amounts of labeled data. Other machine learning methods, such as unsupervised (clustering, distribution estimation, and dimensionality reduction) learning, which can work with unlabeled data, semi-supervised learning, graphical models, and causal inference, are also finding applicability in materials science, [60][61][62][63][64] given their potential to enable interpretable predictions and discovery of unknown physics.</p>
<p>Given that data are not as readily available in the materials science domain (cost of experiments, run-time of simulation codes), in this article, we focus on machine learning methods that can be used to guide data collection. Specifically, this includes methods such as active learning, Bayesian optimization, bandit optimization, and reinforcement learning, where machine learning models are used in a closed loop to iteratively make decisions about what experiments to perform, what simulations to run, what queries to ask an expert, etc. Active learning and Bayesian optimization are increasingly being adopted for experimental design in materials science. Prior work in the past few years 3,4,59,65-70 has demonstrated the promise of active learning and Bayesian optimization for efficient experimental design and AE by yielding significant savings over conventional methods such as factorial design. At the same time, more complex sequential decision-making methods such as bandit optimization and reinforcement learning have not been explored much by the materials science community, barring a few exceptions. 71 Unlike classical supervised learning, machine learning methods for sequential decision-making iteratively refine a model by focusing on regions where the model is most uncertain (explore only) or regions that provide best tradeoff between uncertainty and maximal/minimal value of labeling function (explore-exploit tradeoff). The sampling is determined by an acquisition function that includes a specific notion of uncertainty of prediction and potentially a tradeoff with the predicted function value. These concepts will be explained further as we introduce the different sequential decision-making methods next.</p>
<p>In active learning, the goal is to learn the labeling function everywhere by sampling in regions with most uncertainty, and error is measured by probability of misclassification or mean square error in regression. This leads to explore only setting with common acquisition functions being the variance or standard deviation of prediction, disagreement regions between different ensemble learners, expected model change, expected error reduction, etc., 72 and often amounts to more sampling near the decision boundary in classification and in regions of less smoothness for regression as shown in Figure  3. Although active learning mostly refers to supervised setting, there is also work on active unsupervised learning such as learning clusters, matrices, and graphs using selective sampling of data points, features, edges, similarities, etc. 73- 75 Bayesian optimization falls under the umbrella of more general "stochastic optimization using machine learning" that involves machine learning methods to approximate the unknown function being optimized, where the latter is only accessible via samples. Instead of learning the function everywhere, here the goal is to optimize (find the maximum or minimum of) the function by trading off exploration of uncertain regions with exploitation of promising regions that are more likely to have optimal (maximum/minimum) function value ( Figure 3). Bayesian optimization 76 uses a probabilistic model for the function being optimized, most popular being the Gaussian process regression model. Bayesian models naturally come with a notion of uncertainty due to probabilistic modeling such as variance in Gaussian process models. Common acquisition functions include upper confidence bounds, probability of improvement, expected improvement, entropy, etc.</p>
<p>Bandit optimization also focuses on the goal of optimizing the labeling function; however, it requires controlling the value of the sampled function all through the optimization and not just finding the optimum or optimal function value at the end. 77 This requirement is encoded in the error metric, which is cumulative over all steps for bandit optimization, thus exploring for all but the last step and yielding a good estimate of optimum at the last step does not suffice. This is relevant as materials scientists often want each experiment/simulation to be meaningful (e.g., yield a material with improving property). Often the same acquisition functions used in Bayesian optimization work to control the cumulative function value; however, the role and need for exploitation is much more pronounced when using bandit methods.</p>
<p>Reinforcement learning considers the setting where the same decision (which experiment/simulation to run) could result in different function values depending on the state of the system, and each decision could affect the state (which is not directly controllable) leading to a different labeling function. 78 For example, applying different external fields could change the phase and hence resulting properties of the material. The goal is to optimize the cumulative (over all steps) value of the function while accounting for changed states and resulting labeling function at each step. This requires learning (explicitly or implicitly) the state dynamics and hence optimization is carried out over a longer time duration to find the best policy that generates an optimal sequence of decisions. As a byproduct, reinforcement learning is also able to handle settings where the function values (often called rewards) could be delayed. Methods for reinforcement learning include value-based learning, Qlearning, temporal difference learning, Monte Carlo, and policy gradient, 78 as well as acquisition methods such as upper confidence bound or Thompson sampling, given connections with bandit optimization (the latter is essentially a constant state version of reinforcement learning). 79,80 T h e m e t h o d s for sequential decision-making using machine learning can also be extended to integrate and guide collection of data from multiple modalities, including multi-fidelity e x p e r i m e n t s a n d s i m u l a t i o n s , 8 1 -8 4 combining experiments and simulations, 85 and combining experiments, Figure 3. Sequential decision-making methods-active learning that focuses on exploration by sampling uncertain regions, Bayesian and bandit optimization as well as reinforcement learning (RL), which trade off exploration of uncertain regions with exploitation of optimum function value. Bayesian and bandit optimization focus on finding the optimal value of the function at each step or cumulatively over the entire steps, respectively, while RL considers cumulative value of the labeling function, where the latter can change depending on the state of the system (blue, orange, or green). simulations with expert queries. 56 These methods are also amenable to achieving Pareto optimal solutions under multiple objectives. 86,87 The machine learning methods for sequential decisionmaking use various notions of uncertainty (as previously mentioned) to guide sampling; however, these notions focus on uncertainty arising from noise or randomness in the label generation process, also known as aleatoric uncertainty. In many settings, there are many other epistemic sources of uncertainty such as model misspecification, biased sampling, data distribution or labeling function shift, etc. 88 Although many of these issues are addressed in classical works, they focus on simple settings-for example, model misspecification and model selection are handled for simple models that have limited approximation capabilities and good solutions for complex models such as Gaussian processes and deep neural networks are mostly lacking, 89 even for simple models these solutions are only known for supervised settings, while model selection for bandit optimization and reinforcement learning are just starting to be investigated, 90-92 similarly biased and missing data solutions are known for missing-under-random settings but robustness to challenges of modern real data such as distribution/label shifts have only gained momentum recently. [93][94][95][96] Additionally, uncertainty measures that scale well to high-dimensional settings and associated sequential decision-making methods are still in their infancy. [97][98][99] Thus, while sequential decision-making methods using machine learning are increasingly being used in materials science, and have significant potential to accelerate the pace of material discovery, leveraging this potential to enable efficient exploration of high-dimensional material design space requires further work. In particular, tight-knit integration of materials and machine learning researchers to jointly explore the challenges and opportunities at this intersection holds great promise.</p>
<p>Toward autonomy</p>
<p>As discussed previously, spectroscopy toolsets have provided a platform for ML development due to their complexity. [100][101][102][103] Developments have been focused on electron microscopy data, in part due to the availablility of large amounts of data as well as high data rates. 104 For example, novel detector designs enable manufacturing processes to be viewed "in operando" at high acquisition speeds (&gt;4000 frames per second, or fps). With data acquisition rates on the order of 2-3 TB/min, TEM gathers more data than ever before. 41,105 The availability of rapidly acquired, detailed hyperspectral data sets has enabled the tracking and discovery of intermediate states, providing a hint toward grand challenges in materials at extremes, such as harnessing metastable phenomena at regimes far from equilibrium. These advances are critical, as auto-encoders and related frameworks have become increasingly important as a foundation for autonomous science. A major factor in the shift from automation to autonomy is the ability to make decisions and alter processes on the fly, based on those decisions. For the majority of materials phenomena, this is not achievable due to the speed at which the phenomena (phase transition, defect formation, etc.) occur.</p>
<p>An increased interest in edge computing and hardware specific ML 106 for fast processes has made its way from communities such as high energy physics to materials science, allowing for rapid decisions, large data sets, and thus complex phenomena (as in the case of coupled extremes) to be tackled.</p>
<p>Summary and outlook</p>
<p>Although it is currently challenging to develop and assess materials rapidly and accurately, advances have been made that show the way forward for AI-guided high-throughput methods and AE that provide a foundation for advancing accelerated materials development for coupled extremes. Significant investment in rapid synthesis and in situ/inline characterization methods for high throughput and AE is needed. For materials in coupled extremes, the parameter space for composition, synthesis, and even characterization is highly complex, and not amenable to traditional reductionist approaches. 107 The point of high throughput and AE is a rapid flow of complex results directly followed by the supply of new conditions for processing of subsequent materials iterations, made even more complicated by the multiple objectives inherent in coupled extremes. This complex space drives a need for rapid decision-making capabilities enabled by AI/ML methods. In particular, ML methods that exploit known or hypothesized physical and chemical behavior are expected to be powerful tools to accelerate the development of materials in coupled extremes (Figure 4).</p>
<p>Autonomous platforms provide an opportunity to integrate functionality into materials evolution processes and, thus, we can now reimagine the way in which we develop, synthesize, Figure 4. Spatiotemporal and complexity regimes for various phenomena. Autonomous platforms for a given process must account for latency to a decision and modality of the operando data collection. and manufacture materials. In a world in which automation is pervasive, we must move from robotic tasks to machines that are capable of high-level decision-making. An overarching question when selecting, characterizing, or testing materials at extremes is, "do we need to choose between high throughput and precision approaches?" The answer is no, and thus the future lies in a smarter, more robust "high throughput" through which we maximize efficiency but maintain ability to pinpoint at low latency. Codesign of AI/ML, computing, and control methodologies will enable precise, low-latency decisions "on the fly" 108 and are undoubtedly needed when spanning variable length and time scales for coupled extreme phenomena. </p>
<p>Conflict of interest</p>
<p>On behalf of all authors, the corresponding author states that there is no conflict of interest.</p>
<p>Open access</p>
<p>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.</p>
<p>Figure 1 .
1Bubble plot showing potential acquisition time and resolution ranges of various methods used to characterize fundamental material structure and properties. EBSD-single, Electron backscattered diffraction-single tile; EBSD-tile, EBSD-multiple tiles; EBSD-w/SS = EBSD-with Serial Sectioning; EDS-SDD, Energy Dispersive Spectroscopy-Silicon Drift Detector; GDOES, Glow-Discharge Optical Emission Spectroscopy; n-Indent, Nano-Indentation; OM, Optical Microscope; OM-w/SS, Optical Microscopewith Serial Sectioning; Opt Contrast, Optical Contrast; PLS, Photostimulated Luminescence Spectroscopy; SAWS, Surface Acoustic Wave Spectroscopy; SEM-single, Scanning Electron Microscopy-Single Tile; SEM-tile, SEM-Multiple Tiles; SEM-w/SS, SEM-with Serial Sectioning; TDTR, Time-Domain Thermo-Reflectance; X-ray PS, X-ray Photoelectron Spectroscopy; XRD-Lab, XRD-Lab Standard Wide Angle X-ray scattering; XRD-synch, XRD-synchrotron Standard Wide Angle X-ray Scattering; XRF, X-ray Fluorescence; Î¼-Indent, Micro-Indentation; 3DXRD (HEDM), 3DXRD-synchrotron (HEDM).</p>
<p>Figure 2 .
2Spatiotemporal resolution ranges for core-loss spectroscopy methods. Direct detection electron energy-loss spectroscopy (EELS) offers the highest spatial and temporal resolution, at the cost of signal-to-noise ratio. EELS, Electron Energy-Loss Spectroscopy; STEM-EELS, Scanning Transmission Electron Microscope-Electron Energy-Loss Spectroscopy; XAFS, X-ray Absorption Fine Structure.</p>
<p>Acknowledgments B.M. acknowledges support from the Air Force Office of Scientific Research, LRIR M.-J. Pan. M.L.T. acknowledges funding in part from the US Department of Energy, Office of Basic Energy Sciences through Contract No. DESC0020314, in part from the Office of Naval Research Multidisciplinary University Research Initiative (MURI) program through Contract No. N00014-201-2368. J.H. and M.L.T. acknowledge funding in part from UES on behalf of the Air Force Research Laboratory under Contract No. S-111-085-001, and in part by the AT SCALE Initiative, under the Laboratory Directed Research and Development (LDRD) Program at Pacific Northwest National Laboratory (PNNL). PNNL is a multi-program national laboratory operated for the US Department of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05-76RL01830.
M.L. Taheri is a professor of materials science and engineering at Johns Hopkins University (JHU) and the director of the JHU Materials Characterization and Processing Center, a new user facility focused on artificial intelligence/machine learning (AI/ML)-enabled autonomous materials synthesis, characterization, and processing. Taheri also serves on the leadership council of the Ralph O'Connor Sustainable Energy Institute (ROSEI) at JHU and is a fellow of the Hopkins Extreme Materials Institute. She holds a joint appointment as chief materials scientist at Pacific Northwest National Laboratory. Her research focuses on in situ/operando electron microscopy and spectroscopy in extreme environments, and dynamic materials processing, including thermomechanical processing and additive manufacturing. Most recently, her group has been coupling experimental work with real-time AI/ML toward autonomous experimentation. Taheri can be reached by email at mtaheri4@jhu.edu.
. J J De Pablo, B Jones, C L Kovacs, V Ozolins, A P Ramirez, 10.1021/acscentsci.7b00572Curr. Opin. Solid State Mater. Sci. 2. R. GÃ³mez-Bombarelli, J.N. Wei, D. Duvenaud, J.M. HernÃ¡ndez-Lobato, B. SÃ¡nchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T.D. Hirzel, R.P. Adams, A. Aspuru-Guzik182268ACS Cent. Sci.J.J. De Pablo, B. Jones, C.L. Kovacs, V. Ozolins, A.P. Ramirez, Curr. Opin. Solid State Mater. Sci. 18(2), 99 (2014). https:// doi. org/ 10. 1016/j. cossms. 2014. 02. 003 2. R. GÃ³mez-Bombarelli, J.N. Wei, D. Duvenaud, J.M. HernÃ¡ndez-Lobato, B. SÃ¡nchez- Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T.D. Hirzel, R.P. Adams, A. Aspuru- Guzik, ACS Cent. Sci. 4(2), 268 (2018). https:// doi. org/ 10. 1021/ acsce ntsci. 7b005 72</p>
<p>. J R Deneault, J Chang, J Myung, D Hooper, A Armstrong, M Pitt, B Maruyama, Bull, 10.1557/s43577-021-00051-146566J.R. Deneault, J. Chang, J. Myung, D. Hooper, A. Armstrong, M. Pitt, B. Maruyama, MRS Bull. 46(7), 566 (2021). https:// doi. org/ 10. 1557/ s43577-021-00051-1</p>
<p>. A G Kusne, H Yu, C Wu, H Zhang, J Hattrick-Simpers, B Decost, S Sarker, C Oses, C Toher, S Curtarolo, A V Davydov, R Agarwal, L A Bendersky, M Li, A Mehta, I Takeuchi, 10.1038/s41467-020-19597-wNat. Commun. 1115966A.G. Kusne, H. Yu, C. Wu, H. Zhang, J. Hattrick-Simpers, B. DeCost, S. Sarker, C. Oses, C. Toher, S. Curtarolo, A.V. Davydov, R. Agarwal, L.A. Bendersky, M. Li, A. Mehta, I. Takeuchi, Nat. Commun. 11(1), 5966 (2020). https:// doi. org/ 10. 1038/ s41467-020-19597-w</p>
<p>2011) 6. Prepared under the direction of the ASM International Handbook Committee, ASM Handbook. M F Ashby, Metals Park, 1989) 7. NIST Materials Data Repository. BostonASM InternationalMaterials Selection in Mechanical DesignM.F. Ashby, Materials Selection in Mechanical Design (Elsevier, Boston, 2011) 6. Prepared under the direction of the ASM International Handbook Committee, ASM Handbook (ASM International, Metals Park, 1989) 7. NIST Materials Data Repository (2022). https:// mater ialsd ata. nist. gov 8. ASM online databases (2022). https:// www. asmin terna tional. org/ mater ials-resou rces/ online-datab ases</p>
<p>C B Nascimento, U Donatus, C T RÃ­os, M C L Oliveira, R A Antunes, 10.1590/1980-5373-MR-2021-0442Materials Commons published datasets. 25Materials Commons published datasets (2022). https:// www. mater ialsc ommons. org/ public 10. C.B. Nascimento, U. Donatus, C.T. RÃ­os, M.C.L. Oliveira, R.A. Antunes, Mater. Res. 25(3) (2022). https:// doi. org/ 10. 1590/ 1980-5373-MR-2021-0442</p>
<p>. S Liu, K Lee, P V Balachandran, 10.1063/5.0106124J. Appl. Phys. 132105105S. Liu, K. Lee, P.V. Balachandran, J. Appl. Phys. 132, 105105 (2022). https:// doi. org/ 10. 1063/5. 01061 24</p>
<p>. K Pal, Y Xia, J Shen, J He, Y Luo, M G Kanatzidis, C Wolverton, ; A Roy, M F N Taufique, H Khakurel, R Devanathan, D D Johnson, G ; L N Balasubramanian, L F Walters, J M Huang, Rondinelli, 10.1021/acs.jpcc.1c02505NPJ Comput. Mater. 7114027J. Phys. Chem. CK. Pal, Y. Xia, J. Shen, J. He, Y. Luo, M.G. Kanatzidis, C. Wolverton, NPJ Comput. Mater. 7(1), 82 (2021). https:// doi. org/ 10. 1038/ s41524-021-00549-x 13. A. Roy, M.F.N. Taufique, H. Khakurel, R. Devanathan, D.D. Johnson, G. Bal- asubramanian, NPJ Mater. Degrad. 6(1), 9 (2022). https:// doi. org/ 10. 1038/ s41529-021-00208-y 14. L.N. Walters, L.F. Huang, J.M. Rondinelli, J. Phys. Chem. C 125(25), 14027 (2021). https:// doi. org/ 10. 1021/ acs. jpcc. 1c025 05</p>
<p>National Science and Technology Council (US), in Materials Genome Initiative for Global Competitiveness (Executive Office of the President. K Choudhary, B Decost, C Chen, A Jain, F Tavazza, R Cohn, C W Park, A Choudhary, A Agrawal, S J L Billinge, E Holm, S P Ong, C Wolverton, 10.1038/s41524-022-00734-6National Science and Technology Council. 8159NPJ Comput. Mater.K. Choudhary, B. DeCost, C. Chen, A. Jain, F. Tavazza, R. Cohn, C.W. Park, A. Choudhary, A. Agrawal, S.J.L. Billinge, E. Holm, S.P. Ong, C. Wolverton, NPJ Comput. Mater. 8(1), 59 (2022). https:// doi. org/ 10. 1038/ s41524-022-00734-6 16. National Science and Technology Council (US), in Materials Genome Initiative for Global Competitiveness (Executive Office of the President, National Science and Technology Council, Washington, DC, 2011)</p>
<p>. E Stach, B Decost, A G Kusne, J Hattrick-Simpers, K A Brown, K G Reyes, J Schrier, S Billinge, T Buonassisi, I Foster, C P Gomes, J M Gregoire, A Mehta, J Montoya, E Olivetti, C Park, E Rotenberg, S K Saikin, S Smullin, V Stanev, B Maruyama, ; P Nikolaev, D Hooper, F Webber, R Rao, K Decker, M Krein, J Poleski, R Barto, B Maruyama, 10.1038/npjcompumats.2016.31NPJ Comput. Mater. 4916031MatterE. Stach, B. DeCost, A.G. Kusne, J. Hattrick-Simpers, K.A. Brown, K.G. Reyes, J. Schrier, S. Billinge, T. Buonassisi, I. Foster, C.P. Gomes, J.M. Gregoire, A. Mehta, J. Montoya, E. Olivetti, C. Park, E. Rotenberg, S.K. Saikin, S. Smullin, V. Stanev, B. Maruyama, Matter 4(9), 2702 (2021). https:// doi. org/ 10. 1016/j. matt. 2021. 06. 036 18. P. Nikolaev, D. Hooper, F. Webber, R. Rao, K. Decker, M. Krein, J. Poleski, R. Barto, B. Maruyama, NPJ Comput. Mater. 2(1), 16031 (2016). https:// doi. org/ 10. 1038/ npjco mpuma ts. 2016. 31</p>
<p>. D B Miracle, M Li, Z Zhang, R Mishra, K M Flores, 10.1146/annurev-matsci-080619-022100Annu. Rev. Mater. Res. 51131D.B. Miracle, M. Li, Z. Zhang, R. Mishra, K.M. Flores, Annu. Rev. Mater. Res. 51, 131 (2021). https:// doi. org/ 10. 1146/ annur ev-matsci-080619-022100</p>
<p>. M P Echlin, W C Lenthe, T M Pollock, 10.1186/s40192-014-0021-9Integr. Mater. Manuf. Innov. 31278M.P. Echlin, W.C. Lenthe, T.M. Pollock, Integr. Mater. Manuf. Innov. 3(1), 278 (2014). https:// doi. org/ 10. 1186/ s40192-014-0021-9</p>
<p>. M Moorehead, K Bertsch, M Niezgoda, C Parkin, M Elbakhshwan, K Sridharan, C Zhang, D Thoma, A Couet, 10.1016/j.matdes.2019.108358Mater. Des. 187108358M. Moorehead, K. Bertsch, M. Niezgoda, C. Parkin, M. Elbakhshwan, K. Srid- haran, C. Zhang, D. Thoma, A. Couet, Mater. Des. 187, 108358 (2020). https:// doi. org/ 10. 1016/j. matdes. 2019. 108358</p>
<p>. D C Hofmann, J Kolodziejska, S Roberts, R Otis, R P Dillon, J O Suh, Z K , D.C. Hofmann, J. Kolodziejska, S. Roberts, R. Otis, R.P. Dillon, J.O. Suh, Z.K.</p>
<p>Standard Test Methods for Tension Testing of Metallic Materials (ASTM E8/ E8M-21. J P Liu, Borgonia, 10.1557/jmr.2014.208West Conshohocken, 2021) 24. Standard Test Methods for Elevated Temperature Tension Tests of Metallic Materials. West ConshohockenASTM International291899ASTM E21-20Liu, J.P. Borgonia, J. Mater. Res. 29(17), 1899 (2014). https:// doi. org/ 10. 1557/ jmr. 2014. 208 23. Standard Test Methods for Tension Testing of Metallic Materials (ASTM E8/ E8M-21, ASTM International, West Conshohocken, 2021) 24. Standard Test Methods for Elevated Temperature Tension Tests of Metallic Materials (ASTM E21-20, ASTM International, West Conshohocken, 2020)</p>
<p>. F Zhang, L E Levine, A J Allen, M R Stoudt, G Lindwall, E A Lass, M E Williams, Y Idell, C E Campbell, 10.1016/j.actamat.2021.1173522018. 03. 017Acta Mater. 26. K.S. Vecchio, O.F. Dippo, K.R. Kaufmann, X. Liu152117352Acta Mater.F. Zhang, L.E. Levine, A.J. Allen, M.R. Stoudt, G. Lindwall, E.A. Lass, M.E. Wil- liams, Y. Idell, C.E. Campbell, Acta Mater. 152, 200 (2018). https:// doi. org/ 10. 1016/j. actam at. 2018. 03. 017 26. K.S. Vecchio, O.F. Dippo, K.R. Kaufmann, X. Liu, Acta Mater. 221, 117352 (2021). https:// doi. org/ 10. 1016/j. actam at. 2021. 117352</p>
<p>. J Wheeler, D Armstrong, W Heinz, R Schwaiger, Curr Opin, ; F Zhang, I Mcbrearty, R Ott, E Park, M I Mendelev, M Kramer, C Z Wang, K M Ho, 10.1149/1.1393565Solid State Mater. Sci. J.E. Maslar, W.S. Hurst, W. Bowers, J.H. Hendricks, M. Aquino1962532J. Electrochem. Soc.J. Wheeler, D. Armstrong, W. Heinz, R. Schwaiger, Curr. Opin. Solid State Mater. Sci. 19(6), 354 (2015). https:// doi. org/ 10. 1016/j. cossms. 2015. 02. 002 28. F. Zhang, I. McBrearty, R. Ott, E. Park, M.I. Mendelev, M. Kramer, C.Z. Wang, K.M. Ho, Scr. Mater. 81, 32 (2014). https:// doi. org/ 10. 1016/j. scrip tamat. 2014. 02. 019 29. J.E. Maslar, W.S. Hurst, W. Bowers, J.H. Hendricks, M. Aquino, J. Electrochem. Soc. 147(7), 2532 (2000). https:// doi. org/ 10. 1149/1. 13935 65</p>
<p>. C Corgnale, J Hattrick-Simpers, M Sulic, J Weidner, J Lopata, ; Y Wang, B Goh, P Nelaturu, T Duong, N Hassan, R David, M Moorehead, S Chaudhuri, A Creuziger, J Hattrick-Simpers, D J Thoma, K Sridharan, A Couet, 10.48550/arXiv.2104.10235arXiv:2104.10235Int. J. Hydrogen Energy. 433918363preprintC. Corgnale, J. Hattrick-Simpers, M. Sulic, J. Weidner, J. Lopata, Int. J. Hydro- gen Energy 43(39), 18363 (2018). https:// doi. org/ 10. 1016/j. ijhyd ene. 2018. 08. 025 31. Y. Wang, B. Goh, P. Nelaturu, T. Duong, N. Hassan, R. David, M. Moorehead, S. Chaudhuri, A. Creuziger, J. Hattrick-Simpers, D.J. Thoma, K. Sridharan, A. Couet, pre- print, arXiv: 2104. 10235. https:// doi. org/ 10. 48550/ arXiv. 2104. 10235 (2021)</p>
<p>. P A Shade, W D Musinski, M Obstalecki, D C Pagan, A J Beaudoin, J V Bernier, T J Turner, 10.1016/j.cossms.2019.07.002Curr. Opin. Solid State Mater. Sci. 235100763P.A. Shade, W.D. Musinski, M. Obstalecki, D.C. Pagan, A.J. Beaudoin, J.V. Bernier, T.J. Turner, Curr. Opin. Solid State Mater. Sci. 23(5), 100763 (2019). https:// doi. org/ 10. 1016/j. cossms. 2019. 07. 002</p>
<p>. M P Miller, D C Pagan, A J Beaudoin, K E Nygren, D J Shadle, 10.1007/s11661-020-05888-wMetall. Mater. Trans. A. 5194360M.P. Miller, D.C. Pagan, A.J. Beaudoin, K.E. Nygren, D.J. Shadle, Metall. Mater. Trans. A. 51(9), 4360 (2020). https:// doi. org/ 10. 1007/ s11661-020-05888-w</p>
<p>R E Dinnebier, S J L Billinge, Powder Diffraction: Theory and Practice. CambridgeRoyal Society of ChemistryR.E. Dinnebier, S.J.L. Billinge, Powder Diffraction: Theory and Practice (Royal Society of Chemistry, Cambridge, 2008)</p>
<p>. E Broitman, 10.1007/s11249-016-0805-5Tribol. Lett. 65123E. Broitman, Tribol. Lett. 65(1), 23 (2017). https:// doi. org/ 10. 1007/ s11249-016-0805-5</p>
<p>. J Roa, P S Phani, W C Oliver, L Llanes, 10.1016/j.ijrmhm.2018.04.01904. 019Int. J. Refract. Metals Hard Mater. 75211J. Roa, P.S. Phani, W.C. Oliver, L. Llanes, Int. J. Refract. Metals Hard Mater. 75, 211 (2018). https:// doi. org/ 10. 1016/j. ijrmhm. 2018. 04. 019</p>
<p>. J DÅ¾ugan, R ProchÃ¡zka, P KonopÃ­k, Small Specimen Test Techniques. 6ASTM InternationalMaterials ParkJ. DÅ¾ugan, R. ProchÃ¡zka, P. KonopÃ­k, in Small Specimen Test Techniques, vol. 6 (ASTM International, Materials Park, 2015)</p>
<p>D W Eastman, Z Alam, G Weber, P A Shade, M D Uchic, W C Lenthe, T M Pollock, K J Hemker, Superalloys 2016: Proceedings of the 13th International Symposium of Superalloys. HobokenWileyD.W. Eastman, Z. Alam, G. Weber, P.A. Shade, M.D. Uchic, W.C. Lenthe, T.M. Pollock, K.J. Hemker, in Superalloys 2016: Proceedings of the 13th International Symposium of Superalloys (Wiley, Hoboken, 2016), pp. 811-820</p>
<p>. D Armstrong, C Hardie, J Gibson, A Bushby, P Edmondson, S Roberts, 10.1016/j.jnucmat.2015.01.053J. Nucl. Mater. 462374D. Armstrong, C. Hardie, J. Gibson, A. Bushby, P. Edmondson, S. Roberts, J. Nucl. Mater. 462, 374 (2015). https:// doi. org/ 10. 1016/j. jnucm at. 2015. 01. 053</p>
<p>. M D Uchic, P A Shade, D M Dimiduk, 10.1146/annurev-matsci-082908-145422Annu. Rev. Mater. Res. 391361M.D. Uchic, P.A. Shade, D.M. Dimiduk, Annu. Rev. Mater. Res. 39(1), 361 (2009). https:// doi. org/ 10. 1146/ annur ev-matsci-082908-145422</p>
<p>. J L Hart, A C Lang, A C Leff, P Longo, C Trevor, R D Twesten, M L Taheri, 10.1038/s41598-017-07709-4Sci. Rep. 718243J.L. Hart, A.C. Lang, A.C. Leff, P. Longo, C. Trevor, R.D. Twesten, M.L. Taheri, Sci. Rep. 7(1), 8243 (2017). https:// doi. org/ 10. 1038/ s41598-017-07709-4</p>
<p>. J L Hart, A C Lang, Y Li, K Hantanasirisakul, A I Frenkel, M L Taheri, 10.48550/arXiv.1909.06323arXiv:1909.06323preprintJ.L. Hart, A.C. Lang, Y. Li, K. Hantanasirisakul, A.I. Frenkel, M.L. Taheri, preprint, arXiv: 1909. 06323. https:// doi. org/ 10. 48550/ arXiv. 1909. 06323 (2019)</p>
<p>. J L Hart, K Hantanasirisakul, A C Lang, Y Li, F Mehmood, R Pachter, A I Frenkel, Y Gogotsi, M L Taheri, 10.1002/admi.202001789Adv. Mater. Interfaces. 852001789J.L. Hart, K. Hantanasirisakul, A.C. Lang, Y. Li, F. Mehmood, R. Pachter, A.I. Frenkel, Y. Gogotsi, M.L. Taheri, Adv. Mater. Interfaces 8(5), 2001789 (2021). https:// doi. org/ 10. 1002/ admi. 20200 1789</p>
<p>. F Ren, L Ward, T Williams, K J Laws, C Wolverton, J Hattrick-Simpers, 10.1126/sciadv.aaq1566A. Mehta, Sci. Adv. 441566F. Ren, L. Ward, T. Williams, K.J. Laws, C. Wolverton, J. Hattrick-Simpers, A. Mehta, Sci. Adv. 4(4), eaaq1566 (2018). https:// doi. org/ 10. 1126/ sciadv. aaq15 66</p>
<p>. N Marcella, J S Lim, A M PÅ‚onka, G Yan, C J Owen, J.E.S. van der Hoeven, A.CN. Marcella, J.S. Lim, A.M. PÅ‚onka, G. Yan, C.J. Owen, J.E.S. van der Hoeven, A.C.</p>
<p>. H T Foucher, S B Ngan, N S Torrisi, E A Marinkovic, J F Stach, J Weaver, P Aizenberg, B Sautet, A I Kozinsky, Frenkel, 10.1038/s41467-022-28366-wNat. Commun. 131832Foucher, H.T. Ngan, S.B. Torrisi, N.S. Marinkovic, E.A. Stach, J.F. Weaver, J. Aizenberg, P. Sautet, B. Kozinsky, A.I. Frenkel, Nat. Commun. 13(1), 832 (2022). https:// doi. org/ 10. 1038/ s41467-022-28366-w</p>
<p>. T Konstantinova, P M Maffettone, B Ravel, S I Campbell, A M Barbour, D Olds, 10.1039/D2DD00014HDigit. Discov. 1413T. Konstantinova, P.M. Maffettone, B. Ravel, S.I. Campbell, A.M. Barbour, D. Olds, Digit. Discov. 1, 413 (2022). https:// doi. org/ 10. 1039/ D2DD0 0014H</p>
<p>T M Mitchell, T M Mitchell, Machine Learning. New YorkMcGraw-Hill1T.M. Mitchell, T.M. Mitchell, Machine Learning, vol. 1 (McGraw-Hill, New York, 1997)</p>
<p>. A Seko, H Hayashi, K Nakayama, A Takahashi, I Tanaka, 10.1103/PhysRevB.95.144110Phys. Rev. B. 9514144110A. Seko, H. Hayashi, K. Nakayama, A. Takahashi, I. Tanaka, Phys. Rev. B 95(14), 144110 (2017). https:// doi. org/ 10. 1103/ PhysR evB. 95. 144110</p>
<p>. H Wei, S Zhao, Q Rong, H Bao, 10.1016/j.ijheatmasstransfer.2018.08.0822018. 08. 082Int. J. Heat Mass Transf. 127908H. Wei, S. Zhao, Q. Rong, H. Bao, Int. J. Heat Mass Transf. 127, 908 (2018). https:// doi. org/ 10. 1016/j. ijhea tmass trans fer. 2018. 08. 082</p>
<p>. S K Kauwe, J Graser, A Vazquez, T D Sparks, 10.1007/s40192-018-0108-9Integr. Mater. Manuf. Innov. 7243S.K. Kauwe, J. Graser, A. Vazquez, T.D. Sparks, Integr. Mater. Manuf. Innov. 7(2), 43 (2018). https:// doi. org/ 10. 1007/ s40192-018-0108-9</p>
<p>. E Kim, Z Jensen, A Van Grootel, K Huang, M Staib, S Mysore, H.-S Chang, E Strubell, A Mccallum, S Jegelka, E Olivetti, 10.1021/acs.jcim.9b00995J. Chem. Inf. Model. 6031194E. Kim, Z. Jensen, A. van Grootel, K. Huang, M. Staib, S. Mysore, H.-S. Chang, E. Strubell, A. McCallum, S. Jegelka, E. Olivetti, J. Chem. Inf. Model. 60(3), 1194 (2020). https:// doi. org/ 10. 1021/ acs. jcim. 9b009 95</p>
<p>. M W Gaultois, A O Oliynyk, A Mar, T D Sparks, G J Mulholland, B Meredig, Mater, 10.1063/1.4952607453213M.W. Gaultois, A.O. Oliynyk, A. Mar, T.D. Sparks, G.J. Mulholland, B. Meredig, APL Mater. 4(5), 053213 (2016). https:// doi. org/ 10. 1063/1. 49526 07</p>
<p>. J Behler, 10.1002/qua.24890Int. J. Quantum Chem. 115161032J. Behler, Int. J. Quantum Chem. 115(16), 1032 (2015). https:// doi. org/ 10. 1002/ qua. 24890</p>
<p>. J Graser, S K Kauwe, T D Sparks, 10.1021/acs.chemmater.7b05304Chem. Mater. 30113601J. Graser, S.K. Kauwe, T.D. Sparks, Chem. Mater. 30(11), 3601 (2018). https:// doi. org/ 10. 1021/ acs. chemm ater. 7b053 04</p>
<p>. S Mysore, E Kim, E Strubell, A Liu, H S Chang, S Kompella, K Huang, A Mccallum, E Olivetti, 10.48550/arXiv.1711.06872arXiv:1711.06872S. Mysore, E. Kim, E. Strubell, A. Liu, H.S. Chang, S. Kompella, K. Huang, A. McCallum, E. Olivetti, preprint, arXiv: 1711. 06872. https:// doi. org/ 10. 48550/ arXiv. 1711. 06872 (2017)</p>
<p>Phase Mapper: An AI Platform to Accelerate High Throughput Materials Discovery. Y Xue, J Bai, R Le Bras, B Rappazzo, R Bernstein, J Bjorck, L Longpre, S K Suram, R B Van Dover, J Gregoire, C P Gomes, 29th Conference on Innovative Applications of Artificial Intelligence, IAAI'17. San FranciscoY. Xue, J. Bai, R. Le Bras, B. Rappazzo, R. Bernstein, J. Bjorck, L. Longpre, S.K. Suram, R.B. van Dover, J. Gregoire, C.P. Gomes, "Phase Mapper: An AI Plat- form to Accelerate High Throughput Materials Discovery," 29th Conference on Innovative Applications of Artificial Intelligence, IAAI'17 (San Francisco, February 6-9, 2017)</p>
<p>. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, 10.1038/s41467-022-29939-5Nat. Commun. 1312453S. Batzner, A. Musaelian, L. Sun, M. Geiger, J.P. Mailoa, M. Kornbluth, N. Molinari, T.E. Smidt, B. Kozinsky, Nat. Commun. 13(1), 2453 (2022). https:// doi. org/ 10. 1038/ s41467-022-29939-5</p>
<p>. M Raissi, P Perdikaris, G E Karniadakis, 10.1016/j.jcp.2018.10.045J. Comput. Phys. 37845M. Raissi, P. Perdikaris, G.E. Karniadakis, J. Comput. Phys. 378, 686 (2019). https:// doi. org/ 10. 1016/j. jcp. 2018. 10. 045</p>
<p>. A Mcdannald, M Frontzek, A T Savici, M Doucet, E E Rodriguez, K Meuse, J Opsahl-Ong, D Samarov, I Takeuchi, W Ratcliff, A G Kusne, 10.1063/5.0082956Appl. Phys. Rev. 9221408A. McDannald, M. Frontzek, A.T. Savici, M. Doucet, E.E. Rodriguez, K. Meuse, J. Opsahl-Ong, D. Samarov, I. Takeuchi, W. Ratcliff, A.G. Kusne, Appl. Phys. Rev. 9(2), 021408 (2022). https:// doi. org/ 10. 1063/5. 00829 56</p>
<p>. V Stanev, V V Vesselinov, A G Kusne, G Antoszewski, I Takeuchi, B S Alexandrov, 10.1038/s41524-018-0099-2NPJ Comput. Mater. 4143V. Stanev, V.V. Vesselinov, A.G. Kusne, G. Antoszewski, I. Takeuchi, B.S. Alexandrov, NPJ Comput. Mater. 4(1), 43 (2018). https:// doi. org/ 10. 1038/ s41524-018-0099-2</p>
<p>. H Huo, Z Rong, O Kononova, W Sun, T Botari, T He, V Tshitoyan, G Ceder, 10.1038/s41524-019-0204-1NPJ Comput. Mater. 5162H. Huo, Z. Rong, O. Kononova, W. Sun, T. Botari, T. He, V. Tshitoyan, G. Ceder, NPJ Comput. Mater. 5(1), 62 (2019). https:// doi. org/ 10. 1038/ s41524-019-0204-1</p>
<p>. M Ziatdinov, C T Nelson, X Zhang, R K Vasudevan, E Eliseev, A N Morozovska, I , M. Ziatdinov, C.T. Nelson, X. Zhang, R.K. Vasudevan, E. Eliseev, A.N. Morozovska, I.</p>
<p>. S V Takeuchi, Kalinin, 10.1038/s41524-020-00396-2NPJ Comput. Mater. 61127Takeuchi, S.V. Kalinin, NPJ Comput. Mater. 6(1), 127 (2020). https:// doi. org/ 10. 1038/ s41524-020-00396-2</p>
<p>. K Rajan, C Suh, P F Mendez, 10.1002/sam.10031Data Sci. J. 16361K. Rajan, C. Suh, P.F. Mendez, Data Sci. J. 1(6), 361 (2009). https:// doi. org/ 10. 1002/ sam. 10031</p>
<p>. D Mrdjenovich, M K Horton, J H Montoya, C M Legaspi, S Dwaraknath, V Tshitoyan, A Jain, K A Persson, 10.1016/j.matt.2019.11.013Matter. 22464D. Mrdjenovich, M.K. Horton, J.H. Montoya, C.M. Legaspi, S. Dwaraknath, V. Tshi- toyan, A. Jain, K.A. Persson, Matter 2(2), 464 (2020). https:// doi. org/ 10. 1016/j. matt. 2019. 11. 013</p>
<p>. T Lookman, P V Balachandran, D Xue, R Yuan, 10.1038/s41524-019-0153-8NPJ Comput. Mater. 5121T. Lookman, P.V. Balachandran, D. Xue, R. Yuan, NPJ Comput. Mater. 5(1), 21 (2019). https:// doi. org/ 10. 1038/ s41524-019-0153-8</p>
<p>. A R Will-Cole, A G Kusne, P Tonner, C Dong, X Liang, H Chen, N X Sun, 10.1093/oxfmat/itac006Oxf. Open Mater. Sci. A. Wang, H. Liang, A. McDannald, I. Takeuchi, A.G. Kusne5816IEEE Trans. Magn.A.R. Will-Cole, A.G. Kusne, P. Tonner, C. Dong, X. Liang, H. Chen, N.X. Sun, IEEE Trans. Magn. 58(1), 1 (2021). https:// doi. org/ 10. 1109/ TMAG. 2021. 31252 50 67. A. Wang, H. Liang, A. McDannald, I. Takeuchi, A.G. Kusne, Oxf. Open Mater. Sci. 2(1), itac006 (2022). https:// doi. org/ 10. 1093/ oxfmat/ itac0 06</p>
<p>. F Bateni, R W Epps, K Antami, R Dargis, J A Bennett, K G Reyes, M Abolhasani, ; S Ament, M Amsler, D R Sutherland, M C Chang, D Guevarra, A B Connolly, J M Gregoire, M O Thompson, C P Gomes, R B Van Dover, 10.1126/sciadv.abg4930Adv. Intell. Syst. 454930Sci. Adv.F. Bateni, R.W. Epps, K. Antami, R. Dargis, J.A. Bennett, K.G. Reyes, M. Abolhasani, Adv. Intell. Syst. 4(5), 2200017 (2022). https:// doi. org/ 10. 1002/ aisy. 20220 0017 69. S. Ament, M. Amsler, D.R. Sutherland, M.C. Chang, D. Guevarra, A.B. Connolly, J.M. Gregoire, M.O. Thompson, C.P. Gomes, R.B. van Dover, Sci. Adv. 7(51), eabg4930 (2021). https:// doi. org/ 10. 1126/ sciadv. abg49 30</p>
<p>. J Chang, P Nikolaev, J Carpena-NÃºÃ±ez, R Rao, K Decker, A E Islam, J Kim, M A Pitt, 10.1038/s41598-020-64397-3J.I. Myung, B. Maruyama, Sci. Rep. 1019040J. Chang, P. Nikolaev, J. Carpena-NÃºÃ±ez, R. Rao, K. Decker, A.E. Islam, J. Kim, M.A. Pitt, J.I. Myung, B. Maruyama, Sci. Rep. 10(1), 9040 (2020). https:// doi. org/ 10. 1038/ s41598-020-64397-3</p>
<p>S K Gottipati, B Sattarov, S Niu, Y Pathak, H Wei, S Liu, S Blackburn, K Thomas, C Coley, J Tang, S Chandar, Y Bengio, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR119S.K. Gottipati, B. Sattarov, S. Niu, Y. Pathak, H. Wei, S. Liu, S. Blackburn, K. Thomas, C. Coley, J. Tang, S. Chandar, Y. Bengio, in Proceedings of the 37th International Con- ference on Machine Learning, vol. 119 (PMLR, 2020), pp. 3668-3679</p>
<p>B Settles, Active Learning, Synthesis Lectures on Artificial Intelligence and Machine Learning. A. Krishnamurthy, A. SinghChamSpringer6836B. Settles, in Active Learning, Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 6(1) (Springer, Cham, 2012), p. 1 73. A. Krishnamurthy, A. Singh, Adv. Neural Inf. Process. Syst. 26, 836 (2013)</p>
<p>G Dasarathy, A Singh, M - F Balcan, J H Park, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics. the 19th International Conference on Artificial Intelligence and StatisticsPMLR51G. Dasarathy, A. Singh, M-.F. Balcan, J.H. Park, in Proceedings of the 19th Inter- national Conference on Artificial Intelligence and Statistics, vol. 51 (PMLR, 2016), pp. 1356-1364</p>
<p>. A Krishnamurthy, S Balakrishnan, M Xu, A Singh, 10.48550/arXiv.1206.4672arXiv:1206.4672A. Krishnamurthy, S. Balakrishnan, M. Xu, A. Singh, preprint, arXiv: 1206. 4672. https:// doi. org/ 10. 48550/ arXiv. 1206. 4672 (2012)</p>
<p>Bayesian Optimization. R Garnett, Cambridge University PressCambridge1st edn.R. Garnett, Bayesian Optimization, 1st edn. (Cambridge University Press, Cam- bridge, 2023)</p>
<p>T Lattimore, C SzepesvÃ¡ri, Bandit Algorithms. CambridgeCambridge University PressT. Lattimore, C. SzepesvÃ¡ri, Bandit Algorithms (Cambridge University Press, Cam- bridge, 2020)</p>
<p>R S Sutton, A G Barto, Reinforcement Learning: An Introduction. CambridgeMIT PressR.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction (MIT Press, Cam- bridge, 2018)</p>
<p>. P Auer, J , 10.1109/SFCS.2000.892116Adv. Neural. Inf. Process. Syst. C. Dann, M. Mohri, T. Zhang, J. Zimmert3112040Mach. Learn. Res.P. Auer, J. Mach. Learn. Res. 3(1), 397 (2002). https:// doi. org/ 10. 1109/ SFCS. 2000. 892116 80. C. Dann, M. Mohri, T. Zhang, J. Zimmert, Adv. Neural. Inf. Process. Syst. 34, 12040 (2021)</p>
<p>. K Kandasamy, G Dasarathy, B PÃ³czos, J G Schneider, Adv. Neural Inf. Process. Syst. 291777K. Kandasamy, G. Dasarathy, B. PÃ³czos, J.G. Schneider, Adv. Neural Inf. Process. Syst. 29, 1777 (2016)</p>
<p>. K Kandasamy, G Dasarathy, J B Oliva, J G Schneider, B PÃ³czos, Adv. Neural Inf. Process. Syst. 29992K. Kandasamy, G. Dasarathy, J.B. Oliva, J.G. Schneider, B. PÃ³czos, Adv. Neural Inf. Process. Syst. 29, 992 (2016)</p>
<p>K Kandasamy, G Dasarathy, J Schneider, B PÃ³czos, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningK. Kandasamy, G. Dasarathy, J. Schneider, B. PÃ³czos, in Proceedings of the 34th International Conference on Machine Learning (PMLR, 2017), pp. 1799-1808</p>
<p>. A Tran, J Tranchida, T Wildey, A P Thompson, 10.1063/5.0015672J. Chem. Phys. 153774705A. Tran, J. Tranchida, T. Wildey, A.P. Thompson, J. Chem. Phys. 153(7), 074705 (2020). https:// doi. org/ 10. 1063/5. 00156 72</p>
<p>. A Cesari, A Gil-Ley, G Bussi, 10.1021/acs.jctc.6b00944J. Chem. Theory Comput. 12126192A. Cesari, A. Gil-Ley, G. Bussi, J. Chem. Theory Comput. 12(12), 6192 (2016). https:// doi. org/ 10. 1021/ acs. jctc. 6b009 44</p>
<p>B Paria, K Kandasamy, B PÃ³czos, Proceedings of the 35th Uncertainty in Artificial Intelligence Conference. the 35th Uncertainty in Artificial Intelligence ConferencePMLR115B. Paria, K. Kandasamy, B. PÃ³czos, in Proceedings of the 35th Uncertainty in Artificial Intelligence Conference, vol. 115 (PMLR, 2020), pp. 766-776</p>
<p>. B P Macleod, F G Parlane, C C Rupnow, K E Dettelbach, M S Elliott, T D Morrissey, T H Haley, O Proskurin, M B Rooney, N Taherimakhsousi, D J Dvorak, H N Chiu, C E B Waizenegger, K Ocean, M Mokhtari, C P Berlinguette, 10.1038/s41467-022-28580-6Nat. Commun. 1311B.P. MacLeod, F.G. Parlane, C.C. Rupnow, K.E. Dettelbach, M.S. Elliott, T.D. Mor- rissey, T.H. Haley, O. Proskurin, M.B. Rooney, N. Taherimakhsousi, D.J. Dvorak, H.N. Chiu, C.E.B. Waizenegger, K. Ocean, M. Mokhtari, C.P. Berlinguette, Nat. Commun. 13(1), 1 (2022). https:// doi. org/ 10. 1038/ s41467-022-28580-6</p>
<p>. E HÃ¼llermeier, W Waegeman, 10.1007/s10994-021-05946-3Mach. Learn. 1103457E. HÃ¼llermeier, W. Waegeman, Mach. Learn. 110(3), 457 (2021). https:// doi. org/ 10. 1007/ s10994-021-05946-3</p>
<p>. I Bogunovic, A Krause, Adv. Neural. Inf. Process. Syst. 343004I. Bogunovic, A. Krause, Adv. Neural. Inf. Process. Syst. 34, 3004 (2021)</p>
<p>. D J Foster, A Krishnamurthy, H Luo, Adv. Neural Inf. Process. Syst. 3214741D.J. Foster, A. Krishnamurthy, H. Luo, Adv. Neural Inf. Process. Syst. 32, 14741 (2019)</p>
<p>. A Pacchiano, M Phan, Y Yadkori, A Rao, J Zimmert, T Lattimore, C Szepesvari, Adv. Neural Inf. Process. Syst. 3310328A. Pacchiano, M. Phan, Y. Abbasi Yadkori, A. Rao, J. Zimmert, T. Lattimore, C. Szepesvari, Adv. Neural Inf. Process. Syst. 33, 10328 (2020)</p>
<p>J Lee, A Pacchiano, V Muthukumar, W Kong, E Brunskill, Proceedings of the 24th International Conference on Artificial Intelligence and Statistics. the 24th International Conference on Artificial Intelligence and StatisticsPMLR130J. Lee, A. Pacchiano, V. Muthukumar, W. Kong, E. Brunskill, in Proceedings of the 24th International Conference on Artificial Intelligence and Statistics, vol. 130 (PMLR, 2021), p. 3340-3348</p>
<p>Z Lipton, Y.-X Wang, A Smola, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR80Z. Lipton, Y.-X. Wang, A. Smola, in Proceedings of the 35th International Confer- ence on Machine Learning, vol. 80 (PMLR, 2018), pp. 3122-3130</p>
<p>J G Moreno-Torres, T Raeder, R Alaiz-RodrÃ­guez, N V Chawla, F Herrera, Pattern Recognit, ; B Kulis, K Saenko, T Darrell, 10.1016/j.patcog.2011.06.019Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIEEE45J.G. Moreno-Torres, T. Raeder, R. Alaiz-RodrÃ­guez, N.V. Chawla, F. Herrera, Pat- tern Recognit. 45(1), 521 (2012). https:// doi. org/ 10. 1016/j. patcog. 2011. 06. 019 95. B. Kulis, K. Saenko, T. Darrell, in Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR 2011) (IEEE, 2011), pp. 1785-1792</p>
<p>. A Ghosh, B G Sumpter, O Dyck, S V Kalinin, M Ziatdinov, 10.1038/s41524-021-00569-7NPJ Comput. Mater. 71100A. Ghosh, B.G. Sumpter, O. Dyck, S.V. Kalinin, M. Ziatdinov, NPJ Comput. Mater. 7(1), 100 (2021). https:// doi. org/ 10. 1038/ s41524-021-00569-7</p>
<p>. B Lakshminarayanan, A Pritzel, C Blundell, Adv. Neural Inf. Process. Syst. 306393B. Lakshminarayanan, A. Pritzel, C. Blundell, Adv. Neural Inf. Process. Syst. 30, 6393 (2017)</p>
<p>. A Malinin, M J F Gales, Adv. Neural Inf. Process. Syst. 317047A. Malinin, M.J.F. Gales, Adv. Neural Inf. Process. Syst. 31, 7047 (2018)</p>
<p>. A Olivier, M D Shields, L Graham-Brady, 10.1016/j.cma.2021.114079Comput. Methods Appl. Mech. Eng. 386114079A. Olivier, M.D. Shields, L. Graham-Brady, Comput. Methods Appl. Mech. Eng. 386, 114079 (2021). https:// doi. org/ 10. 1016/j. cma. 2021. 114079</p>
<p>. C M Pate, J L Hart, M L Taheri, 10.1038/s41598-021-97668-8Sci. Rep. 11119515C.M. Pate, J.L. Hart, M.L. Taheri, Sci. Rep. 11(1), 19515 (2021). https:// doi. org/ 10. 1038/ s41598-021-97668-8</p>
<p>. S S Mousavi, M , A Pofelski, G Botton, Microsc, 10.1017/S1431927621005997271626Suppl. 1S.S. Mousavi M, A. Pofelski, G. Botton, Microsc. Microanal. 27 (Suppl. 1), 1626 (2021). https:// doi. org/ 10. 1017/ S1431 92762 10059 97</p>
<p>. N Creange, O Dyck, R K Vasudevan, M Ziatdinov, S V Kalinin, 10.1088/2632-2153/ac3844Mach. Learn. Sci. Technol. 3115024N. Creange, O. Dyck, R.K. Vasudevan, M. Ziatdinov, S.V. Kalinin, Mach. Learn. Sci. Technol. 3(1), 015024 (2022). https:// doi. org/ 10. 1088/ 2632-2153/ ac3844</p>
<p>. Y Auad, M Walls, J.-D Blazit, O StÃ©phan, L H G Tizei, M Kociak, F De La PeÃ±a, M TencÃ©, 10.1016/j.ultramic.2022.113539Ultramicroscopy. 239113539Y. Auad, M. Walls, J.-D. Blazit, O. StÃ©phan, L.H.G. Tizei, M. Kociak, F. De la PeÃ±a, M. TencÃ©, Ultramicroscopy 239, 113539 (2022). https:// doi. org/ 10. 1016/j. ultra mic. 2022. 113539</p>
<p>. S R Spurgeon, C Ophus, L Jones, A Petford-Long, S V Kalinin, M J Olszta, R E Dunin-Borkowski, N Salmon, K Hattar, W.-C D Yang, R Sharma, Y Du, A Chiaramonti, H Zheng, E C Buck, L Kovarik, R L Penn, D Li, X Zhang, M Murayama, M L Taheri, 10.1038/s41563-020-00833-zNat. Mater. 203274S.R. Spurgeon, C. Ophus, L. Jones, A. Petford-Long, S.V. Kalinin, M.J. Olszta, R.E. Dunin-Borkowski, N. Salmon, K. Hattar, W.-C.D. Yang, R. Sharma, Y. Du, A. Chiaramonti, H. Zheng, E.C. Buck, L. Kovarik, R.L. Penn, D. Li, X. Zhang, M. Murayama, M.L. Taheri, Nat. Mater. 20(3), 274 (2021). https:// doi. org/ 10. 1038/ s41563-020-00833-z</p>
<p>. M W Tate, P Purohit, D Chamberlain, K X Nguyen, R Hovden, C S Chang, P Deb, E Turgut, J T Heron, D G Schlom, D C Ralph, G D Fuchs, K S Shanks, H T Philipp, D A Muller, S M Gruner, 10.1017/S1431927615015664Microsc. Microanal. 221237M.W. Tate, P. Purohit, D. Chamberlain, K.X. Nguyen, R. Hovden, C.S. Chang, P. Deb, E. Turgut, J.T. Heron, D.G. Schlom, D.C. Ralph, G.D. Fuchs, K.S. Shanks, H.T. Philipp, D.A. Muller, S.M. Gruner, Microsc. Microanal. 22(1), 237 (2016). https:// doi. org/ 10. 1017/ S1431 92761 50156 64</p>
<p>. F J Alexander, J Ang, J A Bilbrey, J Balewski, T Casey, R Chard, J Choi, S Choudhury, B Debusschere, A M Degennaro, N Dryden, J A Ellis, I Foster, C Garcia-Cardona, S Ghosh, P Harrington, Y Huang, S Jha, T Johnston, A Kagawa, R Kannan, N Kumar, Z Liu, N Maruyama, S Matsuoka, E Mccarthy, J Mohd-Yusof, P Nugent, Y Oyama, T Proffen, D Pugmire, S Rajamanickam, V Ramakrishniah, M Schram, S K Seal, G Sivaraman, C Sweeney, L Tan, R Thakur, B Van Essen, L Ward, P Welch, M Wolf, S S Xantheas, K G Yager, S Yoo, B.-J Yoon, 10.1177/10943420211029302Int. J. High Perform. Comput. Appl. 356598F.J. Alexander, J. Ang, J.A. Bilbrey, J. Balewski, T. Casey, R. Chard, J. Choi, S. Choudhury, B. Debusschere, A.M. DeGennaro, N. Dryden, J.A. Ellis, I. Foster, C. Garcia- Cardona, S. Ghosh, P. Harrington, Y. Huang, S. Jha, T. Johnston, A. Kagawa, R. Kannan, N. Kumar, Z. Liu, N. Maruyama, S. Matsuoka, E. McCarthy, J. Mohd-Yusof, P. Nugent, Y. Oyama, T. Proffen, D. Pugmire, S. Rajamanickam, V. Ramakrishniah, M. Schram, S.K. Seal, G. Sivaraman, C. Sweeney, L. Tan, R. Thakur, B. Van Essen, L. Ward, P. Welch, M. Wolf, S.S. Xantheas, K.G. Yager, S. Yoo, B.-J. Yoon, Int. J. High Perform. Comput. Appl. 35(6), 598 (2021). https:// doi. org/ 10. 1177/ 10943 42021 10293 02</p>
<p>. K G Reyes, B Maruyama, Bull, 10.1557/mrs.2019.15344530K.G. Reyes, B. Maruyama, MRS Bull. 44(7), 530 (2019). https:// doi. org/ 10. 1557/ mrs. 2019. 153</p>
<p>T Geng, A Li, R Shi, C Wu, T Wang, Y Li, P Haghi, A Tumeo, S Che, S Reinhardt, M C Herbordt, 53rd Annual IEEE/ACM International Symposium on Microarchitecture. AthensIEEE2020T. Geng, A. Li, R. Shi, C. Wu, T. Wang, Y. Li, P. Haghi, A. Tumeo, S. Che, S. Reinhardt, M.C. Herbordt, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2020) (IEEE, Athens, October 17-21, 2020), pp. 922-936 âƒž</p>            </div>
        </div>

    </div>
</body>
</html>
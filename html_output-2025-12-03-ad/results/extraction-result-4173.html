<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4173 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4173</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4173</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276813240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.04629v1.pdf" target="_blank">SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing</a></p>
                <p><strong>Paper Abstract:</strong> Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4173.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4173.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous Scientific Discovery (research area)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A research area referenced in the paper where LLMs and language agents are explored for automating aspects of scientific discovery, such as idea generation and data-driven hypothesis formation; mentioned as related work but not implemented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A broad research direction (collection of methods and systems) investigating the use of LLMs and multi-agent language systems to discover, generate, or propose scientific hypotheses, insights, or patterns from literature and data. In this paper it is cited as background and motivation rather than instantiated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multiple / general scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper notes general limitations of LLMs in this area: difficulty synthesizing deep inter-publication relationships, lack of critical thinking/originality, hallucinations and inaccurate citations, and inability to reliably construct temporal methodological evolution or deep quantitative relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4173.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4173.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScienceAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced benchmark intended to evaluate language agents for data-driven scientific discovery; mentioned in related work as an effort to assess agents that could perform scientific-discovery-like tasks on literature/data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ScienceAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A benchmarking/assessment suite for evaluating language agents on tasks related to data-driven scientific discovery (mentioned in the paper's related work). It is referenced as an example of prior work on assessing agents for discovery, not used in SURVEYFORGE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / data-driven scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Benchmark evaluation of language agents (as per the referenced paper); used to assess agent capabilities rather than reported results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4173.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4173.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sciagents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system/work that proposes multi-agent graph reasoning to automate scientific-discovery-like workflows; cited as related work exploring automation of scientific discovery with AI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sciagents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent approach relying on intelligent graph reasoning to automate aspects of scientific discovery (cited in the paper's related work). The SURVEYFORGE paper references it as part of the landscape of efforts to apply LLMs/agents to discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / computational scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Reportedly uses graph-based reasoning across entities/papers (as described by the referenced title); no experimental detail provided in SURVEYFORGE.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4173.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4173.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COI-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COI-Agent (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited system (Li et al., 2024) referenced for leveraging LLMs to generate novel scientific ideas; mentioned in the related work on autonomous idea generation rather than used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COI-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of prior work that uses content models/agents to propose or select scientific ideas; the SURVEYFORGE paper refers to COI-Agent in the context of LLM-powered idea generation but gives no implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited works in the area often produce outputs lacking deep coherence or critical synthesis; SURVEYFORGE emphasizes these shortcomings as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4173.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4173.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SANA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scholar Navigation Agent (SANA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The memory-driven scholar navigation agent introduced and used in this paper (part of SURVEYFORGE) to decompose sub-queries, retrieve context-aware literature from a large research database, and rerank results using temporal-aware criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SANA (Scholar Navigation Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline component in SURVEYFORGE consisting of: Memory for Sub-query (MS) which stores retrieved literature as memory to guide LLM-based query decomposition; Memory for Retrieval (MR) which uses embedding similarity against the memory to retrieve candidate papers for subsections; and a Temporal-aware Reranking Engine (TRE) that re-ranks by publication time groups and citation impact to select top-k per time bin. SANA is used to provide high-quality, temporally balanced literature to the LLM for content generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3-haiku-20240307, GPT-4o-mini-2024-07-18 (used in experiments for generation/decomposition); DeepSeek-v3 (for open-source experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (arXiv corpus used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Research DB: ≈600,000 research papers + 20,000 review articles; retrieval: 1,500 candidate papers for outline stage, 60 relevant papers per chapter-writing stage (as reported in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Retrieval-augmented pipeline: RAG for initial retrieval, LLM-assisted query decomposition using memory M_i to produce sub-queries q_ijk, embedding-similarity retrieval from memory, then temporal-aware reranking that groups by 2-year publication bins and selects top-cited documents per bin. Extraction focuses on selecting relevant papers and extracting/supporting text for survey sections rather than parsing explicit quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluated within SURVEYFORGE using SurveyBench and SAM metrics (SAM-R for reference coverage, SAM-O for outline, SAM-C for content) and compared against AutoSurvey; ablation studies on memory and reranking components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Input Coverage and Reference Coverage (e.g., Input Cov. improved from 0.12→0.22 for Claude-3-Haiku; Reference Cov. improved from 0.23→0.40), Outline Quality (SAM-O scores reported, e.g., 86.67 in ablation), Content Quality sub-scores (structure/relevance/coverage) and aggregated SAM_avg_C; cost/time metrics (≈64k-token survey generation cost <$0.50, ~10 minutes).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Designed for retrieving and assembling literature-informed content, but not specialized for extracting quantitative laws; limitations include hallucinated citations/content, limited ability to synthesize deep cross-paper quantitative relationships, and inability to provide original forward-looking scientific insight at human expert level.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against AutoSurvey (Wang et al., 2024c); SURVEYFORGE (with SANA) improved Input Coverage, Reference Coverage, Outline and Content quality metrics across tested LLMs (Claude-3-Haiku, GPT-4o-mini, and DeepSeek-v3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. <em>(Rating: 2)</em></li>
                <li>Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning. <em>(Rating: 2)</em></li>
                <li>Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>gpt-researcher <em>(Rating: 1)</em></li>
                <li>Re2G: Retrieve, rerank, generate. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4173",
    "paper_id": "paper-276813240",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "Autonomous Scientific Discovery",
            "name_full": "Autonomous Scientific Discovery (research area)",
            "brief_description": "A research area referenced in the paper where LLMs and language agents are explored for automating aspects of scientific discovery, such as idea generation and data-driven hypothesis formation; mentioned as related work but not implemented in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Autonomous Scientific Discovery",
            "system_description": "A broad research direction (collection of methods and systems) investigating the use of LLMs and multi-agent language systems to discover, generate, or propose scientific hypotheses, insights, or patterns from literature and data. In this paper it is cited as background and motivation rather than instantiated.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multiple / general scientific domains",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Paper notes general limitations of LLMs in this area: difficulty synthesizing deep inter-publication relationships, lack of critical thinking/originality, hallucinations and inaccurate citations, and inability to reliably construct temporal methodological evolution or deep quantitative relationships.",
            "comparison_baseline": null,
            "uuid": "e4173.0",
            "source_info": {
                "paper_title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ScienceAgentBench",
            "name_full": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "brief_description": "A referenced benchmark intended to evaluate language agents for data-driven scientific discovery; mentioned in related work as an effort to assess agents that could perform scientific-discovery-like tasks on literature/data.",
            "citation_title": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "ScienceAgentBench",
            "system_description": "A benchmarking/assessment suite for evaluating language agents on tasks related to data-driven scientific discovery (mentioned in the paper's related work). It is referenced as an example of prior work on assessing agents for discovery, not used in SURVEYFORGE experiments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / data-driven scientific discovery",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": "Benchmark evaluation of language agents (as per the referenced paper); used to assess agent capabilities rather than reported results in this paper.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": null,
            "comparison_baseline": null,
            "uuid": "e4173.1",
            "source_info": {
                "paper_title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Sciagents",
            "name_full": "Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning",
            "brief_description": "A referenced system/work that proposes multi-agent graph reasoning to automate scientific-discovery-like workflows; cited as related work exploring automation of scientific discovery with AI.",
            "citation_title": "Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning.",
            "mention_or_use": "mention",
            "system_name": "Sciagents",
            "system_description": "A multi-agent approach relying on intelligent graph reasoning to automate aspects of scientific discovery (cited in the paper's related work). The SURVEYFORGE paper references it as part of the landscape of efforts to apply LLMs/agents to discovery.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / computational scientific discovery",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Reportedly uses graph-based reasoning across entities/papers (as described by the referenced title); no experimental detail provided in SURVEYFORGE.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": null,
            "comparison_baseline": null,
            "uuid": "e4173.2",
            "source_info": {
                "paper_title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "COI-Agent",
            "name_full": "COI-Agent (as cited)",
            "brief_description": "A cited system (Li et al., 2024) referenced for leveraging LLMs to generate novel scientific ideas; mentioned in the related work on autonomous idea generation rather than used or evaluated in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "COI-Agent",
            "system_description": "Mentioned as an example of prior work that uses content models/agents to propose or select scientific ideas; the SURVEYFORGE paper refers to COI-Agent in the context of LLM-powered idea generation but gives no implementation details.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific idea generation",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited works in the area often produce outputs lacking deep coherence or critical synthesis; SURVEYFORGE emphasizes these shortcomings as motivation.",
            "comparison_baseline": null,
            "uuid": "e4173.3",
            "source_info": {
                "paper_title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SANA",
            "name_full": "Scholar Navigation Agent (SANA)",
            "brief_description": "The memory-driven scholar navigation agent introduced and used in this paper (part of SURVEYFORGE) to decompose sub-queries, retrieve context-aware literature from a large research database, and rerank results using temporal-aware criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SANA (Scholar Navigation Agent)",
            "system_description": "A pipeline component in SURVEYFORGE consisting of: Memory for Sub-query (MS) which stores retrieved literature as memory to guide LLM-based query decomposition; Memory for Retrieval (MR) which uses embedding similarity against the memory to retrieve candidate papers for subsections; and a Temporal-aware Reranking Engine (TRE) that re-ranks by publication time groups and citation impact to select top-k per time bin. SANA is used to provide high-quality, temporally balanced literature to the LLM for content generation.",
            "model_name": "Claude-3-haiku-20240307, GPT-4o-mini-2024-07-18 (used in experiments for generation/decomposition); DeepSeek-v3 (for open-source experiments)",
            "model_size": null,
            "scientific_domain": "computer science (arXiv corpus used in experiments)",
            "number_of_papers": "Research DB: ≈600,000 research papers + 20,000 review articles; retrieval: 1,500 candidate papers for outline stage, 60 relevant papers per chapter-writing stage (as reported in experiments).",
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Retrieval-augmented pipeline: RAG for initial retrieval, LLM-assisted query decomposition using memory M_i to produce sub-queries q_ijk, embedding-similarity retrieval from memory, then temporal-aware reranking that groups by 2-year publication bins and selects top-cited documents per bin. Extraction focuses on selecting relevant papers and extracting/supporting text for survey sections rather than parsing explicit quantitative laws.",
            "validation_approach": "Evaluated within SURVEYFORGE using SurveyBench and SAM metrics (SAM-R for reference coverage, SAM-O for outline, SAM-C for content) and compared against AutoSurvey; ablation studies on memory and reranking components.",
            "performance_metrics": "Input Coverage and Reference Coverage (e.g., Input Cov. improved from 0.12→0.22 for Claude-3-Haiku; Reference Cov. improved from 0.23→0.40), Outline Quality (SAM-O scores reported, e.g., 86.67 in ablation), Content Quality sub-scores (structure/relevance/coverage) and aggregated SAM_avg_C; cost/time metrics (≈64k-token survey generation cost &lt;$0.50, ~10 minutes).",
            "success_rate": null,
            "challenges_limitations": "Designed for retrieving and assembling literature-informed content, but not specialized for extracting quantitative laws; limitations include hallucinated citations/content, limited ability to synthesize deep cross-paper quantitative relationships, and inability to provide original forward-looking scientific insight at human expert level.",
            "comparison_baseline": "Compared against AutoSurvey (Wang et al., 2024c); SURVEYFORGE (with SANA) improved Input Coverage, Reference Coverage, Outline and Content quality metrics across tested LLMs (Claude-3-Haiku, GPT-4o-mini, and DeepSeek-v3).",
            "uuid": "e4173.4",
            "source_info": {
                "paper_title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery.",
            "rating": 2,
            "sanitized_title": "scienceagentbench_toward_rigorous_assessment_of_language_agents_for_datadriven_scientific_discovery"
        },
        {
            "paper_title": "Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning.",
            "rating": 2,
            "sanitized_title": "sciagents_automating_scientific_discovery_through_multiagent_intelligent_graph_reasoning"
        },
        {
            "paper_title": "Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2,
            "sanitized_title": "scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "gpt-researcher",
            "rating": 1,
            "sanitized_title": "gptresearcher"
        },
        {
            "paper_title": "Re2G: Retrieve, rerank, generate.",
            "rating": 1,
            "sanitized_title": "re2g_retrieve_rerank_generate"
        }
    ],
    "cost": 0.013746,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SURVEYFORGE: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing
6 Mar 2025</p>
<p>Xiangchao Yan 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Core Contributor</p>
<p>Shiyang Feng 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Core Contributor</p>
<p>Jiakang Yuan 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Renqiu Xia 
Bin Wang 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Bo Zhang zhangbo@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Lei Bai bailei@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory
Fudan University ♢ Shanghai Jiao Tong University</p>
<p>Patrick Lewis 
Ethan Perez 
Aleksandra Piktus 
Fabio Petroni 
Vladimir Karpukhin 
Naman Goyal 
Chris Lu 
Cong Lu 
Robert Tjarko Lange 
Jakob Fo- Erster 
Jeff Clune 
David 2024 Ha 
A I The 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Eric Hambro 
Chao Xu 
Xiaomeng Zhao 
Linke Ouyang 
Fan Wu 
Zhiyuan Zhao 
Rui Xu 
Kaiwen Liu 
Yuan Qu 
Fukai Shang 
Wenxiao Wang 
Lihui Gu 
Liye Zhang 
Yunxiang Luo 
Yi Dai 
Chen Shen 
Liang Xie 
Binbin Lin 
Yidong Wang 
Qi Guo 
Wenjin Yao 
Hongbo Zhang 
Xin Zhang 
Zhen Wu 
Meishan Zhang 
Xinyu Dai 
Min Zhang 
Qingsong Wen 
Song Mao 
Hongbin Zhou 
Haoyang Peng 
Jiahao Pi 
Daocheng Fu 
Hancheng Ye 
Qi Liu 
Zijun Chen 
Min Dou 
Botian Shi 
Junchi Yan 
Shitao Xiao 
Zheng Liu 
Peitian Zhang 
Tao Chen 
Wanli Ouyang 
Yu Qiao </p>
<p>Scientist: Towards Fully Automated Open-Ended Scientific Discovery . ArXiv
Xinzhe Zheng
Haoyang Su, Shixiang Tang2408.06292Renqi Chen</p>
<p>Jingzhe Li
Zhenfei YinWanli Ouyang</p>
<p>Wenjie Wu
Hancheng Ye, et al. 2024a</p>
<p>SURVEYFORGE: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing
6 Mar 2025B290B6D630FCB3C18543B47C7CF65983arXiv:2503.04629v1[cs.CL]Related Survey
Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications.Recently, researchers have begun using LLMs to automate survey generation for better efficiency.However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy.To close these gaps, we introduce SURVEYFORGE, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles.Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SURVEYFORGE can automatically generate and refine the content of the generated article.Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality.Experiments demonstrate that SURVEYFORGE can outperform previous works such as AutoSurvey.</p>
<p>Introduction</p>
<p>With the rapid development of science and technology, the number of published research articles has been growing exponentially, particularly in fastevolving fields like Artificial Intelligence (AI).The rapid growth of the literature makes it increasingly difficult for researchers to gain in-depth knowledge of a specific scientific field.Survey papers, which systematically integrate existing studies and provide comprehensive developments and trends in the specific domain, have become a vital starting point of the scientific research cycle.However, traditional human-driven survey writing requires researchers to review a vast number of articles which Figure 1: Compared to human-written surveys, AIgenerated surveys face two primary challenges.First, regarding the outline, these papers may often lack coherent logic and well-structured organization.Second, with respect to references, they frequently fail to include truly relevant and influential literature.</p>
<p>is time-consuming and makes it challenging to keep up-to-date with the latest advancements in the field.</p>
<p>Inspired by the remarkable advancement and capabilities of Large Language Models (LLMs) (Achiam et al., 2023;Anthropic, 2024;Touvron et al., 2023;Cai et al., 2024), researchers have begun utilizing them to automatically review the literature and generate survey papers.As a pioneer, GPT-Researcher (Assafelovic, 2023) generates survey papers based on the abstract of topicrelevant articles retrieved from multiple online academic databases.To identify more relevant literature to the survey topic, AutoSurvey (Wang et al., 2024c) constructs a local literature database based on arXiv, establishes vector indices for each literature, and concurrently generates content for each subsection.To further align the writing style of LLM-generated content with that of humans, OpenScholar (Asai et al., 2024) proposes a largescale scientific literature dataset, and fine-tunes the LLMs based on this dataset to obtain a model specifically designed for answering scientific questions.</p>
<p>Most of these automated survey generation methods follow the traditional academic survey writing workflow: from literature search, to outline drafting, and finally academic writing, as illustrated in Fig. 1.However, despite the promising achievements of the aforementioned methods, several significant challenges still remain.Firstly, the structure of AI-generated surveys often lacks coherent logic and is often poorly-organized.For example, as shown in Fig. 1, existing works may suffer from structural imbalance in both width and depth, such as overly detailed sectioning or inadequate coverage of key topics.Secondly, AI-generated surveys often fail to reference key influential literature, reducing the overall depth and value of surveys.As shown in Fig. 1, they may cite irrelevant works while overlooking important contributions in the field.Lastly, the evaluation of AI-generated surveys mainly relies on LLMs, focusing on the overall quality of the long-form content.This approach lacks fine-grained analysis of critical aspects such as outline quality, reference relevance, and structural coherence.Moreover, the absence of objective evaluation criteria makes it difficult to establish consistent quality benchmarks or compare different methods effectively.</p>
<p>To address the aforementioned challenges, we propose an automated framework for generating survey papers, namely SURVEYFORGE which contains two stages: Outline Generation and Content Generation.In the first stage, SURVEYFORGE employs a heuristic learning approach to leverage topic-relevant literature and structural patterns from human-written surveys, generating semantically comprehensive and well-organized outlines.In the second stage, a memory-driven scholar navigation agent, with a temporal-aware reranking engine, retrieves high-quality literature for each subsection.Then, the content for each section is combined and refined into a coherent and comprehensive survey.Furthermore, we construct SurveyBench, a multidimensional benchmark to facilitate systematic assessment of automated survey generation systems.</p>
<p>Extensive results highlight the unique strengths of SURVEYFORGE across multiple dimensions, including its ability to generate well-structured outlines, retrieve high-quality and highly relevant references, and produce coherent, comprehensive content.SURVEYFORGE not only delivers measurable improvements in these areas but also demonstrates a remarkable ability to bridge the gap between AIgenerated and human-written surveys.These findings underscore its potential as a robust framework for automated survey generation, setting a new standard for quality and reliability in this domain.</p>
<p>Our contribution can be summarized as follows.</p>
<p>• We propose SURVEYFORGE, a novel automated framework for generating high-quality academic survey papers.• We propose a heuristic outline generation method and a memory-driven scholar navigation agent, which together ensure a wellstructured survey framework and high-quality content generation.• To facilitate objective evaluation, we establish SurveyBench, a comprehensive benchmark featuring quantifiable metrics for assessing outline quality, reference quality, and content quality.</p>
<p>Related Work</p>
<p>Autonomous Scientific Discovery.With the advancement of LLMs (Achiam et al., 2023;Anthropic, 2024;Chen et al., 2024a), an increasing number of researchers have begun exploring their potential for autonomous scientific discovery (Xia et al., 2024b;Li et al., 2024;Xia et al., 2024a;Huang et al., 2024;Ghafarollahi and Buehler, 2024;Chen et al., 2024b).Several studies (Li et al., 2024;Hu et al., 2024a;Kumar et al., 2024;Wang et al., 2024b;Su et al., 2024) have focused on leveraging LLMs for novel scientific idea generation.For instance, COI-Agent (Li et al., 2024)   (Hoang and Kan, 2010;Hu and Wan, 2014;Jha et al., 2015;Chen and Zhuge, 2019) primarily rely on content models to select and organize sentences from papers, often resulting in outputs lacking coherence and readability.Sun et al. (Sun and Zhuge, 2019) introduce a template tree that generates content recursively based on nodes, which improves coherence but remains inflexible.Recognizing the need for more flexible and coherent solutions, the emergence of LLMs has introduced new opportunities for enhancing the automated survey generation.Researchers have begun to leverage LLMs to facilitate efficient literature comprehension and review (Wang et al., 2024c;Hu et al., 2024b).Zhu et al. (Zhu et al., 2023) introduce a novel task of hierarchical catalogue generation for surveys, along with corresponding semantic and structural metrics for evaluation, but it is limited to outline generation with fixed reference papers.AutoSurvey (Wang et al., 2024c) proposes a two-stage LLM-based method for survey generation but fails to focus on the analysis of human academic writing styles and key references, which are crucial for producing high-quality surveys.Subsequently, HiReview (Hu et al., 2024b) introduces a taxonomy-driven framework to explore paper relationships hierarchically, enhancing LLMs' understanding of inter-paper connections.However, relying on 2-hop citation networks from existing surveys instead of commonlycited papers limits its broader applicability.</p>
<p>Method</p>
<p>In this section, we propose SURVEYFORGE, a novel framework based on LLMs for automatically retrieving relevant literature and generating comprehensive survey papers.As shown in Fig. 2, our framework consists of two main stages: outline generation stage and content writing stage.The outline generation stage leverages both research papers and existing survey structures through a heuristic learning mechanism, producing academically structured outlines.The content generation stage employs a memory-driven scholar navigation agent with key paper retrieval strategy to synthesize the content of the survey.Finally, we propose a benchmark Sur-veyBench for automated survey generation tasks.The details are elaborated in Sec.3.1, Sec.3.2 and Sec.3.3, respectively.</p>
<p>Heuristic Outline Generation</p>
<p>The outline of a survey paper is crucial as it defines the logical organization and knowledge structure of the entire work.While LLMs excel at generating textual content, they often fall short in crafting well-structured survey outlines.Common issues include a lack of hierarchical depth, insufficient theoretical grounding, and a tendency toward report-like structures rather than scholarly frameworks.These limitations can be attributed to the limited understanding of academic writing conventions and the organizational principles underlying survey design.To address these challenges, we propose a top-down heuristic learning approach, enabling LLMs to understand the established theoretical frameworks and organizational paradigms from human-written survey outlines.Our approach is underpinned by two domain-specific knowledge bases: a Research Paper Database, which encodes domain knowledge, and a Survey Outline Database, which captures established structural patterns (details provided in Appendix.A.1).As shown in Algorithm 1, the framework begins with crossdatabase knowledge fusion, retrieving relevant papers and outlines for the given topic T from D R and D S .This process identifies key thematic areas and their interrelations, generating the first-level outline O i augmented with semantic queries Q i that specify the scope and focus of each heading.For each section O i , we recursively retrieves relevant materials (P R i , P S i ) and generates second-level outlines O ij with sub-queries q ij .Finally, these headings and their associated queries are systematically merged to construct a academically rigorous and comprehensive survey outline, serving as a foundation for subsequent content generation.</p>
<p>Memory-Driven Content Generation</p>
<p>The memory-driven content generation stage consists of two primary steps: literature retrieval and parallel content creation.These steps are performed sequentially by the proposed Scholar NAvigation Agent (SANA) and the LLM, respectively.A detailed explanation of each step is provided below.</p>
<p>SANA: Scholar Navigation Agent</p>
<p>To ensure that the quality and quantity of references in the generated survey papers, we propose a Scholar Navigation Agent (SANA), equipped with memory and reranking capabilities, designed to facilitate literature retrieval across various generation stages.The SANA includes three modules: Memory for Sub-query (MS), Memory for Retrieval (MR), Temporal-aware Reranking Engine (TRE).Memory for Sub-query.Query decomposition is a common technique that involves breaking down a complex query into smaller sub-queries, thereby enabling more precise information retrieval.Existing query decomposition methods (Fan et al., 2024) are mostly achieved through naive prompts and LLMs.However, such methods require meticulous tuning of prompts to accommodate different tasks and may cause significant semantic differences between the decomposed sub-queries and the original query, which could potentially degrade the quality of the references in the AI-generated surveys.Therefore, we incorporate the memory mechanism into the query decomposition process of SANA to enhance the effectiveness of sub-queries.Specifically, as described in Sec.3.1, when generating the first-level outline O i , a set of literature P R i is retrieved by Retrieval-Augmented Generation (RAG).In the MS module, SANA takes the literature P R i as memory M i , the original query consists of the titles t O ij and descriptions d O ij of each subsection:
q ij = [d O ij , t O ij ].(1)
To achieve query decomposition, q ij and M i are used together as part of the instruction to prompt the LLM to decompose q ij into multiple subqueries q ijk :
q ijk = LLM(q ij , M i ).(2)
Finally, the sub-query q ijk is used in the subsequent MR module to retrieve literature related to the subsection O ij .Memory for Retrieval.The effectiveness of content generation heavily depends on the quality of retrieved information.Traditional retrieval methods (Lewis et al., 2020;Gao et al., 2023), which typically query the entire literature database D R , are often inefficient and lack contextual focus, particularly in generating complex, multi-section documents.These methods treat each section as an isolated unit, failing to account for the global structure and thematic coherence of the document.This results in redundant or irrelevant retrievals and limits the overall coherence of generated content.</p>
<p>To address these limitations, we incorporate the memory mechanism into the retrieval process of SANA to bridge the gap between the outline and content generation stages.Specifically, in the MS module, SANA takes the literature P R related to the entire outline as memory M .Based on the embedding similarity between each sub-query q ijk and the literature in M , the most relevant literature L ijk for each sub-query of section O ij is retrieved.Subsequently, the retrieved literature L ijk is reranked and selected within the following TRE module for content generation.</p>
<p>Temporal-aware Reranking Engine.Reranking plays a important role in enhancing the quality and relevance of retrieved information.Existing methods (Glass et al., 2022;Xiao et al., 2023) typically employ advanced scoring mechanisms to measure textual relevance between queries and documents.However, these surface-level semantic matching may fall short in capturing the academic impact and quality of publications.Besides, The publication date of a paper plays a critical role in determining its influence and significance within its respective field.Consequently, analyzing papers from different time periods within the same research domain is a crucial for identifying high-quality contributions in the research field.For papers published within the same time period, there are various metrics to indicate their impact and quality, such as citation count, Essential Science Indicators (ESI), etc (Clarivate, 2024).Among these, citation count serves as a complementary quality indicator that reflects the scholarly influence and recognition of research works.To address both the limitations of pure semantic matching and the temporal bias in different quality indicators, we propose a temporalaware reranking engine that integrates textual relevance, citation impact, and publication recency.This approach ensures not only the topical relevance but also the academic quality of the retrieved literature while maintaining a balanced representation of both established and emerging research.Specifically, the retrieved literature L ijk based on embedding similarity is categorized into multiple groups L ijk = {n g } G g=1 according to their publication dates, with each group spanning a period of two years.For each group g, the highly cited literature is retained in a top-k manner as the final output for SANA, and the number of literature to be retained for each group is:
k g = |n g | |L ijk | K O ij ,(3)
where K O ij is a hyper-parameter that represents the number of literature utilized for generating the content of each subsection.</p>
<p>Parallel Generation and Refinement</p>
<p>Due to the constraints of maximum context length and inference speed of LLMs, the content of each section is generated in parallel to reduce the generation time and ensure the length of the generated survey.However, due to the independent generation processes of each section in parallel, there may be repetition or redundancy among the contents of different section.Therefore, we employ LLMs to implement the refinement stage, which is aimed at refining the raw survey obtained by concatenating the contents of each section generated in parallel.</p>
<p>Multi-dimensional Evaluation Benchmark</p>
<p>Evaluating AI-generated surveys is challenging due to the lack of standardized benchmarks.Existing methods largely rely on automated scoring by LLMs, which face limitations: they may not adequately assess key literature coverage and depend heavily on internal model judgments without objective metrics.To address these challenges, we introduce SurveyBench, a comprehensive evaluation benchmark, along with SAM (Survey Assessment Metrics), a multi-dimensional evaluation series.SurveyBench consists of approximately 100 human-written survey papers across 10 distinct topics, carefully curated by doctoral-level researchers to ensure thematic consistency and academic rigor.</p>
<p>For each topic t i , we selected one highest-quality survey S * i as the reference for comparison with AI-generated surveys Ŝi .Details of the benchmark construction process are provided in Appendix.A.2.The SAM series integrate objective metrics, expert knowledge, and multi-dimensional criteria through three core components: SAM-R: Reference Quality Evaluation.A comprehensive and relevant bibliography is essential for a well-researched survey.Based on Survey-Bench, we extract a reference set R i for each topic t i , serving as a reliable benchmark representing foundational knowledge in the field.</p>
<p>To measure reference quality, we define the SAM R metric, which quantifies the overlap between the references in the AI-generated survey Ŝi and R i :
SAM R ( Ŝi ) = |R Ŝi ∩ R i | |R Ŝi | ,(4)
where R Ŝi is the set of references in Ŝi .A higher rate indicates better coverage of key literature in the topic t i .SAM-O: Outline Quality Evaluation.This component evaluates the structural quality of AIgenerated surveys.A well-structured and logically coherent outline is crucial for content organization and readability.We assess the outline using a single comprehensive score SAM O , ranging from 0 to 100, where higher scores indicate better quality.</p>
<p>The evaluation is conducted by LLMs following detailed criteria described in Appendix.A.9. SAM-C: Content Quality Evaluation.The final component measures the generated survey's quality across three dimensions: structure (SAM struct</p>
<p>C</p>
<p>), relevance (SAM rel C ), and coverage (SAM cov C ).Using the high-quality survey S * i as reference, we compute avg score of the overall content :
SAM avg C = SAM struct C + SAM rel C + SAM cov C 3 .
(5) Scores range from 0 to 100, with higher values indicating better performance.The LLMs assess these criteria while referencing S * i to ensure alignment with expert-level standards.</p>
<p>Experiment</p>
<p>Experimental Settings</p>
<p>Evaluation Dataset.To assess the performance of our proposed approach, we construct a dedicated benchmark dataset within the Computer Science (CS) domain, based on the arXiv repository.As mentioned in Sec.3.3, we manually select approximately 100 human-written survey papers across 10 distinct topics, and choose one highest-quality survey for direct comparison with AI-generated surveys for each topic.Implementation Details.To establish a baseline for comparison, we adopt AutoSurvey (Wang et al., 2024c), a state-of-the-art system for automated survey generation.Furthermore, we collect a largescale dataset from the CS scientific field of arXiv, consisting of approximately 600,000 research papers and 20,000 review articles.We extract the key metadata to construct a retrieval vector database, including titles, abstracts of all papers and outlines of the review articles.To ensure a fair comparison, we align the timeline of our retrieval database with that of AutoSurvey.During the experimental evaluation, we retrieve 1,500 candidate papers for the outline generation stage and 60 relevant papers for each chapter-writing stage, following the same experimental settings as AutoSurvey.</p>
<p>For survey generation, we employ two LLMs independently: Claude-3-haiku-20240307 and GPT-4o-mini-2024-07-18.Each model generates surveys for 10 predefined topics, with 10 independent trials conducted for each topic, resulting in a total of 100 outputs per model.The average performance across these trials is calculated to ensure stable and reliable results.In addition to the closed-source models, we have also experimented with the open source model with Deepseek-v3 (Liu et al., 2024), with impressive results, as detailed in Appendix A.5.For evaluation, we leverage more advanced models, GPT-4o-2024-08-06 and Claude-3.5-sonnet-20241022,to assess both the AI-generated outlines and the content of the surveys, ensuring a robust and reliable evaluation of their quality.</p>
<p>Main Results</p>
<p>As shown in Table 1, we evaluate the performance of SURVEYFORGE across various dimensions, including reference quality, outline quality, and content quality, comparing it against the baseline Au-toSurvey.The results demonstrate that SURVEY-FORGE achieves significant improvements in all aspects, showcasing its potential as an advanced automated survey generation framework.Additionally, we conduct a cost analysis of the SUR-VEYFORGE framework, demonstrating that generating a 64k-token overview requires less than $0.50, with detailed cost breakdowns provided in Appendix A.6. Results on Reference Quality.In terms of reference quality, SURVEYFORGE outperforms Auto-Survey on both key metrics: Input Coverage, which measures the relevance of retrieved papers, and Reference Coverage, which evaluates the alignment of the references of surveys with expert-curated benchmarks.Specifically, the Input Coverage score improves from 0.12 to 0.22 when using Claude-3-Haiku and from 0.07 to 0.20 with GPT-4o mini.Similarly, the Reference Coverage score increases from 0.23 to 0.40 and from 0.20 to 0.42 for the two respective models, indicating that SURVEYFORGE retrieves and generates references that are not only   more relevant but also more aligned with expert expectations.Notably, high-quality human-written surveys achieve a Reference Coverage score of 0.63, which further validates the reliability of our proposed reference evaluation database, which provides a robust benchmark for reference quality.</p>
<p>Results on Outline Quality.For outline quality, the results show that SURVEYFORGE generates outlines that are more logical, comprehensive, and closer to human-level performance compared to AutoSurvey (Wang et al., 2024c)  As shown in Fig. 3, SURVEYFORGE demonstrates substantial improvements over the baseline AutoSurvey across all key evaluation metrics.</p>
<p>Although not yet matching the quality of expertcrafted surveys, SURVEYFORGE significantly narrows the gap, highlighting its potential as a powerful tool for automated survey generation.</p>
<p>Comparison with Human Evaluation</p>
<p>To validate our automated evaluation system, we compare its performance with expert assess- As shown in Table 2, for outline quality, the automated system achieves a Score Win Rate of 73.00% and a Comparative Win Rate of 75.00%, closely matching the human evaluation rate of 74.00%.This consistency confirms the system's robust scoring logic.For content quality, the automated system's Score Win Rate for SURVEY-FORGE is 69.00%, aligning closely with the human expert rate of 70.00%.In addition, we also conduct Cohen's kappa coefficient consistency experiment, which shows a strong agreement between automated systems and human assessments, as detailed in Appendix A.4.</p>
<p>In summary, the automated system aligns well with human assessments for both outline and content quality, validating its effectiveness as a reliable alternative to manual evaluation.</p>
<p>Ablation Study</p>
<p>To better understand the contribution of individual components in our proposed SURVEYFORGE framework, we conduct a comprehensive ablation study.For ablation experiments, we use Claude-3haiku-20240307 to generate surveys on the same 10 topics, with 3 independent trials per topic to ensure statistical reliability while maintaining computational efficiency.Specifically, we analyze the memory mechanism, sub-query decomposition, and reranking strategies in the scholar navigation agent module, as well as the impact of the use of the database of survey outlines in the outline generation process.The results of the ablation experiments are presented in Table 3 and Table 4. Analysis on Outline Generation.Table 3 highlights the impact of heuristic learning approach on outline quality.The baseline method, which generates outlines solely from retrieved research papers without structural guidance, achieves a score of 81.78.This indicates the absence of organizational cues limits the coherence and logical flow of the outlines.To address this, we first introduce a heuristic approach using outlines from random surveys.These generic outlines, representing common patterns in survey writing, improve the score to 84.58.This shows the effectiveness of structural cues, even without target-domain tailoring.Finally, we retrieve domain-specific outlines, providing both structural guidance and thematic alignment with the target domain.As a result, the outline quality score significantly rises to 86.67, showing the crucial role of domain-specific structural cues in creating coherent and relevant outlines.Analysis on Content Generation.Based on the experimental results presented in Table 4, it can be observed that as the quality of literature obtained by SANA improves, the quality of cited references in surveys also correspondingly enhances.This observation highlights the importance of using SANA during the content generation stage to retrieve highquality literature.Specifically, the integration of a memory mechanism into the query decomposition and retrieval processes significantly enhance the quality of literature.This improvement can be attributed to the incorporation of more comprehensive sub-query semantics and a retrieval scope better aligned with the sub-queries.Besides, the temporal-aware reranking engine ensures the selection of high-quality papers, leading to a more comprehensive and balanced reference collection.</p>
<p>Conclusion and Outlook</p>
<p>We have introduced SURVEYFORGE, an automated framework leveraging a heuristic outline generation and a memory-driven content generation to generate high-quality surveys.We introduce a multidimensional evaluation benchmark to comprehensively assess the quality of surveys.SURVEY-FORGE significantly outperforms prior approaches across multiple evaluation metrics.We hope to reduce the learning curve for researchers venturing into unfamiliar fields, providing convenience and thereby promoting the integration and development of cross-disciplinary and cross-domain knowledge.</p>
<p>Limitations</p>
<p>Despite its strong performance in generating structured and high-quality surveys, SURVEYFORGE has inherent limitations, as discussed in Appendix A.3.While LLMs excel at summarizing existing literature, they face challenges in analyzing and synthesizing relationships across multiple sources, often lacking the critical thinking and originality characteristic of human-authored work, which limits their capability to reflect research trends or provide forward-looking insights.Besides, the accuracy of content and citations is also affected by the hallucination of LLMs.Future work could focus on developing methods to better capture interconnections among references to enhance the logical coherence, depth, and scholarly value of the generated content.</p>
<p>Ethics Statement</p>
<p>This work focuses on the development of an automated framework for survey generation, aiming to assist researchers in efficiently summarizing existing literature.The proposed method relies on publicly available datasets and research papers, ensuring compliance with copyright and intellectual property laws.While the framework is designed to augment human expertise, we encourage users to critically evaluate the generated outputs to ensure their alignment with ethical research practices and to mitigate any potential limitations, such as biases or incomplete summaries.</p>
<p>A Appendix</p>
<p>Due to the page limitation of the manuscript, we provide more details and visualizations from the following aspects:</p>
<p>• Sec.A.1: Database Construction.</p>
<p>• Sec.A.2: Details of SurveyBench.</p>
<p>• Sec.A.3: Discussion about Generated Surveys and Human-written Surveys.</p>
<p>• Sec.A.4: Details of Human Evaluation and Inter-rater Agreement.</p>
<p>• Sec.A.5: Additional Experiments with Open-Source Models.</p>
<p>• Sec.A.6: Details of Time and Economic Cost.</p>
<p>• Sec.Qualitative Results.</p>
<p>• Sec.A.8: Example of Generated Survey.</p>
<p>• Sec.A.9: Prompt Used.</p>
<p>A.1 Database Construction</p>
<p>To ensure the quality and relevance of the AIgenerated surveys, we construct two key databases: the Research Paper Database and the Survey Outline Database, consisting of approximately 600,000 research papers and 20,000 review articles, which together serve as the foundation for content generation and structural guidance.The Research Paper Database comprises the titles and abstracts of research papers relevant to the survey topic, while the Survey Outline Database contains titles, abstracts, and outlines extracted from published survey papers.Specifically, we utilize MinerU (Wang et al., 2024a) to extract content from a corpus of survey articles.Using rule-based extraction techniques, we isolate hierarchical outlines, including section and subsection headings.However, due to variations in formatting and structure across different papers, automatic extraction may introduce noise.To address this, we employ Claude-3.5-sonnet-20241022to refine and standardize the extracted outlines, ensuring consistency in structure and formatting.By leveraging the Survey Outline Database in this way, we provide the LLM with high-quality, expertcrafted outline examples to guide its generation process.</p>
<p>Additionally, we encode these documents using the gte-large-en-v1.5 embedding model (Li et al., 2023), which captures semantic relationships and enables efficient similarity-based retrieval.This combination of structured expert examples and semantic encoding ensures a robust foundation for outline generation and content retrieval.</p>
<p>A.2 Details of SurveyBench</p>
<p>To construct SurveyBench, we select 10 trending topics in the computer science domain, as shown in Table 5.These topics span various cutting-edge areas including multimodal learning, language models, computer vision, and autonomous systems.For each topic, a set of high-quality, human-written surveys is carefully curated by a panel of 20 researchers.Each of these researchers holds doctoral degrees and possesses extensive expertise in the aforementioned 10 trending topics in the computer science domain.This rigorous selection process ensures strong thematic alignment and guarantees the inclusion of authoritative and relevant surveys.Besides, the development of our assessment metrics (e.g.SAM-O and SAM-C) is inspired by peer review guidelines from top-tier computer science venues.However, we observed that traditional review criteria often rely heavily on reviewers' implicit knowledge and experience, making them challenging to implement in automated evaluation systems.To address this limitation, we systematically decomposed these high-level review guidelines into more specific, measurable components that can be reliably assessed by LLMs while maintaining consistency with expert human evaluation.For example, in our outline assessment criteria, abstract concepts like "topic organization" were broken down into concrete, assessable elements such as "topic uniqueness" (checking for duplicate topics, content overlap) and "structural balance" (examining section development and proportionality).This granular approach, developed through discussions with researchers who have at least two years of reviewing experience for top CS venues, enables more consistent and reliable automated evaluation across different survey topics while preserving the essential quality standards of academic peer review.</p>
<p>The curated surveys, predominantly published within the last two years, are chosen to ensure both timeliness and relevance.From each selected survey, we extract the references cited to construct a dedicated reference database for each topic, resulting in comprehensive reference collections ranging Table 5: Overview of selected topics and the representative surveys in our evaluation benchmark.For each topic, we show the total number of unique references (Ref Num) collected from SurveyBench, and the citation count of selected high-quality surveys that serve as our evaluation references.</p>
<p>from 330 to 994 references per topic, as detailed in Table 5.Furthermore, to facilitate robust content evaluation,we identify the highest-quality survey for each topic to serve as the evaluation reference, with these selected surveys demonstrating significant impact through their citation counts (ranging from 128 to 1,690 citations).SurveyBench provides a comprehensive and reliable foundation for assessing the quality of AI-generated surveys, ensuring both reference coverage and content relevance are rigorously evaluated.</p>
<p>A.3 Discussion about Generated Surveys and Human-written Surveys</p>
<p>While our extensive evaluation of SURVEYFORGE demonstrates its effectiveness in automated survey generation, our analysis reveals several fundamental challenges that warrant further investigation.Through systematic examination of the generated surveys, we identify two primary limitations of the current system.The first limitation lies in the depth of academic analysis.Although the system effectively extracts and organizes information from individual papers, it exhibits constraints in establishing profound connections across multiple publications.Specifically, the system's capability falls short in comparative analysis of temporal innovations and methodological evolution patterns, often defaulting to mechanical reference listing rather than providing the nuanced synthesis characteristic of expert-written surveys.This limitation stems primarily from challenges in the accurate identification of the core literature and the construction of deep logical relationships during the processing of long-form knowledge.</p>
<p>The second challenge concerns the accuracy of content and citation.Despite our implementation of multiple verification mechanisms, the system occa- sionally produces inaccurate citations or academic claims, potentially affecting the survey's reliability.This remains a critical area for improvement in automated survey generation systems.</p>
<p>To address these limitations, future work could focus on developing comprehensive knowledge association networks through core entity extraction and citation graph construction, which may enhance the system's capability to identify deep inter-publication connections.</p>
<p>A.4 Details of Human Evaluation and</p>
<p>Inter-rater Agreement</p>
<p>For the human evaluation across the selected 10 topics, we recruited 20 PhD experts in computer science from various prestigious institutions, including several QS Top 50 universities and renowned research institutes within our country.The selection of these experts followed strict criteria to ensure their expertise and qualifications.All evaluators hold PhD degrees in computer science or closely related fields, and each expert has published at least one peer-reviewed paper in the specific topic they were assigned to evaluate.Moreover, all selected experts are currently active researchers in their respective fields.</p>
<p>To maintain evaluation quality and consistency, each expert was provided with a comprehensive evaluation guideline manual, identical to the one used in our LLM evaluation system, ensuring consistent assessment criteria across all evaluators.Before the formal evaluation, we conducted a training session to familiarize the experts with the evaluation criteria and scoring rubrics.The evaluation process was conducted in a double-blind manner to minimize potential biases.Regarding compensation, experts were paid $50 per hour, commensurate with their expertise level.The average evaluation time per survey was approximately 1-3 hours, ensuring thorough and reliable assessment.</p>
<p>To further verify the reliability of the evaluation system, we further conducted Cohen's kappa coefficient experiment to measure the inter-rater agreement between automatic and human evaluations and evaluations inter-rater agreement among human annotators.Specifically, as shown in Table 6, we conducted a systematic evaluation of 100 generated survey papers across 10 different research topics.We used Cohen's kappa coefficient as our evaluation metric, covering two core dimensions: outline and content.</p>
<p>In the outline dimension, based on the evaluation of these 100 surveys, the kappa coefficient between LLM evaluation and human evaluation reached 0.7177, indicating significant agreement the two.Meanwhile, the cross-validation kappa coefficient between human evaluators was 0.7921.This high level of agreement not only validates the reliability of human evaluation but also supports the effectiveness of our automated evaluation method.</p>
<p>In the content dimension, based on the same sample size, the kappa coefficient between LLM evaluation and human evaluation was 0.6462, while the cross-validation kappa coefficient between human evaluators was 0.7098.These results demonstrate that even in the more complex task of evaluating extra-long text content, our evaluation framework still shows good consistency.</p>
<p>A.5 Additional Experiments with Open-Source Models</p>
<p>To validate the generalizability of our framework, we conduct additional experiments using DeepSeek-v3 (Liu et al., 2024), a state-of-the-art open-source language model.As shown in Table 7, the experimental results demonstrate remarkable performance across all evaluation metrics.Specifically, DeepSeek-v3 achieved an Input Coverage of 0.2554 and a Reference Coverage of 0.4553, surpassing other baseline models in literature coverage assessment.In the outline quality evaluation, DeepSeek-v3 attains a score of 87.42, which not only exceeds other models but also ap-proaches the benchmark set by human-written surveys (87.62).Furthermore, across the three dimensions of content quality structure, relevance, and coverage, DeepSeek-v3 demonstrates exceptional performance with scores of 79.20, 80.17, and 81.07 respectively, yielding a mean score of 80.15 that outperforms other comparative models.These empirical results not only corroborate the effectiveness of our methodology but also establish its applicability to open-source models.Notably, DeepSeek-v3 (Liu et al., 2024) exhibits superior performance at a lower operational cost ($0.37 per survey) compared to GPT-4o-mini ($0.43 per survey).Such advancement has substantial implications for the sustainable development of automated research tools and methodologies.</p>
<p>A.6 Details of Time and Economic Cost</p>
<p>The SURVEYFORGE framework generates comprehensive survey papers with approximately 64k tokens in length, comparable to human-written surveys.The generation process requires an average input of 2.37M tokens and produces 0.13M tokens of output.Taking GPT-4-mini-2024-07-18 as an example, the economic cost amounts to merely $0.43.Regarding the temporal efficiency, the entire framework completes the generation within approximately 10 minutes (note that the actual duration may vary depending on API rate limits).These metrics demonstrate that the SURVEYFORGE framework enables researchers to efficiently acquire domain knowledge at a remarkably low cost.</p>
<p>A.7 Qualitative Results</p>
<p>In this section, we present qualitative comparisons to demonstrate the effectiveness of our proposed framework in generating academically structured survey outlines.Specifically, we compare the outlines generated by our method with those produced by baseline approaches, as shown in Fig. 4.</p>
<p>The baseline outlines exhibit several notable issues.First, the logical organization of sections and subsections is often suboptimal, with limited hierarchical depth and coherence.Additionally, there is a tendency to treat individual studies or papers as standalone subsections, resulting in fragmented and overly granular structures.Furthermore, redundancy is frequently observed, with similar or overlapping topics appearing in multiple sections, which reduces clarity and disrupts the logical flow of the outline.In contrast, the outlines generated by our framework effectively address these issues.By leveraging a heuristic learning approach and incorporating domain-specific structural patterns, our method produces well-organized outlines that align with academic writing standards.The generated outlines demonstrate clear hierarchical organization, thematic coherence, and appropriate grouping of related topics, providing a solid foundation for comprehensive and logically structured surveys.</p>
<p>A.8 Example of Generated Survey</p>
<p>As shown in Fig. 5, we have provided the example of the generated survey by SUR-VEYFORGE, more complete examples can be found at https://anonymous.4open.science/r/survey_example-7C37/.Specifically, by observing the generated survey paper, we found that SURVEYFORGE is not only capable of summarizing knowledge within a specific academic field based on logical structures but also excels at providing insights and recommendations for some potential research directions.</p>
<p>For instance, in a survey paper generated by SURVEYFORGE titled "Comprehensive Survey on Multimodal Large Language Models: Advances, Challenges, and Future Directions", Section 8 offers a detailed outlook on several potential future technological pathways for Multimodal Large Language Models (MLLMs), such as scalability enhancements, cross-modal interaction and integration, and efficient training and inference solutions.Besides, the survey paper also raises concerns about the ethical and societal implications of the excessive use of MLLMs, including their potential impact on issues such as gender, race, ethnicity, and socioeconomic status.Furthermore, SURVEYFORGE has outlined numerous application scenarios for MLLMs, including AI-driven agents, interactive systems, Augmented Reality (AR), and specialized domains such as healthcare and education.In addition, SURVEYFORGE further analyzes the challenges that need to be addressed to apply MLLMs to these practical scenarios.For instance, addressing computational limitations and tackling privacy concerns associated with systems that rely on large amounts of data, which require robust frameworks for data management and obtaining user consent.</p>
<p>A.9 Prompt Used</p>
<p>This section outlines the key prompts employed in SURVEYFORGE, covering those for outline generation, content generation, and evaluation.</p>
<p>The outline generation prompt incorporates two key elements: the structure of human-written survey papers and relevant literature on the topic.This prompt ensures that the generated outline adheres to academic conventions, with section titles aligned to the survey topic, maintaining logical connections between sections while avoiding redundancy.The content generation prompt guides LLMs in drafting individual sections of a survey paper.It requires the generated content to be supported by references from relevant literature and specifies length constraints to ensure clarity and precision.</p>
<p>For the prompts used for evaluation, we design the evaluation rules from both the outline and the content.Regarding outline evaluation, LLMs are instructed to score from the aspects of topic uniqueness, structural balance, hierarchical clarity and logical organization, with the total score for each aspect serving as the overall score for the outline.For content evaluation, the process references humanwritten surveys: LLMs first review such surveys on the same topic to establish context before evaluating AI-generated content.This approach grounds the evaluation in established academic writing practices, enhancing the reliability of the assessment.</p>
<p>Regular Process of Writing an Academic Survey Paper (a) The Regular Process of Writing an Academic Survey Paper</p>
<p>Figure 2 :
2
Figure 2: The overview of SURVEYFORGE.The framework consists of two main stages: Outline Generation and Content Writing.In the Outline Generation stage, SURVEYFORGE utilizes heuristic learning to generate well-structured outlines by leveraging topic-relevant literature and structural patterns from existing surveys.In the Content Writing stage, a memory-driven Scholar Navigation Agent (SANA) retrieves high-quality literature for each subsection and LLM generates the content of each subsection.Finally, the content is synthesized and refined into a coherent and comprehensive survey.</p>
<p>Figure 3 :
3
Figure 3: Evaluation results on SurveyBench.Evaluation results of (a) Input Coverage, (b) Reference Coverage, (c) Outline Quality, and (d) Content Quality.</p>
<p>Figure 4 :*
4
Figure 4: Comparisons of survey outlines generated by the baseline method (left) and our proposed framework (right).The baseline displays a fragmented structure, whereas our method yields a more comprehensive, systematically organized outline.</p>
<p>Table 1 :
1
Comparison of SURVEYFORGE and AutoSurvey (Wang et al., 2024c)using Survey Assessment Metrics (SAM) from three aspects: Reference (SAM-R), Outline (SAM-O) and Content quality (SAM-C)."Input Cov." means the coverage of input papers, measuring the overlap between retrieved papers and benchmark references, while "Reference Cov." means the coverage of reference, evaluating the alignment between cited references of the survey and benchmark references.
MethodsModelReference Quality Input Cov. Reference Cov.Outline QualityContent Quality Structure Relevance Coverage AvgHuman-Written --0.629487.62----AutoSurveyClaude-3-Haiku0.11530.234182.1872.8376.4472.3573.87SURVEYFORGE Claude-3-Haiku0.22310.396086.8573.8279.6275.5976.34AutoSurveyGPT-4o mini0.06650.203583.1074.6674.1676.3375.05SURVEYFORGE GPT-4o mini0.20180.423686.6277.1076.9477.1577.06MethodsOutline Comparison Score Win Rate Comparative Win Rate Human Eval Score Win Rate Human Eval Content ComparisonAutoSurvey (Wang et al., 2024c)27.00%25.00%26.00%31.00%30.00%SURVEYFORGE73.00%75.00%74.00%69.00%70.00%</p>
<p>Table 2 :
2
Win-rate comparison of automatic and human evaluations on outline and content quality."Score Win Rate" reflects the win rate based on individual LLM-scores, where the LLM assigns separate score to each survey paper before determining the higher-scoring one."Comparative Win Rate" is derived from LLM pairwise comparisons, where the LLM directly compares two articles side-by-side and decides which one is superior."Human Eval" represents the win rate derived from expert human evaluations.
//0EDVHG 0XOWL$JHQW(YDOXDWLRQ RI//0V//0EDVHG 0XOWL$JHQW(YDOXDWLRQ RI//0V<em>HQHUDWLYH 'LIIXVLRQ 0RGHOV'2EMHFW 'HWHFWLRQLQ$'</em>HQHUDWLYH 'LIIXVLRQ 0RGHOV'2EMHFW 'HWHFWLRQLQ$'<em>UDSK 1HXUDO 1HWZRUNV'</em>DXVVLDQ 6SODWWLQJ<em>UDSK 1HXUDO 1HWZRUNV'</em>DXVVLDQ 6SODWWLQJ+DOOXFLQDWLRQ LQ//0V9LVLRQ 7UDQVIRUPHUV+DOOXFLQDWLRQ LQ//0V9LVLRQ 7UDQVIRUPHUV0XOWLPRGDO /DUJH/DQJXDJH 0RGHOV5$<em>IRU //0V$XWR6XUYH\ 6XUYH)RUJH0XOWLPRGDO /DUJH/DQJXDJH 0RGHOV5$</em>IRU //0V$XWR6XUYH\ 6XUYH)RUJH +XPDQ(a)(b)//0EDVHG 0XOWL$JHQW(YDOXDWLRQ RI//0V//0EDVHG 0XOWL$JHQW(YDOXDWLRQ RI//0V<em>HQHUDWLYH 'LIIXVLRQ 0RGHOV'2EMHFW 'HWHFWLRQLQ$'</em>HQHUDWLYH 'LIIXVLRQ 0RGHOV'2EMHFW 'HWHFWLRQLQ$'<em>UDSK 1HXUDO 1HWZRUNV'</em>DXVVLDQ 6SODWWLQJ<em>UDSK 1HXUDO 1HWZRUNV'</em>DXVVLDQ 6SODWWLQJ+DOOXFLQDWLRQ LQ//0V9LVLRQ 7UDQVIRUPHUV+DOOXFLQDWLRQ LQ//0V9LVLRQ 7UDQVIRUPHUV0XOWLPRGDO /DUJH/DQJXDJH 0RGHOV5$<em>IRU //0V$XWR6XUYH\ 6XUYH)RUJH +XPDQ0XOWLPRGDO /DUJH/DQJXDJH 0RGHOV5$</em>IRU //0V$XWR6XUYH\ 6XUYH)RUJH(c)(d)</p>
<p>. Using Claude-3-Haiku, the outline quality score increases from 82.25 to 86.58, while GPT-4o mini achieves a
MethodHeuristic Learning Demonstration Outline Outline QualityAutoSurvey×-81.78SURVEYFORGE✓From random surveys84.58SURVEYFORGE✓From related surveys86.67</p>
<p>Table 3 :
3
Ablation study for outline generation."Demonstration Outline" means the source of outlines used for heuristic learning.
similar improvement from 83.10 to 86.62. Theseadvancements are driven by the proposed few-shot heuristic learning method, which leveragesexpert-curated examples from the Survey OutlineDatabase to guide the LLMs in producing well-structured and domain-relevant outlines.Results on Content Quality. For content quality,SURVEYFORGE achieves consistent improvementsacross all three evaluation dimensions: structure,relevance, and coverage. The average content qual-ity score increases from 73.87 to 76.34 (Claude-3-Haiku) and 75.05 to 77.06 (GPT-4o mini). Theseresults confirm that SURVEYFORGE generates con-tent that is better organized, more relevant, andmore comprehensive, effectively addressing thecritical aspects of the target domain.</p>
<p>Table 6 :
6
Inter-rater agreement between LLM and human evaluations.κ means the Cohen's kappa coefficient.
Evaluation PairAspectκLLM vs. HumanOutline0.7177LLM vs. HumanContent0.6462Human Cross-ValidationOutline0.7921Human Cross-ValidationContent0.7098</p>
<p>Table 7 :
7
Comparison of open source and closed source models on SurveyBench.
MethodsModelReference Quality Input Cov. Reference Cov.Outline QualityContent Quality Structure Relevance Coverage AvgHuman-Written --0.629487.62----SURVEYFORGE Claude-3-Haiku0.22310.396086.8573.8279.6275.5976.34SURVEYFORGE GPT-4o mini0.20180.423686.6277.1076.9477.1577.06SURVEYFORGE Deepseek-v30.25540.455387.4279.2080.1781.0780.15</p>
<p>Outline Generated by AutoSurvey Outline Generated by AutoSurvey Outline Generated by SurveyForge Outline Generated by SurveyForge A Comprehensive Survey on Vision Transformers 1. Introduction to Vision Transformers 1.1 Introduction to Vision Transformers 1.2 From Transformers to Vision Transformers 1.3 Architecture of Vision Transformers 1.4 Advantages and Limitations of Vision Transformers 2. Vision Transformer Architectures and Advancements
2.1 Dual Vision Transformer (Dual-ViT)2.2 SpectFormer2.3 FcaFormer2.4 Demystify Transformers &amp; Convolutions in Modern Image Deep Networks2.5 ViTALiTy2.6 UniNeXt3. Vision Transformer Applications and Benchmarks3.1 Image Classification3.2 Object Detection3.3 Semantic Segmentation3.4 Video Understanding3.5 Multimodal Tasks4. Efficiency</p>
<p>and Optimization of Vision Transformers 4.1 Model Compression Techniques for Vision Transformers 4.2 Hardware-Aware Optimization of Vision Transformers 4.3 Efficient Training Strategies for Vision Transformers 5. Robustness and Interpretability of Vision Transformers 5.1 Robustness to Adversarial Attacks 5.2 Handling Distribution Shifts 5.3 Visualization and Interpretability 6. Vision Transformer Pretraining and Transfer Learning 6.1 Self-supervised Learning for Vision Transformers 6.2 Knowledge Distillation for Vision Transformers 6.3 Transfer Learning and Fine-tuning of Vision Transformers 7. Future Trends and Challenges 7.1 Integrating Vision Transformers with Other Deep Learning Approaches 7.2 Self-Supervised and Unsupervised Learning with Vision Transformers 7.3 Extending Vision Transformers to Other Modalities A Comprehensive Survey of Vision Transformers 1. Introduction 2. Vision Transformer Architectures 2.1 The Original Vision Transformer 2.2 Hybrid Vision Transformer Architectures 2.3 Efficient and Lightweight Vision Transformers 2.4 Multi-scale and Hierarchical Vision Transformers 3. Vision Transformer Training and Optimization 3.1 Pre-training and Transfer Learning Techniques 3.2 Data Augmentation for Vision Transformers 3.3 Regularization Techniques for Vision Transformer Training 3.4 Efficient Training and Fine-tuning Strategies for Vision Transformers 3.5 Addressing Challenges in Vision Transformer Training 3.6 Emerging Trends in Vision Transformer Training
4. Vision Transformer Applications4.1 Image Classification and Recognition4.2 Object Detection, Segmentation, and Instance Segmentation4.3 Video Understanding Tasks4.4 Multimodal and Cross-modal Applications5. Interpretability and Explainability of Vision Transformers5.1 Attention Visualization and Interpretation5.2 Probing and Analyzing Learned Representations5.3 Generating Human-Interpretable Explanations5.4</p>
<p>Challenges and Opportunities in Interpretability 6. Efficient and Scalable Vision Transformers 6.1 Architectural Innovations for Efficient Vision Transformers 6.2 Token Reduction and Sparsification Techniques 6.3 Hardware-Aware Optimization and Acceleration 6.4 Quantization and Precision Reduction 6.5 Efficient Training and Fine-Tuning Strategies 6.6 Benchmarking and Deployment Considerations 7. Conclusion Outline Generated by AutoSurvey Outline Generated by AutoSurvey Outline Generated by SurveyForge Outline Generated by SurveyForge Multimodal Large Language Models: A Comprehensive Survey 1 Introduction to Multimodal Large Language Models 1.1 The Emergence and Importance of Multimodal Large Language Models 1.2 Multimodal Modeling Approaches 1.3 Applications and Use Cases of Multimodal Large Language Models 1.4 Challenges and Limitations of Multimodal Large Language Models 1.5 Ethical Considerations and Safety Concerns 1.6 Future Directions and Conclusions 2 Multimodal Datasets and Benchmarks 2.1 Multimodal Datasets and Benchmarks 2.2 SEED-Bench-2 -Benchmarking Multimodal Large Language Models 2.3 Charting New Territories -Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs 2.4 Multimodal Datasets and Benchmarks -A Survey 2.5 Beyond Text -Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark 2.6 MME -A Comprehensive Evaluation Benchmark for Multimodal Large Language Models 2.7 MLLM-as-a-Judge -Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark 2.8 MULTI -Multimodal Understanding Leaderboard with Text and Images 3 Architectural Advancements and Training Strategies 3.1 Architectural Components 3.2 Training Strategies 3.3 Modality-Specific Encoders 3.4 Joint Representation Learning
3.5 Multimodal Fusion3.6 Pretraining Objectives4 Applications and Use Cases4.1 Healthcare Applications4.2 Education and Training4.3 Accessibility and Inclusion4.4 Multimodal Biomedical Research4.5 Ethics and Responsible Development5 Challenges and Limitations5.1 Multimodal Hallucination5.2 Cross-Modal Alignment5.3 Interpretability and Explainability5.4 Evaluation and Benchmarking5.5 Mitigation Strategies5.6 Ethical Considerations5.7 Future Directions and Conclusions6 Ethical Considerations and Safety6.1 Bias, Privacy, and User Consent6.2 Potential for Misuse and Malicious Use Cases6.3 Transparency and Interpretability6.4 Environmental and Societal Impact6.5 Governance and Regulatory Frameworks6.6</p>
<p>Future Challenges and Research Directions 7 Future Directions and Conclusions 7.1 The Transformative Potential of Multimodal Large Language Models 7.2 Emerging Trends and Innovative Applications 7.3 Addressing Challenges and Mitigating Limitations 7.4 Responsible Development and Ethical Considerations 7.5 Towards Artificial General Intelligence A Comprehensive Survey on Multimodal Large Language Models 1 Introduction 2 Multimodal Model Architectures and Learning Frameworks 2.1 Multimodal Model Architectures 2.2 Multimodal Learning Frameworks 2.3 Multimodal Reasoning and Interpretation 2.4 Multimodal Alignment and Connecting Modalities 2.5 Efficient Multimodal Model Design 3 Multimodal Pretraining and Datasets 3.1 Multimodal Pretraining Objectives and Tasks 3.2 Large-scale Multimodal Datasets 3.3 Multimodal Data Preprocessing and Representation 3.4 Multimodal Data Augmentation and Synthesis 3.5 Multimodal Pretraining Strategies and Techniques 4 MLLM Evaluation and Benchmarking 4.1 Multimodal Task Taxonomies and Benchmark Suites 4.2 Evaluation Metrics and their Suitability for MLLM Assessment 4.3 Challenges and Limitations of Existing MLLM Evaluation Approaches 4.4 Strategies for Developing Robust and Generalized MLLM Evaluation Frameworks 4.5 Towards Standardized and Automated MLLM Evaluation 4.6 Emerging Evaluation Frontiers for Multimodal Large Language Models 5 Multimodal Applications and Case Studies 5.1 Multimodal Language Generation 5.2 Multimodal Understanding and Reasoning 5.3 Multimodal Task-Oriented Applications 5.4 Emerging Multimodal Domains and Novel Applications 6 Limitations and Future Research Directions 6.1 Limitations in MLLM Multimodal Understanding and Reasoning Capabilities 6.2 Scalability and Computational Efficiency Challenges in MLLM Training and Deployment 6.3 Advancing Multimodal Knowledge Representation and Reasoning 6.4 Enhancing MLLM Generalization and Few-shot Learning Abilities 6.5 Integrating MLLMs with Other AI Systems for Comprehensive Multimodal Intelligence 7 Conclusion</p>
<p>AcknowledgementThe research was supported by Shanghai Artificial Intelligence Laboratory, the Shanghai Municipal Science and Technology Major Project, and Shanghai Rising Star Program (Grant No. 23QD1401000).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Jacqueline Anthropic, Rulin He, Weijia Shao, Amanpreet Shi, Joseph Chee Singh, Kyle Chang, Luca Lo, Sergey Soldaini, Mike D Feldman, arXiv:2411.14199Openscholar: Synthesizing scientific literature with retrieval-augmented lms. 2024. 2024arXiv preprintThe claude 3 model family: Opus, sonnet, haiku</p>
<p>Assafelovic. 2023. gpt-researcher. </p>
<p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.17297Internlm2 technical report. 2024arXiv preprint</p>
<p>Automatic generation of related work through summarizing citations. Jingqiang Chen, Hai Zhuge, Concurrency and Computation: Practice and Experience. 313e42612019</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Zhe Chen, Weiyun Wang, Shenglong Hao Tian, Zhangwei Ye, Erfei Gao, Wenwen Cui, Kongzhi Tong, Jiapeng Hu, Zheng Luo, Ma, arXiv:2404.168212024aarXiv preprint</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, arXiv:2410.05080Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. 2024barXiv preprint</p>
<p>Clarivate, Essential science indicators: Learn the basics. 2024</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.05556Sciagents: Automating scientific discovery through multiagent intelligent graph reasoning. 2024arXiv preprint</p>
<p>Re2G: Retrieve, rerank, generate. Michael Glass, Gaetano Rossiello, Md Faisal, Mahbub Chowdhury, Ankita Naik, Pengshan Cai, Alfio Gliozzo, 10.18653/v1/2022.naacl-main.194Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Hierarchical catalogue generation for literature review: A benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, arXiv:2304.035122023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
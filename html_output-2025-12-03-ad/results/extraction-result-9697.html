<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9697 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9697</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9697</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-276250256</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.06633v1.pdf" target="_blank">Combining Large Language Models with Static Analyzers for Code Review Generation</a></p>
                <p><strong>Paper Abstract:</strong> Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (DataAugmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a realworld dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9697.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9697.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (Llama3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge using Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses Llama3-70B as an automated evaluator to rate and rank generated code-review comments (both for dataset filtering and for large-scale evaluation), substituting humans for scalability and consistency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Combining Large Language Models with Static Analyzers for Code Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automated code-review comment evaluation (accuracy classification and coverage ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Used in two related setups: (1) Data-filtering: Llama3-70B was given the source code and candidate review comments and asked to rate each review on a 10-point scale, retaining reviews >= 8 for the augmented training set; (2) Evaluation: Llama3-70B was instructed (replicating the human manual procedure) to classify each review as 'accurate', 'partially accurate', or 'not accurate' for all test samples and to perform comparative coverage rankings (Rank 1..5) using the code+reviews as context.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual accuracy assessment on a random 10% subset of the test set (~125 examples). Two human evaluators with code-review expertise independently categorized each anonymized, randomized review as 'accurate', 'partially accurate', or 'not accurate'; conflicts were resolved by discussion to reach consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa measuring agreement between Llama3-70B and the two human evaluators on the 10% sample; reported value = 0.72 (interpreted as 'substantial agreement').</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The paper flags potential losses when substituting humans with an LLM judge: (a) possible subtle misalignments across the full dataset — i.e., 'subtle variations' in LLM judgments that may not fully match human assessments; (b) dependence on an LLM introduces a risk that nuanced, context-dependent human judgments are not perfectly captured at scale; and (c) reliance on LLM judgment can propagate any systematic LLM biases or errors into filtered training data and evaluation outcomes, potentially affecting downstream model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper does not provide concrete, itemized cases where the LLM judge definitively disagreed with humans; instead it reports the general concern that 'subtle variations in the LLM's judgment across the full dataset may not fully align with human assessments' and cites this as a validity threat. No explicit per-example divergences between Llama3-70B and the human annotators are shown in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Counterpoints reported by the authors: (a) substantial agreement (Cohen's kappa = 0.72) indicates Llama3-70B closely mirrors human labels on the validation subset; (b) the authors performed manual inspection of random subsets of the LLM-based evaluations for RQ4 and found them 'consistent and reliable'; (c) Llama3-70B was used successfully for two operational tasks in the study — dataset filtering (10-point ratings) and large-scale accuracy/coverage evaluation — enabling scalability that manual labeling could not provide; (d) the paper also cites prior findings that some large LLMs (e.g., Llama-3 70B, GPT-4 Turbo) can exhibit strong alignment with human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>II.E (LLM as a Judge Mechanism); III.C (Data filtering using Llama3-70B); IV.B (RQ2 description of the sanity check); IV.C.2 (results: Cohen's kappa = 0.72); IV.E (Threats to Validity: Dependency on LLM Judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Combining Large Language Models with Static Analyzers for Code Review Generation', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judgelm: Fine-tuned large language models are scalable judges <em>(Rating: 2)</em></li>
                <li>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms <em>(Rating: 2)</em></li>
                <li>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges <em>(Rating: 2)</em></li>
                <li>An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
                <li>Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9697",
    "paper_id": "paper-276250256",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (Llama3-70B)",
            "name_full": "LLM-as-a-Judge using Llama3-70B",
            "brief_description": "The paper uses Llama3-70B as an automated evaluator to rate and rank generated code-review comments (both for dataset filtering and for large-scale evaluation), substituting humans for scalability and consistency checks.",
            "citation_title": "Combining Large Language Models with Static Analyzers for Code Review Generation",
            "mention_or_use": "use",
            "task_domain": "Automated code-review comment evaluation (accuracy classification and coverage ranking)",
            "llm_judge_model": "Llama3-70B",
            "llm_judge_setup": "Used in two related setups: (1) Data-filtering: Llama3-70B was given the source code and candidate review comments and asked to rate each review on a 10-point scale, retaining reviews &gt;= 8 for the augmented training set; (2) Evaluation: Llama3-70B was instructed (replicating the human manual procedure) to classify each review as 'accurate', 'partially accurate', or 'not accurate' for all test samples and to perform comparative coverage rankings (Rank 1..5) using the code+reviews as context.",
            "human_evaluation_setup": "Manual accuracy assessment on a random 10% subset of the test set (~125 examples). Two human evaluators with code-review expertise independently categorized each anonymized, randomized review as 'accurate', 'partially accurate', or 'not accurate'; conflicts were resolved by discussion to reach consensus.",
            "agreement_metric": "Cohen's kappa measuring agreement between Llama3-70B and the two human evaluators on the 10% sample; reported value = 0.72 (interpreted as 'substantial agreement').",
            "losses_identified": "The paper flags potential losses when substituting humans with an LLM judge: (a) possible subtle misalignments across the full dataset — i.e., 'subtle variations' in LLM judgments that may not fully match human assessments; (b) dependence on an LLM introduces a risk that nuanced, context-dependent human judgments are not perfectly captured at scale; and (c) reliance on LLM judgment can propagate any systematic LLM biases or errors into filtered training data and evaluation outcomes, potentially affecting downstream model behavior.",
            "examples_of_loss": "The paper does not provide concrete, itemized cases where the LLM judge definitively disagreed with humans; instead it reports the general concern that 'subtle variations in the LLM's judgment across the full dataset may not fully align with human assessments' and cites this as a validity threat. No explicit per-example divergences between Llama3-70B and the human annotators are shown in the text.",
            "counterexamples_or_caveats": "Counterpoints reported by the authors: (a) substantial agreement (Cohen's kappa = 0.72) indicates Llama3-70B closely mirrors human labels on the validation subset; (b) the authors performed manual inspection of random subsets of the LLM-based evaluations for RQ4 and found them 'consistent and reliable'; (c) Llama3-70B was used successfully for two operational tasks in the study — dataset filtering (10-point ratings) and large-scale accuracy/coverage evaluation — enabling scalability that manual labeling could not provide; (d) the paper also cites prior findings that some large LLMs (e.g., Llama-3 70B, GPT-4 Turbo) can exhibit strong alignment with human evaluators.",
            "paper_reference": "II.E (LLM as a Judge Mechanism); III.C (Data filtering using Llama3-70B); IV.B (RQ2 description of the sanity check); IV.C.2 (results: Cohen's kappa = 0.72); IV.E (Threats to Validity: Dependency on LLM Judgment).",
            "uuid": "e9697.0",
            "source_info": {
                "paper_title": "Combining Large Language Models with Static Analyzers for Code Review Generation",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judgelm: Fine-tuned large language models are scalable judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        },
        {
            "paper_title": "Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms",
            "rating": 2,
            "sanitized_title": "judging_the_judges_a_systematic_investigation_of_position_bias_in_pairwise_comparative_assessments_by_llms"
        },
        {
            "paper_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges",
            "rating": 2,
            "sanitized_title": "judging_the_judges_evaluating_alignment_and_vulnerabilities_in_llmsasjudges"
        },
        {
            "paper_title": "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
            "rating": 2,
            "sanitized_title": "an_empirical_study_of_llmasajudge_for_llm_evaluation_finetuned_judge_models_are_taskspecific_classifiers"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences",
            "rating": 1,
            "sanitized_title": "codeultrafeedback_an_llmasajudge_dataset_for_aligning_large_language_models_to_coding_preferences"
        }
    ],
    "cost": 0.0084895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Combining Large Language Models with Static Analyzers for Code Review Generation
10 Feb 2025</p>
<p>Imen Jaoua imen.jaoua@umontreal.ca 
Université de Montréal Montreal
Canada</p>
<p>Oussama Ben Sghaier oussama.ben.sghaier@umontreal.ca 
Université de Montréal Montreal
Canada</p>
<p>Houari Sahraoui sahraouh@iro.umontreal.ca 
Université de Montréal Montreal
Canada</p>
<p>Combining Large Language Models with Static Analyzers for Code Review Generation
10 Feb 20257AE67834FF955746C13D4BE95D7D6466arXiv:2502.06633v1[cs.SE]Code ReviewKnowledge-Based SystemsLanguage ModelsRetrieval-Augmented Generation
Code review is a crucial but often complex, subjective, and time-consuming activity in software development.Over the past decades, significant efforts have been made to automate this process.Early approaches focused on knowledgebased systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases.More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision.In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews.Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO).We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a realworld dataset.Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.</p>
<p>I. INTRODUCTION</p>
<p>Code review is a common software engineering practice that plays a crucial role in maintaining code quality, identifying bugs, and fostering a culture of continuous improvement within development teams [1], [2].By rigorously inspecting code changes, reviewers provide valuable feedback that can improve software both before and after it is integrated into the system.Modern code review goes beyond defect detection; it aims to enhance the overall quality of software changes by ensuring maintainability, reliability, and adherence to best practices [3], [4].</p>
<p>However, the code review process is often perceived as complex, time-consuming, and subject to various biases, especially in large-scale projects.Factors such as varying levels of developer expertise, interpersonal dynamics, and the lack of standardized guidelines can introduce inconsistencies, ultimately impacting the efficiency and robustness of the codebase [5].Furthermore, the evolving nature of coding standards and best practices requires constant adaptation, adding further complexity to the process.</p>
<p>To address these challenges, there has been growing interest in automating the code review process [6].Initial efforts primarily involved the deployment of knowledge-based systems (KBS), particularly static analysis tools like PMD, FindBugs, and SonarQube.These tools use predefined rule sets to identify common coding issues, systematically scanning the codebase for violations and improving the early stages of development by flagging rule violations in the code.Although effective at detecting surface-level issues, static analyzers are limited by their dependence on manually defined rules, which require frequent updates to remain relevant.Their rigidity makes it difficult to handle complex, context-dependent issues that require a deeper understanding of code intent [7].Consequently, these tools often fall short in adapting to the dynamic and context-sensitive nature of software projects, where factors like architecture, team culture, and specific project requirements demand more flexible and intelligent solutions.</p>
<p>Recent advances in large language models and natural language processing have sparked significant interest in using pre-trained language models to automate code review workflows.These learning-based systems (LBS) aim to overcome the limitations of knowledge-based detection by providing a more nuanced understanding of code, enabling them to identify deeper faults, recommend improvements, and even anticipate the potential impact of code changes on the overall system [8].Although these models capture a broader range of issue patterns than static analysis alone, their precision still remains below acceptable levels [9], [10].</p>
<p>As is common in the automation of software development tasks, some solutions achieve high precision but with limited coverage, while others offer broader coverage at the expense of precision.The most promising approaches, therefore, are those that balance these trade-offs by combining solutions to optimize both precision and coverage.We hypothesize that integrating static analysis with learning-based systems can enhance the effectiveness of automated code review generation tools.In this paper, we explore three combination strategies aimed at integrating the structured knowledge from static analysis into the pipeline for building and operating a large language model (LLM) for code review generation.Specifically, our approach integrates knowledge at three distinct points in the pipeline: during data preparation for fine-tuning (data-augmented training, DAT), at inference time (retrievalaugmented generation, RAG), and after inference (naive concatenation of outputs, NCO).</p>
<p>To evaluate these strategies, we used a combination of human judgments and LLM-based assessments to compare the generated reviews from each approach with baseline systems: standalone KBS and LBS.Our results show that combining KBS and LBS captures the strengths of both: the precision of static analysis and the comprehensiveness of LLMs, resulting in more effective and human-like code reviews.</p>
<p>The remainder of this paper is organized as follows: Section II provides an overview of the background.Section III details the components of our proposed approach.Section IV presents the evaluation results.Section V discusses related work.Finally, Section VI offers concluding remarks and suggests potential directions for future improvements to our approach.</p>
<p>II. BACKGROUND</p>
<p>A. Code Review Automation</p>
<p>Code review is a widely adopted practice among software developers where a reviewer examines changes submitted in a pull request [3], [5], [11].If the pull request is not approved, the reviewer must describe the issues or improvements required, providing constructive feedback and identifying potential issues.This step involves review commment generation, which play a key role in the review process by generating review comments for a given code difference.These comments can be descriptive, offering detailed explanations of the issues, or actionable, suggesting specific solutions to address the problems identified [5].</p>
<p>Various approaches have been explored to automate the code review comments process [9], [12], [13].Early efforts centered on knowledge-based systems, which are designed to detect common issues in code.Although these traditional tools provide some support to programmers, they often fall short in addressing complex scenarios encountered during code reviews [14].More recently, with advancements in deep learning, researchers have shifted their focus toward using large-language models to enhance the effectiveness of code issue detection and code review comment generation.</p>
<p>B. Knowledge-based Code Review Comments Automation</p>
<p>Knowledge-based systems (KBS) are software applications designed to emulate human expertise in specific domains by using a collection of rules, logic, and expert knowledge.KBS often consist of facts, rules, an explanation facility, and knowledge acquisition.In the context of software development, these systems are used to analyze the source code, identifying issues such as coding standard violations, bugs, and inefficiencies [15]- [18].By applying a vast set of predefined rules and best practices, they provide automated feedback and recommendations to developers.Tools such as FindBugs [19], PMD [20], Checkstyle [21], and SonarQube [22] are prominent examples of knowledge-based systems in code analysis, often referred to as static analyzers.These tools have been utilized since the early 1960s, initially to optimize compiler operations, and have since expanded to include debugging tools and software development frameworks [23], [24].</p>
<p>C. LLMs-based Code Review Comments Automation</p>
<p>As the field of machine learning in software engineering evolves, researchers are increasingly leveraging machine learning (ML) and deep learning (DL) techniques to automate the generation of review comments [3], [11], [25]- [28].Large language models (LLMs) are large-scale Transformer models, which are distinguished by their large number of parameters and extensive pre-training on diverse datasets.Recently, LLMs have made substantial progress and have been applied across a broad spectrum of domains.Within the software engineering field, LLMs can be categorized into two main types: unified language models and code-specific models, each serving distinct purposes [29].</p>
<p>Code-specific LLMs, such as CodeGen [30], StarCoder [31] and CodeLlama [32] are optimized to excel in code comprehension, code generation, and other programming-related tasks.These specialized models are increasingly utilized in code review activities to detect potential issues, suggest improvements, and automate review comments [13], [29].</p>
<p>D. Retrieval-Augmented Generation</p>
<p>Retrieval-Augmented Generation (RAG) is a general paradigm that enhances LLMs outputs by including relevant information retrieved from external databases into the input prompt [33].Traditional LLMs generate responses based solely on the extensive data used in pre-training, which can result in limitations, especially when it comes to domainspecific, time-sensitive, or highly specialized information.RAG addresses these limitations by dynamically retrieving pertinent external knowledge, expanding the model's informational scope and allowing it to generate responses that are more accurate, up-to-date, and contextually relevant [34].</p>
<p>To build an effective end-to-end RAG pipeline, the system must first establish a comprehensive knowledge base.It requires a retrieval model that captures the semantic meaning of presented data, ensuring relevant information is retrieved.Finally, a capable LLM integrates this retrieved knowledge to generate accurate and coherent results [10].</p>
<p>E. LLM as a Judge Mechanism</p>
<p>LLM evaluators, often referred to as LLM-as-a-Judge, have gained significant attention due to their ability to align closely with human evaluators' judgments [35], [36].Their adaptability and scalability make them highly suitable for handling an increasing volume of evaluative tasks.</p>
<p>Recent studies have shown that certain LLMs, such as Llama-3 70B and GPT-4 Turbo, exhibit strong alignment with human evaluators, making them promising candidates for automated judgment tasks [37] To enable such evaluations, a proper benchmarking system should be set up with specific components: prompt design, which clearly instructs the LLM to evaluate based on a given metric, such as accuracy, relevance, or coherence; response presentation, guiding the LLM to present its verdicts in a structured format; and scoring, enabling the LLM to assign a score according to a predefined scale [10].Additionally, this evaluation system can be enriched with the ability to explain reasoning behind verdicts, which is a significant advantage of LLM-based evaluation [38].The LLM can outline the criteria it used to reach its judgment, offering deeper insights into its decision-making process.</p>
<p>III. PROPOSED APPROACH</p>
<p>For the task of review comment generation, Knowledge-Based Systems (KBS) draw on codified rules and expert knowledge to deliver feedback that is consistent with established coding standards and best practices.Static analyzers, a prominent example of KBS, systematically follow predefined guidelines to detect code issues, offering reliable and structured feedback.While KBS achieve high precision, they are limited in scope, covering only a subset of possible issues encountered during code changes.In contrast, Learning-Based Systems (LBS) harness the adaptive potential of language models, which, by training on historical data, can recognize intricate patterns and generate contextually relevant review comments.This adaptability allows LBS to cover a broader range of issues present in the dataset, though often at the expense of precision.In this work, we conjecture that by combining these two strategies, it is possible to achieve the best of both approaches, namely, broader issue coverage coupled with improved precision.</p>
<p>A. Overview</p>
<p>B. Baseline Model Preparation and Static Analyzers Selection</p>
<p>While our approach is applicable to a wide range of LLMs and static analysis tools, we propose a specific configuration to illustrate the three strategies and establish the baselines for validating our conjecture.To set up the baseline systems, we first defined the LBS.We fine-tuned a large language model on an extensive code review dataset [25], referred to as D Mi , which pairs code changes with detailed reviews.</p>
<p>The selected model for fine-tuning is CodeLlama-7b, trained for comment generation (i.e., generating review comments from code changes) with the following hyperparameter settings.The training was conducted on four NVIDIA RTX 3090 GPUs, using a batch size of 4 per device.To boost efficiency, we applied gradient accumulation with a step size of 4, updating the optimizer only after multiple batches.We used 4-bit quantization to improve memory and computational efficiency.Additionally, we employed Quantized Low-Rank Adaptation (QLoRA) [39], a Parameter-Efficient Fine-Tuning (PEFT) technique, with r = 16, α = 32, and dropout = 0.05.This method decomposes weight updates into low-rank matrices, reducing the parameters needed for fine-tuning and optimizing training efficiency [39].</p>
<p>This resulted in a model, denoted as M i , capable of generating detailed human-like code reviews.Since M i represents the LBS component and was trained using the data from D Mi , we used the test set to generate reviews by both static analyzers and the fine-tuned model M i .</p>
<p>To focus on a relevant subset of available static analyzers, we filtered the test set to include only Java code samples, producing a subset of 27, 267 entries, denoted D o .Each entry in D o is a tuple (f, c), where f represents the source code file and c denotes the code change.Here, c is input to the LBS, while f serves as input to the KBS.We limited our selection of static analyzers to tools that process Java source code directly.Although this decision excludes tools designed for Java bytecode analysis, it allows for a broader range of issue types.Specifically, we selected two well-established static analyzers: PMD [20] and Checkstyle [21], both of which are designed to identify potential issues directly in source code.</p>
<p>PMD is a static code analysis tool that identifies issues in code by applying a set of rules aimed at detecting common problems, which are categorized into eight groups: best practices, coding style, design, documentation, error-prone, multithreading, performance, and security [40].By analyzing source code against these rules, PMD generates detailed reports highlighting areas for improvement and enables users to create custom rules for specific analyses [41].</p>
<p>Checkstyle is another static code analysis tool for Java that offers predefined style configurations for standard checks, including Google Java Style and Sun Java Style.Its rules cover various aspects such as annotations, class design, coding, and naming conventions.Checkstyle also supports custom configuration files tailored to user needs [6], [27], [40], [41].</p>
<p>C. Data Augmented Training</p>
<p>As shown in Figure 2a, this strategy involves retraining the LBS using an augmented dataset Da, which includes review comments generated by both, static analyzers and the finetuned model M i .Through this retraining process, the LBS learns from both data sources, producing a more refined model referred to as M F T .</p>
<p>A simple approach to augmenting the dataset would have been to apply static analysis to the code in D Mi and add or concatenate the generated comments with the existing ones.However, this method does not guarantee data quality within the augmented dataset and fails to account for the insights inferred by the LBS M i .Therefore, we employ an ensemble learning approach where the two distinct sources-the LBS and the KBS-serve as experts to generate data for fine-tuning a model.The underlying rationale is that both KBS and LBS reviews are inherently synthetic.By combining their outputs, we achieve a more balanced and consistent fine-tuning process.</p>
<p>To produce the augmented dataset D a , we designed a two-step process (i.e., data generation and data filtering), as depicted in Figure 3.</p>
<p>In the Data Generation phase, we used the original dataset D o as input.For each code change c, we employed our finetuned model M i to generate four context-aware, human-like reviews.Simultaneously, static analyzer rules were applied to each source code f to produce structured and precise feedback.Each static analyzer generated a report containing several reviews, including the start and end sections of code where each issue was identified.Since our approach focuses on code changes, we extracted the code section highlighted in each review, adding a few context lines before and after each extracted segment.We then merged the reviews generated by both the static analyzers and M i into a single, unified dataset.Each data point in the dataset consists of tuples in the form (f, c, r, t), where f represents the source code file, c is the code change, r denotes the review comment, and t indicates the method used to generate the review (either KBS or LBS).</p>
<p>After data generation, we applied a systematic Data Filtering step to evaluate and refine the merged dataset, ensuring that only the most relevant and meaningful reviews were retained for each source code.While the fine-tuned model M i can generate context-aware, human-like reviews, its output may sometimes include irrelevant or less meaningful feedback, particularly when handling complex or ambiguous code changes.Similarly, static analyzers, although reliable, may produce output overloaded with false positives [42], [43], making it difficult to separate significant concerns from noise.Therefore, filtering both M i 's and the static analyzers' reviews was essential to maintain a dataset of high-quality, meaningful feedback.This filtering process involved rating each review based on its relevance to the corresponding code.The ratings provided a quantitative measure of the review quality generated by both static analyzers and M i .To ensure a fair and scalable rating system, we leveraged large language models, which have demonstrated remarkable performance in similar assessment tasks [44]- [46].By using these models, we achieved a more accurate and consistent evaluation of each review, enhancing the dataset's quality and making it a valuable resource for fine-tuning.As shown in Figure 4, we used Llama3-70b, inputting the code and its corresponding reviews.We instructed the model to rate each review on a 10-point scale.A threshold rating of 8 was set, with only reviews surpassing this threshold retained in the final dataset.After filtering for relevance and quality, we ensured that each comment exceeding the threshold was treated separately.For a source code f with n reviews (r 1 ...r n ), we generated n distinct data points: &lt; f, r 1 &gt;, &lt; f, r 2 &gt;,..., &lt; f, r n &gt;.Additionally, for each comment, we extracted and included the specific segment of code change related to the issue being addressed, the dataset was then structered as &lt; c 1 , r 1 &gt;, &lt; c 2 , r 2 &gt;,..., &lt; c n , r n &gt;.</p>
<p>To prevent overrepresentation of specific rules, we randomly discarded reviews associated with rules that have an excessively high number of reviews.Furthermore, to maintain a balanced dataset, we randomly discarded a subset of learning-based reviews, ensuring an equal distribution between knowledge-based and learning-based reviews.</p>
<p>The final dataset D a consists of 78, 776 samples, ensuring an equal representation of reviews generated by both KBS and LBS methodologies, as shown in Figure 5.It also ensures a balanced distribution across all KBS rules.</p>
<p>Fig. 5: Distribution of LBS and KBS Reviews in Our Dataset</p>
<p>To obtain the M F T model, we partitioned D a dataset into three subsets: 80% of the samples were assigned to the training set, while the remaining 20% was equally divided, with 10% designated for validation and 10% for testing.Each subset maintained a balanced mix of LBS and KBS reviews.We then fine-tuned the CodeLlama-7b model on this dataset with QLoRA to optimize memory efficiency [39].</p>
<p>D. Retrieval Augmented Generation</p>
<p>Retrieval-Augmented Generation (RAG) is a technique designed to enhance the generative capabilities of language models by incorporating external knowledge into their prompts during the inference phase [47].In the context of code review, this strategy can be used to embed KBS-generated feedback directly into the prompts of a language-based system, as shown in Figure 2b.</p>
<p>In our approach, the fine-tuned model M i takes as input the code changes from the D a dataset, along with outputs from PMD and Checkstyle.Incorporating KBS knowledge into the prompt guides the model to produce more relevant and precise reviews.This combination ensures that the generated review comments are both comprehensive and contextually informed.As a result, the reviews generated align closely with established coding standards and best practices, thereby enhancing their overall quality.The augmented prompt is illustrated in Figure 6.</p>
<p>E. Naive Concatenation of Outputs</p>
<p>The Naive Concatenation of Outputs strategy serves as a baseline approach in which review comments generated separately by the LBS and KBS for the same code are combined to form a single review.As illustrated in Figure 2c, the review comment generated by M i is directly concatenated with the output from the static analyzer (either PMD or Checkstyle).This approach is straightforward, requiring minimal adjustments to the inference pipeline while ensuring that the final review delivers comprehensive feedback from both systems.</p>
<p>IV. EVALUATION</p>
<p>In this section, we first define the research questions that guide our study, followed by a detailed description of the experimental setup.We then present and interpret the results.The replication package and the data of our experiments are available online [48], [49].</p>
<p>A. Research Questions</p>
<p>The goal of our evaluation is to determine whether integrating knowledge into the LBS enhances the accuracy of generated comments while maintaining adequate coverage compared to using only the LBS or KBS.To structure our evaluation, we define the following research questions:</p>
<p>RQ1: Manual evaluation of accuracy: How accurate are the reviews generated by our hybrid approaches compared to those produced by the baseline?</p>
<p>To address this question, we perform a manual assessment involving two reviewers on a sample of 10% of the test set, comparing the accuracy of comments generated by our hybrid approaches against those produced by a fine-tuned language model and static analyzers.Given the size of the test set and the five different comment generation options to evaluate, human assessment is impractical for the entire dataset.As an alternative, we employ an LLM to perform assessments across the whole test set.To evaluate the LLM's ability to mimic human judgment, we replicate the manual assessment procedure used for RQ1, substituting the human reviewers with the LLM and measuring the level of agreement between LLM and human evaluations.</p>
<p>RQ3: LLM evaluation of generated reviews: How does the accuracy of the reviews generated by our hybrid approaches compare to those produced by the baseline when evaluated by an LLM?</p>
<p>We use the LLM-as-a-judge approach to assess the accuracy of the generated reviews, comparing results from our hybrid approaches to those from the baseline, specifically the learning-based models and static analyzers.</p>
<p>RQ4: Ranking of reviews based on coverage: How do our hybrid approaches compare to the baseline in terms of coverage?</p>
<p>For this question, we use an LLM-as-a-judge to conduct comparative evaluations, ranking reviews generated by our approaches and the baseline according to coverage criteria.</p>
<p>B. Experimental Setup</p>
<p>To address our research questions, we conducted a series of experiments.To enable comparison across the different hybrid approaches, we used the test set from our augmented dataset, Da, ensuring that all entries were unseen by the finetuned model M F T .Since the retrieval-augmented generation and naive concatenation of outputs approaches are applicable only to code changes with both KBS and LBS reviews, we filtered the test set to include only samples containing reviews from both sources.As illustrated in Figure 7, we examined the code changes in the selected test set, creating a unified code difference for analysis whenever overlaps were found.For example, if the test set contains a code difference dif f 1 with an LBS review comment r 1 and a code difference dif f 2 having a KBS review comment r 2 , we merged both code differences into a unified difference, dif f u , as shown in Figure 7, forming a new triplet (dif f u , r 1 , r 2 ) in our test set.Out of the 7, 884 total samples, we identified 1, 245 common code differences that included both KBS and LBS reviews.</p>
<p>For each of these selected code changes, we generated five types of reviews: the static analyzer review, the review generated by M i , the review generated by M F T , the retrievalaugmented generation review and the naive concatenated review.</p>
<p>For RQ1, we randomly selected 10% of the 1, 245 test set examples for a manual evaluation focused on accuracy.All reviews were anonymized and presented to the evaluators in a randomized order.This approach minimizes confirmation bias, preventing evaluators from unconsciously favoring certain methods by being unaware of the source of each review.</p>
<p>Two evaluators, each with good expertise in code review, assessed each example by classifying reviews as accurate, partially accurate, or not accurate.A review was considered accurate if it correctly addresses issues without any errors or irrelevant information.If only some parts of the review are valid, and the rest are irrelevant or incorrect, it was deemed partially accurate.Finally, a review was classified as not accurate if it fails to address the identified issues and is completely irrelevant to the context.After the initial assessments, a few conflicts were identified and resolved through discussion, leading to a consensus on the final evaluations.</p>
<p>For RQ2, we conducted a sanity check to evaluate the ability of LLMs, specifically Llama3-70B, to assess review comments reliably and accurately.Using the same 10% subset from RQ1, we tasked Llama3-70B with categorizing each review as accurate, partially accurate, or not accurate.We then measured the agreement between the ratings of human evaluators and those of Llama3-70B by calculating Cohen's kappa, a statistical measure that quantifies inter-rater reliability for categorical data [37], [50].This analysis provides insight into the alignment between LLM and human judgments, helping validate the reliability of LLMs in accurately assessing review comments.</p>
<p>To address RQ3, we instructed Llama3-70B to evaluate the code changes across the full dataset of 1, 245 samples, categorizing each review comment into one of three categories: accurate, partially accurate, or not accurate.</p>
<p>For RQ4, We conducted a comparative evaluation by ranking the generated reviews based on their coverage, which measures how effectively the smaller, fine-tuned LLM detects a wide range of code issues.To assess this, we used the larger LLM, Llama3-70B, as a reference point, assuming it can identify a comprehensive set of issues.The objective was to evaluate how well the smaller model aligns with this benchmark by capturing as many of these issues as possible.The ranking process reflects the relative effectiveness of each model in identifying coding issues, with Rank 1 assigned to the most comprehensive reviews and Rank 5 to the least.</p>
<p>We then analyzed the win-tie-loss ratios to compare the performance of our three proposed strategies (DAT, RAG, NCO) against the baseline models, M i and the static analyzer.A win was recorded if a review was ranked at least two levels higher than the baseline, indicating substantially superior coverage, while a loss was recorded when a review ranked at least two levels lower, reflecting significantly weaker coverage.Rankings within ±1 level were classified as a tie, as the difference in coverage was considered minor.This approach ensured a more meaningful evaluation by emphasizing significant differences in issue coverage rather than minor variations.</p>
<p>C. Results</p>
<p>1) Results for RQ1 -Manual evaluation of reviews' accuracy:</p>
<p>The manual evaluation results in Figure 8 reveal accuracy differences among the various code review generation approaches.As expected, static analyzers (KBS) consistently demonstrate reliability in producing accurate reviews, whereas Fig. 7: Merging LBS and KBS Changes into a Union Diff the LBS model M i yields less accurate reviews, leading to a higher proportion of partially accurate or inaccurate feedback.Our hybrid approaches exhibit diverse accuracy levels compared to the baseline systems (i.e., KBS and LBS).Notably, the RAG approach significantly outperforms M i in terms of accuracy.Meanwhile, the fine-tuned model M F T produced through the data-augmented training (DAT) approach, and the concatenation approach (NCO) achieve accuracy levels similar to that of M i .</p>
<p>In response to RQ1, we conclude that an LLM utilizing the RAG approach generates comments with notably higher accuracy than when used alone.This improvement in accuracy is substantial, although it still falls short of the accuracy achieved by static analyzers.We did not observe an improvement in accuracy for the two other combination approaches.2) Results for RQ2 -Alignment between manual and LLM evaluations: The alignment between the ratings of human evaluators and those provided by Llama3-70B was measured using Cohen's kappa, yielding a score of 0.72.This score indicates substantial agreement, highlighting the LLM's capability to accurately categorize reviews as "accurate," "partially accurate," or "not accurate."Such a high level of concordance suggests that Llama3-70B closely mirrors human evaluations in assessing the quality of code reviews, reinforcing its reliability as a substitute for human judgment.Moreover, this degree of agreement aligns with findings from similar studies, further validating its effectiveness as an evaluator.</p>
<p>To answer RQ2, this substantial alignment between Llama3-70B and human evaluations enables us to confidently employ the LLM as the evaluator for the entire test set, assessing both accuracy and coverage.Given the observed agreement, we trust that the model will deliver consistent and reliable evaluations across a broader dataset, allowing for efficient and scalable assessment without compromising on quality.</p>
<p>3) Results for RQ3 -LLM evaluation of reviews' accuracy: We employed Llama3-70B as an evaluator to assess the review comments generated by each of the five approaches on the test set comprising 1, 245 samples.Each sample included a code change along with five review comments, one from each approach.Figure 9 presents the results of this evaluation.The findings are consistent with those from the manual evaluation in RQ1.As expected, static analyzers (KBS) maintain the highest percentage of accurate reviews due to their deterministic nature in identifying recurring and straightforward coding issues.Similarly, the Retrieval-Augmented Generation (RAG) approach outperforms the original model M i although the improvement in accuracy is slightly less pronounced than what was observed in the smaller RQ1 sample.</p>
<p>The Naive Concatenation of Outputs (NCO) approach achieves comparable accuracy to M i , demonstrating its capability to produce reliable reviews.In contrast, the Data-Augmented Training (DAT) approach yielded the lowest accuracy scores among the methods, but still the same levels of NCO and M i .</p>
<p>In conclusion, the answer to RQ3 is that the RAG approach offers the most significant accuracy improvement over using the LLM alone.The other combination methods, NCO and DAT, show accuracy levels comparable to the standalone LLM model, without notable gains.For the baseline methods, the distribution aligns with expectations.The LBS model displays a broad spread across ranks, with the highest concentration in Rank 4 and less than half of the samples in Ranks 1, 2, and 3.This pattern reflects LBS's real-world training data, likely capturing the selective coverage patterns typical in code review datasets.As a result, LBS effectively identifies common issues but lacks the systematic, rule-based coverage characteristic of KBS.Static analyzers, in contrast, are predominantly ranked in Ranks 4 and 5 due to their rule-based approach, which, while precise, lacks adaptability and the capacity to detect a broader range of issues.This rule-bound structure restricts static analyzers to specific, predictable issue types, limiting their ability to provide comprehensive coverage as LBS might.</p>
<p>The DAT approach has a high proportion of reviews achieving Rank 1 (49%), demonstrating strong coverage in these cases.Notably, the DAT distribution is bimodal, with relatively few reviews in Ranks 2, 3, or 4.This suggests that DAT either ranks at the top, addressing the majority of issues in a code change, or falls to Rank 5 when its coverage is limited.This polarized performance pattern in M F T after data augmentation shows that it either achieves comprehensive coverage or misses key areas, rarely achieving intermediate coverage levels.RAG performs consistently well, with most reviews in Ranks 1 and 2 (70%).This suggests that RAG reliably covers a substantial number of issues in code changes, with a notable share in Rank 2, indicating that it frequently provides strong, if not complete, coverage.</p>
<p>The NCO approach primarily appears in middle ranks, especially in Rank 3, indicating moderate coverage.This outcome reflects the straightforward concatenation process, which combines reviews from LBS and KBS without advanced integration.As a result, NCO's coverage is influenced by the quality of the LBS review; when LBS has low coverage, NCO's coverage also tends to be limited.However, the addition of static analyzer reviews offers slight improvements, giving NCO somewhat broader coverage than LBS alone in the top three ranks.</p>
<p>In summary, DAT and RAG emerge as the top-performing approaches, together accounting for nearly 80% of Rank 1 reviews, each excelling in one of the top two ranks.NCO demonstrates moderate performance, with Rank 3 as its most common position.</p>
<p>To enable a direct comparison between the combination approaches and baselines, we conducted a pairwise win-tieloss analysis based on rankings.As shown in Figure 11, DAT has a high win rate against both LBS and KBS, highlighting its effectiveness in achieving broader coverage.RAG also performs well, consistently identifying a wide range of issues and frequently surpassing LBS in coverage.In contrast, while NCO positions favorably against KBS, it shows a higher tie rate with LBS, indicating that it generally achieves moderate coverage but does not consistently outperform LBS.Cases where NCO provides less coverage than KBS can arise when LBS-generated reviews contradict or override KBS output, reducing the overall coverage of the concatenated result.For example, in one instance, the KBS produced the following comment: "Unused import 'com.google.common.collect.ImmutableList.'"However, the LBS-generated review effectively invalidated this observation: "The code change adds a new import statement for com.google.common.collect.ImmutableList.This is a good addition, as it allows the class to use the ImmutableList class, which can be useful for creating immutable lists."In such cases, the conflicting feedback from LBS can diminish the impact of KBS findings, leading to lower coverage in the NCO approach.</p>
<p>To answer RQ4, we conclude that two of the combination approaches, DAT and RAG, significantly enhance the coverage of issues that can be automatically identified compared to the baselines.</p>
<p>D. Discussion</p>
<p>Based on the results from RQ3 and RQ4, our baseline models performed as expected, each with distinct strengths and limitations.The knowledge-based system (KBS) demonstrated high accuracy in detecting rule-based issues, particularly for violations that could be systematically encoded.However, its coverage was limited, missing many complex issues that require a nuanced understanding of context, which cannot easily be captured through static rules alone.</p>
<p>In cases where the KBS was deemed inaccurate, its reliance on predefined rules is often the primary cause, as these rules may generate irrelevant or outdated reviews.Since they do not always align with the specific context of the code being reviewed, they can lead to misjudgments.For example, rules based on fixed thresholds (e.g., maximum method length or parameter list size) may result in incorrect assessments, particularly when they fail to account for context-specific factors.</p>
<p>For example, as seen in Table I, KBS generated a review comment addressing a basic violation of naming conventions, which is straightforward but ranks lowest.In contrast, the learning-based system M i , ranked fourth, identified a more subtle issue that would be challenging to encode in a rule-based analyzer, highlighting the advantage of LBS in identifying context-dependent issues.</p>
<p>The Data Augmented Training (DAT) approach expanded the coverage beyond M i by exposing the model to a wider variety of issues, as the augmented dataset Da allowed the model to handle both LBS and KBS issues more effectively.This exposure enabled DAT to consider contextual factors and recognize project-specific nuances that KBS would typically overlook, while also incorporating rule-based issues where appropriate.For example, in Table I, DAT achieved the highest rank by generating a detailed and context-sensitive review comment that covered both rule-based and nuanced observations, reflecting its expanded coverage capabilities.</p>
<p>The Retrieval-Augmented Generation (RAG) approach enhanced both the accuracy and coverage of generated reviews by incorporating information retrieved from KBS.This retrieval step enriched RAG's feedback, making it more comprehensive and relevant.However, it still fell short of KBS's precision on rule-based issues due to M i 's inherent inaccuracies during inference.In Table I, RAG's comment, ranked second, provides a more detailed critique than the baseline models, yet it lacks the depth of DAT's analysis.</p>
<p>The Naive Concatenation of Outputs (NCO) strategy did not notably improve accuracy and offered only a marginal increase in coverage.Since part of the NCO approach's output is directly inherited from the LBS, its accuracy is inherently tied to the quality of the LBS-generated comments.If the LBS produces a partially accurate comment, the corresponding NCO output will also reflect this partial accuracy, as the NCO does not refine or correct LBS-generated reviews.</p>
<p>Table I illustrates an NCO's generated comment, ranked third, integrated feedback from both baselines, but without advanced synthesis, resulting in a moderate improvement.Although NCO included the LBS and KBS comments, it was unable to address the more complex issues that DAT and RAG identified.</p>
<p>In summary, DAT and RAG stood out for their enhanced coverage, capturing nearly 80% of Rank 1 assessments.NCO, by contrast, demonstrated only moderate performance, frequently achieving Rank 3 due to its limited integration of baseline insights.This analysis underscores the strengths of hybrid approaches in combining rule-based precision with learning-based adaptability to achieve comprehensive code review coverage.</p>
<p>E. Threats to Validity</p>
<p>During our evaluation process, we identified several potential threats to the validity of our results.The most notable threats are discussed below.</p>
<p>Evaluator Bias: While human evaluations were conducted to assess accuracy in RQ1, there is a potential for evaluator bias.To mitigate this threat, we presented the reviews in an anonymized and randomized order to reduce any unconscious preference for particular methods.</p>
<p>Generalizability: Our study primarily focuses on Java code, using tools and datasets specific to the Java programming Code change @@ -38,5 +38,7 @@ public class GoogleAuthenticatorAccount extends OneTimeTokenAccount { @JsonProperty("validationCode") final int validationCode, @JsonProperty("scratchCodes") final List⟨Integer⟩ scratchCodes) { super(username, secretKey, validationCode, scratchCodes); + long v = java.lang.System.currentTimeMillis();+ this.setId(v); } } DAT #### Potential Issues: -The setId() method uses 'System.currentTimeMillis()' in the constructor, which could lead to issues if the object is not persisted before the constructor finishes.</p>
<p>-The 'id' field should be marked as transient to avoid unnecessary persistence.</p>
<p>-It is possible to remove the final keyword to allow subclasses to override the constructor.#### Areas for Improvement: -Consider adding a 'toString()' method for better readability.</p>
<p>-Validate the constructor parameters (username, secretKey, etc.) to ensure they are not null or empty.</p>
<p>RAG</p>
<p>The use of the short variable name 'v' is discouraged.Consider using more descriptive names like 'currentTimestamp' or 'uniqueIdentifier'.Also, 'System.currentTimeMillis()' may not be the most efficient way to generate a unique identifier; alternatives like 'UUID.randomUUID()' or 'SecureRandom.nextLong()'could be more robust.</p>
<p>NCO</p>
<p>The addition of the 'id' field is unnecessary as the parent class 'OneTimeTokenAccount' already has an 'id' field.Using the existing field would avoid redundancy and confusion.Avoid using short variable names like 'v'.</p>
<p>LBS</p>
<p>The addition of the 'id' field is unnecessary as the parent class 'OneTimeTokenAccount' already has an 'id' field.Using the existing field would avoid redundancy and confusion.</p>
<p>KBS</p>
<p>Avoid using short variable names like 'v'.language.This may limit the generalizability of our findings to other languages and contexts, where static analyzers or learning-based systems could exhibit different strengths and weaknesses.</p>
<p>Dependency on LLM Judgment: For RQ3 and RQ4, we relied on an LLM (Llama3-70B) to evaluate accuracy and coverage across the entire test set.Although Cohen's kappa indicates substantial agreement with human evaluations on a subset of data, subtle variations in the LLM's judgment across the full dataset may not fully align with human assessments, potentially impacting reliability.To address this, in addition to the kappa score, we manually inspected a random subset of evaluations for RQ4 and found them consistent and reliable.</p>
<p>Choice of Static Analyzers: The effectiveness of the KBS approach depends on the static analyzers selected.We chose PMD and Checkstyle, which we believe are state-of-the-art tools; however, other static analyzers might yield different results.Our confidence in these tools is based on their widespread use and established effectiveness in rule-based code analysis.</p>
<p>Evaluation Metrics Limitations: The ranking system (wintie-loss) used in RQ4 considers the number of levels above or below baseline models, but it may not capture subtle differences in review quality.To account for this, we adopted a conservative approach, requiring at least a two-level ranking difference for a "win," to ensure that only substantial improvements were counted as wins.</p>
<p>V. RELATED WORK A. Code review automation</p>
<p>Several approaches were proposed to assist code review.Gupta et al. [51] introduced an LSTM-based model trained on positive and negative (code, review) pairs, selecting candidate reviews based on code similarity and predicting relevance scores.Siow et al. [11] enhanced this with multilevel embeddings, leveraging word-level and character-level representations to better capture the semantics of code and reviews.</p>
<p>With the advent of large language models, the focus has shifted toward generative models to fully automate code review tasks.Tufano et al. [52] developed a transformerbased model to suggest code edits before and after code review, later enhancing it by pre-training T5 on Java and technical English [26].Li et al. [25] pre-trained CodeT5 on a multilingual dataset and fine-tuned it for code review tasks like quality estimation, review generation, and code refinement.Sghaier et al. [5] further advanced this area by applying crosstask knowledge distillation to address successive code review tasks jointly, enhancing performance and promoting tasks' interdependence.</p>
<p>Current research efforts have significantly advanced the automation of code review, introducing different and innovative approaches that enhance code review tasks.However, the performance of these automated approaches remains limited in terms of correctness, as indicated by low BLEU scores, suggesting that further refinement is needed to achieve higher accuracy and reliability as expected in practical software development contexts.</p>
<p>B. LLMs and static analysis combination</p>
<p>Recent research has increasingly focused on enhancing LLM-based solutions for software engineering using several techniques.One is by integrating them with static analysis tools, addressing the challenge of reducing inaccurate or incomplete results.</p>
<p>In automated program repair, RepairAgent [53] employs static analysis to gather contextual data that guides LLMdriven code correction, while PyTy [54] relies on typechecking mechanisms to validate the accuracy of LLMgenerated candidates in resolving static type inconsistencies.For software testing, approaches like TECO [55] apply static analysis to derive semantic features for training transformers in test completion, while ChatTester [56] and TestPilot [57] utilize similar techniques to prepare contextual information that supports iterative LLM-based test code repair processes.For bug detection, LLMs were combined with static analysis to reduce false positives.SkipAnalyzer [58] and GPTScan [59] use static analysis to validate LLM predictions, while D2A [60] and ReposVul [61] refine bug labeling and re-rank predictions.For code completion, STALL+ [62] integrates static analyzers with LLMs through a multi-phase approach involving prompting, decoding, and post-processing.</p>
<p>These studies illustrate the effectiveness of combining LLMs with static analysis across tasks like program repair, bug detection, testing, and code completion.However, this integration has yet to be explored for code review.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we explored hybrid approaches that combine knowledge-based systems (KBS) with large language models (LLMs) to enhance the automation of code review.By integrating KBS-derived insights at three stages-data preparation (Data-Augmented Training, DAT), inference (Retrieval-Augmented Generation, RAG), and post-inference (Naive Concatenation of Outputs, NCO)-we aimed to leverage the strengths of both KBS and learning-based systems (LBS) to generate more accurate and comprehensive code review comments.Our empirical evaluation showed that combination approaches offer distinct advantages.RAG emerged as the most effective, improving both accuracy and coverage of review comments by enriching the generation process with structured, contextually relevant knowledge from static analysis tools.DAT achieved broad coverage by exposing the LLM to diverse issue types in training, sometimes at the expense of precision.NCO, while straightforward, achieved moderate improvements in coverage.</p>
<p>These findings underscore the potential of combining static analysis tools with LLMs to address the limitations of individual approaches in automated code review.Future work will involve exploring more sophisticated integration of knowledge into open-weight LLMs.We also plan to expand this methodology to additional programming languages and exploring bytecode-level analysis for greater depth.Furthermore, the integration of more advanced static analyzers and dynamic analysis tools could further enhance coverage and precision, ultimately contributing to a more robust and versatile code review automation pipeline.</p>
<p>Figure 1
1
Figure 1 illustrates our approach, outlining three strategies to combine Knowledge-Based Systems (KBS) and Learning-Based Systems (LBS) to enhance code review automation.</p>
<p>Fig. 1 :
1
Fig. 1: Different strategies to combine learning and knowledge-based systems</p>
<p>Fig. 2 :
2
Fig. 2: Proposed strategies to combine LBS and KBS</p>
<p>Fig. 4 :
4
Fig. 4: Judgment of review comments using Llama3-70B</p>
<p>Fig. 6 :
6
Fig. 6: Prompt used to generate review comments using RAG</p>
<p>RQ2:</p>
<p>Alignment of LLM judgments with human assessments (sanity check): To what extent do LLM judgments align with human assessments?</p>
<p>Fig. 8 :
8
Fig. 8: Accuracy levels for the different models based on human assessments</p>
<p>Fig. 9 :
9
Fig. 9: Accuracy levels for the different models based on LLM judgments 4) Results for RQ4 -Ranking of Reviews Based on Coverage Criteria Using LLM: Figure 10 presents the cumulative ranking distribution for the five code review generation approaches: the baseline methods (KBS and LBS) and the combined approaches (DAT, RAG, and NCO).Rankings assess each approach's effectiveness in achieving comprehensive issue coverage, with Rank 1 indicating the highest coverage and Rank 5 the least.</p>
<p>Fig. 10 :
10
Fig. 10: Distribution of Ranks Across Models</p>
<p>Fig. 11 :
11
Fig. 11: Win-tie-loss ratios of our proposed approaches compared to the baseline in terms of coverage.TABLE I: Comparison of Review Comments.Excerpts of comments are shown for brevity.</p>
<p>The replication package is available at https://github.com/ImenJaoua/Hybrid-Code-Review and the data is available at https://zenodo.org/records/14061110.
A review of code reviewer recommendation studies: Challenges and future directions. H A C ¸etin, E Dogan, E Tüzün, Science of Computer Programming. 2081026522021</p>
<p>A survey on source code review using machine learning. W Xiaomeng, Z Tao, X Wei, H Changyu, 2018 3rd International Conference on Information Systems Engineering (ICISE). IEEE2018</p>
<p>Commentfinder: a simpler, faster, more accurate code review comments recommendation. Y Hong, C Tantithamthavorn, P Thongtanunam, A Aleti, Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering. the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering2022</p>
<p>Four eyes are better than two: On the impact of code reviews on software quality. G Bavota, B Russo, 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE2015</p>
<p>Improving the learning of code review successive tasks with cross-task knowledge distillation. O , Ben Sghaier, H Sahraoui, FSE. 20241</p>
<p>Finding bugs is easy. D Hovemeyer, W Pugh, Acm sigplan notices. 39122004</p>
<p>Tricorder: Building a program analysis ecosystem. C Sadowski, J Van Gogh, C Jaspan, E Soderberg, C Winter, 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering. IEEE20151</p>
<p>Core: Resolving code quality issues using llms. N Wadhwa, J Pradhan, A Sonwane, S P Sahu, N Natarajan, A Kanade, S Parthasarathy, S Rajamani, Proceedings of the ACM on Software Engineering. 12024FSE</p>
<p>Code review automation: strengths and weaknesses of the state of the art. R Tufano, O Dabić, A Mastropaolo, M Ciniselli, G Bavota, IEEE Transactions on Software Engineering. 2024</p>
<p>Towards contextually aware large language models for software requirements engineering: A retrieval augmented generation framework. M S Ibtasham, 2024</p>
<p>Core: Automating review recommendation for code changes. J K Siow, C Gao, L Fan, S Chen, Y Liu, 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2020</p>
<p>Automating code review. R Tufano, 2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). IEEE2023</p>
<p>Z Yang, C Gao, Z Guo, Z Li, K Liu, X Xia, Y Zhou, arXiv:2405.18216A survey on modern code review: Progresses, challenges and opportunities. 2024arXiv preprint</p>
<p>Code generation using machine learning: A systematic review. E Dehaerne, B Dey, S Halder, S De Gendt, W Meert, Ieee Access. 104552022</p>
<p>Evaluating how static analysis tools can reduce code review effort. D Singh, V R Sekar, K T Stolee, B Johnson, 2017 IEEE symposium on visual languages and human-centric computing (VL/HCC). IEEE2017</p>
<p>Evaluating bug finderstest and measurement of static code analyzers. A Delaitre, B Stivalet, E Fong, V Okun, 2015 IEEE/ACM 1st International Workshop on Complex Faults and Failures in Large Software Systems (COUFLESS). IEEE2015</p>
<p>Using static analysis to find bugs. N Ayewah, W Pugh, D Hovemeyer, J D Morgenthaler, J Penix, IEEE software. 2552008</p>
<p>On adopting linters to deal with performance concerns in android apps. S Habchi, X Blanc, R Rouvoy, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software Engineering2018</p>
<p>FindBugs. 2005</p>
<p>PMD. 2000</p>
<p>Checkstyle. 2001</p>
<p>. " Sonarsource, Sonarqube, 2006</p>
<p>Static code analysis tools: A systematic literature review. D Stefanović, D Nikolić, D Dakić, I Spasojević, S Ristić, Ann. DAAAM Proc. Int. DAAAM Symp. 202031</p>
<p>Analyzing the state of static analysis: A large-scale evaluation in open source software. M Beller, R Bholanath, S Mcintosh, A Zaidman, 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE20161</p>
<p>Automating code review activities by large-scale pre-training. Z Li, S Lu, D Guo, N Duan, S Jannu, G Jenks, D Majumder, J Green, A Svyatkovskiy, S Fu, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Using pre-trained models to boost code review automation. R Tufano, S Masiero, A Mastropaolo, L Pascarella, D Poshyvanyk, G Bavota, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. V Balachandran, 2013 35th International Conference on Software Engineering (ICSE). IEEE2013</p>
<p>Auger: automatically generating review comments with pre-training models. L Li, L Yang, H Jiang, J Yan, T Luo, Z Hua, G Liang, C Zuo, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Llama-reviewer: Advancing code review automation with large language models through parameterefficient fine-tuning. J Lu, L Yu, X Li, L Yang, C Zuo, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). IEEE2023</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, arXiv:2203.134742022arXiv preprint</p>
<p>R Li, L B Allal, Y Zi, N Muennighoff, D Kocetkov, C Mou, M Marone, C Akiki, J Li, J Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, R Sauvestre, T Remez, arXiv:2308.129502023arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, M Wang, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Business insights using rag-llms: a review and case study. M Arslan, S Munawar, C Cruz, Journal of Decision Systems. 2024</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. L Zhu, X Wang, X Wang, arXiv:2310.176312023arXiv preprint</p>
<p>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. L Shi, W Ma, S Vosoughi, arXiv:2406.077912024arXiv preprint</p>
<p>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. A S Thakur, K Choudhary, V S Ramayapally, S Vaidyanathan, D Hupkes, arXiv:2406.126242024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>A critical comparison on six static analysis tools: Detection, agreement, and precision. V Lenarduzzi, F Pecorelli, N Saarimaki, S Lujan, F Palomba, Journal of Systems and Software. 1981115752023</p>
<p>Comparing bug finding tools for java open source software. E H Oskouei, O Kalıpsız, 2018</p>
<p>Why don't software developers use static analysis tools to find bugs?. B Johnson, Y Song, E Murphy-Hill, R Bowdidge, 2013 35th International Conference on Software Engineering (ICSE). IEEE2013</p>
<p>The effectiveness of supervised machine learning algorithms in predicting software refactoring. M Aniche, E Maziero, R Durelli, V H Durelli, IEEE Transactions on Software Engineering. 4842020</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are taskspecific classifiers. H Huang, Y Qu, J Liu, M Yang, T Zhao, arXiv:2403.028392024arXiv preprint</p>
<p>Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. M Weyssow, A Kamanda, H Sahraoui, arXiv:2403.090322024arXiv preprint</p>
<p>Active retrieval augmented generation. Z Jiang, F F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, arXiv:2305.069832023arXiv preprint</p>
<p>Replication package. </p>
<p>Datasets and results. Anonymous, 2024</p>
<p>Interrater reliability: the kappa statistic. M L Mchugh, Biochemia medica. 2232012</p>
<p>Intelligent code reviews using deep learning. A Gupta, N Sundaresan, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'18) Deep Learning Day. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'18) Deep Learning Day2018</p>
<p>Towards automating code review activities. R Tufan, L Pascarella, M Tufanoy, D Poshyvanykz, G Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). 2021</p>
<p>Repairagent: An autonomous, llm-based agent for program repair. I Bouzenia, P Devanbu, M Pradel, arXiv:2403.171342024arXiv preprint</p>
<p>Pyty: Repairing static type errors in python. Y W Chow, L Di Grazia, M Pradel, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Learning deep semantics for test completion. P Nie, R Banerjee, J J Li, R J Mooney, M Gligoric, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>No more manual tests? evaluating and improving chatgpt for unit test generation. Z Yuan, Y Lou, M Liu, S Ding, K Wang, Y Chen, X Peng, arXiv:2305.042072023arXiv preprint</p>
<p>Adaptive test generation using a large language model. M Schäfer, S Nadi, A Eghbali, F Tip, arxiv:2302.065272023</p>
<p>Skipanalyzer: A tool for static code analysis with large language models. M M Mohajer, R Aleithan, N S Harzevili, M Wei, A B Belle, H V Pham, S Wang, arXiv:2310.185322023arXiv preprint</p>
<p>Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis. Y Sun, D Wu, Y Xue, H Liu, H Wang, Z Xu, X Xie, Y Liu, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>D2a: A dataset built for ai-based vulnerability detection methods using differential analysis. Y Zheng, S Pujar, B Lewis, L Buratti, E Epstein, B Yang, J Laredo, A Morari, Z Su, 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE2021</p>
<p>Reposvul: A repository-level high-quality vulnerability dataset. X Wang, R Hu, C Gao, X.-C Wen, Y Chen, Q Liao, Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings. the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings2024</p>
<p>Stall+: Boosting llm-based repository-level code completion with static analysis. J Liu, Y Chen, M Liu, X Peng, Y Lou, arXiv:2406.100182024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
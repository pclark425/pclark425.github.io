<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7581 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7581</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7581</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-142.html">extraction-schema-142</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-270357302</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.04926v1.pdf" target="_blank">Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown exceptional performance in text processing. Notably, LLMs can synthesize information from large datasets and explain their decisions similarly to human reasoning through a chain of thought (CoT). An emerging application of LLMs is the handling and interpreting of numerical data, where ﬁne-tuning enhances their performance over basic inference methods. This paper proposes a novel approach to training LLMs using knowledge transfer from a random forest (RF) ensemble, leveraging its eﬃciency and accuracy. By converting RF decision paths into natural language statements, we generate outputs for LLM ﬁne-tuning, enhancing the model’s ability to classify and explain its decisions. Our method includes verifying these rules through established classiﬁcation metrics, ensuring their correctness. We also examine the impact of preprocessing techniques on the representation of numerical data and their inﬂuence on classiﬁcation accuracy and rule correctness.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7581",
    "paper_id": "paper-270357302",
    "extraction_schema_id": "extraction-schema-142",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0044515,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models
7 Jun 2024</p>
<p>Michał Romaszewski mromaszewski@iitis.pl 
Institute of Theoretical and Applied Informatics
Polish Academy of Sciences
Bałtycka 544-100GliwicePoland</p>
<p>Przemysław Sekuła 
Institute of Theoretical and Applied Informatics
Polish Academy of Sciences
Bałtycka 544-100GliwicePoland</p>
<p>Przemysław Głomb 
Institute of Theoretical and Applied Informatics
Polish Academy of Sciences
Bałtycka 544-100GliwicePoland</p>
<p>Michał Cholewa 
Institute of Theoretical and Applied Informatics
Polish Academy of Sciences
Bałtycka 544-100GliwicePoland</p>
<p>Katarzyna Kołodziej 
Institute of Theoretical and Applied Informatics
Polish Academy of Sciences
Bałtycka 544-100GliwicePoland</p>
<p>Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models
7 Jun 20244C2F92B4E1BFEB23EBEE0078A59D07B8arXiv:2406.04926v1[cs.CL]
Large Language Models (LLMs) have shown exceptional performance in text processing.Notably, LLMs can synthesize information from large datasets and explain their decisions similarly to human reasoning through a chain of thought (CoT).An emerging application of LLMs is the handling and interpreting of numerical data, where fine-tuning enhances their performance over basic inference methods.This paper proposes a novel approach to training LLMs using knowledge transfer from a random forest (RF) ensemble, leveraging its efficiency and accuracy.By converting RF decision paths into natural language statements, we generate outputs for LLM fine-tuning, enhancing the model's ability to classify and explain its decisions.Our method includes verifying these rules through established classification metrics, ensuring their correctness.We also examine the impact of preprocessing techniques on the representation of numerical data and their influence on classification accuracy and rule correctness.</p>
<p>Introduction</p>
<p>Large Language Models (LLM) are deep neural networks based on transformer architecture Vaswani et al. [2017] which demonstrated exceptional performance in various tasks such as question answering Yeo et al. [2023], solving problems requiring general knowledge Ge et al. [2024], code generation Chen et al. [2021] or time-series understanding and processing Zhang et al. [2024].A notable feature of LLMs is their ability to synthesize information from large datasets and explain their decisions in a way that is similar to human reasoning.In particular, they can generate a chain of thought (CoT) Wu et al. [2024], presenting statements that support the model's conclusions.Consequently, LLM may be the first method to provide efficient explanations and insights across multiple specialized domains using a single general model.</p>
<p>An emerging application of LLMs is handling and interpreting numerical data, such as in tabular data mining Fang et al. [2024] or classification tasks Dinh et al. [2022], both of which involve matching patterns in numerical features relative to specific classes.While a basic approach relies on the model's general knowledge for zero-shot, one-shot, or few-shot inference, yielding sometimes accurate results Hou et al. [2024], Zhao et al. [2023], fine-tuning the LLM Dinh et al. generate accurate labels and corresponding rule-based explanations.</p>
<ol>
<li>
<p>We investigate LLMs' comprehension of numerical data, proposing several pre-processing operations that significantly impact the correctness of LLMs' decision explanations.</p>
</li>
<li>
<p>We present a study where an LLM-based model replaces a traditional classification model through knowledge transfer, utilizing random forest ensemble rules transformed into LLM outputs.</p>
</li>
<li>
<p>Our method generates explanations alongside labels, providing arguments to justify decisions and potentially improving model performance by employing the chain of thought (CoT) approach.</p>
</li>
<li>
<p>We introduce a method to verify the correctness of rules generated by LLMs by applying them to the training set and assessing their impact using established classification quality measures, providing an objective assessment of LLM outputs beyond classification accuracy.</p>
</li>
</ol>
<p>Related work</p>
<p>Mathematical Reasoning Since their introduction, LLMs have aimed to effectively represent and process natural language, including the task of handling mathematical reasoning.A significant challenge involves the accurate identification and manipulation of signs and symbols, particularly numbers, within the text.Mathematical reasoning is a crucial area that demonstrates the potential of LLMs in extracting and utilizing quantitative information to solve numerical tasks.However, this field is still in its early stages.Wei et al. [2023b] evaluated several LLM models on the CMATH Dataset of elementary school math word problems, revealing that only GPT-4 achieved the required accuracy across all grade levels.Most models showed decreased performance accuracy with increasing arithmetic or reasoning complexity, indicating that LLMs still need to be capable of handling a wide range of numerical problems.Scientific research on LLMs for mathematical reasoning focuses on developing prompting techniques and fine-tuning methodologies Ahn et al. [2024].Imani et al. [2023] propose the Math-Prompter model based on zero-shot CoT prompting to address arithmetic tasks by dividing them into simple intermediate steps and generating multiple methods (Python code and analytical expressions) to solve these steps, validating each step's correctness.Similarly, Yamauchi et al. [2023] proposes a framework that integrates CoT prompting with an external Python REPL tool to correct CoT mistakes.Self consistency Wang et al. [2023] is a prompting method that utilizes multiple CoT reasoning paths, selecting the final answer by majority voting.A recent example of a fine-tuning-based method is the MetaMath model Yu et al. [2024], which bootstraps mathematical questions in the training dataset by rephrasing them to reflect forward and backward reasoning.Magister et al. [2023] propose improving arithmetic solving performance through knowledge distillation, fine-tuning a small LLM model with CoT generated by a larger teacher model.</p>
<p>One critical aspect of handling numbers in LLMs is tokenization.Dividing complex numbers into multiple tokens can hinder their accurate interpretation.Several tokenization methods have been developed.Leading LLM models, GPT-3.5 and GPT-4, use separate tokens for 1-, 2-, and 3-digit representations, while LLaMa and PaLM use a one-digit tokenization scheme.It has been shown that a right-to-left tokenization scheme (enforced by comma separation of numbers) improves mathematical reasoning performance compared to the traditional left-toright technique Singh and Strouse [2024].A notable approach to addressing the tokenization issue is the number decomposition method Muffo et al. [2023], which supports each number in the prompt with an automatically generated textual description.</p>
<p>Prompting techniques Formulating queries (prompting) is crucial for enhancing LLM performance, with two primary methods being zero-shot and few-shot learning Brown et al. [2020].Few-shot Chain of Thought (CoT) prompting Wei et al. [2023a] introduces intermediate reasoning steps in prompts, improving performance and providing reasoning as part of the model's output.Building on this, zero-shot CoT prompting Kojima et al. [2023] uses a single prompt template, 'Let's think step by step', to encourage task-agnostic reasoning.The simplicity and effectiveness of CoT prompting have led to various extensions, such as sub-problems divisiondecomposing problems into simpler sub-problems, external assistance -using external knowledge sources or tools Yu et al. [2023], and CoTGenius -enhancing initial CoT prompts with derived topics and detailed reasoning paths Cheng et al. [2024].</p>
<p>Another recent proposal is complexity-based prompting Fu et al. [2023], which selects examples with complex reasoning chains to improve multi-step reasoning.Federated prompting Liu et al. [2023] addresses frequently asked questions with identical reasoning paths by federating synonymous questions and using Self-consistency with majority voting or CoT to ensure consistent and practical solutions.</p>
<p>XAI The concept of AI system explainability has been explored even for classical ML blackbox systems, including neural networks and kernel SVMs.In Hoffman et al. [2018], various approaches to providing explanations are discussed, highlighting the importance of helping users understand model operations.This research is increasingly critical for complex models like Deep Neural Networks and LLMs.Dwivedi et al. [2023] provides a comprehensive survey of contemporary methods, from Shapley additive explanations (SHAP) to individual conditional expectation (ICE) and partial dependancy plot (PDP), focusing on one or a few features.These methods are widely used in decision-making processes in areas such as recommender systems Gawde et al. [2024], medical data Van der Velden et al. [2022], pharmaceutical engineering Polzer et al. [2022], where XAI verifies ML responses and classifications.In fields such as cybersecurity Srivastava et al. [2022], the use of XAI is particularly challenging due to the adversarial nature of the domain.</p>
<p>Advancements in natural language processing and LLMs have introduced new tools for AI decision-making explanations through natural language.Cambria et al. [2023] outlines the advantages of natural language explanations, including comprehensiveness and increased trust in decision-making.Works such as Luo and Specia [2024] highlight XAI's positive impact on improving LLM results.</p>
<p>Another intensively researched area involves systems that interact with users; for instance, Yu et al. [2022] proposes a two-step XAI generative framework for providing arguments for classification labelling, while Nguyen et al. [2023] incorporates XAI into a conversational agent.</p>
<p>Method</p>
<p>Given a dataset X ∈ R N ×F with N examples and F features and a set of labels Y, our input data to train the model is a training set T train = {(x i , y i )} i=1,...,|Ttrain| where an example x i ∈ X and its label y i ∈ Y.Our goal is to create an LLM classifier that labels the data and explains its decisions.To do this, we require textual input-output pairs suitable for processing with the sequence-to-sequence (S2S) LLM model.These inputs are created by encoding the feature vectors into a textual form, while outputs are generated by sampling a random forest classifier trained on the set T train .</p>
<p>Preprocessing and substitution of numerical values</p>
<p>As presented in Section 1.1, the format of prompts/outputs and the way the numbers, signs, and symbols are represented significantly impact the ability of LLMs to perform mathematical reasoning.Particularly important for this work is the textual representation of conditional statements in the form: petal length (cm) 3. Relation encoding (RE) -converting inequality signs to words.</p>
<p>We will use the above shortcuts when describing network hyperparameters, e.g.IN+VD+RE denotes that all the options are enabled.</p>
<p>Integer normalisation (IN) Tokenisation has a major impact on how LLM performs arithmetic operations Singh and Strouse [2024].Since LLMs tend to have problems with understanding large numbers and high-precision floating-point values, in our approach, we transform them into 2-digit numerical values encoded by 1 token by GPT-4 tokenisation scheme or 2 tokens by models employing single-digit tokenisation like LLaMa.For every feature, its values are converted into integers from a given range r min , r max .For the data table X ∈ R N ×F , obtained by concatenating vectors in the training set T train , the transformation process is described by the following equation:
X scaled = (X − v min ) (v max − v min ) × (r max − r min ) + r min ,
where X scaled represents the scaled and integer-encoded data and vectors v min , v max ∈ R F contain the minimum/maximum value of each feature.Since the transformation is performed on the training set, encoded values are then clipped to r min , r max .The range used in our experiments was 0, 99 , so e.g. the expression petal length (cm): 4.80' becomes 'petal length (cm): 30 '.The operation is lossy, introducing estimation error into values of example features and thresholds.</p>
<p>Verbal description of values (VD)</p>
<p>The goal of this step is to provide the LLM with an understanding of how the feature values of a given example compare to the entire training set.To support this, we augment numerical values with textual descriptions based on their distribution.This approach is inspired by the box plot method Tukey et al. [1977], relating feature values to their median.Each numerical feature value in the input/output is labelled with one of five class labels:</p>
<p>• Lower outlier: values below the 0.1 percentile • Lower whisker: values around the 25th percentile</p>
<p>• Median: at the 50th percentile • Upper whisker: values around the 75th percentile • Upper outlier: values above the 99.9 percentile.</p>
<p>For example, the text 'petal length (cm): 30 ' becomes 'petal length (cm): 30 (lower whisker)', which indicates that the value is between 0.1 percentile and the 25th percentile.</p>
<p>Relation encoding (RE) We have found out that substituting special signs with their textual descriptions improves the LLM result.This is consistent with observations in Dave et al. [2024].Therefore, to improve the LLM performance, mathematical signs for relations such as (&gt;,=,&lt;) are substituted by their textual description, i.e. 'is less than', 'is equal to', 'is greater than'.</p>
<p>Input (prompt) conversion</p>
<p>The input (prompt) template for every example x ∈ X , can be described as:
prompt header custom header text ft 1 x 1 . . . , ft j x j . . . , ft F x F encoded features prompt footer custom footer text
, where ft j is the name of the jth feature and x j is the value of this feature in vector x.</p>
<p>The prompt header is always the same:</p>
<p>Here is the description of system state:</p>
<p>The format of encoded features depends on preprocessing parameters described in Section 2.1, for example:</p>
<p>• No preprocessing: sepal length (cm): 6.80, sepal width (cm): 2.80, petal length (cm): 4.80, petal width (cm): 1.40</p>
<p>• IN+VD+RE: sepal length (cm): 69 (upper whisker), sepal width (cm): 33 (lower quantile), petal length (cm): 64 (upper quantile), petal width (cm): 54 (upper quantile)</p>
<p>The prompt footer uses a single template, which is also modified to match the format of preprocessing parameters, e.g. for IN+VD+RE (all options enabled):</p>
<p>"Based on values of system features, classify the state of the system and explain the decision.Use logical rules comparing feature values with thresholds using 'is greater/less than'.</p>
<p>Format: '[feature_name] [value] [inequality]</p>
<p>[threshold]', for example: 'feature_name 0 (lower whisker) is less than 10 (upper quantile).Label: 0'.Provide a classification label from uniqe_labels.Explanation and system label:</p>
<p>LLM output prepration</p>
<p>The expected textual LLM output for an input vector x ∈ X is generated by sampling the random forest classifier (RFC) Breiman [2001].This involves randomly selecting a single decision tree classifier (DTC) from the RFC -an ensemble of DTCs that label examples using majority voting -and extracting the decision path for the example x.</p>
<p>Decision path selection in DTC A decision tree classifier is a tree model where internal nodes represent conditional decisions based on feature values, and terminal nodes (leaves) provide class labels derived from these decisions.Given a set of classes Y, the possible classification outcomes form a set of decision paths.A decision path P is defined as:
P = {(n 1 , n 2 , . . . , n H ), y},(2)
where the label y ∈ Y is associated with the terminal node, the decisions at nodes in the path are denoted as n k and H is the length of this path1 .A decision in k-th node n k in the path is defined as a tuple:
n k = (j k , t k , d k ),(3)
where j k &lt; F is a feature index, t k ∈ R is a feature threshold, and d k ∈ {−1, +1} is an indication of whether the right or left path has been selected in the tree, which translates into a conditional statement between the feature value and the threshold.</p>
<p>We define an example x ∈ X as belonging to the path if:
x ∈ P ⇐⇒ ∀ k={1,...,H} x j k ∈ (−∞, t k ) if d k ≤ 0 x j k ∈ (t k , ∞) otherwise,(4)
where x j k denotes the j k -th element (a feature) of the vector x.In other words, an example belongs to the path if it fulfils all of the path conditions.</p>
<p>Sampling the RFC Given a set of decision tree classifiers in the random forest classifier, for every labelled example (x, y) ∈ T train we randomly select a single decision tree classifier and a decision path P in this tree, such that x ∈ P and y = y P where y P is the label assigned by the path P i.e. this decision path correctly classifies the example.We repeat this operation n trees ≥ 1 times, where n trees is a hyperparameter of the algorithm.Each sampling repetition generates a unique tuple (x, y, P), provided that it exists.The set of all returned tuples T LLM will be used to train LLM after the conversion of feature vectors and decision paths into textual forms.It is possible that the size |T LLM | &gt; |T train | if the value n trees &gt; 1.The idea behind this extension is that the LLM may learn to generate explanations that combine multiple decision paths, possibly finding a better description of class features.</p>
<p>Output conversion</p>
<p>Given an example x ∈ X and a feature index j, a statement given by the Eq. 1 can be created from a feature name ft j , feature value x j , the condition and the threshold value t.Therefore, for a decision path P defined by the Eq.(2), and an example x ∈ P, any decision n k in this path, defined by the Eq. ( 3), can be expressed in form of conditional statement given by the Eq. 1.This is because the feature name ft j for any feature index j is known, and the decision given by Eq. ( 3) contains the feature index j k , the condition (as an indicator d k ) and the threshold value t k .This is used to create the LLM output representing a decision path for any labelled example in the form: statement 1 and . . .and statement H</p>
<p>Conditions in textual form</p>
<p>Label: class label</p>
<p>Label of the example</p>
<p>(5)</p>
<p>These textual representations of the decision path undergo postprocessing described in Section 2.1 depending on whether IN, VD, and RE steps are enabled, for example:</p>
<p>No preprocessing: petal length (cm) 4.80 &lt; 4.85 and petal width (cm) 1.40 &gt; 0.80.Label: 1</p>
<p>Preprocessing options IN+VD+RE: petal length (cm) 64 (upper quantile) is less than 64 (upper quantile) and petal width (cm) 54 (upper quantile) is greater than 29 (lower quantile). Label: 1</p>
<p>Random forest classifier parameters: We used a standard scikit-learn Pedregosa et al. [2011] implementation of RF with n = 100 trees.Due to the simplicity of the datasets used and to limit the length of the generated output, we have limited max depth to d max = 2.For every input, we have selected n trees = 2 trees from every forest (almost doubling the size of the training set).</p>
<p>LLM fine-tuning with LoRA</p>
<p>In the experiments, we use the LLM sequence-to-sequence FLAN-T5-base model, based on the T5 model Raffel et al. [2020] and fine-tuned with the FLAN dataset Longpre et al. [2023].We selected FLAN-T5 due to its ability to achieve comparable results to larger models Zhang et al. [2023], with efficient performance and minimal computational overhead.We also tested other architectures, such as FLAN-T5-XL and Llama3, with no better results.</p>
<p>The model was trained using parameter-efficient fine-tuning (PEFT) with LoRA (Low-Rank Adaptation) Hu et al. [2021], targeting the (q, v) model components for the seq2seq task.Training and inference were conducted using the Transformers library Wolf et al. [2020].</p>
<p>During training, the data (pairs of generated inputs/outputs) was split into stratified training, test, and validation sets, ensuring that all input-output pairs with similar inputs are in one set to avoid information leakage between training/test set if n trees &gt; 1.</p>
<p>Training parameters: We used the following LoRA configuration: rank r = 32, scaling factor alpha = 32, targeted model modules (q, v), dropout d = 0.05.The training was performed with learning rate lr = 10 −3 and the number of epochs n = 150.Output generation parameters: maximum new tokens t max = 300, number of beams b n = 6, beam groups b g = 2, diversity penalty p d = 0.5, temperature t = 0.5.</p>
<p>LLM output validation</p>
<p>While objective assessment of classifier accuracy with a classification metric such as balanced accuracy Brodersen et al. [2010] is straightforward, assessment of provided explanations is more complicated.While human evaluation Ziems et al. [2023] is a valid option, it is also costly.Therefore, we propose to verify the correctness of the explanation based on the properties of the subset of the training set T train designated by this explanation.As explained in Section 2.1.2,for any training example x ∈ X train the decision path P of the RFC trained on the set T train can be represented in the textual form given by the Eq. 5. Therefore, for any example x ∈ X , we expect that the output of the trained LLM will represent and can be parsed into an LLM-generated decision path P LLM .We claim that conditions (as defined by the Eq. ( 3)) in this path explain its label -the subset of the training set determined by these conditions belongs to the class assigned by the path P LLM .</p>
<p>To verify the correctness of this claim we find the subset T predicted ⊂ T train , such that for every x ∈ T predicted ⇐⇒ x ∈ P LLM and expect that this subset contains mostly examples of the class y ∈ P LLM , assigned by the LLM.We verify the correctness of LLM explanation using precision and recall metrics applied to the set T predicted , using the predicted label from LLM output and true labels from the training set.</p>
<p>The validation is performed by parsing the rules with a parser that transforms the LLM output into Python code.We use a dedicated regular expression-based parser, but alternatively, this task can be performed by querying a large language model (ChatGPT 3.5) using the following prompt template (modified, depending on preprocessing described in Section 2.1):</p>
<p>Translate decision tree rules "feature (value) operator threshold" directly into Python pandas code to filter features_df.The output should strictly be Python code, consisting of filtering commands only.For "petal width &lt;= 0.80",provide filtered_df = filtered_df[filtered_df["petal width"]  This code is applied to the data frame and a label vector in the set T train to compute performance metrics.</p>
<p>Datasets</p>
<p>We are using three well-known datasets: iris, wine, and breast cancer.</p>
<p>Results</p>
<p>Results are presented in Table 1 for experiments with relation encoding (RE) enabled and Table 2 for RE disabled.These tables show label accuracy, mean statement accuracy, and recall from assessing explanation correctness as detailed in Section 2.2.These metrics describe the average properties of the subset T predicted ⊂ T train indicated by LLM explanations.Experiments were repeated five times with different random seeds, and the results were averaged.</p>
<p>The model's label accuracy was consistently high across all datasets, usually above 95%.The results are comparable to those of the random forest classifier used to train the LLM, which achieved accuracies of 94.93% (Iris), 97.30% (Wine), and 94.72% (Breast cancer) in a 5-fold CV experiment repeated five times (5cv5), consistent with findings in Dinh et al. [2022].</p>
<p>For LLM explanations, we observed high average statement accuracy and recall with RE+IN+VN preprocessing, both above 80% and 74%, respectively.This indicates that the LLM's statements accurately represent the model's assigned class.</p>
<p>Enabling RE sharply increased the number of correct (parsable) statements.Without RE, common errors included missing relation (e.g., inequality) signs, which were rare with RE enabled.This error pattern was consistent across multiple generations, regardless of generation parameters (such as temperature).Table 1 shows that RE+IN+VN preprocessing generally improves label and statement accuracy.However, the impact on statement recall is inconsistent, suggesting a potential trade-off between the accuracy and recall of generated statements.</p>
<p>Discussion</p>
<p>We observed that changes in LLM training hyperparameters moderately impact performance.The most critical parameter is the depth of the RF, as more complex datasets require deeper decision trees, resulting in more extended outputs.Analysing the results, we suspect that improved performance may be achieved with better (non-random) selection of decision paths and reduction of redundant conditions.The number of training epochs used in our experiment was sufficient for the tested datasets, but larger datasets may require more extended training.</p>
<p>We have also applied the described methodology to the hyperspectral classification problem Romaszewski et al. [2016] using standard datasets like the 'Pavia University' image.Initial experiments in spectral classification achieved 78% accuracy (similar parameters as in the paper) to 85% accuracy in spectral classification, depending on the RF parameters, indicating the potential for applying this methodology to Computer Vision problems.</p>
<p>Initial experiments with different models, such as FLAN-T5-XL, showed results comparable to FLAN-T5-base, likely due to the simplicity of the training sets used.Experiments with GPT-3.5 and LLaMA models initially showed worse performance, possibly due to limited training time for these larger models.We have also tried reformulating inputs and outputs using GPT-4.The model was instructed to reduce redundant statements, paraphrase the text while maintaining meaning and information, and create different text versions.After fine-tuning with such preprocessed data, the LLM generated text resembling paraphrased rules in natural language.However, verification of whether such paraphrasing does not degrade classification performance needs additional experiments.This capability may be a crucial step toward developing a model with chatbot functionality, which could function as a class-aware expert.</p>
<p>Conclusions</p>
<p>Our results demonstrate that an LLM trained with an RF classifier can accurately classify and explain its decisions.The high classification accuracy aligns with recent works such as Dinh et al. [2022], and the quality of explanations is promising, as the LLM effectively segments data space fragments where class examples are concentrated.Additionally, our proposed statement verification method is less costly than human-assisted assessment Zhuang et al. [2024] and may be a promising approach for generating data for reward model creation Ouyang et al. [2022].</p>
<p>We believe the proposed training method can be applied in many applications and is particularly suited for recommender systems Hou et al. [2024].LLMs fine-tuned to understand the structure of numerical data can explain or support an ensemble of classical models.</p>
<p>comparison of data features with arbitrary thresholds.Tokenization converts such statements, including the floating-point values, into a series of tokens; for example, the GPT-4 tokenizer generates three tokens for each of the values in the Eq (1).To facilitate LLM understanding and reasoning with such statements, we perform several preprocessing steps, including: 1. Integer normalisation (IN) -transforming floating-point values to fixed-point values with a limited number of digits, 2. Verbal description of values (VD) -extending the value with a verbal description based on box and whisker plot (box plot) Tukey et al. [1977],</p>
<p>Table 1 :
1
Mean performance of LLM with relation encoding (RE) enabled.(IN)denotes integer normalisation, and (VD) denotes verbal description of values as described in Section 2.1.Label accuracy is the accuracy based on assigned labels.Statement accuracy and recall are computed based on a subset of training data indicated by LLM-generated statements as described in Section 2.2.'Correct' denotes the percentage of statements that could be properly parsed.
DS nameLabel accuracy Statement accuracy Statement recall Correct0iris93.1088.3088.29100.001iris(IN)92.3688.0287.17100.002iris(VN)93.7990.0289.54100.003iris(IN+VN)94.8190.2789.18100.004wine91.6780.5477.11100.005wine(IN)98.3377.1170.6799.446wine(VN)94.4485.3980.92100.007wine(IN+VN)100.0081.2174.54100.008breast94.7490.2779.2799.829breast(IN)97.1991.5077.3099.8210 breast(VN)96.6791.2079.54100.0011 breast(IN+VN)97.0291.1477.2599.82DS nameLabel accuracy Statement accuracy Statement recall Correct0iris93.1078.9378.7737.331iris(IN)94.1281.8982.0735.102iris(VN)93.1080.7081.2237.333iris(IN+VN)94.4583.8084.7834.054wine88.8970.4179.1937.785wine(IN)93.8974.9083.0636.116wine(VN)97.2285.6192.4431.117wine(IN+VN)97.2278.0986.0731.118breast93.6891.0073.4833.519breast(IN)96.1495.0774.8431.5810 breast(VN)96.2793.2876.6332.4611 breast(IN+VN)98.2595.2575.2631.80</p>
<p>Table 2 :
2
Mean performance of LLM without relation encoding (RE) enabled.(IN)denotes integer normalisation, and (VD) denotes verbal description of values as described in Section 2.1.Label accuracy is the accuracy based on assigned labels.Statement accuracy and recall are computed based on a subset of training data indicated by LLM-generated statements as described in Section 2.2.'Correct' denotes the percentage of statements that could be properly parsed.</p>
<p>We note that the n k are dependent on P, so should formally be defined as n P k , but we skip the path index for convenience.
AcknowledgementThe authors would like to thank Prof. Maciej Piasecki and Prof. Julian Szymański for their valuable comments during the PP-RAI'24 conference, particularly regarding LLMs' understanding of numerical data in the context of the tokenization mechanism.
Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, 2024</p>
<p>Random forests. Leo Breiman, Machine learning. 452001</p>
<p>The balanced accuracy and its posterior distribution. Kay Henning Brodersen, Soon Cheng, Klaas Ong, Joachim M Enno Stephan, Buhmann, 2010 20th international conference on pattern recognition. IEEE2010</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>A survey on xai and natural language explanations. Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani, Information Processing &amp; Management. 6011031112023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Chainlm: Empowering large language models with improved chain-of-thought prompting. Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, 2024</p>
<p>Investigating symbolic capabilities of large language models. Neisarg Dave, Daniel Kifer, Lee Giles, Ankur Mali, arXiv:2405.132092024arXiv preprint</p>
<p>Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-Yong Sohn, Dimitris Papailiopoulos, Kangwook Lee, Advances in Neural Information Processing Systems. 202235</p>
<p>Explainable ai (xai): Core ideas, techniques, and solutions. Rudresh Dwivedi, Devam Dave, Het Naik, Smiti Singhal, Rana Omer, Pankesh Patel, Bin Qian, Zhenyu Wen, Tejal Shah, Graham Morgan, ACM Computing Surveys. 5592023</p>
<p>Large language models(llms) on tabular data: Prediction, generation, and understanding -a survey. Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos, 2024</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, 2023</p>
<p>Explainable predictive maintenance of rotating machines using lime, shap, pdp, ice. Shreyas Gawde, Shruti Patil, Satish Kumar, Pooja Kamat, IEEE Access. 122024Ketan Kotecha, and Sultan Alfarhood</p>
<p>Openagi: When llm meets domain experts. Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Shane T Robert R Hoffman, Gary Mueller, Jordan Klein, Litman, arXiv:1812.04608Metrics for explainable ai: Challenges and prospects. 2018arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. Springer2024</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, Mathematical reasoning using large language models. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara, arXiv:2305.10613Temporal knowledge graph forecasting without knowledge using in-context learning. 2023arXiv preprint</p>
<p>Treeas-a-prompt: Boosting black-box large language models on few-shot classification of tabular data. Qinbin Li, Yesheng Liang, Yiqun Diao, Chulin Xie, Bo Li, Bingsheng He, Dawn Song, 2023</p>
<p>Federated prompting and chain-of-thought reasoning for improving llms answering. Xiangyang Liu, Tianqi Pang, Chenyou Fan, 2023</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, International Conference on Machine Learning. PMLR2023</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, 201730Advances in neural information processing systems</p>
<p>From understanding to utilization: A survey on explainability for large language models. Haoyan Luo, Lucia Specia, 2024</p>
<p>Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, 2023</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, 2023</p>
<p>From black boxes to conversations: Incorporating xai in a conversational agent. Jörg Van Bach Nguyen, Christin Schlötterer, Seifert, World Conference on Explainable Artificial Intelligence. Springer2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>Validation of ai-based information systems for sensitive use cases: Using an xai approach in pharmaceutical engineering. Anna Polzer, Jürgen Fleiß, Thomas Ebner, Philipp Kainz, Christoph Koeth, Stefan Thalmann, HICSS. 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Semi-supervised hyperspectral classification from a small number of training samples using a co-training approach. Michał Romaszewski, Przemysław Głomb, Michał Cholewa, ISPRS Journal of Photogrammetry and Remote Sensing. 1212016</p>
<p>Tokenization counts: the impact of tokenization on arithmetic in frontier llms. K Aaditya, Singh, Strouse, 2024</p>
<p>Xai for cybersecurity: state of the art, challenges, open issues and future directions. Gautam Srivastava, H Rutvij, Sweta Jhaveri, Sharnil Bhattacharya, Praveen Pandya, Gokul Kumar Reddy Maddikunta, Jon G Yenduri, Mamoun Hall, Thippa Alazab, Reddy Gadekallu, arXiv:2206.035852022arXiv preprint</p>
<p>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>Explainable artificial intelligence (xai) in deep learning-based medical image analysis. John Wilder, Tukey , Medical Image Analysis. 21024701977. 2022SpringerExploratory data analysis</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023a</p>
<p>Cmath: Can your language model pass chinese elementary school math test?. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, Bin Wang, 2023b</p>
<p>Transformers: Stateof-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrations2020</p>
<p>Usable xai: 10 strategies towards exploiting explainability in the llm era. Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, arXiv:2403.089462024arXiv preprint</p>
<p>Promptcast: A new prompt-based learning paradigm for time series forecasting. Hao Xue, Flora D Salim, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>Lpml: Llm-prompting markup language for mathematical reasoning. Ryutaro Yamauchi, Sho Sonoda, Akiyoshi Sannai, Wataru Kumagai, 2023</p>
<p>Assessing the performance of chatgpt in answering questions regarding cirrhosis and hepatocellular carcinoma. Yee Hui Yeo, Wee Jamil S Samaan, Han Ng, Peng-Sheng Ting, Hirsh Trivedi, Aarshi Vipani, Walid Ayoub, Dong Ju, Omer Yang, Brennan Liran, Spiegel, Clinical and molecular hepatology. 2937212023</p>
<p>Interaction: A generative xai framework for natural language inference explanations. Jialin Yu, Alexandra I Cristea, Anoushka Harit, Zhongtian Sun, Olanrewaju Tahir Aduragba, Lei Shi, Noura Al, Moubayed , 2022 International Joint Conference on Neural Networks (IJCNN). IEEE2022</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, Bootstrap your own mathematical questions for large language models. 2024</p>
<p>Towards better chain-of-thought prompting strategies: A survey. Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen, 2023</p>
<p>Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou, arXiv:2310.01074Back to the future: Towards explainable temporal reasoning with large language models. 2023arXiv preprint</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.107922023arXiv preprint</p>
<p>Large language models for time series: A survey. Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K Gupta, Jingbo Shang, arXiv:2402.018012024arXiv preprint</p>
<p>Large language models are complex table parsers. Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, Xiaobo Zhang, arXiv:2312.115212023arXiv preprint</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, 2023</p>
<p>Learning a decision tree algorithm with transformers. Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao, arXiv:2402.037742024arXiv preprint</p>
<p>Explaining tree model decisions in natural language for network intrusion detection. Noah Ziems, Gang Liu, John Flanagan, Meng Jiang, arXiv:2310.196582023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
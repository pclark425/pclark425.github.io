<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1450 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1450</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1450</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-6cd5dfccd9f52538b19a415e00031d0ee4e5b181</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6cd5dfccd9f52538b19a415e00031d0ee4e5b181" target="_blank">Designing Neural Network Architectures using Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> MetaQNN is introduced, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task that beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types.</p>
                <p><strong>Paper Abstract:</strong> At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1450.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1450.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaQNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaQNN (Q-learning based meta-modeling for CNN architecture design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning based meta-modeling agent that sequentially selects CNN layers (from a discretized space) using Q-learning with epsilon-greedy exploration and experience replay to automatically discover high-performing convolutional neural network architectures for image classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaQNN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Q-learning agent that models CNN architecture construction as a finite-horizon Markov Decision Process: states are layer tuples (type, parameters, representation-size bin, layer-depth), actions pick the next layer (or terminate). The agent uses Q-learning updates (alpha=0.01, gamma=1), an epsilon-annealed epsilon-greedy policy (explore-to-exploit schedule provided), and experience replay (sampled replay dictionary of architectures and validation accuracies) to learn Q-values and propose architectures. Architectures are trained with a fixed training protocol during exploration and the validation accuracy is used as the reward; top models are then fine-tuned with longer schedules and ensembled.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Machine learning / Neural architecture search (computer vision: CNN design for image classification)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically discovered convolutional neural network architectures (topologies made from convolution, pooling, fully-connected, global average pooling, softmax layers) that achieve high validation/test accuracy on image classification benchmarks (CIFAR-10, SVHN, MNIST, CIFAR-100). The agent explores a large discretized architecture space, stores (architecture, validation accuracy) in replay memory, updates Q-values, and progressively samples better architectures as epsilon is annealed; top architectures are fine-tuned and ensembled to produce final results.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>not explicitly characterized</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not use explicit terminology such as 'incremental' or 'transformational' to label the discoveries; it frames results as automated generation of tailored architectures that 'beat existing networks designed with the same layer types' and are 'competitive' with state-of-the-art, which supports characterization as performance improvements but the authors do not assert a specific discovery category.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation is primarily empirical: during exploration each candidate architecture is trained with a standardized fast training schedule and its validation accuracy is used as the reward. Final evaluation uses longer fine-tuning schedules and reports test error/accuracy on standard benchmarks (CIFAR-10, SVHN, MNIST, CIFAR-100). Specific metrics include validation accuracy (used as RL reward), test error rate (%) for datasets, rolling-average model accuracy vs iteration to track learning progress, Q-value statistics per layer/type, and ensemble accuracy for top-N models. Numerical examples reported: best CIFAR-10 test error 6.92% (top model), ensemble 7.32%; SVHN ensemble 2.06%, top model 2.28%; MNIST ensemble (10 models) 0.28% test error; improvement in mean validation accuracy on SVHN from 52.25% (epsilon=1) to 88.02% (epsilon=0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation is empirical and comparative: (1) Candidate architectures are validated by held-out validation sets (5,000 samples taken from training) and the validation accuracy is used as the RL reward. (2) Top architectures are fine-tuned on full/extended training sets and evaluated on test sets to produce final reported errors. (3) Comparisons to prior hand-crafted and automated architectures are used to validate novelty and performance (tables comparing error rates). (4) Stability/reproducibility is assessed via 10 independent Q-learning runs on a smaller subset of SVHN, reporting mean best-model accuracy and standard deviation (mean 88.25% with std 0.58%). (5) Transfer learning tests: best CIFAR-10 model re-trained and fine-tuned on other datasets (CIFAR-100, SVHN, MNIST) and reported performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed by empirical outperformance relative to previous automated methods and to hand-crafted networks that use the same layer types. The paper claims MetaQNN outperforms prior meta-modeling/automated methods (e.g., Bergstra et al., NEAT/ neuroevolution) and matches or is competitive with some state-of-the-art human-designed architectures. The authors also analyze Q-values and discovered architectural motifs (e.g., preference for certain layer types and receptive field sizes with depth) as additional evidence of insight gained by the automated method. The paper does not present formal theoretical novelty proofs; novelty is inferred from benchmarking improvements and the fact that the approach automates architecture search using Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Primary impact/quality metrics are dataset test error rates (%) and validation accuracy used in RL. Examples: CIFAR-10 best model test error 6.92%; CIFAR-10 ensemble 7.32%; SVHN ensemble 2.06%, top model 2.28%; MNIST ensemble 0.28% (beats prior state-of-the-art w/o augmentation). Additional metrics: number of parameters per model (range reported, e.g., top CIFAR-10 models from 1.10M to 11.26M parameters), improvements in mean validation accuracy across epsilon schedule (e.g., SVHN mean accuracy increased from 52.25% to 88.02%), Q-value trends per layer type and receptive field size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper directly compares MetaQNN-discovered architectures to human-designed networks and prior automated methods in tables. Key findings: MetaQNN top CIFAR-10 model error 6.92% (compared to All-CNN 7.25% and VGG-style results listed), ensemble results competitive with several state-of-the-art methods that include complex layers (ResNet(110) reported 6.61% and ResNet(1001) 4.62% are cited as stronger in some cases). The paper reports that previous automated methods performed substantially worse in cited cases (e.g., Bergstra et al. reported 21.2% on CIFAR-10 in one referenced automated attempt; Verbancsics & Harguess reported 7.9% on MNIST for neuroevolution methods — the MetaQNN ensemble achieves 0.28% on MNIST). The paper emphasizes that MetaQNN outperforms prior automated/meta-modeling approaches and is competitive with many human-designed architectures that use more complex layer types.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantified indirectly: the agent trains many candidate models (e.g., 1500 models at epsilon=1.0 plus additional models across epsilon schedule per Table 2) and shows a clear improvement in sampling better-performing models as epsilon anneals. Example success indicators: mean SVHN validation accuracy among sampled models increased from 52.25% to 88.02%; best-model performance across 10 independent smaller SVHN runs had mean best-model accuracy 88.25% (std 0.58%). The paper reports top-N selection success by selecting and fine-tuning the top 10 models, then ensembling the top 5 to produce final competitive results. There is no explicit "discovery success rate" percentage reported (e.g., fraction of sampled models exceeding a fixed threshold), but empirical metrics above demonstrate performance improvement and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified limitations include: (1) Constraining and discretizing the state-action space (coarse bins for representation size, limited layer parameter choices) to make search tractable — this reduces search resolution and may miss architectures outside the discretized space. (2) Computational and exploration cost: limited exploration time/hardware (10 GPUs, 8-10 days per dataset) can limit finding many top models, particularly for larger search spaces (authors note CIFAR-10 may have needed more exploration). (3) Use of the same fixed hyperparameters/training schedule for all candidate models during exploration may disadvantage some architectures; only top models receive hyperparameter tuning. (4) The method was confined to standard layer types (C,P,FC) in experiments; more complex modules (residual, recurrent) were not included. (5) The paper does not explicitly label discoveries as incremental or transformational nor propose a formal criterion to distinguish them — making the incremental vs transformational assessment qualitative and based on empirical improvement only.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Designing Neural Network Architectures using Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1450.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1450.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEAT / Neuroevolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NEAT (NeuroEvolution of Augmenting Topologies) / genetic algorithm based neural architecture search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evolutionary / genetic-algorithm approaches that evolve neural network topologies and weights (e.g., NEAT), historically applied to network design and sometimes cited in automated architecture search literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolving neural networks through augmenting topologies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NEAT / neuroevolution approaches</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evolutionary algorithms that search neural network architectures by mutation and recombination of topologies and parameters (e.g., NEAT). In the paper's related work the authors note such methods date back to the 1980s and include approaches that evolve both architecture and weights.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Machine learning / neural architecture search</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Mentioned historically as automated methods that attempt to find network architectures/weights via genetic algorithms. The paper states that networks designed with genetic algorithms such as NEAT have, to the authors' knowledge, been unable to match the performance of hand-crafted networks on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>not explicitly characterized</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not categorize NEAT results as incremental or transformational; it only states they have generally failed to match hand-crafted network performance on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>As reported in this paper (second-hand): comparative benchmark performance on standard datasets (citations indicate NEAT-based designs were evaluated by prior work and found to underperform compared to human-designed networks). No experiment-specific metrics are provided in this paper for NEAT beyond this comparative statement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison to known benchmark results (i.e., performance on standard datasets) as summarized from prior work; no new validation performed in this paper for NEAT.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>The paper treats NEAT-style approaches as prior automated attempts with limited success; novelty of NEAT itself is historical and not claimed here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Not provided in this paper beyond the qualitative statement that NEAT-derived networks have not matched human-designed networks (citing Verbancsics & Harguess, 2013).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper cites that NEAT and similar neuroevolution methods have been unable to match performance of hand-crafted networks on standard benchmarks (reference: Verbancsics & Harguess, 2013). No numeric comparisons are provided in this paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; characterized qualitatively as generally unsuccessful at matching state-of-the-art handcrafted designs according to cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper implies that earlier evolutionary methods struggled to scale to the large architecture spaces and did not reach performance parity with human designs; no further experimental detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Designing Neural Network Architectures using Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1450.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1450.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPE / Bayesian optimization methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured Parzen Estimator (TPE) and Bayesian optimization approaches for hyperparameter and architecture search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian optimization approaches (including TPE) used for automated selection of network architectures and hyperparameters by modelling performance as a function of hyperparameters and searching efficiently in high-dimensional spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TPE / Bayesian hyperparameter/architecture optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Methods that use probabilistic surrogate models (e.g., Tree-of-Parzen-Estimators) or other Bayesian optimization techniques to propose hyperparameters and sometimes architecture choices, optimizing validation performance with relatively fewer evaluations than naive search.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Machine learning / hyperparameter and architecture optimization</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Mentioned as prior automated/meta-modeling approaches for architecture/hyperparameter selection; the paper notes Bergstra et al.'s TPE-based meta-modeling failed to match performance of handcrafted networks in their cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>not explicitly characterized</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not characterize any discoveries from TPE methods as incremental or transformational; it only reports empirical outcome claims from cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Reported evaluation in cited works: benchmark performance on vision datasets; in this paper the authors summarize that TPE-based approaches did not match hand-crafted networks (no numerical details beyond citations are provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison to benchmark results from prior literature (as summarized by the authors). No new validation executed in this paper for TPE methods.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Paper treats Bayesian optimization as an established prior approach to automated model search; novelty assessment is relative (MetaQNN claimed to outperform such methods empirically).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Not provided in this paper beyond comparative statements; the authors state MetaQNN outperforms previous meta-modeling approaches such as Bergstra et al. (2011/2013).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper claims that Bergstra et al.'s TPE-based meta-modeling failed to match handcrafted network performance and that MetaQNN outperforms previous automated network design methods (citing specific relative errors in some cases). No detailed numeric breakdown of TPE results is given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not specified in this paper; characterized qualitatively via cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper implies Bayesian/meta-modeling approaches can be limited by search space definitions and their particular modeling assumptions; specifics are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Designing Neural Network Architectures using Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evolving neural networks through augmenting topologies <em>(Rating: 2)</em></li>
                <li>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures <em>(Rating: 2)</em></li>
                <li>A high-throughput screening approach to discovering good forms of biologically inspired visual representation <em>(Rating: 2)</em></li>
                <li>Generative neuroevolution for deep learning <em>(Rating: 2)</em></li>
                <li>Algorithms for hyper-parameter optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1450",
    "paper_id": "paper-6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "MetaQNN",
            "name_full": "MetaQNN (Q-learning based meta-modeling for CNN architecture design)",
            "brief_description": "A reinforcement-learning based meta-modeling agent that sequentially selects CNN layers (from a discretized space) using Q-learning with epsilon-greedy exploration and experience replay to automatically discover high-performing convolutional neural network architectures for image classification tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MetaQNN",
            "system_description": "A Q-learning agent that models CNN architecture construction as a finite-horizon Markov Decision Process: states are layer tuples (type, parameters, representation-size bin, layer-depth), actions pick the next layer (or terminate). The agent uses Q-learning updates (alpha=0.01, gamma=1), an epsilon-annealed epsilon-greedy policy (explore-to-exploit schedule provided), and experience replay (sampled replay dictionary of architectures and validation accuracies) to learn Q-values and propose architectures. Architectures are trained with a fixed training protocol during exploration and the validation accuracy is used as the reward; top models are then fine-tuned with longer schedules and ensembled.",
            "discovery_domain": "Machine learning / Neural architecture search (computer vision: CNN design for image classification)",
            "discovery_description": "Automatically discovered convolutional neural network architectures (topologies made from convolution, pooling, fully-connected, global average pooling, softmax layers) that achieve high validation/test accuracy on image classification benchmarks (CIFAR-10, SVHN, MNIST, CIFAR-100). The agent explores a large discretized architecture space, stores (architecture, validation accuracy) in replay memory, updates Q-values, and progressively samples better architectures as epsilon is annealed; top architectures are fine-tuned and ensembled to produce final results.",
            "discovery_type": "not explicitly characterized",
            "discovery_type_justification": "The paper does not use explicit terminology such as 'incremental' or 'transformational' to label the discoveries; it frames results as automated generation of tailored architectures that 'beat existing networks designed with the same layer types' and are 'competitive' with state-of-the-art, which supports characterization as performance improvements but the authors do not assert a specific discovery category.",
            "evaluation_methods": "Evaluation is primarily empirical: during exploration each candidate architecture is trained with a standardized fast training schedule and its validation accuracy is used as the reward. Final evaluation uses longer fine-tuning schedules and reports test error/accuracy on standard benchmarks (CIFAR-10, SVHN, MNIST, CIFAR-100). Specific metrics include validation accuracy (used as RL reward), test error rate (%) for datasets, rolling-average model accuracy vs iteration to track learning progress, Q-value statistics per layer/type, and ensemble accuracy for top-N models. Numerical examples reported: best CIFAR-10 test error 6.92% (top model), ensemble 7.32%; SVHN ensemble 2.06%, top model 2.28%; MNIST ensemble (10 models) 0.28% test error; improvement in mean validation accuracy on SVHN from 52.25% (epsilon=1) to 88.02% (epsilon=0.1).",
            "validation_approaches": "Validation is empirical and comparative: (1) Candidate architectures are validated by held-out validation sets (5,000 samples taken from training) and the validation accuracy is used as the RL reward. (2) Top architectures are fine-tuned on full/extended training sets and evaluated on test sets to produce final reported errors. (3) Comparisons to prior hand-crafted and automated architectures are used to validate novelty and performance (tables comparing error rates). (4) Stability/reproducibility is assessed via 10 independent Q-learning runs on a smaller subset of SVHN, reporting mean best-model accuracy and standard deviation (mean 88.25% with std 0.58%). (5) Transfer learning tests: best CIFAR-10 model re-trained and fine-tuned on other datasets (CIFAR-100, SVHN, MNIST) and reported performance.",
            "novelty_assessment": "Novelty is assessed by empirical outperformance relative to previous automated methods and to hand-crafted networks that use the same layer types. The paper claims MetaQNN outperforms prior meta-modeling/automated methods (e.g., Bergstra et al., NEAT/ neuroevolution) and matches or is competitive with some state-of-the-art human-designed architectures. The authors also analyze Q-values and discovered architectural motifs (e.g., preference for certain layer types and receptive field sizes with depth) as additional evidence of insight gained by the automated method. The paper does not present formal theoretical novelty proofs; novelty is inferred from benchmarking improvements and the fact that the approach automates architecture search using Q-learning.",
            "impact_metrics": "Primary impact/quality metrics are dataset test error rates (%) and validation accuracy used in RL. Examples: CIFAR-10 best model test error 6.92%; CIFAR-10 ensemble 7.32%; SVHN ensemble 2.06%, top model 2.28%; MNIST ensemble 0.28% (beats prior state-of-the-art w/o augmentation). Additional metrics: number of parameters per model (range reported, e.g., top CIFAR-10 models from 1.10M to 11.26M parameters), improvements in mean validation accuracy across epsilon schedule (e.g., SVHN mean accuracy increased from 52.25% to 88.02%), Q-value trends per layer type and receptive field size.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "The paper directly compares MetaQNN-discovered architectures to human-designed networks and prior automated methods in tables. Key findings: MetaQNN top CIFAR-10 model error 6.92% (compared to All-CNN 7.25% and VGG-style results listed), ensemble results competitive with several state-of-the-art methods that include complex layers (ResNet(110) reported 6.61% and ResNet(1001) 4.62% are cited as stronger in some cases). The paper reports that previous automated methods performed substantially worse in cited cases (e.g., Bergstra et al. reported 21.2% on CIFAR-10 in one referenced automated attempt; Verbancsics & Harguess reported 7.9% on MNIST for neuroevolution methods — the MetaQNN ensemble achieves 0.28% on MNIST). The paper emphasizes that MetaQNN outperforms prior automated/meta-modeling approaches and is competitive with many human-designed architectures that use more complex layer types.",
            "success_rate": "Quantified indirectly: the agent trains many candidate models (e.g., 1500 models at epsilon=1.0 plus additional models across epsilon schedule per Table 2) and shows a clear improvement in sampling better-performing models as epsilon anneals. Example success indicators: mean SVHN validation accuracy among sampled models increased from 52.25% to 88.02%; best-model performance across 10 independent smaller SVHN runs had mean best-model accuracy 88.25% (std 0.58%). The paper reports top-N selection success by selecting and fine-tuning the top 10 models, then ensembling the top 5 to produce final competitive results. There is no explicit \"discovery success rate\" percentage reported (e.g., fraction of sampled models exceeding a fixed threshold), but empirical metrics above demonstrate performance improvement and reproducibility.",
            "challenges_limitations": "Identified limitations include: (1) Constraining and discretizing the state-action space (coarse bins for representation size, limited layer parameter choices) to make search tractable — this reduces search resolution and may miss architectures outside the discretized space. (2) Computational and exploration cost: limited exploration time/hardware (10 GPUs, 8-10 days per dataset) can limit finding many top models, particularly for larger search spaces (authors note CIFAR-10 may have needed more exploration). (3) Use of the same fixed hyperparameters/training schedule for all candidate models during exploration may disadvantage some architectures; only top models receive hyperparameter tuning. (4) The method was confined to standard layer types (C,P,FC) in experiments; more complex modules (residual, recurrent) were not included. (5) The paper does not explicitly label discoveries as incremental or transformational nor propose a formal criterion to distinguish them — making the incremental vs transformational assessment qualitative and based on empirical improvement only.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1450.0",
            "source_info": {
                "paper_title": "Designing Neural Network Architectures using Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "NEAT / Neuroevolution",
            "name_full": "NEAT (NeuroEvolution of Augmenting Topologies) / genetic algorithm based neural architecture search",
            "brief_description": "Evolutionary / genetic-algorithm approaches that evolve neural network topologies and weights (e.g., NEAT), historically applied to network design and sometimes cited in automated architecture search literature.",
            "citation_title": "Evolving neural networks through augmenting topologies",
            "mention_or_use": "mention",
            "system_name": "NEAT / neuroevolution approaches",
            "system_description": "Evolutionary algorithms that search neural network architectures by mutation and recombination of topologies and parameters (e.g., NEAT). In the paper's related work the authors note such methods date back to the 1980s and include approaches that evolve both architecture and weights.",
            "discovery_domain": "Machine learning / neural architecture search",
            "discovery_description": "Mentioned historically as automated methods that attempt to find network architectures/weights via genetic algorithms. The paper states that networks designed with genetic algorithms such as NEAT have, to the authors' knowledge, been unable to match the performance of hand-crafted networks on standard benchmarks.",
            "discovery_type": "not explicitly characterized",
            "discovery_type_justification": "The paper does not categorize NEAT results as incremental or transformational; it only states they have generally failed to match hand-crafted network performance on benchmarks.",
            "evaluation_methods": "As reported in this paper (second-hand): comparative benchmark performance on standard datasets (citations indicate NEAT-based designs were evaluated by prior work and found to underperform compared to human-designed networks). No experiment-specific metrics are provided in this paper for NEAT beyond this comparative statement.",
            "validation_approaches": "Comparison to known benchmark results (i.e., performance on standard datasets) as summarized from prior work; no new validation performed in this paper for NEAT.",
            "novelty_assessment": "The paper treats NEAT-style approaches as prior automated attempts with limited success; novelty of NEAT itself is historical and not claimed here.",
            "impact_metrics": "Not provided in this paper beyond the qualitative statement that NEAT-derived networks have not matched human-designed networks (citing Verbancsics & Harguess, 2013).",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Paper cites that NEAT and similar neuroevolution methods have been unable to match performance of hand-crafted networks on standard benchmarks (reference: Verbancsics & Harguess, 2013). No numeric comparisons are provided in this paper itself.",
            "success_rate": "Not quantified in this paper; characterized qualitatively as generally unsuccessful at matching state-of-the-art handcrafted designs according to cited literature.",
            "challenges_limitations": "Paper implies that earlier evolutionary methods struggled to scale to the large architecture spaces and did not reach performance parity with human designs; no further experimental detail in this paper.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1450.1",
            "source_info": {
                "paper_title": "Designing Neural Network Architectures using Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "TPE / Bayesian optimization methods",
            "name_full": "Tree-structured Parzen Estimator (TPE) and Bayesian optimization approaches for hyperparameter and architecture search",
            "brief_description": "Bayesian optimization approaches (including TPE) used for automated selection of network architectures and hyperparameters by modelling performance as a function of hyperparameters and searching efficiently in high-dimensional spaces.",
            "citation_title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
            "mention_or_use": "mention",
            "system_name": "TPE / Bayesian hyperparameter/architecture optimization",
            "system_description": "Methods that use probabilistic surrogate models (e.g., Tree-of-Parzen-Estimators) or other Bayesian optimization techniques to propose hyperparameters and sometimes architecture choices, optimizing validation performance with relatively fewer evaluations than naive search.",
            "discovery_domain": "Machine learning / hyperparameter and architecture optimization",
            "discovery_description": "Mentioned as prior automated/meta-modeling approaches for architecture/hyperparameter selection; the paper notes Bergstra et al.'s TPE-based meta-modeling failed to match performance of handcrafted networks in their cited work.",
            "discovery_type": "not explicitly characterized",
            "discovery_type_justification": "The paper does not characterize any discoveries from TPE methods as incremental or transformational; it only reports empirical outcome claims from cited works.",
            "evaluation_methods": "Reported evaluation in cited works: benchmark performance on vision datasets; in this paper the authors summarize that TPE-based approaches did not match hand-crafted networks (no numerical details beyond citations are provided here).",
            "validation_approaches": "Comparison to benchmark results from prior literature (as summarized by the authors). No new validation executed in this paper for TPE methods.",
            "novelty_assessment": "Paper treats Bayesian optimization as an established prior approach to automated model search; novelty assessment is relative (MetaQNN claimed to outperform such methods empirically).",
            "impact_metrics": "Not provided in this paper beyond comparative statements; the authors state MetaQNN outperforms previous meta-modeling approaches such as Bergstra et al. (2011/2013).",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Paper claims that Bergstra et al.'s TPE-based meta-modeling failed to match handcrafted network performance and that MetaQNN outperforms previous automated network design methods (citing specific relative errors in some cases). No detailed numeric breakdown of TPE results is given in this paper.",
            "success_rate": "Not specified in this paper; characterized qualitatively via cited literature.",
            "challenges_limitations": "Paper implies Bayesian/meta-modeling approaches can be limited by search space definitions and their particular modeling assumptions; specifics are not detailed here.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1450.2",
            "source_info": {
                "paper_title": "Designing Neural Network Architectures using Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evolving neural networks through augmenting topologies",
            "rating": 2
        },
        {
            "paper_title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
            "rating": 2
        },
        {
            "paper_title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation",
            "rating": 2
        },
        {
            "paper_title": "Generative neuroevolution for deep learning",
            "rating": 2
        },
        {
            "paper_title": "Algorithms for hyper-parameter optimization",
            "rating": 1
        }
    ],
    "cost": 0.014834749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DESIGNING NEURAL NETWORK ARCHITECTURES USING REINFORCEMENT LEARNING</h1>
<p>Bowen Baker, Otkrist Gupta, Nikhil Naik \&amp; Ramesh Raskar<br>Media Laboratory<br>Massachusetts Institute of Technology<br>Cambridge MA 02139, USA<br>{bowen, otkrist, naik, raskar}@mit.edu</p>
<h4>Abstract</h4>
<p>At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$ learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.</p>
<h2>1 INTRODUCTION</h2>
<p>Deep convolutional neural networks (CNNs) have seen great success in the past few years on a variety of machine learning problems (LeCun et al., 2015). A typical CNN architecture consists of several convolution, pooling, and fully connected layers. While constructing a CNN, a network designer has to make numerous design choices: the number of layers of each type, the ordering of layers, and the hyperparameters for each type of layer, e.g., the receptive field size, stride, and number of receptive fields for a convolution layer. The number of possible choices makes the design space of CNN architectures extremely large and hence, infeasible for an exhaustive manual search. While there has been some work (Pinto et al., 2009; Bergstra et al., 2013; Domhan et al., 2015) on automated or computer-aided neural network design, new CNN architectures or network design elements are still primarily developed by researchers using new theoretical insights or intuition gained from experimentation.</p>
<p>In this paper, we seek to automate the process of CNN architecture selection through a metamodeling procedure based on reinforcement learning. We construct a novel $Q$-learning agent whose goal is to discover CNN architectures that perform well on a given machine learning task with no human intervention. The learning agent is given the task of sequentially picking layers of a CNN model. By discretizing and limiting the layer parameters to choose from, the agent is left with a finite but large space of model architectures to search from. The agent learns through random exploration and slowly begins to exploit its findings to select higher performing models using the $\epsilon$ greedy strategy (Mnih et al., 2015). The agent receives the validation accuracy on the given machine learning task as the reward for selecting an architecture. We expedite the learning process through repeated memory sampling using experience replay (Lin, 1993). We refer to this $Q$-learning based meta-modeling method as MetaQNN, which is summarized in Figure 1. ${ }^{1}$</p>
<p>We conduct experiments with a space of model architectures consisting of only standard convolution, pooling, and fully connected layers using three standard image classification datasets: CIFAR-10,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Designing CNN Architectures with $\boldsymbol{Q}$-learning: The agent begins by sampling a Convolutional Neural Network (CNN) topology conditioned on a predefined behavior distribution and the agent's prior experience (left block). That CNN topology is then trained on a specific task; the topology description and performance, e.g. validation accuracy, are then stored in the agent's memory (middle block). Finally, the agent uses its memories to learn about the space of CNN topologies through $Q$-learning (right block).</p>
<p>SVHN, and MNIST. The learning agent discovers CNN architectures that beat all existing networks designed only with the same layer types (e.g., Springenberg et al. (2014); Srivastava et al. (2015)). In addition, their performance is competitive against network designs that include complex layer types and training procedures (e.g., Clevert et al. (2015); Lee et al. (2016)). Finally, the MetaQNN selected models comfortably outperform previous automated network design methods (Stanley \&amp; Miikkulainen, 2002; Bergstra et al., 2013). The top network designs discovered by the agent on one dataset are also competitive when trained on other datasets, indicating that they are suited for transfer learning tasks. Moreover, we can generate not just one, but several varied, well-performing network designs, which can be ensembled to further boost the prediction performance.</p>
<h1>2 RELATED WORK</h1>
<p>Designing neural network architectures: Research on automating neural network design goes back to the 1980s when genetic algorithm-based approaches were proposed to find both architectures and weights (Schaffer et al., 1992). However, to the best of our knowledge, networks designed with genetic algorithms, such as those generated with the NEAT algorithm (Stanley \&amp; Miikkulainen, 2002), have been unable to match the performance of hand-crafted networks on standard benchmarks (Verbancsics \&amp; Harguess, 2013). Other biologically inspired ideas have also been explored; motivated by screening methods in genetics, Pinto et al. (2009) proposed a high-throughput network selection approach where they randomly sample thousands of architectures and choose promising ones for further training. In recent work, Saxena \&amp; Verbeek (2016) propose to sidestep the architecture selection process through densely connected networks of layers, which come closer to the performance of hand-crafted networks.</p>
<p>Bayesian optimization has also been used (Shahriari et al., 2016) for automatic selection of network architectures (Bergstra et al., 2013; Domhan et al., 2015) and hyperparameters (Snoek et al., 2012; Swersky et al., 2013). Notably, Bergstra et al. (2013) proposed a meta-modeling approach based on Tree of Parzen Estimators (TPE) (Bergstra et al., 2011) to choose both the type of layers and hyperparameters of feed-forward networks; however, they fail to match the performance of handcrafted networks.</p>
<p>Reinforcement Learning: Recently there has been much work at the intersection of reinforcement learning and deep learning. For instance, methods using CNNs to approximate the $Q$-learning utility function (Watkins, 1989) have been successful in game-playing agents (Mnih et al., 2015; Silver et al., 2016) and robotic control (Lillicrap et al., 2015; Levine et al., 2016). These methods rely on phases of exploration, where the agent tries to learn about its environment through sampling, and exploitation, where the agent uses what it learned about the environment to find better paths. In traditional reinforcement learning settings, over-exploration can lead to slow convergence times, yet over-exploitation can lead to convergence to local minima (Kaelbling et al., 1996). However, in the case of large or continuous state spaces, the $\epsilon$-greedy strategy of learning has been empirically shown to converge (Vermorel \&amp; Mohri, 2005). Finally, when the state space is large or exploration is costly,</p>
<p>the experience replay technique (Lin, 1993) has proved useful in experimental settings (Adam et al., 2012; Mnih et al., 2015). We incorporate these techniques- $Q$-learning, the $\epsilon$-greedy strategy and experience replay-in our algorithm design.</p>
<h1>3 BACKGROUND</h1>
<p>Our method relies on $Q$-learning, a type of reinforcement learning. We now summarize the theoretical formulation of $Q$-learning, as adopted to our problem. Consider the task of teaching an agent to find optimal paths as a Markov Decision Process (MDP) in a finite-horizon environment. Constraining the environment to be finite-horizon ensures that the agent will deterministically terminate in a finite number of time steps. In addition, we restrict the environment to have a discrete and finite state space $\mathcal{S}$ as well as action space $\mathcal{U}$. For any state $s_{i} \in \mathcal{S}$, there is a finite set of actions, $\mathcal{U}\left(s_{i}\right) \subseteq \mathcal{U}$, that the agent can choose from. In an environment with stochastic transitions, an agent in state $s_{i}$ taking some action $u \in \mathcal{U}\left(s_{i}\right)$ will transition to state $s_{j}$ with probability $p_{s^{\prime} \mid s, u}\left(s_{j} \mid s_{i}, u\right)$, which may be unknown to the agent. At each time step $t$, the agent is given a reward $r_{t}$, dependent on the transition from state $s$ to $s^{\prime}$ and action $u . r_{t}$ may also be stochastic according to a distribution $p_{r \mid s^{\prime}, s, u}$. The agent's goal is to maximize the total expected reward over all possible trajectories, i.e., $\max <em i="i">{\mathcal{T}</em>} \in \mathcal{T}} R_{\mathcal{T<em i="i">{i}}$, where the total expected reward for a trajectory $\mathcal{T}</em>$ is</p>
<p>$$
R_{\mathcal{T}<em _left_s_="\left(s," s_prime="s^{\prime" u_="u,">{i}}=\sum</em>}\right) \in \mathcal{T<em _mid="\mid" r="r" s_="s," s_prime="s^{\prime" u_="u,">{i}} \mathbb{E}</em>\right]
$$}}\left[r \mid s, u, s^{\prime</p>
<p>Though we limit the agent to a finite state and action space, there are still a combinatorially large number of trajectories, which motivates the use of reinforcement learning. We define the maximization problem recursively in terms of subproblems as follows. For any state $s_{i} \in \mathcal{S}$ and subsequent action $u \in \mathcal{U}\left(s_{i}\right)$, we define the maximum total expected reward to be $Q^{<em>}\left(s_{i}, u\right) . Q^{</em>}(\cdot)$ is known as the action-value function and individual $Q^{*}\left(s_{i}, u\right)$ are know as $Q$-values. The recursive maximization equation, which is known as Bellman's Equation, can be written as</p>
<p>$$
Q^{<em>}\left(s_{i}, u\right)=\mathbb{E}<em j="j">{s</em>} \mid s_{i}, u}\left[\mathbb{E<em i="i">{r \mid s</em>\right]+\gamma \max }, u, s_{j}}\left[r \mid s_{i}, u, s_{j<em j="j">{u^{\prime} \in \mathcal{U}\left(s</em> Q^{}\right)</em>}\left(s_{j}, u^{\prime}\right)\right]
$$</p>
<p>In many cases, it is impossible to analytically solve Bellman's Equation (Bertsekas, 2015), but it can be formulated as an iterative update</p>
<p>$$
Q_{t+1}\left(s_{i}, u\right)=(1-\alpha) Q_{t}\left(s_{i}, u\right)+\alpha\left[r_{t}+\gamma \max <em j="j">{u^{\prime} \in \mathcal{U}\left(s</em>\right)\right]
$$}\right)} Q_{t}\left(s_{j}, u^{\prime</p>
<p>Equation 3 is the simplest form of $Q$-learning proposed by Watkins (1989). For well formulated problems, $\lim <em t="t">{t \rightarrow \infty} Q</em>(s, u)$, as long as each transition is sampled infinitely many times (Bertsekas, 2015). The update equation has two parameters: (i) $\alpha$ is a $Q$-learning rate which determines the weight given to new information over old information, and (ii) $\gamma$ is the discount factor which determines the weight given to short-term rewards over future rewards. The $Q$-learning algorithm is model-free, in that the learning agent can solve the task without ever explicitly constructing an estimate of environmental dynamics. In addition, $Q$-learning is off policy, meaning it can learn about optimal policies while exploring via a non-optimal behavioral distribution, i.e. the distribution by which the agent explores its environment.}(s, u)=Q^{*</p>
<p>We choose the behavior distribution using an $\epsilon$-greedy strategy (Mnih et al., 2015). With this strategy, a random action is taken with probability $\epsilon$ and the greedy action, $\max <em i="i">{u \in \mathcal{U}\left(s</em>, u\right)$, is chosen with probability $1-\epsilon$. We anneal $\epsilon$ from $1 \rightarrow 0$ such that the agent begins in an exploration phase and slowly starts moving towards the exploitation phase. In addition, when the exploration cost is large (which is true for our problem setting), it is beneficial to use the experience replay technique for faster convergence (Lin, 1992). In experience replay, the learning agent is provided with a memory of its past explored paths and rewards. At a given interval, the agent samples from the memory and updates its $Q$-values via Equation 3.}\right)} Q_{t}\left(s_{i</p>
<h2>4 DESIGNING NEURAL NETWORK ARCHITECTURES WITH $Q$-LEARNING</h2>
<p>We consider the task of training a learning agent to sequentially choose neural network layers. Figure 2 shows feasible state and action spaces (a) and a potential trajectory the agent may take along with the CNN architecture defined by this trajectory (b). We model the layer selection process as a Markov Decision Process with the assumption that a well-performing layer in one network should</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Markov Decision Process for CNN Architecture Generation: Figure 2(a) shows the full state and action space. In this illustration, actions are shown to be deterministic for clarity, but they are stochastic in experiments. $C(n, f, l)$ denotes a convolutional layer with $n$ filters, receptive field size $f$, and stride $l . P(f, l)$ denotes a pooling layer with receptive field size $f$ and stride $l . G$ denotes a termination state (Softmax/Global Average Pooling). Figure 2(b) shows a path the agent may choose, highlighted in green, and the corresponding CNN topology.
also perform well in another network. We make this assumption based on the hierarchical nature of the feature representations learned by neural networks with many hidden layers (LeCun et al., 2015). The agent sequentially selects layers via the $\epsilon$-greedy strategy until it reaches a termination state. The CNN architecture defined by the agent's path is trained on the chosen learning problem, and the agent is given a reward equal to the validation accuracy. The validation accuracy and architecture description are stored in a replay memory, and experiences are sampled periodically from the replay memory to update $Q$-values via Equation 3. The agent follows an $\epsilon$ schedule which determines its shift from exploration to exploitation.</p>
<p>Our method requires three main design choices: (i) reducing CNN layer definitions to simple state tuples, (ii) defining a set of actions the agent may take, i.e., the set of layers the agent may pick next given its current state, and (iii) balancing the size of the state-action space-and correspondingly, the model capacity-with the amount of exploration needed by the agent to converge. We now describe the design choices and the learning process in detail.</p>
<h1>4.1 The State Space</h1>
<p>Each state is defined as a tuple of all relevant layer parameters. We allow five different types of layers: convolution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM), though the general method is not limited to this set. Table 1 shows the relevant parameters for each layer type and also the discretization we chose for each parameter. Each layer has a parameter layer depth (shown as Layer $1,2, \ldots$ in Figure 2). Adding layer depth to the state space allows us to constrict the action space such that the state-action graph is directed and acyclic (DAG) and also allows us to specify a maximum number of layers the agent may select before terminating.
Each layer type also has a parameter called representation size ( $R$-size). Convolutional nets progressively compress the representation of the original signal through pooling and convolution. The presence of these layers in our state space may lead the agent on a trajectory where the intermediate signal representation gets reduced to a size that is too small for further processing. For example, five $2 \times 2$ pooling layers each with stride 2 will reduce an image of initial size $32 \times 32$ to size $1 \times 1$. At this stage, further pooling, or convolution with receptive field size greater than 1 , would be meaningless and degenerate. To avoid such scenarios, we add the $R$-size parameter to the state tuple $s$, which allows us to restrict actions from states with $R$-size $n$ to those that have a receptive field size less than or equal to $n$. To further constrict the state space, we chose to bin the representation sizes into three discrete buckets. However, binning adds uncertainty to the state transitions: depending on the true underlying representation size, a pooling layer may or may not change the $R$-size bin. As a result, the action of pooling can lead to two different states, which we model as stochasticity in state transitions. Please see Figure A1 in appendix for an illustrated example.</p>
<table>
<thead>
<tr>
<th>Layer Type</th>
<th>Layer Parameters</th>
<th>Parameter Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>Convolution (C)</td>
<td>$i \sim$ Layer depth <br> $f \sim$ Receptive field size <br> $\ell \sim$ Stride <br> $d \sim #$ receptive fields <br> $n \sim$ Representation size</td>
<td>Square. $\in{1,3,5}$ <br> Square. Always equal to 1 <br> $\in{64,128,256,512}$ <br> $\in{(\infty, 8],(8,4],(4,1]}$</td>
</tr>
<tr>
<td>Pooling (P)</td>
<td>$i \sim$ Layer depth <br> $(f, \ell) \sim$ (Receptive field size, Strides) <br> $n \sim$ Representation size</td>
<td>Square. $\in{(5,3),(3,2),(2,2)}$ <br> $\in{(\infty, 8],(8,4]$ and $(4,1]}$</td>
</tr>
<tr>
<td>Fully Connected (FC)</td>
<td>$i \sim$ Layer depth <br> $n \sim #$ consecutive FC layers <br> $d \sim #$ neurons</td>
<td>$&lt;3$ <br> $\in{512,256,128}$</td>
</tr>
<tr>
<td>Termination State</td>
<td>$s \sim$ Previous State <br> $t \sim$ Type</td>
<td>Global Avg. Pooling/Softmax</td>
</tr>
</tbody>
</table>
<p>Table 1: Experimental State Space. For each layer type, we list the relevant parameters and the values each parameter is allowed to take.</p>
<h1>4.2 The Action Space</h1>
<p>We restrict the agent from taking certain actions to both limit the state-action space and make learning tractable. First, we allow the agent to terminate a path at any point, i.e. it may choose a termination state from any non-termination state. In addition, we only allow transitions for a state with layer depth $i$ to a state with layer depth $i+1$, which ensures that there are no loops in the graph. This constraint ensures that the state-action graph is always a DAG. Any state at the maximum layer depth, as prescribed in Table 1, may only transition to a termination layer.</p>
<p>Next, we limit the number of fully connected (FC) layers to be at maximum two, because a large number of FC layers can lead to too may learnable parameters. The agent at a state with type FC may transition to another state with type FC if and only if the number of consecutive FC states is less than the maximum allowed. Furthermore, a state $s$ of type FC with number of neurons $d$ may only transition to either a termination state or a state $s^{\prime}$ of type FC with number of neurons $d^{\prime} \leq d$.</p>
<p>An agent at a state of type convolution (C) may transition to a state with any other layer type. An agent at a state with layer type pooling ( P ) may transition to a state with any other layer type other than another P state because consecutive pooling layers are equivalent to a single, larger pooling layer which could lie outside of our chosen state space. Furthermore, only states with representation size in bins $(8,4]$ and $(4,1]$ may transition to an FC layer, which ensures that the number of weights does not become unreasonably huge. Note that a majority of these constraints are in place to enable faster convergence on our limited hardware (see Section 5) and not a limitation of the method in itself.</p>
<h3>4.3 Q-learning Training Procedure</h3>
<p>For the iterative $Q$-learning updates (Equation 3), we set the $Q$-learning rate $(\alpha)$ to 0.01 . In addition, we set the discount factor $(\gamma)$ to 1 to not over-prioritize short-term rewards. We decrease $\epsilon$ from 1.0 to 0.1 in steps, where the step-size is defined by the number of unique models trained (Table 2). At $\epsilon=1.0$, the agent samples CNN architecture with a random walk along a uniformly weighted Markov chain. Every topology sampled by the agent is trained using the procedure described in Section 5, and the prediction performance of this network topology on the validation set is recorded. We train a larger number of models at $\epsilon=1.0$ as compared to other values of $\epsilon$ to ensure that the agent has adequate time to explore before it begins to exploit. We stop the agent at $\epsilon=0.1$ (and not at $\epsilon=0$ ) to obtain a stochastic final policy, which generates perturbations of the global minimum. ${ }^{2}$ Ideally, we want to identify several well-performing model topologies, which can then be ensembled to improve prediction performance.</p>
<p>During the entire training process (starting at $\epsilon=1.0$ ), we maintain a replay dictionary which stores (i) the network topology and (ii) prediction performance on a validation set, for all of the sampled</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\epsilon$</th>
<th style="text-align: center;">1.0</th>
<th style="text-align: center;">0.9</th>
<th style="text-align: center;">0.8</th>
<th style="text-align: center;">0.7</th>
<th style="text-align: center;">0.6</th>
<th style="text-align: center;">0.5</th>
<th style="text-align: center;">0.4</th>
<th style="text-align: center;">0.3</th>
<th style="text-align: center;">0.2</th>
<th style="text-align: center;">0.1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># Models Trained</td>
<td style="text-align: center;">1500</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table>
<p>Table 2: $\epsilon$ Schedule. The learning agent trains the specified number of unique models at each $\epsilon$.
models. If a model that has already been trained is re-sampled, it is not re-trained, but instead the previously found validation accuracy is presented to the agent. After each model is sampled and trained, the agent randomly samples 100 models from the replay dictionary and applies the $Q$-value update defined in Equation 3 for all transitions in each sampled sequence. The $Q$-value update is applied to the transitions in temporally reversed order, which has been shown to speed up $Q$-values convergence (Lin, 1993).</p>
<h1>5 EXPERIMENT DETAILS</h1>
<p>During the model exploration phase, we trained each network topology with a quick and aggressive training scheme. For each experiment, we created a validation set by randomly taking 5,000 samples from the training set such that the resulting class distributions were unchanged. For every network, a dropout layer was added after every two layers. The $i^{t h}$ dropout layer, out of a total $n$ dropout layers, had a dropout probability of $\frac{1}{2 n}$. Each model was trained for a total of 20 epochs with the Adam optimizer (Kingma \&amp; Ba, 2014) with $\beta_{1}=0.9, \beta_{2}=0.999, \varepsilon=10^{-8}$. The batch size was set to 128 , and the initial learning rate was set to 0.001 . If the model failed to perform better than a random predictor after the first epoch, we reduced the learning rate by a factor of 0.4 and restarted training, for a maximum of 5 restarts. For models that started learning (i.e., performed better than a random predictor), we reduced the learning rate by a factor of 0.2 every 5 epochs. All weights were initialized with Xavier initialization (Glorot \&amp; Bengio, 2010). Our experiments using Caffe (Jia et al., 2014) took 8-10 days to complete for each dataset with a hardware setup consisting of 10 NVIDIA GPUs.</p>
<p>After the agent completed the $\epsilon$ schedule (Table 2), we selected the top ten models that were found over the course of exploration. These models were then finetuned using a much longer training schedule, and only the top five were used for ensembling. We now provide details of the datasets and the finetuning process.</p>
<p>The Street View House Numbers (SVHN) dataset has 10 classes with a total of 73,257 samples in the original training set, 26,032 samples in the test set, and 531,131 additional samples in the extended training set. During the exploration phase, we only trained with the original training set, using 5,000 random samples as validation. We finetuned the top ten models with the original plus extended training set, by creating preprocessed training and validation sets as described by Lee et al. (2016). Our final learning rate schedule after tuning on validation set was 0.025 for 5 epochs, 0.0125 for 5 epochs, 0.0001 for 20 epochs, and 0.00001 for 10 epochs.</p>
<p>CIFAR-10, the 10 class tiny image dataset, has 50,000 training samples and 10,000 testing samples. During the exploration phase, we took 5,000 random samples from the training set for validation. The maximum layer depth was increased to 18. After the experiment completed, we used the same validation set to tune hyperparameters, resulting in a final training scheme which we ran on the entire training set. In the final training scheme, we set a learning rate of 0.025 for 40 epochs, 0.0125 for 40 epochs, 0.0001 for 160 epochs, and 0.00001 for 60 epochs, with all other parameters unchanged. During this phase, we preprocess using global contrast normalization and use moderate data augmentation, which consists of random mirroring and random translation by up to 5 pixels.</p>
<p>MNIST, the 10 class handwritten digits dataset, has 60,000 training samples and 10,000 testing samples. We preprocessed each image with global mean subtraction. In the final training scheme, we trained each model for 40 epochs and decreased learning rate every 5 epochs by a factor of 0.2 . For further tuning details please see Appendix C.</p>
<h2>6 ReSults</h2>
<p>Model Selection Analysis: From $Q$-learning principles, we expect the learning agent to improve in its ability to pick network topologies as $\epsilon$ reduces and the agent enters the exploitation phase. In</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $\boldsymbol{Q}$-Learning Performance. In the plots, the blue line shows a rolling mean of model accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model. Each bar (in light blue) marks the average accuracy over all models that were sampled during the exploration phase with the labeled $\epsilon$. As $\epsilon$ decreases, the average accuracy goes up, demonstrating that the agent learns to select better-performing CNN architectures.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CIFAR-10</th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;">CIFAR-100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Maxout (Goodfellow et al., 2013)</td>
<td style="text-align: center;">9.38</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">38.57</td>
</tr>
<tr>
<td style="text-align: left;">NIN (Lin et al., 2013)</td>
<td style="text-align: center;">8.81</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">35.68</td>
</tr>
<tr>
<td style="text-align: left;">FitNet (Romero et al., 2014)</td>
<td style="text-align: center;">8.39</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">35.04</td>
</tr>
<tr>
<td style="text-align: left;">HighWay (Srivastava et al., 2015)</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">VGGnet (Simonyan \&amp; Zisserman, 2014)</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">All-CNN (Springenberg et al., 2014)</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.71</td>
</tr>
<tr>
<td style="text-align: left;">MetaQNN (ensemble)</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">$\mathbf{2 . 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 2}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MetaQNN (top model)</td>
<td style="text-align: center;">$\mathbf{6 . 9 2}$</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">$\mathbf{2 7 . 1 4}^{*}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Error Rate Comparison with CNNs that only use convolution, pooling, and fully connected layers. We report results for CIFAR-10 and CIFAR-100 with moderate data augmentation and results for MNIST and SVHN without any data augmentation.</p>
<p>Figure 3, we plot the rolling mean of prediction accuracy over 100 models and the mean accuracy of models sampled at different $\epsilon$ values, for the CIFAR-10 and SVHN experiments. The plots show that, while the prediction accuracy remains flat during the exploration phase $(\epsilon=1)$ as expected, the agent consistently improves in its ability to pick better-performing models as $\epsilon$ reduces from 1 to 0.1 . For example, the mean accuracy of models in the SVHN experiment increases from $52.25 \%$ at $\epsilon=1$ to $88.02 \%$ at $\epsilon=0.1$. Furthermore, we demonstrate the stability of the $Q$-learning procedure with 10 independent runs on a subset of the SVHN dataset in Section D. 1 of the Appendix. Additional analysis of $Q$-learning results can be found in Section D.2.</p>
<p>The top models selected by the $Q$-learning agent vary in the number of parameters but all demonstrate high performance (see Appendix Tables 1-3). For example, the number of parameters for the top five CIFAR-10 models range from 11.26 million to 1.10 million, with only a $2.32 \%$ decrease in test error. We find design motifs common to the top hand-crafted network architectures as well. For example, the agent often chooses a layer of type $C(N, 1,1)$ as the first layer in the network. These layers generate $N$ learnable linear transformations of the input data, which is similar in spirit to preprocessing of input data from RGB to a different color spaces such as YUV, as found in prior work (Sermanet et al., 2012; 2013).</p>
<p>Prediction Performance: We compare the prediction performance of the MetaQNN networks discovered by the $Q$-learning agent with state-of-the-art methods on three datasets. We report the accuracy of our best model, along with an ensemble of top five models. First, we compare MetaQNN with six existing architectures that are designed with standard convolution, pooling, and fully-connected layers alone, similar to our designs. As seen in Table 3, our top model alone, as well as the committee ensemble of five models, outperforms all similar models. Next, we compare our results with six top networks overall, which contain complex layer types and design ideas, including generalized pooling functions, residual connections, and recurrent modules. Our results are competitive with these methods as well (Table 4). Finally, our method outperforms existing automated network de-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CIFAR-10</th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;">CIFAR-100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DropConnect (Wan et al., 2013)</td>
<td style="text-align: center;">9.32</td>
<td style="text-align: center;">1.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DSN (Lee et al., 2015)</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">34.57</td>
</tr>
<tr>
<td style="text-align: left;">R-CNN (Liang \&amp; Hu, 2015)</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">$\mathbf{0 . 3 1}$</td>
<td style="text-align: center;">31.75</td>
</tr>
<tr>
<td style="text-align: left;">MetaQNN (ensemble)</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MetaQNN (top model)</td>
<td style="text-align: center;">6.92</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">$27.14^{\star}$</td>
</tr>
<tr>
<td style="text-align: left;">Resnet(110) (He et al., 2015)</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Resnet(1001) (He et al., 2016)</td>
<td style="text-align: center;">$\mathbf{4 . 6 2}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 2 . 7 1}$</td>
</tr>
<tr>
<td style="text-align: left;">ELU (Clevert et al., 2015)</td>
<td style="text-align: center;">6.55</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.28</td>
</tr>
<tr>
<td style="text-align: left;">Tree+Max-Avg (Lee et al., 2016)</td>
<td style="text-align: center;">6.05</td>
<td style="text-align: center;">$\mathbf{1 . 6 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1}$</td>
<td style="text-align: center;">32.37</td>
</tr>
</tbody>
</table>
<p>Table 4: Error Rate Comparison with state-of-the-art methods with complex layer types. We report results for CIFAR-10 and CIFAR-100 with moderate data augmentation and results for MNIST and SVHN without any data augmentation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">CIFAR-100</th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;">MNIST</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training from scratch</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: left;">Finetuning</td>
<td style="text-align: center;">34.93</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">State-of-the-art</td>
<td style="text-align: center;">24.28 (Clevert et al., 2015)</td>
<td style="text-align: center;">1.69 (Lee et al., 2016)</td>
<td style="text-align: center;">0.31 (Lee et al., 2016)</td>
</tr>
</tbody>
</table>
<p>Table 5: Prediction Error for the top MetaQNN (CIFAR-10) model trained for other tasks. Finetuning refers to initializing training with the weights found for the optimal CIFAR-10 model.
sign methods. MetaQNN obtains an error of $6.92 \%$ as compared to $21.2 \%$ reported by Bergstra et al. (2011) on CIFAR-10; and it obtains an error of $0.32 \%$ as compared to $7.9 \%$ reported by Verbancsics $\&amp;$ Harguess (2013) on MNIST.</p>
<p>The difference in validation error between the top 10 models for MNIST was very small, so we also created an ensemble with all 10 models. This ensemble achieved a test error of $\mathbf{0 . 2 8 \%}$ —which beats the current state-of-the-art on MNIST without data augmentation.</p>
<p>The best CIFAR-10 model performs 1-2\% better than the four next best models, which is why the ensemble accuracy is lower than the best model's accuracy. We posit that the CIFAR-10 MetaQNN did not have adequate exploration time given the larger state space compared to that of the SVHN experiment, causing it to not find more models with performance similar to the best model. Furthermore, the coarse training scheme could have been not as well suited for CIFAR-10 as it was for SVHN, causing some models to under perform.</p>
<p>Transfer Learning Ability: Network designs such as VGGnet (Simonyan \&amp; Zisserman, 2014) can be adopted to solve a variety of computer vision problems. To check if the MetaQNN networks provide similar transfer learning ability, we use the best MetaQNN model on the CIFAR-10 dataset for training other computer vision tasks. The model performs well (Table 5) both when training from random initializations, and finetuning from existing weights.</p>
<h1>7 CONCLUDING REMARKS</h1>
<p>Neural networks are being used in an increasingly wide variety of domains, which calls for scalable solutions to produce problem-specific model architectures. We take a step towards this goal and show that a meta-modeling approach using reinforcement learning is able to generate tailored CNN designs for different image classification tasks. Our MetaQNN networks outperform previous metamodeling methods as well as hand-crafted networks which use the same types of layers.
While we report results for image classification problems, our method could be applied to different problem settings, including supervised (e.g., classification, regression) and unsupervised (e.g., autoencoders). The MetaQNN method could also aid constraint-based network design, by optimizing parameters such as size, speed, and accuracy. For instance, one could add a threshold in the state-action space barring the agent from creating models larger than the desired limit. In addition,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>one could modify the reward function to penalize large models for constraining memory or penalize slow forward passes to incentivize quick inference.</p>
<p>There are several future avenues for research in reinforcement learning-driven network design as well. In our current implementation, we use the same set of hyperparameters to train all network topologies during the $Q$-learning phase and further finetune the hyperparameters for top models selected by the MetaQNN agent. However, our approach could be combined with hyperparameter optimization methods to further automate the network design process. Moreover, we constrict the state-action space using coarse, discrete bins to accelerate convergence. It would be possible to move to larger state-action spaces using methods for $Q$-function approximation (Bertsekas, 2015; Mnih et al., 2015).</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank Peter Downs for creating the project website and contributing to illustrations. We acknowledge Center for Bits and Atoms at MIT for their help with computing resources. Finally, we thank members of Camera Culture group at MIT Media Lab for their help and support.</p>
<h2>REFERENCES</h2>
<p>Sander Adam, Lucian Busoniu, and Robert Babuska. Experience replay for real-time reinforcement learning control. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(2):201-212, 2012.</p>
<p>James Bergstra, Daniel Yamins, and David D Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. ICML (1), 28:115-123, 2013.</p>
<p>James S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. NIPS, pp. 2546-2554, 2011.</p>
<p>Dimitri P Bertsekas. Convex optimization algorithms. Athena Scientific Belmont, 2015.
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). arXiv preprint arXiv:1511.07289, 2015.</p>
<p>Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. IJCAI, 2015.</p>
<p>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTATS, 9:249-256, 2010.</p>
<p>Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron C Courville, and Yoshua Bengio. Maxout networks. ICML (3), 28:1319-1327, 2013.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016.</p>
<p>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.</p>
<p>Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4:237-285, 1996.</p>
<p>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015.</p>
<p>Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. AISTATS, 2(3):6, 2015.</p>
<p>Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree. International Conference on Artificial Intelligence and Statistics, 2016.</p>
<p>Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. $J M L R, 17(39): 1-40,2016$.</p>
<p>Ming Liang and Xiaolin Hu. Recurrent convolutional neural network for object recognition. CVPR, pp. 3367-3375, 2015.</p>
<p>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3-4):293-321, 1992.</p>
<p>Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.</p>
<p>Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Nicolas Pinto, David Doukhan, James J DiCarlo, and David D Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Computational Biology, 5(11):e1000579, 2009.</p>
<p>Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.</p>
<p>Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. In Advances in Neural Information Processing Systems 29, pp. 4053-4061. 2016.</p>
<p>J David Schaffer, Darrell Whitley, and Larry J Eshelman. Combinations of genetic algorithms and neural networks: A survey of the state of the art. International Workshop on Combinations of Genetic Algorithms and Neural Networks, pp. 1-37, 1992.</p>
<p>Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied to house numbers digit classification. ICPR, pp. 3288-3291, 2012.</p>
<p>Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian detection with unsupervised multi-stage feature learning. CVPR, pp. 3626-3633, 2013.</p>
<p>Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1): $148-175,2016$.</p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p>Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. NIPS, pp. 2951-2959, 2012.</p>
<p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.</p>
<p>Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computation, 10(2):99-127, 2002.</p>
<p>Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. NIPS, pp. 2004-2012, 2013.</p>
<p>Phillip Verbancsics and Josh Harguess. Generative neuroevolution for deep learning. arXiv preprint arXiv:1312.5355, 2013.</p>
<p>Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. European Conference on Machine Learning, pp. 437-448, 2005.</p>
<p>Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. ICML, pp. 1058-1066, 2013.</p>
<p>Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge, England, 1989.</p>
<h1>APPENDIX</h1>
<h2>A Algorithm</h2>
<p>We first describe the main components of the MetaQNN algorithm. Algorithm 1 shows the main loop, where the parameter $M$ would determine how many models to run for a given $\epsilon$ and the parameter $K$ would determine how many times to sample the replay database to update $Q$-values on each iteration. The function TRAIN refers to training the specified network and returns a validation accuracy. Algorithm 2 details the method for sampling a new network using the $\epsilon$-greedy strategy, where we assume we have a function TRANSITION that returns the next state given a state and action. Finally, Algorithm 3 implements the $Q$-value update detailed in Equation 3, with discounting factor set to 1 , for an entire state sequence in temporally reversed order.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="n">learning</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">CNN</span><span class="w"> </span><span class="n">Topologies</span>
<span class="w">    </span><span class="k">Initialize</span><span class="err">:</span>
<span class="w">        </span><span class="n">replay_memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">[]\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\{</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">forall</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">S</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">U</span><span class="err">}</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="mf">0.5</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">episode</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">SAMPLE</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">_</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">NEW</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">_</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">NETWORK</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">epsilon</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">accuracy</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">TRAIN</span><span class="err">}</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">replay_memory</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="w"> </span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="p">,</span><span class="w"> </span><span class="n">accuracy</span><span class="p">))</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">S_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">U_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}\</span><span class="p">),</span><span class="w"> </span><span class="n">accuracy</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Uniform</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\{\</span><span class="p">)</span><span class="w"> </span><span class="n">replay_memory</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">UPDATE_Q_VALUES</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">S_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">U_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">accuracy</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">SAMPLE</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">SAMPLE_NEW_NETWORK</span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">epsilon</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="k">Initialize</span><span class="err">:</span>
<span class="w">        </span><span class="k">state</span><span class="w"> </span><span class="k">sequence</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">s_{\text {START }}\right</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">action</span><span class="w"> </span><span class="k">sequence</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">U</span><span class="o">=</span><span class="err">[]\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">U</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">neq</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">terminate</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">alpha</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Uniform</span><span class="err">}</span><span class="o">[</span><span class="n">0,1)\)</span>
<span class="n">        if \(\alpha&gt;\epsilon\) then</span>
<span class="n">            \(u=\operatorname{argmax}_{u \in \mathcal{U}(S[-1</span><span class="o">]</span><span class="p">)</span><span class="err">}</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">(S[-1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="err">]\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">TRANSITION</span><span class="err">}</span><span class="p">(</span><span class="n">S</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">u</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Uniform</span><span class="err">}\{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">U</span><span class="err">}</span><span class="p">(</span><span class="n">S</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="p">)</span><span class="err">\}\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">TRANSITION</span><span class="err">}</span><span class="p">(</span><span class="n">S</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">U</span><span class="err">\</span><span class="p">).</span><span class="n">append</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">u</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">u</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">terminate</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="err">\</span><span class="p">).</span><span class="n">append</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="err">\</span><span class="p">)</span>
<span class="n">Algorithm</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">UPDATE_Q_VALUES</span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">accuracy</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="o">[</span><span class="n">S[-1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="err">]</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">S[-1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="o">[</span><span class="n">-1</span><span class="o">]</span><span class="err">]</span><span class="o">+</span><span class="err">\</span><span class="n">alpha</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">accuracy</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">S</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="o">[</span><span class="n">S[i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">]</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">S[i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">]</span><span class="o">+</span><span class="err">\</span><span class="n">alpha</span><span class="w"> </span><span class="err">\</span><span class="nf">max</span><span class="w"> </span><span class="n">_</span><span class="err">{</span><span class="n">u</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">U</span><span class="err">}</span><span class="p">(</span><span class="n">S</span><span class="o">[</span><span class="n">i+1</span><span class="o">]</span><span class="p">)</span><span class="err">}</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">S[i+1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="err">]\</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<h1>B REPRESENTATION SIZE BINNING</h1>
<p>As mentioned in Section 4.1 of the main text, we introduce a parameter called representation size to prohibit the agent from taking actions that can reduce the intermediate signal representation to a size that is too small for further processing. However, this process leads to uncertainties in state transitions, as illustrated in Figure A1, which is handled by the standard $Q$-learning formulation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A1: Representation size binning: In this figure, we show three example state transitions. The true representation size ( $R$-size) parameter is included in the figure to show the true underlying state. Assuming there are two $R$-size bins, $R$-size $\operatorname{Bin}<em 2="2">{1}:[8, \infty)$ and $R$-size $\operatorname{Bin}</em>}:(0,7]$, Figure A1a shows the case where the initial state is in $R$-size $\operatorname{Bin<em 2="2">{1}$ and true representation size is 18 . After the agent chooses to pool with a $2 \times 2$ filter with stride 2 , the true representation size reduces to 9 but the $R$-size bin does not change. In Figure A1b, the same $2 \times 2$ pooling layer with stride 2 reduces the actual representation size of 14 to 7 , but the bin changes to $R$-size $\operatorname{Bin}</em>$. Therefore, in figures A1a and A1b, the agent ends up in different final states, despite originating in the same initial state and choosing the same action. Figure A1c shows that in our state-action space, when the agent takes an action that reduces the representation size, it will have uncertainty in which state it will transition to.</p>
<h2>C MNIST EXPERIMENT</h2>
<p>We noticed that the final MNIST models were prone to overfitting, so we increased dropout and did a small grid search for the weight regularization parameter. For both tuning and final training, we warmed the model with the learned weights from after the first epoch of initial training. The final models and solvers can be found on our project website https://bowenbaker.github.io/metaqnu/. Figure A2 shows the $Q$-Learning performance for the MNIST experiment.</p>
<h2>D FURTHER ANALYSIS OF $Q$-LEARNING</h2>
<p>Figure 3 of the main text and Figure A2 show that as the agent begins to exploit, it improves in architecture selection. It is also informative to look at the distribution of models chosen at each $\epsilon$. Figure A4 gives further insight into the performance achieved at each $\epsilon$ for both experiments.</p>
<h2>D. 1 Q-LEARNING StABILITY</h2>
<p>Because the $Q$-learning agent explores via a random or semi-random distribution, it is natural to ask whether the agent can consistently improve architecture performance. While the success of the three independent experiments described in the main text allude to stability, here we present further evidence. We conduct 10 independent runs of the $Q$-learning procedure on $10 \%$ of the SVHN dataset (which corresponds to $\sim 7,000$ training examples). We use a smaller dataset to reduce the computation time of each independent run to 10GPU-days, as opposed to the 100GPU-days it would take on the full dataset. As can be seen in Figure A3, the $Q$-learning procedure with the exploration schedule detailed in Table 2 is fairly stable. The standard deviation at $\epsilon=1$ is notably smaller than at other stages, which we attribute to the large difference in number of samples at each stage.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure A2: MNIST $\boldsymbol{Q}$-Learning Performance. The blue line shows a rolling mean of model accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model. Each bar (in light blue) marks the average accuracy over all models that were sampled during the exploration phase with the labeled $\epsilon$. As $\epsilon$ decreases, the average accuracy goes up, demonstrating that the agent learns to select better-performing CNN architectures.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure A3: Figure A3a shows the mean model accuracy and standard deviation at each $\epsilon$ over 10 independent runs of the $Q$-learning procedure on $10 \%$ of the SVHN dataset. Figure A3b shows the mean model accuracy at each $\epsilon$ for each independent experiment. Despite some variance due to a randomized exploration strategy, each independent run successfully improves architecture performance.</p>
<p>Furthermore, the best model found during each run had remarkably similar performance with a mean accuracy of $88.25 \%$ and standard deviation of $0.58 \%$, which shows that each run successfully found at least one very high performing model. Note that we did not use an extended training schedule to improve performance in this experiment.</p>
<h1>D. 2 Q-Value Analysis</h1>
<p>We now analyze the actual $Q$-values generated by the agent during the training process. The learning agent iteratively updates the $Q$-values of each path during the $\epsilon$-greedy exploration. Each $Q$-value is initialized at 0.5 . After the $\epsilon$-schedule is complete, we can analyze the final $Q$-value associated with each path to gain insights into the layer selection process. In the left column of Figure A5, we plot the average $Q$-value for each layer type at different layer depths (for both SVHN and CIFAR10) datasets. Roughly speaking, a higher $Q$-value associated with a layer type indicates a higher probability that the agent will pick that layer type. In Figure A5, we observe that, while the average $Q$-value is higher for convolution and pooling layers at lower layer depths, the $Q$-values for fullyconnected and termination layers (softmax and global average pooling) increase as we go deeper into the network. This observation matches with traditional network designs.
We can also plot the average $Q$-values associated with different layer parameters for further analysis. In the right column of Figure A5, we plot the average $Q$-values for convolution layers with receptive</p>
<p>field sizes 1,3 , and 5 at different layer depths. The plots show that layers with receptive field size of 5 have a higher $Q$-value as compared to sizes 1 and 3 as we go deeper into the networks. This indicates that it might be beneficial to use larger receptive field sizes in deeper networks.</p>
<p>In summary, the $Q$-learning method enables us to perform analysis on the relative benefits of different design parameters of our state space, and possibly gain insights for new CNN designs.</p>
<h1>E Top TOPOLOGIES SELECTED BY ALGORITHM</h1>
<p>In Tables A1 through A3, we present the top five model architectures selected with Q-learning for each dataset, along with their prediction error reported on the test set, and their total number of parameters. To download the Caffe solver and prototext files, please visit https://bowenbaker.github.io/metaqnn/.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Architecture</th>
<th style="text-align: center;">Test Error (\%)</th>
<th style="text-align: center;"># Params $\left(10^{6}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(512,5,1), \mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{C}(256,3,1), \mathrm{P}(5,3), \mathrm{C}(512,3,1),\right.$ <br> $\left.\mathrm{C}(512,5,1), \mathrm{P}(2,2), \mathrm{SM}(10)\right]$</td>
<td style="text-align: center;">6.92</td>
<td style="text-align: center;">11.18</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,1,1), \mathrm{C}(512,3,1), \mathrm{C}(64,1,1), \mathrm{C}(128,3,1), \mathrm{P}(2,2), \mathrm{C}(256,3,1),\right.$ <br> $\mathrm{P}(2,2), \mathrm{C}(512,3,1), \mathrm{P}(3,2), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">2.17</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,3,1), \mathrm{C}(128,1,1), \mathrm{C}(512,5,1), \mathrm{P}(2,2), \mathrm{C}(128,3,1), \mathrm{P}(2,2),\right.$ <br> $\mathrm{C}(64,3,1), \mathrm{C}(64,5,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">8.88</td>
<td style="text-align: center;">2.42</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(256,3,1), \mathrm{C}(256,3,1), \mathrm{P}(5,3), \mathrm{C}(256,1,1), \mathrm{C}(128,3,1), \mathrm{P}(2,2),\right.$ <br> $\mathrm{C}(128,3,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">9.24</td>
<td style="text-align: center;">1.10</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,5,1), \mathrm{C}(512,3,1), \mathrm{P}(2,2), \mathrm{C}(128,1,1), \mathrm{C}(128,5,1), \mathrm{P}(3,2),\right.$ <br> $\mathrm{C}(512,3,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">11.63</td>
<td style="text-align: center;">1.66</td>
</tr>
</tbody>
</table>
<p>Table A1: Top 5 model architectures: CIFAR-10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Architecture</th>
<th style="text-align: center;">Test Error (\%)</th>
<th style="text-align: center;"># Params $\left(10^{6}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,3,1), \mathrm{P}(2,2), \mathrm{C}(64,5,1), \mathrm{C}(512,5,1), \mathrm{C}(256,3,1), \mathrm{C}(512,3,1),\right.$ <br> $\mathrm{P}(2,2), \mathrm{C}(512,3,1), \mathrm{C}(256,5,1), \mathrm{C}(256,3,1), \mathrm{C}(128,5,1), \mathrm{C}(64,3,1),$ <br> $\mathrm{SM}(10)]$</td>
<td style="text-align: center;">2.24</td>
<td style="text-align: center;">9.81</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,1,1), \mathrm{C}(256,5,1), \mathrm{C}(128,5,1), \mathrm{P}(2,2), \mathrm{C}(256,5,1), \mathrm{C}(256,1,1),\right.$ <br> $\mathrm{C}(256,3,1), \mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{C}(512,5,1), \mathrm{C}(256,3,1),$ <br> $\mathrm{C}(128,3,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">10.38</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,5,1), \mathrm{C}(128,3,1), \mathrm{C}(64,5,1), \mathrm{P}(5,3), \mathrm{C}(128,3,1), \mathrm{C}(512,5,1),\right.$ <br> $\mathrm{C}(256,5,1), \mathrm{C}(128,5,1), \mathrm{C}(128,5,1), \mathrm{C}(128,3,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">2.32</td>
<td style="text-align: center;">6.83</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,1,1), \mathrm{C}(256,5,1), \mathrm{C}(128,5,1), \mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{P}(2,2),\right.$ <br> $\mathrm{C}(128,1,1), \mathrm{C}(512,3,1), \mathrm{C}(256,5,1), \mathrm{P}(2,2), \mathrm{C}(64,5,1), \mathrm{C}(64,1,1)$, <br> $\mathrm{SM}(10)]$</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">6.99</td>
</tr>
<tr>
<td style="text-align: left;">$\left[\mathrm{C}(128,1,1), \mathrm{C}(256,5,1), \mathrm{C}(128,5,1), \mathrm{C}(256,5,1), \mathrm{C}(256,5,1),\right.$ <br> $\mathrm{C}(256,1,1), \mathrm{P}(3,2), \mathrm{C}(128,1,1), \mathrm{C}(256,5,1), \mathrm{C}(512,5,1), \mathrm{C}(256,3,1)$, <br> $\mathrm{C}(128,3,1), \mathrm{SM}(10)]$</td>
<td style="text-align: center;">2.36</td>
<td style="text-align: center;">10.05</td>
</tr>
</tbody>
</table>
<p>Table A2: Top 5 model architectures: SVHN. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved $2.28 \%$ on the test set performed the best on the validation set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Architecture</th>
<th style="text-align: center;">Test Error (\%)</th>
<th style="text-align: center;"># Params $\left(10^{9}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; {\left[\mathrm{C}(64,1,1), \quad \mathrm{C}(256,3,1), \quad \mathrm{P}(2,2), \quad \mathrm{C}(512,3,1), \quad \mathrm{C}(256,1,1), \quad \mathrm{P}(5,3)\right.} \ &amp; \left.\mathrm{C}(256,3,1), \mathrm{C}(512,3,1), \mathrm{FC}(512), \mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(128,3,1), \mathrm{C}(64,1,1), \mathrm{C}(64,3,1), \mathrm{C}(64,5,1), \mathrm{P}(2,2), \mathrm{C}(128,3,1), \mathrm{P}(3,2)\right.} \ &amp; \left.\mathrm{C}(512,3,1), \mathrm{FC}(512), \mathrm{FC}(128), \mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(512,1,1), \mathrm{C}(128,3,1), \mathrm{C}(128,5,1), \mathrm{C}(64,1,1), \mathrm{C}(256,5,1), \mathrm{C}(64,1,1)\right.} \ &amp; \left.\mathrm{P}(5,3), \mathrm{C}(512,1,1), \mathrm{C}(512,3,1), \mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{C}(256,5,1)\right.} \ &amp; \left.\mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(64,3,1), \mathrm{C}(128,3,1), \mathrm{C}(512,1,1), \mathrm{C}(256,1,1), \mathrm{C}(256,5,1), \mathrm{C}(128,3,1)\right.} \ &amp; \left.\mathrm{P}(5,3), \mathrm{C}(512,1,1), \mathrm{C}(512,3,1), \mathrm{C}(128,5,1), \mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(64,3,1), \mathrm{C}(128,1,1), \mathrm{P}(2,2), \mathrm{C}(256,3,1), \mathrm{C}(128,5,1), \mathrm{C}(64,1,1)\right.} \ &amp; \left.\mathrm{C}(512,5,1), \mathrm{C}(128,5,1), \mathrm{C}(64,1,1), \mathrm{C}(512,5,1), \mathrm{C}(256,5,1), \mathrm{C}(64,5,1)\right.} \ &amp; \left.\mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(64,1,1), \mathrm{C}(256,5,1), \mathrm{C}(256,5,1), \mathrm{C}(512,1,1), \mathrm{C}(64,3,1), \mathrm{P}(5,3)\right.} \ &amp; \left.\mathrm{C}(256,5,1), \mathrm{C}(256,5,1), \mathrm{C}(512,5,1), \mathrm{C}(64,1,1), \mathrm{C}(128,5,1), \mathrm{C}(512,5,1)\right.} \ &amp; \left.\mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(128,3,1), \mathrm{C}(512,3,1), \mathrm{P}(2,2), \mathrm{C}(256,3,1), \mathrm{C}(128,5,1), \mathrm{C}(64,1,1)\right.} \ &amp; \left.\mathrm{C}(64,5,1), \mathrm{C}(512,5,1), \mathrm{GAP}(10), \mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{C}(512,3,1), \mathrm{C}(256,5,1), \mathrm{C}(512,1,1), \mathrm{P}(5,3)\right.} \ &amp; \left.\mathrm{C}(256,3,1), \mathrm{C}(64,3,1), \mathrm{C}(256,5,1), \mathrm{C}(512,3,1), \mathrm{C}(128,5,1), \mathrm{C}(512,5,1)\right.} \ &amp; \left.\mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(512,5,1), \quad \mathrm{C}(128,5,1), \quad \mathrm{C}(128,5,1), \quad \mathrm{C}(128,3,1), \quad \mathrm{C}(256,3,1)\right.} \ &amp; \left.\mathrm{C}(512,5,1), \mathrm{C}(256,3,1), \mathrm{C}(128,3,1), \mathrm{SM}(10)\right] \ &amp; {\left[\mathrm{C}(64,5,1), \mathrm{C}(512,5,1), \mathrm{P}(3,2), \mathrm{C}(256,5,1), \mathrm{C}(256,3,1), \mathrm{C}(256,3,1)\right.} \ &amp; \left.\mathrm{C}(128,1,1), \mathrm{C}(256,3,1), \mathrm{C}(256,5,1), \mathrm{C}(64,1,1), \mathrm{C}(256,3,1), \mathrm{C}(64,3,1)\right.} \ &amp; \left.\mathrm{SM}(10)\right] \ &amp; \hline \end{aligned}$ 0.56 0.55 7.25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table A3: Top 10 model architectures: MNIST. We report the top 10 models for MNIST because we included all 10 in our final ensemble. Note that we do not report the best accuracy on test set from the above models in Tables 3 and 4 from the main text. This is because the model that achieved $0.44 \%$ on the test set performed the best on the validation set.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure A4: Accuracy Distribution versus $\epsilon$ : Figures A4a, A4c, and A4e show the accuracy distribution for each $\epsilon$ for the SVHN, CIFAR-10, and MNIST experiments, respectively. Figures A4b, A4d, and A4f show the accuracy distributions for the initial $\epsilon=1$ and the final $\epsilon=0.1$. One can see that the accuracy distribution becomes much more peaked in the high accuracy ranges at small $\epsilon$ for each experiment.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure A5: Average $Q$-Value versus Layer Depth for different layer types are shown in the left column. Average $Q$-Value versus Layer Depth for different receptive field sizes of the convolution layer are shown in the right column.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Results in this column obtained with the top MetaQNN architecture for CIFAR-10, trained from random initialization with CIFAR-100 data.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
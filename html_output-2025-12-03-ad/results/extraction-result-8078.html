<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8078 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8078</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8078</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-7868eec09c2999bf1022fc0f5766878a528ceaf5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7868eec09c2999bf1022fc0f5766878a528ceaf5" target="_blank">A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies, suggesting that automatically generated UMBRELA judgments can replace fully manual judgments to accurately capture run-level effectiveness.</p>
                <p><strong>Paper Abstract:</strong> The application of large language models to provide relevance assessments presents exciting opportunities to advance information retrieval, natural language processing, and beyond, but to date many unknowns remain. This paper reports on the results of a large-scale evaluation (the TREC 2024 RAG Track) where four different relevance assessment approaches were deployed in situ: the"standard"fully manual process that NIST has implemented for decades and three different alternatives that take advantage of LLMs to different extents using the open-source UMBRELA tool. This setup allows us to correlate system rankings induced by the different approaches to characterize tradeoffs between cost and quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system rankings induced by automatically generated relevance assessments from UMBRELA correlate highly with those induced by fully manual assessments across a diverse set of 77 runs from 19 teams. Our results suggest that automatically generated UMBRELA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits. Overall, human assessors appear to be stricter than UMBRELA in applying relevance criteria. Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8078.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8078.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMBRELA vs Fully Manual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA (automatic LLM judgments) compared to fully manual NIST relevance assessments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In-situ comparison of fully automatic LLM-generated relevance judgments (via UMBRELA using GPT-4o) against gold-standard NIST human (fully manual) assessments for the TREC 2024 RAG retrieval task, measuring run-level agreement in retrieval metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval (retrieval stage of Retrieval-Augmented Generation task)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (301 topics; subset of 27 topics used for direct overlap with fully manual)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (via UMBRELA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Publicly available OpenAI GPT-4o used zero-shot within UMBRELA's DNA-style prompt (prompt shown in paper Figure 2); UMBRELA is an open-source toolkit reproducing Thomas et al.'s method.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST assessors (experienced/expert assessors used as gold-standard reference in TREC process)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.89</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>more lenient relevance criteria by LLM; LLM draws inferences humans find unwarranted; disagreements on topic interpretation; absence of human-human inter-annotator baseline to contextualize differences</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Overall high run-level correlation between UMBRELA and fully manual (nDCG@20, nDCG@100, Recall@100), but per topic/run scatter exists; humans tend to be stricter, downgrading items the LLM rates higher; some cases where UMBRELA aligns better with an alternate plausible interpretation than the human assessor; LLM sometimes misses subtle mentions that humans pick up.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High run-level agreement with gold standard while enabling full-coverage automatic assessment across all 301 topics; reduced assessment effort (UMBRELA reduced pool size shown to assess fewer passages in LLM-assisted workflows by ~40%); scalability and lower cost.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA generated zero-shot relevance grades via GPT-4o using the DNA-style prompt (Figure 2). Pools constructed to depth 20 from 77 runs (19 teams). Metrics: nDCG@20, nDCG@100, Recall@100. Agreement computed with Kendall's tau on run-level scores for overlapping 27 topics; additional analyses: per-topic averaged tau and correlation across all topic/run combos; Monte Carlo sampling (2/3, 1/3 topics) for confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8078.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8078.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMBRELA vs Manual w/ Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA (automatic LLM judgments) compared to NIST manual assessment with LLM-based filtering (assessors not shown LLM labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison where UMBRELA first filters out passages judged non-relevant and then NIST assessors judge the remaining pool (assessors unaware of UMBRELA labels); analysis of agreement and cost savings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval (retrieval stage of RAG task)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (29 topics in this condition; overlapping topic subset with UMBRELA used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (via UMBRELA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Public GPT-4o used zero-shot in UMBRELA; UMBRELA filtered out passages labeled non-relevant so assessors only saw remaining passages.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST assessors (expert annotators), assessors did not see UMBRELA labels in this condition</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.885</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>LLM filtering can remove passages humans might later judge relevant (risk of false negatives); LLM exhibits more lenient labeling for positives that humans downgrade; reliance on single-LLM decisions without human inspection risks missed relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Run-level agreement with fully manual remains high; UMBRELA reduced pool sizes substantially (~38–41% reductions reported), saving assessor effort; nevertheless, some topics showed disagreements due to topic interpretation differences and missed subtle relevance signals by LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Substantial reduction in human assessment effort (~40% pool size decrease reported), faster triage of non-relevant passages while maintaining high run-level agreement with full manual judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA automatically labeled all documents; passages labeled non-relevant by UMBRELA were not shown to NIST assessors (assessors unaware of LLM labels). Overlapping topics compared with fully manual; metrics and Kendall's tau computed as in main analysis; descriptive statistics in Table 1 show pool size decreases and grade distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8078.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8078.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMBRELA vs Manual Post-Editing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA (automatic LLM judgments) compared to human post-editing of UMBRELA labels (assessors shown LLM labels and asked to edit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison where UMBRELA provides labels and NIST assessors review and post-edit these labels (assessors aware of LLM grades), used to study whether human-in-the-loop anchoring improves agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval (retrieval stage of RAG task)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (30 topics in this condition; overlapping with UMBRELA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (via UMBRELA)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Public GPT-4o zero-shot used within UMBRELA; assessors were shown grouped passages by UMBRELA-assigned grade and asked to post-edit.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST assessors (expert annotators) who were provided UMBRELA labels as anchors and allowed to change them</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.867</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Human anchoring to LLM labels can still result in LLM over-generation of relevance (LLM labels often higher than final human grade); some passages remain misclassified by LLM and require downgrading by humans; cases exist where LLM inferences are overreaching.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Post-editing provides an anchor and in some visualizations reduced scatter for topic/run combos, but overall human involvement did not noticeably increase run-level correlation with fully manual gold standard; humans often downgraded UMBRELA labels (more common) though occasional upgrades by humans occurred.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Provides anchors that can speed up human review by grouping by predicted grade; reduces assessor workload relative to fully manual while still preserving high run-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA produced labels and grouped passages by predicted grade; NIST assessors reviewed grouped items with knowledge of LLM labels and could edit; metrics (nDCG@20/100, Recall@100) computed and Kendall's tau measured at run-level, per-topic average, and across topic/run combos; confusion matrices produced to inspect grade-level disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models Can Accurately Predict Searcher Preferences <em>(Rating: 2)</em></li>
                <li>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor <em>(Rating: 2)</em></li>
                <li>LLMJudge: LLMs for Relevance Judgments <em>(Rating: 2)</em></li>
                <li>Don't Use LLMs to Make Relevance Judgments <em>(Rating: 1)</em></li>
                <li>A Comparison of Methods for Evaluating Generative IR <em>(Rating: 1)</em></li>
                <li>Judging LLM-as-a-Judge with MTBench and Chatbot Arena <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8078",
    "paper_id": "paper-7868eec09c2999bf1022fc0f5766878a528ceaf5",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "UMBRELA vs Fully Manual",
            "name_full": "UMBRELA (automatic LLM judgments) compared to fully manual NIST relevance assessments",
            "brief_description": "In-situ comparison of fully automatic LLM-generated relevance judgments (via UMBRELA using GPT-4o) against gold-standard NIST human (fully manual) assessments for the TREC 2024 RAG retrieval task, measuring run-level agreement in retrieval metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "evaluation_task": "Ad-hoc passage retrieval (retrieval stage of Retrieval-Augmented Generation task)",
            "dataset_name": "MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (301 topics; subset of 27 topics used for direct overlap with fully manual)",
            "judge_model_name": "GPT-4o (via UMBRELA)",
            "judge_model_details": "Publicly available OpenAI GPT-4o used zero-shot within UMBRELA's DNA-style prompt (prompt shown in paper Figure 2); UMBRELA is an open-source toolkit reproducing Thomas et al.'s method.",
            "human_evaluator_type": "NIST assessors (experienced/expert assessors used as gold-standard reference in TREC process)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.89,
            "reported_loss_aspects": "more lenient relevance criteria by LLM; LLM draws inferences humans find unwarranted; disagreements on topic interpretation; absence of human-human inter-annotator baseline to contextualize differences",
            "qualitative_findings": "Overall high run-level correlation between UMBRELA and fully manual (nDCG@20, nDCG@100, Recall@100), but per topic/run scatter exists; humans tend to be stricter, downgrading items the LLM rates higher; some cases where UMBRELA aligns better with an alternate plausible interpretation than the human assessor; LLM sometimes misses subtle mentions that humans pick up.",
            "advantages_of_llm_judge": "High run-level agreement with gold standard while enabling full-coverage automatic assessment across all 301 topics; reduced assessment effort (UMBRELA reduced pool size shown to assess fewer passages in LLM-assisted workflows by ~40%); scalability and lower cost.",
            "experimental_setting": "UMBRELA generated zero-shot relevance grades via GPT-4o using the DNA-style prompt (Figure 2). Pools constructed to depth 20 from 77 runs (19 teams). Metrics: nDCG@20, nDCG@100, Recall@100. Agreement computed with Kendall's tau on run-level scores for overlapping 27 topics; additional analyses: per-topic averaged tau and correlation across all topic/run combos; Monte Carlo sampling (2/3, 1/3 topics) for confidence intervals.",
            "uuid": "e8078.0",
            "source_info": {
                "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "UMBRELA vs Manual w/ Filtering",
            "name_full": "UMBRELA (automatic LLM judgments) compared to NIST manual assessment with LLM-based filtering (assessors not shown LLM labels)",
            "brief_description": "Comparison where UMBRELA first filters out passages judged non-relevant and then NIST assessors judge the remaining pool (assessors unaware of UMBRELA labels); analysis of agreement and cost savings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "evaluation_task": "Ad-hoc passage retrieval (retrieval stage of RAG task)",
            "dataset_name": "MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (29 topics in this condition; overlapping topic subset with UMBRELA used for comparison)",
            "judge_model_name": "GPT-4o (via UMBRELA)",
            "judge_model_details": "Public GPT-4o used zero-shot in UMBRELA; UMBRELA filtered out passages labeled non-relevant so assessors only saw remaining passages.",
            "human_evaluator_type": "NIST assessors (expert annotators), assessors did not see UMBRELA labels in this condition",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.885,
            "reported_loss_aspects": "LLM filtering can remove passages humans might later judge relevant (risk of false negatives); LLM exhibits more lenient labeling for positives that humans downgrade; reliance on single-LLM decisions without human inspection risks missed relevance.",
            "qualitative_findings": "Run-level agreement with fully manual remains high; UMBRELA reduced pool sizes substantially (~38–41% reductions reported), saving assessor effort; nevertheless, some topics showed disagreements due to topic interpretation differences and missed subtle relevance signals by LLM.",
            "advantages_of_llm_judge": "Substantial reduction in human assessment effort (~40% pool size decrease reported), faster triage of non-relevant passages while maintaining high run-level agreement with full manual judgments.",
            "experimental_setting": "UMBRELA automatically labeled all documents; passages labeled non-relevant by UMBRELA were not shown to NIST assessors (assessors unaware of LLM labels). Overlapping topics compared with fully manual; metrics and Kendall's tau computed as in main analysis; descriptive statistics in Table 1 show pool size decreases and grade distributions.",
            "uuid": "e8078.1",
            "source_info": {
                "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "UMBRELA vs Manual Post-Editing",
            "name_full": "UMBRELA (automatic LLM judgments) compared to human post-editing of UMBRELA labels (assessors shown LLM labels and asked to edit)",
            "brief_description": "Comparison where UMBRELA provides labels and NIST assessors review and post-edit these labels (assessors aware of LLM grades), used to study whether human-in-the-loop anchoring improves agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "evaluation_task": "Ad-hoc passage retrieval (retrieval stage of RAG task)",
            "dataset_name": "MS MARCO V2.1 deduped segment collection; TREC 2024 RAG Track topics (30 topics in this condition; overlapping with UMBRELA)",
            "judge_model_name": "GPT-4o (via UMBRELA)",
            "judge_model_details": "Public GPT-4o zero-shot used within UMBRELA; assessors were shown grouped passages by UMBRELA-assigned grade and asked to post-edit.",
            "human_evaluator_type": "NIST assessors (expert annotators) who were provided UMBRELA labels as anchors and allowed to change them",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.867,
            "reported_loss_aspects": "Human anchoring to LLM labels can still result in LLM over-generation of relevance (LLM labels often higher than final human grade); some passages remain misclassified by LLM and require downgrading by humans; cases exist where LLM inferences are overreaching.",
            "qualitative_findings": "Post-editing provides an anchor and in some visualizations reduced scatter for topic/run combos, but overall human involvement did not noticeably increase run-level correlation with fully manual gold standard; humans often downgraded UMBRELA labels (more common) though occasional upgrades by humans occurred.",
            "advantages_of_llm_judge": "Provides anchors that can speed up human review by grouping by predicted grade; reduces assessor workload relative to fully manual while still preserving high run-level agreement.",
            "experimental_setting": "UMBRELA produced labels and grouped passages by predicted grade; NIST assessors reviewed grouped items with knowledge of LLM labels and could edit; metrics (nDCG@20/100, Recall@100) computed and Kendall's tau measured at run-level, per-topic average, and across topic/run combos; confusion matrices produced to inspect grade-level disagreements.",
            "uuid": "e8078.2",
            "source_info": {
                "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models Can Accurately Predict Searcher Preferences",
            "rating": 2
        },
        {
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "rating": 2
        },
        {
            "paper_title": "LLMJudge: LLMs for Relevance Judgments",
            "rating": 2
        },
        {
            "paper_title": "Don't Use LLMs to Make Relevance Judgments",
            "rating": 1
        },
        {
            "paper_title": "A Comparison of Methods for Evaluating Generative IR",
            "rating": 1
        },
        {
            "paper_title": "Judging LLM-as-a-Judge with MTBench and Chatbot Arena",
            "rating": 1
        }
    ],
    "cost": 0.010654,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</h1>
<p>Shivani Upadhyay<br>University of Waterloo Ontario, Canada<br>Daniel Campos Snowflake San Mateo, USA</p>
<p>Ronak Pradeep<br>University of Waterloo Ontario, Canada<br>Nick Craswell Microsoft<br>Seattle, USA</p>
<p>Nandan Thakur<br>University of Waterloo Ontario, Canada<br>Ian Soboroff NIST Gaithersburg, USA</p>
<h2>Hoa Trang Dang</h2>
<p>NIST
Gaithersburg, USA</p>
<h2>Jimmy Lin</h2>
<p>University of Waterloo
Ontario, Canada</p>
<h2>Abstract</h2>
<p>The application of large language models to provide relevance assessments presents exciting opportunities to advance information retrieval, natural language processing, and beyond, but to date many unknowns remain. This paper reports on the results of a large-scale evaluation (the TREC 2024 RAG Track) where four different relevance assessment approaches were deployed in situ: the "standard" fully manual process that NIST has implemented for decades and three different alternatives that take advantage of LLMs to different extents using the open-source UMBRELA tool. This setup allows us to correlate system rankings induced by the different approaches to characterize tradeoffs between cost and quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system rankings induced by automatically generated relevance assessments from UMBRELA correlate highly with those induced by fully manual assessments across a diverse set of 77 runs from 19 teams. Our results suggest that automatically generated UMBRELA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits. Overall, human assessors appear to be stricter than UMBRELA in applying relevance criteria. Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.</p>
<h2>1 Introduction</h2>
<p>Relevance assessments are critical for evaluating information access systems, providing guidance for model training in information retrieval (IR), natural language processing (NLP), and beyond. Acquiring relevance assessments from humans, of course, is an expensive proposition, not only in terms of compensation for the assessors, but also overhead in articulating clear guidelines, coordinating large-scale annotation efforts, ensuring consistent quality, etc. The advent of large language models (LLMs) provides an opportunity to automate these assessments, potentially accelerating improvements in retrieval methods [15]-although see Soboroff [27] for a contrary position.</p>
<p>Most recently, Thomas et al. [28] revealed that LLMs have been used to provide relevance assessments at Bing since late 2022. While
the authors quite clearly assert that LLMs can accurately predict searcher preferences in the context of Bing, they began from the industry perspective and retrospectively "backported" their proposed fully automatic technique to an old TREC collection for validation. In this work, we adopt a complementary strategy, exploring different applications of LLMs to generate relevance assessments in situ, directly deployed in a large-scale retrieval-augmented generation (RAG) evaluation organized by the National Institute of Standards and Technology (NIST) as part of the Text Retrieval Conference (TREC) series. Now in its 33rd year, TREC is widely acknowledged as representing the "gold standard" in academic IR evaluations.</p>
<p>We examined three different evaluation approaches that vary in levels of LLM involvement, which are compared against fully manual relevance assessments. Our study used the UMBRELA tool [29], which has been previously validated as a successful reproduction of Thomas et al. [28]. Given this setup, we explored the following research questions:</p>
<p>RQ1 To what extent can automatically generated relevance assessments from LLMs replace NIST assessors? Specifically, we examined three scenarios involving different levels of LLM involvement: fully automatic, manual post-editing, and manual filtering.
RQ2 Different levels of LLM involvement lead to different tradeoffs between cost and quality in relevance assessments. Can we characterize these tradeoffs?
RQ3 Are there any quantitative or qualitative differences between human versus LLM judgments? Are these differences affected by different levels of LLM involvement?</p>
<p>In this work, we analyzed a diverse set of 77 runs from 19 teams that contributed to the TREC 2024 RAG Track. Our findings can be summarized as follows:</p>
<ul>
<li>For RQ1, we find that system rankings induced by automatically generated relevance assessments using UMBRELA correlate highly with those induced by manual assessments in terms of nDCG@20, nDCG@100, and Recall@100.</li>
<li>
<p>For RQ2, we find that, surprisingly, LLM assistance does not appear to increase correlation with fully manual assessments. Thus, the additional costs associated with hybrid human-LLM solutions do not appear to have obvious tangible benefits.</p>
</li>
<li>
<p>For RQ3, analyses suggest that assessors apply stricter relevance criteria than UMBRELA. We find cases where the LLM draws inferences that humans would consider unwarranted, and vice versa, but overall, the first case is more common.
The contribution of this work is, to our knowledge, the first largescale study of different in situ approaches to using automatically generated LLM assessments, where system rankings induced by these approaches are correlated against reference rankings generated by fully manual assessments. This represents a first step outside the industry setting to validate methods for automatically generating relevance assessments using LLMs. In this context, our work contributes to a long tradition of meta-evaluation in the information retrieval literature, evaluations that evaluate evaluation methodologies.</p>
</li>
</ul>
<h2>2 Background and Related Work</h2>
<p>Relevance assessments, alternatively called labels, judgments, annotations, or qrels (and used interchangeably throughout this paper), drive progress in information retrieval and beyond by enabling the training of better models and the evaluation of their quality. These assessments can come from a variety of sources. Explicit highquality labels can be provided by humans, for example, editors hired by web search engine companies or retired intelligence analysts in the case of TREC evaluations [19]. Alternatively, implicit and often noisy labels can be extracted from user behavior logs [20], and most recently, the field has seen the growing popularity of synthetically generated labels [4, 12]. Despite these diverse approaches, humangenerated labels are widely acknowledged to represent the best source of data, against which all other techniques are compared. In the academic context, relevance assessments organized by NIST for TREC are recognized as the gold standard.</p>
<p>Faggioli et al. [15] (whose explorations began in early 2023, shortly after the release of ChatGPT) were among the earliest to discuss prompting LLMs to provide relevance judgments. Their paper explored a spectrum of human-machine collaboration strategies, articulating arguments for and against different approaches. We can readily situate the various conditions in our study along their proposed spectrum.</p>
<p>Far more than academic speculation, Thomas et al. [28] quite clearly stated, in the context of the Bing search engine: "We have been using LLMs, in conjunction with expert human labellers, for most of our offline metrics since late 2022." They assert that LLMs can accurately predict searcher preferences and that LLMs are as accurate as human labellers for evaluating systems. These claims were validated using an old TREC collection (2004 Robust Track), where an "in-house" version of GPT-4 was used (re-)label old queries and documents with different prompt configurations.</p>
<p>While Thomas et al. [28] was no doubt enlightening in revealing industry practices, we can point to a few shortcomings. First, the retrospective experimental design is not ideal-the authors were essentially "backporting" a modern technique to a collection that is 20 years old. Second, there is a concern about data contamination [13], whereby publicly available relevance labels might have (inadvertently) become incorporated into LLM pretraining data. ${ }^{1}$ Third,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>while the authors described processes at Bing that involve human intervention, their published experiments were fully automatic, and hence do not probe the potential synergies of human-machine hybrids. Finally, Thomas et al. [28] reported using an "in-house" custom version of GPT-4 that is not publicly available.</p>
<p>The last limitation was addressed by Upadhyay et al. [29], who built and released UMBRELA, which is a recursive acronym that stands for UMbrela is the Bing RELevance Assessor. As the name suggests, UMBRELA is an open-source toolkit that successfully reproduced the techniques described in Thomas et al. [28] and affirmed the paper's main claims. While the tool still depends on a proprietary large language model, at least the model is publicly accessible. UMBRELA was used by two teams independently in submissions to the LLM4Eval Workshop at SIGIR 2024 [24], which provides confidence that internal work inside Bing can be reproduced by the broader community.</p>
<p>This work specifically addresses all the remaining identified shortcomings of Thomas et al. [28]: We report on an in situ evaluation where four different approaches to relevance assessments were deployed in a "typical" large-scale TREC evaluation (in our case, the TREC 2024 RAG Track). To reduce the possibility of data contamination as much as possible, the test topics were publicly released only a few weeks before the evaluation deadline. This prospective deployment provides a fair comparison between the "standard" (fully manual) NIST relevance assessment process and three alternative approaches that involve LLMs to varying degrees. Soboroff [27] advocated against the fully automatic approach, but entertained the human-in-the-loop processes as possible steps forward. We let empirical analyses have the last word.</p>
<p>Our work, of course, does not appear in a vacuum. The research community has known for some time now that ChatGPT (and more generally, LLMs) can be used for data labeling and other annotation tasks [18], outperforming crowd-workers. LLMs can emulate the opinions of subpopulations in answering survey questions [7, 21], although this idea has been questioned [14]. Another thread of work has extensively explored the use of LLMs to evaluate the output of other LLMs for a variety of tasks [2, 8, 16, 17, 32]. This general approach is dubbed "LLM-as-a-judge" and represents an active area of research. Our work builds on this vast literature, but to our knowledge, no other study like ours has ever been conducted, in terms of scale, focus on relevance assessment, and careful calibration against human-based reference baselines.</p>
<h2>3 Methods</h2>
<p>The overall structure of our study adopts the meta-evaluation methodology common in the information retrieval literature dating back several decades [5, 6, 19, 26, 31, 33]. Existing NIST processes for pooling and relevance assessment, refined over several decades, serve as the reference point of comparison. System rankings generated by relevance judgments from NIST assessors are acknowledged by the community as representing the "gold standard" and provide a point of calibration. This is worth emphasizing: unlike in prior studies dependent on crowd-sourced workers of questionable quality, our human "baselines" can be considered highly reliable.</p>
<p>We explore three different approaches using LLMs to generate relevance assessments with our UMBRELA tool, detailed in Section 3.3.</p>
<ul>
<li>what is vicarious trauma and how can it be coped with?</li>
<li>how did the northwest coast people develop and use animal imagery in their homes?</li>
<li>why disability insurance is a smart investment</li>
<li>how bad did the vietnam war devastate the economy in 1968</li>
<li>what target stors's policies for shoplifting</li>
</ul>
<p>Figure 1: The first five topics from the TREC 2024 RAG Track.</p>
<p>System rankings induced by these alternatives are compared against those induced by the NIST relevance judgments. Correlation between these system rankings, captured in terms of Kendall's $\tau$, is taken as the measure of quality. In other words, we ask (RQ1): Do relevance judgments derived from UMBRELA lead to the same conclusions about the quality of the systems?</p>
<p>Of course, quality is balanced against cost. Again, taking existing NIST processes as the reference, we ask (RQ2): Can we achieve the same level of quality (i.e., ability to rank systems in terms of effectiveness), but at lower costs? With increasing LLM involvement, costs decrease-but what's the tradeoff with respect to quality? Finally, beyond a narrow focus on just rank correlations, we ask (RQ3): Are there any systematic differences between human and UMBRELA judgments? We seek to find out the answers to all these research questions.</p>
<h3>3.1 TREC 2024 RAG Track</h3>
<p>The context for this work is the Retrieval-Augmented Generation (RAG) Track at TREC 2024. The track was divided into three tasks: retrieval, augmented generation, and full RAG. Here, we use data only from the retrieval task, which adopts a standard ad hoc setup. Systems are presented with queries (called topics in TREC parlance), and their task is to return ranked lists of relevant passages from the MS MARCO V2.1 deduped segment collection [23], which traces its lineage back to the original MS MARCO dataset [3].</p>
<p>The MS MARCO V2.1 deduped segment collection [23] contains 113,520,750 text passages, derived from a deduplicated version of the MS MARCO V2 document collection [10]. This underlying document collection was first cleaned by removing near-duplicates using locality sensitive hashing (LSH) with MinHash and 9-gram shingles, reducing the original document count from 11,959,635 to 10,960,555 documents. From this cleaned document base, passages (segments) were created using a sliding window technique with overlap-specifically, using windows of 10 sentences with a stride of 5 sentences, producing passages typically between 500-1000 characters. To be clear, the passages represent the basic unit of retrieval, but following common IR parlance, we interchangeably refer to these as "documents" in a generic sense, especially when it is clear from context what we mean.</p>
<p>For the TREC 2024 RAG Track topics, we used a fresh scrape from Bing Search logs containing non-factoid topics warranting long-form answers that are often multifaceted and subjective [25]. We gathered the topics close to the submission period (July 2024) to ensure a fair evaluation when using commercial LLMs by minimizing any potential data leakage. Here, we are most concerned about contamination with respect to relevance judgments. Since we are</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">passage</span>,<span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">must</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">score</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">an</span>
<span class="nv">integer</span><span class="w"> </span><span class="nv">scale</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">meaning</span>:
<span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represent</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">nothing</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,
<span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">seems</span><span class="w"> </span><span class="nv">related</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">but</span>
<span class="nv">does</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">contain</span><span class="w"> </span><span class="nv">any</span><span class="w"> </span><span class="nv">part</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">it</span>.
<span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">contains</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">partial</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">complete</span>
<span class="nv">answer</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,<span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">may</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">bit</span><span class="w"> </span><span class="nv">unclear</span>,<span class="w"> </span><span class="nv">or</span>
<span class="nv">hidden</span><span class="w"> </span><span class="nv">amongst</span><span class="w"> </span><span class="nv">extraneous</span><span class="w"> </span><span class="nv">information</span><span class="w"> </span><span class="nv">and</span>
<span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">dedicated</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span>
<span class="nv">contains</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">partial</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">complete</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">it</span>.
</code></pre></div>

<p>Query: {query}
Passage: {passage}</p>
<p>Split this problem into steps:
Consider the underlying intent of the search.
Measure how well the content matches a likely intent of the query
(M).</p>
<p>Measure how trustworthy the passage is (T).
Consider the aspects above and the relative importance of each,
and decide on a final score (0). Final score must be an integer
value only.
Do not provide any code in result. Provide each score in the
format of: ##final score: score without providing any reasoning.</p>
<p>Figure 2: The prompt utilized with UMBRELA for relevance assessment.
retrieving from web corpora, it is highly likely that our passages have already been included in LLM pretraining data.</p>
<p>After some manual filtering by NIST annotators, we provided participants with a total of 301 topics. The first five topics are shown in Figure 1. Note that real-world information needs are often phrased awkwardly and contain misspellings; we intentionally made no attempt to fix these issues. The end-to-end evaluation involved RAG answers to these topics, but here we only examine the retrieval stage.</p>
<p>Consistent with previous iterations of the TREC Deep Learning Track $[10,11]$, our evaluation used the following relevance grades, with the associated descriptions:</p>
<p>0 Non-relevant: the passage has nothing to do with the query
1 Related: the passage seems related to the query but does not contain any part of an answer to it
2 Highly relevant: the passage contains a partial or complete answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information
3 Perfectly relevant: the passage is dedicated to the query and contains a partial or complete answer to it
At a high level, the retrieval task in TREC 2024 RAG can be viewed as a natural continuation of the previous TREC Deep Learning Tracks. Unless otherwise specified, our evaluation methodology follows what was laid out previously in those tracks.</p>
<h3>3.2 UMBRELA</h3>
<p>We used UMBRELA [29] to generate automatic relevance labels in this work. As discussed in Section 2, the tool is an open-source reproduction of the work described in Thomas et al. [28], utilizing the DNA prompting technique described by the authors.</p>
<p>Figure 2 shows the exact prompt used in UMBRELA for relevance assessment. Similar to the details provided in Upadhyay et al. [29],</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"># topics</th>
<th style="text-align: center;">avg pool <br> size</th>
<th style="text-align: center;">avg # docs assessed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">avg pool size <br> decrease</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1a)</td>
<td style="text-align: center;">fully manual</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(1b)</td>
<td style="text-align: center;">manual w/ filtering</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(1c)</td>
<td style="text-align: center;">manual w/ post-editing</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(2a)</td>
<td style="text-align: center;">UMBRELA (for fully manual)</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(2b)</td>
<td style="text-align: center;">UMBRELA (for manual w/ filtering)</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$-38 \%$</td>
</tr>
<tr>
<td style="text-align: center;">(2c)</td>
<td style="text-align: center;">UMBRELA (for manual w/ post-editing)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">124</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$-41 \%$</td>
</tr>
<tr>
<td style="text-align: center;">(3)</td>
<td style="text-align: center;">UMBRELA (overall)</td>
<td style="text-align: center;">301</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Descriptive statistics for relevance judgments under different conditions. Note that (a), (b), and (c) are disjoint.
we employed zero-shot prompting to perform relevance assessment using OpenAI's GPT-4o model. Note that we used the publicly available version, as opposed to a private variant used in Thomas et al. [28]. UMBRELA leverages precise instructions to assist the LLM in understanding the semantic relation between the query and the passage and thus guides the LLM in assigning (hopefully) accurate labels.</p>
<h3>3.3 Level of LLM Involvement</h3>
<p>We explored four different approaches to relevance assessments, detailed below. Common to all these approaches is the construction of the pools to depth 20 from the run submissions to the TREC 2024 RAG Track.</p>
<p>Fully Manual Assessment. This condition represents the existing NIST methodology for relevance assessment. Operationally, the deployed processes were taken from the TREC 2023 Deep Learning Track [11], although the overall workflow dates back to the ad hoc document retrieval tasks from the 1990s (with various refinements over the years).</p>
<p>Manual Assessment with Filtering. In this condition, UMBRELA was used to filter the pools, discarding passages labelled as nonrelevant by the model. That is, such passages were not shown to the NIST assessor. Importantly, however, the assessors did not have access to the UMBRELA judgments. Operationally, this condition is indistinguishable from fully manual assessment; the assessors were just presented with different (smaller) pools.</p>
<p>Manual Post-Editing of Automatic Assessment. In this condition, we used UMBRELA to assess the pools, and after discarding passages judged to be non-relevant, we presented the remaining to the assessors. Here, the assessors were made aware of the UMBRELA labels. In other words, the NIST assessors were post-editing LLM judgments. This was operationally performed by grouping all passages of the same relevance grade together and explicitly informing the assessor. That is, the assessor was told (for example), "Here are all the passages that the LLM found to be highly relevant".</p>
<p>Fully Automatic Assessment. In this condition, we used relevance labels from UMBRELA, unmodified, to evaluate runs directly. This required no manual involvement and thus we were able to assess all evaluation topics.</p>
<h2>4 Results</h2>
<h3>4.1 Descriptive Statistics</h3>
<p>For the TREC 2024 RAG Track, we received a total of 77 retrievalonly runs from 19 teams. Of these, 5 runs were from the organizers, who contributed a range of baselines; these are included in the statistics provided above. All participants were given the set of 301 topics and were requested to return ranked lists for all of them.</p>
<p>Descriptive statistics for the relevance assessments are shown in Table 1, where we break down statistics into the various conditions described in Section 3.3. NIST assessors were given the option to judge whatever topics interested them, selecting from the set of 301 topics. In total, they were able to assess 27 topics using the fully manual process, 31 topics for manual with filtering, and 31 topics for manual with post-editing. However, for 3 topics, NIST assessors found no passages with at least a relevance grade of 1 (i.e., "related"): two of these occurred in the manual with filtering condition and one in the manual with post-editing condition. Thus, these topics were removed from consideration, leading to the final figures presented in Table 1, shown in rows (1a), (1b), and (1c). To be clear, the topics for each of these conditions are disjoint. Since UMBRELA is fully automatic, we were able to apply it to all 301 topics in the test set, whose statistics are shown in row (3).</p>
<p>In Table 1, the row (2) entries provide descriptive statistics for the common (i.e., overlapping) topics between the UMBRELA condition and each of the other conditions. That is, (2a) provides statistics from the UMBRELA assessments that correspond to the 27 topics that received fully manual assessments, i.e., row (1a). The same correspondence holds between (2b) and (1b), and between (2c) and (1c). By comparing row (2b) vs. row (1b) and row (2c) vs. row (1c), we can quantify the amount of effort saved from using an LLM, since in both cases, passages assessed as non-relevant by UMBRELA are not shown to the assessors. This is presented in the last column, where UMBRELA reduces the assessment pool by roughly $40 \%$.</p>
<p>From these results, we see differences in distributions across the relevance grades. Focusing on the LLM-assisted approaches, row (2b) vs. (1b) and row (2c) vs. (1c), we see more passages assessed as "related". However, there does not appear to be obvious differences in the higher grades, "highly relevant" and "perfectly relevant". The differences between row (2a) vs. (1a) suggest that UMBRELA adopts more lenient relevance criteria, leading to more passages at each of the 1,2 , and 3 grades, compared to fully manual assessments.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: Comparisons between UMBRELA scores and scores from fully manual (top row), manual with filtering (middle row), and manual with post-editing (bottom row). Columns show different metrics: nDCG@20, nDCG@100, and Recall@100. In each scatter plot, red dots show run-level scores and the blue dots show all topic/run combinations. Each scatter plot is annotated with rank correlations in terms of Kendall's $\tau$. This analysis is performed on common (i.e., overlapping) topics.</p>
<h3>4.2 Rank Correlations</h3>
<p>At a high-level, our analyses compute rank correlations between evaluation scores induced by UMBRELA judgments (qrels) vs. judgments (qrels) derived by the other assessment processes. Following common practice in IR meta-evaluations, rank correlation is captured using Kendall's $\tau$. However, there are multiple ways to design a rank correlation analysis. In the following, we explain the options in terms of UMBRELA vs. fully manual assessments, but we apply the same methodology to the other conditions:</p>
<ul>
<li>Run-level correlations on common topics (run-level). For each run, we evaluate with qrels from the 27 topics with fully manual judgments and UMBRELA qrels for the same 27 topics. These correspond to row (1a) and row (2a) in Table 1, respectively. We computed Kendall's $\tau$ based on these two scores.</li>
<li>Average of per-topic correlations (per-topic avg). Same as above: For each run, we evaluate with qrels from the 27 topics with fully manual judgments and UMBRELA qrels for the same 27 topics. These correspond to row (1a) and row (2a) in Table 1, respectively. However, unlike above, we compute Kendall's $\tau$ for each topic, and then average across the per-topic correlations.</li>
<li>Correlation across all topic/run combinations (all topics/runs). We evaluate with the same qrels as above, but each topic/run combination is considered an independent observation, and we computed Kendall's $\tau$ across all these observations.</li>
</ul>
<p>Another important aspect of experimental design is the effectiveness metric. We focus on three: nDCG@20, nDCG@100, and Recall@100. The first is perhaps the most common as it evaluates early precision, representing a common setting in RAG applications; i.e.,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">nDCG@20</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">nDCG@100</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Recall@100</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UMBRELA vs.</td>
<td style="text-align: center;">Run-Level</td>
<td style="text-align: center;">Per-Topic</td>
<td style="text-align: center;">Run-Level</td>
<td style="text-align: center;">Per-Topic</td>
<td style="text-align: center;">Run-Level</td>
<td style="text-align: center;">Per-Topic</td>
</tr>
<tr>
<td style="text-align: center;">(1a)</td>
<td style="text-align: center;">fully manual</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.829</td>
</tr>
<tr>
<td style="text-align: center;">(1b)</td>
<td style="text-align: center;">fully manual (2/3)</td>
<td style="text-align: center;">$0.811 \pm 0.06$</td>
<td style="text-align: center;">$0.544 \pm 0.09$</td>
<td style="text-align: center;">$0.904 \pm 0.04$</td>
<td style="text-align: center;">$0.762 \pm 0.09$</td>
<td style="text-align: center;">$0.903 \pm 0.05$</td>
<td style="text-align: center;">$0.826 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">(1c)</td>
<td style="text-align: center;">fully manual (1/3)</td>
<td style="text-align: center;">$0.768 \pm 0.08$</td>
<td style="text-align: center;">$0.566 \pm 0.12$</td>
<td style="text-align: center;">$0.875 \pm 0.08$</td>
<td style="text-align: center;">$0.777 \pm 0.10$</td>
<td style="text-align: center;">$0.878 \pm 0.07$</td>
<td style="text-align: center;">$0.841 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: center;">(2a)</td>
<td style="text-align: center;">manual w/ filtering</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.873</td>
</tr>
<tr>
<td style="text-align: center;">(2b)</td>
<td style="text-align: center;">manual w/ filtering (2/3)</td>
<td style="text-align: center;">$0.861 \pm 0.05$</td>
<td style="text-align: center;">$0.615 \pm 0.08$</td>
<td style="text-align: center;">$0.905 \pm 0.04$</td>
<td style="text-align: center;">$0.785 \pm 0.04$</td>
<td style="text-align: center;">$0.912 \pm 0.04$</td>
<td style="text-align: center;">$0.874 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">(2c)</td>
<td style="text-align: center;">manual w/ filtering (1/3)</td>
<td style="text-align: center;">$0.814 \pm 0.10$</td>
<td style="text-align: center;">$0.611 \pm 0.13$</td>
<td style="text-align: center;">$0.878 \pm 0.07$</td>
<td style="text-align: center;">$0.780 \pm 0.07$</td>
<td style="text-align: center;">$0.887 \pm 0.07$</td>
<td style="text-align: center;">$0.873 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;">(3a)</td>
<td style="text-align: center;">manual w/ post-editing</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.880</td>
</tr>
<tr>
<td style="text-align: center;">(3b)</td>
<td style="text-align: center;">manual w/ post-editing (2/3)</td>
<td style="text-align: center;">$0.844 \pm 0.07$</td>
<td style="text-align: center;">$0.719 \pm 0.09$</td>
<td style="text-align: center;">$0.917 \pm 0.04$</td>
<td style="text-align: center;">$0.824 \pm 0.06$</td>
<td style="text-align: center;">$0.872 \pm 0.07$</td>
<td style="text-align: center;">$0.876 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">(3c)</td>
<td style="text-align: center;">manual w/ post-editing (1/3)</td>
<td style="text-align: center;">$0.814 \pm 0.10$</td>
<td style="text-align: center;">$0.726 \pm 0.14$</td>
<td style="text-align: center;">$0.897 \pm 0.06$</td>
<td style="text-align: center;">$0.834 \pm 0.10$</td>
<td style="text-align: center;">$0.862 \pm 0.11$</td>
<td style="text-align: center;">$0.887 \pm 0.10$</td>
</tr>
</tbody>
</table>
<p>Table 2: Rank correlations (Kendall's $\tau$ ) at the run level and averaged on a per-topic basis, comparing UMBRELA vs. fully manual in row (1), vs. manual with filtering in row (2), vs. manual with post-editing in row (3). Values in rows (<em>a) are exactly those shown in Figure 3. Rows ( ${ }^{</em>}$ bc) report correlations based on sampling $2 / 3$ and $1 / 3$ of topics, respectively, using a Monte Carlo simulation, with $95 \%$ confidence intervals reported.
top-20 results are fed into the LLM prompt for answer generation. However, the downside of nDCG@20 is that most topics have more than 20 relevant passages (see Table 1), in which case nDCG@100 better captures effectiveness. Recall is typically used to quantify the upper bound of effectiveness in a multi-stage pipeline, and a cutoff at 100 passages represents a typical setting.</p>
<p>Results of our analyses are shown in Figure 3 as scatter plots: the left column shows nDCG@20 scores, the middle column shows nDCG@100 scores, and the right column shows Recall@100 scores. The top row compares UMBRELA with fully manual judgments; the middle row, manual with filtering, and the bottom row, manual with post-editing. In each plot, the red solid dots represent run-level scores (simple average across all topics, per standard IR practice) and the blue dots show all topic/run combinations. In other words, the correlation between the red dots captures run-level correlations on common topics and the correlation between the blue dots capture correlation across all topic/run combinations. The average of pertopic correlations is not readily visualized on these plots.</p>
<p>In the discussion below, we introduce the notion of "scatter" to describe score disagreements across runs and topics. Perfect agreement (i.e., no scatter) would have all run/topic combinations (i.e., the blue dots in the plots) lie on a straight line. We can quantify scatter in terms of Kendall's $\tau$ on all topic/run observations (i.e., the blue dots): the lower the scatter, the greater the correlation.</p>
<p>In terms of nDCG@20 (left column in Figure 3), we see a high degree of correlation between scores induced by UMBRELA and the other conditions, i.e., the red dots. We do see a fair amount of scatter for the blue dots, indicating disagreement across individual run/topic observations. In the manual with post-editing case (bottom left), there appears to be less scatter, as the assessors are provided an anchor in terms of the UMBRELA judgments (but are free to change them). Interestingly, there does not appear to be a noticeable difference between the fully manual assessments (top) and the other two LLM-assisted alternatives (middle and bottom) in terms of run-level rank correlations.</p>
<p>The conclusions are similar for nDCG@100 (middle column) and Recall@100 (right column), with slightly higher rank correlations
than their nDCG@20 counterparts. We observe less scatter for nDCG@100 and Recall@100 across all topic/run observations (i.e., the blue dots) compared to nDCG@20, which makes sense since evaluating deeper ranked lists should improve metric stability. As with nDCG@20, the two LLM-assisted processes do not exhibit noticeably higher rank correlations with UMBRELA.
So, to provide an answer to RQ1:
RQ1 Automatically generated UMBRELA judgments can replace fully manual assessments for common effectiveness metrics at the run level (in the context of the TREC-style evaluations).
Overall, this finding is consistent with results from the Workshop on Large Language Models (LLMs) for Evaluation in Information Retrieval at SIGIR 2024 [24].</p>
<p>The plots in Figure 3 show that despite variance across topic/run combinations (i.e., the scatter of the blue dots), we observe good rank correlations after aggregating scores across topics ( $\sim 30$ in our case). A natural question is: How many topics do we need?</p>
<p>The answer can be found in Table 2, where row (1a), (2a), and (3a) simply repeat the Kendall's $\tau$ correlation figures from Figure 3. The other rows repeat exactly the same procedure for computing rank correlation, but over (roughly) thirds of the topics using a Monte Carlo simulation. For example, in row (2b), we compute Kendall's $\tau$ over $2 / 3$ of the topics (random sampling without replacement) in the fully manual condition (i.e., 27 choose 18). We repeat the process 100 times, from which we can compute the mean and confidence intervals across all the trials. Here, the $95 \%$ confidence interval is determined by taking the $2.5 \%$ and $97.5 \%$ percentiles of the means computed over the trials. In row (1c), we repeat, but with only $1 / 3$ of the topics; the (b) and (c) rows of (2) and (3) represent the corresponding conditions with the LLM-assisted processes. As expected, run-level rank correlations decrease with fewer topics, but these results suggest that we can achieve reasonable run-level rank correlations with a few as a $\sim 10$ topics.</p>
<p>In Figure 4, we set aside UMBRELA assessments and compare scores induced by the fully manual process ( $y$-axis) with manual with filtering (top row) and manual with post-editing (bottom row). The columns show different metrics: nDCG@20, nDCG@100, and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Run-level rank correlations (Kendall's $\tau$ ) comparing manual with filtering vs. fully manual (top row) and manual with post-editing vs. fully manual (bottom row). Columns show different metrics: nDCG@20, nDCG@100, and Recall@100. Note that topics are disjoint in this analysis.</p>
<p>Recall@100, as in Figure 3. Note that here we are evaluating scores from disjoint topics that slightly differ in quantity (e.g., 27 topics for fully manual, but 30 topics for manual with post-editing). Because of this setup, we can only plot run-level correlations (i.e., red dots); there is no equivalent notion of each topic/run combination as independent observations (corresponding to the blue dots).</p>
<p>Comparing Figure 4 to Figure 3, we do not see any obvious differences in rank correlations. That is, if we take the fully manual assessment process as the "gold standard", rankings induced by UMBRELA do not seem any worse than the rankings induced by the other LLM-assisted processes. Phrased another way, human involvement does not appear to increase correlation with the gold standard (fully manual). this suggests that expending human effort does not seem to yield a payoff in terms of more accurate system rankings. Thus, to directly answer RQ2:
RQ2 The additional costs associated with hybrid human-LLM solutions (i.e., human-in-the-loop efforts) do not appear to have obvious tangible benefits over fully automatic processes.
Phrased differently, we do not notice any obvious payoffs that justify the higher costs associated with human-LLM hybrid strategies, such as those advocated in previous work [15, 27].</p>
<h3>4.3 Confusion Matrices</h3>
<p>We next attempt to characterize differences between UMBRELA judgments and those arising from the fully manual and LLM-assisted processes. Confusion matrices are shown in Figure 5: UMBRELA is compared to fully manual on the left, to manual with filtering
in the middle, and to manual with post-editing on the right. While Arabzadeh and Clarke [1] conducted experiments that are similar in spirit to these, validating the alignment of human and LLM assessments using TREC DL 2019-20 data, their results are not directly comparable to our confusion matrices.</p>
<p>Consistent with the descriptive statistics in Table 1, these results suggest that NIST assessors tend to adopt stricter relevance criteria. For example, comparing UMBRELA with fully manual judgments, the left panel shows that for 263 cases, both UMBRELA and NIST assessors agreed that a document is "perfectly relevant" to a query. However, there is an even greater number ( 384 passages) that UMBRELA assessed as "perfectly relevant", but the human assessor thought was only "highly relevant". We can draw similar conclusions by examining the numbers above the diagonals: human assessors annotate many passages as being less relevant than the labels provided by UMBRELA. In contrast, there are relatively fewer passages that the human assessors considered more relevant than UMBRELA (i.e., numbers below the diagonal). Note that in this condition, the fully manual assessment process is completely independent from UMBRELA.</p>
<p>However, based on our own manual examination of the evaluation data, we can identify cases where our judgments align closer to UMBRELA than the NIST assessors. One example is for the query "why are cancer rates higher on the east coast"; our interpretation is that "east coast" refers to the east coast of the United States. There is a document in the pool about cancer rates in populations with "Ashkenazi Jewish (Eastern European) heritage", which the NIST</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Confusion matrices comparing UMBRELA with the fully manual process (left), manual with filtering (middle), and manual with post-editing (right).
assessor labeled as perfectly relevant, but UMBRELA labeled as not relevant. Perhaps this divergence stems from a different interpretation of the topic (or perhaps this is simply an "erroneous" judgment), but here we agree with UMBRELA.</p>
<p>Next, consider the LLM-assisted conditions. In the middle panel and right panel in Figure 5, the leftmost columns are empty because the NIST assessors are never shown passages judged non-relevant by UMBRELA. Recall that the only difference between the two conditions is that with post-editing, the NIST assessor is shown the UMBRELA relevance grade to provide an anchor point of reference. Thus, in the rightmost confusion matrix, where we can interpret each column as the NIST assessor either "downgrading" or "upgrading" UMBRELA assessments (since the human assessor is provided the UMBRELA label). For example, in the final ("3") column, NIST assessors "downgraded" 182 passages from "perfectly relevant" to "highly relevant", and 4 passages all the way down to not relevant.</p>
<p>Looking at the LLM-assisted conditions, we provide a few interesting case studies: For the topic "what change did marius make in the roman empire", we encounter a document written as a series of "clues", e.g., "He was a member of the lower-class of Romans who rose to become commander of the army." Although this document does not explicitly reference Marius, the clues describe him accurately. UMBRELA marked this document as "perfectly relevant", having made the correct inference, but the NIST assessor marked the document as non-relevant.</p>
<p>There are also cases where differences can be attributed to topic interpretation. Consider the topic "what steps should individuals take to dispose of their electronic devices?" A document that discusses recycling batteries and light bulbs was marked perfectly relevant by UMBRELA but was considered not relevant by the NIST assessor. We suspect that the difference arises from whether batteries and light bulbs should be considered "electronic devices"; here, UMBRELA takes a more expansive view. Again, we might consider this as another example of UMBRELA making inferences that humans would find less warranted.</p>
<p>Interestingly, from the rightmost confusion matrix in Figure 5, for 75 passages that UMBRELA labeled as only "related", the human assessor "upgraded" the document to be "perfectly relevant". In at least some of these cases, it appears that the NIST assessor made an inference that UMBRELA either didn't make or missed. For the query "what society issues did goth rock address", we found
a document that mentions goth in passing, but the connection was not picked up by UMBRELA. Overall, the cases where NIST assessors found a document to be more relevant than UMBRELA are relatively rare, compared to the other way around.</p>
<p>Our analysis is missing a comparison between two human assessors, since it is well known that humans disagree with each other in making relevance judgments [9, 19, 22, 31]. Due to budget and time limitations, we were not able to have the same topics annotated by multiple human assessors independently. Without this, we are missing an important point of reference, as the divergence between human and LLM-assisted processes needs to be compared to human-human inter-annotator agreement. Nevertheless, with respect to RQ3, the following conclusion seems warranted:</p>
<p>RQ3 Human assessors appear to apply stricter relevance criteria than the current implementation of UMBRELA.</p>
<p>That is, UMBRELA often finds a passage to be more relevant than the NIST assessor. This is in part because the LLM draws inferences that a human would consider unwarranted. The opposite case is observed as well, but appears to be less common. To be precise, this is a statement about the current state of LLMs with prompts used today (specifically, UMBRELA). Previous studies (e.g., [30]) have shown that human assessors may exhibit systematic tendencies to be more lenient or strict, and thus it is difficult to draw conclusions about humans and LLMs in general. For example, we can perhaps alter LLM behavior with better prompting.</p>
<h2>5 Conclusions</h2>
<p>Recent studies have highlighted the potential of using LLMs for automating relevance assessments, as part of the broader "LLM-as-a-judge" literature [2, 8, 17, 32]. Production deployment of LLMbased assessments at Bing [28] affirms this potential, but there remain many unknowns and different perspectives from the community [15, 27]. In this study, we have taken an important step in validating three approaches to using LLMs for relevance assessment, deployed in situ at a large-scale TREC evaluation. Our results provide an empirically grounded validation study for academic TREC-style evaluations, serving as a foundation on which other researchers can build. This study clarifies a few unknowns, but many questions remain unaddressed.</p>
<h2>Acknowledgments</h2>
<p>This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada. Additional funding is provided by Snowflake and Microsoft via the Accelerating Foundation Models Research program.</p>
<h2>References</h2>
<p>[1] Negar Arabzadeh and Charles L. A. Clarke. 2024. A Comparison of Methods for Evaluating Generative IR. arXiv:2404.04044 (2024).
[2] Yantao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamille Lukosuite, Lizzie Lovitt, Michael Sellittu, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henzghan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 (2022).
[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mie Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 (2018).
[4] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Data Augmentation for Information Retrieval using Large Language Models. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022). Madrid, Spain, 2387-2392.
[5] Chris Buckley and Ellen M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of the 25rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2000). Athens, Greece, $33-40$.
[6] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete Information. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004). Sheffield, United Kingdom, 25-32.
[7] Eric Chu, Jacob Andreas, Stephen Ansolabehere, and Deb Roy. 2023. Language Models Trained on Media Diets Can Predict Public Opinion. arXiv:2303.16779 (2023).
[8] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross Examination. arXiv:2305.15281 (2023).
[9] Gordon V. Cormack, Charles L.A. Clarke, Christopher R. Palmer, and Samuel S.L. To. 2000. Passage-Based Query Refinement: (MultiText Experiments for TREC-6). Information Processing and Management 36, 1 (2000), 155-153.
[10] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2022. Overview of the TREC 2022 Deep Learning Track. In Proceedings of the Thirty-First Text REtrieval Conference (TREC 2022). Gaithersburg, Maryland.
[11] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Hossein A. Rahmani, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2023. Overview of the TREC 2023 Deep Learning Track. In Proceedings of the Thirty-Second Text REtrieval Conference (TREC 2023). Gaithersburg, Maryland.
[12] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2023. Promptagator: Fewshot Dense Retrieval From 8 Examples. In Proceedings of the 11th International Conference on Learning Representations (ICLR 2023).
[13] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2024. Investigating Data Contamination in Modern Benchmarks for Large Language Models. arXiv:2311.09783 (2024).
[14] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2024. Questioning the Survey Responses of Large Language Models. arXiv:2306.07951 (2024).
[15] Guglielmo Faggioli, Laura Dietz, Charles L. A. Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval. Taipei, Taiwan.
[16] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as You Desire. arXiv:2302.04166 (2023).
[17] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like Summarization Evaluation with ChatGPT. arXiv:2304.02554 (2023).
[18] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. Proceedings of the National Academy of Science 120, 30 (2023), e2305016120.
[19] Donna Harman. 2011. Information Retrieval Evaluation. Morgan \&amp; Claypool Publishers.
[20] Thorsten Joachims, Laura Granka, Bing Pang, Helene Hembrooke, and Geri Gay. 2005. Accurately Interpreting Clickthrough Data as Implicit Feedback. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005). Salvador, Brazil, 154-161.
[21] Jumoil Kim and Byungkyu Lee. 2024. AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction. arXiv:2305.09620 (2024).
[22] Michael E. Lesk and Gerard Salton. 1968. Relevance Assessments and Retrieval System Evaluation. Information Storage and Retrieval 4, 4 (1968), 343-359.
[23] Ronak Pradeep, Nandan Thakur, Sabel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, and Jimmy Lin. 2024. Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track. arXiv:2406.16828 (2024).
[24] Hossein A. Rahmani, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas, Charles L. A. Clarke, Mohammad Aliamnejadi, Clemencia Siro, and Guglielmo Faggioli. 2024. LLMJudge: LLMs for Relevance Judgments. arXiv:2408.08896 (2024).
[25] Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C. Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, and Nikhil Rao. 2024. Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents. arXiv:2402.17896 (2024).
[26] Mark Sanderson and Justin Zobel. 2005. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005). Salvador, Brazil, 162-169.
[27] Ian Soboroff. 2024. Don't Use LLMs to Make Relevance Judgments. arXiv:2409.15133 (2024).
[28] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large Language Models Can Accurately Predict Searcher Preferences. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024). Washington, D.C., 1930-1940.
[29] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, and Jimmy Lin. 2024. UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. arXiv:2406.06519 (2024).
[30] Matteo Venanzi, John Guiver, Gabriella Kazai, Pushmeet Kohli, and Milad Shokouhi. 2014. Community-Based Bayesian Aggregation Models for Crowdsourcing. In Proceedings of the 23rd International World Wide Web Conference (WWW 2014). Seoul, South Korea, 155-164.
[31] Ellen M. Voorhees. 1998. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998). Melbourne, Australia, 315-323.
[32] Liammin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Liu, Zhuohan Li, Ducheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MTBench and Chatbot Arena. arXiv:2306.05685 (2023).
[33] Justin Zobel. 1998. How Reliable Are the Results of Large-Scale Information Retrieval Experiments?. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998). Melbourne, Australia, 307-314.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ To us, the first and second issues are related but orthogonal: the first concern is about the methodology itself, while the second is about the data used.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-208297717</p>
                <p><strong>Paper Title:</strong> Fuzzy Reinforcement Learning and Curriculum Transfer Learning for Micromanagement in Multi-Robot Confrontation</p>
                <p><strong>Paper Abstract:</strong> : Multi-Robot Confrontation on physics-based simulators is a complex and time-consuming task, but simulators are required to evaluate the performance of the advanced algorithms. Recently, a few advanced algorithms have been able to produce considerably complex levels in the context of the robot confrontation system when the agents are facing multiple opponents. Meanwhile, the current confrontation decision-making system su ﬀ ers from di ﬃ culties in optimization and generalization. In this paper, a fuzzy reinforcement learning (RL) and the curriculum transfer learning are applied to the micromanagement for robot confrontation system. Firstly, an improved Q-learning in the semi-Markov decision-making process is designed to train the agent and an e ﬃ cient RL model is deﬁned to avoid the curse of dimensionality. Secondly, a multi-agent RL algorithm with parameter sharing is proposed to train the agents. We use a neural network with adaptive momentum acceleration as a function approximator to estimate the state-action function. Then, a method of fuzzy logic is used to regulate the learning rate of RL. Thirdly, a curriculum transfer learning method is used to extend the RL model to more di ﬃ cult scenarios, which ensures the generalization of the decision-making system. The experimental results show that the proposed method is e ﬀ ective.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSAQ_migration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SSAQ algorithm on binary-tree robot migration task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated embodied agent trained with the proposed SSAQ (softmax + simulated annealing Q-learning) algorithm to navigate a binary-tree map; used to measure convergence behaviour under a high-action-space, single-environment setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>robot migration agent (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated navigation agent trained with the SSAQ improved Q-learning algorithm (softmax / Boltzmann action selection with simulated annealing) operating in a semi-Markov decision process; Q-values are approximated with tabular updates in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Binary-tree robot migration map (Robocode experimental task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A fixed binary-tree structured map with N layers; the agent starts at the root and must choose branches to reach endpoints. Complexity arises from a large discrete action set corresponding to endpoints and long decision horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of endpoints/actions = 2^N - 1 (N is number of layers); in experiment N = 10 → 2^10 - 1 = 1023 possible endpoints/actions; decision horizon equals tree depth (N).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (N=10 → 1023 discrete endpoint actions)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Single static map instance (no procedural variation reported); variation effectively low (one environment instance / no randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Q-value convergence time (iterations to convergence per recorded Q entries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Convergence times reported for sampled Q-values: Q3 = 214 iterations, Q7 = 168 iterations, Q15 = 122 iterations (SSAQ); SSAQ converged faster than ε-greedy baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The experiment isolates high complexity (large action space) with low variation (single static task) and shows SSAQ achieves faster convergence and smoother Q-value curves than ε-greedy baselines, indicating the proposed exploration (softmax+annealing) handles large action spaces efficiently; no explicit formal trade-off equation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Convergence times: Q3=214, Q7=168, Q15=122 iterations (SSAQ) — reported performance in this condition.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>single-environment RL training (SSAQ tabular Q-learning) with simulated annealing softmax action selection</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Convergence measured in iterations; representative convergence times for sampled Q entries given (122–214 iterations); overall episodes not explicitly enumerated for this task beyond iteration counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a high-action-space, single-instance task the SSAQ algorithm converged faster and produced smoother Q-value learning curves than ε-greedy baselines, demonstrating improved stability and faster convergence in high-complexity, low-variation settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DMNN_4v4_FLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision-Making Neural Network (DMNN) multi-agent tanks with fuzzy learning-rate (FLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent simulated tank team trained with a shared-parameter decision-making neural network (DMNN) using adaptive momentum and a fuzzy system to adjust RL learning rate, evaluated on Robocode 4v4 micromanagement scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>tank agent (DMNN with adaptive momentum and fuzzy LR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated tank agents using multi-agent reinforcement learning with parameter sharing; state-action values are approximated by a BP neural network with adaptive momentum (DMNN); the RL learning rate is dynamically adjusted by a fuzzy inference system; action selection uses a softmax (Boltzmann) layer with simulated annealing.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual tanks in Robocode)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robocode micromanagement 4v4 tank battles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Physics-like simulated multi-robot confrontation environment (Robocode) with discrete state encoding from the game engine: absolute orientation angle (discretized into 4), relative direction angle (4), and distance (20 discrete bins); actions include movement directions (forward/backward) and rotations (clockwise/anticlockwise) and firing bullets with varying energies; complexity arises from partial observability, multi-agent interactions, and combinatorial state-action space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State space discretization: absolute angle (4) × relative angle (4) × distance bins (20) = 320 discrete state combinations; action space: movement/rotation types (4 primary movement actions) plus firing decisions (multiple bullet energy levels described qualitatively); multi-agent interactions (teams of 4 versus 4).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium–high (320-state discretization per agent; multi-agent interaction multiplies effective complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation comes from different enemy behaviors and random initial conditions; experiments compare different learning-rate settings and models (FLM vs LM-0.2 vs LM-0.95) and NN with/without adaptive momentum; number of random seeds / exact environment instances not quantified, but evaluation over repeated rounds (500 rounds) provides variation sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (multiple episodes/rounds, varied opponent behaviour; not heavily procedurally varied)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Score ratio (agent/team win-score relative to opponent), defensive score, fluctuation of score ratio (first-order difference), and stability of score curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FLM (fuzzy LR + DMNN with AWM) achieved a score ratio around ~0.8; LM with fixed learning rates (LM-0.2 and LM-0.95) achieved ~0.7. DMNN with adaptive momentum produced higher defensive scores and lower fluctuation vs neural networks without adaptive momentum. Evaluations used 500 rounds with scores recorded every 10 rounds (50 sessions).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper reports that using neural-network function approximation (DMNN) with adaptive momentum and a fuzzy dynamic learning rate yields more stable and higher performance under the multi-agent, partially-observed Robocode environment. It implies that algorithmic improvements (adaptive momentum, fuzzy LR) mitigate detrimental effects of environment complexity and per-episode variation, but it does not provide a formal quantitative trade-off curve between complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-agent RL with parameter sharing using DMNN; softmax/Boltzmann action selection with simulated annealing; fuzzy inference to adapt RL learning rate; trained for 500 rounds (evaluation recorded every 10 rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization evaluated by training agents on specific scenarios and testing against different opponents and larger teams; FLM generalized better than fixed-rate LMs (score ratio ~0.8 vs ~0.7) and produced more stable defensive performance, indicating better transfer across varied opponent behaviours within Robocode.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Learning evaluated over 500 rounds; performance curves and stability reported across 50 sessions (every 10 rounds); exact number of environment interactions per round not explicitly enumerated but rounds provide the unit of training/assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameter-sharing multi-agent DMNN with adaptive momentum and a fuzzy-controlled RL learning rate yields higher win rates (~0.8 score ratio) and more stable defensive performance than models with fixed learning rates and networks without adaptive momentum, indicating these algorithmic elements improve learning under moderate-to-high complexity and episodic variation in Robocode micromanagement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum_TF_multiopponents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum transfer learning for increasing-opponent Robocode scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum transfer-learning approach that transfers policies from easier source tasks through intermediate tasks to more difficult Robocode micromanagement scenarios (facing 6 or 10 opponents), with a decay function (Newton's cooling law) to reduce interference from prior experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>tank agent (LM with TF)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated tank agents trained using the learning model (DMNN + fuzzy LR) combined with curriculum transfer learning: pretrained source-task policies are probabilistically exploited in the target task using a decaying threshold ε(t) (Newton cooling-inspired), optionally with intermediate tasks when target is much harder.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual tanks in Robocode)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robocode multi-opponent micromanagement (6-opponent and 10-opponent battles)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Increasingly difficult multi-agent confrontation scenarios in Robocode where our team faces larger opponent teams (six or ten TD-trained tanks); complexity grows with opponent count and coordinated opponent behaviour; variation across tasks arises from different opponent team sizes and policies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Primary complexity measure used is opponent count (team size): e.g., 6-opponent and 10-opponent target tasks; task difficulty is treated qualitatively (more opponents = more difficult). Also intermediate tasks used when target >> source difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>higher complexity for 6-opponent and 10-opponent tasks relative to baseline 4v4; labelled 'more difficult' in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured as difference between source and target tasks (task difficulty shift), and number of different scenario instances (multiple rounds); curriculum includes intermediate tasks to bridge large variations. The transfer probability decays according to ε(t) = ε(t0) * exp(-p t + p t0) with decay coefficient p.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium–high (task difficulty shift introduced by increasing opponent counts; domain shift handled via curriculum/intermediate tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Score ratio and defensive score over rounds (same metrics as other Robocode experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LM with curriculum transfer learning achieved faster learning and higher score ratios and higher defensive scores in the target (more difficult) scenarios compared to LM without TF; specific numeric score ratios not consistently tabulated in text for 6/10-opponent tests, but plots referenced show TF variant outperforming non-TF across training rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper reports that curriculum transfer learning with a decay on prior-experience usage improves performance when the target environment is more complex (more opponents) than the source; if target is much harder, adding an intermediate task helps. The authors describe a qualitative relationship: larger complexity increases difficulty, and controlled use of prior experience (decay) reduces interference and enables better transfer — but no quantitative function linking complexity magnitude and required curriculum parameters is given.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>LM with TF outperforms LM without TF in tasks with higher complexity and variation (6-opponent and 10-opponent scenarios) — qualitative improvement in score ratio and defensive score documented; exact numeric score ratios for these specific tests are not explicitly tabulated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum transfer learning (source → optional intermediate → target) with probabilistic exploitation of prior policy controlled by decay ε(t) (Newton's cooling law); DMNN + fuzzy LR used as base learner.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — transfer learning accelerated learning speed and yielded higher score ratios and defensive scores in more difficult target tasks (6- and 10-opponent scenarios) compared to training from scratch; the curriculum with intermediate tasks further improved stability and final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported improvements in learning speed (faster attainment of higher score ratios) over 500 rounds; exact sample counts for convergence not numerically reported for the multi-opponent cases, but curves in the paper show faster progression with TF.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum transfer learning with a decaying reliance on prior experience (Newton-cooling decay) allows micromanagement policies learned in simpler tasks to accelerate training and improve final performance in harder multi-opponent Robocode scenarios; when the target is much harder, inserting intermediate tasks further helps. The paper demonstrates qualitative trade-offs: uncontrolled reuse of prior experience can interfere, so a decay schedule mitigates negative transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning <em>(Rating: 2)</em></li>
                <li>Multi-agent bidirectionally-coordinated nets for learning to play StarCraft combat games <em>(Rating: 2)</em></li>
                <li>Evolving Robocode tanks for Evo Robocode <em>(Rating: 2)</em></li>
                <li>Play games using Reinforcement Learning and Artificial Neural Networks with Experience Replay <em>(Rating: 1)</em></li>
                <li>Decoupled Visual Servoing With Fuzzy Q-Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1014",
    "paper_id": "paper-208297717",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SSAQ_migration",
            "name_full": "SSAQ algorithm on binary-tree robot migration task",
            "brief_description": "A simulated embodied agent trained with the proposed SSAQ (softmax + simulated annealing Q-learning) algorithm to navigate a binary-tree map; used to measure convergence behaviour under a high-action-space, single-environment setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "robot migration agent (simulated)",
            "agent_description": "A simulated navigation agent trained with the SSAQ improved Q-learning algorithm (softmax / Boltzmann action selection with simulated annealing) operating in a semi-Markov decision process; Q-values are approximated with tabular updates in this experiment.",
            "agent_type": "simulated agent",
            "environment_name": "Binary-tree robot migration map (Robocode experimental task)",
            "environment_description": "A fixed binary-tree structured map with N layers; the agent starts at the root and must choose branches to reach endpoints. Complexity arises from a large discrete action set corresponding to endpoints and long decision horizons.",
            "complexity_measure": "Number of endpoints/actions = 2^N - 1 (N is number of layers); in experiment N = 10 → 2^10 - 1 = 1023 possible endpoints/actions; decision horizon equals tree depth (N).",
            "complexity_level": "high (N=10 → 1023 discrete endpoint actions)",
            "variation_measure": "Single static map instance (no procedural variation reported); variation effectively low (one environment instance / no randomization).",
            "variation_level": "low",
            "performance_metric": "Q-value convergence time (iterations to convergence per recorded Q entries)",
            "performance_value": "Convergence times reported for sampled Q-values: Q3 = 214 iterations, Q7 = 168 iterations, Q15 = 122 iterations (SSAQ); SSAQ converged faster than ε-greedy baselines.",
            "complexity_variation_relationship": "The experiment isolates high complexity (large action space) with low variation (single static task) and shows SSAQ achieves faster convergence and smoother Q-value curves than ε-greedy baselines, indicating the proposed exploration (softmax+annealing) handles large action spaces efficiently; no explicit formal trade-off equation is provided.",
            "high_complexity_low_variation_performance": "Convergence times: Q3=214, Q7=168, Q15=122 iterations (SSAQ) — reported performance in this condition.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "single-environment RL training (SSAQ tabular Q-learning) with simulated annealing softmax action selection",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Convergence measured in iterations; representative convergence times for sampled Q entries given (122–214 iterations); overall episodes not explicitly enumerated for this task beyond iteration counts.",
            "key_findings": "In a high-action-space, single-instance task the SSAQ algorithm converged faster and produced smoother Q-value learning curves than ε-greedy baselines, demonstrating improved stability and faster convergence in high-complexity, low-variation settings.",
            "uuid": "e1014.0"
        },
        {
            "name_short": "DMNN_4v4_FLM",
            "name_full": "Decision-Making Neural Network (DMNN) multi-agent tanks with fuzzy learning-rate (FLM)",
            "brief_description": "A multi-agent simulated tank team trained with a shared-parameter decision-making neural network (DMNN) using adaptive momentum and a fuzzy system to adjust RL learning rate, evaluated on Robocode 4v4 micromanagement scenarios.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "tank agent (DMNN with adaptive momentum and fuzzy LR)",
            "agent_description": "Simulated tank agents using multi-agent reinforcement learning with parameter sharing; state-action values are approximated by a BP neural network with adaptive momentum (DMNN); the RL learning rate is dynamically adjusted by a fuzzy inference system; action selection uses a softmax (Boltzmann) layer with simulated annealing.",
            "agent_type": "simulated agent (virtual tanks in Robocode)",
            "environment_name": "Robocode micromanagement 4v4 tank battles",
            "environment_description": "Physics-like simulated multi-robot confrontation environment (Robocode) with discrete state encoding from the game engine: absolute orientation angle (discretized into 4), relative direction angle (4), and distance (20 discrete bins); actions include movement directions (forward/backward) and rotations (clockwise/anticlockwise) and firing bullets with varying energies; complexity arises from partial observability, multi-agent interactions, and combinatorial state-action space.",
            "complexity_measure": "State space discretization: absolute angle (4) × relative angle (4) × distance bins (20) = 320 discrete state combinations; action space: movement/rotation types (4 primary movement actions) plus firing decisions (multiple bullet energy levels described qualitatively); multi-agent interactions (teams of 4 versus 4).",
            "complexity_level": "medium–high (320-state discretization per agent; multi-agent interaction multiplies effective complexity)",
            "variation_measure": "Variation comes from different enemy behaviors and random initial conditions; experiments compare different learning-rate settings and models (FLM vs LM-0.2 vs LM-0.95) and NN with/without adaptive momentum; number of random seeds / exact environment instances not quantified, but evaluation over repeated rounds (500 rounds) provides variation sampling.",
            "variation_level": "medium (multiple episodes/rounds, varied opponent behaviour; not heavily procedurally varied)",
            "performance_metric": "Score ratio (agent/team win-score relative to opponent), defensive score, fluctuation of score ratio (first-order difference), and stability of score curves.",
            "performance_value": "FLM (fuzzy LR + DMNN with AWM) achieved a score ratio around ~0.8; LM with fixed learning rates (LM-0.2 and LM-0.95) achieved ~0.7. DMNN with adaptive momentum produced higher defensive scores and lower fluctuation vs neural networks without adaptive momentum. Evaluations used 500 rounds with scores recorded every 10 rounds (50 sessions).",
            "complexity_variation_relationship": "The paper reports that using neural-network function approximation (DMNN) with adaptive momentum and a fuzzy dynamic learning rate yields more stable and higher performance under the multi-agent, partially-observed Robocode environment. It implies that algorithmic improvements (adaptive momentum, fuzzy LR) mitigate detrimental effects of environment complexity and per-episode variation, but it does not provide a formal quantitative trade-off curve between complexity and variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-agent RL with parameter sharing using DMNN; softmax/Boltzmann action selection with simulated annealing; fuzzy inference to adapt RL learning rate; trained for 500 rounds (evaluation recorded every 10 rounds).",
            "generalization_tested": true,
            "generalization_results": "Generalization evaluated by training agents on specific scenarios and testing against different opponents and larger teams; FLM generalized better than fixed-rate LMs (score ratio ~0.8 vs ~0.7) and produced more stable defensive performance, indicating better transfer across varied opponent behaviours within Robocode.",
            "sample_efficiency": "Learning evaluated over 500 rounds; performance curves and stability reported across 50 sessions (every 10 rounds); exact number of environment interactions per round not explicitly enumerated but rounds provide the unit of training/assessment.",
            "key_findings": "Parameter-sharing multi-agent DMNN with adaptive momentum and a fuzzy-controlled RL learning rate yields higher win rates (~0.8 score ratio) and more stable defensive performance than models with fixed learning rates and networks without adaptive momentum, indicating these algorithmic elements improve learning under moderate-to-high complexity and episodic variation in Robocode micromanagement.",
            "uuid": "e1014.1"
        },
        {
            "name_short": "Curriculum_TF_multiopponents",
            "name_full": "Curriculum transfer learning for increasing-opponent Robocode scenarios",
            "brief_description": "A curriculum transfer-learning approach that transfers policies from easier source tasks through intermediate tasks to more difficult Robocode micromanagement scenarios (facing 6 or 10 opponents), with a decay function (Newton's cooling law) to reduce interference from prior experience.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "tank agent (LM with TF)",
            "agent_description": "Simulated tank agents trained using the learning model (DMNN + fuzzy LR) combined with curriculum transfer learning: pretrained source-task policies are probabilistically exploited in the target task using a decaying threshold ε(t) (Newton cooling-inspired), optionally with intermediate tasks when target is much harder.",
            "agent_type": "simulated agent (virtual tanks in Robocode)",
            "environment_name": "Robocode multi-opponent micromanagement (6-opponent and 10-opponent battles)",
            "environment_description": "Increasingly difficult multi-agent confrontation scenarios in Robocode where our team faces larger opponent teams (six or ten TD-trained tanks); complexity grows with opponent count and coordinated opponent behaviour; variation across tasks arises from different opponent team sizes and policies.",
            "complexity_measure": "Primary complexity measure used is opponent count (team size): e.g., 6-opponent and 10-opponent target tasks; task difficulty is treated qualitatively (more opponents = more difficult). Also intermediate tasks used when target &gt;&gt; source difficulty.",
            "complexity_level": "higher complexity for 6-opponent and 10-opponent tasks relative to baseline 4v4; labelled 'more difficult' in the paper.",
            "variation_measure": "Variation measured as difference between source and target tasks (task difficulty shift), and number of different scenario instances (multiple rounds); curriculum includes intermediate tasks to bridge large variations. The transfer probability decays according to ε(t) = ε(t0) * exp(-p t + p t0) with decay coefficient p.",
            "variation_level": "medium–high (task difficulty shift introduced by increasing opponent counts; domain shift handled via curriculum/intermediate tasks)",
            "performance_metric": "Score ratio and defensive score over rounds (same metrics as other Robocode experiments).",
            "performance_value": "LM with curriculum transfer learning achieved faster learning and higher score ratios and higher defensive scores in the target (more difficult) scenarios compared to LM without TF; specific numeric score ratios not consistently tabulated in text for 6/10-opponent tests, but plots referenced show TF variant outperforming non-TF across training rounds.",
            "complexity_variation_relationship": "The paper reports that curriculum transfer learning with a decay on prior-experience usage improves performance when the target environment is more complex (more opponents) than the source; if target is much harder, adding an intermediate task helps. The authors describe a qualitative relationship: larger complexity increases difficulty, and controlled use of prior experience (decay) reduces interference and enables better transfer — but no quantitative function linking complexity magnitude and required curriculum parameters is given.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "LM with TF outperforms LM without TF in tasks with higher complexity and variation (6-opponent and 10-opponent scenarios) — qualitative improvement in score ratio and defensive score documented; exact numeric score ratios for these specific tests are not explicitly tabulated in the text.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum transfer learning (source → optional intermediate → target) with probabilistic exploitation of prior policy controlled by decay ε(t) (Newton's cooling law); DMNN + fuzzy LR used as base learner.",
            "generalization_tested": true,
            "generalization_results": "Yes — transfer learning accelerated learning speed and yielded higher score ratios and defensive scores in more difficult target tasks (6- and 10-opponent scenarios) compared to training from scratch; the curriculum with intermediate tasks further improved stability and final performance.",
            "sample_efficiency": "Reported improvements in learning speed (faster attainment of higher score ratios) over 500 rounds; exact sample counts for convergence not numerically reported for the multi-opponent cases, but curves in the paper show faster progression with TF.",
            "key_findings": "Curriculum transfer learning with a decaying reliance on prior experience (Newton-cooling decay) allows micromanagement policies learned in simpler tasks to accelerate training and improve final performance in harder multi-opponent Robocode scenarios; when the target is much harder, inserting intermediate tasks further helps. The paper demonstrates qualitative trade-offs: uncontrolled reuse of prior experience can interfere, so a decay schedule mitigates negative transfer.",
            "uuid": "e1014.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning",
            "rating": 2,
            "sanitized_title": "starcraft_micromanagement_with_reinforcement_learning_and_curriculum_transfer_learning"
        },
        {
            "paper_title": "Multi-agent bidirectionally-coordinated nets for learning to play StarCraft combat games",
            "rating": 2,
            "sanitized_title": "multiagent_bidirectionallycoordinated_nets_for_learning_to_play_starcraft_combat_games"
        },
        {
            "paper_title": "Evolving Robocode tanks for Evo Robocode",
            "rating": 2,
            "sanitized_title": "evolving_robocode_tanks_for_evo_robocode"
        },
        {
            "paper_title": "Play games using Reinforcement Learning and Artificial Neural Networks with Experience Replay",
            "rating": 1,
            "sanitized_title": "play_games_using_reinforcement_learning_and_artificial_neural_networks_with_experience_replay"
        },
        {
            "paper_title": "Decoupled Visual Servoing With Fuzzy Q-Learning",
            "rating": 1,
            "sanitized_title": "decoupled_visual_servoing_with_fuzzy_qlearning"
        }
    ],
    "cost": 0.01445175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fuzzy Reinforcement Learning and Curriculum Transfer Learning for Micromanagement in Multi-Robot Confrontation
Published: 2 November 2019</p>
<p>Chunyang Hu huchunyang@hbuas.edu.cn 
School of Computer Engineering
Hubei University of Arts and Science
441053XiangyangChina</p>
<p>Meng Xu 
School of Computer Science
Northwestern Polytechnical University
710072Xi'anChina</p>
<p>Fuzzy Reinforcement Learning and Curriculum Transfer Learning for Micromanagement in Multi-Robot Confrontation
Published: 2 November 201910.3390/info10110341Received: 12 October 2019; Accepted: 30 October 2019;information Articlemulti-robot confrontationfuzzy reinforcement learningcurriculum transfer learningneural network
Multi-Robot Confrontation on physics-based simulators is a complex and time-consuming task, but simulators are required to evaluate the performance of the advanced algorithms. Recently, a few advanced algorithms have been able to produce considerably complex levels in the context of the robot confrontation system when the agents are facing multiple opponents. Meanwhile, the current confrontation decision-making system suffers from difficulties in optimization and generalization. In this paper, a fuzzy reinforcement learning (RL) and the curriculum transfer learning are applied to the micromanagement for robot confrontation system. Firstly, an improved Q-learning in the semi-Markov decision-making process is designed to train the agent and an efficient RL model is defined to avoid the curse of dimensionality. Secondly, a multi-agent RL algorithm with parameter sharing is proposed to train the agents. We use a neural network with adaptive momentum acceleration as a function approximator to estimate the state-action function. Then, a method of fuzzy logic is used to regulate the learning rate of RL. Thirdly, a curriculum transfer learning method is used to extend the RL model to more difficult scenarios, which ensures the generalization of the decision-making system. The experimental results show that the proposed method is effective.In this paper, we focus on the robot confrontation system to explore an effective learning method for agent control. In the robot confrontation system, the condition for the agent is similar to a decision-making (RTS) game[7]. Real-time strategic games are different from taking turns to play in board games because it runs in real-time and requires continuous decision-making for agents. RTS game provides a physical-based simulation environment to study the control of agents with different learning levels, such as StarCraft [8], Dota 2, and Namco. Reinforcement learning (RL) [9] is an effective machine learning method and the goal of reinforcement learning for an agent is to learn an optimal action strategy and obtain optimal rewards. This year, it has attracted extensive attention from scholars [10-12].Machine Learning AlgorithmsMachine learning algorithms have been applied in many fields, such as electricity price forecasting[13]. Generally, machine learning algorithms can be divided into three categories: supervised learning, unsupervised learning and reinforcement learning. Reinforcement learning includes value-based RL method[14], policy-based RL method[15], and the hybrid method named actor-critic[16]. Generally, the RL method is very suitable for sequential real-time scenarios when these scenarios are modeled as the Markov decision-making sequences or semi-Markov decision sequences.[17] uses a Q-learning method for evolving a few basic behaviors and learning to flight for bots in the FPS game of Counter-Strike. The author has carried out experiments on how bots can evolve its behavior in a more detailed model of the world using the knowledge learned from abstract models. For the RL method, the learning rate is important, because it affects the performance and convergence speed of the RL system. However, the performance of the fixed learning rate for RL methods often encounters bottlenecks. Reference [18] uses a neural network as a function approximator to estimate the action-value function for a group of units in StarCraft micromanagement. As an effective state representation, the neural network breaks down the dilemma caused by the large state-action space in the game scenarios. However, for the neural network, the overcorrection of the weights at one time may lead to the dilemma of learning instability, which will lead to low learning efficiency. The adaptive momentum[19]breaks down the dilemma between stability and efficiency.Recently, deep learning has achieved record-breaking performance in a variety of complex scenarios, and which provides an opportunity of scaling to problems with infinite state spaces for the RL algorithms[20]. For StarCraft micromanagement, reference [21] uses the actor-critic algorithm and Multi-agent Bidirectionally-Coordinated Network (BiCNet) to control bot's behavior. They model the dependency of units by the BiCNet, which can handle different types of combats with arbitrary numbers of AI agents for both sides. However, deep learning requires relatively high computational power and cannot work on all platforms. As a powerful technology, transfer learning is very effective for expanding the Machine Learning model to another domain and avoiding many expensive data-labeling efforts. The relationship between transfer learning and other related machine learning techniques is discussed in[22].Research Motivation in This WorkThe main research contents of Multi-Robot Confrontation include situation assessment and strategy selection. This study focuses on strategy selection. The scenes of Multi-Robot Confrontation are usually modeled as a semi-Markov decision-making process, and there are some strategy selection models that are based on classical reinforcement learning. However, these existing models often have the following four dilemmas. Firstly, most of the reinforcement learning models that are used for the strategy selection are tabular reinforcement learning methods, and the performance of these methods becomes worse with the expansion of the state-action space of the learning agent. Secondly, the learning rate of classical reinforcement learning methods is a fixed value. The process of turning the learning rate is quite empirical and often costs a lot of time. Thirdly, for the learning model of the Multi-Robot Confrontation based on RL, most models will use the ε-greedy strategy. This strategy Information 2019, 10, 341 3 of 22 chooses each action with the same probability, whether good or bad. Finally, the existing works suffer from generalization dilemma, which causes a learning gap between a known environment and the unknown environment.In this work, we investigate the Multi-Robot Confrontation in the semi-Markov decision-making process. An improved Q-learning algorithm with softmax function is introduced to train an agent in the semi-Markov decision-making process. A defined state-action space will not be expanded to avoid the curse of dimensionality due to the scale of the scenario. A reward function of this RL model helps agents balance losses of our agents and opponents.Contributions in This WorkThe main contributions of this paper include four parts. Firstly, in order to address the first dilemma mentioned above, this study introduces a Multi-agent RL algorithm with parameter sharing to train agents. A neural network (NN) with adaptive momentum is used as a function approximator to estimate the state-action value function. This NN model not only accelerates the efficiency of decision-making as an effective state representation but also guarantees the stability of neural network learning. Secondly, this study introduces an adaptive method with the fuzzy method to adjust the learning rate for the RL model to tackle the second dilemma. This method has been proved to be effective in improving the performance of micromanagement in the experiments. Thirdly, in order to address the third dilemma, we use a Boltzmann distribution to describe a statistical probability that decides the selection of each action. Fourthly, this study introduces a curriculum transfer learning method to address the generalization dilemma. This method improves the performance of micromanagement in different scenarios instead of learning from scratch. Meanwhile, we set the decay function according to Newton's cooling law. The decay function can reduce the influence of interference information in prior experience on a new micromanagement scenario for curriculum transfer learning. As far as the method itself is concerned, the developed method can be applied not only to the robot confrontation system but also to other application scenarios, such as Starcraft II.</p>
<p>Introduction</p>
<p>The Robot Confrontation System</p>
<p>The aim of Artificial Intelligence (AI) is to develop a computer program that can realize human-level intelligence, self-consciousness, and knowledge application. Multi-Agent Systems (MAS) have recently become popular as an important means for the study of confrontational decision-making, strategic behavior in electricity markets [1], and so on. As an effective tool for AI research, the simulation platform allows the agent to rely on the predefined algorithm to perform various kinds of actions in a certain scenario, which plays a role in replacing the real physical environment [2]. These simulations can not only be used as substitutes for the physical environment that can touch but can also be set to some scenarios that cannot exist in real-life depending on our imagination [3]. Recently, computer games have been used for AI research, which helps the agent to grow since its birth, including the Atari video games [4], the imperfect information game and so on [5]. The Multi-Robot Confrontation [6], as a platform to imitate real battlefields, provides convenience for military command, situation assessment, and intelligent decision-making, and is also an effective platform to develop AI applications.</p>
<p>Paper Structure</p>
<p>The remainder of the paper is organized as follows: Section 2 is devoted to the background for the proposed technique, such as reinforcement learning, softmax function. Section 3 presents a learning model for a single agent and this model uses an improved Q-learning with the fuzzy method. A neural network with adaptive momentum and the proposed multi-agent RL algorithm with parameter sharing for Multi-Robot Confrontation are introduced in Section 4. Section 5 introduces a curriculum transfer learning to address the generalization issue. This method transfers the prior learning experience to different scenarios without starting from scratch. Experiments are detailed in Section 6, to illustrate the performance of the proposed learning model. Conclusions are drawn in the last section.</p>
<p>Background</p>
<p>Reinforcement Learning</p>
<p>The goal of reinforcement learning is to obtain an optimal strategy π in which the agent selects action A under state S, which is given by π(S) = A. The architecture for reinforcement learning is shown in Figure 1. s t is the current state of the agent and s t+1 is the next state. a t is the current action that the agent takes. r t is the current reward for the process of s t → s t+1 . γ is the discount factor. V(s) is the state value function for the state s. T a ss is the transition probability from the state s to the next state s . R a ss is the current reward that is obtained from the environment from the state s to the next state s . α is the learning rate and its value range is (0, 1).</p>
<p>The agent which is in state s t selects action a t until the final state is reached, and the cumulative reward obtained in this process is shown in Equation (1). R(S t ) = r t + γr t+1 + γ 2 r t+2 + . . . = r t + γR(S t+1 ) = ∞ i=0 γ i r t+i (1) Information 2019, 11 The agent which is in state t s selects action t a until the final state is reached, and the cumulative reward obtained in this process is shown in Equation (1).  </p>
<p>where the learning rate reflects the efficiency of an RL algorithm.</p>
<p>Softmax Function Based on Simulated Annealing</p>
<p>In order to control the randomness of action selection, a simulated annealing (SA) algorithm [23,24] is used to optimize softmax function. The softmax function is a method for balancing Exploration and exploitation [25] in the RL method, which chooses the action according to the average reward of each action, and the probability of the action t a being chosen is higher if the average reward produced by the action is higher than the average reward produced by the other action.</p>
<p>The probability distribution of the action in the softmax algorithm is based on the Boltzmann distribution. The probability i P of action i a selected is given by, For the reinforcement learning algorithm that uses the future P-step average rewards, the mathematical expression for the value function V π (s t ) is shown in Equation (2).
V π (s t ) = lim p→∞ 1 p t=p t=0 r t(2)
When an agent takes a t a strategy π, the value function represents the expectation of the cumulative reward obtained by the agent.</p>
<p>The value function V π (s) of an agent which is in the state s is given by:
V π (s) = E π {R t |s t = s} = E π r t+1 + γr t+2 + γ 2 r t+3 + . . . s t = s = a π(s, a) s T a ss R a ss + γV π (s )(3)
Q-Learning is a model-free reinforcement learning algorithm, and it is an off-policy reinforcement learning algorithm. The Q value Q(s, a) is estimated via the Time Difference Method (TD Method) [21]:
Q t+1 (s t , a) = (1 − α)Q t (s t , a) + α<a href="4">r t + γmaxQ t (s t+1 , a )</a>
where the learning rate reflects the efficiency of an RL algorithm.</p>
<p>Softmax Function Based on Simulated Annealing</p>
<p>In order to control the randomness of action selection, a simulated annealing (SA) algorithm [23,24] is used to optimize softmax function. The softmax function is a method for balancing Exploration and exploitation [25] in the RL method, which chooses the action according to the average reward of each action, and the probability of the action a t being chosen is higher if the average reward produced by the action is higher than the average reward produced by the other action.</p>
<p>The probability distribution of the action in the softmax algorithm is based on the Boltzmann distribution. The probability P i of action a i selected is given by,
e (a i ) K k=0 e (a k ) → P i(5)
where P i represents the probability of choosing action a i , and the total number of actions is K. The action selection policy based on Boltzmann distribution is used to ensure the randomness of the action selection, and the simulated annealing algorithm is added. In this method, the probability of the action a i being selected is given by,
e (Q(s t ,a i )/T t ) K k=1 e (Q(s t ,a k )/T t )
→ P(a i |s t ) (6) where T t is the temperature parameter. The smaller the temperature parameter is T min , the bigger the probability of action with the high average reward being chosen is T max . The temperature value turned by the simulated annealing is given by,
T t+1 = η(T t − T min ) + T min T 0 = T max(7)
where η is the annealing factor, and the value range is 0 ≤ η ≤ 1.</p>
<p>An RL Model for a Single Agent</p>
<p>An Improved Q-Learning Method in Semi-Markov Decision Processes</p>
<p>For Markov dynamic systems, the stochastic control problems are modeled as Semi-Markov decision processes (SMDPs) [26,27]. The time cost for the RL system to transit from one state to the next state is defined as the sojourn time. The robot confrontation process is regarded as an SMDP, and the agent may take a serial of the same actions before transiting into the next state. The RL method can diminish the uncertainty resulting from modeling purposely, compared with the dynamic modeling method.</p>
<p>If our agent defeats the opponents with a better probability, the output of the agent is required to be more stable in the robot confrontation system for micromanagement. The classical Q-learning method solves the dilemma of exploration and exploitation using the ε-greedy algorithm, which makes the agent have a certain probability to explore new actions [28]. However, the probability of each action being selected is the same when the ε-greedy algorithm is used, so the action that can produce better rewards is not easy to choose. To tackle the problem of the greedy algorithm for micromanagement in the robot confrontation system, we design an improved Q-learning with the softmax function, which can make our agent explore more actions in the early stage of the learning process and exploit previous experience in the later stage of the learning process. For the learning process of SMDP, in each epoch, the next state is transited from the current state using the same actions after T learning cycles. r t+i i = 0, 1, 2, . . . , T − 1 is the real-time reward. Equations (8)- (10) gives the updating method of the state-action function.
γQ t+1+i (s t , a t+1+i ) + r t+i → Q t+i (s t , a t+ j ), 1 ≤ i &lt; T − 1 (8) γ a π(a s t+1 )Q(s t+1 , a) + r t+T−1 → Q t+T−1 (s t , a t+T−1 ) ⇒ γ a exp{Q(s t+1 ,a)/T t} a exp{Q(s t+1 ,a)/T t} Q(s t+1 , a) + r t+T−1 → Q t+T−1 (s t , a t+T−1 ) (9) Q t (s t , a t ) + α( T−1 k=1 Q t+k + r t )/T → Q t+T (s t , a t )(10)
where T t is the temperature parameter. The detailed step of this algorithm (SSAQ algorithm) is shown in Algorithm 1. The SSAQ algorithm is a way to solve the dilemma of exploration and exploitation, and this method can output more stable actions for micromanagement scenarios. Initialize s t , a t , s t+1 , a t+1 , r t each value of Q matrix ← arbitrarily value; Repeat (for each step)</p>
<p>Choose an initial state s 0 t ← 0 ; i ← 0 ; Repeat (for each step of the episode) P(a t |s t ) = max ps:1→K t P(a ps k |s t ) = max
ps:1→K t exp[Q(s t ,a ps k )/T t ] K t k=1 exp[Q(s t ,a k )/T t ] → a t
Observe s t+1 , after T learning cycles by the same actions a t Obtain reward r t , r t+1 , . . . , r t+T−1 P(a t+1 |s t+1 ) = max ps:1→K t P(a ps k |s t+1 ) = max
ps:1→K t exp[Q(s t+1 ,a ps k )/T t ] K t k=1 exp[Q(s t+1 ,a k )/T t ]
→ a t+1 s t ← s t+1 t + + Q(s t , a t ) ← f (Q(s t+1 , a t+1 ), r t , . . . , r t+T−1 , α, γ) until s is terminal. Until Q matrix is convergence.</p>
<p>A Reinforcement Learning Method using a Fuzzy System</p>
<p>In order to reduce the learning cost, a learning method using a dynamic learning rate is a good solution [29]. For the reward function in the RL system, when the reward is positive after using the RL method, the influence of the positive feedback and a faster learning rate can be ensured by a high learning rate; on the contrary, a low learning rate can guide the RL method to a faster convergence. Therefore, there will be a relationship between the obtained reward and the value of the learning rate. Meanwhile, the reward is a fuzzy concept. For instance, a fixed value does not divide the "large positive number" and "large negative number". So, a fuzzy system is used to develop a dynamic learning rate to improve the performance of this learning system. The reward r is taken as the input of the fuzzy system and the learning rate α F is taken as the output. We set the fuzzy description of r as "little negative number, the large negative number, zero, large positive number, little positive number", and their abbreviations are shown as "TN, LN, ZO, LP, TP". We set the fuzzy description of α F as "little small, very small, medium, very large, little large", and their abbreviations are shown as "LS, VS, M, VL, LL". Figure 2 gives the corresponding fuzzy membership functions. Previous work has shown that the performance of the fuzzy system is affected by the shape of the membership functions [30]. In the fuzzy system, triangular membership function and trapezoidal membership function are simple and effective, so we choose these two membership functions.</p>
<p>In  Figure 2 shows the curve for these membership functions. The degree of trust for the input shows a general mathematical symmetry. We take "LP" as an example, and its membership function is shown in Equation (11). Previous work has shown that the performance of the fuzzy system is affected by the shape of the membership functions [30]. In the fuzzy system, triangular membership function and trapezoidal membership function are simple and effective, so we choose these two membership functions.</p>
<p>(a) (b) . Figure 2 shows the curve for these membership functions. The degree of trust for the input shows a general mathematical symmetry. We take "LP" as an example, and its membership function is shown in Equation (11). 
μ &lt;   = − − &lt; &lt;   &lt; (11)
The number of discrete points for the input domain is represented by . These five points have their own degrees of truth corresponding to five fuzzy input descriptions. Equation (12) gives the input degree for the truth discrete matrix
5 [ ] r r r ij N ID × = ID . ( ) r r r ij i j ID I μ =(12)
The number of discrete points for the output domain is
F M α . We select five independent discrete points { | 1,..., } F F F j O j M α α α = = O
from the output domain. Equation (13) shows the corresponding output degree for the truth discrete matrix
5 [ ] F F F ij M OD α α α × = OD . ( ) F F F ij i j OD O α α α μ =(13)
Then, we design the fuzzy rules, as follows. "If reward is "LP" then the learning rate is "VL", If reward is "TP" then learning rate is "LL", if reward if "ZO" then learning rate is "M", if reward is "TN" then learning rate is "LS", if reward is "LN" then learning rate is "VS"". The fuzzy inference is
represented by [ ] F F r F r r ij N M r α α α × = RS
and Equation (14) gives its mathematical expression. 
µ r 1 (r) =          1, r &lt; r 2 (r 3 − r)/(r 3 − r 2 ), r 2 &lt; r &lt; r 3 0, r 3 &lt; r(11)
The number of discrete points for the input domain is represented by N r . We select five independent discrete points from the input domain, which are represented by I r = I r j j = 1, . . . , N r . These five points have their own degrees of truth corresponding to five fuzzy input descriptions. Equation (12) gives the input degree for the truth discrete matrix
ID r = [ID r ij ] 5×N r . ID r ij = µ r i (I r j )(12)
The number of discrete points for the output domain is M α F . We select five independent discrete (13) shows the corresponding output degree for the truth discrete matrix
points O α F = O α F j j = 1, . . . , M α F from the output domain. EquationOD α F = [OD α F ij ] 5×M α F . OD α F ij = µ α F i (O α F j )(13)
Then, we design the fuzzy rules, as follows. "If reward is "LP" then the learning rate is "VL", If reward is "TP" then learning rate is "LL", if reward if "ZO" then learning rate is "M", if reward is "TN" then learning rate is "LS", if reward is "LN" then learning rate is "VS"". The fuzzy inference is represented by
RS rα F = [r rα F ij ] N r ×M α F
and Equation (14) gives its mathematical expression.
r rα F ij = 5 ∨ k=1 (ID r ki ∧ OD r k j )(14)
where "∨" represents the operation of choosing the maximum value. "∧" represents the operation of choosing the minimum value. The operation of fuzzification transforms the reward r 0 into a fuzzy input vector FI r 0 = [FI r 0 j ] 1×N r if r 0 is measured. The degrees of the truth for r 0 corresponding to the five inputs are µ
r 0 i µ r 0 i = µ r i (r 0 ); i = 1, .
. . , 5 . Equation (15) uses the weighted-average method to calculate FI r 0 . We use the "min-max compose" operation to calculate the fuzzy output vector
FI r 0 j = 5 i=1 (ID r ij × µ r 0 i ) 5 k=1 µ r 0 i(15)FO r 0 = [FO r 0 j ] 1×M α F
using Equations (14) and (15).
FO α FS j = N r ∨ k=1 (rs k j ∧ FI k )(16)
The de-fuzzifying operation uses a weighted average method to transform the fuzzy output into an output value and the weighted average method is shown in Equation (17).
α FS = M α F j=1 (FO α FS j × O α j ) M α F k=1 FO α FS j(17)
where the learning rate α FS is the final result of the fuzzy system relative to r 0 .</p>
<p>A Proposed Learning Model for Multi-Robot Confrontation</p>
<p>The classical Q-learning algorithm chooses actions by the look-up table method. However, with the increase in action space and state space, the look-up table method is obviously no longer suitable, and which results in low learning efficiency. In order to solve this problem, the RL method based on the neural network approximating the value function of Q learning is proposed [31]. In the random task scenarios, the state variable s t is used as the input of the neural network and the Q value is used as the output of the neural network, which is also the estimation of the Q value of the neural network based on previous experience. This Q value is Q current , and the action that was taken by the agent is the action corresponding to the maximum Q current . After the agent takes action, the environment will give agent reward, and the state of the agent will be transferred to s t+1 . Similarly, the state s t+1 is input into the neural network, and the Q value of the corresponding state s t+1 is obtained, which is Q next . Finally, Q next and Q current is used to update the Q value using (4). The gradient descent method is used to update the gradient of the neural network. The loss function L t of the neural network is shown in Equation (18).
L t = (Q_current * − Q_current) = (1 − α)Q_current + α[r t + γmaxQ_next] − Q_current(18)
where Q_current * is the Q value after the state s t is updated by (4), and Q_next is the Q value before the state s t is updated.</p>
<p>Neural Network Model with Adaptive Momentum</p>
<p>Since the efficient action selection strategy should be considered for micromanagement scenarios, and our agent's experience usually has a limited subset of the large state space, it will be difficult to apply the conventional reinforcement learning to learn an optimal policy. To address this problem, a BP neural network is used to approximate the state-action values to improve the generalization of our RL model. An acceleration algorithm using adaptive momentum is considered to be used in the BP neural network to ensure the efficiency of action selection.</p>
<p>In addition to the output layer and the input layer, the input signal of any neuron j in a layer is represented by net j . y j is the output signal. y j represents the output signal for the neuron i in the lower layer. Equation (19) gives the computing method for this output.
           net j = i y i ω ji f (x) = (1−e −bx )a 1+e −bx y j = f (θ j + i y i ω ji )(19)
Information 2019, 10, 341
9 of 22
where the constant a = 1.725, b = 0.566, the threshold for a neuron k is represented by θ k . In the output layer, y k is the label output signal for the k − th neuron. net k is the input signal. y k and net k are computed by Equation (20), if y j is the output signal of the neuron j − th in the hidden layer next to the output layer.
net k = j y j ω k j y k = f ( j y j ω k j + θ k )(20)
In the t − th iteration for updating weight, an input value is x p (t). O pk (t), is the label output signal for the k − th neuron. y pk (t) is the actual output signal. Equation (21) gives the mean square error.
E p (t) = k (O pk (t) − y pk (t)) 2 2(21)
The weight of this neural network is updated by the back-propagation method. The mean square error of this neural network is given by Equation (22) if there are M n input values.
E p (t) = p k (O pk (t) − y pk (t)) 2 2M n(22)
The weights ω k j (t) is updated according to the gradient direction of E p (t) to minimize the square error. The correction of ω k j (t) is ∆ p ω k j (t), which is given by:
                   ∂E p (t) ∂ω k j (t) = ∂E p (t) ∂net k (t) · ∂net u (t) ∂ω k j (t) ∂net u (t) ∂ω k j (t) = ∂ j y pj (t)ω k j (t) ∂ω k j (t) ∆ p ω k j (t) = β∆ p ω k j (t) − λ ∂E p (t) ∂ω k j (t+1)(23)
where the learning rate is λ and the momentum constant is β that is 0.95.</p>
<p>If the learning rate of the neural network is too large, the weight correction ∆ p ω k j (t) will be too large, so the stability of the learning process will be affected. Therefore, we use an adaptive learning rate. α(p) represents the adaptive learning rate, and mean square error is E p (t). The adaptive learning rate satisfies Equation (24).
α(p) = 1 − exp(−E p (t))(24)
If the mean square error increases, the learning rate increases, and the convergence rate of the neural network accelerate; on the contrary, the neural network tends to be stable.
We set δ pk (t) = − ∂E p (t)
∂net k (t) , and (25) can be obtained.
               f (θ k (t) + net k (t)) = ∂ ∂net k (t) ( 2a 1+exp(−F(θ k (t)+net k (t))) ) = (1 − y pk (t))y pk (t) δ pk (t) = − ∂E p (t) ∂net k (t) · ∂y pk (t) ∂net k (t) = f (θ k (t) + net k (t)) · (O pk (t) − y pk (t) )(25)
Therefore, Equation (26) can be obtained.
       ∆ p ω k j (t) = β∆ p ω k j (t) − δ pk (t)y pk (t) δ pk (t) = (O pk (t) − y pk (t)) ∂ ∂net k (t) ( 2a 1+exp(−F(θ k (t)+net k (t))) )(26)
The updating for the weights is given by Equation (27).
           δ pj = ∂E p ∂net j = ∂E p ∂y pj · ∂y pj ∂net j = y pj (y pj − 1) ∂E p ∂y pj ∆ p ω ji = −λ ∂E p ∂net j · ∂net j ∂ω ji = λy pj δ pj(27)
where,
− ∂E p ∂y pj = k ω k j δ pk = − k ∂E p ∂net k · ∂net k ∂y pj = k ( ∂ m y pm ω km ∂y pj )y pj (1 − y pj ) ∂E p ∂y pj(28)
Then,
δ pj = −y pj (y pj − 1) · δ pk ω k j(29)
Finally, ∆ p ω ji = λy pi δ pj is the correction of the weights for the hidden layer.</p>
<p>Multi-Agent RL Algorithm Based on Decision-Making Neural Network with Parameter Sharing</p>
<p>In this paper, an accelerated BP neural network with adaptive momentum is used as an approximator of the state-action value function. In this study, we extend the SSAQ algorithm to multi-agent by sharing the parameters of the neural network. Agents behave differently because each one receives different states and actions in the environment. Therefore, it is feasible for multi-agent to use the same neural network via parameter sharing. The input of the neural network is the agent state set in this paper, and the output is the state-action value function of the corresponding state. In order to ensure the continuous actions of the agent, a softmax layer is added after the output layer of the neural network, and the softmax layer uses the softmax function to select the action for the agent. Meanwhile, the simulated annealing algorithm is added to the softmax function to adjust the temperature parameters. This kind of neural network is called a decision-making neural network (DMNN) in this paper. The neural network is used as the approximator for the SSAQ algorithm mentioned above, and the learning model for our agents is shown in Figure 3.</p>
<p>As shown in Figure 4, the decision-making neural network includes an input layer, a multilayer hidden layer, an output layer, and a softmax layer. The state s t of the agent is inputted to the input layer of the decision-making neural network, and the Q value corresponding to the state s t is outputted to the output layer of the decision-making neural network, which is recorded as
Q(s t , a t ; θ) i = 1, 2, . . . , K t .
where θ is the weight vector of the decision-making neural network. The Q values of all actions are entered into the softmax layer with a non-linear transformation function which is shown in Equation (30), and the action a t is selected and outputted using the Max operation as shown in Equation (31).
φ(x) = exp(x)/T i (30) max φ(x 1 )/ m i=1 φ(x i ), φ(x 2 )/ m i=1 φ(x i ), . . . , φ(x m )/ m i=1 φ(x i )(31)
As shown in Figure 4, the decision-making neural network includes an input layer, a multilayer hidden layer, an output layer, and a softmax layer. The state t s of the agent is inputted to the input layer of the decision-making neural network, and the Q value corresponding to the state t s is outputted to the output layer of the decision-making neural network, which is recorded as<br />
( ) { } , ; | 1, 2,..., t t t Q s a i K θ = .( ) ( ) exp / i x x T φ = (30) ( ) ( ) ( ) ( ) ( ) ( ) { } 1 2 1 1 1 max / , / ,..., / m m m i i m i i i i x x x x x x φ φ φ φ φ φ = = =   (31)
The agent takes action t a and the environment gives instant rewards ( ) R t . The RL based on state-action function stores the state-action function in the form of a table, but this method cannot solve the large-scale continuous state-space problem, and the powerful generalization ability of the As shown in Figure 4, the decision-making neural network includes an input layer, a multilayer hidden layer, an output layer, and a softmax layer. The state t s of the agent is inputted to the input layer of the decision-making neural network, and the Q value corresponding to the state t s is outputted to the output layer of the decision-making neural network, which is recorded as   where θ is the weight vector of the decision-making neural network. The Q values of all actions are entered into the softmax layer with a non-linear transformation function which is shown in Equation (30), and the action t a is selected and outputted using the Max operation as shown in Equation (31).
( ) ( ) exp / i x x T φ = (30) ( ) ( ) ( ) ( ) ( ) ( ) { } 1 2 1 1 1 max / , / ,..., / m m m i i m i i i i x x x x x x φ φ φ φ φ φ = = =   (31)
The agent takes action t a and the environment gives instant rewards ( ) R t . The RL based on state-action function stores the state-action function in the form of a table, but this method cannot solve the large-scale continuous state-space problem, and the powerful generalization ability of the The agent takes action a t and the environment gives instant rewards R(t). The RL based on state-action function stores the state-action function in the form of a table, but this method cannot solve the large-scale continuous state-space problem, and the powerful generalization ability of the neural network is used to approximate the value function Q(s t , a t ; θ) for RL, which solves the problem of high dimensional continuous state space. The proposed method has a good generalization and can be used in other combat games, such as Starcraft II [18].</p>
<p>To update the weights of the neural network efficiently, the TD error of the RL method is used for the Loss function for the decision-making neural network. The TD error is shown in Equation (32). The back propagation algorithm is used to update the weights of neural networks. The Multi-agent SSAQ algorithm with network parameter sharing is given by Algorithm 2. 
         Q next (s t , a t , θ t ) = Q(s t , a t , θ t ) cur + α[R(t) + γQ(s t+1 , a t+1 , θ t ) − Q(s t , a t , θ t ) cur ] φ t = Q next (s t , a t , θ t ) − Q(s t , a t , θ t ) cur(32)t ) = max ps:1→K t exp[Q(s t ,a ps k )/T t ] K t k=1 exp[Q(s t ,a k )/T t ] → a t
Observe s t+1 , after T learning cycles by the same actions a t Obtain reward r t , r t+1 , . . . , r t+T−1 P(a t+1 |s t+1 ) = max ps:1→K t P(a ps k |s t+1 ) = max
ps:1→K t exp[Q(s t+1 ,a ps k )/T t ] K t k=1 exp[Q(s t+1 ,a k )/T t ] → a t+1
Update TD error and weights:
φ t ← α[ f (Q(s t+1 , a t+1 ), r t , . . . , r t+T−1 , α, γ) − Q(s t , a t , θ t )] E t ← 1 2 φ 2 t ; α t ← 1 − exp(−E t ) θ t+1 ← θ t + α t φ t t + + until s is terminal.</p>
<p>Curriculum Transfer Learning</p>
<p>Curriculum Transfer Learning for Different Micromanagement Scenarios</p>
<p>It will cost a lot of time if the agent starts learning from scratch in a new environment. Many researchers focus on improving the learning performance by exploiting domain knowledge between some related tasks. The prior learning experience is exploited from the source task to the target task by the transfer learning to accelerate the learning rate. Therefore, we use the transfer learning method to take the well-trained model of source task as the prior experience to build the learning model to the target task.</p>
<p>In the curriculum transfer learning for micromanagement scenarios in the confrontation decision-making system, the mapping ρ : π * pasttime → π * currenttime represents the process that transfers the learning policy of the source task to the learning policy of the target task. In this paper, the state space and action space remain unchanged, as shown in Equation (33).</p>
<p>ρA : A last time → A current time ρS : S last time → S current time (33) Many interference information exists in the learning process. Therefore, we set up a decay function using the Newton law of cooling. The decay function enables agents to exploit the domain knowledge with a decreasing probability. A steady-state is achieved eventually. The threshold is ε. Equation (34) shows the mathematical relationship between the threshold ε and time t. The agent uses the domain knowledge from the source task if the random number ε &lt; random. Otherwise, the agent uses the conventional maximum Q value strategy to select an action.
ε(t) = ε(t 0 ) · exp −pt + pt 0(34)
where the decay coefficient is p and the initial time is t 0 . The probability of using the prior experience from the source task gradually decreases until a stable value is achieved. If the target task is too difficult compared with the source task, an intermediate task is usually set up in curriculum transfer learning, and the agent can gain more experience by the learning model for the intermediate task. The curriculum transfer learning with an intermediate task and decay function for micromanagement scenarios is shown in Figure 5. </p>
<p>Many interference information exists in the learning process. Therefore, we set up a decay function using the Newton law of cooling. The decay function enables agents to exploit the domain knowledge with a decreasing probability. A steady-state is achieved eventually. The threshold is ε . Equation (34) shows the mathematical relationship between the threshold ε and time t . The agent uses the domain knowledge from the source task if the random number random ε &lt; . Otherwise, the agent uses the conventional maximum Q value strategy to select an action.
0 0 ( ) ( ) exp{ } t t pt pt ε ε = ⋅ − +(34)
where the decay coefficient is p and the initial time is 0 t . The probability of using the prior experience from the source task gradually decreases until a stable value is achieved. If the target task is too difficult compared with the source task, an intermediate task is usually set up in curriculum transfer learning, and the agent can gain more experience by the learning model for the intermediate task. The curriculum transfer learning with an intermediate task and decay function for micromanagement scenarios is shown in Figure 5. The integral framework for the proposed learning model for micromanagement is shown in Figure 6. The proposed method has three parts: the decision-making neural network, the loss function that uses the TD error for the neural network and a fuzzy method. The state t s of the agent is input into the neural network, and the neural network outputs the action t a . So is the next state 1 t s + . The reward ( ) R t is obtained. The TD error is calculated as a loss function and a fuzzy method is used to adjust the learning rate of the RL method. The integral framework for the proposed learning model for micromanagement is shown in Figure 6. The proposed method has three parts: the decision-making neural network, the loss function that uses the TD error for the neural network and a fuzzy method. The state s t of the agent is input into the neural network, and the neural network outputs the action a t . So is the next state s t+1 . The reward R(t) is obtained. The TD error is calculated as a loss function and a fuzzy method is used to adjust the learning rate of the RL method.  </p>
<p>Experiment and Analysis</p>
<p>Generally, in the robotic systems that are based on reinforcement learning, higher learning rates enable robots to utilize the previous learning experience. The larger discount rate makes learning agents think more about long-term returns in the future. For the exploration and exploitation, the εgreedy algorithm with a larger threshold allows the learning agent to more utilize prior experience. In this work, the initial value for the threshold of the decay function is the same as that of the classical ε-greedy algorithm. In these experiments, the values for the learning rate, the discount factor, the Exploring rate, the Annealing factor, the Maximum temperature parameter, and the Minimum temperature parameter are given manually and empirically. </p>
<p>Experiment and Analysis</p>
<p>Generally, in the robotic systems that are based on reinforcement learning, higher learning rates enable robots to utilize the previous learning experience. The larger discount rate makes learning agents think more about long-term returns in the future. For the exploration and exploitation, the ε-greedy algorithm with a larger threshold allows the learning agent to more utilize prior experience. In this work, the initial value for the threshold of the decay function is the same as that of the classical ε-greedy algorithm. In these experiments, the values for the learning rate, the discount factor, the Exploring rate, the Annealing factor, the Maximum temperature parameter, and the Minimum temperature parameter are given manually and empirically.</p>
<p>RL Model for a Confrontation Decision-Making System</p>
<p>Robocode [32,33] is an open-source platform, where the goal is to develop a robot to battle against other robots. The platform is shown in Figure 7. In this paper, the experiments are conducted in this platform and we consider several scenarios with the different enemies to test the generalization of the proposed method. </p>
<p>Experiment and Analysis</p>
<p>Generally, in the robotic systems that are based on reinforcement learning, higher learning rates enable robots to utilize the previous learning experience. The larger discount rate makes learning agents think more about long-term returns in the future. For the exploration and exploitation, the εgreedy algorithm with a larger threshold allows the learning agent to more utilize prior experience. In this work, the initial value for the threshold of the decay function is the same as that of the classical ε-greedy algorithm. In these experiments, the values for the learning rate, the discount factor, the Exploring rate, the Annealing factor, the Maximum temperature parameter, and the Minimum temperature parameter are given manually and empirically.</p>
<p>RL Model for a Confrontation Decision-Making System</p>
<p>Robocode [32,33] is an open-source platform, where the goal is to develop a robot to battle against other robots. The platform is shown in Figure 7. In this paper, the experiments are conducted in this platform and we consider several scenarios with the different enemies to test the generalization of the proposed method. Effective state-action space definition of the RL model is still an open problem with no universal solution. An RL model for robot confrontation with inputs from the game engine is constructed, which ensures the size of the state-action space remaining unchanged to avoid the curse of dimensionality.</p>
<p>State-space: The combination of the relative direction angle, the absolute orientation angle, and the distance between the robots form the state space. The absolute direction angle and the relative solution. An RL model for robot confrontation with inputs from the game engine is constructed, which ensures the size of the state-action space remaining unchanged to avoid the curse of dimensionality.</p>
<p>State-space: The combination of the relative direction angle, the absolute orientation angle, and the distance between the robots form the state space. The absolute direction angle and the relative direction angle are discretized into four kinds of states and the range of absolute direction angle is 0 ∼ 360 • . We divide the distance between the robots into 20 discrete parts.</p>
<p>Action space: Movement and rotation are two movements for the robot in this platform. At each time step, each robot can move to arbitrary directions with arbitrary distances in the ground. Similar to other types of combat games, our robot can choose to attack their opponents with bullets of different energies. Forward, backward, clockwise rotation and anticlockwise rotation four kinds of different movements form the action space.</p>
<p>Reward function: If a robot fires bullets hit the enemy or is hit by bullets, the health point of this robot will change. We propose a reward function to help agent balance losses of our and opponents, as shown in:
reward(t) = ((E m t − E m t−1 ) − (E e t − E e t−1 )) * E e t−1 −E e t E m t −E m t−1(35)
In this reward function, E m t represents the health point of our agent at t time, E m t−1 represents the health point of our agent at the t−1 time, E e t represents the health point of the opponent at the t time and E e t−1 represents the health point of the opponent at the t−1 time. If we lose fewer health points than the opponent loses, we will get a positive reward. The ratio of absolute health point changes between the two sides will encourage our agent to hurt the opponent more in battle. According to the experimental results, the RL model is effective at controlling our agent in the micromanagement scenarios.</p>
<p>Proposed RL Algorithm Test</p>
<p>In order to verify the effectiveness of the proposed SSAQ algorithm, a comparative experiment of robot migration is designed. The map for robot migration satisfies the binary tree structure. As shown in Figure 8, there are N layers on the map, and the number of endpoints is 2 N − 1. t time and 1 e t E − represents the health point of the opponent at the t−1 time. If we lose fewer health points than the opponent loses, we will get a positive reward. The ratio of absolute health point changes between the two sides will encourage our agent to hurt the opponent more in battle. According to the experimental results, the RL model is effective at controlling our agent in the micromanagement scenarios.</p>
<p>Proposed RL Algorithm Test</p>
<p>In order to verify the effectiveness of the proposed SSAQ algorithm, a comparative experiment of robot migration is designed. The map for robot migration satisfies the binary tree structure. As shown in Figure 8, there are N layers on the map, and the number of endpoints is 2 1 This paper uses the SSAQ method to compare with the ε-greedy policy (Greedy-policy) and ε-greedy policy using the decay threshold (Dgreedy-policy) [34]. The settings of parameters for this experiment are listed in Table 1.
N − .… … … … … … … … … begining position desired position … … … … … … … … …
Reward +1000 The robot can get the reward corresponding to each endpoint by starting from the beginning and repeating the branch to the bottom endpoint. A total of 2 N − 1 actions can be selected by the robot. The corresponding Q values of each endpoint are expressed as Q 1 ∼ Q 2 N −1 . When the robot reaches the endpoint 2 N − 1, it gets a reward of +1000, and it does not get a reward reaching the rest of the endpoints. Q values can be obtained through experiments. This paper uses the SSAQ method to compare with the ε-greedy policy (Greedy-policy) and ε-greedy policy using the decay threshold (Dgreedy-policy) [34]. The settings of parameters for this experiment are listed in Table 1. </p>
<p>Parameter Value</p>
<p>Learning rate α 0.3 Discount rate γ 0.9 Exploring rate ε 0.9 Annealing factor η 0.9 Maximum temperature parameter T max 0.1 Minimum temperature parameter T min 0.01 Layers of the map N 10</p>
<p>Through the observation during the experiments, it shows that Q 3 , Q 7 and Q 15 will change in the experiment, and we record these Q values. Therefore, the changes in three kinds of Q values of Q 3 , Q 7 and Q 15 are used in the experiment to compare three different strategies. As shown in Figure 9, the convergence time of the proposed method (SSAQ algorithm) is 214, 168 and 122 for Q 3 , Q 7 and Q 15 , respectively. The convergence time of the proposed method is the shortest and the convergence rate of it is the fastest compared with the other two methods. In addition, the Q-value curve of the proposed strategy is relatively smooth, which also proves that the proposed strategy is more stable and can achieve the target state quickly while ensuring the stability of learning performance. 7 15 the convergence time of the proposed method (SSAQ algorithm) is 214, 168 and 122 for 3 Q , 7 Q and 15 Q , respectively. The convergence time of the proposed method is the shortest and the convergence rate of it is the fastest compared with the other two methods. In addition, the Q-value curve of the proposed strategy is relatively smooth, which also proves that the proposed strategy is more stable and can achieve the target state quickly while ensuring the stability of learning performance. </p>
<p>Effect Test for Multi-Agent RL Based on DMNN and Fuzzy Method</p>
<p>The convergence and stability of the neural network are important factors affecting the performance of micromanagement for an agent. Hence, this paper proposes a decision-making neural network with adaptive momentum. In order to verify the effectiveness of the proposed Multi-agent RL algorithm based on DMNN, a team of agents trained by the neural network with adaptive momentum (NN with AWM) and another team of tank agents trained by the neural network without </p>
<p>Effect Test for Multi-Agent RL Based on DMNN and Fuzzy Method</p>
<p>The convergence and stability of the neural network are important factors affecting the performance of micromanagement for an agent. Hence, this paper proposes a decision-making neural network with adaptive momentum. In order to verify the effectiveness of the proposed Multi-agent RL algorithm based on DMNN, a team of agents trained by the neural network with adaptive momentum (NN with AWM) and another team of tank agents trained by the neural network without adaptive momentum (NN without AWM) are used to fight the team of tank agents trained by TD method separately. Each team has four tank agents. The settings of parameters for this experiment are listed in Table 2. We compare "NN with AWM" and "NN without AWM" from three different perspectives: total score ratio, defensive score and fluctuation value of score ratio. The number of rounds is 500, and the score is recorded every 10 rounds. Every 10 rounds are called a session. Figures 10-12 represent the resulting curve of this experiment, in which the horizontal axis represents the number of sessions and the vertical axis represents the score ratio or fluctuation value or defensive score. This paper calculates the first-order difference of the total score ratio, that is, Score ratio t+1 − Score ratio t , and obtains the fluctuation value curve of the total score ratio, as shown in Figure 11. From the experimental results, it can be seen that these score ratios of the tank agents trained by the NN with the AWM method are obviously higher than the NN without AWM method, and the fluctuation value is relatively small. Figure 12 shows the curve of the defensive score. Similar to Figure 10, the defensive score of the NN with the AWM method is more stable and higher than the NN without the AWM method. The experimental results show that the proposed Multi-agent RL algorithm can not only make the tank agent get higher scores in combat, but also get more stable scores. Hence, we can see that the proposed method has outstanding performances for micromanagement. score ratio, defensive score and fluctuation value of score ratio. The number of rounds is 500, and the score is recorded every 10 rounds. Every 10 rounds are called a session. Figures 10-12 represent the resulting curve of this experiment, in which the horizontal axis represents the number of sessions and the vertical axis represents the score ratio or fluctuation value or defensive score. This paper calculates the first-order difference of the total score ratio, that is, 1 S S t t core ratio core ratio + − , and obtains the fluctuation value curve of the total score ratio, as shown in Figure 11. From the experimental results, it can be seen that these score ratios of the tank agents trained by the NN with the AWM method are obviously higher than the NN without AWM method, and the fluctuation value is relatively small. Figure 12 shows the curve of the defensive score. Similar to Figure 10, the defensive score of the NN with the AWM method is more stable and higher than the NN without the AWM method. The experimental results show that the proposed Multi-agent RL algorithm can not only make the tank agent get higher scores in combat, but also get more stable scores. Hence, we can see that the proposed method has outstanding performances for micromanagement. Figure 10. The comparison curve of the total score ratio. Figure 10. The comparison curve of the total score ratio. The number of discrete points is 7. According to the fuzzy method used in this paper, the results of the fuzzy system are shown in Figure 13. As shown in Figure 13, the fuzzy method converts the linear input into the smooth output, which is appropriate for micromanagement in the Multi-Robot Confrontation system. Then, the experimental results of the fuzzy-inspired learning model (FLM), learning model (LM) with learning rate being 0.2 (LM-0.2) [12] and learning model with learning rate The number of discrete points is 7. According to the fuzzy method used in this paper, the results of the fuzzy system are shown in Figure 13. As shown in Figure 13, the fuzzy method converts the linear input into the smooth output, which is appropriate for micromanagement in the Multi-Robot Confrontation system. Then, the experimental results of the fuzzy-inspired learning model (FLM), learning model (LM) with learning rate being 0.2 (LM-0.2) [12] and learning model with learning rate According to the fuzzy method used in this paper, the results of the fuzzy system are shown in Figure 13. As shown in Figure 13, the fuzzy method converts the linear input into the smooth output, which is appropriate for micromanagement in the Multi-Robot Confrontation system. Then, the experimental results of the fuzzy-inspired learning model (FLM), learning model (LM) with learning rate being 0.2 (LM-0.2) [12] and learning model with learning rate being 0.95 (LM-0.95) are tested in the Robocode platform, as shown in Figure 14. The number of discrete points is 7. According to the fuzzy method used in this paper, the results of the fuzzy system are shown in Figure 13. As shown in Figure 13, the fuzzy method converts the linear input into the smooth output, which is appropriate for micromanagement in the Multi-Robot Confrontation system. Then, the experimental results of the fuzzy-inspired learning model (FLM), learning model (LM) with learning rate being 0.2 (LM-0.2) [12] and learning model with learning rate being 0.95 (LM-0.95) are tested in the Robocode platform, as shown in Figure 14.  The three methods of FLM, LM-0.2, and LM-0.95 were respectively fought with the TD method. The solid line represents the score of three methods respectively, while the dotted lines represent the scores of TD methods fighting against three methods. From Figure 14, in FLM, the score ratio remains at around 0.8. In LM-0.2 and LM-0.95, the score ratio remains at around 0.7. According to the above results, it can be obtained that the fuzzy method can get a higher score in combat than the method of the fixed learning rate for micromanagement.</p>
<p>Effect Test for Curriculum Transfer Learning (TF)</p>
<p>Six tank agents trained by the TD method are formed, which is called "TD_team", and the multiple robots in formation will not attack each other. Then, the learning model proposed in this paper is used to fight the TD_team. Compared to the above experiments, our tank agents will face six opponents at the same time, as shown in Figure 15. In Figure 15, The agent in the red box is the enemy team, while the agent in the yellow box is our team. The three methods of FLM, LM-0.2, and LM-0.95 were respectively fought with the TD method. The solid line represents the score of three methods respectively, while the dotted lines represent the scores of TD methods fighting against three methods. From Figure 14, in FLM, the score ratio remains at around 0.8. In LM-0.2 and LM-0.95, the score ratio remains at around 0.7. According to the above results, it can be obtained that the fuzzy method can get a higher score in combat than the method of the fixed learning rate for micromanagement.</p>
<p>Effect Test for Curriculum Transfer Learning (TF)</p>
<p>Six tank agents trained by the TD method are formed, which is called "TD_team", and the multiple robots in formation will not attack each other. Then, the learning model proposed in this paper is used to fight the TD_team. Compared to the above experiments, our tank agents will face six opponents at the same time, as shown in Figure 15. In Figure 15, The agent in the red box is the enemy team, while the agent in the yellow box is our team.</p>
<p>Effect Test for Curriculum Transfer Learning (TF)</p>
<p>Six tank agents trained by the TD method are formed, which is called "TD_team", and the multiple robots in formation will not attack each other. Then, the learning model proposed in this paper is used to fight the TD_team. Compared to the above experiments, our tank agents will face six opponents at the same time, as shown in Figure 15. In Figure 15, The agent in the red box is the enemy team, while the agent in the yellow box is our team. In order to prove the efficiency and practicality of the learning model with curriculum transfer learning (LM with TF), it is compared with the learning model without curriculum transfer learning (LM without TF) for 500 rounds.</p>
<p>The tank agent using curriculum transfer learning will use the prior experience gained in the above experiments. From Figure 16, transfer learning accelerates the learning speed of tank agents in the new micromanagement scenario and achieves higher scores than the method without transfer learning. From Figure 17, it is obvious that the defensive score of the LM with the TF method is higher, which means that the agent can defend the opponent's attack well when attacking opponents. Ten tank agents trained by the TD method were formed, and then the LM with the TF method and the In order to prove the efficiency and practicality of the learning model with curriculum transfer learning (LM with TF), it is compared with the learning model without curriculum transfer learning (LM without TF) for 500 rounds.</p>
<p>The tank agent using curriculum transfer learning will use the prior experience gained in the above experiments. From Figure 16, transfer learning accelerates the learning speed of tank agents in the new micromanagement scenario and achieves higher scores than the method without transfer learning. From Figure 17, it is obvious that the defensive score of the LM with the TF method is higher, which means that the agent can defend the opponent's attack well when attacking opponents. Ten tank agents trained by the TD method were formed, and then the LM with the TF method and the LM without TF method were used to fight against them respectively. The task of fighting TD_team is regarded as the intermediate task of this task because it is more difficult than the original task. From Figure 18, the score ratio of the LM with the TF method is still higher than that of the LM without the TF method, which means the learning model with curriculum transfer learning has a strong and stable property for micromanagement scenarios in confrontation decision-making system. LM without TF method were used to fight against them respectively. The task of fighting TD_team is regarded as the intermediate task of this task because it is more difficult than the original task. From Figure 18, the score ratio of the LM with the TF method is still higher than that of the LM without the TF method, which means the learning model with curriculum transfer learning has a strong and stable property for micromanagement scenarios in confrontation decision-making system.    </p>
<p>Conclusions</p>
<p>In this paper, the confrontation decision-making for micromanagement scenarios in SMDP is studied. This paper makes several contributions, including an improved Q-learning in SMDP (SSAQ algorithm) for confrontation decision-making, the fuzzy method for adjusting learning rate of the RL   </p>
<p>Conclusions</p>
<p>In this paper, the confrontation decision-making for micromanagement scenarios in SMDP is studied. This paper makes several contributions, including an improved Q-learning in SMDP (SSAQ algorithm) for confrontation decision-making, the fuzzy method for adjusting learning rate of the RL Figure 18. Comparisons of learning models in score ratio.</p>
<p>Conclusions</p>
<p>In this paper, the confrontation decision-making for micromanagement scenarios in SMDP is studied. This paper makes several contributions, including an improved Q-learning in SMDP (SSAQ algorithm) for confrontation decision-making, the fuzzy method for adjusting learning rate of the RL method, Multi-agent RL algorithm with parameter sharing, accelerated neural network for the representation of state (DMNN) and a curriculum transfer learning method with decay function. The RL model designed ensures the size of the state-action space remaining unchanged to avoid the curse of dimensionality and the reward function to help agent balance losses of our agents and opponents. The decision-making neural network uses adaptive momentum, which is used as an approximator to estimate the state-action function. The accelerated algorithm using adaptive momentum allows the decision-making neural network to react quickly in micromanagement scenarios. Finally, the curriculum transfer learning method with the decay function extends our model to other different scenarios. The proposed transfer learning method can still achieve better experimental results in more complex confrontation scenarios. The results of experiments demonstrate proposed methods can achieve an excellent and stable control for micromanagement in robot confrontation system. In the future, we will extend this model to the game scenario of Starcraft II. In addition, the behavior decomposition methods [35][36][37][38] will be studied to achieve more advanced Multi-agent confrontation strategies. </p>
<p>Figure 1 .
1Reinforcement learning architecture.</p>
<p>reinforcement learning algorithm that uses the future P-step average rewards, the mathematical expression for the value function</p>
<p>agent takes t a a strategy π , the value function represents the expectation of the cumulative reward obtained by the agent. agent which is in the state s is given by: is a model-free reinforcement learning algorithm, and it is an off-policy reinforcement learning algorithm. The Q value ( , ) Q s a is estimated via the Time Difference Method (TD Method)[21]:</p>
<dl>
<dt>PFigure 1 .</dt>
<dt>1represents the probability of choosing action i a , and the total number of actions is K .The action selection policy based on Boltzmann distribution is used to ensure the randomness of the Reinforcement learning architecture.</dt>
<dd>
<p>= Current temperature parameters T t+1 : = Next temperature parameters r t : = Current reward T min : = Minimum temperature parameter T max : = Maximum temperature parameter α: = Learning rate γ: = Discount factor K t : = The number of actions in t time f un(): = Updating state-action function in SMDP Initialization</p>
</dd>
</dl>
<p>Figure 2 ,
2{r i |i = 1, 2, . . . , 6} and α F i i = 1, 2, . . . , 5 are the division points that correspond to the input and output membership functions respectively. Five different input descriptions, "LP, TP, ZO, TN, LN" are µ r i (r) i = 1, . . . , 5 , which correspond to each of these membership functions. Similarly, five fuzzy output descriptions, "VL, LL, M, LS, VS", correspond to the output membership α) i = 1, . . . , 5 .</p>
<p>Figure 2 .
2Curves for the Membership functions. (a) Reward/Input membership function. division points that correspond to the input and output membership functions respectively. Five different input descriptions, "LP, TP, ZO, TN, LN" are { ( ) | 1,...correspond to each of these membership functions. Similarly, five fuzzy output descriptions, "VL, LL, M, LS, VS", correspond to the output membership functions, expressed as { ( ) | 1,...</p>
<p>Figure 2 .
2Curves for the Membership functions. (a) Reward/Input membership function. (b) Learning rate/Output membership function.</p>
<p>Figure 3 .
3The learning model of agent architecture. The decision-making neural network is used as the approximator for the state-action function.</p>
<p>Figure 4 .
4Decision-making neural network architecture.where θ is the weight vector of the decision-making neural network. The Q values of all actions are entered into the softmax layer with a non-linear transformation function which is shown in Equation(30), and the action t a is selected and outputted using the Max operation as shown in Equation(31).</p>
<p>Figure 3 .
3The learning model of agent architecture. The decision-making neural network is used as the approximator for the state-action function.</p>
<p>Figure 3 .
3The learning model of agent architecture. The decision-making neural network is used as the approximator for the state-action function.</p>
<p>Figure 4 .
4Decision-making neural network architecture.</p>
<p>Figure 4 .
4Decision-making neural network architecture.</p>
<p>Algorithm 2 :
2Multi-agent SSAQ algorithm Definition T t : = Current temperature parameters T t+1 : = Next temperature parameters T min : = Minimum temperature parameter T max : = Maximum temperature parameter α t : = Adaptive learning rate of the neural network γ: = Discount factor K t : = The number of actions in t time f un(): = Updating state-action function in SMDP InitializationInitialize s t , a t , s t+1 , a t+1 , r t Repeat (for each step)Choose an initial state s 0 t ← 0 ; i ← 0 ; Repeat (for each step of the episode) P(a t |s t ) = max</p>
<p>Figure 5 .
5Curriculum transfer learning with an intermediate task and decay function.</p>
<p>Figure 5 .
5Curriculum transfer learning with an intermediate task and decay function.</p>
<p>Figure 6 .
6Framework for the proposed method for micromanagement.</p>
<p>Figure 6 .
6Framework for the proposed method for micromanagement.</p>
<p>Figure 6 .
6Framework for the proposed method for micromanagement.</p>
<p>Figure 7 .
7Robot confrontation platform. (a) a platform for the tank battle system, (b) diagram of absolute and relative angles.</p>
<p>Figure 7 .
7Robot confrontation platform. (a) a platform for the tank battle system, (b) diagram of absolute and relative angles. Effective state-action space definition of the RL model is still an open problem with no universal</p>
<p>Figure 8 .
8The experiment for robot migration. The robot can get the reward corresponding to each endpoint by starting from the beginning and repeating the branch to the bottom endpoint. A total of 2 1 N − actions can be selected by the robot. The corresponding Q values of each endpoint are expressed as − . When the robot reaches the endpoint 2 1 N − , it gets a reward of +1000, and it does not get a reward reaching the rest of the endpoints. Q values can be obtained through experiments.</p>
<p>Figure 8 .
8The experiment for robot migration.</p>
<p>Figure 9 .
9The comparison curve of Q value: (a) The change curve of 3 Q ; (b) The change curve of 7 Q ; (c) The change curve of 15 Q .</p>
<p>Figure 9 .
9The comparison curve of Q value: (a) The change curve of Q 3 ; (b) The change curve of Q 7 ; (c) The change curve of Q 15 .</p>
<p>Figure 11 .Figure 12 .
1112The curve of the fluctuation value of the total score ratio. The curve of the Defensive score. (a) The defensive score of neural network (NN) without adaptive momentum (AWM); (b) The defensive score of NN with AWM. A fuzzy method is used to tuning the learning rate for the RL model. The input domain of the fuzzy method is set as { } { } | 1, 2,..., 7 30, 20, 10, 0,10, 20,30 i r i = = − − − , and the output domain of the fuzzy method is set as { } | 1, 2,..., 7 {0.95, 0.80, 0.65, 0.50, 0.35, 0.20, 0.05} i i α = = .</p>
<p>Figure 11 . 22 Figure 11 .Figure 12 .
11221112The curve of the fluctuation value of the total score ratio.Information 2019, 11, 341 18 of The curve of the fluctuation value of the total score ratio. The curve of the Defensive score. (a) The defensive score of neural network (NN) without adaptive momentum (AWM); (b) The defensive score of NN with AWM. A fuzzy method is used to tuning the learning rate for the RL model. The input domain of the fuzzy method is set as { } { } | 1, 2,..., 7 30, 20, 10, 0,10, 20,30 i r i = = − − − , and the output domain of the fuzzy method is set as { } | 1, 2,..., 7 {0.95, 0.80, 0.65, 0.50, 0.35, 0.20, 0.05} i i α = = .</p>
<p>Figure 12 .
12The curve of the Defensive score. (a) The defensive score of neural network (NN) without adaptive momentum (AWM); (b) The defensive score of NN with AWM. A fuzzy method is used to tuning the learning rate for the RL model. The input domain of the fuzzy method is set as {r i |i = 1, 2, . . . , 7} = {−30, −20, −10, 0, 10, 20, 30}, and the output domain of the fuzzy method is set as {α i |i = 1, 2, . . . , 7} = {0.95, 0.80, 0.65, 0.50, 0.35, 0.20, 0.05}. The number of discrete points is 7.</p>
<p>Figure 13 .
13Results of the fuzzy method.</p>
<p>Figure 13 . 22 Figure 14 .
132214Results of the fuzzy method. Information 2019, 11, 341 19 of Comparison of learning results for different learning rates.</p>
<p>Figure 14 .
14Comparison of learning results for different learning rates.</p>
<p>Figure 15 .
15Simulation experiments for the TF method.</p>
<p>Figure 15 .
15Simulation experiments for the TF method.</p>
<p>Figure 16 .Figure 17 .
1617Comparisons of learning models in score ratio. One model uses transfer learning, the other does not use transfer learning. Comparisons of learning models in the defensive score. (a) The defensive score for LM with</p>
<p>Figure 16 .
16Comparisons of learning models in score ratio. One model uses transfer learning, the other does not use transfer learning.</p>
<p>Figure 16 .Figure 17 .
1617Comparisons Comparisons of learning models in the defensive score. (a) The defensive score for LM with TF and TD_team; (b) Defensive score for LM without TF and TD_team.</p>
<p>Figure 18 .
18Comparisons of learning models in score ratio.</p>
<p>Figure 17 .
17Comparisons of learning models in the defensive score. (a) The defensive score for LM with TF and TD_team; (b) Defensive score for LM without TF and TD_team.</p>
<p>Figure 16 .Figure 17 .
1617Comparisons Comparisons of learning models in the defensive score. (a) The defensive score for LM with TF and TD_team; (b) Defensive score for LM without TF and TD_team.</p>
<p>Figure 18 .
18Comparisons of learning models in score ratio.</p>
<p>Author
Contributions: C.H. and M.X. conceived the idea of the paper. C.H. and M.X. designed and performed the experiments; C.H. and M.X. analyzed the data; C.H. contributed reagents/materials/analysis tools; C.H. wrote and revised the paper.</p>
<p>Table 1 .
1Experimental parameters.</p>
<p>Table 2 .
2Experimental parameters.Parameter 
Value </p>
<p>Learning rate of RL α 
0.1 
Discount rate γ 
0.9 
Exploring rate ε 
0.95 
Annealing factor η 
0.9 
Maximum temperature parameter T max 
0.1 
Minimum temperature parameter T min 
0.01 
Learning rate of Neural network α 
0.9 
Momentum constant β 
0.95 </p>
<p>© 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).
Conflicts of Interest:The authors declare no conflict of interest.
Electricity price forecasting: A review of the state-of-the-art with a look into the future. R Weron, 10.1016/j.ijforecast.2014.08.008Int. J. Forecast. 4Weron, R. Electricity price forecasting: A review of the state-of-the-art with a look into the future. Int. J. Forecast. 2014, 4, 1030-1081. [CrossRef]</p>
<p>Generator of Feasible and Engaging Levels for Angry Birds. L N Ferreira, C Toledo, A Tanager, 10.1109/TCIAIG.2017.2766218IEEE Trans. Games. 10Ferreira, L.N.; Toledo, C.; Tanager, A. Generator of Feasible and Engaging Levels for Angry Birds. IEEE Trans. Games 2017, 10, 304-316. [CrossRef]</p>
<p>A computer simulation platform for the estimation of measurement uncertainties in dimensional X-ray computed tomography. J Hiller, L M Reindl, 10.1016/j.measurement.2012.05.030Measurement. 45Hiller, J.; Reindl, L.M. A computer simulation platform for the estimation of measurement uncertainties in dimensional X-ray computed tomography. Measurement 2012, 45, 2166-2182. [CrossRef]</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, 10.1038/nature14236Nature. 518PubMedMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.; Ostrovski, G. Human-level control through deep reinforcement learning. Nature 2015, 518, 529-533. [CrossRef] [PubMed]</p>
<p>Expert level artificial intelligence in heads-up no-limit poker. M Moravik, M Schmid, N Burch, V Lisy, D Morrill, N Bard, T Davis, K Waugh, M Johanson, M Bowling, Deepstack, 10.1126/science.aam6960356Moravik, M.; Schmid, M.; Burch, N.; Lisy, V.; Morrill, D.; Bard, N.; Davis, T.; Waugh, K.; Johanson, M.; Bowling, M. Deepstack: Expert level artificial intelligence in heads-up no-limit poker. Science 2017, 356, 508-513. [CrossRef]</p>
<p>Distributed Static and Dynamic Circumnavigation Control with Arbitrary Spacings for a Heterogeneous Multi-robot System. W Yao, H Lu, Z Zeng, J Xiao, Z Zheng, 10.1007/s10846-018-0906-5J. Intell. Robot. Syst. 4Yao, W.; Lu, H.; Zeng, Z.; Xiao, J.; Zheng, Z. Distributed Static and Dynamic Circumnavigation Control with Arbitrary Spacings for a Heterogeneous Multi-robot System. J. Intell. Robot. Syst. 2018, 4, 1-23. [CrossRef]</p>
<p>Survey of Real-Time Strategy Game AI Research and Competition in StarCraft. S Ontanon, G Synnaeve, A Uriarte, F Richoux, D Churchill, M A Preuss, 10.1109/TCIAIG.2013.2286295IEEE Trans. Comput. Intell. AI Games. 5Ontanon, S.; Synnaeve, G.; Uriarte, A.; Richoux, F.; Churchill, D.; Preuss, M.A. Survey of Real-Time Strategy Game AI Research and Competition in StarCraft. IEEE Trans. Comput. Intell. AI Games 2013, 5, 293-311. [CrossRef]</p>
<p>Multiscale Bayesian Modeling for RTS Games: An Application to StarCraft AI. G Synnaeve, P Bessiere, 10.1109/TCIAIG.2015.2487743IEEE Trans. Comput. Intell. AI Games. 8Synnaeve, G.; Bessiere, P. Multiscale Bayesian Modeling for RTS Games: An Application to StarCraft AI. IEEE Trans. Comput. Intell. AI Games 2016, 8, 338-350. [CrossRef]</p>
<p>Reinforcement Learning: An Introduction. S Thrun, M L Littman, IEEE Trans. Neural Netw. 16Thrun, S.; Littman, M.L. Reinforcement Learning: An Introduction. IEEE Trans. Neural Netw. 2005, 16, 285-286.</p>
<p>Decoupled Visual Servoing With Fuzzy Q-Learning. H Shi, X Li, K S Hwang, W Pan, G Xu, 10.1109/TII.2016.2617464IEEE Trans. Ind. Inform. 14Shi, H.; Li, X.; Hwang, K.S.; Pan, W.; Xu, G. Decoupled Visual Servoing With Fuzzy Q-Learning. IEEE Trans. Ind. Inform. 2018, 14, 241-252. [CrossRef]</p>
<p>An adaptive Decision-making Method with Fuzzy Bayesian Reinforcement Learning for Robot Soccer. H Shi, Z Lin, S Zhang, X Li, K S Hwang, 10.1016/j.ins.2018.01.032Inform. Sci. 436Shi, H.; Lin, Z.; Zhang, S.; Li, X.; Hwang, K.S. An adaptive Decision-making Method with Fuzzy Bayesian Reinforcement Learning for Robot Soccer. Inform. Sci. 2018, 436, 268-281. [CrossRef]</p>
<p>Play games using Reinforcement Learning and Artificial Neural Networks with Experience Replay. M Xu, H Shi, Y Wang, Proceedings of the 2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS). the 2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)Singapore, 6-8Xu, M.; Shi, H.; Wang, Y. Play games using Reinforcement Learning and Artificial Neural Networks with Experience Replay. In Proceedings of the 2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS), Singapore, 6-8 June 2018; pp. 855-859.</p>
<p>Modeling and forecasting of electricity spot-prices: Computational intelligence vs. classical econometrics. S Cincotti, G Gallo, L Ponta, M Raberto, AI Commun. 3Cincotti, S.; Gallo, G.; Ponta, L.; Raberto, M. Modeling and forecasting of electricity spot-prices: Computational intelligence vs. classical econometrics. AI Commun. 2014, 3, 301-314.</p>
<p>A value-function-based reinforcement learning framework for education and research. A Geramifard, C Dann, R H Klein, W Dabney, J P How, Rlpy, J. Mach. Learn. Res. 16Geramifard, A.; Dann, C.; Klein, R.H.; Dabney, W.; How, J.P. RLPy. A value-function-based reinforcement learning framework for education and research. J. Mach. Learn. Res. 2015, 16, 1573-1578.</p>
<p>Optimal Output-Feedback Control of Unknown Continuous-Time Linear Systems Using Off-policy Reinforcement Learning. H Modares, F L Lewis, Z P Jiang, 10.1109/TCYB.2015.2477810IEEE Trans. Cybern. 46PubMedModares, H.; Lewis, F.L.; Jiang, Z.P. Optimal Output-Feedback Control of Unknown Continuous-Time Linear Systems Using Off-policy Reinforcement Learning. IEEE Trans. Cybern. 2016, 46, 2401-2410. [CrossRef] [PubMed]</p>
<p>Actor-critic algorithms. V Konda, 10.1137/S0363012901385691Siam J. Control Optim. 42Konda, V. Actor-critic algorithms. Siam J. Control Optim. 2003, 42, 1143-1166. [CrossRef]</p>
<p>Tuning computer gaming agents using Q-learning. P G Patel, N Carver, S Rahimi, Proceedings of the Computer Science and Information Systems (FedCSIS). the Computer Science and Information Systems (FedCSIS)Szczecin, PolandPatel, P.G.; Carver, N.; Rahimi, S. Tuning computer gaming agents using Q-learning. In Proceedings of the Computer Science and Information Systems (FedCSIS), Szczecin, Poland, 18-21 September 2011; pp. 581-588.</p>
<p>StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning. K Shao, Y Zhu, D Zhao, 10.1109/TETCI.2018.2823329IEEE Trans. Emerg. Top. Comput. Intell. 3Shao, K.; Zhu, Y.; Zhao, D. StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning. IEEE Trans. Emerg. Top. Comput. Intell. 2018, 3, 73-84. [CrossRef]</p>
<p>New adaptive momentum algorithm for split-complex recurrent neural networks. D Xu, H Shao, H A Zhang, 10.1016/j.neucom.2012.03.013Neurocomputing. 93Xu, D.; Shao, H.; Zhang, H.A. New adaptive momentum algorithm for split-complex recurrent neural networks. Neurocomputing 2012, 93, 133-136. [CrossRef]</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Nature. 521Lecun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015, 521, 436. [CrossRef]</p>
<p>Multi-agent bidirectionally-coordinated nets for learning to play StarCraft combat games. P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, J Wang, arXiv:1703.10069Peng, P.; Yuan, Q.; Wen, Y.; Yang, Y.; Tang, Z.; Long, H.; Wang, J. Multi-agent bidirectionally-coordinated nets for learning to play StarCraft combat games. arXiv 2017, arXiv:1703.10069.</p>
<p>A Survey on Transfer Learning. S J Pan, Q Yang, 10.1109/TKDE.2009.191IEEE Trans. Knowl. Data Eng. 22Pan, S.J.; Yang, Q. A Survey on Transfer Learning. IEEE Trans. Knowl. Data Eng. 2010, 22, 1345-1359. [CrossRef] Information 2019, 10, 341 22 of 22</p>
<p>Building a reputation-based bootstrapping mechanism for newcomers in collaborative alert systems. P Gil, M Rez, M Mez, A F Gómez, J. Comput. Syst. Sci. 80Gil, P.; Rez, M.; Mez, M.; Gómez, A.F.S. Building a reputation-based bootstrapping mechanism for newcomers in collaborative alert systems. J. Comput. Syst. Sci. 2014, 80, 571-590.</p>
<p>. D Bertsimas, Tsitsiklis, 10.1214/ss/1177011077J. Simulated Annealing. Stat. Sci. 8Bertsimas, D.; Tsitsiklis, J. Simulated Annealing. Stat. Sci. 1993, 8, 10-15. [CrossRef]</p>
<p>Aggregation Approach to Experiences Replay of Dyna-Q Learning. H Shi, S Yang, K Hwang, J Chen, M Hu, H Zhang, Sample, 10.1109/ACCESS.2018.2847048IEEE Access. 6Shi, H.; Yang, S.; Hwang, K.; Chen, J.; Hu, M.; Zhang, H. A Sample Aggregation Approach to Experiences Replay of Dyna-Q Learning. IEEE Access 2018, 6, 37173-37184. [CrossRef]</p>
<p>A Fuzzy Adaptive Approach to Decoupled Visual Servoing for a Wheeled Mobile Robot. H Shi, M Xu, K Hwang, 10.1109/TFUZZ.2019.2931219IEEE Trans. Fuzzy Syst. Shi, H.; Xu, M.; Hwang, K. A Fuzzy Adaptive Approach to Decoupled Visual Servoing for a Wheeled Mobile Robot. IEEE Trans. Fuzzy Syst. 2019. [CrossRef]</p>
<p>An Adaptive Strategy Selection Method with Reinforcement Learning for Robotic Soccer Games. H Shi, Z Lin, K S Hwang, S Yang, J Chen, 10.1109/ACCESS.2018.2808266IEEE Access. 6Shi, H.; Lin, Z.; Hwang, K.S.; Yang, S.; Chen, J. An Adaptive Strategy Selection Method with Reinforcement Learning for Robotic Soccer Games. IEEE Access 2018, 6, 8376-8386. [CrossRef]</p>
<p>Toward Self-Driving Bicycles Using State-of-the-Art Deep Reinforcement Learning Algorithms. S Y Choi, T Le, Q Nguyen, M A Layek, S Lee, T Chung, 10.3390/sym1102029011Choi, S.Y.; Le, T.; Nguyen, Q.; Layek, M.A.; Lee, S.; Chung, T. Toward Self-Driving Bicycles Using State-of-the-Art Deep Reinforcement Learning Algorithms. Symmetry 2019, 11, 290. [CrossRef]</p>
<p>A Reinforcement Learning System with Time Constraints Exploration Planning for Accelerating the Learning Rate. G Zhao, S Tatsumi, R Sun, Rtp-Q, IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 82Zhao, G.; Tatsumi, S.; Sun, R. RTP-Q: A Reinforcement Learning System with Time Constraints Exploration Planning for Accelerating the Learning Rate. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 1999, 82, 2266-2273.</p>
<p>A New Fuzzy Time Series Model Using Triangular and Trapezoidal Membership Functions. A I Basyigit, C Ulu, M Guzelkaya, Proceedings of the International Work-Conference On Time Series. the International Work-Conference On Time SeriesGranada, SpainBasyigit, A.I.; Ulu, C.; Guzelkaya, M. A New Fuzzy Time Series Model Using Triangular and Trapezoidal Membership Functions. In Proceedings of the International Work-Conference On Time Series, Granada, Spain, 25-27 June 2014; pp. 25-27.</p>
<p>Observer-based adaptive neural network control for nonlinear stochastic systems with time delay. Q Zhou, P Shi, S Xu, H Li, 10.1109/TNNLS.2012.2223824IEEE Trans. Neural Netw. Learn. Syst. 24Zhou, Q.; Shi, P.; Xu, S.; Li, H. Observer-based adaptive neural network control for nonlinear stochastic systems with time delay. IEEE Trans. Neural Netw. Learn. Syst. 2012, 24, 71-80. [CrossRef]</p>
<p>Evolving Robocode tanks for Evo Robocode. R Harper, 10.1007/s10710-014-9224-2Genet. Program. Evol. Mach. 15Harper, R. Evolving Robocode tanks for Evo Robocode. Genet. Program. Evol. Mach. 2014, 15, 403-431. [CrossRef]</p>
<p>Unified Behavior Framework for Reactive Robot Control. B G Woolley, G L Peterson, 10.1007/s10846-008-9299-1J. Intell. Robot. Syst. 55Woolley, B.G.; Peterson, G.L. Unified Behavior Framework for Reactive Robot Control. J. Intell. Robot. Syst. 2009, 55, 155-176. [CrossRef]</p>
<p>Finite-time Analysis of the Multiarmed Bandit Problem. P Auer, N Cesabianchi, P Fischer, 10.1023/A:1013689704352Mach. Learn. 47Auer, P.; Cesabianchi, N.; Fischer, P. Finite-time Analysis of the Multiarmed Bandit Problem. Mach. Learn. 2002, 47, 235-256. [CrossRef]</p>
<p>A Multiple Attribute Decision-Making Approach to Reinforcement Learning. H Shi, M Xu, 10.1109/TCDS.2019.2924724IEEE Trans. Cogn. Dev. Syst. Shi, H.; Xu, M. A Multiple Attribute Decision-Making Approach to Reinforcement Learning. IEEE Trans. Cogn. Dev. Syst. 2019. [CrossRef]</p>
<p>A Data Classification Method Using Genetic Algorithm and K-Means Algorithm with Optimizing Initial Cluster Center. H Shi, M Xu, Proceedings of the 2018 IEEE International Conference on Computer and Communication Engineering Technology (CCET). the 2018 IEEE International Conference on Computer and Communication Engineering Technology (CCET)Beijing, ChinaShi, H.; Xu, M. A Data Classification Method Using Genetic Algorithm and K-Means Algorithm with Optimizing Initial Cluster Center. In Proceedings of the 2018 IEEE International Conference on Computer and Communication Engineering Technology (CCET), Beijing, China, 18-20 August 2018; pp. 224-228.</p>
<p>Fuzzy Approach to Visual Servoing with A Bagging Method for Wheeled Mobile Robot. M Xu, H Shi, K Jiang, L Wang, X A Li, Proceedings of the 2019 IEEE International Conference on Mechatronics and Automation. the 2019 IEEE International Conference on Mechatronics and AutomationTianjin, ChinaXu, M.; Shi, H.; Jiang, K.; Wang, L.; Li, X.A. Fuzzy Approach to Visual Servoing with A Bagging Method for Wheeled Mobile Robot. In Proceedings of the 2019 IEEE International Conference on Mechatronics and Automation, Tianjin, China, 4-7 August 2019; pp. 444-449.</p>
<p>Behavior Fusion for Deep Reinforcement Learning. H Shi, M Xu, K Hwang, B Y Cai, 10.1016/j.isatra.2019.08.054ISA Trans. PubMedShi, H.; Xu, M.; Hwang, K.; Cai, B.Y. Behavior Fusion for Deep Reinforcement Learning. ISA Trans. 2019. [CrossRef] [PubMed]</p>            </div>
        </div>

    </div>
</body>
</html>
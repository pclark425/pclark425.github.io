<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-452 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-452</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-452</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-246864036</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2108.12383v2.pdf" target="_blank">A Guide to Computational Reproducibility in Signal Processing and Machine Learning</a></p>
                <p><strong>Paper Abstract:</strong> Computational reproducibility is a growing problem that has been extensively studied among computational researchers and within the signal processing and machine learning research community. However, with the changing landscape of signal processing and machine learning research come new obstacles and unseen challenges in creating reproducible experiments. Due to these new challenges most computational experiments have become difficult, if not impossible, to be reproduced by an independent researcher. In 2016 a survey conducted by the journal Nature found that 50% of researchers were unable to reproduce their own experiments. While the issue of computational reproducibility has been discussed in the literature and specifically within the signal processing community, it is still unclear to most researchers what are the best practices to ensure reproducibility without impinging on their primary responsibility of conducting research. We feel that although researchers understand the importance of making experiments reproducible, the lack of a clear set of standards and tools makes it difficult to incorporate good reproducibility practices in most labs. It is in this regard that we aim to present signal processing researchers with a set of practical tools and strategies that can help mitigate many of the obstacles to producing reproducible computational experiments.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e452.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e452.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNG / Random Seed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Number Generators and Random Seeds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper highlights use of random number generators in experiments (e.g., data shuffling, sampling for SGD) and stresses that failing to save/control random seeds produces run-to-run variability that prevents exact reproduction of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>general computational experiments involving randomized operations (data generation, sampling, training)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>random number generator state / unspecified random seeds, random sampling order for stochastic algorithms, nondeterministic library-level RNG behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Not saving or publishing RNG seeds leads to different sequences of random choices across runs and thus different outputs; library implementations and RNG defaults can differ across platforms/versions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Record and publish random seeds and RNG state as part of experimental metadata; include RNG-setting code in shared scripts; preserve software environment to avoid differing RNG implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uncontrolled randomness (unsaved RNG seeds) is identified as a major source of non-determinism; saving seeds is recommended to enable exact reproduction of a run.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e452.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGD stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Gradient Descent as a Source of Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites SGD as an example where random sampling of training examples changes optimization trajectories and final parameters, making repeated runs yield different results unless randomness is controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>training machine learning models using stochastic optimization</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>random mini-batch/sample order in SGD, initialization randomness, nondeterministic parallel reductions or BLAS/CUDA kernels</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Dependence of convergence rate and final parameter values on sample selection order and initial state; parallel hardware and library nondeterminism can further change outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Fix random seeds, document optimization hyperparameters and initialization, preserve software/hardware environment (containers or env specs); explicitly describe/sharing training order or use deterministic training modes where available.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stochastic training methods (SGD) inherently introduce run-to-run variability; controlling seeds and documenting optimizer settings are necessary to approach reproducibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e452.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter Tuning and Cross-Validation Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper emphasizes that hyperparameter settings and the procedures used to find them (including cross-validation splits) strongly affect results and that incomplete reporting of these details undermines reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>model selection and evaluation (hyperparameter search, cross-validation)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>different hyperparameter choices, different cross-validation splits or data partitioning, stochasticity in hyperparameter search algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Omitting exact hyperparameter values or the method for selecting them (seeded CV splits, search ranges, search algorithm) leads to different reported performance and can change claims of state-of-the-art.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Publish exact hyperparameter values and search protocol (including random seeds for CV splits), provide scripts that reproduce hyperparameter search and the splits, or fix data splits in shared artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hyperparameter and CV details are critical meta-data; careful reporting or sharing of selection procedures and seeds is required to reproduce reported performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e452.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dependency/environment drift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependency and Computational Environment Differences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies differences in software dependencies, their versions, and the computational environment (OS, libraries) as a frequent cause of irreproducible computational experiments and recommends environment capture tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>running codebases that depend on third-party libraries</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>different library/package versions, deprecated features, platform-specific behavior (OS, CPU/GPU drivers), driver and BLAS/CUDA differences</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Brittle codebases that only function on the original machine; later versions of libraries may break or change behavior so reproduced runs yield different outputs or errors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use dependency/environment management: conda environment.yml files, Docker containers to encapsulate environments, pin exact package versions, export and share environment specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Capturing and sharing the exact computational environment (via environment.yml or Docker) is essential to avoid variability introduced by dependency/version drift.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e452.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset/versioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset Versions, Sources, and Preprocessing Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper notes that different circulating versions of benchmark datasets and undocumented preprocessing steps can yield differing results and obstruct reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>data-driven experiments relying on benchmark or synthetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>different dataset releases/variants, missing description of data source, undocumented preprocessing or intermediate data transformations, randomness in synthetic data generation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Ambiguity about exact dataset source or preprocessing leads to unnoticed differences; synthetic data generation procedures often insufficiently documented.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Share raw data and all preprocessing scripts, document dataset source and exact versions, publish synthetic data-generation code or seeds, modularize preprocessing as shareable scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Precise dataset provenance and sharing of preprocessing code are necessary to prevent variability from dataset differences and to enable exact reproduction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e452.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Version control (git)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Version Control Systems (git)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper advocates using git (and hosting services like GitHub/GitLab) to track code changes, avoid multiple ad-hoc file copies, and enable collaborators to reproduce the exact code state tied to results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>development and sharing of reproducible codebases</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>multiple code versions and untracked ad-hoc copies leading to ambiguity about which code produced reported results</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Researchers keep many slightly different source files (e.g., my_algo.py, my_algo_1.py) making it impossible to know which produced published figures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Adopt git for commit history, branching, and SHA-1 commit identifiers so exact code states can be recovered; push repos to hosted services and tag commits corresponding to paper results.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proper use of version control prevents ambiguity about code versions and enables reproducible retrieval of the exact code used to produce published results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e452.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Documentation / Metadata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Documentation and Experimental Metadata (README, runtime, resources, env.yml)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper prescribes detailed README files and metadata (dependencies, computational resources, runtime, scripts order, environment.yml) as essential for reproducibility by others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>sharing experiments and enabling others to reproduce specific results or figures</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>lack of instruction about script execution order, missing computational requirements, omitted meta-data like hyperparameters, seeds, and runtime environment</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Code without clear instructions, missing env.yml or README entries leaves reproducer unsure how to run experiments or what resources are required.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Provide README with project description, install steps, resource and runtime info, preprocessing scripts, exact script order; include environment.yml and pin crucial package versions; assign DOI for permanence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Comprehensive documentation and explicit metadata (env files, computational requirements, script ordering) are practical, low-overhead steps that greatly increase the chance others can reproduce results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e452.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e452.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Encapsulation tools caveat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encapsulation Tools (Whole Tale, containerization) Limitations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper discusses encapsulation platforms (e.g., Whole Tale, Docker) as ways to capture experiments but warns they can treat experiments as black boxes, be inflexible, and risk obsolescence if not maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing and machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>packaging full computational workflows for reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>reliance on third-party reproducibility platforms that may change or be abandoned, black-box encapsulation hiding internal steps</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Encapsulation can impede interpretability and long-term maintenance; platform abandonment can make reproducible artifacts inaccessible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer widely used, robust open-source tools (git, conda, Docker where appropriate) and supplement encapsulation with readable modular code and documentation; assign DOIs to artifacts for permanence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encapsulation platforms can help but are not a panacea; combine environment capture with clear, modular code and documentation to maximize reproducibility and longevity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reproducible research in signal processing <em>(Rating: 2)</em></li>
                <li>Reproducible research: Best practices and potential misuse <em>(Rating: 2)</em></li>
                <li>Computing environments for reproducibility: Capturing the "Whole Tale" <em>(Rating: 1)</em></li>
                <li>An introduction to docker for reproducible research <em>(Rating: 1)</em></li>
                <li>Stochastic gradient descent tricks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-452",
    "paper_id": "paper-246864036",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "RNG / Random Seed",
            "name_full": "Random Number Generators and Random Seeds",
            "brief_description": "Paper highlights use of random number generators in experiments (e.g., data shuffling, sampling for SGD) and stresses that failing to save/control random seeds produces run-to-run variability that prevents exact reproduction of results.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "general computational experiments involving randomized operations (data generation, sampling, training)",
            "variability_sources": "random number generator state / unspecified random seeds, random sampling order for stochastic algorithms, nondeterministic library-level RNG behaviors",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Not saving or publishing RNG seeds leads to different sequences of random choices across runs and thus different outputs; library implementations and RNG defaults can differ across platforms/versions.",
            "mitigation_methods": "Record and publish random seeds and RNG state as part of experimental metadata; include RNG-setting code in shared scripts; preserve software environment to avoid differing RNG implementations.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Uncontrolled randomness (unsaved RNG seeds) is identified as a major source of non-determinism; saving seeds is recommended to enable exact reproduction of a run.",
            "uuid": "e452.0"
        },
        {
            "name_short": "SGD stochasticity",
            "name_full": "Stochastic Gradient Descent as a Source of Variability",
            "brief_description": "The paper cites SGD as an example where random sampling of training examples changes optimization trajectories and final parameters, making repeated runs yield different results unless randomness is controlled.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "training machine learning models using stochastic optimization",
            "variability_sources": "random mini-batch/sample order in SGD, initialization randomness, nondeterministic parallel reductions or BLAS/CUDA kernels",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Dependence of convergence rate and final parameter values on sample selection order and initial state; parallel hardware and library nondeterminism can further change outcomes.",
            "mitigation_methods": "Fix random seeds, document optimization hyperparameters and initialization, preserve software/hardware environment (containers or env specs); explicitly describe/sharing training order or use deterministic training modes where available.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Stochastic training methods (SGD) inherently introduce run-to-run variability; controlling seeds and documenting optimizer settings are necessary to approach reproducibility.",
            "uuid": "e452.1"
        },
        {
            "name_short": "Hyperparameter variability",
            "name_full": "Hyperparameter Tuning and Cross-Validation Variability",
            "brief_description": "The paper emphasizes that hyperparameter settings and the procedures used to find them (including cross-validation splits) strongly affect results and that incomplete reporting of these details undermines reproducibility.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "model selection and evaluation (hyperparameter search, cross-validation)",
            "variability_sources": "different hyperparameter choices, different cross-validation splits or data partitioning, stochasticity in hyperparameter search algorithms",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Omitting exact hyperparameter values or the method for selecting them (seeded CV splits, search ranges, search algorithm) leads to different reported performance and can change claims of state-of-the-art.",
            "mitigation_methods": "Publish exact hyperparameter values and search protocol (including random seeds for CV splits), provide scripts that reproduce hyperparameter search and the splits, or fix data splits in shared artifacts.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Hyperparameter and CV details are critical meta-data; careful reporting or sharing of selection procedures and seeds is required to reproduce reported performance.",
            "uuid": "e452.2"
        },
        {
            "name_short": "Dependency/environment drift",
            "name_full": "Dependency and Computational Environment Differences",
            "brief_description": "The paper identifies differences in software dependencies, their versions, and the computational environment (OS, libraries) as a frequent cause of irreproducible computational experiments and recommends environment capture tools.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "running codebases that depend on third-party libraries",
            "variability_sources": "different library/package versions, deprecated features, platform-specific behavior (OS, CPU/GPU drivers), driver and BLAS/CUDA differences",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Brittle codebases that only function on the original machine; later versions of libraries may break or change behavior so reproduced runs yield different outputs or errors.",
            "mitigation_methods": "Use dependency/environment management: conda environment.yml files, Docker containers to encapsulate environments, pin exact package versions, export and share environment specifications.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Capturing and sharing the exact computational environment (via environment.yml or Docker) is essential to avoid variability introduced by dependency/version drift.",
            "uuid": "e452.3"
        },
        {
            "name_short": "Dataset/versioning",
            "name_full": "Dataset Versions, Sources, and Preprocessing Variability",
            "brief_description": "Paper notes that different circulating versions of benchmark datasets and undocumented preprocessing steps can yield differing results and obstruct reproducibility.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "data-driven experiments relying on benchmark or synthetic datasets",
            "variability_sources": "different dataset releases/variants, missing description of data source, undocumented preprocessing or intermediate data transformations, randomness in synthetic data generation",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Ambiguity about exact dataset source or preprocessing leads to unnoticed differences; synthetic data generation procedures often insufficiently documented.",
            "mitigation_methods": "Share raw data and all preprocessing scripts, document dataset source and exact versions, publish synthetic data-generation code or seeds, modularize preprocessing as shareable scripts.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Precise dataset provenance and sharing of preprocessing code are necessary to prevent variability from dataset differences and to enable exact reproduction.",
            "uuid": "e452.4"
        },
        {
            "name_short": "Version control (git)",
            "name_full": "Use of Version Control Systems (git)",
            "brief_description": "The paper advocates using git (and hosting services like GitHub/GitLab) to track code changes, avoid multiple ad-hoc file copies, and enable collaborators to reproduce the exact code state tied to results.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "development and sharing of reproducible codebases",
            "variability_sources": "multiple code versions and untracked ad-hoc copies leading to ambiguity about which code produced reported results",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Researchers keep many slightly different source files (e.g., my_algo.py, my_algo_1.py) making it impossible to know which produced published figures.",
            "mitigation_methods": "Adopt git for commit history, branching, and SHA-1 commit identifiers so exact code states can be recovered; push repos to hosted services and tag commits corresponding to paper results.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Proper use of version control prevents ambiguity about code versions and enables reproducible retrieval of the exact code used to produce published results.",
            "uuid": "e452.5"
        },
        {
            "name_short": "Documentation / Metadata",
            "name_full": "Documentation and Experimental Metadata (README, runtime, resources, env.yml)",
            "brief_description": "The paper prescribes detailed README files and metadata (dependencies, computational resources, runtime, scripts order, environment.yml) as essential for reproducibility by others.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "sharing experiments and enabling others to reproduce specific results or figures",
            "variability_sources": "lack of instruction about script execution order, missing computational requirements, omitted meta-data like hyperparameters, seeds, and runtime environment",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Code without clear instructions, missing env.yml or README entries leaves reproducer unsure how to run experiments or what resources are required.",
            "mitigation_methods": "Provide README with project description, install steps, resource and runtime info, preprocessing scripts, exact script order; include environment.yml and pin crucial package versions; assign DOI for permanence.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Comprehensive documentation and explicit metadata (env files, computational requirements, script ordering) are practical, low-overhead steps that greatly increase the chance others can reproduce results.",
            "uuid": "e452.6"
        },
        {
            "name_short": "Encapsulation tools caveat",
            "name_full": "Encapsulation Tools (Whole Tale, containerization) Limitations",
            "brief_description": "Paper discusses encapsulation platforms (e.g., Whole Tale, Docker) as ways to capture experiments but warns they can treat experiments as black boxes, be inflexible, and risk obsolescence if not maintained.",
            "citation_title": "A Guide to Computational Reproducibility in Signal Processing and Machine Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "signal processing and machine learning",
            "experimental_task": "packaging full computational workflows for reproducibility",
            "variability_sources": "reliance on third-party reproducibility platforms that may change or be abandoned, black-box encapsulation hiding internal steps",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Encapsulation can impede interpretability and long-term maintenance; platform abandonment can make reproducible artifacts inaccessible.",
            "mitigation_methods": "Prefer widely used, robust open-source tools (git, conda, Docker where appropriate) and supplement encapsulation with readable modular code and documentation; assign DOIs to artifacts for permanence.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Encapsulation platforms can help but are not a panacea; combine environment capture with clear, modular code and documentation to maximize reproducibility and longevity.",
            "uuid": "e452.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reproducible research in signal processing",
            "rating": 2,
            "sanitized_title": "reproducible_research_in_signal_processing"
        },
        {
            "paper_title": "Reproducible research: Best practices and potential misuse",
            "rating": 2,
            "sanitized_title": "reproducible_research_best_practices_and_potential_misuse"
        },
        {
            "paper_title": "Computing environments for reproducibility: Capturing the \"Whole Tale\"",
            "rating": 1,
            "sanitized_title": "computing_environments_for_reproducibility_capturing_the_whole_tale"
        },
        {
            "paper_title": "An introduction to docker for reproducible research",
            "rating": 1,
            "sanitized_title": "an_introduction_to_docker_for_reproducible_research"
        },
        {
            "paper_title": "Stochastic gradient descent tricks",
            "rating": 1,
            "sanitized_title": "stochastic_gradient_descent_tricks"
        }
    ],
    "cost": 0.01177875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Guide to Computational Reproducibility in Signal Processing and Machine Learning</p>
<p>Joseph Shenouda 
Waheed U Bajwa 
A Guide to Computational Reproducibility in Signal Processing and Machine Learning
1</p>
<p>I. INTRODUCTION</p>
<p>A computational experiment is deemed reproducible if the same data and methods are available to replicate quantitative results by any independent researcher, anywhere and at any time, granted they have the required computing power. Such computational reproducibility is a growing challenge that has been extensively studied among computational researchers as well as within the signal processing and machine learning research community [1], [2]. Signal processing research is in particular becoming increasingly reliant on computational experiments to test hypotheses and validate claims, which is in contrast to the yesteryears when one typically used computational experiments to elucidate rigorous theory and mathematical proofs. Therefore it has become more important than ever to ensure the reproducibility of computational experiments, as this is the first step in confirming the validity of research claims supported through the outcomes of computational experiments. But this is not turning out to be an easy task.</p>
<p>The paradigm shift from the theory-driven research to the compute-driven claims in signal processing and machine learning has been facilitated by powerful computing resources, accessibility of massive datasets, and a myriad of new libraries and frameworks (such as NumPy [3], Scikit-learn [4], MATLAB Toolboxes [5], and TensorFlow [6]) that provide a layer of abstraction allowing for rapid implementation of complex algorithms. Unfortunately this changing research landscape is also bringing with it new obstacles and unseen challenges in developing reproducible experiments.</p>
<p>Computational experiments today often incorporate various scripts for preprocessing data, running algorithms, and plotting results, all while utilizing huge datasets which require computing clusters that often take days or weeks to finish computing with multiple manual interventions needed to successfully produce the desired results. This is contrary to the way computational experiments used to be conducted and the way new researchers are introduced to computational resources in the classroom, where they typically use simple and intuitive interactive computing software consisting of a single script that runs locally on one's computer [7]. This new paradigm of computational experiments is now requiring the scientific community to rethink how we publicize and share our code to encapsulate all the necessary arXiv:2108.12383v2 [eess.SP] 15 Feb 2022 2 information about our experiments and make computational reproducibility practically possible. Additionally our extensive dependence on libraries and frameworks leads to brittle codebases that typically only output correct results when executed on the original machine of the researcher. Furthermore, the nature of these data-driven experiments often require careful parameter tuning, random number generations, and data preprocessing, all of which are independent from the main finding, such as a new algorithm, being implemented or investigated.</p>
<p>Evolution of Computational Experiments:</p>
<p>Computational experiments in signal processing and machine learning today have transformed from the way they used to be conducted in the early days of these fields. Due to the rise in the availability of computational resources and large datasets, many of our computational experiments can no longer be carried out on local workstations, as was the norm in the past. Rather, they are often carried out on large computing clusters and can take hours or days to complete. This fact, coupled with the dependencies on multiple third-party libraries, hyperparameter tuning, and random number generators makes it very time consuming, and sometimes nearly impossible, to try and reproduce published computational results by trial and error.</p>
<p>Due to these new challenges most experiments have become difficult, if not impossible, to be reproduced by an independent researcher. As an anecdote, when attempting to reproduce computational results in our lab from a paper published just months prior, even the original authors of the experiment were unable to completely reproduce the results. However, this phenomenon is not unique to our lab. In 2016 a survey conducted by the journal Nature found that 50% of researchers were unable to reproduce their 3 own experiments [8]. And while the issue of reproducibility has been discussed in the literature [1] and specifically within the signal processing community [9], [10], it is still unclear to most researchers what are the most practical approaches to ensure computational reproducibility without impinging on their primary responsibility of conducting research. This is because the guidelines and best practices provided for computational reproducibility in the existing works [9], [10] do not account for all the obstacles to reproducibility of the increasingly complex and large-scale computational experiments. In addition to the complexity of modern computational experiments, these obstacles include the potential for human errors and the rapidly evolving technological landscape that is changing at an unprecedented rate. This article complements the existing works by explicitly focusing on these and related obstacles for computational reproducibility and, in contrast to the discussion in [9], [10], advocates that researchers should plan for the computational reproducibility of their experiments long before any code is written.</p>
<p>In summary, although works such as [1], [9], [10] have helped researchers understand the importance of making computational experiments reproducible, the lack of a clear set of standards and tools makes it difficult to incorporate good reproducibility practices in most labs. It is in this regard that we aim to present signal processing researchers with a set of practical tools and strategies that can help mitigate many of the obstacles to producing reproducible computational experiments.</p>
<p>A. Why Computational Reproducibility</p>
<p>Making computational experiments reproducible is a necessary step for ensuring the credibility of the conclusions made from a research study. If researchers are regularly unable to validate the computational results in a study, it becomes impossible to investigate whether or not the latest results presented in a research paper are indeed state-of-the-art. For example, a group of researchers recently published work that presented a new recurrent neural network architecture for language modelling which appeared to achieve state-of-the-art performance in terms of perplexity on the Penn Treebank dataset. However, after carefully controlling for hyperparameters, a different group of researchers found that the traditional long shortterm memory (LSTM) recurrent neural network model achieved better perplexity on the same dataset, contrary to what was found by the first group. This specific example demonstrates the significance of hyperparameters and other meta data in assessing the performance of new machine learning algorithms.</p>
<p>The claims of state-of-the-art performance in particular are directly related to these incidental details and without the ability to reproduce previous computational results it can become difficult to discern the true novelty of new research findings.</p>
<p>This example, which is far from a rarity, illustrates the broader point that the field of computational sciences cannot truly advance until we are confident in the past progress. Not only does ensuring 4 computational reproducibility protect the integrity of the research, it also allows fellow researchers to develop their own experiments quickly by utilizing code written by others investigating the same problem. In addition, implementing reproducible computational experiments has numerous benefits for the researchers themselves [11]. By utilizing the techniques outlined in this article and making the experiments as transparent and detailed as possible, one can avoid catastrophes such as a hard drive that gets corrupted, resulting in a loss of all data and source code, or a loss of older promising results due to a bug that was introduced in the code later on. Additionally, old research problems often get revisited and improved upon; thus having reproducible code for the experiments in the original study can save hours of frustration for the researcher(s) trying to reproduce old computational results, before embarking on the new research. Finally, making one's computational experiments reproducible can add value to the research itself; by lowering the barrier of entry for other researchers to engage and expand on the experiments, it makes the research more accessible to the community, which can in turn lead to more impactful work [12].</p>
<p>While the importance of reproducible research in general and computational reproducibility in particular is obvious to many, the exact tools and techniques that one must utilize when building computational experiments to ensure that they are reproducibile for the foreseeable future after publication are still not very clear. In particular, a common misconception is that simply publishing all the code and data used to obtain the results makes computational experiments reproducible. But this is almost never the case, as the raw source code and data along with the paper alone cannot give enough details on how to run the experiment, the necessary dependencies, or the required computational power. It is in this regard that we worked on making two experiments from two different research projects here at the INSPIRE Lab reproducible and, in the process, investigated the best ways to create reproducible experiments organically, taking into consideration the extra overhead that comes with such an endeavour. The best techniques and tools were considered in light of the fact that one's goal as a computational researcher is typically to conduct and disseminate research and not maintain or develop commercial software. By sharing our experiences and best techniques with the readers of the IEEE Signal Processing Magazine, it is our hope that we can go beyond just highlighting the importance of computational reproducibility by providing a clear and practical guide for developing experiments in a reproducible manner.</p>
<p>We have organized the rest of the article as follows. First, the common pitfalls to computational reproducibility are explained. Next, the standards for computational reproducibility are discussed. The last three sections discuss solutions and tools that can be utilized to avoid the reproducibility pitfalls discussed earlier. These topics include version control for organizing a dynamic and collaborative codebase, package 5 managers for handling multiple dependencies and finally, techniques on how to eventually share the code accompanying the computational experiments.</p>
<p>II. THE COMPUTATIONAL REPRODUCIBILITY PITFALLS</p>
<p>Most papers in the field of signal processing and machine learning tend to include a section at the end where the authors explain their computational experiments along with figures that provide some evidence that the proposed research has practical implications. However, due to the limitations in space and in the interest of conciseness, this section of the research papers cannot provide all the necessary details to reproduce the results. Even when computational experiments are only meant to give some justification for a rigorous theory, these results are important and should be explained as clearly as the theorems and proofs on which they are based. While publishing all the source code used for the computational experiments is a step in the right direction and may seem to fill in all the missing gaps from the research paper, it is often still not enough to completely reproduce the original results [13]. This is in part due to the fact that most researchers are not trained on how to write clean, maintainable code in a collaborative setting [14]. This lack of training has the potential to result in highly disorganized code that is difficult to parse and understand by an independent researcher.</p>
<p>Another potential pitfall we identified, which can seem benign for small experiments, is that researchers have multiple versions of the same computational experiment with slightly different code, which makes it impossible to know which was the one used in obtaining the reported results. Thus, attempting to reproduce the final set of results requires one to first run each version of the codebase individually until they produce the desired results. On large computational experiments that take days to finish, this is of course impractical. Another issue is that researchers typically do not describe in enough detail the dependencies needed for running the code used for the experiment [13]. Even when the dependencies are mentioned, researchers might omit the exact version that was used when originally obtaining results, which could make computational reproducibility impossible for those attempting to run the code on a different machine. For example, if the original code used a certain feature from a library that has since been deprecated in later versions, those running the code using the latest version of the library cannot reproduce the results, and may not know that they need to downgrade their dependency. Therefore, there must be a way for the original researchers to preserve and share the exact computational environment used when generating results in order to share it with others looking to run their code to reproduce results.</p>
<p>Additionally, as mentioned earlier, the source code alone does not provide instructions on how to run the computational experiments nor the order in which the scripts should be executed. Another piece of information that is rarely mentioned in enough detail when sharing the code is the computational power 6 needed for an experiment [13] and the time it takes to finish executing. This information is necessary for those trying to reproduce the computational results as they must first verify that they are equipped with the right amount of computational power to run the code.</p>
<p>Even when these pitfalls are accounted for, there is still the challenge of sharing the necessary meta data accompanying the experiments. The description of the experiments present in most research papers simply cannot encapsulate all the necessary meta data needed to successfully reproduce computational results. For instance, while the authors typically mention the dataset being used in a particular experiment, there may exist some ambiguity about the exact source of the data. And this can be true even for established benchmark datasets such as the "House" image or the MNIST dataset [15]. While the authors might believe that these are "standard" datasets, it is often the case that different versions of these datasets are circulating around the internet, each with slight variations that may not be immediately noticeable but can yield different results when used for the same experiment [9]. Even if the sources of the datasets are explained, the preprocessing steps performed on the data can be vital to obtaining the published computational results, but these might not be thoroughly explained in research papers.</p>
<p>All of these seemingly benign or superfluous details can have an effect on the computational results produced from the experiment that others may be trying to reproduce. Moreover, for computational experiments that use synthetic data, the way in which the data was produced may not be explained in enough detail. Furthermore, signal processing and machine learning algorithms typically rely on finely tuned hyperparameters that, when changed, can also give different results and even invalidate the conclusions made in the paper, yet the hyperparameters or the methods by which they are found are not always described in enough detail [13]. For example, most machine learning experiments make use of cross-validation to find optimal hyperparameters and while the researchers may mention this detail, they might omit information about how exactly the dataset was split up. This in turn could lead to different hyperparameters when the experiment is run by someone trying to reproduce the original results.</p>
<p>Finally, most experiments rely on random number generators somewhere in the codebase. An example of this would be the stochastic gradient descent (SGD) algorithm [16], which is a popular method for large-scale training in machine learning. In each iteration of the vanilla SGD, a random sample from the dataset is used to compute the gradient of the loss function and update the parameters of interest. As a result, the rate of convergence and the values of the optimized parameters depend on the order in which the samples were selected [17] and will be different if the randomly chosen sample in each iteration is not consistent every time the experiment is run. By not saving the random seed in the code as part of the experimental meta data, the sequence of randomly chosen samples will vary every time someone tries to 7 run the experiment, making it almost impossible to exactly reproduce the original computational results each time the codebase is run.</p>
<p>In the rest of this article we discuss what it means for a computational experiment to be computationally reproducible and the potential techniques for overcoming each of the pitfalls discussed. While the suggested methodologies for creating computational experiments may incur some additional work for the researcher, it strikes a good balance between ensuring reproducible results and becoming an obstacle to further research. Furthermore, through practice over time, it is our hope that the tools and techniques discussed below will become commonplace for researchers, giving them the ability to naturally create computional experiments that are readily computationally reproducible. </p>
<p>III. THE GOALS FOR REPRODUCIBILITY</p>
<p>There has been a lot of discussion across many domains, including within the signal processing and machine learning community, about how to successfully make computational experiments reproducible.</p>
<p>However, much of the work discussing computational reproducibility tends to advocate for new software tools that can be used to easily publish computational experiments that are reproducible. One such example is the "Whole Tale" [18], a platform that enables researchers to create tales-executable research objects that capture data, code, and computational environments-for the computational reproducibility of the research findings. While tools like this may seem promising, they have limitations. The first is that these reproducibility tools attempt to encapsulate the whole process of running an experiment from preprocessing to plotting, treating the computational experiment as a blackbox and this in turn leaves the actual code in a state that is difficult to interpret for those who might be interested in 8 digging deeper or expanding upon it. Additionally, due to the fact that they attempt to encapsulate every component of an experiment, they tend to be highly inflexibility and may not be appropriate for every computational experiment, making the process of creating computationally reproducible experiments even more cumbersome. This is especially true for the research labs that tend to focus more on the theoretical and algorithmic aspects of signal processing and machine learning, rather than the applied aspects, and therefore provide only simple computational experiments to give empirical justifications for their claims.</p>
<p>In such labs the actual codebases for the computational experiments are not very large and they tend to focus narrowly on a newly proposed idea, insight, or algorithm, which is in contrast to large data analysis projects found in other computational sciences. In addition, the encapsulation-based reproducibility tools are often funded by grant money and are typically only maintained by a single lab; thus when the grant money runs out, there is no guarantee that the tool will be maintained and it may become obsolete within a few short years [19]. Therefore relying on these computational reproducibility tools would require one Therefore, in order to support organized and understandable codebases, we first present the best tools for implementing version control. Next, in an effort to ensure that dependencies are accurately and easily shared, we discuss the simplest tools for dependency management that allow the researchers to disseminate their exact computational environment to others. Finally, in order to make the codebase easily explorable and provide thorough instructions on how to computationally reproduce the experiments, we offer some advice on the best ways to document and publish the code for sharing with the rest of the research community.</p>
<p>IV. MANAGING A DEVELOPING EXPERIMENT</p>
<p>Research is all about taking incremental steps towards a result. This is true when trying to prove theorems as well as in developing computational experiments. In practice the algorithms that get applied to datasets always need some level of tuning in order to run the computational experiments correctly or obtain the best possible results. Oftentimes one wants to investigate how changing a certain piece of the code (e.g., a specific hyperparameter) alters the results without losing the current version of the code they have. A nave solution to this problem is to make a copy of the source code with the desired changes without deleting the original. For example, a file named my_algo.py containing an implementation of the main algorithm for the experiment might get copied and renamed to my_algo_1.py with a few subtle changes made within it. Later the researchers may be interested in changing things a little differently but still keep my_algo.py and my_algo_1.py just in case the new version performs worse, so they create another file named my_algo_2.py, and this pattern repeats for multiple versions of the file. Finally, a few months after the work is published, someone might ask about how the figures were produced and the original researchers are left scrambling through up to a dozen different versions of the codebase, trying to find the one that actually produced the right results. Clearly this is not a good solution and can quickly become an unwieldy situation, forcing one to re-run multiple versions of the codebase while trying to find the correct one. Though this ad-hoc approach may have been feasible for small computational experiments that only take seconds or minutes to finish computing, with the rise in 10 larger datasets and higher wall-clock time per computational experiment, re-running multiple versions of the experiments until one finds the right version can take days or even weeks. This is the precise problem we found in one of our codebases that we tried to computationally reproduce, which had multiple versions of the same experiment. Even the original researchers on the project were not able to recall the version that corresponded to figures in the published work. Given that some of these computational experiments took up to five days to finish running, finding the version that reproduced the original plots took us weeks.</p>
<p>Another common hurdle that researchers must overcome is trying to OneDrive [22] and email to share their code amongst collaborators. While these tools are great for sharing images, documents and other forms of media, they are not the best tools for sharing a developing codebase, as they require the collaborators to constantly download and then upload the codebase every time a change has been made. There is also a responsibility on the one making the changes to alert all other collaborators in order for them to re-sync their codebases. In short, this mode of collaboration can actually slow down collaboration and is prone to errors because it makes it very cumbersome to track changes made in the codebase and ensure that everyone is using the latest version of the codebase.</p>
<p>One of the best solutions to both of the aforementioned problems is an effective use of a good version control system, the most popular being git, a free and open-source distributed version control system known for its efficiency compared to other version control systems, making it easy to keep track of all the changes being made to an experiment throughout the development process. Due to its popularity and utility git integrates well with many online code hosting services, such as GitHub [23], BitBucket [24], and GitLab [25]. The advantage of git is that it allows one to track every change that has been made to any file in the codebase, often referred to as the repository. Once one of the files in the repository being tracked has been changed, the researcher can then move that file or set of files to the staging area.</p>
<p>From the staging area the changes are then committed, which assigns the current state of the repository a unique SHA-1 hash and saves it in a history database so that it can be compared to future versions of the codebase or recovered when things go wrong. These commits also contain information of who made the 11 changes and are accompanied by a short comment or message to explain what change were made and why. By committing regularly, a researcher is able to traverse through all changes of the codebase and see precisely which lines of code were changed, by whom they were changed, and a short description of why they were changed. The git branching feature also lends itself very well to experimenting and trying out different parameters or techniques. By branching, one can create an alternate tracking history of the repository starting from the current commit, and from there one can edit the code and run the experiments without affecting the current implementation on the main branch.</p>
<p>However, git truly shines in a collaborative setting. This is primarily due to its design as a distributed version control system, which means that it provides each collaborator a full commit history of the entire repository. When used jointly with a code hosting platform, such as GitHub [23], BitBucket [24], or GitLab [25], it allows researchers to upload their repository to the internet and pull down changes from the hosting service to their local environment every time they intend to develop on the codebase. This ability to pull down the changes is substantially better than the traditional approach of re-downloading a codebase from cloud storage providers because it eliminates the need to ask collaborators about what was altered or alert other collaborators of the changes implemented. This is because every commit that was made is already documented by the commit message accompanying it, along with the name of the person who made the commit. Furthermore, due to the efficient design of git, pulling down changes from a repository only takes seconds even if numerous changes were made; this is in contrast to re-downloading an entire codebase from a cloud storage provider, which can take minutes every time. Additionally git is intelligently designed to handle merging the changes between the current version of the code on one's local machine and the updated codebase online, making the process very seamless and facilitating efficient asynchronous collaboration.</p>
<p>While some researchers may argue that git is a complex tool which ultimately impedes their productivity, this is only temporary and we believe that that it is worth the investment for computational researchers.</p>
<p>Furthermore, most researchers in machine learning and signal processing collaborate extensively with industry where version control is expected to be used in maintaining any codebase. Perhaps the most appealing feature of using git for version control over other alternatives is its widespread use among programmers. Over the years, this has resulted in numerous resources all over the internet in the form of videos, blog posts [26], and books [27]. Additionally, online communities such as StackOverflow [28] provide answers to nearly any confusion one may have about git and its features. There are also a number of graphical interface tools for using git for those not yet comfortable using the command line. While git is an extremely flexible tool and can be a tremendous aid in writing organized and 12 manageable code for experiments across multiple researchers working together, it is only effective when utilized properly. Researchers must ensure that they are committing changes frequently and are providing providing concise and accurate descriptions of the changes made for easy reference as the codebase is developed.</p>
<p>Version Control: By effectively using version control and making frequent commits, researchers do not have to save multiple versions of the same file in their codebase, because they are always able to easily revert back to an earlier commit. With version control, specifically git, one is also free to pursue multiple versions of the experiment without impinging on each other. This is most effectively done by branching, which allows one to create multiple version histories.</p>
<p>Going beyond version control, the next important step for ensuring reproducibility is to make sure that those trying to reproduce the code are able to capture the same exact computational environment as the one used by the original researchers. This is the topic of our next discussion.</p>
<p>V. MANAGING DEPENDENCIES</p>
<p>One of the simplest ways to ensure a robust and reproducible codebase is to try and minimize the use of external packages and libraries in the code. However, due to the immense utility of modern software libraries, the codebases for modern signal processing and machine learning experiments are seldom developed without the use of several external libraries. These include popular scientific programming and machine learning libraries such as NumPy [3], Scikit-learn [4], TensorFlow [6], CVX [29], and Tensor</p>
<p>Toolbox [30]. Although there are multiple programming languages available for scientific computing, such 13 as R [31], Julia [32], and MATLAB [5], we focus the discussion specifically on the Python programming language as it is free, open source and widely popular in the machine learning community. However, the techniques discussed in this section can still be utilized for other popular programming languages.</p>
<p>Without first knowing which exact dependencies need to be installed on a researcher's machine, it becomes impossible to reproduce results. The reproducer must not only be aware of what dependencies are being used, but also the precise version being used at the time when the results were originally generated. One popular way to encapsulate the original researcher's computational environment perfectly is by using Docker [33], as suggested in [34]. While Docker is a powerful containerization tool and it does indeed solve the problem of dependency management by encapsulating the original researcher's computational environment into isolated containers, it could be too much of an overhead for researchers who have no prior experience using Docker or managing large software projects. This is especially true in labs and research groups where researchers focus primarily on the mathematical and algorithmic aspects of their research and only utilize a small codebase for experiments, which relies on only a few dependencies.</p>
<p>In these computational experiments one typically does not attempt to synthesize and manage multiple programming languages, frameworks and dependencies, as is often the case in real-world commercial applications, making Docker-based solutions an overkill.</p>
<p>Therefore, a more appropriate tool would be a simple and light dependency manager similar to pip, the default dependency manager for Python. But the most fitting environment and dependency manager for computational experiments is conda [35], an open-source environment and package management tool.</p>
<p>Although conda can be used independently in any codebase, it is automatically included in the Anaconda [36] distribution of Python, which is already the distribution of choice for many computational researchers.</p>
<p>Unlike other package managers, conda is specifically designed to easily manage the dependencies most commonly encountered in scientific computing, and overcomes the many shortcomings of pip. One major advantage of using conda is that when one attempts to installs a new package, conda ensures that all the requirements for this new package are met before adding it, and if this is not the case an error is shown immediately with steps on how to rectify the issue. This is contrary to pip, which does not check this condition before installing a new package and can result in unexpected errors later during development. However, conda's greatest advantage is allowing one to create independent virtual environments for each experiment, so that all projects do not share the same global dependencies. This in turn ensures that each environment contains only the packages that are absolutely needed for the current experiment associated with it and nothing more. Creating these virtual environments is also important because when the version of a package from one project gets upgraded on the researcher's machine, it 14 will not interfere with the current version of that same package in the other environments, allowing the original computational environment on which the experiment was carried out to be preserved. Finally, while Python's standard library does include its own virtual environment manager through the venv module, conda is unique in that it allows one to create environments with different versions of Python itself.</p>
<p>Conda Environments: A conceptual diagram that illustrates the usefulness of conda environments for reproducible research. On the left, "env1" is a python environment with four different libraries commonly used in machine learning experiments. On the right, the environment "env2" has some of the same libraries but with different version numbers and a different version of python itself; this could, for example, be an environment for an older variant of the experiment. Both these environments are independent of each other and preserve the computational environment that was originally used to produce experimental results. They can also be easily exported and shared with outside researchers through an environment.yml file. The environment can be reconstructed on a new computer using the command ''conda env -f environment.yml''.</p>
<p>Based on the above, we recommend using conda for managing dependencies in reproducible experiments. This involves creating an environment.yml (or a similarly named) file for each project, which specifies the necessary dependencies, and then constructing a new environment based on the specifications in this file, which can be done with a simple command. This should be done before any code for the experiment is written in order to ensure no dependencies are unaccounted for. If one wants to later add a dependency they can do so by adding it to their environment.yml file and updating their environment according to the new additions made to the file. Then, when another independent researcher wishes to run the experiment, they can quickly and easily reconstruct the same conda environment with all the necessary dependencies by simply using the environment.yml file. Additionally, one can specify the exact version number for the important dependencies in the "environment.yml" file to ensure that when the "conda" environment gets reconstructed on another machine (potentially with another operating system) the same dependency version is used. Also, for those who prefer some other dependency manager as opposed to conda, they can still inspect the plain text yaml file to see all the necessary dependencies with their respective versions. A typical "environment.yml" for a machine learning experiment is shown in Figure 1, demonstrating its readability and effectiveness in organizing dependencies. Here we have pinned the version for TensorFlow to ensure that this same version is always preserved when the environment is reconstructed on a different machine. By utilizing this tool, managing dependencies becomes very straightforward and makes it possible for others to replicate the same computational environment that one had used when originally running the experiment, without the need to resort to trial and error or to contact the original researchers. 16 Proper version control and dependency management tools are critical first steps to promoting reproducibility throughout development of an experiment. However, ensuring reproducibility for the foreseeable future is made most probable through properly shared code and data, detailed documentation and thoughtful organization of the codebase developed. We now shift our focus to this aspect of reproducibility.</p>
<p>VI. SHARING CODE</p>
<p>While version control and dependency management make it possible for others to run the code and help to keep the codebase organized, this does not provide any information about the order in which the scripts should be executed and what computational resources were utilized in obtaining the original results.</p>
<p>Furthermore, when researchers look to reproduce other's experiments, they do not necessarily want to reproduce each and every figure in the paper. This is especially true for papers with multiple experiments.</p>
<p>Sometimes they are only interested in a specific plot or the implementation of a particular algorithm that was described in the original work. Therefore it is important to ensure that one's experiments do not get crammed into a single source file that includes preprocessing, algorithmic implementations, analysis and visualization. Indeed, by making the codebase modular and organizing the project structure carefully, as discussed in the following, it becomes significantly easier for others to download and inspect what they want to reproduce from the codebase without too much digging.</p>
<p>Project Structure: Typically, a computational experiment in signal processing and machine learning can be broken down into three parts:</p>
<p>1) Preprocessing of data or generation of synthetic data.</p>
<p>2) Execution of the actual experiment on the data.</p>
<p>3) Analysis of results and generation of figures.</p>
<p>It is often advantageous to keep the project folder organized in a similar fashion by creating separate scripts, or even subfolders, for each of these tasks. Keeping the codebase modular and structured in this manner allows others looking to reproduce a particular set of results to do so without needing to analyze or execute the entire codebase. For example, in our lab's experiments on learning mixtures of separable dictionaries for tensor data [37], we found it necessary to split the experiments up into different subfolders as the original work [38] compared four different algorithms on both synthetic and real datasets. Objectoriented design within the code can also be helpful as this increases the modularity of the experiments and allows extensions to be developed by other researchers naturally. Last, but not the least, the data used in the experiments must also be shared correctly; this involves ensuring that not only the raw data get shared but also all the intermediate steps or scripts used to preprocess the data before using them in the experiments are shared.</p>
<p>Coding Standards: Additionally, it is worth following a set of guidelines for clear and concise comments that can describe every function or class definition in the codebase. For Python the convention is to add a docstring [39] to each module, as many text editors and integrated development environments (IDEs) often search for these in the project files to allow programmers to quickly inspect what a function is doing without actually going to its destination in the source code. This should be accompanied by standard coding practices, such as proper variable naming conventions and indentations, which can make the codebase easier to read and interpret by others.</p>
<p>Documentation:</p>
<p>The most important thing that must accompany the project being shared is a detailed README file. We recommend that the README contain all the following elements:</p>
<p>1) Brief description of the project and related paper(s).</p>
<p>2) Steps to install necessary dependencies, e.g., via conda and environment.yml file.</p>
<p>3) Computational resources used to run the experiments along with their respective runtimes.</p>
<p>4) Scripts used to preprocess the data, in the case of real data, or generate them for the case of synthetic data.</p>
<p>5)</p>
<p>Description of which scripts one should run for which experiments and how to then plot the results.</p>
<p>We found that each of these pieces of information were necessary when attempting to reproduce the results from a codebase. Such documentation that accompanies the code provides a full picture of how an experiment should be run and how one actually acquires the desired results without having to spend much time laboring through every detail in the code.</p>
<p>Publicization: The way in which the code is publicized also needs careful consideration to ensure ease of access and, most importantly, permanency. Typically experiments are shared by hosting the accompanying source code on the lab's website or on a code hosting platform, such as GitHub, GitLab or BitBucket. However, posting it on the lab's website is unlikely to be the most robust solution; for instance, if a researcher leaves his or her position, their website often disappears along with the code.</p>
<p>Code hosting services, however, are linked to an account and repositories can be easily transferred to different owners with minimal hassle. Sharing the code this way also allows others to fork the repository and develop further on it. Additionally, others can add their comments or questions about the code in the form of "Issues." These discussions become public for anyone else viewing the repository as well, which can eliminate redundant questions that the community may have. One of the biggest issues with both of these solutions is that they are still dependent on the organization hosting the website. This means that if the hosting website were to ever disappear, the code hosted on it would go with it. But permanency on the internet is necessary for ensuring that the results are reproducible for the forseeable future. One way 18 to ensure permanency is to assign the codebase a digital object identifier (DOI) to give it a permanent presence on the internet, which can be done with tools such as Zenodo [40].</p>
<p>VII. CONCLUSION</p>
<p>While there has been much discussion in the literature about the reproducibility crisis in computational research, not enough emphasis has been given on the best practices and techniques to solve this problem with established tools. The solutions to computational reproducibility have been especially unaddressed for the more theoretically bent research groups within the signal processing and machine learning community that typically develop smaller computational experiments compared to other computational sciences. In this article, we presented the main pitfalls to achieving reproducible experiments and then provided common tools and techniques that can be used to overcome each of those pitfalls, while bearing in mind that making experiments reproducible can entail extra effort that may divert our attention away from our primary task of research. By utilizing the right tools for version control and dependency management as well as careful structuring and documentation when sharing the codebase, we can work towards ensuring that every computational experiment in our research is readily reproducible.</p>
<p>to relearn a new platform for publishing their experiments in a computationally reproducible way every time an old one gets abandoned. In summary, while an encapsulation tool could in theory be the optimal solution to the computational reproducibility crisis, there currently does not exist any widely adopted off-the-shelf tool that overcomes the issues highlighted here.Because of the aforementioned reasons, the focus of this article is not on finding or creating the best computational reproducibility software that can automatically encapsulate all the components of the computational experiment. Rather, the goal is to find how one can achieve computational reproducibility by relying on robust open-source tools, used across both industry and academia. This involves formulating a methodology for developing computational experiments such that the reproducible codebase supporting a new research finding can be released in parallel with the publication of the paper. To be more precise, the methodology should enable any independent researcher studying a similar problem to obtain, understand, and easily run the code used in the computational experiments in order to reproduce the exact same figures, plots or values without enduring a painstaking trial and error process. Furthermore, this should be possible without the need to ever contact the researchers responsible for the original computational experiment.The goals of computational reproducibility espoused in this article also emphasize the importance of making experiments computationally reproducible throughout their development, since trying to retroactively make the experiments computationally reproducible after the results have been obtained and published is usually a much more difficult task. If computational reproducibility is only attempted after publication, the researcher is likely to have already fallen into one of the pitfalls mentioned earlier, and 9 must struggle with first reproducing their own results, perhaps unsuccessfully, before being able to share the code with others. While it may be cumbersome at first, after researchers get accustomed to the tools and techniques presented in this article, the process of making experiments computationally reproducible should incur minimal overhead on their research. In particular, it is our hope that the solutions put forth in this article can help overcome all the current hurdles to seamless computational reproducibility.These solutions are discussed under three main themes in the following. Version control systems keep track of changes in the codebase as well as eliminate the issue of multiple concurrent versions of the experiments.</p>
<p>develop computational experiments with collaborators. Due to the nature of computational research, developing computational experiments collaboratively can be done without having to physically share the same computational resources, lab or even the same country. While this certainly makes collaboration easier compared to other scientific domains, collaborating in the development of a codebase is still not a simple task. Some of the current solutions that researchers employ are faulty and have many drawbacks. For instance a researcher may treat source code like any other file and rely on tools such as Google Drive [20], DropBox [21], Microsoft</p>
<p>Fig. 1 .
1An example of an environment.yml file with common libraries and dependencies used in typical machine learning experiments.</p>
<p>TABLE I A
ITABLE OF THE COMMON PITFALLS DISCUSSED IN THIS SECTION AND WHERE TO FIND THEIR RESPECTIVE REMEDIES THROUGHOUT THIS ARTICLE.Pitfall 
Short description 
Remedy </p>
<p>1 
Disorganized codebases and multiple </p>
<p>versions. </p>
<p>Section IV </p>
<p>2 
Differences in computational </p>
<p>environments and failure to </p>
<p>disseminate necessary dependencies. </p>
<p>Section V </p>
<p>3 
Missing critical meta data such as </p>
<p>hyperparameters, computing power </p>
<p>and dataset sources. </p>
<p>Section VI </p>
<p>ACKNOWLEDGEMENTSThe authors gratefully acknowledge the support of the NSF (CCF-1453073, CCF-1907658, CCF-1910110, OAC-1940074)
V Stodden, F Leisch, R D Peng, Implementing Reproducible Research. CRC PressV. Stodden, F. Leisch, and R. D. Peng, Implementing Reproducible Research. CRC Press, 2014.</p>
<p>A Step Toward Quantifying Independently Reproducible Machine Learning Research. E Raff, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing Systems32E. Raff, "A Step Toward Quantifying Independently Reproducible Machine Learning Research," in Proc. Advances in Neural Information Processing Systems, vol. 32, 2019, pp. 5485-5495.</p>
<p>Array programming with NumPy. C R Harris, Nature. 5857825C. R. Harris et al., "Array programming with NumPy," Nature, vol. 585, no. 7825, pp. 357-362, 2020.</p>
<p>Scikit-learn: Machine Learning in Python. F Pedregosa, Journal of Machine Learning Research. 1285F. Pedregosa et al., "Scikit-learn: Machine Learning in Python," Journal of Machine Learning Research, vol. 12, no. 85, pp. 2825-2830, 2011.</p>
<p>. &quot; Mathworks, Matlab, Mathworks, "MATLAB." [Online]. Available: www.mathworks.com</p>
<p>Tensorflow: A system for large-scale machine learning. M Abadi, Proc. 12th USENIX symposium on operating systems design and implementation. 12th USENIX symposium on operating systems design and implementationM. Abadi et al., "Tensorflow: A system for large-scale machine learning," in Proc. 12th USENIX symposium on operating systems design and implementation (OSDI 16), 2016, pp. 265-283.</p>
<p>Making massive computational experiments painless. H Monajemi, D L Donoho, V Stodden, Proc. IEEE International Conference on Big Data. IEEE International Conference on Big DataH. Monajemi, D. L. Donoho, and V. Stodden, "Making massive computational experiments painless," in Proc. IEEE International Conference on Big Data, 2016, pp. 2368-2373.</p>
<p>1,500 scientists lift the lid on reproducibility. M Baker, Nature News. 5337604452, section: News FeatureM. Baker, "1,500 scientists lift the lid on reproducibility," Nature News, vol. 533, no. 7604, p. 452, May 2016, section: News Feature. [Online]. Available: http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970</p>
<p>Reproducible research in signal processing. P Vandewalle, J Kovacevic, M Vetterli, IEEE Signal Processing Magazine. 263P. Vandewalle, J. Kovacevic, and M. Vetterli, "Reproducible research in signal processing," IEEE Signal Processing Magazine, vol. 26, no. 3, pp. 37-47, May 2009.</p>
<p>Reproducible research: Best practices and potential misuse. E Bjornson, IEEE Signal Processing Magazine. 363E. Bjornson, "Reproducible research: Best practices and potential misuse [Perspectives]," IEEE Signal Processing Magazine, vol. 36, no. 3, pp. 106-123, May 2019.</p>
<p>Five selfish reasons to work reproducibly. F Markowetz, Genome Biology. 161274F. Markowetz, "Five selfish reasons to work reproducibly," Genome Biology, vol. 16, no. 1, p. 274, Dec. 2015.</p>
<p>Sharing detailed research data is associated with increased citation rate. H A Piwowar, R S Day, D B Fridsma, PLoS ONE. 23308H. A. Piwowar, R. S. Day, and D. B. Fridsma, "Sharing detailed research data is associated with increased citation rate," PLoS ONE, vol. 2, no. 3, p. e308, Mar. 2007.</p>
<p>State of the art: Reproducibility in artificial intelligence. O E Gundersen, S Kjensmo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32O. E. Gundersen and S. Kjensmo, "State of the art: Reproducibility in artificial intelligence," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.</p>
<p>Towards computational reproducibility: researcher perspectives on the use and sharing of software. Y Alnoamany, J A Borghi, PeerJ Computer Science. 4163Y. AlNoamany and J. A. Borghi, "Towards computational reproducibility: researcher perspectives on the use and sharing of software," PeerJ Computer Science, vol. 4, p. e163, 2018.</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998.</p>
<p>A stochastic approximation method. H Robbins, S Monro, The Annals of Mathematical Statistics. H. Robbins and S. Monro, "A stochastic approximation method," The Annals of Mathematical Statistics, pp. 400-407, 1951.</p>
<p>Stochastic gradient descent tricks. L Bottou, Neural Networks, Tricks of the Trade, Reloaded. Springer77002nd ed.L. Bottou, "Stochastic gradient descent tricks," in Neural Networks, Tricks of the Trade, Reloaded, 2nd ed. Springer, January 2012, vol. 7700, pp. 430-445.</p>
<p>Computing environments for reproducibility: Capturing the "Whole Tale. A Brinckman, Future Generation Computer Systems. 94A. Brinckman et al., "Computing environments for reproducibility: Capturing the "Whole Tale"," Future Generation Computer Systems, vol. 94, pp. 854-867, May 2019.</p>
<p>Publishing computational research -a review of infrastructures for reproducible and transparent scholarly communication. M Konkol, D Nst, L Goulier, Research Integrity and Peer Review. 5110M. Konkol, D. Nst, and L. Goulier, "Publishing computational research -a review of infrastructures for reproducible and transparent scholarly communication," Research Integrity and Peer Review, vol. 5, no. 1, p. 10, Dec. 2020.</p>
<p>Google Drive Website. Llc Google, Google LLC, "Google Drive Website." [Online]. Available: https://drive.google.com</p>
<p>Dropbox Website. Dropbox Inc, Dropbox Inc., "Dropbox Website." [Online]. Available: https://www.dropbox.com</p>
<p>Microsoft OneDrive Website. Microsoft Corp, Microsoft Corp., "Microsoft OneDrive Website." [Online]. Available: https://onedrive.com</p>
<p>GitHub Website. Inc Github, GitHub, Inc., "GitHub Website." [Online]. Available: https://www.github.com</p>
<p>Bitbucket Website. Atlassian Corp, Atlassian Corp., "Bitbucket Website." [Online]. Available: https://bitbucket.org</p>
<p>GitLab Website. Gitlab Inc, GitLab Inc., "GitLab Website." [Online]. Available: https://gitlab.com</p>
<p>Understanding Git Conceptually. C Duan, C. Duan, "Understanding Git Conceptually." [Online]. Available: https://www.sbf5.com/~cduan/technical/git/</p>
<p>S Chacon, B Straub, Pro Git. 2nd ed. ApressS. Chacon and B. Straub, Pro Git, 2nd ed. Apress, Nov. 2014.</p>
<p>Stack Overflow Website. Stack Exchange Inc.Stack Exchange Inc., "Stack Overflow Website." [Online]. Available: https://stackoverflow.com/ 20</p>
<p>CVX: Matlab software for disciplined convex programming, version 2.1. M Grant, S Boyd, M. Grant and S. Boyd, "CVX: Matlab software for disciplined convex programming, version 2.1," Mar. 2014. [Online].</p>
<p>Tensor toolbox for MATLAB, version 3.1. B Bader, T G Kolda, B. Bader, T. G. Kolda, et al., "Tensor toolbox for MATLAB, version 3.1," Jun. 2019. [Online]. Available: www.tensortoolbox.org</p>
<p>R Core Team, R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. R Core Team, R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing, 2020. [Online]. Available: https://www.R-project.org/</p>
<p>Julia: A fresh approach to numerical computing. J Bezanson, A Edelman, S Karpinski, V B Shah, https:/epubs.siam.org/doi/10.1137/141000671SIAM Review. 591J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah, "Julia: A fresh approach to numerical computing," SIAM Review, vol. 59, no. 1, pp. 65-98, Jan. 2017. [Online]. Available: https://epubs.siam.org/doi/10.1137/141000671</p>
<p>Docker: Lightweight Linux containers for consistent development and deployment. D Merkel, Linux Journal. 2014239D. Merkel, "Docker: Lightweight Linux containers for consistent development and deployment," Linux Journal, vol. 2014, no. 239, p. 2:2, Mar. 2014.</p>
<p>An introduction to docker for reproducible research. C Boettiger, ACM SIGOPS Operating Systems Review. 491C. Boettiger, "An introduction to docker for reproducible research," ACM SIGOPS Operating Systems Review, vol. 49, no. 1, pp. 71-79, 2015.</p>
<p>. Anaconda Inc, Conda, vers. 4.10.1. [OnlineAnaconda Inc., "Conda," 2021, vers. 4.10.1. [Online]. Available: https://anaconda.org/anaconda/conda</p>
<p>Anaconda software distribution. Inc Anaconda, 2020, vers. 2-2.4.0. [OnlineAnaconda, Inc., "Anaconda software distribution," 2020, vers. 2-2.4.0. [Online]. Available: https://docs.anaconda.com/</p>
<p>Codebase-Learning mixtures of separable dictionaries for tensor data: Analysis and algorithms. J Shenouda, M Ghassemi, Z Shakeri, A D Sarwate, W U Bajwa, 6GitHub RepositoryJ. Shenouda, M. Ghassemi, Z. Shakeri, A. D. Sarwate, and W. U. Bajwa, "Codebase-Learning mixtures of separable dictionaries for tensor data: Analysis and algorithms," GitHub Repository, 6 2020. [Online]. Available: https://github.com/INSPIRE-Lab-US/LSR-dictionary-learning</p>
<p>Learning mixtures of separable dictionaries for tensor data: Analysis and algorithms. M Ghassemi, Z Shakeri, A D Sarwate, W U Bajwa, IEEE Transactions on Signal Processing. 68M. Ghassemi, Z. Shakeri, A. D. Sarwate, and W. U. Bajwa, "Learning mixtures of separable dictionaries for tensor data: Analysis and algorithms," IEEE Transactions on Signal Processing, vol. 68, pp. 33-48, 2020.</p>
<p>PEP 257 -Docstring Conventions. python-dev, "PEP 257 -Docstring Conventions." [Online]. Available: https://www.python.org/dev/peps/pep-0257/</p>
<p>Zenodo Website. CERN, "Zenodo Website." [Online]. Available: https://zenodo.org/</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6467 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6467</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6467</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-256827635</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.05507v2.pdf" target="_blank">Language Decision Transformers with Exponential Tilt for Interactive Text Environments</a></p>
                <p><strong>Paper Abstract:</strong> Text-based game environments are challenging because agents must deal with long sequences of text, execute compositional actions using text and learn from sparse rewards. We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs). Our LDTs extend DTs with 3 components: (1) exponential tilt to guide the agent towards high obtainable goals, (2) novel goal conditioning methods yielding better results than the traditional return-to-go (sum of all future rewards), and (3) a model of future observations that improves agent performance. LDTs are the first to address offline RL with DTs on these challenging games. Our experiments show that LDTs achieve the highest scores among many different types of agents on some of the most challenging Jericho games, such as Enchanter.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6467.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6467.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LDT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Decision-Transformer style agent built by fine-tuning a pre-trained LongT5 encoder–decoder on offline game trajectories to predict a goal condition, the next action, and (optionally) the next observation; introduces exponential tilt for sampling high-return goal conditions and an auxiliary next-observation loss (MB-RCP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Language Decision Transformer (LDT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encoder–decoder transformer (pre-trained LongT5-base) fine-tuned offline on noisy walkthrough trajectories; input is a tokenized sequence of observations, goal-conditions and actions (intermediate observations shortened with a placeholder), decoder predicts g_t (goal condition), a_t (action) and optionally o_{t+1} (next observation). Uses exponential tilt on predicted goal-condition probabilities to bias sampling toward high-return goals, and an auxiliary cross-entropy loss to predict next observations (MB-RCP).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LongT5 (pre-trained LongT5-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LongT5-base (parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho interactive fiction benchmark (33 games; experiments focused on enchanter, sorcerer, spellbrkr, spirit, ztuu)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-context transformer context (long‑context window) with intermediate observations collapsed to a placeholder token ('<STATE>')</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>concatenated text tokens representing observations, goal conditions and actions; only the initial and current full observations are fully written, intermediate observations replaced by '<STATE>' placeholder</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>implicit: append new tokens (observation/GC/action) to transformer context during rollout; no explicit external write network — long-context window / truncation and placeholdering used to limit sequence length</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>implicit retrieval via transformer self-attention over the context tokens (no explicit retrieval/indexing module)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>offline supervised fine-tuning of a pre-trained language model on collected trajectories (decision-transformer style reward/goal conditioning); termed offline RL/sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score (absolute) and normalized final score (normalized by best human score), averaged across random seeds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable (no explicit external memory). The paper does report that models trained with the auxiliary next-observation prediction loss (λ=0.5) performed better across all tested games than models without that auxiliary loss; exact numeric deltas are not reliably reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (no explicit external memory). Models trained without the auxiliary next-observation loss (λ=0.0) scored worse on average than λ=0.5 models; numeric values not precisely reported in the body text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Ablations performed include (1) exponential tilt amount α for sampling predicted goal conditions — higher α (e.g., 10 or 20) biases toward higher-return goals and recovers performance close to being conditioned on the optimal goal, (2) different goal-conditioning formulations (Return-to-Go, Immediate Reward, Final Score, Average RTG) — RTG was the weakest and best strategy depended on the game, (3) auxiliary next-observation prediction (λ=0.5) vs no auxiliary loss (λ=0.0) — adding next-observation prediction improved performance across all games. The paper also notes experiments where intermediate observations were encoded via a frozen encoder were inconclusive (resource/context limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>No explicit external memory module; due to compute limits, intermediate observations were replaced with a placeholder token which reduces explicit history retention; earlier attempts to encode all intermediate states with a frozen encoder were inconclusive; context length / resource limits restrict full trajectory encoding and hurt zero-shot generalization to unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use exponential tilt when sampling predicted goal-conditions to bias toward higher-return behaviors (tune α); consider predicting next observations as an auxiliary loss (λ≈0.5) to improve robustness and performance; leverage long-context transformer architectures but mitigate context overflow (e.g., placeholdering or summarization) when trajectories are extremely long.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6467.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6467.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Knowledge-Graph A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online RL approach that constructs and uses a knowledge graph representation of the environment state to guide policy/value learning for text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a knowledge graph from observations to represent state; uses that structured representation with an actor-critic (A2C) style method to learn game policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho/text-based games (prior work evaluated on such environments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (structured symbolic state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>symbolic entities and relations extracted from observations forming a knowledge graph representing the environment state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>rule-based / parser-based graph construction and incremental updates from each observation (as described in prior KG-based work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>graph encoders / graph neural networks (GNN) or graph attention to access relevant nodes/subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>online reinforcement learning (A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score / reward</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Online training is sample-inefficient compared to offline sequence-modeling approaches; constructing and maintaining knowledge graphs requires domain engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Represent long-term structured state with a knowledge graph to capture entities/relations when solving text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6467.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6467.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that leverages structured state representations (knowledge graphs) and RL/Q-learning style approaches to handle long-range dependencies in text games (mentioned as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a knowledge-graph-like state representation to encode environment state/history and learns value/policy functions for text games (described in related work though exact implementation details and title mapping are not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho/text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph / structured state memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>symbolic game state (entities, relations, history graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>graph updates per step from parsed observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>graph encoders / attention over graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>online RL / value-learning approaches</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires domain-specific graph construction and is trained online (sample inefficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use knowledge-graph encodings to capture long-term state for planning in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6467.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6467.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHA-KG (Stacked Hierarchical Attention with Knowledge Graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses graph attention mechanisms to encode game history / knowledge graphs and learn value functions for text-based games (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with stacked hierarchical attention for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes game history using hierarchical attention over a graph-like structure (knowledge graph) to produce state encodings used by RL value/policy learners.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho/text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph + graph attention network</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>graph encoding of the game history / entities and relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>incremental graph updates from textual observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>graph attention networks to focus on relevant nodes/relations</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>online reinforcement learning / value-learning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Online training sample inefficiency; engineering required to build/maintain structured graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Apply graph-attention encoders over structured state representations to capture long-range dependencies in textual environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6467.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6467.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RC-DQN (Reading-Comprehension / Retrieval-based DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that retrieves relevant previous observations (an episodic retrieval buffer), encodes them (e.g., with GRUs), and uses a DQN-like learner to select actions in text games (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieves relevant past observations (to provide context), encodes retrieved context with recurrent encoders (GRUs), and learns Q-values over candidate actions (DQN family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho/text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic retrieval buffer (retrieval of past observations) with recurrent encoding</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw previous observation texts (retrieved relevant observations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append new observations to episodic store; retrieval selects relevant subset for encoding</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>retrieval of relevant previous observations (heuristic/learned retrieval) then GRU encoding</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>online reinforcement learning (DQN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score / reward</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires retrieval heuristics and can be slow when validating many candidate actions; online training costs.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Retrieve and encode a small set of relevant past observations to augment state for Q-value estimation in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6467.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6467.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning (CBR) for Textual RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A case-based memory approach that stores previous interactions (cases) and uses graph attention to compute similarity between current state and stored cases to guide action selection (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Case-based reasoning for better generalization in textual reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language Decision Transformers with Exponential Tilt for Interactive Text Environments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CBR (case-based reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a case base of previous interactions; at decision time, computes similarity between current state and stored cases (using graph attention) to retrieve useful actions or value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho/text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic case memory (case base)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored interaction cases (state/action/outcome tuples) possibly encoded as graph nodes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append new interaction cases to case base; may include indexing or similarity structures</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>graph attention network over stored cases to compute similarity and retrieve matches</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>online RL with case-based augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>final game score / normalized score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Online sample inefficiency; maintaining and searching a large case base can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Store useful past interactions and apply graph-attention similarity to retrieve relevant cases for generalization across textual states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Decision Transformers with Exponential Tilt for Interactive Text Environments', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Case-based reasoning for better generalization in textual reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with stacked hierarchical attention for text-based games <em>(Rating: 2)</em></li>
                <li>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 2)</em></li>
                <li>Decision Transformer: Reinforcement Learning via Sequence Modeling <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6467",
    "paper_id": "paper-256827635",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "LDT",
            "name_full": "Language Decision Transformer",
            "brief_description": "A Decision-Transformer style agent built by fine-tuning a pre-trained LongT5 encoder–decoder on offline game trajectories to predict a goal condition, the next action, and (optionally) the next observation; introduces exponential tilt for sampling high-return goal conditions and an auxiliary next-observation loss (MB-RCP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "Language Decision Transformer (LDT)",
            "agent_description": "Encoder–decoder transformer (pre-trained LongT5-base) fine-tuned offline on noisy walkthrough trajectories; input is a tokenized sequence of observations, goal-conditions and actions (intermediate observations shortened with a placeholder), decoder predicts g_t (goal condition), a_t (action) and optionally o_{t+1} (next observation). Uses exponential tilt on predicted goal-condition probabilities to bias sampling toward high-return goals, and an auxiliary cross-entropy loss to predict next observations (MB-RCP).",
            "model_name": "LongT5 (pre-trained LongT5-base)",
            "model_size": "LongT5-base (parameter count not specified in paper)",
            "benchmark_name": "Jericho interactive fiction benchmark (33 games; experiments focused on enchanter, sorcerer, spellbrkr, spirit, ztuu)",
            "memory_used": false,
            "memory_type": "long-context transformer context (long‑context window) with intermediate observations collapsed to a placeholder token ('&lt;STATE&gt;')",
            "memory_representation": "concatenated text tokens representing observations, goal conditions and actions; only the initial and current full observations are fully written, intermediate observations replaced by '&lt;STATE&gt;' placeholder",
            "memory_update_mechanism": "implicit: append new tokens (observation/GC/action) to transformer context during rollout; no explicit external write network — long-context window / truncation and placeholdering used to limit sequence length",
            "memory_retrieval_method": "implicit retrieval via transformer self-attention over the context tokens (no explicit retrieval/indexing module)",
            "training_method": "offline supervised fine-tuning of a pre-trained language model on collected trajectories (decision-transformer style reward/goal conditioning); termed offline RL/sequence modeling",
            "evaluation_metric": "final game score (absolute) and normalized final score (normalized by best human score), averaged across random seeds",
            "performance_with_memory": "Not applicable (no explicit external memory). The paper does report that models trained with the auxiliary next-observation prediction loss (λ=0.5) performed better across all tested games than models without that auxiliary loss; exact numeric deltas are not reliably reported in the main text.",
            "performance_without_memory": "Not applicable (no explicit external memory). Models trained without the auxiliary next-observation loss (λ=0.0) scored worse on average than λ=0.5 models; numeric values not precisely reported in the body text.",
            "has_comparative_results": false,
            "ablation_findings": "Ablations performed include (1) exponential tilt amount α for sampling predicted goal conditions — higher α (e.g., 10 or 20) biases toward higher-return goals and recovers performance close to being conditioned on the optimal goal, (2) different goal-conditioning formulations (Return-to-Go, Immediate Reward, Final Score, Average RTG) — RTG was the weakest and best strategy depended on the game, (3) auxiliary next-observation prediction (λ=0.5) vs no auxiliary loss (λ=0.0) — adding next-observation prediction improved performance across all games. The paper also notes experiments where intermediate observations were encoded via a frozen encoder were inconclusive (resource/context limitations).",
            "reported_limitations": "No explicit external memory module; due to compute limits, intermediate observations were replaced with a placeholder token which reduces explicit history retention; earlier attempts to encode all intermediate states with a frozen encoder were inconclusive; context length / resource limits restrict full trajectory encoding and hurt zero-shot generalization to unseen games.",
            "best_practices_recommendations": "Use exponential tilt when sampling predicted goal-conditions to bias toward higher-return behaviors (tune α); consider predicting next observations as an auxiliary loss (λ≈0.5) to improve robustness and performance; leverage long-context transformer architectures but mitigate context overflow (e.g., placeholdering or summarization) when trajectories are extremely long.",
            "uuid": "e6467.0",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (Knowledge-Graph A2C)",
            "brief_description": "An online RL approach that constructs and uses a knowledge graph representation of the environment state to guide policy/value learning for text-based games.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "KG-A2C",
            "agent_description": "Builds a knowledge graph from observations to represent state; uses that structured representation with an actor-critic (A2C) style method to learn game policies.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho/text-based games (prior work evaluated on such environments)",
            "memory_used": true,
            "memory_type": "knowledge graph (structured symbolic state memory)",
            "memory_representation": "symbolic entities and relations extracted from observations forming a knowledge graph representing the environment state",
            "memory_update_mechanism": "rule-based / parser-based graph construction and incremental updates from each observation (as described in prior KG-based work)",
            "memory_retrieval_method": "graph encoders / graph neural networks (GNN) or graph attention to access relevant nodes/subgraphs",
            "training_method": "online reinforcement learning (A2C)",
            "evaluation_metric": "final game score / reward",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Online training is sample-inefficient compared to offline sequence-modeling approaches; constructing and maintaining knowledge graphs requires domain engineering.",
            "best_practices_recommendations": "Represent long-term structured state with a knowledge graph to capture entities/relations when solving text-based games.",
            "uuid": "e6467.1",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT",
            "brief_description": "A prior method that leverages structured state representations (knowledge graphs) and RL/Q-learning style approaches to handle long-range dependencies in text games (mentioned as related work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "Q*BERT",
            "agent_description": "Uses a knowledge-graph-like state representation to encode environment state/history and learns value/policy functions for text games (described in related work though exact implementation details and title mapping are not provided in this paper).",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho/text-based games",
            "memory_used": true,
            "memory_type": "knowledge graph / structured state memory",
            "memory_representation": "symbolic game state (entities, relations, history graph)",
            "memory_update_mechanism": "graph updates per step from parsed observations",
            "memory_retrieval_method": "graph encoders / attention over graph structure",
            "training_method": "online RL / value-learning approaches",
            "evaluation_metric": "final game score",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Requires domain-specific graph construction and is trained online (sample inefficiency).",
            "best_practices_recommendations": "Use knowledge-graph encodings to capture long-term state for planning in text games.",
            "uuid": "e6467.2",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "SHA-KG",
            "name_full": "SHA-KG (Stacked Hierarchical Attention with Knowledge Graphs)",
            "brief_description": "A method that uses graph attention mechanisms to encode game history / knowledge graphs and learn value functions for text-based games (cited in related work).",
            "citation_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "mention_or_use": "mention",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "SHA-KG",
            "agent_description": "Encodes game history using hierarchical attention over a graph-like structure (knowledge graph) to produce state encodings used by RL value/policy learners.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho/text-based games",
            "memory_used": true,
            "memory_type": "knowledge graph + graph attention network",
            "memory_representation": "graph encoding of the game history / entities and relations",
            "memory_update_mechanism": "incremental graph updates from textual observations",
            "memory_retrieval_method": "graph attention networks to focus on relevant nodes/relations",
            "training_method": "online reinforcement learning / value-learning",
            "evaluation_metric": "final game score",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Online training sample inefficiency; engineering required to build/maintain structured graphs.",
            "best_practices_recommendations": "Apply graph-attention encoders over structured state representations to capture long-range dependencies in textual environments.",
            "uuid": "e6467.3",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "RC-DQN",
            "name_full": "RC-DQN (Reading-Comprehension / Retrieval-based DQN)",
            "brief_description": "A prior approach that retrieves relevant previous observations (an episodic retrieval buffer), encodes them (e.g., with GRUs), and uses a DQN-like learner to select actions in text games (mentioned in related work).",
            "citation_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "RC-DQN",
            "agent_description": "Retrieves relevant past observations (to provide context), encodes retrieved context with recurrent encoders (GRUs), and learns Q-values over candidate actions (DQN family).",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho/text-based games",
            "memory_used": true,
            "memory_type": "episodic retrieval buffer (retrieval of past observations) with recurrent encoding",
            "memory_representation": "raw previous observation texts (retrieved relevant observations)",
            "memory_update_mechanism": "append new observations to episodic store; retrieval selects relevant subset for encoding",
            "memory_retrieval_method": "retrieval of relevant previous observations (heuristic/learned retrieval) then GRU encoding",
            "training_method": "online reinforcement learning (DQN variants)",
            "evaluation_metric": "final game score / reward",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Requires retrieval heuristics and can be slow when validating many candidate actions; online training costs.",
            "best_practices_recommendations": "Retrieve and encode a small set of relevant past observations to augment state for Q-value estimation in text games.",
            "uuid": "e6467.4",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "CBR",
            "name_full": "Case-Based Reasoning (CBR) for Textual RL",
            "brief_description": "A case-based memory approach that stores previous interactions (cases) and uses graph attention to compute similarity between current state and stored cases to guide action selection (cited in related work).",
            "citation_title": "Case-based reasoning for better generalization in textual reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
            "agent_name": "CBR (case-based reasoning)",
            "agent_description": "Maintains a case base of previous interactions; at decision time, computes similarity between current state and stored cases (using graph attention) to retrieve useful actions or value estimates.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho/text-based games",
            "memory_used": true,
            "memory_type": "external episodic case memory (case base)",
            "memory_representation": "stored interaction cases (state/action/outcome tuples) possibly encoded as graph nodes",
            "memory_update_mechanism": "append new interaction cases to case base; may include indexing or similarity structures",
            "memory_retrieval_method": "graph attention network over stored cases to compute similarity and retrieve matches",
            "training_method": "online RL with case-based augmentation",
            "evaluation_metric": "final game score / normalized score",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Online sample inefficiency; maintaining and searching a large case base can be computationally expensive.",
            "best_practices_recommendations": "Store useful past interactions and apply graph-attention similarity to retrieve relevant cases for generalization across textual states.",
            "uuid": "e6467.5",
            "source_info": {
                "paper_title": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
            "rating": 2,
            "sanitized_title": "casebased_reasoning_for_better_generalization_in_textual_reinforcement_learning"
        },
        {
            "paper_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_stacked_hierarchical_attention_for_textbased_games"
        },
        {
            "paper_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 2,
            "sanitized_title": "interactive_fiction_game_playing_as_multiparagraph_reading_comprehension_with_reinforcement_learning"
        },
        {
            "paper_title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
            "rating": 2,
            "sanitized_title": "decision_transformer_reinforcement_learning_via_sequence_modeling"
        }
    ],
    "cost": 0.017684,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Decision Transformers with Exponential Tilt for Interactive Text Environments
17 Nov 2023</p>
<p>Nicolas Gontier nicolas.gontier@servicenow.com 
Pau Rodriguez 
Issam Laradji 
David Vazquez 
Christopher Pal </p>
<p>ServiceNow Research Quebec Artificial Intelligence Institute (Mila) Polytechnique Montreal Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Montreal
Canada</p>
<p>ServiceNow Research Quebec Artificial Intelligence Institute (Mila) Polytechnique Montreal
Canada</p>
<p>CIFAR AI Chair Montreal
Canada</p>
<p>Language Decision Transformers with Exponential Tilt for Interactive Text Environments
17 Nov 20239D7BC84D2E95D5AD769A5E4FB3638042arXiv:2302.05507v2[cs.CL]
Text-based game environments are challenging because agents must deal with long sequences of text, execute compositional actions using text and learn from sparse rewards.We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs).Our LDTs extend DTs with 3 components:(1) exponential tilt to guide the agent towards high obtainable goals, (2) novel goal conditioning methods yielding better results than the traditional return-to-go (sum of all future rewards), and (3) a model of future observations that improves agent performance.LDTs are the first to address offline RL with DTs on these challenging games.Our experiments show that LDTs achieve the highest scores among many different types of agents on some of the most challenging Jericho games, such as Enchanter.</p>
<p>Introduction</p>
<p>People spend a significant fraction of their lives performing activities closely linked with natural languages, such as having conversations, writing e-mails, filling out forms, reading and writing documents, and so on.Recently, the excitement around the use of Large Language Models (LLMs) for dialogue has brought the setting of interactive dialogue into the spotlight.Interactive text-based games allow one to explore and test interactive agents, alternative neural architectures, and techniques.However, text environments remain challenging for existing Reinforcement Learning (RL) agents since the action space is vast due to the compositional nature of language, making exploration difficult.Fortunately, language has the advantage that knowledge can often be reused across environments, such as the fact that fire burns or that doors open.To solve real-world text-based tasks and play rich text-based games well, RL agents can also benefit from the knowledge about the human world acquired from large offline data sources by leveraging pre-trained LLMs.</p>
<p>In real-world settings, the low-performing behavior exhibited by online RL agents during learning makes them impractical to use with humans in the loop.This situation arises in many other contexts (Levine et al., 2020) and has motivated a lot of research on offline RL.Offline RL methods have a long history, but more recently, several approaches have been proposed that focused on using powerful transformer-based sequence models, including Trajectory Transformers (TTs) (Janner et al., 2021), and Decision Transformers (DTs) (Chen et al., 2021).However, these approaches are formulated and examined within continuous control robotics problems.Unlike the methods above, our approach is designed to handle the complexity and richness of human language by leveraging pre-trained LLMs. Figure 1: Overview of our approach: Noisy trajectories are generated from a high quality game walkthrough by taking 100 random steps at each 5% of the trajectory.The collection of trajectories on multiple games is used to train our LDT model offline to predict a goal condition, next action, and next observation.The LDT is then evaluated in each game environment, initialized with 5 random seeds.</p>
<p>Motivated by the analogy of textgames to intelligent text assistants helping people with various tasks, we assume that a few expensive expert demonstrations are available for learning.As such, we use the Jericho text games (Hausknecht et al., 2020), which provide a single golden path trajectory per game.To create a large and diverse dataset, we then generate trajectories with perturbations from that golden path as described in Section 4 and depicted in Figure 1.The complexity and richness of Jericho games make them a reasonable proxy for the kind of data one might obtain in real-world assistive agent settings.</p>
<p>In this work, we use a pre-trained Transformer language model that we fine-tune on offline game trajectories to predict in order: (i) a numerical trajectory quality measure used to condition the generation of the next actions (termed "goal condition"), (ii) next actions and (iii) future observations.To sample high-quality trajectories from our model, we convert distributions over discrete token representations of goal conditions into continuous ones, allowing us to maximize them through an exponential tilting technique.In addition, we compare different definitions of trajectory quality measures, and introduce an auxiliary loss to predict future observations.We will refer to trajectory quality measures as "goal conditions" in the rest of this paper and our approach as Language Decision Transformers (LDTs) with exponential tilt.Our approach is visualized in Figure 2. See Table 1 for a comparison of how our formulation for density estimation and decision-making is situated with respect to prior frameworks.We also note that none of these previous frameworks have been applied to text-based action spaces, so none have leveraged pre-trained LLMs as in our framework.</p>
<p>To conclude, our contributions can be summarized as follows: (1) Our work is the first to address the challenging Jericho text-based games in an offline return conditioned sequence learning setup, wherein we train models on noisy walkthrough trajectories from multiple games simultaneously.(2) We improve agent behavior with fewer assumptions by letting the model predict goal conditions in a manner where no knowledge of the maximum score is needed through our use of an exponential tilting technique.(Section 5.1).(3) We explore and empirically compare 3 novel definitions of goal conditioning that perform better than the return-to-go perspective of Decision Transformers (DTs).(Section 5.2). ( 4) We propose a novel auxiliary loss to train DTs that draws parallels to model-based RL and empirically shows better performance compared to the traditional model-free loss of DTs (Section 5.3).We test our proposed solutions on 33 different, realistic and complex text environments and show that LDTs performs 10% better than previous baselines on the hardest environments, and up to 30% better on average across all environments.</p>
<p>Methodology</p>
<p>Problem setup</p>
<p>Text-based games can be formulated as partially observable Markov decision processes (POMDP) described by (S, T , A, O, R, γ).The current game state s t ∈ S is partially observable in o t ∈ O which is often a text description of the current scene (inventory, location, items).The agent can take an action a t ∈ A to interact with the environment and causes a state change based on a transition function T (s t , a t ) leading to a new state s t+1 ∈ S. Some games are stochastic in that the same action for the same state can lead to different states.Once the agent transitions to the new state, a reward r t is given by an unknown reward function R(s t , a t ) that the game designers defined.The reward can either be positive, negative, or neutral.Offline Reinforcement Learning.The goal of the agent is to learn a policy π(a t |s t ) which maximizes the expected return E[ T t=0 r t ] in the POMDP by observing a series of static trajectories obtained in the same or similar environments.Each trajectory is defined as τ = (o 0 , a 0 , r 0 , o 1 , a 1 , r 1 , ..., o T , a T , r T ), and it is obtained by observing rollouts of arbitrary policies.This setup is similar to supervised learning, where models are trained from a static dataset.It is more difficult than online reinforcement learning since agents cannot interact with the environment to recollect more data.</p>
<p>Reinforcement Learning in text-based games.One of the main differences between traditional RL environments, such as Atari or Mujoco, and text-based environments is that both A and O consist of text.Therefore, due to the compositional nature of language, A is significantly more complex than in common RL scenarios, where the action space is restricted to a few well-defined actions.</p>
<p>To deal with such complexity, we model A, O and R with a large pre-trained language model: x i = LLM(x i |x 1:i−1 ), where x i is the i th text token in a text sequence of length L. The goal is that the LLM uses its pre-existing knowledge about the world (e.g., doors can be opened), to propose valid actions given an observation.</p>
<p>Decision Transformers.To perform offline learning on text-based games, we adapt the language model (particularly LongT5 (Guo et al., 2022)) to be a decision transformer (DT) (Chen et al., 2021) which abstracts reinforcement learning as a sequential modeling problem.DTs are trained with the language modeling objective on sequences of {g t , o t , a t } T t=0 triples, where the goal condition g t is defined as the undiscounted sum of future rewards, or return-to-go: g t = T i=t r i .Consequently, we have a model that can be conditioned on a desired goal (or return, in this case).In the following subsections, we discuss the novelties we bring to the original formulation of DTs.</p>
<p>Goal conditioning</p>
<p>One limitation of DTs is that the best final score of a game must be known to condition on it at the first step with g 0 (Chen et al., 2021).Although we have g 0 for the training trajectories, it is impossible to know the best target score when starting a new game.This is especially problematic for Jericho games where maximum scores vary greatly between games (Hausknecht et al., 2020).</p>
<p>One solution is to normalize g 0 during training with the maximum game score.This procedure leads to goal conditions between 0 and 1 for the training games and allows to use an initial goal condition of 1 at test time.However, this solution also assumes that we know the maximum score of every game since intermediate rewards returned by the environment r t also need to be normalized: g t+1 = g t − rt max score .To remove the dependence on manual goal conditioning and knowledge of the best obtainable score, we take a similar approach to Lee et al. (2022) and train the model on ordered sequences of {o t , g t , a t } T t=0 triples instead of {g t , o t , a t } T t=0 .Moving the goal condition g t after the observation o t allows us to predict the goal condition based on the current observation by modeling the joint probability of a t and g t as: P θ (a t , g t |o t ) = P θ (a t |g t , o t ) • P θ (g t |o t ).One challenge is that sampling g t can produce low and inaccurate target returns.To mitigate this issue, we perform exponential tilting on the predicted probabilities of g t .In particular we sample g t like so:
g t = argmax g P θ (g t |o t ) • exp(αg t ) ,(1)
with α ≥ 0 being a hyper-parameter that controls the amount of tilting we perform.This allows us to sample high but probable target returns.We compare results with α = {0, 1, 10, 20} in Section 5.1.</p>
<p>Another significant advantage of predicting the goal condition g t based on o t is that we can explore various strategies of goal conditions that cannot be defined manually at inference time.We describe below the original return-to-go used by decision transformers and three novel goal condition strategies.</p>
<p>Return-To-Go (RTG): g t = T i=t r i , is the original strategy of the return-to-go.It is the undiscounted sum of future rewards, which will be high at the beginning of trajectories achieving a high score.These values will decrease as the agent progresses since fewer future rewards will be available in a trajectory with intermediate rewards.</p>
<p>Immediate Reward (ImR): In the setting where g t = r t , each step is conditioned on the reward observed right after the predicted action.We expect that with this goal condition method, the agent will learn what type of actions usually yield higher rewards (opening chest -vs-moving in a direction).We expect this strategy to encourage the model to get high rewards as fast as possible.However, we expect this strategy to work well only for environments with dense reward signals.</p>
<p>Final Score (FinS): g t = T i=0 r i .In this setting, each step is conditioned on the final score achieved by the agent.The final score is defined as the sum of all rewards observed during the entire trajectory.Note that, unlike all the other goal condition definitions, this score will not change over the course of a trajectory.This setting is closer to the traditional RL paradigm in which we often define rewards based on the final performance of an agent: did it win or did it lose.We expect the agent to learn to differentiate successful from unsuccessful trajectories in this setting.Since the model is not conditioned on immediate rewards, we expect it will produce longer trajectories, which can eventually achieve higher final scores.</p>
<p>Average Return-To-Go (AvgRTG): g t = T i=t ri (T −t) .In this setting, each step is conditioned on the average of all future rewards.This is also defined as the return-to-go divided by the number of steps remaining.The motivation for this goal condition is that it will capture the sparsity of rewards in a trajectory, unlike all the others.To reduce the variance in the numbers observed between different games, all goal condition numbers during training are normalized by the maximum score of the current game: g t = int 100 • gt max score .At inference time, we can either manually specify goal condition numbers (assuming we know the game maximum score), or we can let the model predict those goal condition numbers with exponential tilt (more flexible).We experiment with all these goal condition definitions in our experiments and report results in Section 5.2.</p>
<p>Next State Prediction</p>
<p>To give more training signal to the model and make it more robust to stochastic environments, we also experiment with learning to predict the next observation o t+1 .Concretely, we predict o t+1 after taking action a t in state s t .Although the prediction of the next observation is not used to interact with the environment at test time, we believe that the agent will perform better if it can predict how its action will impact the world.Furthermore, predicting the next observation indirectly informs the model about the stochasticity of the environment.This technique draws parallels with the model-based paradigm in Reinforcement Learning, where the agent can predict how the environment will evolve after each action.Formally, the model estimates the following probability:
P θ (o t+1 , a t , g t |o t ) =P θ (o t+1 |a t , g t , o t ) • P θ (a t |g t , o t ) • P θ (g t |o t ),
(2) which is a type of Reward Conditioned Policy (RCP) with the additional term P θ (o t+1 |a t , g t , o t ).We call our technique model-based reward conditioned policy (MB-RCP).We compare our formulation to Density Estimation (L(θ)) Decision Making (π(a|s; η))  2022)).We compare our approach with Decision Transformers (DTs) (Chen et al., 2021), Reward Weighted Regression (RWR) (Peters &amp; Schaal, 2007;Dayan &amp; Hinton, 1997), Reward-Conditioned Policies (RCP) (Kumar et al., 2019) (also used by Multi-Game Decision Transformers (Lee et al., 2022)), Reweighted Behavior Cloning (RBC) (Piché et al., 2019) (also used by Trajectory Transformer (TT) (Janner et al., 2021)), and Implicit RL via supervised learning (IRvS) (Piché et al., 2022).Where s represents the state as encoded by the model and depends on the architecture and inputs used.
DTs log p θ (a t | o t , G t ) p θ (a | s t , G t ) RWR exp(η −1 G t ) log p θ (a t | o t ) p θ (a | s t ) RCP log p θ (a t | o t , G t )p θ (G t | o t ) p θ (a | s t , G)p θ (G | s t ) exp(η −1 G − κ(η)) RBC log p θ (G t | o t , a t )p θ (a t | o t ) p θ (G | s t , a)p θ (a | s t ) exp(η −1 G − κ(η)) IRvS log p θ (a t , G t | o t ) p θ (a, G | s t ) exp(η −1 G − κ(η)) MB-RCP (ours) log[p θ (o t+1 | a t , o t , G t ) p θ (a | s t , G)p θ (G | s t ) exp(η −1 G − κ(η)) •p θ (a t | o t , G t ) • p θ (G t | o t )]
prior work in Table 1.We are interested in using this additional prediction as a form of regularization and therefore treat predicting the next observation as an auxiliary loss, leading to:
L = (1 + λ) −1 L CE ([ĝ t ât ]; [g t a t ]) + λ • L CE (ô t+1 ; o t+1 ) ,(3)
with L CE being the regular cross entropy loss and λ being a hyper-parameter set to 0.5 in all our experiments.This weighted average prevents the model from spending too much of its representation power on the next observation prediction, as it is not strictly required to be able to interact in an environment.At inference time, only the next goal condition and next action predictions will be used.We perform an ablation study on this aspect of our approach by comparing models trained with (λ = 0.5) and without (λ = 0) this auxiliary loss and report our results in Section 5.3.</p>
<p>Related Work</p>
<p>Upside-down RL (UDRL) (Schmidhuber, 2019;Kumar et al., 2019;Piché et al., 2022) poses the task of learning a policy as a supervised learning problem where an agent is conditioned on an observation and a target reward to produce an action.Instead of generating the next action for a target reward, goal conditioning methods generate trajectories conditioned on an end-goal (Ghosh et al., 2019;Paster et al., 2020).Most relevant to our work, Chen et al. (2021) recast supervised RL as a sequence modeling problem with decision transformers (DTs), but they did not examine text environments.DTs have been extended to multi-task environments by training them on multiple Atari games (Lee et al., 2022).To address the problem of modelling text-based environments Furman et al. (2022) proposed DT-BERT for question answering in TextWorld environments (Côté et al., 2018).However, the maximum number of steps in their trajectories is 50, and the environments only differ in their number of rooms and objects.Wang et al. (2022) propose ScienceWorld, a text game environment similar to TextWorld and a Text Decicion Transformer (TDT) baseline.However, their TDT model predicts only the next action based on a given expected return and the previous observation.Here we go a step further and (i) propose different conditioning methods never considered before in DTs, (ii) predict the expected return with exponential tilt rather than relying on expert knowledge to condition on it, and (iii) predict next observation after the next action prediction.In addition, we train our agents on offline trajectories across multiple games and test them on complex and realistic environments using Jericho games (Hausknecht et al., 2020) with diverse dynamics and scenarios.</p>
<p>Jericho is a challenging python framework composed of 33 text-based interactive fiction games (Hausknecht et al., 2020).It was initially introduced with a new Template-DQN, and compared with the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016).However, both methods are trained online, which requires an expensive simulator and requires domain-specific knowledge, such as the set of possible actions.Yao et al. (2020) proposed CALM, extending DRRNs to solve the problem of it needing to know the set of possible actions in advance.They use a GPT-2 (Radford et al., 2019) language model to generate a set of possible candidate actions for each game state.Then, they use an RL agent to select the best action among the (top-k=30) generated ones.</p>
<p>One of the main challenges of leveraging language models to solve Jericho games is to encode the full context of the game trajectory.As such, KG-A2C (Ammanabrolu &amp; Hausknecht, 2020) and Q*BERT (Ammanabrolu et al., 2020) use a knowledge graph to represent the environment state at each step and learn a Q-value function.SHA-KG (Xu et al., 2020) uses graph attention network (Veličković et al., 2018) to encode the game history and learn a value function.RC-DQN (Guo et al., 2020) uses a reading comprehension approach by retrieving relevant previous observations, encoding them with GRUs (Cho et al., 2014), and learning a Q-value function.DBERT-DRRN (Singh et al., 2021) leverages a DistilBERT to encode state and action and feed it to an MLP to learn a Q-value function.XTX (Tuyls et al., 2022) re-visits different frontiers in the state space and performs local exploration to overcome bottleneck states and dead-ends.CBR (Atzeni et al., 2022) stores previous interactions in memory and leverages a graph attention network (Veličković et al., 2018) to encode the similarity between states.The above previous methods are online-based RL, thus suffering from sample inefficiencies.Here, we take a simpler approach by leveraging long-context transformers like LongT5 (Guo et al., 2022) to model the sequence of state observations, target goal scores, and actions of past game trajectories as a sequence of tokens.Then, given a state observation, we leverage exponential tilt (Piché et al., 2022;Lee et al., 2022) to produce the action with the best possible target goal score.We find that our LDT approach is effective enough to outperform all previous methods that we have examined on Jericho games.(Shinn et al., 2023) converts feedback from the environment into natural language sentences used in language based form of reinforcement learning.</p>
<p>Experimental setup</p>
<p>The Jericho Engine Jericho2 is a well-known Python framework that consists of 33 text-based interactive fiction games that are challenging learning environments (Hausknecht et al., 2020).Developers manually create them, each having its own way of defining the rules and goals for each game, making the games quite diverse.Text adventure games are challenging on their own because of their combinatorially large action space and sparse rewards.Usually, text adventure games have a large action vocabulary (around 2000 words on average), and each action is made of multiple words (1 to 4 on average).This makes the action space as big as 2000 4 = 1.6 × 10 13 .To alleviate this issue, the Jericho benchmark provides a list of valid actions for each state.However, this makes the environment much slower as the game engine validates all possible actions against the simulator.In addition, the action space becomes dynamic as it changes from state to state.The above challenge in combination with extremely sparse rewards makes text adventure games very challenging for current RL methods.For brevity3 , we focus on 5 of the hardest Jericho games belonging to the Zork Universe: enchanter, sorcerer, spellbrkr, spirit, and ztuu.We report in Appendix B results on all 33 Jericho games.We generate trajectories for each of these games and train our model on the collection of all trajectories from all games.</p>
<p>Data Collection</p>
<p>Jericho provides one human walkthrough trajectory per game that achieves the maximum score.However, since some games are stochastic, every walkthrough is only valid for a specific default seed when initializing the game.To obtain a more diverse dataset with incorrect or partially correct trajectories, we propose to generate trajectories by following the walkthrough trajectories for some steps and then deviating from them.Concretely, to collect a large number of trajectories with different performances we follow the walkthrough trajectory for X% of its total number of steps and then take 100 additional random steps.We repeat that procedure 10 times for each X ∈ [0, 5, 10, ..., 85, 90, 95].When X = 0%, this is the same as a fully random trajectory.When X = 95%, the agent follows the walkthrough path for 95% of the steps and then takes 100 random steps.This results in a collection of 201 trajectories, including 1 original walkthrough for each game.Note that we also tried to include TDQN and DRRN trajectories trained on individual games, but these agents did not bring any significant information gain in our collection of trajectories.To not overfit on the default seed for each game, we ran the same procedure on 5 different seeds.This resulted in 1,005 trajectories of various lengths and qualities for each game.Note that only 1 of those obtain a 100% final score by following the walkthrough actions given by Jericho.We report in Appendix C the normalized scores (Figure 5) and lengths (Figure 6) observed in the collection of trajectories collected for each game.The top part of Figure 1 illustrates the data generation procedure.</p>
<p>Sequence Definition</p>
<p>To train an encoder-decoder architecture, trajectories are split between input and output sequences after a random number of steps t ∈ [0, T − 1] .The input sequence is then defined as [o 0 , g 0 , a 0 , o 1 , ..., g t−1 , a t−1 , o t ] and the output sequence as [g t , a t , o t+1 ] (also depicted in Figure 2).Each of these {o t , g t , a t } T t=0 elements are represented in natural language text (described below) and concatenated together to form input/output text sequence pairs.a t : intermediate actions are written as returned by agents playing the game, with the addition of special token delimiters: "Action: {a_t} </s></s>".g t : goal conditions are computed with one strategy among the ones described in Section 2.2 based on the list of intermediate rewards returned by the environment.Each goal condition is written in text like this: "GC: {g_t} </s></s>".o t : state observations are defined by multiple state characteristics available to Jericho games: (i) candidate actions available, (ii) the message returned by the game engine, (iii) the description of the current room, and (iv) the current inventory of the agent.Each observation is written in text like this: "Actions: {cand} </s></s> State: {msg} </s></s> Description: {desc} </s></s> Inventory: {inv} </s></s>", with {cand}, {msg}, {desc} and {inv} being the list of candidate actions, the game message, the description of the current room, and the inventory of the player respectively.However, as some game trajectories contain hundreds of steps, the current definition of {o t , g t , a t } T t=0 triples can make input sequences as long as tens of thousands of tokens.To shorten input sequences, we replaced state observations o t to be a single placeholder token "<STATE>" for all intermediate observations except the first (o 0 ) and current one (o t ) as depicted in Figure 2.</p>
<p>Experimental Results</p>
<p>Since Jericho games have long storylines, we leverage LongT5 (Guo et al., 2022), a text-to-text Transformer with a wide attention span.We use the pre-trained LongT5-base model as hosted by HuggingFace4 (Wolf et al., 2020) in all experiments as the base for our encoder-decoder architecture.We then fine-tuned the model for multiple epochs on the generated trajectories from Section 4. The hyperparameter settings can be found in Appendix A.</p>
<p>For each game, we initialize its environment with a random seed.We let the model predict the next goal condition and action at each step.The agent performs the predicted action, leading to the next observation in the environment.The model uses this observation as context for the next step.We run these steps in a cycle until we reach the end of the game and compute the final score.The game ends when the agent reaches the final state, or the model generates an invalid sequence.We repeat this process on 5 random seeds and take the average final score.The bottom part of Figure 1 illustrates the training and evaluation process.</p>
<p>The Effect of Exponential Tilt</p>
<p>In this section, we fine-tuned our model with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).The model was trained with the regular return-to-go goal condition (g t = T i=t r i ) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the model on all games5 , normalized the obtained score based on the maximum human score for each game, and recorded the average across games and 5 random seeds for each game.Predicted GC / alpha=1</p>
<p>Predicted GC / alpha=10</p>
<p>Predicted GC / alpha=20</p>
<p>Imitation Learning TDQN (Hausknecht et al., 2020) DRRN (Hausknecht et al., 2020) KG-A2C (Ammanabrolu &amp; Hausknecht, 2020) CALM (Yao et al., 2020) SHA-KG (Xu et al., 2020) MPRC-DQN (Guo et al., 2020) RC-DQN (Guo et al., 2020) Bike+CBR (Atzeni et al., 2022) Figure 3: Average normalized score across different Jericho games (enchanter, sorcerer, spellbrkr, spirit, ztuu) with various amounts of exponential tilt ("Predicted GC" lines).We also report the performance of a model being conditioned on the optimal goal according to each game's maximum score ("Optimal GC" line).The average normalized score of various baselines is depicted in dotted lines.</p>
<p>To measure the effect of exponential tilt, the predicted g t were sampled according to Equation 1 with α = 0, 1, 10, 20 ("Predicted GC / alpha=α" in Figure 3).We also evaluated the model with the goal condition being manually given ("Optimal GC" in Figure 3) at each step.In the first step, the model is conditioned with g 0 = 100%, and at every next step g t is reduced by the amount of observed reward: g t+1 = g t − rt max score .This "Optimal GC" evaluation assumes we know the game's maximum score.We aim to achieve similar performance by simply predicting g t instead of manually defining it.</p>
<p>We report in Figure 3 the normalized score averaged across games for each method of predicting g t at different training stages of the model.During training the model is exposed to trajectories of various performances (detailed in Figure 5), so without any exponential tilt the model will output the most probable goal condition based on what it observed during training, which is less than ideal (solid red "Predicted GC / alpha=0" line).However, as we prioritize high numerical return-to-go over their likelihood (α increasing), the model's performance is getting closer to the "Optimal GC" performance, with α = 20 (solid orange line) being on par with the model that was manually given the "optimal" goal condition.In realistic scenarios, we do not have the optimal goal condition when starting a new game.In addition, predicting the goal condition offers greater flexibility in the design of goal conditions.We can now explore conditioning methods that would be impossible to define manually during run time.This is exactly what we explore in the next section.Overall, these results demonstrate that: (1) the numerical value of the goal condition has indeed an effect on the quality of the next generated answer, and (2) it is possible to recover the same performance as the "optimal" goal conditioning by increasing the amount of exponential tilt without knowing the game's max score.</p>
<p>Furthermore, we report in Figure 3 the average performance of previous works on the same set of games (dotted horizontal lines).Our offline method with exponential tilt beats previous methods with very little training when α = 10 or 20.However, since all previous methods were trained on each game in an online RL fashion, to fairly compare our approach we also trained an imitation learning (IL) baseline on human trajectories only (semi-dotted pink line).As expected, this offline RL baseline performs better than our approach without exponential tilt as it was trained on better trajectories, but as we increase exponential tilt (α = 10 or 20), our method outperforms the IL strategy.This can be explained by the diversity of interactions our agent saw during training compared to IL.This difference in performance also illustrates the stochasticity of the environments, as the IL baseline would perform close to 100% on deterministic games.</p>
<p>The Effect of Goal Conditioning Strategies</p>
<p>We fine-tuned 4 models with the loss function in Equation (3) on all generated trajectories split into input and output pairs (Section 4).Each model was trained with a different goal condition (Section 2.2) and with λ = 0.5 for the auxiliary loss predicting o t+1 .We tested the models on all games6 after 31.4ktraining steps and recorded the average score across 5 random seeds per game.In these experiments, the model generates goal conditions because at inference time, unlike with return-to-go (RTG), we cannot compute the immediate next reward (ImR) and the average return-to-go (AvgRTG), even if we know the game maximum score.To provide the immediate next  reward condition, we need to know at each step the maximum achievable reward among all candidate actions, which is infeasible in practice.To provide the average RTG condition, we need to know the number of steps remaining after each state, which is infeasible in practice.Fortunately, our model can generate goal conditions while leveraging the exponential tilt for producing better trajectories.All models in these experiments were evaluated by sampling g t according to Equation 1 with α = 10.</p>
<p>Table 2 reports the average score, standard deviation, and best score obtained on each game across 5 random seeds for all goal conditioning methods.Overall, these results show that the classical return-to-go conditioning method yields weaker performance than other methods in all environments.However, the best strategy depends on the game which can vary between ImR, FinS, or AvgRTG.These results further motivate the advantages of generating goal conditions that cannot be computed at runtime such as ImR and AvgRTG.Here we analyze the effect of predicting the next observation o t+1 as part of the loss function.We fine-tuned another 4 models, each with a different goal condition similar to the above section, but with the loss function described in Equation 3 with λ = 0.0 for the auxiliary loss of predicting o t+1 .We tested the models on the same set of games after 31.4ktraining steps and recorded the average score across 5 random seeds for each game.To compare the effect of the auxiliary loss, we averaged the scores across all goal conditioning methods.</p>
<p>The Effect of Predicting the Next Observation
λ = 0.0 λ = 0.
Table 3 reports the average score, standard deviation, and best score obtained on each game over 20 runs (5 random seeds × 4 goal conditioning methods) for models trained with (λ = 0.5) and without (λ = 0.0) the auxiliary loss on the predicted next observation o t+1 .In all games, models trained to predict the next observation o t+1 resulting from the predicted action a t and goal condition g t perform better than models trained to only predict the goal condition g t and next action a t .Overall, these results show that our proposed model-based reward-conditioned policy (MB-RCP) learning objective yields stronger performance than the classical reward-conditioned policy (RCP) objective.</p>
<p>Conclusion</p>
<p>In this work, we have proposed Language Decision Transformers (LDTs) as an offline reinforcement learning method for interactive text environments, and we have performed experiments using the challenging text-based games of Jericho.LDTs are built from pre-trained LLMs followed by training on multiple games simultaneously to predict: the trajectory goal condition, the next action, and the next observation.We have shown that by using exponential tilt, LDT-based agents get much better performance than otherwise.In fact, the model obtains similar performance as if it was conditioned on the optimal goal, despite the fact that in most realistic scenarios, we do not have access to that optimal goal condition.We have also explored different conditioning methods and observed that the traditional return-to-go was the weakest strategy.Finally we have seen that training the model to predict the next observation as an auxiliary loss improves performance.For future work, we plan on extending this framework to multiple and more diverse games and environments.We hope this work can provide a missing piece to the substantial advances in the application of large language models in the context of real-world interactive task-oriented dialogues.</p>
<p>Limitations.One limitation of this work is that we did not spend an extensive amount of effort in building high-quality online RL agents to train our offline agent.This is intended because we use Jericho games as a proxy for real-world chat agents helping people solve problems, and in such environments training a descent online agent is impractical as people would find it very challenging to interact with live RL agents.Another limiting factor of this work is the fact that due to computing resource limitations, intermediate states were replaced with fixed tokens.Earlier experiments tried to encode them with a frozen encoder network but results were inconclusive.Further research in long-context Transformers will eventually alleviate this limitation.Eventually, at current capacity, our models are unable to generalize to unseen games in zero-shot settings.</p>
<p>A Training Details</p>
<p>B Results on All Games</p>
<p>This section reports the results of models trained on all game trajectories at the same time (33 games ×1005 trajectories).Models are then evaluated on each individual games.</p>
<p>B.1 The Effect of Exponential Tilt</p>
<p>Similarly as in Section 5.1, we fine-tuned our model with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).The model was trained with the regular return-to-go goal condition (g t = T i=t r i ) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the model on all games, normalized the obtained score based on the maximum human score for each game, and recorded the average across games and 5 random seeds for each game.Predicted GC / alpha=1</p>
<p>Predicted GC / alpha=10</p>
<p>Predicted GC / alpha=20 TDQN (Hausknecht et al., 2020) DRRN (Hausknecht et al., 2020) MPRC-DQN (Guo et al., 2020) RC-DQN (Guo et al., 2020) Figure 4: Average normalized score across all Jericho games with various amounts of exponential tilt ("Predicted GC" lines).We also report the performance of a model being conditioned on the optimal goal according to each game's maximum score ("Optimal GC" line).The average normalized score of various baselines is depicted in dotted lines.</p>
<p>We can observe the same conclusions as in Section 5.1: (1) the numerical value of the goal condition has indeed an effect on the quality of the next generated answer, (2) it is possible to recover the same performance as the "optimal" goal conditioning by increasing the amount of exponential tilt without knowing the game's maximum score, and (3) our offline method beats previous methods with very little training.Note: fewer baselines are reported than in Section 5.1 because many previous work do not report their performance on so many different environments.</p>
<p>B.2 The Effect of Goal Conditioning Strategies</p>
<p>Similarly as in Section 5.2, we fine-tuned 4 models with the loss function described in Equation 3 on all generated trajectories split into input and output pairs (Section 4).Each model was trained with a different goal condition (Section 2.2) and with λ = 0.5 for the auxiliary loss of predicting o t+1 .We tested the models on all games after 60k training steps and recorded the average score across 5 random seeds for each game.We can observe two things: (1) averaged over a wider set of games, all goal conditioning strategies yield similar performances, except Avg.RTG which is lower, and (2) results on enchanter, sorcerer, spellbrkr, spirit and ztuu are weaker than in Table 2.This is because although models are trained for twice as many steps, the training data is 6 times larger, hence the model observed 3 times less interaction from each game compared to our setting in Section 5.2.</p>
<p>B.3 The Effect of Predicting the Next Observation</p>
<p>Similarly as in Section 5.3, we fine-tuned another 4 models, each with a different goal condition similar to the above section, but with the loss function described in Equation 3 with λ = 0.0 for the auxiliary loss of predicting o t+1 .We tested the models on all games after 60k training steps and recorded the average score across 5 random seeds for each game.To compare the effect of the auxiliary loss, we averaged the scores across all goal conditioning methods.</p>
<p>Figure 2 :
2
Figure 2: Our Language Decision Transformer framework.A trajectory of length T is split at a random index t ∈ [0, T − 1].The model encodes the sequence of observations (o), goal conditions (g), and actions (a) up to time step t.The first o 1 and last o t observations are fully written, but to shorten the input sequence, the other intermediate observations are replaced by a special token.The decoder predicts the goal condition g t , action to take a t , and next observation o t+1 .</p>
<p>Other prior work examining text based agents and leveraging LLMs include: The SayCan work of Ahn et al. (2022) using LLMs as a value functions in a reinforcement learning setup for completing tasks in a real world robotics setting; the ReAct work of Yao et al. (2023) examines a prompt based few shot in-context learning solution based on a PaLM-540B model; and Reflexion</p>
<p>Figure 5 :
5
Figure 5: Proportion of trajectory normalized scores.In each sub-figure title, n is the number of trajectories and ms is the maximum score.The X-axis is the normalized score the trajectory achieves.The Y-axis is the proportion of trajectories finishing with that score.</p>
<p>Figure 6 :
6
Figure 6: Proportion of trajectory lengths.In each sub-figure title, n is the number of trajectories.The X-axis is the number of steps in a trajectory.The Y-axis is the proportion of trajectories of that length.</p>
<p>Table 1 :
1
Comparison of different policy training and action selection techniques (adapted from Piché et al. (</p>
<p>Table 2 :
2
Average and best score obtained on each game across 5 random seeds for each goal condition (GC) variation.The bottom row is the normalized average based on the best human score.</p>
<p>Table 3 :
3
Average and best score obtained on each game across 5 random seeds and 4 goal conditioning methods, with (λ = 0.5) and without (λ = 0.0) the auxiliary loss on the prediction of the next observation o t+1 .The bottom row is the normalized average based on the best human score.</p>
<p>Table 4 :
4
Average and best score obtained on each game across 5 random seeds for each goal condition (GC) variation.The bottom row is the normalized average based on the best human score.</p>
<p>https://github.com/microsoft/jericho
These games are also used in previous works, allowing us to compare our results. Most prior work has only evaluated on a subset of the 33 environments. We report in Appendix B results on all 33 games.
https://huggingface.co/google/long-t5-tglobal-base
games seen at training time: enchanter, sorcerer, spellbrkr, spirit, ztuu. We report in Appendix B results on all 33 games.
Games seen at training time: enchanter, sorcerer, spellbrkr, spirit, ztuu. We report in Appendix B results on all 33 games.
C Trajectories StatisticsIn this section, we report the normalized scores (Figure5) and lengths (Figure6) observed in the collection of trajectories collected for each game as described in Section 4.C.1 Trajectories Scores C.2 Trajectories Lengths
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations (ICLR). 2020</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Case-based reasoning for better generalization in textual reinforcement learning. Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, Mrinmaya Sachan, International Conference on Learning Representations (ICLR). 2022</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, Yoshua Bengio, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8). 2014</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. 2018</p>
<p>Using expectation-maximization for reinforcement learning. Peter Dayan, Geoffrey E Hinton, Neural Computation. 921997</p>
<p>A sequence modelling approach to question answering in text-based games. Greg Furman, Edan Toledo, Jonathan Shock, Jan Buys, Association for Computational Linguistics (ACL), 2022. Dibya Ghosh, Abhishek Gupta. Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach, Sergey Levine, OpenReview2019Learning to reach goals without reinforcement learning</p>
<p>LongT5: Efficient text-to-text transformer for long sequences. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang, American Association for Computational Linguistics2022</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Conference on Artificial Intelligence (AAAI). 2020</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 2016Association for Computational Linguistics</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>. Aviral Kumar, Xue Bin Peng, Sergey Levine, arXiv:1912.134652019Reward-conditioned policies. arXiv preprint</p>
<p>Multi-game decision transformers. Kuang-Huei Lee, Ofir Nachum, Sherry Yang, Lisa Lee, C Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, Igor Mordatch, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, arXiv:2005.016432020arXiv preprint</p>
<p>Planning from pixels using inverse dynamics models. Keiran Paster, Sheila A Mcilraith, Jimmy Ba, arXiv:2012.024192020arXiv preprint</p>
<p>Reinforcement learning by reward-weighted regression for operational space control. Jan Peters, Stefan Schaal, International Conference on Machine learning (ICML). 2007</p>
<p>Probabilistic planning with sequential monte carlo methods. Alexandre Piché, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, Chris Pal, International Conference on Learning Representations (ICLR). 2019</p>
<p>A probabilistic perspective on reinforcement learning via supervised learning. Alexandre Piché, Rafael Pardinas, David Vazquez, Christopher Pal, In ICLR Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Reinforcement learning upside down: Don't predict rewards-just map them to actions. Juergen Schmidhuber, arXiv:1912.028752019arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, arXiv:2107.084082021arXiv preprint</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations (ICLR). 2022</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations (ICLR). 2018</p>
<p>ScienceWorld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 10.18653/v1/2022.emnlp-main.775Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP). 2020</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 2020</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, arXiv:2010.029032020arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>and without (λ = 0.0) the auxiliary loss on the prediction of the next observation o t+1 . The bottom row is the normalized average based on the best human score. We can observe the same conclusions as in Section 5.3: models trained to predict the next observation o t+1 resulting from the predicted action a t and goal condition g t perform better than models trained to only predict the goal condition g t and next action a t . Overall, these results show that our proposed model-based reward-conditioned policy (MB-RCP). 32.78% 54.45% 50.78% 7378% Table 5: Average and best score obtained on each game across 5 random seeds and 4 goal conditioning methods, with (λ = 0.5). Average Normalized. learning objective yields stronger performance than the classical reward-conditioned policy (RCP) objective</p>            </div>
        </div>

    </div>
</body>
</html>
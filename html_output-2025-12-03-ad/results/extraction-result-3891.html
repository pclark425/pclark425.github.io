<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3891 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3891</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3891</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-94.html">extraction-schema-94</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-264393368</p>
                <p><strong>Paper Title:</strong> Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET</p>
                <p><strong>Paper Abstract:</strong> Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of people worldwide. Early and accurate prediction of AD progression is crucial for early intervention and personalized treatment planning. Although AD does not yet have a reliable therapy, several medications help slow down the disease’s progression. However, more study is still needed to develop reliable methods for detecting AD and its phases. In the recent past, biomarkers associated with AD have been identified using neuroimaging methods. To uncover biomarkers, deep learning techniques have quickly emerged as a crucial methodology. A functional molecular imaging technique known as fluorodeoxyglucose positron emission tomography (18F-FDG-PET) has been shown to be effective in assisting researchers in understanding the morphological and neurological alterations to the brain associated with AD. Convolutional neural networks (CNNs) have also long dominated the field of AD progression and have been the subject of substantial research, while more recent approaches like vision transformers (ViT) have not yet been fully investigated. In this paper, we present a self-supervised learning (SSL) method to automatically acquire meaningful AD characteristics using the ViT architecture by pretraining the feature extractor using the self-distillation with no labels (DINO) and extreme learning machine (ELM) as classifier models. In this work, we examined a technique for predicting mild cognitive impairment (MCI) to AD utilizing an SSL model which learns powerful representations from unlabeled 18F-FDG PET images, thus reducing the need for large-labeled datasets. In comparison to several earlier approaches, our strategy showed state-of-the-art classification performance in terms of accuracy (92.31%), specificity (90.21%), and sensitivity (95.50%). Then, to make the suggested model easier to understand, we highlighted the brain regions that significantly influence the prediction of MCI development. Our methods offer a precise and efficient strategy for predicting the transition from MCI to AD. In conclusion, this research presents a novel Explainable SSL-ViT model that can accurately predict AD progress based on 18F-FDG PET scans. SSL, attention, and ELM mechanisms are integrated into the model to make it more predictive and interpretable. Future research will enable the development of viable treatments for neurodegenerative disorders by combining brain areas contributing to projection with observed anatomical traits.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3891.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3891.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Amyloid accumulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amyloid-β accumulation / abnormal protein deposits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accumulation of abnormal protein deposits (amyloid-β) in brain tissue is described as a central, hypothesized driver of neuronal loss in Alzheimer's disease and a target for amyloid PET imaging biomarkers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Deposition of abnormal amyloid protein in the brain leading to neural dysfunction and degeneration (amyloid cascade hypothesis).</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Presented as background (literature-cited) evidence only; the paper states amyloid accumulation is ‘‘one of the causes of AD’’ and that amyloid PET can visualize deposited amyloid. No new experimental amyloid data are produced in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Amyloid PET (tracer injection and PET imaging) — mentioned as an established functional imaging biomarker for amyloid deposition.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Amyloid deposits visualized by amyloid-PET tracers (general reference; specific tracer names not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Described as useful for early/preclinical detection in the introduction (prodromal/preclinical stages).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Background citation to prior human imaging studies / review-level statements; no primary amyloid-data from this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Paper notes that more study is needed and that AD lacks a reliable therapy; amyloid is discussed as only one cause/biomarker and not definitive alone. The present study does not evaluate amyloid PET performance and points out need for multimodal approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3891.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3891.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Glucose hypometabolism (FDG signature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regional brain glucose hypometabolism measured by 18F-FDG-PET</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reduced regional glucose uptake on 18F-FDG-PET (hypometabolism) is described as a functional correlate of neuronal dysfunction in AD and is used here as the primary imaging biomarker for MCI→AD progression prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Regional neuronal dysfunction/degeneration leading to reduced glucose metabolism in specific brain regions (interpreted as a mechanistic signature of AD-related neurodegeneration).</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Evidence in this paper: analysis of ADNI 18F-FDG-PET (n=469; 224 converters (MCI-c) and 245 stable (MCI-s)) with attention maps and model feature importance showing hypometabolism in thalamus, medial frontal, hippocampus, posterior temporal lobe, parietal lobe, posterior cingulate gyrus, left parahippocampal gyrus and occipital regions — consistent with prior FDG literature cited. This is observational human imaging evidence (retrospective ADNI cohort).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>18F-FDG-PET imaging processed (PVE correction, coregistration to T1, MNI normalization, smoothing, intensity normalization); features learned by self-supervised ViT (DINO) then classified by ELM/SVM/KNN.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Regional decreases in FDG uptake (hypometabolism) in temporo-parietal, posterior cingulate, hippocampal and related regions; attention maps from ViT-DINO highlight thalamus, medial frontal, hippocampus, posterior temporal, parietal, posterior cingulate, left parahippocampal gyrus and occipital regions as predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Using 18F-FDG-PET features with ViT-DINO feature extractor + ELM classifier on ADNI MCI baseline images to predict conversion within 3 years: accuracy 92.31% ± 1.07, sensitivity 90.21% ± 3.37, specificity 95.50% ± 2.15, AUC = 0.96, F1-score 93.92% ± 1.33 (percent ± SD). (Comparative results reported for KNN and SVM also shown.)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Prodromal / mild cognitive impairment (MCI) — prediction of conversion from MCI to AD within 3 years.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Retrospective human imaging study (computational analysis) using ADNI FDG-PET baseline scans and clinical follow-up to label converters vs stable MCI.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors emphasize FDG-PET is not a definitive AD biomarker; similar hypometabolic patterns can occur in other dementias (e.g., DLB, FTD), limiting specificity. Additional limitations noted: model used 2D slices (not full 3D), domain shift from ImageNet pretraining, requirement for two-stage training (pretrain + classifier), and generalization needs validation on larger/multimodal datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3891.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3891.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>18F-FDG-PET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluorodeoxyglucose positron emission tomography (18F-FDG-PET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional molecular imaging technique that measures regional brain glucose metabolism and is used here as the primary input biomarker for automated prediction of MCI→AD conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>This paper uses ADNI 18F-FDG-PET scans (dynamic acquisition, summed images acquired ~30 min after injection) preprocessed with PVE correction, co-registration to T1, MNI normalization, smoothing and intensity normalization; FDG metabolic patterns are linked to conversion risk via model-derived features and attention maps.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Acquisition and standardized preprocessing pipeline followed by slice extraction (2D coronal/axial/sagittal) and input into self-supervised ViT (DINO) for feature learning, followed by classifiers (ELM, SVM, KNN).</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Global and regional FDG uptake patterns (normalized intensity values), with regions of reduced uptake as predictive features (see hypometabolism regions listed).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>When FDG-PET features learned by ViT-DINO were classified by ELM: accuracy 92.31%, sensitivity 90.21%, specificity 95.50%, AUC 0.96, F1 93.92% (percent).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>MCI (prediction of conversion within 3 years).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human observational dataset analysis (ADNI), n=469 MCI subjects with minimum 36-month follow-up to label converters vs stable cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note FDG-PET lacks perfect specificity (other dementias may show similar metabolic changes), preprocessing choices (PVE correction, smoothing) and choice to slice into 2D may omit 3D context; the method needs multimodal validation and larger external cohorts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3891.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3891.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViT-DINO-ELM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-supervised Vision Transformer (DINO-pretrained ViT) with Extreme Learning Machine classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage automated pipeline that self-supervised-pretrains a Vision Transformer (DINO) on 18F-FDG-PET slices to learn features, then classifies MCI converters vs stable using an Extreme Learning Machine (ELM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Self-supervised representation learning (DINO) on ViT backbone (ViT-B, patch size 16 preferred) applied to preprocessed 2D FDG-PET slices; extracted features fed to ELM (multilayer ELM with 300 hidden nodes) for supervised classification of conversion labels.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Automatically learned FDG-PET feature embeddings that capture regional hypometabolism; attention maps localize predictive brain regions (thalamus, medial frontal, hippocampus, posterior temporal, parietal, posterior cingulate, parahippocampal gyrus, occipital).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Best reported pipeline (DINO ViT-B + ELM): accuracy 92.31% ± 1.07, sensitivity 90.21% ± 3.37, specificity 95.50% ± 2.15, AUC 0.96, F1-score 93.92% ± 1.33 (percent ± SD) on ADNI MCI cohort (5-fold cross-validation).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>MCI (prediction of conversion to AD within 3 years).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Computational/machine-learning analysis applied to human ADNI FDG-PET data (retrospective classification experiment with cross-validation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Model limitations stated by authors: two-stage training complexity; use of 2D slices instead of full 3D volumes (possible loss of global anatomical context); dependence on quality of learned features in attention layers; domain mismatch from ImageNet pretraining (mitigated by DINO self-supervision but still a concern); possible conflation with other dementias that have similar FDG patterns; need for multimodal data and external validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3891.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3891.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cognitive tests (MMSE / CDR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mini-Mental State Examination (MMSE) and Clinical Dementia Rating (CDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard bedside cognitive assessments used as clinical measures in the ADNI dataset and reported as group-level covariates differing between MCI converters and stable MCI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Neuropsychological / clinical cognitive testing (MMSE score, CDR score reported in demographics and group comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>MMSE and CDR scores differed across groups in this dataset (authors report MMSE and CDR varied across group pairings with p < 0.05), indicating worse baseline cognition in converters.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Not presented as an automated classifier in this study; statistical group differences reported (p < 0.05) but no sensitivity/specificity metrics are given for MMSE/CDR alone within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>MCI / prodromal stage (used to define groups and clinical baseline status).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Clinical neuropsychological assessment data from ADNI (human observational cohort).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Cognitive tests are non-specific (can be affected by education, comorbidities); authors use them as descriptors and not as the primary classification input; alone they are insufficient for optimal early prediction compared to imaging+ML pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3891.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3891.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APOE ε4 genotyping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Apolipoprotein E (APOE) ε4 allele genotyping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Genetic risk marker for Alzheimer's disease reported in cohort demographics; higher APOE ε4 positivity observed in converters versus stable MCI in the ADNI subset used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>APOE ε4 allele is a genetic risk factor associated with increased likelihood of AD pathology and earlier onset.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Demographic reporting in Table 1: APOE ε4 positivity reported per group (authors state MCI-c had a higher probability of developing AD and report higher APOE ε4 rate); however the genotyping data were not used as features in the trained ML classifiers in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Genetic testing (APOE genotyping) as a risk biomarker; only included in cohort descriptive statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>APOE ε4 carrier status (percentages reported in cohort table; e.g., MCI-c higher than MCI-s as reported in the manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Risk marker across preclinical/prodromal stages; used here only for cohort characterization.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human genetic/demographic data from ADNI cohort; not included in classifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Not integrated into prediction pipeline here; APOE is a risk marker (not deterministic) and the paper does not provide performance metrics for genotyping as a predictor in this analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Amyloid-β Pathway in Alzheimer's Disease <em>(Rating: 2)</em></li>
                <li>Toward a biological definition of Alzheimer's disease <em>(Rating: 2)</em></li>
                <li>FDG-PET and CSF biomarker accuracy in prediction of conversion to different dementias in a large multicentre MCI cohort <em>(Rating: 2)</em></li>
                <li>Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging <em>(Rating: 2)</em></li>
                <li>Added value of semiquantitative analysis of brain FDG-PET for the differentiation between MCI-Lewy bodies and MCI due to Alzheimer's disease <em>(Rating: 2)</em></li>
                <li>Vision transformers for the prediction of mild cognitive impairment to Alzheimer's disease progression using mid-sagittal sMRI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3891",
    "paper_id": "paper-264393368",
    "extraction_schema_id": "extraction-schema-94",
    "extracted_data": [
        {
            "name_short": "Amyloid accumulation",
            "name_full": "Amyloid-β accumulation / abnormal protein deposits",
            "brief_description": "Accumulation of abnormal protein deposits (amyloid-β) in brain tissue is described as a central, hypothesized driver of neuronal loss in Alzheimer's disease and a target for amyloid PET imaging biomarkers.",
            "citation_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET",
            "mention_or_use": "mention",
            "proposed_cause": "Deposition of abnormal amyloid protein in the brain leading to neural dysfunction and degeneration (amyloid cascade hypothesis).",
            "cause_evidence": "Presented as background (literature-cited) evidence only; the paper states amyloid accumulation is ‘‘one of the causes of AD’’ and that amyloid PET can visualize deposited amyloid. No new experimental amyloid data are produced in this study.",
            "detection_method": "Amyloid PET (tracer injection and PET imaging) — mentioned as an established functional imaging biomarker for amyloid deposition.",
            "biomarker_or_finding": "Amyloid deposits visualized by amyloid-PET tracers (general reference; specific tracer names not provided in this paper).",
            "detection_performance": null,
            "detection_stage": "Described as useful for early/preclinical detection in the introduction (prodromal/preclinical stages).",
            "study_type": "Background citation to prior human imaging studies / review-level statements; no primary amyloid-data from this study.",
            "limitations_or_counter_evidence": "Paper notes that more study is needed and that AD lacks a reliable therapy; amyloid is discussed as only one cause/biomarker and not definitive alone. The present study does not evaluate amyloid PET performance and points out need for multimodal approaches.",
            "uuid": "e3891.0",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Glucose hypometabolism (FDG signature)",
            "name_full": "Regional brain glucose hypometabolism measured by 18F-FDG-PET",
            "brief_description": "Reduced regional glucose uptake on 18F-FDG-PET (hypometabolism) is described as a functional correlate of neuronal dysfunction in AD and is used here as the primary imaging biomarker for MCI→AD progression prediction.",
            "citation_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET",
            "mention_or_use": "use",
            "proposed_cause": "Regional neuronal dysfunction/degeneration leading to reduced glucose metabolism in specific brain regions (interpreted as a mechanistic signature of AD-related neurodegeneration).",
            "cause_evidence": "Evidence in this paper: analysis of ADNI 18F-FDG-PET (n=469; 224 converters (MCI-c) and 245 stable (MCI-s)) with attention maps and model feature importance showing hypometabolism in thalamus, medial frontal, hippocampus, posterior temporal lobe, parietal lobe, posterior cingulate gyrus, left parahippocampal gyrus and occipital regions — consistent with prior FDG literature cited. This is observational human imaging evidence (retrospective ADNI cohort).",
            "detection_method": "18F-FDG-PET imaging processed (PVE correction, coregistration to T1, MNI normalization, smoothing, intensity normalization); features learned by self-supervised ViT (DINO) then classified by ELM/SVM/KNN.",
            "biomarker_or_finding": "Regional decreases in FDG uptake (hypometabolism) in temporo-parietal, posterior cingulate, hippocampal and related regions; attention maps from ViT-DINO highlight thalamus, medial frontal, hippocampus, posterior temporal, parietal, posterior cingulate, left parahippocampal gyrus and occipital regions as predictive.",
            "detection_performance": "Using 18F-FDG-PET features with ViT-DINO feature extractor + ELM classifier on ADNI MCI baseline images to predict conversion within 3 years: accuracy 92.31% ± 1.07, sensitivity 90.21% ± 3.37, specificity 95.50% ± 2.15, AUC = 0.96, F1-score 93.92% ± 1.33 (percent ± SD). (Comparative results reported for KNN and SVM also shown.)",
            "detection_stage": "Prodromal / mild cognitive impairment (MCI) — prediction of conversion from MCI to AD within 3 years.",
            "study_type": "Retrospective human imaging study (computational analysis) using ADNI FDG-PET baseline scans and clinical follow-up to label converters vs stable MCI.",
            "limitations_or_counter_evidence": "Authors emphasize FDG-PET is not a definitive AD biomarker; similar hypometabolic patterns can occur in other dementias (e.g., DLB, FTD), limiting specificity. Additional limitations noted: model used 2D slices (not full 3D), domain shift from ImageNet pretraining, requirement for two-stage training (pretrain + classifier), and generalization needs validation on larger/multimodal datasets.",
            "uuid": "e3891.1",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "18F-FDG-PET",
            "name_full": "Fluorodeoxyglucose positron emission tomography (18F-FDG-PET)",
            "brief_description": "A functional molecular imaging technique that measures regional brain glucose metabolism and is used here as the primary input biomarker for automated prediction of MCI→AD conversion.",
            "citation_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": "This paper uses ADNI 18F-FDG-PET scans (dynamic acquisition, summed images acquired ~30 min after injection) preprocessed with PVE correction, co-registration to T1, MNI normalization, smoothing and intensity normalization; FDG metabolic patterns are linked to conversion risk via model-derived features and attention maps.",
            "detection_method": "Acquisition and standardized preprocessing pipeline followed by slice extraction (2D coronal/axial/sagittal) and input into self-supervised ViT (DINO) for feature learning, followed by classifiers (ELM, SVM, KNN).",
            "biomarker_or_finding": "Global and regional FDG uptake patterns (normalized intensity values), with regions of reduced uptake as predictive features (see hypometabolism regions listed).",
            "detection_performance": "When FDG-PET features learned by ViT-DINO were classified by ELM: accuracy 92.31%, sensitivity 90.21%, specificity 95.50%, AUC 0.96, F1 93.92% (percent).",
            "detection_stage": "MCI (prediction of conversion within 3 years).",
            "study_type": "Human observational dataset analysis (ADNI), n=469 MCI subjects with minimum 36-month follow-up to label converters vs stable cases.",
            "limitations_or_counter_evidence": "Authors note FDG-PET lacks perfect specificity (other dementias may show similar metabolic changes), preprocessing choices (PVE correction, smoothing) and choice to slice into 2D may omit 3D context; the method needs multimodal validation and larger external cohorts.",
            "uuid": "e3891.2",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ViT-DINO-ELM",
            "name_full": "Self-supervised Vision Transformer (DINO-pretrained ViT) with Extreme Learning Machine classifier",
            "brief_description": "A two-stage automated pipeline that self-supervised-pretrains a Vision Transformer (DINO) on 18F-FDG-PET slices to learn features, then classifies MCI converters vs stable using an Extreme Learning Machine (ELM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "Self-supervised representation learning (DINO) on ViT backbone (ViT-B, patch size 16 preferred) applied to preprocessed 2D FDG-PET slices; extracted features fed to ELM (multilayer ELM with 300 hidden nodes) for supervised classification of conversion labels.",
            "biomarker_or_finding": "Automatically learned FDG-PET feature embeddings that capture regional hypometabolism; attention maps localize predictive brain regions (thalamus, medial frontal, hippocampus, posterior temporal, parietal, posterior cingulate, parahippocampal gyrus, occipital).",
            "detection_performance": "Best reported pipeline (DINO ViT-B + ELM): accuracy 92.31% ± 1.07, sensitivity 90.21% ± 3.37, specificity 95.50% ± 2.15, AUC 0.96, F1-score 93.92% ± 1.33 (percent ± SD) on ADNI MCI cohort (5-fold cross-validation).",
            "detection_stage": "MCI (prediction of conversion to AD within 3 years).",
            "study_type": "Computational/machine-learning analysis applied to human ADNI FDG-PET data (retrospective classification experiment with cross-validation).",
            "limitations_or_counter_evidence": "Model limitations stated by authors: two-stage training complexity; use of 2D slices instead of full 3D volumes (possible loss of global anatomical context); dependence on quality of learned features in attention layers; domain mismatch from ImageNet pretraining (mitigated by DINO self-supervision but still a concern); possible conflation with other dementias that have similar FDG patterns; need for multimodal data and external validation.",
            "uuid": "e3891.3",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Cognitive tests (MMSE / CDR)",
            "name_full": "Mini-Mental State Examination (MMSE) and Clinical Dementia Rating (CDR)",
            "brief_description": "Standard bedside cognitive assessments used as clinical measures in the ADNI dataset and reported as group-level covariates differing between MCI converters and stable MCI.",
            "citation_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "Neuropsychological / clinical cognitive testing (MMSE score, CDR score reported in demographics and group comparisons).",
            "biomarker_or_finding": "MMSE and CDR scores differed across groups in this dataset (authors report MMSE and CDR varied across group pairings with p &lt; 0.05), indicating worse baseline cognition in converters.",
            "detection_performance": "Not presented as an automated classifier in this study; statistical group differences reported (p &lt; 0.05) but no sensitivity/specificity metrics are given for MMSE/CDR alone within this paper.",
            "detection_stage": "MCI / prodromal stage (used to define groups and clinical baseline status).",
            "study_type": "Clinical neuropsychological assessment data from ADNI (human observational cohort).",
            "limitations_or_counter_evidence": "Cognitive tests are non-specific (can be affected by education, comorbidities); authors use them as descriptors and not as the primary classification input; alone they are insufficient for optimal early prediction compared to imaging+ML pipeline.",
            "uuid": "e3891.4",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "APOE ε4 genotyping",
            "name_full": "Apolipoprotein E (APOE) ε4 allele genotyping",
            "brief_description": "Genetic risk marker for Alzheimer's disease reported in cohort demographics; higher APOE ε4 positivity observed in converters versus stable MCI in the ADNI subset used.",
            "citation_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET",
            "mention_or_use": "mention",
            "proposed_cause": "APOE ε4 allele is a genetic risk factor associated with increased likelihood of AD pathology and earlier onset.",
            "cause_evidence": "Demographic reporting in Table 1: APOE ε4 positivity reported per group (authors state MCI-c had a higher probability of developing AD and report higher APOE ε4 rate); however the genotyping data were not used as features in the trained ML classifiers in this study.",
            "detection_method": "Genetic testing (APOE genotyping) as a risk biomarker; only included in cohort descriptive statistics.",
            "biomarker_or_finding": "APOE ε4 carrier status (percentages reported in cohort table; e.g., MCI-c higher than MCI-s as reported in the manuscript).",
            "detection_performance": null,
            "detection_stage": "Risk marker across preclinical/prodromal stages; used here only for cohort characterization.",
            "study_type": "Human genetic/demographic data from ADNI cohort; not included in classifier training.",
            "limitations_or_counter_evidence": "Not integrated into prediction pipeline here; APOE is a risk marker (not deterministic) and the paper does not provide performance metrics for genotyping as a predictor in this analysis.",
            "uuid": "e3891.5",
            "source_info": {
                "paper_title": "Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer’s Disease Progression Using 18F-FDG PET",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Amyloid-β Pathway in Alzheimer's Disease",
            "rating": 2,
            "sanitized_title": "the_amyloidβ_pathway_in_alzheimers_disease"
        },
        {
            "paper_title": "Toward a biological definition of Alzheimer's disease",
            "rating": 2,
            "sanitized_title": "toward_a_biological_definition_of_alzheimers_disease"
        },
        {
            "paper_title": "FDG-PET and CSF biomarker accuracy in prediction of conversion to different dementias in a large multicentre MCI cohort",
            "rating": 2,
            "sanitized_title": "fdgpet_and_csf_biomarker_accuracy_in_prediction_of_conversion_to_different_dementias_in_a_large_multicentre_mci_cohort"
        },
        {
            "paper_title": "Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging",
            "rating": 2,
            "sanitized_title": "predicting_cognitive_decline_with_deep_learning_of_brain_metabolism_and_amyloid_imaging"
        },
        {
            "paper_title": "Added value of semiquantitative analysis of brain FDG-PET for the differentiation between MCI-Lewy bodies and MCI due to Alzheimer's disease",
            "rating": 2,
            "sanitized_title": "added_value_of_semiquantitative_analysis_of_brain_fdgpet_for_the_differentiation_between_mcilewy_bodies_and_mci_due_to_alzheimers_disease"
        },
        {
            "paper_title": "Vision transformers for the prediction of mild cognitive impairment to Alzheimer's disease progression using mid-sagittal sMRI",
            "rating": 1,
            "sanitized_title": "vision_transformers_for_the_prediction_of_mild_cognitive_impairment_to_alzheimers_disease_progression_using_midsagittal_smri"
        }
    ],
    "cost": 0.01651625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET
20 October 2023</p>
<p>Uttam Khatri uttamkhatri03@gmail.com 
Department of Information and Communication Engineering
Chosun University
309 Pilmun-daero, Dong-gu61452GwangjuRepublic of Korea</p>
<p>Goo-Rak Kwon grkwon@chosun.ac.kr 0000-0003-3486-8812
Department of Information and Communication Engineering
Chosun University
309 Pilmun-daero, Dong-gu61452GwangjuRepublic of Korea</p>
<p>Explainable Vision Transformer with Self-Supervised Learning to Predict Alzheimer's Disease Progression Using 18F-FDG PET
20 October 20238FB17F189EB54DB7034B8DEB468BE6C610.3390/bioengineering10101225Received: 20 September 2023 Revised: 17 October 2023 Accepted: 18 October 2023Khatri, U.Kwon, G.-R Alzheimer's diseasevision transformerELMFDG-PETself-supervised learningDINO
Alzheimer's disease (AD) is a progressive neurodegenerative disorder that affects millions of people worldwide.Early and accurate prediction of AD progression is crucial for early intervention and personalized treatment planning.Although AD does not yet have a reliable therapy, several medications help slow down the disease's progression.However, more study is still needed to develop reliable methods for detecting AD and its phases.In the recent past, biomarkers associated with AD have been identified using neuroimaging methods.To uncover biomarkers, deep learning techniques have quickly emerged as a crucial methodology.A functional molecular imaging technique known as fluorodeoxyglucose positron emission tomography (18F-FDG-PET) has been shown to be effective in assisting researchers in understanding the morphological and neurological alterations to the brain associated with AD.Convolutional neural networks (CNNs) have also long dominated the field of AD progression and have been the subject of substantial research, while more recent approaches like vision transformers (ViT)  have not yet been fully investigated.In this paper, we present a self-supervised learning (SSL) method to automatically acquire meaningful AD characteristics using the ViT architecture by pretraining the feature extractor using the self-distillation with no labels (DINO) and extreme learning machine (ELM) as classifier models.In this work, we examined a technique for predicting mild cognitive impairment (MCI) to AD utilizing an SSL model which learns powerful representations from unlabeled 18F-FDG PET images, thus reducing the need for largelabeled datasets.In comparison to several earlier approaches, our strategy showed state-of-the-art classification performance in terms of accuracy (92.31%), specificity (90.21%), and sensitivity (95.50%).Then, to make the suggested model easier to understand, we highlighted the brain regions that significantly influence the prediction of MCI development.Our methods offer a precise and efficient strategy for predicting the transition from MCI to AD.In conclusion, this research presents a novel Explainable SSL-ViT model that can accurately predict AD progress based on 18F-FDG PET scans.SSL, attention, and ELM mechanisms are integrated into the model to make it more predictive and interpretable.Future research will enable the development of viable treatments for neurodegenerative disorders by combining brain areas contributing to projection with observed anatomical traits.</p>
<p>Introduction</p>
<p>Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative disorder that primarily affects memory, cognition, and behavior [1].It is the most common cause of dementia among the elderly, accounting for a substantial global health burden [2].Currently, there are about 90 million people who have been diagnosed with AD, and it is predicted that by 2050, there will be an estimated 300 million AD patients worldwide [3].Mild cognitive impairment (MCI), which is a transitional state from normal control (NC) to AD dementia, is frequently regarded as a clinical precursor of AD [4].Two variants of MCI are often recognized: convertible MCI (MCI-c), which will eventually lead to AD, and stable MCI (MCI-s), which will not.Since there is currently no effective treatment for AD, accurate diagnosis and early detection at the prodromal stage are essential for patient care and the development of future therapies.As a result, patients may begin preventative interventions to delay or stop the disease's progression if the conversion process from MCI to AD can be accurately predicted.Alzheimer's is characterized by the accumulation of abnormal protein deposits in the brain, leading to the deterioration and loss of nerve cells [5].As the world's population continues to age, understanding and finding effective treatments for AD have become pressing challenges in modern healthcare.For the early detection of diseases in people with mild or no cognitive impairment, AD biomarkers can be used [6,7].One of the causes of AD, amyloid accumulation in the brain, is known to happen when an abnormal form of amyloid is deposited in the brain as a result of a metabolic issue [8].An amyloid biomarker is injected into the body as part of an amyloid positron emission tomography (PET) test, which produces a brain image and reveals the location and volume of the deposited amyloid.It serves as an effective functional imaging tool to aid doctors in the diagnosis of AD.As a result, 18F-FDG-PET brain imaging has become one of the potent functional biomarkers for AD diagnosis in clinical and computer-assisted diagnosis (CAD) [9][10][11][12].In order to identify the patterns associated with AD and decode the disease states for CAD, a number of pattern recognition techniques have been investigated in recent years for analysis of 18F-FDG-PET brain images [13][14][15][16][17].</p>
<p>As researchers and healthcare professionals strive to improve the early detection and management of AD [18,19] the integration of cutting-edge artificial intelligence (AI) [20] techniques has emerged as a promising avenue for advancing diagnostic accuracy and understanding disease progression [21][22][23].In this journal, we explore the potential of Vision ViTs and 18F-FDG PET in the context of AD research.CNNs have revolutionized various computer vision tasks, demonstrating exceptional performance in image recognition and classification [24][25][26].These deep learning models have shown promise in medical image analysis [21,27,28], including the interpretation of neuroimaging data, such as magnetic resonance imaging (MRI) and PET scans, which are crucial for diagnosing and monitoring AD [23,[29][30][31].On the other hand, ViT [32,33], a recent breakthrough in deep learning has also gained attention in the computer vision community.These models rely on self-attention mechanisms to learn meaningful hierarchical representations from images, making them effective in handling large-scale image datasets [33].In the computer vision domain, the self-attention mechanism has shown promising results in tasks such as image classification, object detection, and image captioning.By incorporating self-attention into computer vision models, researchers aim to capture long-range dependencies in images and improve their ability to understand complex visual patterns.This migration has opened new possibilities for advancing computer vision research and pushing the boundaries of what is achievable in visual understanding tasks.Given their strong generalization capability and efficient use of computational resources, ViTs may offer promising results and open new possibilities for improving Alzheimer's recognition [34].The AD recognition problem is approached using a supervised method in all the previously mentioned techniques.To overcome these limitations, we have explored SSL techniques for AD recognition.SSL does not require annotated samples and can potentially reduce the cost of data collection.Additionally, simpler model architectures can be used in unsupervised learning, leading to faster training and convergence while reducing the number of parameters that need to be tuned.</p>
<p>This journal investigates the application of ViTs in AD-related tasks, such as early detection, disease progression prediction, and biomarker identification.We discuss their strengths and limitations in handling neuroimaging data and explore how combining the strengths of ViTs may lead to more accurate and interpretable results.Additionally, we delve into the emerging area of explainable AI in AD research, in which the understanding of model decisions becomes paramount for clinical acceptance and integration.The exploration of ViTs in AD research opens new possibilities for improved diagnostics and personalized treatment strategies [35][36][37][38][39].By harnessing the power of AI, we aim to enhance our understanding of this complex neurodegenerative disorder and pave the way for more effective interventions to improve the quality of life for individuals affected by AD.Traditional methods for predicting the progression from MCI to AD have often relied on supervised learning techniques, for which labeled data are required to train algorithms.However, obtaining sufficient labeled data for such complex neurological conditions can be challenging, time-consuming, and expensive, restricting the development of accurate and generalized predictive models.In recent years, self-supervised learning has emerged as a promising alternative [40][41][42][43] for harnessing the unlabeled data available and enabling the extraction of meaningful representations from medical images without the need for explicit annotations [44,45].The DINO [43] approach has gained popularity due to its effectiveness in self-supervised learning.It introduces a novel training framework that combines both instance discrimination and clustering objectives.By leveraging these two objectives, DINO achieves state-of-the-art performance on various downstream tasks, such as image classification and object detection.Additionally, the DINO approach also demonstrates strong generalization capabilities across different datasets and domains, making it a promising choice for our research.This discovery suggests that ViTs have a unique ability to capture meaningful visual representations without relying on handcrafted features or explicit supervision.Using the ImageNet image classification dataset, DINO performed exceptionally well and outperformed earlier CNN-based self-supervised methods at a much-reduced computational cost.The ViT model, which has an intriguing feature when compared to CNNs trained in the same manner, serves as the foundation for this method [43].Self-supervised learning is a type of unsupervised learning in which the algorithm formulates tasks that involve predicting certain aspects of the data using its inherent structure.These tasks effectively generate pseudo-labels or supervise the learning process implicitly, leading to the development of powerful representations that can later be fine-tuned for specific downstream tasks, such as predicting disease progression.By applying self-supervised learning techniques to the study of MCI to AD progression, we have made significant strides in unraveling the underlying patterns and mechanisms that govern this complex transition.</p>
<p>Our method leverages the general ViT architecture as a backbone model to learn valuable Alzheimer's features from individual 18F-FDG-PET images via the DINO selfsupervised learning.These features can then be fed into an ELM classifier to classify individuals.We proved the superiority of the approach in terms of algorithm performance and many medical metrics, including accuracy, specificity, sensitivity, and precision, by validating the suggested framework using the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset.Through this approach, the algorithm learns to discover meaningful biomarkers, subtle cognitive changes, and other relevant factors that might contribute to disease progression.The three things that best describe this paper's contributions are summarized below:</p>
<p>•</p>
<p>A transformer model is suggested for the identification of MCI progression.The model expands upon the ViT backbone by utilizing 18F-FDG-PET and self-supervised learning to tackle the issue of MCI progression and disease identification.</p>
<p>•</p>
<p>To address the issue of inadequate data in the field of brain imaging, we suggested a cross-domain transfer learning technique.We used ViT as the backbone with DINO.</p>
<p>•</p>
<p>In the MCI recognition, experimental data show that the proposed method can achieve more competitive outcomes than current models.The model accuracy levels with the ADNI dataset were 92.31%, which is higher than the baseline's ViT approach.Finally, we visualized important metabolic brain regions, which can assist the physician for proper analysis of MCI.</p>
<p>Materials and Methods</p>
<p>Figure 1 depicts the three-step method that makes up this study's framework.Initially, we preprocessed the PET data that had been collected, mostly taking care of partial volume effects (PVE) correction, smoothing, skull-stripping and normalization.In the comparative experiment, we utilized a self-supervised feature extraction method known as DINO with a ViT backbone to learn brain 18F-FDG-PET images.Additionally, we employed t-SNE (t-distributed stochastic neighbor embedding) feature visualization with a different classification algorithm called extreme learning machine (ELM), k-nearest neighbors (KNN), and support vector machine (SVM) to evaluate its effectiveness in classifying MCI-s and MCI-c.The results of experiments are presented and discussed in the results sections.</p>
<p>Materials and Methods</p>
<p>Figure 1 depicts the three-step method that makes up this study's framework.Initially, we preprocessed the PET data that had been collected, mostly taking care of partial volume effects (PVE) correction, smoothing, skull-stripping and normalization.In the comparative experiment, we utilized a self-supervised feature extraction method known as DINO with a ViT backbone to learn brain 18F-FDG-PET images.Additionally, we employed t-SNE (t-distributed stochastic neighbor embedding) feature visualization with a different classification algorithm called extreme learning machine (ELM), k-nearest neighbors (KNN), and support vector machine (SVM) to evaluate its effectiveness in classifying MCI-s and MCI-c.The results of experiments are presented and discussed in the results sections.</p>
<p>ELM DINO Model MCIs/MCIc features</p>
<p>MCIs/MCIc</p>
<p>Dataset</p>
<p>Based on a public-private partnership led by Principal Investigator Michael W. Weiner, MD, the ADNI database (http://adni.loni.usc.edu/27 September 2022) provided all the data.ADNI has made significant contributions to our understanding of the early stages of Alzheimer's disease.The study has provided valuable insights into the biomarkers and cognitive assessments that can help in the early detection and monitoring of MCI and AD.The main objective of ADNI was to determine whether the progression of early AD and mild cogMCI could be monitored using a combination of clinical and neuropsychological assessment, PET, other biological markers, serial magnetic resonance imaging (MRI), and other analyses.This study is multicenter and longitudinal in nature, involving over 63 participating centers.The website http://www.adni-info.org(27 September 2022) offers a comprehensive range of resources, including research findings, study protocols, and data access instructions.Additionally, it serves as a platform for researchers to collaborate and share their findings in the field of AD and related disorders.</p>
<p>We acquired PET scan data from the ADNI 1, ADNI 2, and ADNI GO cohorts in the ADNI database for this study, comprising 224 MCI-c and 245 MCI-s.Following a minimum of 36 months of clinical follow-up, eligible participants with MCI underwent clinical cognitive evaluations and FDG-PET scanning at baseline.Table 1 displays the demographic information of the dataset, which includes age, gender, sex, education, and results from neuropsychological cognitive assessment tests like the dementia rating scale (CDRSB).It also includes information about the apolipoprotein E (APOE) ε4 genotyping characteristics.The groups' ages did not differ much.The MMSE and CDR did, however, vary across all group pairings (p ˂ 0.05).It revealed that compared to MCI-s, MCI-c patients had a higher probability of developing AD.Male dominance prevails in all groups, and the male-to-female ratio is 53:47.Furthermore we also listed the ADNI diagnostic criteria for MCI-s and MCI-c below which can be found details on ADNI website mentioned above.</p>
<p>MCI-s criteria: MMSE scores between 24-30 (inclusive), a subjective memory concern reported by subject, informant, or clinician, objective memory loss measured by</p>
<p>Dataset</p>
<p>Based on a public-private partnership led by Principal Investigator Michael W. Weiner, MD, the ADNI database (http://adni.loni.usc.edu/27 September 2022) provided all the data.ADNI has made significant contributions to our understanding of the early stages of Alzheimer's disease.The study has provided valuable insights into the biomarkers and cognitive assessments that can help in the early detection and monitoring of MCI and AD.The main objective of ADNI was to determine whether the progression of early AD and mild cogMCI could be monitored using a combination of clinical and neuropsychological assessment, PET, other biological markers, serial magnetic resonance imaging (MRI), and other analyses.This study is multicenter and longitudinal in nature, involving over 63 participating centers.The website http://www.adni-info.org(27 September 2022) offers a comprehensive range of resources, including research findings, study protocols, and data access instructions.Additionally, it serves as a platform for researchers to collaborate and share their findings in the field of AD and related disorders.</p>
<p>We acquired PET scan data from the ADNI 1, ADNI 2, and ADNI GO cohorts in the ADNI database for this study, comprising 224 MCI-c and 245 MCI-s.Following a minimum of 36 months of clinical follow-up, eligible participants with MCI underwent clinical cognitive evaluations and FDG-PET scanning at baseline.Table 1 displays the demographic information of the dataset, which includes age, gender, sex, education, and results from neuropsychological cognitive assessment tests like the dementia rating scale (CDRSB).It also includes information about the apolipoprotein E (APOE) ε4 genotyping characteristics.The groups' ages did not differ much.The MMSE and CDR did, however, vary across all group pairings (p &lt; 0.05).It revealed that compared to MCI-s, MCI-c patients had a higher probability of developing AD.Male dominance prevails in all groups, and the male-to-female ratio is 53:47.Furthermore we also listed the ADNI diagnostic criteria for MCI-s and MCI-c below which can be found details on ADNI website mentioned above.</p>
<p>MCI-s criteria: MMSE scores between 24-30 (inclusive), a subjective memory concern reported by subject, informant, or clinician, objective memory loss measured by educationadjusted scores on delayed recall of one paragraph from Wechsler Memory Scale Logical Memory II (≥16 years: 9-11; 8-15 years: 5-9; 0-7 years: 3-6), a CDR of 0.5, absence of significant levels of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia.</p>
<p>MCI-c criteria: MMSE scores between 24-30 (inclusive), a subjective memory concern reported by subject, informant, or clinician, objective memory loss measured by educationadjusted scores on delayed recall of one paragraph from Wechsler Memory Scale Logical Memory II (≥16 years: ≤8; 8-15 years: ≤4; 0-7 years: ≤2), a CDR of 0.5, absence of significant levels of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia.</p>
<p>FDG-PET Image Acquisition and Preprocessing</p>
<p>The ADNI project's web page contains comprehensive information about the PET acquisition procedure.Thirty minutes after injecting 185 ± 18.5 MBq FDG, 469 cases underwent dynamic 3D scans with six 5 min frames.Each frame was motion-corrected to the first frame and then summed to produce a single image file.</p>
<p>Individual PET scan preprocessing [46] was carried out using MatlabR2021a and the statistical parametric mapping (SPM12) [47] program.Prior to applying PVE correction based on the Muller-Gartner algorithm, PET images were first co-registered with the matching T1-weighted images [48].This was done to reduce the PVE on PET measurements.The images were then spatially normalized to a PET template in the Montreal Neurological Institute (MNI) brain space using linear and non-linear 3D transformations.The individual anatomical variations were blurred, and the signal-to-noise ratio was increased for further analysis by smoothing the normalized PET images using an 8 mm full-width at halfmaximum (FWHM) Gaussian filter over a 3D space.Lastly, the intensity of each PET scan was normalized to the average brain uptake globally.With a voxel size of 2 × 2 × 2 mm 3 , the processed images had a spatial resolution of 91 × 109 × 91.Finally, for the purpose of pre-training the model later, each three-dimensional PET image was divided into twodimensional images by slicing and tiling it to a size of 224 × 224 pixels.</p>
<p>Self-Supervised Learning</p>
<p>Researchers' attention has been drawn to self-supervised learning, a new deep learning paradigm, in recent years.The persistent issue of insufficient data for deep learning model training is the focus of self-supervised learning.Through pretext learning-in which one portion of the input data is learned from another portion of the same input-the model learns without labels when it employs self-supervision.Such self-supervised techniques as [40,43] are widely available today.With a contrastive loss function, SimCLR [41] employed contrastive learning by maximizing the similarity between two augmented views of the same image.Two networks-the target network and the online network-with identical architectures but distinct weights were used in BYOL [40].The target network uses the online network's exponential moving average to update its weights while the target network trains the latter.Using instance-level discrimination, each image or its transformation is treated as a distinct class in SwAV [49].The technique uses contrastive loss and image augmentation to learn an embedding such that semantically similar images are clustered closer together in the features space.The label-free knowledge distillation method is applied in DINO [43].The teacher g θs and student g θt networks make up the DINO framework.They have the same architecture, but g θt and g θs , the respective parameters, differ.The objective of the student network is to align with the teacher network's probability distribution.To generate two global views (roughly 50% of the input image) and multiple local views (less than 50% of the input image) for each input image, the method employs a multi-crop strategy [49] during training.Local and global views both flow through the student network, but the global views flow through the teacher network.The similarity between the output vectors from the teacher and student networks is measured using cross-entropy loss.Using stochastic gradient descent, the student parameters θ s are learned by minimizing the cross-entropy loss, and the teacher parameters θ t are defined as an exponential moving average of the student parameters.By doing this, the framework can progressively pick up valuable characteristics from the input images, discovering the global to local correspondences between various perspectives on the same image.Additionally, DINO does not need negative samples, which makes training much easier than it would be with many SSL methods [41,42].Figure 2 below depicts the general architecture of DINO model proposed in [43], which we utilized to predict the AD progression prediction using 18F-FDG-PET in our method.</p>
<p>ages are clustered closer together in the features space.The label-free knowledge distillation method is applied in DINO [43].The teacher   and student   networks make up the DINO framework.They have the same architecture, but   and   , the respective parameters, differ.The objective of the student network is to align with the teacher network's probability distribution.To generate two global views (roughly 50% of the input image) and multiple local views (less than 50% of the input image) for each input image, the method employs a multi-crop strategy [49] during training.Local and global views both flow through the student network, but the global views flow through the teacher network.The similarity between the output vectors from the teacher and student networks is measured using cross-entropy loss.Using stochastic gradient descent, the student parameters   are learned by minimizing the cross-entropy loss, and the teacher parameters   are defined as an exponential moving average of the student parameters.By doing this, the framework can progressively pick up valuable characteristics from the input images, discovering the global to local correspondences between various perspectives on the same image.Additionally, DINO does not need negative samples, which makes training much easier than it would be with many SSL methods [41,42].Figure 2 below depicts the general architecture of DINO model proposed in [43], which we utilized to predict the AD progression prediction using 18F-FDG-PET in our method.[43].Given many viewpoints of the same input image, the student network's objective is to use cross-entropy loss to match the probability distribution of a teacher network.</p>
<p>Vision Transformer (ViT)</p>
<p>Although the standard transformer model was designed for natural language processing, it was given a one-dimensional sequence of word embeddings as input.When the transformer model is used for the computer vision task of image classification, on the other hand, two-dimensional images are used as the input data.It is necessary to divide the input image-which has dimensions of height , width , and number of channels -into smaller two-dimensional patches to structure the data in a way that is similar to how the input is structured in the NLP domain (that is, as a series of individual words) [33].The outcome is several patches  =   2 ⁄ , each with a resolution of (, ) pixels.The subsequent procedures are carried out prior to supplying the data to the transformer:</p>
<p>Each patch of an image is flattened to create a vector    of length  2 × , where,  = 1,2,3, … , .By using a trainable linear projection to map the flattened patches to dimensions , a series of embedded image patches  is produced.Following this, the series of embedded image patches is appended a learnable class embedding   .The categorization output  is represented by the value of   .The final step involves adding  [43].Given many viewpoints of the same input image, the student network's objective is to use cross-entropy loss to match the probability distribution of a teacher network.</p>
<p>Vision Transformer (ViT)</p>
<p>Although the standard transformer model was designed for natural language processing, it was given a one-dimensional sequence of word embeddings as input.When the transformer model is used for the computer vision task of image classification, on the other hand, two-dimensional images are used as the input data.It is necessary to divide the input image-which has dimensions of height H, width W, and number of channels C-into smaller two-dimensional patches to structure the data in a way that is similar to how the input is structured in the NLP domain (that is, as a series of individual words) [33].The outcome is several patches N = HW /P 2 , each with a resolution of (P, P) pixels.The subsequent procedures are carried out prior to supplying the data to the transformer:</p>
<p>Each patch of an image is flattened to create a vector X P n of length P 2 × C, where, n = 1, 2, 3, . . ., N. By using a trainable linear projection to map the flattened patches to dimensions D, a series of embedded image patches E is produced.Following this, the series of embedded image patches is appended a learnable class embedding X clas .The categorization output y is represented by the value of X class .The final step involves adding one-dimensional positional embeddings E pos to the patch embeddings.This adds positional information to the input, which is also learnt during training.Following the previously specified operations, the following array of embedding vectors is produced:
Z 0 = X class ; X 1 P E; . . . ; X N P + E pos(1)
The sequence of embedding vectors that results from the operations represents the encoded representation of the image patches.This encoded representation captures both spatial and positional information, enabling effective classification and analysis of the image data.Ultimately, a multilayer perceptron (MLP) model receives the transformer encoder's output class token for categorization.We employ the [43] ViT-B model with patch size 16.</p>
<p>18F-FDG-PET Feature Learning with ViT-Dino</p>
<p>Our suggested method involves training the feature extractor as the second phase.In this work, we address the challenge of learning discriminative MCI characteristics by proposing to use a self-supervised learning paradigm.We employ the recently suggested DINO approach [43], which has demonstrated promising performance in a range of computer vision applications, including image retrieval and classification.Figure 2 shows the construction of the DINO.Initially, DINO creates two global views of 224 × 224 crops passed via both θ t and θ s and eight local views of 96 × 96 crops transmitted exclusively through θ s .Furthermore, since DINO was initially trained on ImageNet, we modified the augmentations applied during training.Specifically, we eliminated most of the image augmentations' color jitter, Gaussian blur, and solarization and instead used rando horizontal flip, vertical flip, height shift, and random zoom augmentation because the AD related 18F-FDG-PET brain imaging data did not improve performance with the augmentations.</p>
<p>The cross-domain transfer learning technique is employed in this work as AD datasets don't include the substantial quantity of data required to train the ViT model from scratch [32].After being trained on the ImageNet dataset, the DINO model is adjusted for Alzheimer's ADNI data.To generate discriminative features from input brain 18F-FDG-PET for use in classification later, we suggest utilizing the DINO approach as a feature extractor.Figure 3 below illustrates the different slices (coronal, sagittal, and axial) view of input 18F-FDG-PET images to extract the MCI features using ViT DINO architecture for further classification purposes.</p>
<p>spatial and positional information, enabling effective classification and analysis age data.Ultimately, a multilayer perceptron (MLP) model receives the transf coder's output class token for categorization.We employ the [43] ViT-B model w size 16.</p>
<p>18F-FDG-PET Feature Learning with ViT-Dino</p>
<p>Our suggested method involves training the feature extractor as the second this work, we address the challenge of learning discriminative MCI characteristi posing to use a self-supervised learning paradigm.We employ the recently DINO approach [43], which has demonstrated promising performance in a rang puter vision applications, including image retrieval and classification.The cross-domain transfer learning technique is employed in this work a tasets don't include the substantial quantity of data required to train the ViT m scratch [32].After being trained on the ImageNet dataset, the DINO model is ad Alzheimer's ADNI data.To generate discriminative features from input brain PET for use in classification later, we suggest utilizing the DINO approach as extractor.Figure 3 below illustrates the different slices (coronal, sagittal, and a of input 18F-FDG-PET images to extract the MCI features using ViT DINO ar for further classification purposes.</p>
<p>Classifiers</p>
<p>ELM has gained popularity in various fields such as pattern recognition, im cessing, and data mining due to its efficient learning process [50].Additionally lytical estimation of output layer parameters eliminates the need for iterative opt algorithms, making ELM computationally efficient.As a result, gradient-based b agation is not needed for the tuning of hidden layer parameters.This makes for i quick training, which makes it especially well-suited for big data analysis.In co to traditional neural networks and support vector machines (SVM), ELM has a n benefits, including quick learning, simple implementation, and little user inv</p>
<p>Classifiers</p>
<p>ELM has gained popularity in various fields such as pattern recognition, image processing, and data mining due to its efficient learning process [50].Additionally, the analytical estimation of output layer parameters eliminates the need for iterative optimization algorithms, making ELM computationally efficient.As a result, gradient-based backpropagation is not needed for the tuning of hidden layer parameters.This makes for incredibly quick training, which makes it especially well-suited for big data analysis.In comparison to traditional neural networks and support vector machines (SVM), ELM has a number of benefits, including quick learning, simple implementation, and little user involvement [51].Each layer is connected to the layer above it in a feedforward manner, as shown in Figure 4, and creates a feedforward connection with the layer above it.</p>
<p>The multilayer ELM increases the depth of the network by adding extra layers, resulting in improved feature learning capabilities.The multi-layer ELM's (MLELM) algorithm can be summarized as follows:</p>
<p>ELM algorithm: For each layer l from 1 to L, randomize the input-to-hidden layer weights.</p>
<p>Bioengineering 2023, 10, x FOR PEER REVIEW 8 of 2</p>
<p>[51].Each layer is connected to the layer above it in a feedforward manner, as shown in Figure 4, and creates a feedforward connection with the layer above it.The multilayer ELM increases the depth of the network by adding extra layers, re sulting in improved feature learning capabilities.The multi-layer ELM's (MLELM) algo rithm can be summarized as follows:</p>
<p>ELM algorithm: For each layer  from 1 to L, randomize the input-to-hidden layer weights.Calculate the hidden layer output   for each layer  between 1 and L using the for mula 2:
𝐻 𝑙 = 𝑔 𝑙 (𝑋 × 𝑊 𝑙 )(2)
where  stands for the input data,   is the layer  activation function, and   repre sent the layer  weight matrix.</p>
<p>To get the final hidden layer output , combine the outputs of every hidden layer The following equation represent the output weights:  = () ×  (3) where () represents the Moore-Penrose pseudoinverse of the  output from th hidden layers.</p>
<p>The MLELM algorithm offers a proficient approach to train deep architectures, capi talizing on ELM's rapid learning capabilities while harnessing the expressive potential o multiple hidden layers.As a result, MLELM adeptly captures intricate patterns and ex tracts high-level features from intricate datasets, thereby bolstering its classification per formance.</p>
<p>Training Setup</p>
<p>Using the official GitHub repository [52], the DINO method was implemented.T optimize the student and teacher networks, the ImageNet pretrained DINO model check point was employed.Only ViT-B models with patch size 8,16, and 32 architecture wer employed in our studies.The remaining DINO model parameters are the same as in th original publication [43], including global and local crop scales, teacher temperature, and momentum teacher value.</p>
<p>Using the ADNI 18F-FDG-PET imaging datasets for all experiments, we trained th DINO models for 300 epochs with a batch size of 32.With a learning rate of 0.0001 AdamW [53] was the optimizer that was employed.Python 3.9.13 with a compute equipped with an Nvidia GeForce RTX 3090 GPU and the Windows 10 × 64 operatin system was used for the training.The performance of the MLELM classifier is strongly influenced by the number of hidden layer nodes used.In this experiment, we generated extremely accurate performance results using 300 hidden layers.Moreover, we performed 5-fold cross-validation for the robustness of classifier in our models.Since training th model for a longer period did not increase accuracy, the number of epochs used to train Calculate the hidden layer output H l for each layer l between 1 and L using the Formula (2):
H l = g l (X × W l )(2)
where X stands for the input data, g l is the layer l activation function, and W l represent the layer l weight matrix.</p>
<p>To get the final hidden layer output H, combine the outputs of every hidden layer.The following equation represent the output weights:
β = pinv(H) × Y(3)
where pinv(H) represents the Moore-Penrose pseudoinverse of the H output from the hidden layers.</p>
<p>The MLELM algorithm offers a proficient approach to train deep architectures, capitalizing on ELM's rapid learning capabilities while harnessing the expressive potential of multiple hidden layers.As a result, MLELM adeptly captures intricate patterns and extracts high-level features from intricate datasets, thereby bolstering its classification performance.</p>
<p>Training Setup</p>
<p>Using the official GitHub repository [52], the DINO method was implemented.To optimize the student and teacher networks, the ImageNet pretrained DINO model checkpoint was employed.Only ViT-B models with patch size 8,16, and 32 architecture were employed in our studies.The remaining DINO model parameters are the same as in the original publication [43], including global and local crop scales, teacher temperature, and momentum teacher value.</p>
<p>Using the ADNI 18F-FDG-PET imaging datasets for all experiments, we trained the DINO models for 300 epochs with a batch size of 32.With a learning rate of 0.0001, AdamW [53] was the optimizer that was employed.Python 3.9.13 with a computer equipped with an Nvidia GeForce RTX 3090 GPU and the Windows 10 × 64 operating system was used for the training.The performance of the MLELM classifier is strongly influenced by the number of hidden layer nodes used.In this experiment, we generated extremely accurate performance results using 300 hidden layers.Moreover, we performed 5-fold cross-validation for the robustness of classifier in our models.Since training the model for a longer period did not increase accuracy, the number of epochs used to train the DINO model was fixed at 300 epochs.Using steps to the power of 2, the ideal value for the batch sizes of 32 was found to determine the batch size of models.</p>
<p>Evaluation Matrixs</p>
<p>The findings were assessed using specificity, sensitivity, precision, recall, F1 score, and accuracy; we reported our results in term of mean and standard deviation.These parameters were expressed mathematically as follows:
Accuracy = T n + T P T n + T p + F n + F p(4)Sensitivity = T p T p + F n (5) Speci f icity = T n T n + F p(6)Precission = T p T p + F p(7)Recall = T p T p + F n(8)F1 − score = 2 × Precision × recall Precission + recall(9)
True negatives, true positives, false negatives, and false positives are represented by the letters T n , T p , F n , and F p , respectively.Concurrently, a receiver operating characteristic (ROC) curve was generated to provide an understandable comparison of the outcomes of the various methodologies.</p>
<p>Results</p>
<p>This study develops and implements a CAD system which is automated for the diagnosis of AD.The suggested approach was used to distinguish between MCI-s and MCI-c patients progressing to AD.The simulation made use of the 18F-FDG-PET image, which was taken from the ADNI database; 469 patients had their 18F-FDG-PET scans taken, comprising 245 MCI-s patients without conversion within 3 years and 224 MCI-c patients who converted to AD within 3 years.</p>
<p>Classification Performance on 18F-FDG-PET</p>
<p>It is crucial to identify AD in a timely manner for patient care.To distinguish MCI-s from MCI-c, a 2D ViT base DINO model is utilized in this research.The proposed CAD system's structure is shown in Figure 1.According to Figure 3, each 3D 18F-FDG-PET image is split into several 2D images along the coronal, axial and sagittal axis.The first and last 15 slices are eliminated to remove the skull and other undesirable regions.Table 2, Figures 4 and 5 display how well 18F-FDG-PET-based ViT performed in predicting the transition of MCI to AD.</p>
<p>We utilize transfer learning for ViT by initializing the model with weights that were pre-trained on ImageNet [54] to enhance the model's performance.However, since the images in ImageNet differ from brain images, many of the weights may not be relevant.To address this issue, we employ a self-supervised pre-training target dataset, which has gained popularity recently due to the lack of a large brain imaging dataset.Our approach incorporates the DINO self-supervised method, which shares a similar overall structure with other self-supervised algorithms.The input images are transformed to generate alternative views, which are then passed through the student and teacher branches.Subsequently, the resulting features are used to compute a loss.The student and teacher networks in DINO have identical structures and initial weight parameters, but the teacher network's weights are not involved in training and do not have gradients.The parameter updates are based on the student network's parameters.Additionally, the teacher network includes a phoebe module.two stages accurately.We can say DINO successfully applies ViT to self-supervised learning and achieves superior performance compared to baseline ViT for AD dataset.To determine the appropriate model for classifying MCI-c vs. MCI-s, the performance of several models, including the baseline ViT variant and the self-supervised ViT model [43], were compared.The classification results for these models, including accuracy, sensitivity, and specificity, are summarized in Table 2. Since MCI serves as a transitional stage between AD and NC, there are numerous factors that complicate the classification task.It is evident that classifying MCI-s vs. MCI-c is more challenging compared to the other AD classification tasks mentioned earlier [34].First, we extracted the glucose metabolic features from 18F-FDG-PET images using the ViT-DINO model without labeling data.Secondly extracted features are fed into the different classifiers, namely ELM, SVM and KNN.Specifically, the ELM model achieved an accuracy of 92.31%, a sensitivity of 90.21%, a specificity of 95.50%, an AUC of 0.96, and a 93.92% F1-score.Although KNN achieved comparable specificity of 95.08%, their results were lower in terms of accuracy and sensitivity.Among these models, ViT-DINO with ELM was found to be the most suitable, as it not only had the best classification performance in the independent test group but also had a shorter training time.Therefore, ELM was chosen as the classification model for extracted features in this research.Furthermore, we also evaluated the ROC curve, which is a mathematical tool that evaluates how well a classification system can distinguish between positive and negative cases.It compares the true positive rate to the false positive rate on a ROC chart, which is determined by adjusting the threshold value.Furthermore, we utilized the t-SNE algorithm to reduce the complexity of the features obtained from the DINO network and projected them onto a two-dimensional space for visualization purposes.As depicted in Figure 5a, in the MCI conversion prediction, clear boundaries were observed between the two categories.Additionally, only a small number of samples from other categories were scattered within each category, suggesting that the model is more effective at identifying MCI cases without labeling 18F-FDG-PET imaging data.Similarly, as shown in Figure 5b, the model successfully separated the samples into two clusters using Euclidean distance for MCI-s and MCI-c classification.Although a few cases were mixed at the intersection of the two clusters, indicating a transitional stage from MCI-s to MCI-c, our model was able to extract the features between the two stages accurately.We can say DINO successfully applies ViT to self-supervised learning and achieves superior performance compared to baseline ViT for AD dataset.To determine the appropriate model for classifying MCI-c vs. MCI-s, the performance of several models, including the baseline ViT variant and the self-supervised ViT model [43], were compared.The classification results for these models, including accuracy, sensitivity, and specificity, are summarized in Table 2. Since MCI serves as a transitional stage between AD and NC, there are numerous factors that complicate the classification task.It is evident that classifying MCI-s vs. MCI-c is more challenging compared to the other AD classification tasks mentioned earlier [34].First, we extracted the glucose metabolic features from 18F-FDG-PET images using the ViT-DINO model without labeling data.Secondly extracted features are fed into the different classifiers, namely ELM, SVM and KNN.Specifically, the ELM model achieved an accuracy of 92.31%, a sensitivity of 90.21%, a specificity of 95.50%, an AUC of 0.96, and a 93.92% F1-score.Although KNN achieved comparable specificity of 95.08%, their results were lower in terms of accuracy and sensitivity.Among these models, ViT-DINO with ELM was found to be the most suitable, as it not only had the best classification performance in the independent test group but also had a shorter training time.Therefore, ELM was chosen as the classification model for extracted features in this research.Furthermore, we also evaluated the ROC curve, which is a mathematical tool that evaluates how well a classification system can distinguish between positive and negative cases.It compares the true positive rate to the false positive rate on a ROC chart, which is determined by adjusting the threshold value.Figure 6 displays the ROC of the suggested system, with an AUC value of 0.96.The comparison of ROC curves for different classifiers in the classification of MCI-c and MCI-s can be seen in Figure 6a.</p>
<p>ViT Based on the experimental results, we can say that in our task of early prediction classification for MCI, the DINO model can be used instead of the baseline ViT.We observed that when we used the weights obtained from the DINO self-supervised pre-training on the ADNI dataset, which was initialized by ImageNet, all evaluation metrics of the model improved.The accuracy increased by 5.99%, and the F1 score increased by 4.95% compared to the best-performing baseline ViT model.This suggests that self-supervised learning pre-training enables the ViT model to perform better on a small dataset like ADNI.Furthermore, by incorporating the ELM, the model's performance improved even more.Each of the four metrics showed varying degrees of improvement, with accuracy reaching 92.31% (a 3.95% improvement) F1 score reaching 93.92% (a 5.67% improvement) as compared to KNN classifiers.This indicates that the ELM classifiers effectively classified the MCI pathology using features extracted from DINO model that was previously overlooked.</p>
<p>Ablation Study</p>
<p>We performed an ablation experiment to examine the impact of transformer design decisions on MCI-to-AD categorization.We investigated the effects of several patch sizes in the experiment.Three variations of patches size were tested: the patch size of 8, the patch size of 16, and the patch size of 32.All models were trained using pre-trained Based on the experimental results, we can say that in our task of early prediction classification for MCI, the DINO model can be used instead of the baseline ViT.We observed that when we used the weights obtained from the DINO self-supervised pre-training on the ADNI dataset, which was initialized by ImageNet, all evaluation metrics of the model improved.The accuracy increased by 5.99%, and the F1 score increased by 4.95% compared to the best-performing baseline ViT model.This suggests that self-supervised learning pretraining enables the ViT model to perform better on a small dataset like ADNI.Furthermore, by incorporating the ELM, the model's performance improved even more.Each of the four metrics showed varying degrees of improvement, with accuracy reaching 92.31% (a 3.95% improvement) F1 score reaching 93.92% (a 5.67% improvement) as compared to KNN classifiers.This indicates that the ELM classifiers effectively classified the MCI pathology using features extracted from DINO model that was previously overlooked.</p>
<p>Ablation Study</p>
<p>We performed an ablation experiment to examine the impact of transformer design decisions on MCI-to-AD categorization.We investigated the effects of several patch sizes in the experiment.Three variations of patches size were tested: the patch size of 8, the patch size of 16, and the patch size of 32.All models were trained using pre-trained weights from DINO that were included in the Python image model implementation [54].There were three options for patch size: 8, 16, and 32.Table 3 presents the outcomes of different patch sizes on MCI progression prediction.We noted that the greatest classification performance is provided by the DINO ViT-B, which has a patch size of 16 with 12 attention heads.According to our analysis, the patch size of 16 may capture the 18F-FDG-PET images' most useful and instructive glucose metabolic aspects.By separating the brain areas with similar patch sizes, the proposed model generates predictions.The information gathered by the model becomes overly generic and loses many specifics with a greater patch size, which results in underfitting.On the other hand, an image patch size that is too tiny might obliterate the 18F-FDG-PET scan's glucose metabolic information.Detailed studies of our investigation are presented in Table 3 below.</p>
<p>Performance Comparison with State-of-Art Methods</p>
<p>In recent times, there has been significant research conducted on the use of machine learning techniques for predicting MCI stage using brain imaging.Most of these studies have focused on using structural imaging of the brain, with only a few utilizing functional imaging, specifically 18F-FDG-PET.In this section, we are comparing our results with recent findings in the literature from the ADNI database for diagnosing MCI.Some researchers have attempted to analyze 18F-FDG-PET for AD prediction, but these studies have still relied on manual and supervised features extraction [55,56].Table 4 provides an overview of the latest deep learning methods for predicting AD using neuroimaging techniques.Most of the methods examined can only distinguish between AD and normal control (CN) or mild cognitive impairment (MCI) and CN, whereas our method analyzed the predictive diagnosis of MCI stage.Furthermore, our experiments utilized self-supervised learning compared to these methods, demonstrating the superior generalization capability of our approach.Specifically, we compare our results with five methods described by Nozadi et al. [55], Bae et al. [56], Hoang et al. [34], Duan J et al. [57], and Choi and Jin et al. [58] since they utilized FDG-PET images in their experiments as summarized in Table 4. Nozadi et al. [55] proposed a traditional machine learning method which compared multiple simple classifiers and performed feature selection simultaneously with FDG-PET parcellation to improve classification performance.Bae et al. [56] proposed a CNN with ResNet backbone deep learning, generated in the 3D-space of each subject, to extract regional glucose metabolic area.Hoang et al. [34] extracted mild sagittal-slice-based features of sMRI neuroimages using ViT models for stage of MCI classification.Choi and Jin et al. [58] proposed deep learning achieved an accuracy rate of 84.2% with AUC of 0.89.These methods involve supervised features and voxel-wise feature extraction and traditional classification on FDG-PET and sMRI images from the ADNI database.However, Hoang et al. utilized the latest ViT-based deep learning models in supervised manner.Therefore, most of the stateof-art methods rely on the supervised learning methods for MCI diagnostic classification.To address this issue, we implemented fully automated self-supervised learning in deep learning to identify the MCI stage, which is crucial for timely AD identifications without human intervention.Tables 2 and 3 and Figure 7 display the results of our FDG-PET-based vision transformers to predict MCI-to-AD progression in a fully automated manner.Table 4 presents our findings as well as those from other studies, including the methodology used and the performance measures.We introduced a self-supervised version of vision transformers along with ELM.Our method consistently outperforms previous studies in three classification performance indicators: sensitivity, specificity, and accuracy.ELM achieves accuracies of 92.31% and 6.05% improvement in comparison to highest-performing Bae et al. [56] study in terms of accuracy.KNN also demonstrates a significant enhancement in specificity with 95.08%.Although their results achieved similar accuracy, their results are lower in specificity and sensitivity.Figure 6 illustrates the confusion matrix of our model, which yields the best result among our methods, with an AUC of 0.96.Ultimately, our proposed method is highly efficient compared to the latest neuroimaging-based research for the predictive diagnosis of MCI.</p>
<p>Pathological Attention Regions on FDG-PET by ViT DINO</p>
<p>For computer-aided diagnosis, identifying the brain area most closely associated with the deep learning model prediction is crucial.Observing the structural change in the brain is one of the most important factors in the clinical diagnosis of AD and the progression of MCI to AD.We study the potential diseased brain area associated with the prediction of our method as a predictive brain region.To categorize MCI-c and MCI-s classes, we employ self-attention visualization [43] to look at which brain regions attention layers see and focus on (Figure 7), which demonstrates glucose metabolic regions in axial, coronal, and sagittal slices that were found using our suggested strategy.The highlighted regions display the corresponding glucose metabolic activities of FDG-PET.Our findings reveal that the thalamus, medial frontal, hippocampus, posterior temporal lobe, parietal lobe, posterior cingulate gyrus, left Para hippocampal gyrus, and occipital regions are the most informative for our model's prediction.These marked regions align with previous studies on AD diagnosis [34,[59][60][61], which supports the reliability of our proposed model.</p>
<p>Discussion</p>
<p>As the population ages, the number of patients with Alzheimer's disease continues to rise.However, progress in finding a cure for AD has been slow, leading researchers to focus on early diagnosis to delay the progression of the disease through preventive measures.Nevertheless, identifying patients in the prodromal stage of AD remains a difficult task.A neural-network-based model has shown promise in accurately identifying patients with AD at different stages, surpassing the performance of professional radiologists in terms of sensitivity and specificity.Previous studies have identified a specific pattern of reduced brain metabolism in 18F-FDG-PET scans of AD patients, particularly in the bilateral temporo-parietal regions.As the disease advances, reduced FDG uptake is also observed in the frontal, parietal, and lateral temporal lobes.However, 18F-FDG-PET alone is not a definitive biomarker for AD and MCI.While previous attempts to develop CAD diagnostic methods for AD using other imaging modalities have been made, few studies have focused on using machine learning approaches to classify AD patients based on 18F-FDG-PET scans alone.In addition to predicting AD, our model can accurately classify patients with MCI-s and MCI-c, achieving high sensitivity and specificity.The advantages of our model include its ability to dynamically update without retraining from scratch when new imaging studies are added as well as its superior performance in identifying the early stage of AD.Effective and accurate prediction of MCI transitioning into AD holds utmost importance in facilitating timely intervention and disease management.Consequently, numerous studies undertake endeavors to investigate and enhance the predictive capabilities for MCI progression.In this investigation, a comprehensive comparative analysis was conducted to assess the predictive capabilities of DINO-ELM in utilizing 18F-FDG-PET data from the ADNI.Notably, our proposed method exhibited superior performance when compared to the prevailing state-of-the-art MRI-based studies pertaining to MCI progression diagnosis.With an accuracy rate of 92.31%, specificity rate of 95.50%, and sensitivity rate of 90.21% along with 0.96 AUC, our findings demonstrate the potential of employing vision transformers equipped with attention mechanisms with SSL without any human intervention to achieve heightened classification accuracy in contrast to prevailing CNN architectures.This improvement may be attributed to the attention mechanism within vision transformers effectively highlighting distinctions within the brain regions between MCI-c and MCI-s classes.</p>
<p>Additionally, we have also examined the brain regions that impact the prediction of our proposed method.Discovering these regions will facilitate the future advancement of deep learning models, enhancing their classification performance.Furthermore, it will aid doctors in effortlessly identifying the regions of interest for diagnosis.We have identified primary regions with the highest attention score: the thalamus, medial frontal, hippocampus, posterior temporal lobe, parietal lobe, posterior cingulate gyrus, left Para hippocampal gyrus, and occipital.Notably, 18F-FDG-PET scans have revealed brain atrophy in these regions.Figure 7 illustrates examples of 18F-FDG-PET scans for MCI-c cases.The thalamus serves as the primary relay for sensorimotor information in the brain and is believed to be vital for memory processing, early affected by AD [60].The medial frontal area also plays a crucial role in various cognitive functions, including attention, spatial perception, and long-term memory [61].The occipital region, responsible for visual perception encompassing color, form, and motion, experiences volume reduction due to AD [62].The posterior cingulate gyrus and left parahippocampal gyrus also exhibit consistent involvement [59,63].These findings imply informative regions for future feature extraction to enhance our proposed method by allocating more attention to these locations.Additionally, these marked brain regions, crucial for the method's prediction, offer valuable insights for doctors in clinical diagnosis.</p>
<p>Our model has some limitations.Firstly, the training process is complex and needs to be completed in two stages.Additionally, our current method does not utilize a full 3D scan model, instead, it only extracts slices from the brain.This approach may result in missing global anatomical information from other brain regions, which could affect the accuracy of our predictions.The quality of feature extraction in the attention layer also affects the performance of the self-supervised model in the second stage.However, in real clinical scenarios, the causes of hypometabolism observed in 18F-FDG-PET may be more complex.Other types of dementia, such as dementia with Lewy bodies (DLB) or frontotemporal dementia (FTD), can also lead to similar pathological changes such as AD.Further studies on more complex data can provide more reliable clinical aids for the diagnosis of AD.In this work, our study only focuses on MCI to AD progression.In future studies, we should focus on other diagnostic groups, including healthy control, MCI, and AD.Therefore, future studies will focus on incorporating multimodal brain data, including functional MRI (fMRI), structural magnetic resonance imaging (sMRI), and other modalities to identify different diagnostic groups.By integrating multiple imaging modalities, researchers aim to enhance the discriminative power of the models and achieve even better performance in the classification of brain-related conditions.</p>
<p>Conclusions</p>
<p>In summary, utilizing brain 18F-FDG-PET, our study has created a ViT-DINO-based features extractor network along with an ELM classifier for diagnostic prediction of MCI.Features were extracted by decomposing the 18F-FDG-PET images into 2D slices.The slices were then arranged at a few intervals without overlapping.The ADNI dataset verified the suggested CAD system.This integrated approach demonstrates strong performance in the MCI classification task following pre-training through DINO self-supervised learning.Additionally, results of the simulations clearly showed that the utilization of ELM enables the vision transformer to achieve enhanced performance in AD tasks with superior classification accuracy and resilience.Furthermore, our approach primarily had a profound effect on specific brain regions that were visually portrayed.The thalamus, medial frontal, hippocampus, and occipital regions of 18F-FDG-PET emerged as the pivotal components within our proposed framework.These discoveries highlight the potential for early identification and classification of individuals with MCI, utilizing patterns of functional atrophy as reliable indicators, prior to subjecting them to interventional clinical studies.Future research will concentrate on expanding the recommended CAD system to include data from additional sources to increase the classification accuracy.Several different samples will be used to assess the overall performance of the proposed CAD system.</p>
<p>Figure 1 .
1
Figure 1.Overall architecture for computer-aided self-supervised Alzheimer's diagnosis system using ViT-DINO and ELM model.</p>
<p>Figure 2 .
2
Figure 2. Illustration of self-supervised DINO model[43].Given many viewpoints of the same input image, the student network's objective is to use cross-entropy loss to match the probability distribution of a teacher network.</p>
<p>Figure 2 .
2
Figure 2. Illustration of self-supervised DINO model[43].Given many viewpoints of the same input image, the student network's objective is to use cross-entropy loss to match the probability distribution of a teacher network.</p>
<p>Figure 2 construction of the DINO.Initially, DINO creates two global views of 224 × passed via both   and   and eight local views of 96 × 96 crops transmitted e through   .Furthermore, since DINO was initially trained on ImageNet, we mo augmentations applied during training.Specifically, we eliminated most of t augmentations' color jitter, Gaussian blur, and solarization and instead used ra zontal flip, vertical flip, height shift, and random zoom augmentation because t lated 18F-FDG-PET brain imaging data did not improve performance with the a tions.</p>
<p>Figure 3 .
3
Figure 3.The experiment utilized 18F-FDG-PET ADNI dataset and included illustrations onal, sagittal, and axial slices.</p>
<p>Figure 3 .
3
Figure 3.The experiment utilized 18F-FDG-PET ADNI dataset and included illustrations of the coronal, sagittal, and axial slices.</p>
<p>Figure 4 .
4
Figure 4. Illustration of multilayer extreme learning machine with multiple hidden layers with inpu and output layers.</p>
<p>Figure 4 .
4
Figure 4. Illustration of multilayer extreme learning machine with multiple hidden layers with input and output layers.</p>
<p>Figure 6 Figure 5 .
65
Figure 5. Illustrative visualizations of ViT-DINO model learning: (a) t-SNE projections for MCIs/MCI-c group identification; (b) distance matrix between two groups; white means smaller Euclidean distances; and the squares near the white diagonal represent t-SNE and try to roughly preserve the distances between samples.</p>
<p>Figure 5 .
5
Figure 5. Illustrative visualizations of ViT-DINO model learning: (a) t-SNE projections for MCIs/MCI-c group identification; (b) distance matrix between two groups; white means smaller Euclidean distances; and the squares near the white diagonal represent t-SNE and try to roughly preserve the distances between samples.</p>
<p>Figure 6 .
6
Figure 6.Illustrative visualizations of classification performance of proposed model: (a) comparison of ROC curve for different classifiers, (b) confusion matrix for KNN classifier, (c) confusion matrix for SVM classifier, and (d) confusion matrix for ELM classifier.</p>
<p>Figure 6 .
6
Figure 6.Illustrative visualizations of classification performance of proposed model: (a) comparison of ROC curve for different classifiers, (b) confusion matrix for KNN classifier, (c) confusion matrix for SVM classifier, and (d) confusion matrix for ELM classifier.</p>
<p>Figure 7 .
7
Figure 7. Cont.</p>
<p>Figure 7 .
7
Figure 7. Illustrative visualizations of ViT-DINO attention maps on 18F-FDG-PET glucose metabolic regions, where highlighted regions represented highly sensitive brain area corresponding to each attention head: (a) axial slice view, (b) sagittal slice view, and (c) coronal slice view.</p>
<p>Unsupervised features extraction Classifier Unlabeled input Axial, Sagittal and Coronal slice (FDG-PET) Figure 1. Overall</p>
<p>architecture for computer-aided self-supervised Alzheimer's diagnosis system using ViT-DINO and ELM model.</p>
<p>Table 1 .
1
Demographic and statistical information regarding clinical assessments at the time data was collected can be found below.data except APOEE4 positive rate were presented as mean ± standard deviation; education; MMSE = minimental state examination; MoCA = Montreal cognitive assessment; CDR = clinical dementia rating.# Group-level two-sample t-tests are conducted for age, education, MMSE, MoCA, and CDR; * group-level chi-square tests are conducted for gender.
GroupsGender (M/F)EducationAge (Years)MoCAMMSECDRAPOEE4MCI-s130/115 #16.3 ± 2.772.3 ± 7.5 <em>23.7 ± 2.4 </em>28.0 ± 1.7 <em>1.1 ± 0.5 </em>43.1%MCI-c119/10516.2 ± 2.174.1 ± 7.321.1 ± 2.726.3 ± 2.12.3 ± 1.074.0%
All</p>
<p>Table 2 .
2
Comparison of the proposed model with ViT based studies for predicting the progression of mild cognitive impairment (MCI).
ModelClassifiersACC % (mean ± std)SEN % (mean ± std)SPE % (mean ± std)PRE % (mean ± std)Recall % (mean ± std)F1-Score % (mean ± std)ViT-S82.37 ± 1.2975.51 ± 2.0188.71 ± 1.0383.87 ± 1.4584.33 ± 3.0283.30 ± 1.05ViT-B81.75 ± 2.1385.38 ± 3.1479.85 ± 1.7183.54 ± 2.5282.47 ± 2.4582.71 ± 1.71ViT-L78.93 ± 1.0767.83 ± 2.7390.97 ± 1.0781.75 ± 2.5978.83 ± 3.7479.34 ± 2.15DINO ViT-BKNN88.36 ± 1.9181.71 ± 2.4795.08 ± 1.3289.15 ± 3.1188.31 ± 2.1488.25 ± 1.72SVM85.24 ± 3.7392.92 ± 1.0178.06 ± 3.4586.01 ± 2.4785.49 ± 1.7585.21 ± 1.04ELM92.31 ± 1.0790.21 ± 3.3795.50 ± 2.1593.10 ± 1.8892.95 ± 2.3193.92 ± 1.33
ACC: accuracy; SEN: sensitivity; SPE: specificity; PRE: precision; std: standard deviation.</p>
<p>Table 3 .
3
Investigation of the efficiency of different patch sizes of DINO ViT-B for predicting the progression of mild cognitive impairment (MCI).
ModelPatch SizeClassifiersACC % (mean ± std)SEN % (mean ± std)SPE % (mean ± std)PRE % (mean ± std)Recall % (mean ± std)F1-Score % (mean ± std)DINO ViT-B8KNN87.49 ± 1.2395.93 ± 3.1179.61 ± 2.1588.45 ± 1.3387.77 ± 1.0487.46 ± 1.87SVM86.47 ± 1.5578.16 ± 2.4594.23 ± 1.0787.44 ± 1.5886.2 ± 2.7886.31 ± 1.95ELM91.56 ± 1.0386.75 ± 1.4796.06 ± 1.5691.98 ± 1.1991.4 ± 1.7491.51 ± 1.5116KNN88.36 ± 1.9181.71 ± 2.4795.08 ± 1.3289.15 ± 3.1188.31 ± 2.1488.25 ± 1.72SVM85.24 ± 3.7392.92 ± 1.0178.06 ± 3.4586.01 ± 2.4785.49 ± 1.7585.21 ± 1.04ELM92.95 ± 1.0790.21 ± 3.3795.50 ± 2.1593.10 ± 1.8892.95 ± 2.3193.92 ± 1.3332KNN82.62 ± 3.0193.52 ± 1.4172.43 ± 4.7584.15 ± 2.1382.98 ± 1.7182.58 ± 1.37SVM81.31 ± 2.4564.16 ± 5.7897.33 ± 1.2185.07 ± 2.0480.74 ± 3.4180.58 ± 1.78ELM86.84 ± 1.5875.45 ± 3.7197.47 ± 1.0388.74 ± 1.2386.64 ± 1.5986.57 ± 1.79ACC: accuracy; SEN: sensitivity; SPE: specificity; PRE: precision; std: standard deviation.</p>
<p>Table 4 .
4
Comparison of the proposed model with ADNI data-based studies for predicting the progression of mild cognitive impairment (MCI).
StudyModalityMethodACCSENSPENozadi et al. [55]FDG-PETRF72.579.269.9Bae et al. [56]MRIResNet86.18474.8Zhu et al. [28]MRIDual attention multi-instance deep learning network80.277.182.6MRIViT-S83.2785.0781.48Hoang et al. [34]ViT-B80.6779.182.22ViT-L72.8674.6371.11Duan J et al. [57]FDG-PETCNN-81.6385.19Choi and Jin et al. [58]FDG-PETDeep Learning84.281.087.0OurFDG-PETDINO-ELM92.9590.2195.50
ACC: accuracy; SEN: sensitivity; SPE: specificity.</p>
<p>Acknowledgments: The Alzheimer's Disease Neuroimaging Initiative (ADNI) and DOD (Department of Defense award number W81XWH-12-2-0012) provided funding for the project's data gathering and dissemination, respectively.As a result, the ADNI investigators provided data and/or helped to the design and execution of ADNI but did not take part in the analysis or writing of this report.Access to this document was made possible on 27 September 2022, at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.The National Institutes of Health's Foundation for Support (www.fnih.org,viewed on 27 September 2022) facilitates contributions from the private sector.Correspondence should be addressed to GR-K, grkwon@chosun.ac.kr.Funding: This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (Grant No. NRF-2021R1I1A3050703).This research was supported by the BrainKorea21Four Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Education (Grant No. 4299990114316).Additionally, this work was supported by a project for the Industry University Research Institute platform cooperation R&amp;D funded by the Korean Ministry of SMEs and Startups in 2022 (S3312710).Correspondence should be addressed to GR-K, grkwon@chosun.ac.kr.Author Contributions: The concept was created by U.K., who also conducted the study.G.-R.K. researched the idea, and the conclusions were supported.All authors participated in, evaluated, and approved the final version of the manuscript.All authors have read and agreed to the published version of the manuscript.Institutional Review Board Statement: Not applicable.Informed Consent Statement: Not applicable.Data Availability Statement:The dataset used in this article was obtained from the ADNI webpage, which can be easily accessed from the ADNI websites and is freely available for use by all researchers and scientists conducting research on Alzheimer's disease: http://adni.loni.usc.edu/about/contactus/,accessed on 27 September 2022.Conflicts of Interest:The Alzheimer's Disease Neuroimaging Initiative (ADNI) (adni.loni.usc.edu,accessed: 27 September 2022) was used to retrieve the data that the authors used to quantify this study.A participant in this study received written informed consent from the patients or participants.Therefore, the researchers and the funder inside ADNI agreed to data collecting but did not become involved in the categorization, article preparation, or publishing arranging processes.
Association of short-term cognitive decline and MCI-to-AD dementia conversion with CSF, MRI, amyloid-and 18F-FDG-PET imaging. J Ottoy, E Niemantsverdriet, J Verhaeghe, E De Roeck, H Struyfs, C Somers, L Wyffels, S Ceyssens, S Van Mossevelde, T Van Den Bossche, 10.1016/j.nicl.2019.101771NeuroImage Clin. 222019. 101771</p>
<p>Journey through the Diagnosis of Dementia. World Alzheimer Report. 2021. 10 September 2023</p>
<p>Alzheimer's disease facts and figures. 10.1002/alz.130162023. 2023, 19, alz.13016Alzheimer's Dement</p>
<p>Boosting Alzheimer Disease Diagnosis Using PET Images. M Silveira, J Marques, Proceedings of the 2010 20th International Conference on Pattern Recognition. the 2010 20th International Conference on Pattern RecognitionIstanbul, TurkeyAugust 2010</p>
<p>The Amyloid-β Pathway in Alzheimer's Disease. H Hampel, J Hardy, K Blennow, C Chen, G Perry, S H Kim, V L Villemagne, P Aisen, M Vendruscolo, T Iwatsubo, 10.1038/s41380-021-01249-0Mol. Psychiatry. 262021</p>
<p>Toward a biological definition of Alzheimer's disease. C R Jack, D A Bennett, K Blennow, M C Carrillo, B Dunn, S B Haeberlein, D M Holtzman, W Jagust, F Jessen, J Karlawish, 10.1016/j.jalz.2018.02.018201814Alzheimer's Dement</p>
<p>Morphological feature visualization of Alzheimer's disease via Multidirectional Perception GAN. W Yu, B Lei, Y Shen, S Wang, Y Liu, Z Feng, Y Hu, M K Ng, 10.1109/TNNLS.2021.3118369arXiv:2111.128862021</p>
<p>The impact of aging in dementia: It is time to refocus attention on the main risk factor of dementia. P Mecocci, V Boccardi, 10.1016/j.arr.2020.101210Ageing Res. Rev. 652021. 101210</p>
<p>Alzheimer's Disease Neuroimaging Initiative Classification of Alzheimer's Disease by Combination of Convolutional and Recurrent Neural Networks Using FDG-PET Images. M Liu, D Cheng, W Yan, 10.3389/fninf.2018.00035Front. Neuroinform. 122018</p>
<p>Combined use of biochemical and volumetric biomarkers to assess the risk of conversion of mild cognitive impairment to Alzheimer's disease. M Nesteruk, T Nesteruk, M Styczy Ńska, M Mandecka, A Barczak, M Barcikowska, 10.5114/fn.2016.64815Folia Neuropathol. 542016</p>
<p>FDG-PET and CSF biomarker accuracy in prediction of conversion to different dementias in a large multicentre MCI cohort. S P Caminiti, T Ballarini, A Sala, C Cerami, L Presotto, R Santangelo, F Fallanca, E G Vanoli, L Gianolli, S Iannaccone, 10.1016/j.nicl.2018.01.019NeuroImage Clin. 182018</p>
<p>Qualitative and Quantitative Analyses of Brain 18Fluoro-Deoxy-Glucose Positron Emission Tomography in Primary Progressive Aphasia. S Nuvoli, G Tanda, M L Stazza, G Madeddu, A Spanu, Dement. Geriatr. Cogn. Disord. 482020</p>
<p>Inter-modality relationship constrained multi-modality multi-task feature selection for Alzheimer's Disease and mild cognitive impairment identification. F Liu, C.-Y Wee, H Chen, D Shen, 10.1016/j.neuroimage.2013.09.015NeuroImage. 842014</p>
<p>Semi-supervised manifold learning with affinity regularization for Alzheimer's disease identification using positron emission tomography imaging. S Lu, Y Xia, T W Cai, D D Feng, Proceedings of the 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). the 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)Milan, ItalyAugust 2015</p>
<p>Deep Learning in Medical Image Analysis. D Shen, G Wu, H.-I Suk, 10.1146/annurev-bioeng-071516-044442Annu. Rev. Biomed. Eng. 192017</p>
<p>Predicting Cognitive Decline in Subjects at Risk for Alzheimer Disease by Using Combined Cerebrospinal Fluid, MR Imaging, and PET Biomarkers. J L Shaffer, J R Petrella, F C Sheldon, K R Choudhury, V D Calhoun, R E Coleman, P M Doraiswamy, 10.1148/radiol.12120010Radiology. 2662013</p>
<p>Added value of semiquantitative analysis of brain FDG-PET for the differentiation between MCI-Lewy bodies and MCI due to Alzheimer's disease. F Massa, A Chincarini, M Bauckneht, S Raffa, E Peira, D Arnaldi, M Pardini, M Pagani, B Orso, M I Donegani, 10.1007/s00259-021-05568-wEur. J. Nucl. Med. Mol. Imaging. 492022</p>
<p>Diagnosis of early Alzheimer's disease based on dynamic high order networks. B Lei, S Yu, X Zhao, A F Frangi, E.-L Tan, A Elazab, T Wang, S Wang, 10.1007/s11682-019-00255-9Brain Imaging Behav. 152021</p>
<p>Q Zuo, B Lei, Y Shen, Y Liu, Z Feng, S Wang, arXiv:2107.09928Multimodal Representations Learning and Adversarial Hypergraph Fusion for Early Alzheimer's Disease Prediction. arXiv 2021. </p>
<p>Medical Image Reconstruction Using Generative Adversarial Network for Alzheimer Disease Assessment with Class-Imbalance Problem. S Hu, W Yu, Z Chen, S Wang, Proceedings of the 2020 IEEE 6th International Conference on Computer and Communications (ICCC). the 2020 IEEE 6th International Conference on Computer and Communications (ICCC)Chengdu, ChinaDecember 2020</p>
<p>Diagnostic accuracy of deep learning in medical imaging: A systematic review and meta-analysis. R Aggarwal, V Sounderajah, G Martin, D S W Ting, A Karthikesalingam, D King, H Ashrafian, A Darzi, 10.1038/s41746-021-00438-zNPJ Digit. Med. 2021, 4, 65. [CrossRef</p>
<p>Machine learning for medical imaging: Methodological failures and recommendations for the future. G Varoquaux, V Cheplygina, 10.1038/s41746-022-00592-yNPJ Digit. Med. 2022, 5, 48</p>
<p>Transparency of deep neural networks for medical image analysis: A review of interpretability methods. Z Salahuddin, H C Woodruff, A Chatterjee, P Lambin, 10.1016/j.compbiomed.2021.105111Comput. Biol. Med. 2022, 140, 105111</p>
<p>M Sandler, A Howard, M Zhu, A Zhmoginov, L.-C Chen, arXiv:1801.04381MobileNetV2: Inverted Residuals and Linear Bottlenecks. 2019</p>
<p>Deep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, arXiv:1512.033852015</p>
<p>Very Deep Convolutional Networks for Large-Scale Image Recognition. K Simonyan, A Zisserman, arXiv:1409.15562015</p>
<p>3-D CNN-Based Multichannel Contrastive Learning for Alzheimer's Disease Automatic Diagnosis. J Li, Y Wei, C Wang, Q Hu, Y Liu, L Xu, 10.1109/TIM.2022.3162265IEEE Trans. Instrum. Meas. 712022</p>
<p>Dual Attention Multi-Instance Deep Learning for Alzheimer's Disease Diagnosis With Structural MRI. W Zhu, L Sun, J Huang, L Han, D Zhang, 10.1109/TMI.2021.3077079IEEE Trans. Med. Imaging. 402021</p>
<p>Convolutional neural networks to predict brain tumor grades and Alzheimer's disease with MR spectroscopic imaging data. J Acquarelli, T Van Laarhoven, G J Postma, J J Jansen, A Rijpma, S Van Asten, A Heerschap, L M C Buydens, E Marchiori, 10.1371/journal.pone.0268881PLoS ONE. 172022. e0268881</p>
<p>Multi-model and multi-slice ensemble learning architecture based on 2D convolutional neural networks for Alzheimer's disease diagnosis. W Kang, L Lin, B Zhang, X Shen, S Wu, 10.1016/j.compbiomed.2021.104678Comput. Biol. Med. 1362021. 104678</p>
<p>Deep learning application for the classification of Alzheimer's disease using 18F-flortaucipir (AV-1451) tau positron emission tomography. S W Park, N Y Yeo, Y Kim, G Byeon, J.-W Jang, 10.1038/s41598-023-35389-wSci. Rep. 132023</p>
<p>A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An Image is Worth 16 × 16 Words: Transformers for Image Recognition at Scale. arXiv 2021. </p>
<p>Polosukhin, I. Attention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, Advances in Neural Information Processing Systems. NY, USACurran Associates, Inc.: Red Hook201730</p>
<p>Vision transformers for the prediction of mild cognitive impairment to Alzheimer's disease progression using mid-sagittal sMRI. G M Hoang, U.-H Kim, J G Kim, 10.3389/fnagi.2023.1102869Front. Aging Neurosci. 152023. 1102869</p>
<p>Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer. Y Lyu, X Yu, D Zhu, L Zhang, Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments. the 15th International Conference on PErvasive Technologies Related to Assistive EnvironmentsCorfu, Greece; New York, NY, USAAssociation for Computing Machinery29 June-1 July 2022. 2022</p>
<p>The Alzheimer's Disease Neuroimaging Initiative OViTAD: Optimized Vision Transformer to Predict Various Stages of Alzheimer's Disease Using Resting-State fMRI and Structural MRI Data. S Sarraf, A Sarraf, D D Desouza, J A E Anderson, M Kabia, 10.3390/brainsci13020260Brain Sci. 132023</p>
<p>Multiple Instance Learning and Self-supervised Vision Transformer network for Early Alzheimer's disease classification. Y Yin, W Jin, J Bai, R Liu, H Zhen, Smil-Deit, Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN). the 2022 International Joint Conference on Neural Networks (IJCNN)Padua, ItalyJuly 2022</p>
<p>Efficient self-attention mechanism and structural distilling model for Alzheimer's disease diagnosis. J Zhu, Y Tan, R Lin, J Miao, X Fan, Y Zhu, P Liang, J Gong, H He, 10.1016/j.compbiomed.2022.105737Comput. Biol. Med. 1472022. 105737</p>
<p>Introducing Vision Transformer for Alzheimer's Disease classification task with 3D input. Z Zhang, F Khalvati, arXiv:2210.011772022</p>
<p>J.-B Grill, F Strub, F Altché, C Tallec, P H Richemond, E Buchatskaya, C Doersch, B A Pires, Z D Guo, M G Azar, arXiv:2006.07733Bootstrap your own latent: A new approach to self-supervised Learning. 2020</p>
<p>T Chen, S Kornblith, M Norouzi, G Hinton, Simple, arXiv:2002.05709Framework for Contrastive Learning of Visual Representations. 2020</p>
<p>K He, H Fan, Y Wu, S Xie, R Girshick, arXiv:1911.05722Momentum Contrast for Unsupervised Visual Representation Learning. 2020</p>
<p>Emerging Properties in Self-Supervised Vision Transformers. M Caron, H Touvron, I Misra, H Jégou, J Mairal, P Bojanowski, A Joulin, arXiv:2104.142942021</p>
<p>Self-supervised learning for medical image analysis using image context restoration. L Chen, P Bentley, K Mori, K Misawa, M Fujiwara, D Rueckert, 10.1016/j.media.2019.101539Med. Image Anal. 582019. 101539</p>
<p>Self-Supervised Pre-Training of Swin Transformers for 3D. Y Tang, D Yang, W Li, H Roth, B Landman, D Xu, V Nath, A Hatamizadeh, arXiv:2111.14791Medical Image Analysis. arXiv. 2022</p>
<p>Glucose metabolism in the right middle temporal gyrus could be a potential biomarker for subjective cognitive decline: A study of a Han population. Q.-Y Dong, T.-R Li, X.-Y Jiang, X.-N Wang, Y Han, J.-H Jiang, Alzheimer's Res. Ther. 13742021</p>
<p>SPM-Statistical Parametric Mapping. 11 January 2023</p>
<p>PETPVE12: An SPM toolbox for Partial Volume Effects correction in brain PET-Application to amyloid imaging with AV45-PET. G Gonzalez-Escamilla, C Lange, S Teipel, R Buchert, M J Grothe, 10.1016/j.neuroimage.2016.12.0772017147</p>
<p>M Caron, I Misra, J Mairal, P Goyal, P Bojanowski, A Joulin, arXiv:2006.09882Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. arXiv 2021. </p>
<p>Multiclass Classification for the Differential Diagnosis on the ADHD Subtypes Using Recursive Feature Elimination and Hierarchical Extreme Learning Machine: Structural MRI Study. M N I Qureshi, B Min, H J Jo, B Lee, PLoS ONE. 11e01606972016</p>
<p>Identification of Mild Cognitive Impairment Using Extreme Learning Machines Model. W Zhang, H Shen, Z Ji, G Meng, B Wang, Intelligent Computing Theories and Methodologies. D.-S Huang, K.-H Jo, A Hussain, Cham, SwitzerlandSpringer International Publishing2015</p>
<p>Self-Supervised Vision Transformers with DINO. Meta Research. August 2023self-supervised-vision-transformers-with-dino</p>
<p>. I Loshchilov, F Hutter, arXiv:1711.051012019Decoupled Weight Decay Regularization. arXiv</p>
<p>Alzheimer's Disease Neuroimaging Initiative. Classification of Alzheimer's and MCI Patients from Semantically Parcelled PET Images: A Comparison between AV45 and FDG-PET. S H Nozadi, S Kadoury, 10.1155/2018/1247430Int. J. Biomed. Imaging. 2018. 2018. e1247430</p>
<p>Transfer learning for predicting conversion from mild cognitive impairment to dementia of Alzheimer's type based on a three-dimensional convolutional neural network. J Bae, J Stocks, A Heywood, Y Jung, L Jenkins, V Hill, A Katsaggelos, K Popuri, H Rosen, M F Beg, 10.1016/j.neurobiolaging.2020.12.005Neurobiol. Aging. 992021</p>
<p>Broad learning for early diagnosis of Alzheimer's disease using FDG-PET of the brain. J Duan, Y Liu, H Wu, J Wang, L Chen, C L P Chen, 10.3389/fnins.2023.1137567Front. Neurosci. 172023. 1137567</p>
<p>Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging. H Choi, K H Jin, 10.1016/j.bbr.2018.02.017Behav. Brain Res. 3442018</p>
<p>Multimodal classification of Alzheimer's disease and mild cognitive impairment. D Zhang, Y Wang, L Zhou, H Yuan, D Shen, 10.1016/j.neuroimage.2011.01.008NeuroImage. 552011</p>
<p>Why do lesions in the rodent anterior thalamic nuclei cause such severe spatial deficits?. J P Aggleton, A J D Nelson, 10.1016/j.neubiorev.2014.08.013Neurosci. Biobehav. Rev. 542015</p>
<p>Role of the Medial Prefrontal Cortex in Cognition, Ageing and Dementia|Brain Communications|Oxford Academic. 28 June 2023</p>
<p>Visual cortex in aging and Alzheimer's disease: Changes in visual field maps and population receptive fields. A Brewer, B Barton, 10.3389/fpsyg.2014.00074Front. Psychol. 52014</p>
<p>ADNI Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease. S Liu, S Liu, W Cai, H Che, S Pujol, R Kikinis, D Feng, M J Fulham, 10.1109/TBME.2014.2372011IEEE Trans. Biomed. Eng. 622015</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
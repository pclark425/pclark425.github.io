<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-202539277</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1909.03772v2.pdf" target="_blank">A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</a></p>
                <p><strong>Paper Abstract:</strong> As reinforcement learning (RL) achieves more success in solving complex tasks, more care is needed to ensure that RL research is reproducible and that algorithms herein can be compared easily and fairly with minimal bias. RL results are, however, notoriously hard to reproduce due to the algorithms' intrinsic variance, the environments' stochasticity, and numerous (potentially unreported) hyper-parameters. In this work we investigate the many issues leading to irreproducible research and how to manage those. We further show how to utilise a rigorous and standardised evaluation approach for easing the process of documentation, evaluation and fair comparison of different algorithms, where we emphasise the importance of choosing the right measurement metrics and conducting proper statistics on the results, for unbiased reporting of the results.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Seeds / Randomisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preset Random Seeds and Randomisation Sources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use and reporting of random seeds and the role of randomisation (network initialization, target sampling, action selection) as a primary source of run-to-run variability in RL experiments; recommended as a simple control to enable repeatability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Deep Reinforcement Learning / Robotics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluation of TRPO and PPO on the UR-Reacher-2D real-world robot (and simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different randomisation seeds causing different network initialisations, different sampled target positions, and different stochastic action selections; additionally seeds interact with other stochastic components (data shuffling, minibatch sampling, dropout).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Empirical distribution over mean returns across independent runs; standard error (SE) of mean reward; 95% confidence intervals (CIs) computed via bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Experiments run with different seeds: 10 independent runs per hyperparameter configuration; SE and 95% CI reported per configuration (see Table 1). Authors note large CIs for some algorithms (e.g., PPO) and that single-run results are not representative.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeatability checks with identical seeds (same-seed runs) and multi-run statistical evaluation over varied seeds; comparison of empirical distribution function (EDF) to reported single-run means from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>When running same-seed experiments on real robot, 10 runs diverged (7 runs similar, last 3 diverged), but in simulator same-seed runs were consistent; authors conclude presetting and reporting seeds is essential but that real-world stochasticity may still produce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Even with seeding, real-world factors (sensor delays, TCP timing, controller interface issues) introduce variability; authors also admit they did not preset random seeds for the presented results (an error they highlight).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Preset and report random seeds; use separate preset seeds for training and evaluation; prefer multiple independent runs with seed variation; log seeds in configuration files.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 runs per hyperparameter configuration (different seeds); also performed same-seed repeatability tests (10 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random seeds are a major controllable source of variability and must be preset and reported; however, in real-world robotics, seeding alone may not remove all stochasticity due to hardware and timing issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapping & EDF Fitting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapping to estimate Empirical Distribution Function (EDF) and theoretical distribution fitting (with KS test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of bootstrap resampling to estimate the empirical distribution of performance over limited samples, and fitting of multiple theoretical distributions to the EDF to enable statistical hypothesis testing and probability estimates versus reported single-run results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Deep Reinforcement Learning / Robotics / Experimental statistics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Statistical assessment of TRPO/PPO performance on UR-Reacher-2D using bootstrapped EDFs</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Small sample sizes; run-to-run stochasticity from algorithm and environment; outliers (failed runs).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Bootstrap-based EDF (10k resamples from 10 observed runs), empirical mean, 95% confidence intervals, standard error (SE); D'Agostino-Pearson normality test; Kolmogorov-Smirnov (KS) goodness-of-fit for theoretical distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Each hyperparameter configuration had 10 observed runs; bootstrapped 10,000 resamples to obtain EDF and CIs. Normality test rejected normality for 8/10 empirical distributions. Authors fitted 100 theoretical distributions (52 converged) and selected best fits via KS p-values (examples in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Computed probability P{v ≥ μ | data} of obtaining a mean at least as good as the single value μ reported in prior work by integrating over best-fit distributions: P{v ≥ μ | data} = Σ (P_dist * P_v).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Using the fitted-distribution probabilities, authors rejected 55 out of 60 hypotheses that a new single sample would be at least as good as previously reported single-run values, concluding prior single-run reports poorly represent the performance distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Small sample sizes (10 runs) limit power; many EDFs are non-normal so normal-assumption tests are invalid; outliers (e.g., a failed PPO run) complicate inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use bootstrapping on multiple independent runs to obtain EDFs; fit appropriate theoretical distributions rather than assuming normality; perform goodness-of-fit testing (KS) before hypothesis testing; report CIs and p-values rather than single-run means.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Bootstrapping exposed the high variability and non-normality in empirical performance; enabled computation that showed 55/60 hypothesis rejections against single-run claims (quantitative demonstration of effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 observed runs per configuration; 10,000 bootstrap resamples per configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bootstrapping with 10k resamples from 10 runs provides an EDF useful for statistical inference; many empirical performance distributions are non-normal (8/10 rejected normality), and fitting/distribution-aware hypothesis testing revealed that single-run reports often overstate reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sources of Non-Determinism (A6)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enumerated Sources of Non-Determinism in Machine Learning and Deep Reinforcement Learning (Appendix A6)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comprehensive list of sources that cause variability and non-determinism in ML and DRL experiments, spanning hardware, libraries, algorithmic randomness, data handling, and real-world environmental factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / Deep Learning / Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>General analysis of non-determinism sources applicable to ML and DRL experiments</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>GPU non-bitwise reproducibility across architectures (CuDNN), third-party library stochasticity, random weight initialisation, dataset shuffling, random sampling/subsampling, train/test splits, dropout randomness, replay-buffer sampling, environment stochasticity (sensor delays), minibatch sampling, controller timing, software/hardware versions and dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Many causes are outside the experimenter’s direct control (hardware differences, library internals); lacking reporting of these factors hampers reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Seed random number generators, document hardware and library versions, manage dependencies (e.g., Docker), open-source code and data, and log experimental artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-determinism in ML/DRL arises from a wide range of sources (hardware, libraries, algorithms, environment), so thorough documentation, seeding, and environment capture (containers) are necessary to reduce but not always eliminate variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e493.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e493.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulator vs Real-World Repeatability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of Repeatability in Simulator (URSim) versus Real UR5 Robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct experimental comparison showing that the same-seed runs are repeatable in simulator but can diverge on the physical robot due to real-world timing and hardware issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Robotic Reinforcement Learning / Experimental reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Ten same-seed runs of TRPO hyperparameter configuration 1 on UR5 real robot and on URSim v3.9.1 simulator</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Real-world sensor delays, TCP socket traffic timing, UR controller/interface (PolyScope) losing connection, physical hardware irregularities.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Visual comparison of learning curves; divergence observed in learning curves across same-seed real runs vs close proximity in simulator runs.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>On the real robot, 10 same-seed runs produced 7 similar runs and 3 that diverged mid-training; in the simulator, same-seed runs produced closely proximate learning curves, indicating repeatability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Repeatability (same team, same code and seed) via repeated runs; visual and statistical comparison of returns over time.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Repeatability upheld in simulator; on the real robot repeatability failed likely due to physical/controller issues. Authors conclude code-base changes did not cause divergence (simulator checks), implicating physical issues.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Diagnosing physical-controller/network timing issues is difficult; controller (PolyScope) occasionally lost real-time connection causing systematic stoppages every four runs; some issues never fully diagnosed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Workaround: restart robot controller between trials; use simulator for repeatability checks; careful hardware debugging; ensure PC can process TCP traffic fast enough.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Simulator experiments verified that code changes were not causing divergence; restarting controller mitigated systematic stops but root cause remained unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 same-seed runs on real robot and 10 same-seed runs in simulator (per configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulator-based repeatability can be achieved (same seed, same code), but real-world robotics introduces hard-to-control variability (timing, controller connections) that can break repeatability even when software is identical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e493.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e493.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Configuration Files (YAML) & Logging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified Configuration Files (YAML) and Detailed Logging of Hyperparameters & Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using standardized configuration files (YAML) to record all hyperparameters and experiment modules, and enhanced logging (including per-step returns) to improve documentation, reproducibility and ease of re-running experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Experimental methodology for Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Framework-level intervention to record and manage RL experiments (SenseAct + YAML config files + Docker + logging callbacks)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Omitted or unreported hyperparameters, inconsistent logging granularity (some implementations only log end-of-iteration returns), and hidden default values in libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Completeness of config (hyperparameters, module paths), presence of logs and reproducible Docker images; authors used these to reproduce and document experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Authors created YAML configuration files containing all experiment parameters and released Docker images and library-code adaptations; these artefacts enable third parties to re-run experiments more faithfully (repositories and Docker hub provided).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Original prior work lacked experimental code and some hyperparameters; legal restrictions sometimes prevent sharing experimental code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Keep algorithm/environment/metric modules separate and referenced in config; store and share full config files; log all rewards during training (not only per-iteration); provide Docker images with environment to reduce dependency issues; open-source code and data.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uniform configuration files and complete logging significantly ease reproducibility and tracking of hyperparameters; authors released configs and Docker images to support reproducibility of their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e493.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e493.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical Testing & Metrics (Normality, KS, CIs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Statistical Hypothesis Testing and Choice of Metrics (D'Agostino-Pearson, KS, CIs, SE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of a battery of statistical tools—normality tests (D'Agostino-Pearson), Kolmogorov-Smirnov goodness-of-fit, confidence intervals and p-value based hypothesis testing—to rigorously evaluate RL performance variability and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Experimental statistics for Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Statistical analysis of return distributions for TRPO and PPO configurations on UR-Reacher-2D</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Non-normal empirical distributions of returns across runs; outlier runs (failed PPO run); small sample sizes increasing CI width.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>D'Agostino-Pearson normality test (p-values reported; 8/10 distributions rejected normality), KS test p-values for fitted theoretical distributions (Table 6), means, 95% CIs, standard errors (SE).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Normality test rejected 8 out of 10 empirical distributions (e.g., p=0.0002, p=5.17e-33 for some TRPO configs; detailed in Table 8). KS p-values for best-fit distributions are reported (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Hypothesis testing of probability of obtaining at least the previously reported single-run mean using best-fit distributions and bootstrapped EDFs; p-value thresholds (α = 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Using statistical tests and distribution fits, authors found low probabilities that single-run reported values represent the underlying performance (leading to 55/60 hypothesis rejections).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Common field practice of assuming normality is frequently invalid; small sample sizes and outliers render some common tests unreliable without bootstrapping or nonparametric treatment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Perform normality testing before assuming distributional forms; use bootstrapping for small samples; fit theoretical distributions and use goodness-of-fit tests (KS) before performing parametric hypothesis tests; report CIs and p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Applying these statistical procedures revealed non-normality and large variability, enabling robust rejection of many single-run reproducibility claims (quantitative evidence that prior single-value reports are insufficient).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10 runs per configuration; 10k bootstrap resamples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard statistical assumptions (normality) often fail for RL empirical returns; robust reproducibility assessment requires bootstrapping, distribution fitting, and explicit reporting of CIs and hypothesis-test results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning that matters <em>(Rating: 2)</em></li>
                <li>Benchmarking Reinforcement Learning Algorithms on Real-World Robots <em>(Rating: 2)</em></li>
                <li>Re-evaluate: Reproducibility in evaluating reinforcement learning algorithms <em>(Rating: 2)</em></li>
                <li>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control <em>(Rating: 2)</em></li>
                <li>Setting up a Reinforcement Learning Task with a Real-World Robot <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-493",
    "paper_id": "paper-202539277",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Random Seeds / Randomisation",
            "name_full": "Preset Random Seeds and Randomisation Sources",
            "brief_description": "Use and reporting of random seeds and the role of randomisation (network initialization, target sampling, action selection) as a primary source of run-to-run variability in RL experiments; recommended as a simple control to enable repeatability.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Deep Reinforcement Learning / Robotics",
            "experimental_task": "Evaluation of TRPO and PPO on the UR-Reacher-2D real-world robot (and simulator)",
            "variability_sources": "Different randomisation seeds causing different network initialisations, different sampled target positions, and different stochastic action selections; additionally seeds interact with other stochastic components (data shuffling, minibatch sampling, dropout).",
            "variability_measured": true,
            "variability_metrics": "Empirical distribution over mean returns across independent runs; standard error (SE) of mean reward; 95% confidence intervals (CIs) computed via bootstrapping.",
            "variability_results": "Experiments run with different seeds: 10 independent runs per hyperparameter configuration; SE and 95% CI reported per configuration (see Table 1). Authors note large CIs for some algorithms (e.g., PPO) and that single-run results are not representative.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeatability checks with identical seeds (same-seed runs) and multi-run statistical evaluation over varied seeds; comparison of empirical distribution function (EDF) to reported single-run means from prior work.",
            "reproducibility_results": "When running same-seed experiments on real robot, 10 runs diverged (7 runs similar, last 3 diverged), but in simulator same-seed runs were consistent; authors conclude presetting and reporting seeds is essential but that real-world stochasticity may still produce variability.",
            "reproducibility_challenges": "Even with seeding, real-world factors (sensor delays, TCP timing, controller interface issues) introduce variability; authors also admit they did not preset random seeds for the presented results (an error they highlight).",
            "mitigation_methods": "Preset and report random seeds; use separate preset seeds for training and evaluation; prefer multiple independent runs with seed variation; log seeds in configuration files.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "10 runs per hyperparameter configuration (different seeds); also performed same-seed repeatability tests (10 runs).",
            "key_findings": "Random seeds are a major controllable source of variability and must be preset and reported; however, in real-world robotics, seeding alone may not remove all stochasticity due to hardware and timing issues.",
            "uuid": "e493.0",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Bootstrapping & EDF Fitting",
            "name_full": "Bootstrapping to estimate Empirical Distribution Function (EDF) and theoretical distribution fitting (with KS test)",
            "brief_description": "Use of bootstrap resampling to estimate the empirical distribution of performance over limited samples, and fitting of multiple theoretical distributions to the EDF to enable statistical hypothesis testing and probability estimates versus reported single-run results.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Deep Reinforcement Learning / Robotics / Experimental statistics",
            "experimental_task": "Statistical assessment of TRPO/PPO performance on UR-Reacher-2D using bootstrapped EDFs",
            "variability_sources": "Small sample sizes; run-to-run stochasticity from algorithm and environment; outliers (failed runs).",
            "variability_measured": true,
            "variability_metrics": "Bootstrap-based EDF (10k resamples from 10 observed runs), empirical mean, 95% confidence intervals, standard error (SE); D'Agostino-Pearson normality test; Kolmogorov-Smirnov (KS) goodness-of-fit for theoretical distributions.",
            "variability_results": "Each hyperparameter configuration had 10 observed runs; bootstrapped 10,000 resamples to obtain EDF and CIs. Normality test rejected normality for 8/10 empirical distributions. Authors fitted 100 theoretical distributions (52 converged) and selected best fits via KS p-values (examples in Table 6).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Computed probability P{v ≥ μ | data} of obtaining a mean at least as good as the single value μ reported in prior work by integrating over best-fit distributions: P{v ≥ μ | data} = Σ (P_dist * P_v).",
            "reproducibility_results": "Using the fitted-distribution probabilities, authors rejected 55 out of 60 hypotheses that a new single sample would be at least as good as previously reported single-run values, concluding prior single-run reports poorly represent the performance distribution.",
            "reproducibility_challenges": "Small sample sizes (10 runs) limit power; many EDFs are non-normal so normal-assumption tests are invalid; outliers (e.g., a failed PPO run) complicate inference.",
            "mitigation_methods": "Use bootstrapping on multiple independent runs to obtain EDFs; fit appropriate theoretical distributions rather than assuming normality; perform goodness-of-fit testing (KS) before hypothesis testing; report CIs and p-values rather than single-run means.",
            "mitigation_effectiveness": "Bootstrapping exposed the high variability and non-normality in empirical performance; enabled computation that showed 55/60 hypothesis rejections against single-run claims (quantitative demonstration of effectiveness).",
            "comparison_with_without_controls": false,
            "number_of_runs": "10 observed runs per configuration; 10,000 bootstrap resamples per configuration.",
            "key_findings": "Bootstrapping with 10k resamples from 10 runs provides an EDF useful for statistical inference; many empirical performance distributions are non-normal (8/10 rejected normality), and fitting/distribution-aware hypothesis testing revealed that single-run reports often overstate reproducibility.",
            "uuid": "e493.1",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Sources of Non-Determinism (A6)",
            "name_full": "Enumerated Sources of Non-Determinism in Machine Learning and Deep Reinforcement Learning (Appendix A6)",
            "brief_description": "Comprehensive list of sources that cause variability and non-determinism in ML and DRL experiments, spanning hardware, libraries, algorithmic randomness, data handling, and real-world environmental factors.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine Learning / Deep Learning / Reinforcement Learning",
            "experimental_task": "General analysis of non-determinism sources applicable to ML and DRL experiments",
            "variability_sources": "GPU non-bitwise reproducibility across architectures (CuDNN), third-party library stochasticity, random weight initialisation, dataset shuffling, random sampling/subsampling, train/test splits, dropout randomness, replay-buffer sampling, environment stochasticity (sensor delays), minibatch sampling, controller timing, software/hardware versions and dependencies.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": null,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Many causes are outside the experimenter’s direct control (hardware differences, library internals); lacking reporting of these factors hampers reproducibility.",
            "mitigation_methods": "Seed random number generators, document hardware and library versions, manage dependencies (e.g., Docker), open-source code and data, and log experimental artefacts.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "Non-determinism in ML/DRL arises from a wide range of sources (hardware, libraries, algorithms, environment), so thorough documentation, seeding, and environment capture (containers) are necessary to reduce but not always eliminate variability.",
            "uuid": "e493.2",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Simulator vs Real-World Repeatability",
            "name_full": "Comparison of Repeatability in Simulator (URSim) versus Real UR5 Robot",
            "brief_description": "Direct experimental comparison showing that the same-seed runs are repeatable in simulator but can diverge on the physical robot due to real-world timing and hardware issues.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Robotic Reinforcement Learning / Experimental reproducibility",
            "experimental_task": "Ten same-seed runs of TRPO hyperparameter configuration 1 on UR5 real robot and on URSim v3.9.1 simulator",
            "variability_sources": "Real-world sensor delays, TCP socket traffic timing, UR controller/interface (PolyScope) losing connection, physical hardware irregularities.",
            "variability_measured": true,
            "variability_metrics": "Visual comparison of learning curves; divergence observed in learning curves across same-seed real runs vs close proximity in simulator runs.",
            "variability_results": "On the real robot, 10 same-seed runs produced 7 similar runs and 3 that diverged mid-training; in the simulator, same-seed runs produced closely proximate learning curves, indicating repeatability.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Repeatability (same team, same code and seed) via repeated runs; visual and statistical comparison of returns over time.",
            "reproducibility_results": "Repeatability upheld in simulator; on the real robot repeatability failed likely due to physical/controller issues. Authors conclude code-base changes did not cause divergence (simulator checks), implicating physical issues.",
            "reproducibility_challenges": "Diagnosing physical-controller/network timing issues is difficult; controller (PolyScope) occasionally lost real-time connection causing systematic stoppages every four runs; some issues never fully diagnosed.",
            "mitigation_methods": "Workaround: restart robot controller between trials; use simulator for repeatability checks; careful hardware debugging; ensure PC can process TCP traffic fast enough.",
            "mitigation_effectiveness": "Simulator experiments verified that code changes were not causing divergence; restarting controller mitigated systematic stops but root cause remained unknown.",
            "comparison_with_without_controls": true,
            "number_of_runs": "10 same-seed runs on real robot and 10 same-seed runs in simulator (per configuration).",
            "key_findings": "Simulator-based repeatability can be achieved (same seed, same code), but real-world robotics introduces hard-to-control variability (timing, controller connections) that can break repeatability even when software is identical.",
            "uuid": "e493.3",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Configuration Files (YAML) & Logging",
            "name_full": "Unified Configuration Files (YAML) and Detailed Logging of Hyperparameters & Metrics",
            "brief_description": "Using standardized configuration files (YAML) to record all hyperparameters and experiment modules, and enhanced logging (including per-step returns) to improve documentation, reproducibility and ease of re-running experiments.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Experimental methodology for Deep Reinforcement Learning",
            "experimental_task": "Framework-level intervention to record and manage RL experiments (SenseAct + YAML config files + Docker + logging callbacks)",
            "variability_sources": "Omitted or unreported hyperparameters, inconsistent logging granularity (some implementations only log end-of-iteration returns), and hidden default values in libraries.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Completeness of config (hyperparameters, module paths), presence of logs and reproducible Docker images; authors used these to reproduce and document experiments.",
            "reproducibility_results": "Authors created YAML configuration files containing all experiment parameters and released Docker images and library-code adaptations; these artefacts enable third parties to re-run experiments more faithfully (repositories and Docker hub provided).",
            "reproducibility_challenges": "Original prior work lacked experimental code and some hyperparameters; legal restrictions sometimes prevent sharing experimental code.",
            "mitigation_methods": "Keep algorithm/environment/metric modules separate and referenced in config; store and share full config files; log all rewards during training (not only per-iteration); provide Docker images with environment to reduce dependency issues; open-source code and data.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "Uniform configuration files and complete logging significantly ease reproducibility and tracking of hyperparameters; authors released configs and Docker images to support reproducibility of their experiments.",
            "uuid": "e493.4",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Statistical Testing & Metrics (Normality, KS, CIs)",
            "name_full": "Use of Statistical Hypothesis Testing and Choice of Metrics (D'Agostino-Pearson, KS, CIs, SE)",
            "brief_description": "Application of a battery of statistical tools—normality tests (D'Agostino-Pearson), Kolmogorov-Smirnov goodness-of-fit, confidence intervals and p-value based hypothesis testing—to rigorously evaluate RL performance variability and reproducibility.",
            "citation_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Experimental statistics for Deep Reinforcement Learning",
            "experimental_task": "Statistical analysis of return distributions for TRPO and PPO configurations on UR-Reacher-2D",
            "variability_sources": "Non-normal empirical distributions of returns across runs; outlier runs (failed PPO run); small sample sizes increasing CI width.",
            "variability_measured": true,
            "variability_metrics": "D'Agostino-Pearson normality test (p-values reported; 8/10 distributions rejected normality), KS test p-values for fitted theoretical distributions (Table 6), means, 95% CIs, standard errors (SE).",
            "variability_results": "Normality test rejected 8 out of 10 empirical distributions (e.g., p=0.0002, p=5.17e-33 for some TRPO configs; detailed in Table 8). KS p-values for best-fit distributions are reported (Table 6).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Hypothesis testing of probability of obtaining at least the previously reported single-run mean using best-fit distributions and bootstrapped EDFs; p-value thresholds (α = 0.05).",
            "reproducibility_results": "Using statistical tests and distribution fits, authors found low probabilities that single-run reported values represent the underlying performance (leading to 55/60 hypothesis rejections).",
            "reproducibility_challenges": "Common field practice of assuming normality is frequently invalid; small sample sizes and outliers render some common tests unreliable without bootstrapping or nonparametric treatment.",
            "mitigation_methods": "Perform normality testing before assuming distributional forms; use bootstrapping for small samples; fit theoretical distributions and use goodness-of-fit tests (KS) before performing parametric hypothesis tests; report CIs and p-values.",
            "mitigation_effectiveness": "Applying these statistical procedures revealed non-normality and large variability, enabling robust rejection of many single-run reproducibility claims (quantitative evidence that prior single-value reports are insufficient).",
            "comparison_with_without_controls": false,
            "number_of_runs": "10 runs per configuration; 10k bootstrap resamples.",
            "key_findings": "Standard statistical assumptions (normality) often fail for RL empirical returns; robust reproducibility assessment requires bootstrapping, distribution fitting, and explicit reporting of CIs and hypothesis-test results.",
            "uuid": "e493.5",
            "source_info": {
                "paper_title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning that matters",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_that_matters"
        },
        {
            "paper_title": "Benchmarking Reinforcement Learning Algorithms on Real-World Robots",
            "rating": 2,
            "sanitized_title": "benchmarking_reinforcement_learning_algorithms_on_realworld_robots"
        },
        {
            "paper_title": "Re-evaluate: Reproducibility in evaluating reinforcement learning algorithms",
            "rating": 2,
            "sanitized_title": "reevaluate_reproducibility_in_evaluating_reinforcement_learning_algorithms"
        },
        {
            "paper_title": "Reproducibility of benchmarked deep reinforcement learning tasks for continuous control",
            "rating": 2,
            "sanitized_title": "reproducibility_of_benchmarked_deep_reinforcement_learning_tasks_for_continuous_control"
        },
        {
            "paper_title": "Setting up a Reinforcement Learning Task with a Real-World Robot",
            "rating": 1,
            "sanitized_title": "setting_up_a_reinforcement_learning_task_with_a_realworld_robot"
        }
    ],
    "cost": 0.015063249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots</p>
<p>Nicolai A Lynnerup 
Robot Technology
Danish Technological Institute (DTI)</p>
<p>Embodied Systems for Robot Learning
University of Southern Denmark (SDU)</p>
<p>Laura Nolling 
Robot Technology
Danish Technological Institute (DTI)</p>
<p>Embodied Systems for Robot Learning
University of Southern Denmark (SDU)</p>
<p>Rasmus Hasle 
Robot Technology
Danish Technological Institute (DTI)</p>
<p>John Hallam 
Embodied Systems for Robot Learning
University of Southern Denmark (SDU)</p>
<p>A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots
FEB4DCB6D9037E9A726B2962847D3516CoRLRobotsLearningReinforcement LearningReproducibilityStatistics
As reinforcement learning (RL) achieves more success in solving complex tasks, more care is needed to ensure that RL research is reproducible and that algorithms therein can be compared easily and fairly with minimal bias.RL results are, however, notoriously hard to reproduce due to the algorithms' intrinsic variance, the environments' stochasticity, and numerous (potentially unreported) hyper-parameters.In this work we investigate the many issues leading to irreproducible research and how to manage those.We further show how to utilise a rigorous and standardised evaluation approach for easing the process of documentation, evaluation and fair comparison of different algorithms, where we emphasise the importance of choosing the right measurement metrics and conducting proper statistics on the results, for unbiased reporting of the results.</p>
<p>Introduction</p>
<p>The ability critically to assess and evaluate the claims made by other scientists in published research is fundamental and a cornerstone of science.The impartial and independent verification of others' research serves the purpose of credibility-confirmation and allows for building on top of a "body of knowledge," referred to as extensible research.Research in robotics and machine learning (ML) is not excluded from this strict scientific requirement, even though it is notoriously hard to ensure reproducibility in computational studies of this nature [1].</p>
<p>In the domain of robotic RL, algorithms such as trust region policy optimization (TRPO) [2], proximal policy optimization (PPO) [3], deep deterministic policy gradients (DDPG) [4] and Soft Q-Learning (Soft-Q) [5] have gained popularity due to their success in simulated robotic tasks [6]; but as Mahmood et al. [7,8] shows, setting up tasks and evaluating RL algorithms on real-world robots is seldom straightforward and requires many practical considerations in order to ensure reproducibility.One of the most common issues when replicating ML research is omission of one or more hyperparameter choices in the manuscript.Often hyper-parameters have significant impacts on how the algorithm performs so it is critical to report their values including how they were obtained [9,10].One reason for neglecting to report hyper-parameters is that they are simply forgotten, which may be due to multiple reasons, e.g.; a) they are not considered important or b) their value is simply the 3rd Conference on Robot Learning (CoRL 2019), Osaka, Japan.</p>
<p>arXiv:1909.03772v2 [cs.LG] 11 Sep 2019</p>
<p>default value, specified by the underlying implementation used.Both reasons are obviously important challenges to handle but are often hard to discover and subsequently enforce.</p>
<p>To add to the complexity, real-world robotic RL methods require large amounts of data.Results from the real world are thus expensive to obtain.Further, many industrial researchers are forced by their company's legal department to omit specific details to remain in front of their competitors.In combination with the "publish or perish" pressure on academic researchers, this seems to result in deviations from the standards of good science [9].To maintain progress in deep reinforcement learning (DRL), research must be reproducible and comparable so that improvements can be verified and built upon.</p>
<p>Differences in evaluation metrics and the lack of significance testing in the field of DRL potentially cause misleading reporting of results [11].With no statistical evaluation of the results, it is difficult to conclude if there are meaningful improvements.If results are to be trusted, complete and statistically correct evaluations of proposed methods are needed.</p>
<p>We find inspiration from a pipeline proposed by Khetarpal et al. [11], which provides common interfaces for both environments, algorithms, and evaluation schemes.We build on this idea by adding experiment configuration files, to create a unified experiment framework.The configuration file contains all (hyper-)parameters related to specific experiments.With the configuration files, we aim to ease the process of a) running new experiments with new hyper-parameters during the tuning process, b) keeping track of past experiments and their hyper-parameters, and c) reporting all the hyper-parameters in the scientific paper.</p>
<p>We evaluate our pipeline on the SenseAct framework 1 which provides an interface letting RL agents interact with the real world through Universal Robots (UR)' 6 DoF robotic manipulators [8].We utilise the proposed benchmarking task UR-Reacher-2D in which the agent is to reach arbitrarily chosen target points in a 2D plane with its wrist joint (end-effector).</p>
<p>Our key contributions to the field are the demonstration of a rigorous method for easy documentation of parameters; reproducing RL results and determining significance by a statistics-based evaluation of common RL baseline algorithms on real-world robots; and our suggestions on ways to ensure correct choices of measurement metrics based on the task at hand.</p>
<p>A Reproducibility Taxonomy</p>
<p>The taxonomy of reproducible research is widely discussed [12,13].Here we present the terminology that we conform to: Repeatability (same team, same experimental setup) refers to the same team with the same experimental setup re-running the experiment.This procedure is needed when wanting to report statistically sound results.The procedure implies the exact same team and the same code.</p>
<p>Reproducibility (different team, same experimental setup) refers to a different team conducting the same experiment with the same setup, achieving results within marginals of experimental error.The setup includes both library code, experimental code, data and environments.This procedure can be viewed as software testing at the level of a complete study.</p>
<p>Replicability (different team, different experimental setup) refers to teams, attempting to obtain the same (or similar enough) results as reported in the original work, who do not have access to either code, data, environment or all of them.Note that the Claerbout, Donoho, Peng [12,14,15,16] convention omits the repeatability term, but as we show in appendix A1, the ACM, Drummond convention's take on this term is applicable, as it is not contradictory.In our work we conform to the Claerbout, Donoho, Peng Convention [14,15,16] and add the repeatability term.</p>
<p>In addition to the taxonomy, we present a practical view of reproducibility in appendix A2.</p>
<p>Methods</p>
<p>We propose a simple method for uniformly configuring RL experiments by collecting all (hyper-)parameters in a configuration file using an open data format, in our case YAML.These configuration files contain the comprehensive list of parameters used for the specific experiments ranging from environment and agent specifics, such as robot kinematics, to algorithm specifics such as number of hidden layers in the function approximator 2 .</p>
<p>We additionally separate the algorithms from the environments and metric collection routines to ease the evaluation of additional algorithms, as proposed by Khetarpal et al. [11].</p>
<p>In practice, the configuration files contain module specifications for each of the aforementioned, meaning that changing out e.g. the algorithm can be done directly by changing the module path in the YAML file.The same applies to environments and logging routines.</p>
<p>Reporting the Evaluation</p>
<p>The most common way of reporting results obtained by RL is to present a plot of the average cumulative reward (average returns).However, the performance of an algorithm can vary to a great extent due to the stochasticity of the algorithms and environments, and the average returns alone will not depict an algorithm's range of performance.Instead, a proper evaluation requires multiple runs with different preset randomisation seeds [9].Further, proper statistics are needed to determine if a higher return in fact does represent better performance, such as confidence intervals (CIs) on the mean or probability values of obtaining a certain threshold performance value.</p>
<p>In general, the more trials, the easier it will be to support the conclusions with the proper statistics.However, as in life sciences, robotics suffers from the fact that samples are expensive to obtain so cost-effective methods such as bootstrapping are of particular interest.Bootstrapping is a method used for estimating a population distribution from a small sample by sampling with replacement.The empirical distribution obtained by bootstrapping allows for statistical inference.Utilising this can further verify that no errors have occurred across trials.We conduct multiple trials to obtain a statistical significance measure of our results.</p>
<p>In the context of reporting the evaluation, we acknowledge that some cases are so time-consuming that multiple runs are not an option, not even for the small number of samples needed for bootstrapping.We encourage researchers who find themselves in such cases to state why they could not conduct proper statistics.</p>
<p>Experimental Protocol</p>
<p>Task Description</p>
<p>To evaluate our proposed method we use the task by Mahmood et al. [7], the UR-Reacher-2D.In this task, the agent's objective is to reach arbitrary target positions by low-level control where the real-world UR5 (figure 1) is restricted to only move its 2 nd and 3 rd axis.The reward function is defined as
R t = −d t + exp −100d 2 t
, where d t is the Euclidean distance between the point target and flange pose of the robot.The observation vector consists of the robot joint angles, joint velocities, its previous action, and the vector difference between the target and the flange coordinates.We keep the episodes to be 4 seconds, as in [8].A list of all hyper-parameter values used to conduct our evaluation is in appendix A3.</p>
<p>RL Algorithms</p>
<p>As in [8] we use the OpenAI Baselines implementations for the two model-free policy-gradient algorithms TRPO and PPO, to ensure that the code-base used is not a source of difference [9].It should be noted that the work by Mahmood et al. [8] benchmarks two additional algorithms, DDPG [4] and Soft Q-learning [5], which are not included in our work as they require inline modifications to the underlying code-bases (for extracting the evaluation metrics) which were not made publicly Figure 1: Robot used for evaluation: The 6 DOF robotic manipulator used for evaluating our proposed methodology is the UR5 robot.For the UR-Reacher-2D task, the robot is limited to actuate its second and third joints to reach the arbitrary points (red circle) with its tool.</p>
<p>available by [8] for legal reasons 3 .The choice of RL algorithm has received little attention through this work, as it is beyond the scope of our study.</p>
<p>Evaluation</p>
<p>We evaluate the RL algorithms on the UR-Reacher-2D task partly to investigate our proposed methodology, and partly the reproducibility of the original authors work [8].</p>
<p>We take the top 5 performing hyper-parameter configurations, shown in appendix A3, from the 30 randomly selected ones in [8] for each of the two RL algorithms and evaluate their performance on the UR-Reacher-2D task.We repeat each of the experiments (1 hyper-parameter configuration of 1 algorithm) 10 times to determine the statistical significance of the performance of each hyperparameter configuration, which means running the experiments with different randomisation seeds that result in different network initialisations, target positions, and action selections.We approximate the empirical distribution function (EDF) using bootstrapping, to which we fit theoretical distributions.From the theoretical distributions we determine the probability of obtaining at least the performance reported in the original work [8].</p>
<p>Evaluation metrics are computed the same way as in [8] to allow for comparison.The computations consist of a rolling average using a window size of 5, 000 steps, calculated every 1, 000 steps.The metrics are collected during training.</p>
<p>Processing the results: first, the average returns for each of the ten runs using the same configuration are calculated.Then we perform bootstrapping using 10k resamples on the ten average return values to find the EDF.We then test for normality, which appears to be a common assumption in the field, and further explore a general approach for when normality cannot be assumed: fitting a theoretical distribution to the EDF.In section 5 we show the results of fitting 100 theoretical distributions 4 to our EDFs, while the plots are presented in appendix A4.Using a significance level of α = 0.05, we determine if we successfully replicated the results reported in [8].</p>
<p>Results</p>
<p>Repeatability of Code Base</p>
<p>To verify that our changes to the code bases did not interfere with the ability to repeat experiments reported in [8], we performed ten runs of TRPO on the real-world robot using the same seed and same experiment configuration.Initial results were promising but, after the first seven runs, something unknown happens and the last (bottom) three runs diverge from the seven previous.We speculate that the deviations on the real-world robot might suggest physical issues, such as delayed sensor readings, The plots show the average return obtained over time during training, computed as a rolling average with a window size of 5.000 steps, calculated every 1.000 steps.(left) Ten runs of hyperparameter configuration 1 for TRPO using same seed and evaluated on the real world robot, (right) Ten runs using the same RL algorithm with same code base using the same seed but evaluated in UR's simulator: URSim v. 3.9.1 5 .</p>
<p>rather than stochasticity in the code-base.Therefore we decided to run the same experiment using the simulator to exclude the stochasticity of the real world.We can use the simulator provided by UR as it uses the exact same robot controller, code base, inverse kinematics solver, etc.We find that the resulting learning curves are in close proximity to one another.Thus we conclude that the ability to repeat experiments is upheld.The results are visualised in figure 2.</p>
<p>Evaluation of Baseline Algorithms</p>
<p>The resulting learning curves from evaluating the top 5 configurations are plotted in figure 3, which consists of the mean rewards and their standard error (SE).Through the evaluation of TRPO, we observed that the worst performing configuration was configuration 4, which is different from the original work [8].Our evaluation of PPO results in better performance for configurations 1 and 4. Note that for PPO, the second last run of configuration 4 failed.Figure 4 shows the return over time for this run.We assume it is an outlier, not representative of the true population, and exclude it from the statistical analysis.</p>
<p>To test the significance of our results we perform bootstrapping on the original 10 observations for each configuration.The resulting sample statistics (means and CIs) are presented in table 1, where we obtain a different order of performance for the five configurations from that originally reported in [8].</p>
<p>For TRPO, we find that even though configuration 1 appears to show the best performance in figure 3, the mean performance of configuration 2 is higher, suggesting a faster increase in performance.</p>
<p>For PPO, we obtain the largest mean values from configurations 1 and 4. While it appears that the CIs are large for PPO, suggesting a greater range of performance, we recall that Henderson et al. [9] speculates that exceedingly large confidence bounds might suggest an insufficient sample size.</p>
<p>The empirical distributions we obtain from bootstrapping are presented in appendix A5, with a theoretical normal distribution fitted to the EDF.To determine if the data was normally distributed we performed a normality test and, even though our data initially appears normally distributed, we found that 8 of 10 configurations would reject our null-hypothesis using a significance level of α = 0.05.</p>
<p>Thus the data cannot be assumed normally distributed and other theoretical distributions must be considered.</p>
<p>Fitting theoretical distributions to the EDFs is performed to determine which distribution fits the empirical data best.We tested 100 theoretical distributions, of which 52 converged successfully.On these 52, we compute a goodness of fit by performing a Kolmogorow-Smirnow (KS) test (see appendix A4) from which we choose the most promising distributions determined from their p-values.This results in the six distributions presented in appendix A4.The failed run of PPO: The 9 th run of the 4 th configuration for PPO failed for some unknown reason.We assume that this run is an outlier and not part of the true population.Thus, we exclude it from the statistical analysis.The mean and 95% CI for each hyper-parameter configuration is computed from the 10k samples obtained by bootstrapping from our original ten samples.All trials are conducted using the UR environment which corresponds to 300 hours of robot wall time.The reported average return by [8] is denoted μ, while μ denotes the empirical mean we obtained from bootstrapping.</p>
<p>Hyper-parameter configuration</p>
<p>Verifying reproducibility.To verify the claims made by the original authors, we compute the probability that we would obtain a mean performance at least as good as the originally reported average rewards (shown in appendix A3).We do this for each of the best-fit distributions and</p>
<p>Discussions and Conclusions</p>
<p>Experimental Conclusions</p>
<p>From table 2 we reject 55 out of 60 of our hypotheses stating that it would be possible to obtain a single sample at least as good as the one reported in [8].Our probability values conclude that the values reported in [8] do not depict the range of performance very well.From the p-values we conclude that one run is not a representative sample of the underlying distribution, rather than dismissing the original work as irreproducible.The ideal way to compare results is to compare the resulting EDFs from statistical analysis, which was not possible in our case as [8] only reports one value of average return per configuration.</p>
<p>The key takeaway is that it is -indeed -very challenging to create reproducible robotic RL research, and that having systems of uniform testing and ways to describe experiments can assist researchers in reporting their results without missing those important hyperparameters.</p>
<p>Which recommendations can we draw from the experiments?</p>
<p>Reproducing RL results on a real-world robot is not trivial and introduces many practical issues.</p>
<p>The main issue we encountered was that the robot systematically stopped every four runs as UR's controller interface (PolyScope) lost its connection to the real-time controller.It was challenging to figure out where and why the error occurred, and we never found the cause, but eventually worked around it by shutting down and restarting the robot controller after each trial.We conclude that physical issues are difficult to debug since it is hard to predict and solve bugs in the internal software of the physical robot.</p>
<p>Managing software dependencies is essential to creating reproducible experiments.We encourage reducing the number of dependencies to the minimum necessary to run the experimental code.Further, dependencies should be easy to install and use, which means that authors should, as a bare minimum, provide a list of dependencies and versions used to carry out their experimental work.We used the software container platform Docker6 to manage our dependencies.</p>
<p>Presetting and reporting the random seeds used is key for ensuring that results are reproducible.However, when dealing with real-world robotics, where the sensor readings can be delayed and contain real measurement error, stochasticity will prevail.From our experiments, we found that if the PC used cannot process the traffic across the TCP socket fast enough, the sensor readings will not occur at the same time across different runs, leading to continuous stochasticity even if all parts of the software are seeded.This is visualised in Fig. 2. We argue that presetting (and reporting) the random seeds should be considered good practice, and we admit that we did not preset the random seeds for obtaining the results presented in this work.Not presetting and reporting the random seeds is an error we wish to highlight for others to avoid.</p>
<p>Distinguishing between experimental code and library code helps others to run the code more easily.One of our initial discoveries when attempting to reproduce the work of Mahmood et al. [8] was that the experimental code they used for obtaining the reported results was not available.The open-sourced code7 was library code and example code.We reached out to the lead author multiple times requesting both missing hyperparameters and experimental code, but did not obtain the code due to legal reasons.This restriction meant that we were forced to replicate, rather than reproduce, the original work.</p>
<p>Logging of return values is done by using the callback interface provided by the OpenAI implementations [17], where return values are only available at the end of each iteration which, in turn, is dependent on the algorithm's batch size.In order to obtain comparable results, all computed rewards during training should be logged and used for statistical inference.We did not do this as it would require us to diverge more significantly from the original code-base.</p>
<p>Hyper-parameters have a significantly different effect across algorithms and environments, as shown in [9].Reporting both how the selected parameters were obtained, and all parameters themselves, is essential for allowing others to replicate work.</p>
<p>Which general recommendations can we draw from our work?</p>
<p>Separating evaluation from training as highlighted by Khetarpal et al. [11] is significant as RL agents are shown to overfit quite robustly to training instances [18].The use of different preset seeds for training and evaluation also is to be encouraged.While we wished to follow and extend the pipeline in [11], the current states of the proposed pipeline and OpenAI Baselines are incompatible and are thus subject for future work.</p>
<p>Thorough documentation is essential to ensure reproducibility and comparability, as even small details of an experiment can be critical.One may argue that documentation can become very time consuming and take valuable time away from potential progress.We believe that it is a question of establishing a culture in the field where the documentation of the experimental work is an integrated part of conducting research.Many tools can be utilised to ease the process.Our proposed method, including the configuration files, presents an attempt to ease the documentation process.</p>
<p>Measurement metrics are one of the key issues when comparing RL algorithms.Often, how and when performance is recorded remains unreported [11].Different implementations of algorithms collect, process and store the performance metrics differently, making comparisons difficult if not impossible.</p>
<p>When choosing the right metric when reporting the results of conducted experiments, the designer must first make clear what makes an algorithm good.Next, the designer must determine how to measure that.In general, there are two aspects to consider: 1) Good performance describes how well the algorithm works in the specific task addressed, and 2) Efficient use describes the cost of achieving a satisfactory performance and might include the time spent optimising hyper-parameter choices, the effort required to compute the algorithm's output, and/or the cost of the data obtained during training.Metrics can measure either of these, and it is the designer's choice according to what problem is sought solved, but in order to ensure comparability in scientific papers the authors should check which standard metrics are reported and report those, as well as potential additional ones selected by the authors.Further, the metrics chosen should be reported, explained, and argued for, including how they are computed.</p>
<p>Open-Source Research Through this work we advocate for open-sourcing all aspects of research, including source code, as we believe it to be paramount to ensure the continued progress of the scientific fields.As a bare minimum, a study or experiment should be advertised with enough details so any third-party scientist, with sufficient skills, can obtain the same results within the marginals of experimental error [12].Many researchers fail even to meet these simple though crucial requirements due to all sorts of reasons [1].</p>
<p>The idea of a trade-off between fast continued progress and rigorous analysis is one we have met from multiple sources.We believe that irreproducible research is not actually research, merely data -and poor quality of data at that -and that there is not really a trade-off between going somewhere carefully and possibly nowhere fast.As the number of papers re-coining the terms are very large we present only a few of them here as a complete review is beyond the scope of this work.</p>
<p>Goodman et al. [27] proposes a lexicon for reproducibility by differentiating between methods reproducibility, results reproducibility and inferential reproducibility to solve the terminology confusion.</p>
<p>The three new reproducibility terms are defined below, where Goodmann et al. [27] argues that the definitions should be clear in principle but operationally elusive, why they provide many operational examples specific to certain scientific fields.</p>
<p>• Methods reproducibility: provide enough detail about study procedures and data so the same procedures could, in theory or in actuality, be exactly repeated.</p>
<p>• Results reproducibility: obtain same results from the conduct of an independent study whose procedures are as closely matched to the original experiment as possible.</p>
<p>• Inferential reproducibility: draw similar conclusions from either an independent study replication or a a reanalysis of the original study.See Gilbert et al. [28] for a debate on the difference between results and inferential reproducibility.</p>
<p>The coining of these three terms is an attempt to make it more specific what aspects of "trustworthiness" we focus on, when analysing a study.Thus avoiding the ambiguity caused by everyday language's indifferent use of the words; repeatable, reproducible and replicable.</p>
<p>Tatman et al. [29] proposes a practically oriented taxonomy for reproducibility more or less specific for ML research.As Goodman et al. [27], they are attempting to make the principally clear definitions of the terms less operationally elusive by comprehensive descriptions of to what extend details, code and data are shared.While we applaud the author's take on a simple and practical taxonomy we discourage the seemingly indifferent use of the term reproducibility [29] as it contradicts the Claerbout, Donoho, Peng convention, which could have been easily avoided.</p>
<p>• Low reproducibility: Finished paper only is essentially replicability in the Claerbout, Donoho, Peng convention where a well-written publication -in theory -should be enough for a third party scientist to replicate the study or experiment.However, as the authors argue, this is often impractical and even impossible given time constraints and/or missing information.</p>
<p>• Medium reproducibility: Code and data, no environment relates to reproducibility in the Claerbout, Donoho, Peng convention.The original paper from Claerbout [14] does not explicitly describe whether the environment should be a part of the scholarship or not as the tools making this possible simply weren't available at the time.With today's tools such as Docker, we argue that it should be a part of it.LeVeque [30], argues that as long as the code is present alongside the publication then it is not critical whether the code runs or not, as the code itself contains a wealth of details that may not appear in the description.The bare minimum requirement should hold that information regarding the environment -versions, etc. -should be a critical part of the publication.</p>
<p>• High reproducibility: Code, data and environment also relates to reproducibility in the Claerbout, Donoho, Peng convention where, as discussed above, the environment is included, making the reproducibility process easier for the reviewer or reader as this researcher does not have to mess around with installing all kinds of libraries in certain versions on top of his/hers functioning system.</p>
<p>A1.3 Discussion &amp; Conclusion</p>
<p>Firstly, we believe that we should honor the often cited quote from Buckheit &amp; Donoho [15] paraphrasing Jon Claerbout [14] and submit our code alongside our publications.</p>
<p>"An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship.The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures."</p>
<p>Secondly we encourage the community to find some common ground regarding the terminology, so focus once again can be on evolving the scientific field instead of terminology.The bare minimum requirement is that researchers at least state what they mean when they use the terms.</p>
<p>We wish to highlight that reproducibility is not a "free-pass" for the readers to use without properly investigating the submitted code.Consider the case where a researcher is to reproduce the results of a published scientific paper and has hence obtained the original code and data.This third-party researcher can now re-run the code and (hopefully) obtain the same results.The problem occurs when the code is run without it being understood, making it possible for the third-party researcher to obtain the results without him finding the potential bugs in the code from the original author.This is likewise the case when someone commits fraud and makes the fraud reproducible.Our claim is that when other researchers tries to build on top of faulted experiments and thus transfer the methodologies to other domains the fraud or bugs will -often, if not always -become apparent.This, as we assume that it must take some significant hand-engineering to make the fraud reproducible to the specific problem, making it non-generic.</p>
<p>A2 Practicalities for Ensuring Reproducibility</p>
<p>As discussed in [7] there exists many practical issues when setting up RL tasks on real-world robots, especially when attempting to ensure reproducibility.Below we outline some of the most important practices that should be encouraged in order to ensure reproducibility.</p>
<p>Seed the Random Number Generator A simple, yet effective, approach is to seed the random number generator, with a set of predefined seeds, so experiments can be repeated and reproduced.</p>
<p>For an overview of the different sources of non-determinism in ML see appendix A6.</p>
<p>Avoiding the Dependency Hell To avoid (most of) the problems related to the colloquial term dependency hell, researchers can advantageously utilise tools such as Docker.For this work, we adapted the SenseAct framework so we could build it into a Docker image.</p>
<p>Code Reviews considers how to ensure the integrity of code by letting others review it prior to conducting experiments.This indirectly enforces developers to write understandable code (including comments).This might take a little longer than simply throwing pieces of code together, but the benefits of code reviewing prior to testing should motivate most development teams to conform to this practice.One of the largest benefits is the increased probability for avoiding bad experiments due to bugs in the code, as two sets of eyes are commonly known to be better than one.</p>
<p>Open-Sourcing Gathered Artefacts Gathered data should, as well as code, be open-sourced.This helps verifying reproductions and potential replications.All data used for describing the methods shown in this work is available at our GitHub repository 9 .</p>
<p>A3 Comprehensive List of Hyper-parameter Ranges and Values</p>
<p>This appendix presents all hyper-parameter values that we have used to conduct our experiments.The hyper-parameter configurations used in this work are presented in table 3 and table 4. The hyperparameters that the authors of the original work chose not to tune have been kept fixed throughout our experiments and are presented in table 5. We wish to highlight that for; a) TRPO the step-size is denoted vf_stepsize, b) PPO the step-size is denoted optim_stepsize.This information is obtained through correspondence with the original authors of [8].</p>
<p>TRPO</p>
<p>A4 Fitting of Theoretical Distributions</p>
<p>We fit 100 theoretical distributions 10 to our EDFs and use the KS test to determine the goodness of fit.From this, we can determine which statistic to use in order to conduct the hypothesis testing.</p>
<p>KS Test on TRPO</p>
<p>A6 Sources of Non-Determinism in Machine Learning</p>
<p>In ML, in contrast to studies in e.g.chemistry and social sciences, we have the ability to create code (methods) and datasets (observations) that, if open-sourced, other scientists can use to reproduce the results from the original published research.From this overly simplified statement it should seem that reproducibility shouldn't be a problem in ML.There do, however, exists a reproducibility crisis in the field of ML.This crisis is often caused by the nature of ML but also non-rigorous testing approaches and sparsely documented hyperparameters have part in the crisis [1,9].</p>
<p>A6.1 Common Causes of Non-Determinism</p>
<p>Some of the most important causes for general non-determinism in ML are listed below.It should be noted that this list assumes that we are attempting to reproduce an ML experiment, hence these problems can occur even when we have obtained the original code and data.</p>
<p>• GPU: GPU floating point calculations -made through Nvidia's neural network library; CuDNN -are not guaranteed to generate the bit-wise reproducibility across different GPU versions and architectures, but should generate the same bit-wise results across runs when executed on GPUs with the same architecture and number of SMs11 .</p>
<p>• Third-Party Libraries Often, the libraries used are using other libraries which might in turn use stochastic processes needing a seed to a different random number generator.</p>
<p>A6.2 Non-Determinism in Deep Learning</p>
<p>In addition to the general sources of non-determinism we here present sources specific for deep learning (DL).</p>
<p>• Random Initialization of Weights: Often, the layer weights of a neural network is initialized by sampling from a particular distribution to aspire faster convergence [33,34].The initialization must be the same from run-to-run in order to expect same results and not similar results.</p>
<p>• Shuffling of the Datasets: To avoid the optimization functions getting stuck in local minima, the training of neural networks often occurs by dividing the dataset into mini-batches.It is also shown that shuffling the data after each epoch reduces the bias between gradient updates making the model more general as it tends to overfit less.</p>
<p>• Random Sampling: If we are in that luxurious position that we have to much data to reasonably work with we draw a random subsample from the dataset to train our model with.• Random Train/Test/Validation Splits: When data availability is low the go-to validation method is k-fold cross validation where the dataset is stochastically split into two or three sets of data.• Stochastic Attributes of the Hidden Layers: One of the most often used techniques for preventing overfitting is dropout [35] which is inherently random during the training process.</p>
<p>To reproduce or repeat the training process of a certain neural network originally trained with dropout one must know which neurons are excluded at what times through the original training process.</p>
<p>A6.3 Non-Determinism in (Deep) Reinforcement Learning</p>
<p>Here we describe the sources of non-determinism related to both RL and DRL.</p>
<p>• Environment: Especially when dealing with real-world robotic RL, sensor delays, etc. supports the statement that our world is stochastic.• Network initialisation: As in DL the initialisation of the neural networks' weights are a stochastic process and must thus be controlled for to ensure reproducibility.• Minibatch sampling: Several algorithms within DRL includes sampling randomly from the training data and from replay buffers [36].</p>
<p>Although some of the aforementioned sources of non-determinism can be successfully managed, many researchers does not control, or report how they have controlled for the non-determinism in their experiments.We find it necessary to advocate for presetting seeds for the random processes in the code, in order to remove or at least reduce the stochasticity of the reported experiments.</p>
<p>Figure 2 :
2
Figure2: Repeatability of learning: The plots show the average return obtained over time during training, computed as a rolling average with a window size of 5.000 steps, calculated every 1.000 steps.(left) Ten runs of hyperparameter configuration 1 for TRPO using same seed and evaluated on the real world robot, (right) Ten runs using the same RL algorithm with same code base using the same seed but evaluated in UR's simulator: URSim v. 3.9.1 5 .</p>
<p>Figure 3 :
3
Figure 3: Top-5 hyperparameter configurations from the random search:The mean average reward is plotted with its SE, computed from the ten runs conducted for each of the five hyperparameter configurations for each of the two algorithms; (left) TRPO and (right) PPO.The average return is computed by a rolling average with a window size of 5000, and computed every 1.000 steps.</p>
<p>Figure 4 :
4
Figure4: The failed run of PPO: The 9 th run of the 4 th configuration for PPO failed for some unknown reason.We assume that this run is an outlier and not part of the true population.Thus, we exclude it from the statistical analysis.</p>
<p>Figure 5 :Figure 5 :
55
Figure 5: Theoretical distribution fitting on empirical data from TRPO: (top) 52 theoretical distributions fitted to the empirical data of the 1 st configuration of TRPO, (left) top-6 theoretical distributions (chosen based on p-value), (right) best fitted theoretical distributions, each row corresponds to the hyperparameter configuration.(figure continues on next page)</p>
<p>Figure 6 :
6
Figure 6: Theoretical distribution fitting on empirical data from PPO: (top) 52 theoretical distributions fitted to the empirical data of the 1 st configuration of PPO, (left) top-6 theoretical distributions (chosen based on p-value), (right) best fitted theoretical distributions, each row corresponds to the hyperparameter configuration.</p>
<p>Figure 7 :
7
Figure 7: Empirical distributions obtained by bootstrapping: (top) TRPO and (bottom) PPO.All ten hyper-parameter configurations are represented in the histograms with a normal distribution fitted to it.Note that even though the data appears normally distributed, our normality test rejects 8/10 of our null-hypothesis, meaning that the samples were not normally distributed.See table 8 for corresponding p-values.(figure continues on next page)</p>
<p>Table 1 :
1
Overall performance achieved by the two baselines:
AlgorithmsTRPOPPO[8]Ours[8]Oursμμ95% CIμμ95% CIc1158.56 135.78 (127.31, 144.78) 176.62 137.08 (116.64, 157.73)c2138.58 139.65 (128.04, 153.28) 150.2586.51 (58.48, 115.48)c3131.35 112.37 (91.38, 134.72)137.9290.12 (64.28, 118.38)c4123.4598.03 (93.34, 103.18)137.26 119.43 (107.98, 130.31)c5122.60 106.62 (95.57, 118.60)136.0982.42 (62.58, 104.15)</p>
<p>Table 2 :
2
[8]ification of reproducibility: The table depicts the probabilities of obtaining a new sample at least as good as the one reported in[8]under the distributions listed.A failure to reject our hypothesis is indicated by .theresultsareshown in table 2. If we have the probabilities P d that the distributions match the underlying EDFs (reported in appendix A4) described as P {dist = d|data} = P d , and the probability P v that we can get a value, v, at least as good (from that specific distribution), then we have P {v ≥ μ|dist = d, data} = P v .Thus P {v ≥ μ|data} = P d • P v , where μ denotes the single value of average return reported in[8]and table 1.
AlgorithmsDistributionsTRPOPPOc1c2c3c4c5c1c2c3c4c5beta0.0000 0.59900.04360.00000.00220.00000.00000.00000.00150.0000johnsonsb0.0000 0.59850.02460.00000.00220.00000.00000.00000.00150.0000johnsonsu0.0000 0.37490.04170.00000.00150.00000.00000.00000.00110.0000loggamma0.0000 0.38940.04240.00000.00150.00000.00000.00000.00040.0000powernorm0.0000 0.27980.04070.00000.00130.00000.00000.00000.00120.0000skewnorm0.0000 0.25180.04110.00000.00140.00000.00000.00000.00100.0000</p>
<p>Supplementary Materials: All library code, hence adaptations to the SenseAct framework, is publicly available at: https://github.com/dti-research/SenseActand the docker image we have used throughout this work is available at: https://hub.docker.com/r/dtiresearch/senseact.All the experimental code and data to regenerate the figures in this manuscript is available at https://github.com/dti-research/SenseActExperiments.If you find something missing or not working, please feel free to contact the lead author.All authors have made substantial contributions to the conception and design of the work.Nicolai A. Lynnerup and Laura Nolling wrote the original draft of this manuscript while John Hallam and Rasmus Hasle have substantively revised it.Laura Nolling made the changes to the SenseAct framework in order to make it callable by external programs and added logging functionality.Further, Laura Nolling devised the bootstrapping and evaluation scripts, while Nicolai A. Lynnerup programmed the distribution-fitting script and reviewed and edited evaluation and bootstrapping scripts.Nicolai A. Lynnerup created the Docker images and adapted the SenseAct framework to work with all versions of the CB-series UR robots, based on the work of Oliver Limoyo 8 , Ph.D. Student at the University of Toronto.Further, Nicolai A. Lynnerup conducted the literature review on reproducibility.Rasmus Hasle and John Hallam provided critical feedback and helped shape the research and analyses.Nicolai A. Lynnerup, Rasmus Hasle, and John Hallam secured the research funding, while all authors have approved the submitted version.Funding: The work presented in this paper is partially funded by the Danish Technological Institute and partially Innovation Fund Denmark through their Industrial Researcher Program, grant 8053-00057B.
Author Contributions:</p>
<p>Table 3 :
3
[8]er-parameter configurations found by random search: The table presents the top-5 configurations found for TRPO in[8].</p>
<h1>Average ReturnHidden LayersHidden SizeBatch SizeStep Sizeγλδ KL1158.562644096 0.00472 0.96833 0.99874 0.024372138.5811282048 0.00475 0.99924 0.99003 0.019093131.354648192 0.00037 0.97433 0.99647 0.312224123.4541284096 0.00036 0.99799 0.92958 0.019525122.604322048 0.00163 0.96801 0.96893 0.00510PPO#Average ReturnHidden LayersHidden SizeBatch SizeStep SizeγλOpt. Batch Size1176.62364512 0.00005 0.96836 0.99944162150.25116256 0.00050 0.99926 0.98226643137.9212048512 0.00011 0.99402 0.9018584137.264322048 0.00163 0.96801 0.9689310245136.0911282048 0.00280 0.99924 0.9900332</h1>
<p>Table 4 :
4
[8]er-parameter configurations found by random search: The table presents the top-5 configurations found for PPO in[8].
HyperparameterFixed ValuesTRPOPPOMax. Timesteps150, 000150, 000Entropy Coef.0.00.0CG Iterations10-CG Damping1e-2-VF Iterations3-Clip Parameter-0.2Optim. Epochs-10Adam-1e-5</p>
<p>Table 5 :
5
Fixed hyperparameter values:The table shows the fixed hyperparameter values that are not included in the random search.</p>
<p>Table 6 :
6
Probability values obtained by KS test: The table shows the p-values from fitting six theoretical distributions to our EDFs.
Distribution Statistics p-value Distribution ParametersConfiguration 1beta johnsonsb johnsonsu loggamma powernorm 0.0075 0.0082 0.0082 0.0079 0.00800.5188 (824.65, 167.66, -175.37, 374.38) 0.5087 (-7.13, 9.58, 3.31, 195.55) 0.5644 (10.94, 15.94, 178.02, 56.88) 0.5406 (79.15, -36.56, 39.48) 0.6235 (1.77, 138.22, 5.22)skewnorm0.00750.6189 (-0.92, 138.62, 5.29)Configuration 2beta johnsonsb johnsonsu loggamma powernorm 0.0085 0.0044 0.0044 0.0075 0.00740.9911 (18.83, 8.83, 89.22, 74.13) 0.9903 (-1.62, 2.71, 89.67, 78.02) 0.6204 (12.79, 8.57, 189.57, 23.46) 0.6443 (10.59, 92.24, 20.53) 0.4630 (5.39, 151.53, 9.81)skewnorm0.00880.4167 (-1.53, 145.51, 8.69)Configuration 3beta johnsonsb johnsonsu loggamma powernorm 0.0063 0.0058 0.0083 0.0061 0.00600.8847 (456.27, 282.03, -269.74, 618.35) 0.4996 (12132.2, 16581.66, -270975.92, 834554.97) 0.8466 (13.44, 32.78, 253.12, 333.52) 0.8605 (645.48, -1703.51, 280.7) 0.8245 (1.2, 114.22, 11.64)skewnorm0.00620.8343 (-0.58, 117.21, 12.05)Configuration 4beta johnsonsb johnsonsu loggamma powernorm 0.0079 0.0064 0.0064 0.0071 0.00730.8108 (22.36, 12.28, 77.62, 31.58) 0.8028 (-1.5, 3.17, 76.79, 34.56) 0.6874 (14.6, 11.91, 123.33, 16.21) 0.6683 (22.52, 61.28, 11.88) 0.5575 (2.92, 100.79, 3.36)skewnorm0.01340.0555 (0.0, 98.01, 2.53)Configuration 5beta johnsonsb johnsonsu loggamma powernorm 0.0078 0.0053 0.0053 0.0073 0.00730.9402 (41.71, 27.17, 45.53, 100.9) 0.9391 (-1.53, 4.6, 41.09, 112.69) 0.6597 (18.73, 20.62, 194.25, 84.29) 0.6580 (88.61, -141.39, 55.38) 0.5762 (1.67, 109.54, 6.81)skewnorm0.00770.5887 (-0.87, 110.27, 6.93)</p>
<p>Table 7 :
7
[31,32]lity values obtained by KS test:The table shows the p-values from fitting six theoretical distributions to our EDFs.A5 Normality Test of Empirical DistributionsTo test if the empirical distributions, created by bootstrapping, are normally distributed we conduct a normality test (based on D'Agostino and Pearson's test[31,32]) which rejects 8/10 of our nullhypothesis, H 0 .Thus we cannot assume that the empirical distributions in figure7.The resulting p-values from the normality test is presented in table 8.</p>
<p>All ten hyper-parameter configurations are represented in the histograms with a normal distribution fitted to it.Note that even though the data appears normally distributed, our normality test rejects 8/10 of our null-hypothesis, meaning that the samples were not normally distributed.See table 8 for corresponding p-values.(figure continues on next page)
Hyper-parameterAlgorithmsconfigurationTRPOPPO1Rejected (p=0.0002)Failed to reject (p=0.1604)2Rejected (p=5.17e−33)Failed to reject (p=0.1212)3Rejected (p=0.0034)Rejected (p=7.24e−17)4Rejected (p=1.41e−6)Rejected (p=0.0012)5Rejected (p=0.0005)Rejected (p=4.69e−09)</p>
<p>Table 8 :
8
Hypothesis testing for normality: The table depicts the decision based on the normality test and the resulting p-values to support those decisions.Formally, the test rejects 8 out of 10 hypotheses, thus failing to reject 2 of them.From this we derive that none of the 10 empirical distributions are normally distributed.</p>
<p>https://github.com/kindredresearch/SenseAct/
For a complete example of a configuration file, see: https://github.com/dti-research/ SenseActExperiments/blob/master/code/experiments/ur5/trpo_kindred_example.yaml
Based on correspondence with the lead author
For the comprehensive list see: https://docs.scipy.org/doc/scipy/reference/stats.html
Available for download here: https://www.universal-robots.com/download/?option=51823
https://www.docker.com/
https://github.com/kindredresearch/SenseAct
https://github.com/kindredresearch/SenseAct/pull/29
https://github.com/dti-research/SenseActExperiments
For the comprehensive list see: https://docs.scipy.org/doc/scipy/reference/stats.html
https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html# reproducibility
Acknowledgements: This work benefited from the help of many people beyond the authors.We want to show our gratitude to Jens-Jakob Bentsen, and Thomas Mosgaard Giselsson, specialists at Danish Technological Institute (DTI) for numerous discussions on different aspects of the reproducibility terminology.We further thank Jens-Jakob Bentsen for all his help debugging the UR robot's communication interface and underlying controller functionality.Next, we would like to thank Rasmus Lunding Henriksen, specialist at DTI, for his comments that significantly improved the manuscript.Additionally, we would like to show our gratitude to Kasper Stoy, Professor at ITU Copenhagen, for his comments on our work, which helped us see more perspectives on reproducibility and reporting in science.We would also like to show our gratitude to the two anonymous reviewers for their insights and constructive comments.Any errors are our own and should not tarnish the reputations of these persons nor the institutions.Conflicts of Interest:The authors declare no conflict of interest.The funding sponsors had no role in the design of the research; in the collection, analyses or interpretation of data; in the writing of this manuscript, nor in the decision to, or where to, publish the results.Appendices A1 A Brief Overview of a Confused Taxonomy A1.1 Two Perspectives on TerminologyUnfortunately there exists some confusion on the meaning of repeatability, reproducibility and replicability which in turn negatively affects the overall development of science.Barba [13]group a series of papers into 2 groups; A. Those who do not distinguish between the words; reproducibility and replicability, and B. Those who do distinguish between the two words.Group B is then divided into two additional groups who's contradicting conventions are shown below.B.1. The Claerbout, Donoho, Peng ConventionFrom the pioneering work of Claerbout[14]Buckheit and Donoho[15]and Peng et al.[16]the following convention has been derived, but as the papers are somewhat dated it might be beneficial to read the more recent work by Schwab et al.[19], Donoho[20]and Peng et al.[21]instead.Reproducibility describes a study where the original authors has provided all the necessary observations and potentially computer code to run the method again, allowing a third party scientist to reproduce the same results.Hence; different team with same experimental setup.Replicability is used to describe a third party study that arrives at the same conclusions as an original study, where new observations are collected and the method is implemented based on the published paper.Hence; different team with different experimental setup.B.2. The ACM, Drummond ConventionDrummond[22]published his article at the International Conference on Machine Learning (ICML) in 2006 where he unfortunately switched the definitions of reproducibility and replicability around, which according to Professor of Linguistics Mark Liberman should be rejected[23]as the term was coined much earlier by Claerbout[14].Prior to the suggestion of rejecting the re-coining of terms, Drummond's "new definitions" spread through several scientific papers.Fang et al.[24]and Mende et al.[25]seems to have picked up the confusion of the terms from Drummond.Further the Association for Computing Machinery (ACM) is also using the terms wrong in their badging of artifacts system[26], and they seem to stick with the definition, as it apparently is revised latest in 2017, two years after Mark Liberman published his findings.The ACM, Drummond convention is as follows.Repeatability (same team, same experimental setup) The observations can be obtained with the stated precision by the same researchers using the same method and the same hardware under the same conditions in the same location.Replicability (different team, same experimental setup) The observations can be obtained with the stated precision by third party researchers using the same method, the same hardware under the same conditions in the same or different location.Reproducibility (different team, different experimental setup) The observations can be obtained with the stated precision by third party researchers using different hardware on a different location.We advocate that all researchers refrain from using the terms the ACM, Drummond way as the re-coining of the terms is not justified.The conflicting terminologies are annoying at least and at worst a blockade for the progress of science.A1.2 Expanding the TerminologiesIn addition to re-coining terms, there exists various papers suggesting coining new more descriptive terms as a way out of the heated discussions regarding repeatability, reproducibility and replicability.All ten hyper-parameter configurations are represented in the histograms with a normal distribution fitted to it.Note that even though the data appears normally distributed, our normality test rejects 8/10 of our null-hypothesis, meaning that the samples were not normally distributed.See table 8 for corresponding p-values.
Ten simple rules for reproducible computational research. Geir Kjetil, Sandve , PLoS computational biology. 9e10032852013</p>
<p>Trust region policy optimization. John Schulman, International Conference on Machine Learning. 2015</p>
<p>Proximal policy optimization algorithms. John Schulman, arXiv:1707.063472017arXiv preprint</p>
<p>Continuous control with deep reinforcement learning. Timothy P Lillicrap, arXiv:1509.029712015arXiv preprint</p>
<p>Reinforcement learning with deep energy-based policies. Tuomas Haarnoja, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>Benchmarking deep reinforcement learning for continuous control. Yan Duan, International Conference on Machine Learning. 2016</p>
<p>Setting up a Reinforcement Learning Task with a Real-World Robot. Mahmood Rupam, arXiv:1803.070672018arXiv preprint</p>
<p>Benchmarking Reinforcement Learning Algorithms on Real-World Robots. Mahmood Rupam, arXiv:1809.077312018arXiv preprint</p>
<p>Deep reinforcement learning that matters. Peter Henderson, Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. Riashat Islam, arXiv:1708.041332017arXiv preprint</p>
<p>Re-evaluate: Reproducibility in evaluating reinforcement learning algorithms. Khimya Khetarpal, International Conference on Machine Learning. 2018</p>
<p>Reproducibility vs. replicability: a brief history of a confused terminology. Hans E Plesser, Frontiers in neuroinformatics. 11762018</p>
<p>Terminologies for reproducible research. Lorena A Barba, arXiv:1802.033112018arXiv preprint</p>
<p>Electronic documents give reproducible research a new meaning. Jon F Claerbout, Martin Karrenbach, SEG Technical Program Expanded Abstracts 1992. Society of Exploration Geophysicists. 1992</p>
<p>Wavelab and reproducible research. Jonathan B Buckheit, David L Donoho, Wavelets and statistics. Springer1995</p>
<p>Reproducible epidemiologic research. Francesca Roger D Peng, Scott L Dominici, Zeger, American journal of epidemiology. 16392006</p>
<p>Openai gym. Greg Brockman, arXiv:1606.015402016arXiv preprint</p>
<p>A study on overfitting in deep reinforcement learning. Chiyuan Zhang, arXiv:1804.068932018arXiv preprint</p>
<p>Making scientific computations reproducible. Matthias Schwab, Jon Karrenbach, Claerbout, Computing in Science &amp; Engineering. 262000</p>
<p>Reproducible research in computational harmonic analysis. David L Donoho, Computing in Science &amp; Engineering. 1112009</p>
<p>Reproducible research in computational science. D Roger, Peng, Science. 3342011</p>
<p>Replicability is not reproducibility: nor is it good science. Chris Drummond, 2009</p>
<p>Replicability vs. reproducibility -or is it the other way around?. Mark Liberman, 2015visited on 02/14/2019</p>
<p>Arturo Casadevall and Ferric C Fang. Reproducible science. 2010</p>
<p>Replication of defect prediction studies: problems, pitfalls and recommendations. Thilo Mende, Proceedings of the 6th International Conference on Predictive Models in Software Engineering. the 6th International Conference on Predictive Models in Software EngineeringACM20105</p>
<p>. ACM Result. Artifact Review and Badging. 2017</p>
<p>What does research reproducibility mean. Daniele Steven N Goodman, John Pa Fanelli, Ioannidis, Science translational medicine. 82016</p>
<p>Estimating the reproducibility of psychological science. Gilbert Daniel, Science. 3512016Comment on</p>
<p>A Practical Taxonomy of Reproducibility for Machine Learning Research. Rachael Tatman, Jake Vanderplas, Sohier Dane, 2018</p>
<p>Top ten reasons to not share your code (and why you should anyway). Leveque Randall, Siam News. 4632013</p>
<p>An omnibus test of normality for moderate and large size samples. Ralph B D'agostino, Biometrika. 581971</p>
<p>Tests for departure from normality. Ralph , ' Agostino, Egon S Pearson, Biometrika. 6031973</p>
<p>Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statistics2010</p>
<p>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, The Journal of Machine Learning Research. 1512014</p>
<p>Prioritized experience replay. Tom Schaul, arXiv:1511.059522015arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
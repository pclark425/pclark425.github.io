<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3325 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3325</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3325</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4f03e69963b9649950ba29ae864a0de8c14f1f86</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f03e69963b9649950ba29ae864a0de8c14f1f86" target="_blank">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> K-Adapter is proposed, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion and captures richer factual and commonsense knowledge than RoBERTa.</p>
                <p><strong>Paper Abstract:</strong> We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3325.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3325.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-ADAPTER (F+L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-ADAPTER with Factual and Linguistic Adapters (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-infused model that keeps RoBERTa_LARGE parameters frozen and attaches two independently pre-trained adapters: a factual adapter (relation-classification pretraining) and a linguistic adapter (dependency-head prediction). The adapters' outputs are concatenated with RoBERTa features for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-ADAPTER (F+L) — RoBERTa_LARGE + factual & linguistic adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa_LARGE (24 layers, H=1024, 355M params) is frozen; two knowledge-specific adapter modules (~42M params each) are trained independently: factual adapter (pretrained on relation classification from T-REx-rc) and linguistic adapter (pretrained on dependency head prediction from parsed BookCorpus). For downstream tasks the last hidden features of RoBERTa and each adapter are concatenated and fed to task heads.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa_LARGE 355M + two adapters ≈ 355M + 2×42M ≈ 439M (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Knowledge-infused reasoning via factual-adapter (relation classification pretraining)', 'Knowledge-infused reasoning via linguistic-adapter (dependency relation prediction)', 'Feature-fusion reasoning (concatenation of frozen pre-trained model features with adapter features)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Factual-adapter: pretrained to classify relations between entity pairs using T-REx-rc (5.5M sentences, 430 relations); linguistic-adapter: pretrained to predict dependency head index for tokens using parser outputs on BookCorpus (1M examples). Both adapters are trained with RoBERTa frozen; at fine-tuning, adapter and RoBERTa outputs are concatenated to produce representations for downstream reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — integrates two distinct reasoning/knowledge-infusion styles (factual and syntactic) trained independently and combined at inference, providing complementary signals rather than a single homogeneous objective.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple knowledge-driven tasks: entity typing (OpenEntity, FIGER), relation classification (TACRED), question answering (SearchQA, Quasar-T, CosmosQA), and LAMA probing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Entity typing: predict fine-grained types of entities in context; Relation classification (TACRED): predict relation between two entities in a sentence; QA: extractive/open and multiple-choice commonsense QA; LAMA: cloze-style factual probing (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Entity typing (OpenEntity micro F1): K-ADAPTER (F+L) = 77.61 vs RoBERTa = 76.23 vs RoBERTa+multitask = 76.97. FIGER (Accuracy / Ma-F1 / Mi-F1): K-ADAPTER (F+L) = 61.81 / 84.87 / 80.54 vs RoBERTa = 56.31 / 82.43 / 77.83 vs RoBERTa+multitask = 59.86 / 84.45 / 78.84. QA SearchQA (EM / F1): 61.96 / 67.31 vs RoBERTa 59.01 / 65.62 vs RoBERTa+multitask 59.92 / 66.67. QA Quasar-T (EM / F1): 46.32 / 53.00 vs RoBERTa 40.83 / 48.84 vs RoBERTa+multitask 44.62 / 51.17. CosmosQA Accuracy: 81.83 vs RoBERTa 80.59 vs RoBERTa+multitask 81.19. TACRED (micro F1): K-ADAPTER (F+L) = 72.04 vs RoBERTa = 71.25 vs RoBERTa+multitask = 71.62. LAMA P@1 (selected): LAMA-Google-RE = 7.0 vs RoBERTa = 4.8; LAMA-T-REx = 29.1 vs RoBERTa = 27.1; LAMA-UHN-T-REx = 23.0 vs RoBERTa = 20.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper explicitly compares K-ADAPTER (combined adapters) to (a) the base RoBERTa_LARGE, and (b) RoBERTa pretrained with a joint multi-task objective (RoBERTa+multitask). Key experimental contrasts show that combining distinct adapters (factual + linguistic) typically yields higher scores across entity typing, relation classification, QA and LAMA probing than both RoBERTa and RoBERTa+multitask. Ablations (single adapters) also evaluated to show complementary benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse knowledge-specific adapters (factual + linguistic) provides consistent improvements over both the unfrozen baseline and a single multi-task joint-training approach; independent adapter pretraining prevents interference and supports continual infusion of heterogeneous knowledge sources, yielding richer factual and commonsense knowledge and better downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Single adapters (F or L) sometimes fall slightly behind RoBERTa+multitask on a few metrics (the paper notes ablated K-ADAPTER models are 'clearly better than RoBERTa, but slightly lose compared with RoBERTa+multitask' in some QA ablations). K-ADAPTER without knowledge (adapter structure but no knowledge pretraining) performed slightly worse than RoBERTa on OpenEntity, indicating gains come from injected knowledge rather than extra parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3325.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3325.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-ADAPTER (F)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-ADAPTER with Factual Adapter only</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>K-ADAPTER variant attaching only the factual adapter pretrained on relation classification (T-REx-rc) with RoBERTa_LARGE frozen; used to infuse factual knowledge into downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-ADAPTER (F) — RoBERTa_LARGE + factual adapter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa_LARGE frozen; a single factual adapter (~42M params) pretrained on relation classification over automatically aligned Wikipedia-Wikidata triples (T-REx-rc). Adapter output concatenated with RoBERTa features for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa_LARGE 355M + factual adapter ≈ 397M (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Factual-knowledge infusion via relation-classification pretraining']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Adapter trained to predict the relation label between entity pairs given context; last hidden features of RoBERTa and adapter are concatenated; training used cross-entropy loss on 5.5M sentences from T-REx-rc.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style — focuses on factual relation-based knowledge rather than combining heterogeneous knowledge sources.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same downstream tasks as main experiments (entity typing, relation classification, QA, LAMA probing) evaluated as an ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above; factual adapter intended to improve tasks relying on factual relations and entity knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>OpenEntity micro F1: 77.53 (vs RoBERTa 76.23). FIGER (Acc/Ma-F1/Mi-F1): 59.50 / 84.52 / 80.42. SearchQA (EM / F1): 61.85 / 67.17. Quasar-T (EM / F1): 46.20 / 52.86. CosmosQA Acc: 80.93. TACRED micro F1: 71.89. LAMA (selected): not reported separately in table for F-only, but factual adapter improves P@1 over RoBERTa when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Ablation shows factual-only adapter improves over base RoBERTa across many tasks; compared to RoBERTa+multitask, factual-only sometimes performs slightly worse on some QA metrics but still yields strong gains; combining with linguistic adapter gives additive improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Factual-only adapter injects useful relation knowledge that improves entity- and relation-focused downstream tasks; factual knowledge complements linguistic adapter when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Factual-only sometimes slightly underperforms RoBERTa+multitask on certain QA ablations (paper notes single adapters 'slightly lose compared with RoBERTa+multitask' in some QA metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3325.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3325.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-ADAPTER (L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-ADAPTER with Linguistic Adapter only</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>K-ADAPTER variant attaching only the linguistic adapter pretrained on dependency-relation prediction (predicting head indices) with RoBERTa_LARGE frozen; used to infuse syntactic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-ADAPTER (L) — RoBERTa_LARGE + linguistic adapter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa_LARGE frozen; a single linguistic adapter (~42M params) pretrained on dependency head prediction using parsed BookCorpus (1M examples). Adapter outputs concatenated with RoBERTa features for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa_LARGE 355M + linguistic adapter ≈ 397M (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Linguistic/syntactic infusion via dependency-head prediction pretraining']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Adapter trained with binary cross-entropy (BCEWithLogits) on head-index prediction per token; concatenated adapter+RoBERTa features used for downstream classification or span prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style — focuses on syntactic/linguistic reasoning rather than multiple heterogeneous knowledge sources.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Evaluated across the same downstream benchmarks (entity typing, relation classification, QA).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Linguistic adapter targets syntactic signal injection to help tasks sensitive to syntactic relations (e.g., relation classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>OpenEntity micro F1: 76.89. FIGER (Acc/Ma-F1/Mi-F1): 61.10 / 83.61 / 79.18. SearchQA (EM / F1): 61.15 / 66.82. Quasar-T (EM / F1): 45.66 / 52.39. CosmosQA Acc: 80.76. TACRED micro F1: 71.96.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Linguistic-only adapter improves over base RoBERTa on multiple tasks and often complements factual adapter; combining adapters typically yields the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Linguistic (syntactic) knowledge injected via an adapter provides complementary improvements to factual knowledge, and alone improves many reasoning tasks compared to base RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>As with factual-only, linguistic-only can underperform the multi-task RoBERTa+multitask baseline on some QA ablations; gains are most pronounced when combined with factual adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3325.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3325.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa+multitask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa_LARGE pretrained with multi-task knowledge objectives</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where RoBERTa_LARGE parameters are updated (not frozen) and trained jointly on multiple knowledge-infusion pretraining tasks (relation classification and dependency relation prediction) via multi-task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa+multitask</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa_LARGE (355M) fine-tuned / further pretrained with multiple knowledge-driven objectives simultaneously (multi-task learning across factual and linguistic pretraining tasks), updating the entire model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>RoBERTa_LARGE 355M (no adapters)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Multi-task joint training with entangled parameter updates (factual relation prediction + dependency prediction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>RoBERTa's parameters are jointly updated with multiple objectives so the model learns to satisfy both factual and linguistic pretraining tasks simultaneously; this is a single entangled model rather than separate modular adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar-style (single-model, multi-task): multiple objectives are used but training updates the same shared parameters, leading to entangled representations rather than explicitly modular diverse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same downstream tasks in paper: entity typing, relation classification, question answering, LAMA probing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-task pretraining aims to incorporate heterogeneous knowledge into the same parameter set via joint optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>OpenEntity micro F1: 76.97 (vs RoBERTa 76.23). FIGER (Acc/Ma-F1/Mi-F1): 59.86 / 84.45 / 78.84 (vs RoBERTa 56.31 / 82.43 / 77.83). SearchQA (EM / F1): 59.92 / 66.67 (vs RoBERTa 59.01 / 65.62). Quasar-T (EM / F1): 44.62 / 51.17 (vs RoBERTa 40.83 / 48.84). CosmosQA Acc: 81.19 (vs RoBERTa 80.59). TACRED micro F1: 71.62 (vs RoBERTa 71.25).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RoBERTa+multitask (single shared parameters) generally improves over vanilla RoBERTa; however, the K-ADAPTER combined approach (diverse modular adapters) often outperforms RoBERTa+multitask, showing modular, independently trained adapters give better utilization of heterogeneous knowledge in many downstream benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint multi-task training of the entire RoBERTa parameters can improve over base RoBERTa, but modular adapter-based infusion of distinct knowledge types (K-ADAPTER) typically yields equal or better downstream reasoning performance and avoids catastrophic forgetting when adding new knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper notes that multi-task joint training entangles representations and, when injecting new knowledge later, previously injected knowledge may fade (catastrophic forgetting). Empirically, RoBERTa+multitask is sometimes slightly stronger than single adapters on certain ablations, indicating joint training can be competitive but lacks modularity and continual-infusion advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3325.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3325.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa_LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large masked-language-model transformer (BERT variant) used as the frozen backbone in K-ADAPTER experiments; baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa_LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based pretrained language model (24 layers, hidden size 1024, 16 attention heads, 355M parameters) trained with robust LM pretraining recipes; used here as backbone whose parameters are frozen during adapter pretraining and optionally finetuned during downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['General masked-language-model pretraining (surface-form and distributional reasoning)', 'Baseline feature provider for adapters']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>RoBERTa learns contextual representations via masked-language modeling; in K-ADAPTER experiments it provides base features (frozen during adapter pretraining) which are concatenated with adapter features at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single general pretraining style (masked language modeling); not explicitly using multiple modular reasoning strategies unless combined with adapters or multi-task finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Used as base model for downstream tasks: entity typing, relation classification, QA, LAMA probing</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Provides contextual embeddings used directly (baseline) or augmented by adapters for downstream reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>OpenEntity micro F1: 76.23. FIGER (Acc/Ma-F1/Mi-F1): 56.31 / 82.43 / 77.83. SearchQA (EM / F1): 59.01 / 65.62. Quasar-T (EM / F1): 40.83 / 48.84. CosmosQA Acc: 80.59. TACRED micro F1: 71.25. LAMA P@1 (selected): Google-RE 4.8; T-REx 27.1; LAMA-UHN-T-REx 20.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Baseline comparisons show that augmenting RoBERTa with either multi-task retraining or modular adapters improves downstream reasoning, with modular diverse adapters (K-ADAPTER F+L) typically providing larger and more consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RoBERTa provides strong general representations, but explicitly injecting knowledge via adapters or multi-task finetuning yields improved reasoning capabilities; modular adapter approach better preserves previously injected knowledge and supports continual infusion.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RoBERTa sometimes underperforms adapter-augmented variants on factual probing (LAMA) and several downstream tasks, indicating that vanilla masked-language pretraining does not capture some factual or syntactic knowledge as strongly as targeted adapter pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ERNIE: Enhanced Language Representation with Informative Entities <em>(Rating: 2)</em></li>
                <li>Knowledge Enhanced Contextual Word Representations <em>(Rating: 2)</em></li>
                <li>Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model <em>(Rating: 2)</em></li>
                <li>Parameter-Efficient Transfer Learning for NLP <em>(Rating: 2)</em></li>
                <li>Language Models as Knowledge Bases? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3325",
    "paper_id": "paper-4f03e69963b9649950ba29ae864a0de8c14f1f86",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "K-ADAPTER (F+L)",
            "name_full": "K-ADAPTER with Factual and Linguistic Adapters (combined)",
            "brief_description": "A knowledge-infused model that keeps RoBERTa_LARGE parameters frozen and attaches two independently pre-trained adapters: a factual adapter (relation-classification pretraining) and a linguistic adapter (dependency-head prediction). The adapters' outputs are concatenated with RoBERTa features for downstream tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "K-ADAPTER (F+L) — RoBERTa_LARGE + factual & linguistic adapters",
            "model_description": "RoBERTa_LARGE (24 layers, H=1024, 355M params) is frozen; two knowledge-specific adapter modules (~42M params each) are trained independently: factual adapter (pretrained on relation classification from T-REx-rc) and linguistic adapter (pretrained on dependency head prediction from parsed BookCorpus). For downstream tasks the last hidden features of RoBERTa and each adapter are concatenated and fed to task heads.",
            "model_size": "RoBERTa_LARGE 355M + two adapters ≈ 355M + 2×42M ≈ 439M (approx.)",
            "reasoning_methods": [
                "Knowledge-infused reasoning via factual-adapter (relation classification pretraining)",
                "Knowledge-infused reasoning via linguistic-adapter (dependency relation prediction)",
                "Feature-fusion reasoning (concatenation of frozen pre-trained model features with adapter features)"
            ],
            "reasoning_methods_description": "Factual-adapter: pretrained to classify relations between entity pairs using T-REx-rc (5.5M sentences, 430 relations); linguistic-adapter: pretrained to predict dependency head index for tokens using parser outputs on BookCorpus (1M examples). Both adapters are trained with RoBERTa frozen; at fine-tuning, adapter and RoBERTa outputs are concatenated to produce representations for downstream reasoning tasks.",
            "diversity_of_methods": "Diverse — integrates two distinct reasoning/knowledge-infusion styles (factual and syntactic) trained independently and combined at inference, providing complementary signals rather than a single homogeneous objective.",
            "reasoning_task_name": "Multiple knowledge-driven tasks: entity typing (OpenEntity, FIGER), relation classification (TACRED), question answering (SearchQA, Quasar-T, CosmosQA), and LAMA probing",
            "reasoning_task_description": "Entity typing: predict fine-grained types of entities in context; Relation classification (TACRED): predict relation between two entities in a sentence; QA: extractive/open and multiple-choice commonsense QA; LAMA: cloze-style factual probing (zero-shot).",
            "performance_by_method": "Entity typing (OpenEntity micro F1): K-ADAPTER (F+L) = 77.61 vs RoBERTa = 76.23 vs RoBERTa+multitask = 76.97. FIGER (Accuracy / Ma-F1 / Mi-F1): K-ADAPTER (F+L) = 61.81 / 84.87 / 80.54 vs RoBERTa = 56.31 / 82.43 / 77.83 vs RoBERTa+multitask = 59.86 / 84.45 / 78.84. QA SearchQA (EM / F1): 61.96 / 67.31 vs RoBERTa 59.01 / 65.62 vs RoBERTa+multitask 59.92 / 66.67. QA Quasar-T (EM / F1): 46.32 / 53.00 vs RoBERTa 40.83 / 48.84 vs RoBERTa+multitask 44.62 / 51.17. CosmosQA Accuracy: 81.83 vs RoBERTa 80.59 vs RoBERTa+multitask 81.19. TACRED (micro F1): K-ADAPTER (F+L) = 72.04 vs RoBERTa = 71.25 vs RoBERTa+multitask = 71.62. LAMA P@1 (selected): LAMA-Google-RE = 7.0 vs RoBERTa = 4.8; LAMA-T-REx = 29.1 vs RoBERTa = 27.1; LAMA-UHN-T-REx = 23.0 vs RoBERTa = 20.1.",
            "comparison_of_methods": "The paper explicitly compares K-ADAPTER (combined adapters) to (a) the base RoBERTa_LARGE, and (b) RoBERTa pretrained with a joint multi-task objective (RoBERTa+multitask). Key experimental contrasts show that combining distinct adapters (factual + linguistic) typically yields higher scores across entity typing, relation classification, QA and LAMA probing than both RoBERTa and RoBERTa+multitask. Ablations (single adapters) also evaluated to show complementary benefits.",
            "key_findings": "Combining diverse knowledge-specific adapters (factual + linguistic) provides consistent improvements over both the unfrozen baseline and a single multi-task joint-training approach; independent adapter pretraining prevents interference and supports continual infusion of heterogeneous knowledge sources, yielding richer factual and commonsense knowledge and better downstream reasoning.",
            "counter_examples_or_negative_results": "Single adapters (F or L) sometimes fall slightly behind RoBERTa+multitask on a few metrics (the paper notes ablated K-ADAPTER models are 'clearly better than RoBERTa, but slightly lose compared with RoBERTa+multitask' in some QA ablations). K-ADAPTER without knowledge (adapter structure but no knowledge pretraining) performed slightly worse than RoBERTa on OpenEntity, indicating gains come from injected knowledge rather than extra parameters.",
            "uuid": "e3325.0",
            "source_info": {
                "paper_title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "K-ADAPTER (F)",
            "name_full": "K-ADAPTER with Factual Adapter only",
            "brief_description": "K-ADAPTER variant attaching only the factual adapter pretrained on relation classification (T-REx-rc) with RoBERTa_LARGE frozen; used to infuse factual knowledge into downstream models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "K-ADAPTER (F) — RoBERTa_LARGE + factual adapter",
            "model_description": "RoBERTa_LARGE frozen; a single factual adapter (~42M params) pretrained on relation classification over automatically aligned Wikipedia-Wikidata triples (T-REx-rc). Adapter output concatenated with RoBERTa features for downstream tasks.",
            "model_size": "RoBERTa_LARGE 355M + factual adapter ≈ 397M (approx.)",
            "reasoning_methods": [
                "Factual-knowledge infusion via relation-classification pretraining"
            ],
            "reasoning_methods_description": "Adapter trained to predict the relation label between entity pairs given context; last hidden features of RoBERTa and adapter are concatenated; training used cross-entropy loss on 5.5M sentences from T-REx-rc.",
            "diversity_of_methods": "Single-style — focuses on factual relation-based knowledge rather than combining heterogeneous knowledge sources.",
            "reasoning_task_name": "Same downstream tasks as main experiments (entity typing, relation classification, QA, LAMA probing) evaluated as an ablation.",
            "reasoning_task_description": "As above; factual adapter intended to improve tasks relying on factual relations and entity knowledge.",
            "performance_by_method": "OpenEntity micro F1: 77.53 (vs RoBERTa 76.23). FIGER (Acc/Ma-F1/Mi-F1): 59.50 / 84.52 / 80.42. SearchQA (EM / F1): 61.85 / 67.17. Quasar-T (EM / F1): 46.20 / 52.86. CosmosQA Acc: 80.93. TACRED micro F1: 71.89. LAMA (selected): not reported separately in table for F-only, but factual adapter improves P@1 over RoBERTa when combined.",
            "comparison_of_methods": "Ablation shows factual-only adapter improves over base RoBERTa across many tasks; compared to RoBERTa+multitask, factual-only sometimes performs slightly worse on some QA metrics but still yields strong gains; combining with linguistic adapter gives additive improvement.",
            "key_findings": "Factual-only adapter injects useful relation knowledge that improves entity- and relation-focused downstream tasks; factual knowledge complements linguistic adapter when combined.",
            "counter_examples_or_negative_results": "Factual-only sometimes slightly underperforms RoBERTa+multitask on certain QA ablations (paper notes single adapters 'slightly lose compared with RoBERTa+multitask' in some QA metrics).",
            "uuid": "e3325.1",
            "source_info": {
                "paper_title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "K-ADAPTER (L)",
            "name_full": "K-ADAPTER with Linguistic Adapter only",
            "brief_description": "K-ADAPTER variant attaching only the linguistic adapter pretrained on dependency-relation prediction (predicting head indices) with RoBERTa_LARGE frozen; used to infuse syntactic knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "K-ADAPTER (L) — RoBERTa_LARGE + linguistic adapter",
            "model_description": "RoBERTa_LARGE frozen; a single linguistic adapter (~42M params) pretrained on dependency head prediction using parsed BookCorpus (1M examples). Adapter outputs concatenated with RoBERTa features for downstream tasks.",
            "model_size": "RoBERTa_LARGE 355M + linguistic adapter ≈ 397M (approx.)",
            "reasoning_methods": [
                "Linguistic/syntactic infusion via dependency-head prediction pretraining"
            ],
            "reasoning_methods_description": "Adapter trained with binary cross-entropy (BCEWithLogits) on head-index prediction per token; concatenated adapter+RoBERTa features used for downstream classification or span prediction.",
            "diversity_of_methods": "Single-style — focuses on syntactic/linguistic reasoning rather than multiple heterogeneous knowledge sources.",
            "reasoning_task_name": "Evaluated across the same downstream benchmarks (entity typing, relation classification, QA).",
            "reasoning_task_description": "Linguistic adapter targets syntactic signal injection to help tasks sensitive to syntactic relations (e.g., relation classification).",
            "performance_by_method": "OpenEntity micro F1: 76.89. FIGER (Acc/Ma-F1/Mi-F1): 61.10 / 83.61 / 79.18. SearchQA (EM / F1): 61.15 / 66.82. Quasar-T (EM / F1): 45.66 / 52.39. CosmosQA Acc: 80.76. TACRED micro F1: 71.96.",
            "comparison_of_methods": "Linguistic-only adapter improves over base RoBERTa on multiple tasks and often complements factual adapter; combining adapters typically yields the best results.",
            "key_findings": "Linguistic (syntactic) knowledge injected via an adapter provides complementary improvements to factual knowledge, and alone improves many reasoning tasks compared to base RoBERTa.",
            "counter_examples_or_negative_results": "As with factual-only, linguistic-only can underperform the multi-task RoBERTa+multitask baseline on some QA ablations; gains are most pronounced when combined with factual adapter.",
            "uuid": "e3325.2",
            "source_info": {
                "paper_title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "RoBERTa+multitask",
            "name_full": "RoBERTa_LARGE pretrained with multi-task knowledge objectives",
            "brief_description": "A baseline where RoBERTa_LARGE parameters are updated (not frozen) and trained jointly on multiple knowledge-infusion pretraining tasks (relation classification and dependency relation prediction) via multi-task learning.",
            "citation_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa+multitask",
            "model_description": "RoBERTa_LARGE (355M) fine-tuned / further pretrained with multiple knowledge-driven objectives simultaneously (multi-task learning across factual and linguistic pretraining tasks), updating the entire model parameters.",
            "model_size": "RoBERTa_LARGE 355M (no adapters)",
            "reasoning_methods": [
                "Multi-task joint training with entangled parameter updates (factual relation prediction + dependency prediction)"
            ],
            "reasoning_methods_description": "RoBERTa's parameters are jointly updated with multiple objectives so the model learns to satisfy both factual and linguistic pretraining tasks simultaneously; this is a single entangled model rather than separate modular adapters.",
            "diversity_of_methods": "Similar-style (single-model, multi-task): multiple objectives are used but training updates the same shared parameters, leading to entangled representations rather than explicitly modular diverse methods.",
            "reasoning_task_name": "Same downstream tasks in paper: entity typing, relation classification, question answering, LAMA probing",
            "reasoning_task_description": "Multi-task pretraining aims to incorporate heterogeneous knowledge into the same parameter set via joint optimization.",
            "performance_by_method": "OpenEntity micro F1: 76.97 (vs RoBERTa 76.23). FIGER (Acc/Ma-F1/Mi-F1): 59.86 / 84.45 / 78.84 (vs RoBERTa 56.31 / 82.43 / 77.83). SearchQA (EM / F1): 59.92 / 66.67 (vs RoBERTa 59.01 / 65.62). Quasar-T (EM / F1): 44.62 / 51.17 (vs RoBERTa 40.83 / 48.84). CosmosQA Acc: 81.19 (vs RoBERTa 80.59). TACRED micro F1: 71.62 (vs RoBERTa 71.25).",
            "comparison_of_methods": "RoBERTa+multitask (single shared parameters) generally improves over vanilla RoBERTa; however, the K-ADAPTER combined approach (diverse modular adapters) often outperforms RoBERTa+multitask, showing modular, independently trained adapters give better utilization of heterogeneous knowledge in many downstream benchmarks.",
            "key_findings": "Joint multi-task training of the entire RoBERTa parameters can improve over base RoBERTa, but modular adapter-based infusion of distinct knowledge types (K-ADAPTER) typically yields equal or better downstream reasoning performance and avoids catastrophic forgetting when adding new knowledge.",
            "counter_examples_or_negative_results": "The paper notes that multi-task joint training entangles representations and, when injecting new knowledge later, previously injected knowledge may fade (catastrophic forgetting). Empirically, RoBERTa+multitask is sometimes slightly stronger than single adapters on certain ablations, indicating joint training can be competitive but lacks modularity and continual-infusion advantages.",
            "uuid": "e3325.3",
            "source_info": {
                "paper_title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "RoBERTa_LARGE",
            "name_full": "RoBERTa: A Robustly Optimized BERT Pretraining Approach (Large)",
            "brief_description": "A large masked-language-model transformer (BERT variant) used as the frozen backbone in K-ADAPTER experiments; baseline for comparison.",
            "citation_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa_LARGE",
            "model_description": "Transformer-based pretrained language model (24 layers, hidden size 1024, 16 attention heads, 355M parameters) trained with robust LM pretraining recipes; used here as backbone whose parameters are frozen during adapter pretraining and optionally finetuned during downstream tasks.",
            "model_size": "355M parameters",
            "reasoning_methods": [
                "General masked-language-model pretraining (surface-form and distributional reasoning)",
                "Baseline feature provider for adapters"
            ],
            "reasoning_methods_description": "RoBERTa learns contextual representations via masked-language modeling; in K-ADAPTER experiments it provides base features (frozen during adapter pretraining) which are concatenated with adapter features at inference.",
            "diversity_of_methods": "Single general pretraining style (masked language modeling); not explicitly using multiple modular reasoning strategies unless combined with adapters or multi-task finetuning.",
            "reasoning_task_name": "Used as base model for downstream tasks: entity typing, relation classification, QA, LAMA probing",
            "reasoning_task_description": "Provides contextual embeddings used directly (baseline) or augmented by adapters for downstream reasoning tasks.",
            "performance_by_method": "OpenEntity micro F1: 76.23. FIGER (Acc/Ma-F1/Mi-F1): 56.31 / 82.43 / 77.83. SearchQA (EM / F1): 59.01 / 65.62. Quasar-T (EM / F1): 40.83 / 48.84. CosmosQA Acc: 80.59. TACRED micro F1: 71.25. LAMA P@1 (selected): Google-RE 4.8; T-REx 27.1; LAMA-UHN-T-REx 20.1.",
            "comparison_of_methods": "Baseline comparisons show that augmenting RoBERTa with either multi-task retraining or modular adapters improves downstream reasoning, with modular diverse adapters (K-ADAPTER F+L) typically providing larger and more consistent gains.",
            "key_findings": "RoBERTa provides strong general representations, but explicitly injecting knowledge via adapters or multi-task finetuning yields improved reasoning capabilities; modular adapter approach better preserves previously injected knowledge and supports continual infusion.",
            "counter_examples_or_negative_results": "RoBERTa sometimes underperforms adapter-augmented variants on factual probing (LAMA) and several downstream tasks, indicating that vanilla masked-language pretraining does not capture some factual or syntactic knowledge as strongly as targeted adapter pretraining.",
            "uuid": "e3325.4",
            "source_info": {
                "paper_title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ERNIE: Enhanced Language Representation with Informative Entities",
            "rating": 2
        },
        {
            "paper_title": "Knowledge Enhanced Contextual Word Representations",
            "rating": 2
        },
        {
            "paper_title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
            "rating": 2
        },
        {
            "paper_title": "Parameter-Efficient Transfer Learning for NLP",
            "rating": 2
        },
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 1
        }
    ],
    "cost": 0.017184249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters</h1>
<p>Ruize Wang ${ }^{1}$ Duyu Tang ${ }^{2}$, Nan Duan ${ }^{2}$, Zhongyu Wei ${ }^{1}$, Xuanjing Huang ${ }^{1}$, Jianshu $\mathrm{Ji}^{3}$, Guihong Cao ${ }^{3}$, Daxin Jiang ${ }^{2}$, Ming Zhou ${ }^{2}$<br>${ }^{1}$ Fudan University, Shanghai, China<br>${ }^{2}$ Microsoft, Beijing, China<br>${ }^{3}$ Microsoft, Redmond WA, USA<br>{rzwang18,zywei, xjhuang}@fudan.edu.cn<br>{dutang, nanduan, jianshuj, gucao, djiang, mingzhou}@microsoft.com</p>
<h4>Abstract</h4>
<p>We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, the historically injected knowledge would be flushed away. To address this, we propose KADAPTER, a framework that retains the original parameters of the pre-trained model fixed and supports the development of versatile knowledge-infused model. Taking RoBERTa as the backbone model, K-ADAPTER has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus multiple adapters can be efficiently trained in a distributed way. As a case study, we inject two kinds of knowledge in this work, including (1) factual knowledge obtained from automatically aligned texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-ADAPTER captures versatile knowledge than RoBERTa. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Schütze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. "not"). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa.</p>
<p>Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fade away. Meanwhile, the resulting models produce entangled representations, so that it is hard to investigate the effect of each kind of knowledge.</p>
<p>In this paper, we propose K-ADAPTER, a flexible and simple framework that supports the infusion of multiple kinds of knowledge into large pretrained models. K-ADAPTER leaves the original representation of a pre-trained model unchanged and exports different representations for different types of infused knowledge. This is achieved by</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Knowledge Source</th>
<th style="text-align: left;">Objective</th>
<th style="text-align: left;">BERT fixed <br> in training?</th>
<th style="text-align: left;">Continual knowl- <br> edge infusion?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ERNIE (Zhang et al., <br> 2019)</td>
<td style="text-align: left;">Wikipedia, WikiData</td>
<td style="text-align: left;">entity linking</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">LIBERT (Lauscher et al., <br> 2019)</td>
<td style="text-align: left;">WordNet</td>
<td style="text-align: left;">synonym word prediction, <br> hyponym-hypernym prediction</td>
<td style="text-align: left;">from scratch</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">SenseBERT (Levine <br> et al., 2020)</td>
<td style="text-align: left;">WordNet</td>
<td style="text-align: left;">word-supersense prediction</td>
<td style="text-align: left;">from scratch</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">KnowBERT (Peters <br> et al., 2019)</td>
<td style="text-align: left;">Wordnet, Wikipedia, <br> CrossWikis</td>
<td style="text-align: left;">entity linking, hypernym link- <br> ing</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">WKLM (Xiong et al., <br> 2020)</td>
<td style="text-align: left;">WikiPedia, WikiData</td>
<td style="text-align: left;">replaced entity detection</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">BERT-MK (He et al., <br> 2020)</td>
<td style="text-align: left;">Unified Medical Lan- <br> guage System</td>
<td style="text-align: left;">discriminate between real and <br> fake facts</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">K-Adapter (this work)</td>
<td style="text-align: left;">Wikipedia, Wikidata, <br> dependency parser</td>
<td style="text-align: left;">predication prediction, depen- <br> dency relation prediction</td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">Y</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison between our approach (K-ADAPTER) and previous works on injecting knowledge into BERT.
the integration of compact neural models dubbed adapters. Adapters are knowledge-specific models plugged outside of a pre-trained model, whose inputs are the output hidden-states of intermediate layers of the pre-trained model. We take RoBERTa (Liu et al., 2019) as the base pre-trained model and integrate two types of knowledge, including factual knowledge obtained by aligned Wikipedia text to Wikidata triplets and linguistic knowledge obtained by applying off-the-shell dependency parser to web texts. In the pre-training phase, we train two adapters independently. Since adapters have much less trainable parameters compared with RoBERTa, the training process is memory efficient.</p>
<p>We conduct extensive experiments on six benchmark datasets across three knowledge-driven tasks, i.e., relation classification, entity typing, and question answering. Experiments show that K-ADAPTER consistently performs better than RoBERTa, and achieves state-of-the-art performance on five datasets. Case study and probing experiments indicate that K-ADAPTER captures versatile knowledge than RoBERTa.</p>
<h2>2 Related Work</h2>
<p>Our work relates to the area of injecting knowledge into pre-trained models. As stated in Table 1, previous works mainly differ from the knowledge sources and the objective used for training.</p>
<p>ERNIE (Zhang et al., 2019) injects a knowledge graph into BERT. They align entities from Wikipedia sentences to fact triples in WikiData, and discard sentences with less than three entities. In the training process, the input includes sentences and linked facts, and the knowledge-aware learning
objective is to predict the correct token-entity alignment. Entity embeddings are trained on fact triples from WikiData via TransE (Bordes et al., 2013). LIBERT (Lauscher et al., 2019) injects pairs of words with synonym and hyponym-hypernym relations in WordNet. The model takes a pair of words separated by a special token as the input, and is optimized by a binary classification problem, which predicts whether the input holds a particular relation or not. SenseBERT (Levine et al., 2020) considers word-supersense knowledge. It inject knowledge by predicting the supersense of the masked word in the input, where the candidates are nouns and verbs and the ground truth comes from WordNet. KnowBERT (Peters et al., 2019) incorporates knowledge bases into BERT using Knowledge attention and recontextualization, where the knowledge comes from synset-synset and lemmalemma relationships in WordNet, and entity linking information in Wikipedia. If entity linking supervision is available, the model is learned with an additional knowledge-aware log-likelihood or maxmargin objective. WKLM (Xiong et al., 2020) replaces entity mentions in the original document with names of other entities of the same type. The model is trained to distinguish the correct entity mention from randomly chosen ones. BERT-MK (He et al., 2020) integrates fact triples from knowledge graph. For each entity, it sample incoming and outcoming instances from the neighbors on the knowledge graph, and replaces head or tail entity to create negative instances. The model is learned to discriminate between real and fake facts.</p>
<p>As shown in Table 1, our model (K-ADAPTER) differs from previous studies in three aspects. First,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Pre-trained language models inject multiple kinds of knowledge with multi-task learning. Model parameters need to be retrained when injecting new kinds of knowledge, which may result in the catastrophic forgetting (b) Our K-ADAPTER injects multiple kinds of knowledge by training adapters independently on different pre-train tasks, which supports continual knowledge infusion. When we inject new kinds of knowledge, the existing knowledge-specific adapters will not be affected. KIA represents the adapter layer and TRM represents the transformer layer, both of which are shown in Figure 2.
we consider both fact-related objective (i.e. predicate/relation prediction) and linguistic-related objective (i.e. dependency relation prediction). Second, the original parameter of BERT is clamped in the knowledge infusion process. Third, our approach supports continual learning, which means that the learning of different adapters are not entangled. This flexibility enables us to efficiently inject different types of knowledge independently, and inject more types of knowledge without any loss on the previously injected knowledge.</p>
<h2>3 K-ADAPTER</h2>
<p>As illustrated in Figure 1 (a), most of the previous works enhance pre-trained language models by injecting knowledge and update model parameters through multi-task learning. Regardless of these different versions of knowledge-injected methods with multi-task learning, common issues not fully studied are catastrophic forgetting of previous knowledge. To address this, we present KADAPTER as shown in Figure 1(b), where multiple kinds of knowledge are injected into different compact neural models (i.e., adapters in this paper) individually instead of directly injecting knowledge into pre-trained models. It keeps the original representation of a pre-trained model fixed and supports continual knowledge infusion, i.e., injecting each kind of knowledge into the corresponding knowledge-specific adapter and producing disen-
tangled representation. Specifically, adapters are knowledge-specific models (with few parameters) plugged outside of a pre-trained model. The inputs of adapters are the output hidden-states of intermediate layers of the pre-trained model. Each adapter is pre-trained independently on different tasks for injecting discriminative knowledge while the original parameters of the pre-trained model are frozen. In this paper, we exploit RoBERTa (Liu et al., 2019) as the pre-trained model, and mainly infuse factual knowledge and linguistic knowledge with two kinds of adapters, i.e., factual adapter and linguistic adapter which are pre-trained on the relation classification task and dependency relation prediction task respectively. In this section, we first describe the structure of our adapter, and then present the process of pre-training knowledgespecific adapters.</p>
<h3>3.1 Adapter Structure</h3>
<p>In this work, we present a different adapter structure as shown in Figure 2, which is referred to as the knowledge-specific adapter. In contrast to Houlsby et al. (2019) add adapter layers into each transformer layer, our adapter works as outside plug-ins. Each adapter model consists of $K$ adapter layers that contain $N$ transformer (Vaswani et al., 2017) layers and two projection layers. A skipconnection is applied across two projection layers. Specifically, for each adapter model, we plug</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Structure of the adapter layer (left). The adapter layer consists of two projection layers and $N=2$ transformer layers, and a skip-connection between two projection layers.
adapter layers among different transformer layers of the pre-trained model. We concatenate the output hidden feature of the transformer layer in the pre-trained model and the output feature of the former adapter layer, as the input feature of the current adapter layer. For each knowledge-specific adapter, we concatenate the last hidden features of the pre-trained model and adapter as the final output feature of this adapter model.</p>
<p>In the pre-training procedure, we train each knowledge-specific adapter on different pretraining tasks individually. For various downstream tasks, K-ADAPTER can adopt the fine-tuning procedure similar to RoBERTa and BERT. When only one knowledge-specific adapter is adopted, we can take the final output feature of this adapter model as the input for task-specific layers of the downstream task. When multiple knowledge-specific adapters are adopted, we concatenate the output features of different adapter models as the input for task-specific layers of the downstream task.</p>
<h3>3.2 Pre-training settings</h3>
<p>We use RoBERTa $<em A="A">{\text {LARGE }}$ ( $\mathrm{L}=24, \mathrm{H}=1024, \mathrm{~A}=16$, 355 M params) implementation by Huggingface ${ }^{2}$ as the pre-trained model in all our experiments. As for each adapter layer, we denote the number of transformer layer as $N$, the hidden dimension of transformer layer as $H</em>=768$. The RoBERTa lay-}$, the number of self-attention heads as $A_{A}$, the hidden dimension of down-projection and up-projection layers as $H_{d}$ and $H_{u}$. In detail, we have the following adapter size: $N=2, H_{A}=768, A_{A}=12$, $H_{u}=1024$ and $H_{d</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ers where adapter layers plug in are ${0,11,23}$, and different adapter layers do not share parameters. Thus the total parameters for each adapter model are about 42 M , which are much smaller than RoBERTa $_{\text {LARGE }}$ and make the training process memory efficient. It should be noticed that RoBERTa is fixed during training and the parameters of adapters are trainable and initialized randomly. Then we describe how to inject different knowledge into knowledge-specific adapters as below.</p>
<h3>3.3 Factual Adapter</h3>
<p>Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5 M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input representations of the given entities. Then, we concatenate two entity representations to perform relation classification.</p>
<h3>3.4 Linguistic Adapter</h3>
<p>Linguistic knowledge is implicitly contained in natural language texts, e.g., syntactic and semantic information. In this work, we acquire linguistic knowledge from dependency relationships among words in natural language text. We build a dataset consisting of 1 M examples. In particular, we run the off-the-shell dependency parser from Stanford Parser ${ }^{3}$ on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">OpenEntity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FIGER</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Mi-F $_{1}$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Ma-F ${ }_{1}$</td>
<td style="text-align: center;">Mi-F $_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">NFGEC (Shimaoka et al., 2016)</td>
<td style="text-align: center;">68.80</td>
<td style="text-align: center;">53.30</td>
<td style="text-align: center;">60.10</td>
<td style="text-align: center;">55.60</td>
<td style="text-align: center;">75.15</td>
<td style="text-align: center;">71.73</td>
</tr>
<tr>
<td style="text-align: center;">BERT-base (Zhang et al., 2019)</td>
<td style="text-align: center;">76.37</td>
<td style="text-align: center;">70.96</td>
<td style="text-align: center;">73.56</td>
<td style="text-align: center;">52.04</td>
<td style="text-align: center;">75.16</td>
<td style="text-align: center;">71.63</td>
</tr>
<tr>
<td style="text-align: center;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">78.42</td>
<td style="text-align: center;">72.90</td>
<td style="text-align: center;">75.56</td>
<td style="text-align: center;">57.19</td>
<td style="text-align: center;">75.61</td>
<td style="text-align: center;">73.39</td>
</tr>
<tr>
<td style="text-align: center;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">78.60</td>
<td style="text-align: center;">73.70</td>
<td style="text-align: center;">76.10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">KEPLER (Wang et al., 2021)</td>
<td style="text-align: center;">77.20</td>
<td style="text-align: center;">74.20</td>
<td style="text-align: center;">75.70</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">WKLM (Xiong et al., 2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">60.21</td>
<td style="text-align: center;">81.99</td>
<td style="text-align: center;">77.00</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">77.55</td>
<td style="text-align: center;">74.95</td>
<td style="text-align: center;">76.23</td>
<td style="text-align: center;">56.31</td>
<td style="text-align: center;">82.43</td>
<td style="text-align: center;">77.83</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa + multitask</td>
<td style="text-align: center;">77.96</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">76.97</td>
<td style="text-align: center;">59.86</td>
<td style="text-align: center;">84.45</td>
<td style="text-align: center;">78.84</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (w/o knowledge)</td>
<td style="text-align: center;">74.47</td>
<td style="text-align: center;">74.91</td>
<td style="text-align: center;">76.17</td>
<td style="text-align: center;">56.93</td>
<td style="text-align: center;">82.56</td>
<td style="text-align: center;">77.90</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (F)</td>
<td style="text-align: center;">79.30</td>
<td style="text-align: center;">75.84</td>
<td style="text-align: center;">77.53</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">84.52</td>
<td style="text-align: center;">80.42</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (L)</td>
<td style="text-align: center;">80.01</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">76.89</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">83.61</td>
<td style="text-align: center;">79.18</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (F+L)</td>
<td style="text-align: center;">78.99</td>
<td style="text-align: center;">76.27</td>
<td style="text-align: center;">77.61</td>
<td style="text-align: center;">61.81</td>
<td style="text-align: center;">84.87</td>
<td style="text-align: center;">80.54</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on two entity typing datasets OpenEntity and FIGER.
to input representations of each token to perform classification. More training details of facAdapter and linAdapter can be found in the Appendix.</p>
<h2>4 Experiments</h2>
<p>We evaluate our K-ADAPTER on three knowledgedriven downstream tasks, i.e., entity typing, question answering and relation classification. Furthermore, we conduct detailed analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-ADAPTER (F+L), K-ADAPTER (F), and K-ADAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix.</p>
<h3>4.1 Entity Typing</h3>
<p>We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token "@" before and after a certain entity, then the first "@" special token representation is adopted to perform classification. As for OpenEntity, we adopt micro $F_{1}$ score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro $F_{1}$ scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works.</p>
<p>Baselines NFGEC (Shimaoka et al., 2016) employs attentive recursive neural networks to compose context representations. KEPLER (Wang et al., 2021) integrates factual knowledge with the supervision of the knowledge embedding objective. RoBERTa+multitask is our RoBERTa model pretrained with multi-task learning (as shown in Figure 1(a)) for injecting multiple kinds of knowledge on two pre-training tasks. K-ADAPTER (w/o knowledge) consists of a RoBERTa model and an adapter without being injected knowledge. Other baseline models are described in Section 2.</p>
<p>Results and Discussion The results on OpenEntity and FIGER are shown in Table 2. K-ADAPTER ( $\mathrm{F}+\mathrm{L})$ achieves consistent improvements across these datasets. As for OpenEntity, our RoBERTa achieve better results than other baseline models. K-ADAPTER ( $\mathrm{F}+\mathrm{L})$ further achieves improvement of $1.38 \% F_{1}$ over RoBERTa, which means factual knowledge and linguistic knowledge help to predict the types more accurately. As for FIGER, it covers more entity types, and is more fine-grained than OpenEntity. Compared with WKLM, KADAPTER ( $\mathrm{F}+\mathrm{L})$ improves the macro $F_{1}$ by $2.88 \%$, micro $F_{1}$ by $2.54 \%$ and accuracy by $1.60 \%$. This demonstrates that K-ADAPTER (F+L) benefits finegrained entity typing.</p>
<p>In addition, we further conduct several experiments on our ablated model K-ADAPTER (w/o knowledge), to explore whether the performance gains came from introducing knowledge or additional parameters. Results show that K-ADAPTER (F) significantly outperforms K-ADAPTER (w/o knowledge). Moreover, it is worth noting that on OpenEntity dataset, K-ADAPTER (w/o knowledge) even performs slightly worse than RoBERTa.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">SearchQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quasar-T</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CosmosQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">$\mathbf{F}_{1}$</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">$\mathbf{F}_{1}$</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">BiDAF (Seo et al., 2017)</td>
<td style="text-align: center;">28.60</td>
<td style="text-align: center;">34.60</td>
<td style="text-align: center;">25.90</td>
<td style="text-align: center;">28.50</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AQA (Buck et al., 2018)</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">47.40</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">R'3 (Wang et al., 2018a)</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">55.30</td>
<td style="text-align: center;">35.30</td>
<td style="text-align: center;">41.70</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">DSQA (Lin et al., 2018)</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">55.30</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">49.30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Evidence Agg. (Wang et al., 2018b)</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">63.20</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">49.60</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERT (Xiong et al., 2020)</td>
<td style="text-align: center;">57.10</td>
<td style="text-align: center;">61.90</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">WKLM (Xiong et al., 2020)</td>
<td style="text-align: center;">58.70</td>
<td style="text-align: center;">63.30</td>
<td style="text-align: center;">43.70</td>
<td style="text-align: center;">49.90</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">WKLM + Ranking (Xiong et al., 2020)</td>
<td style="text-align: center;">61.70</td>
<td style="text-align: center;">66.70</td>
<td style="text-align: center;">45.80</td>
<td style="text-align: center;">52.20</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERT-FT $_{R A C E+S W A G}$ (Huang et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.70</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">59.01</td>
<td style="text-align: center;">65.62</td>
<td style="text-align: center;">40.83</td>
<td style="text-align: center;">48.84</td>
<td style="text-align: center;">80.59</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa + multitask</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">44.62</td>
<td style="text-align: center;">51.17</td>
<td style="text-align: center;">81.19</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (F)</td>
<td style="text-align: center;">61.85</td>
<td style="text-align: center;">67.17</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">52.86</td>
<td style="text-align: center;">80.93</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (L)</td>
<td style="text-align: center;">61.15</td>
<td style="text-align: center;">66.82</td>
<td style="text-align: center;">45.66</td>
<td style="text-align: center;">52.39</td>
<td style="text-align: center;">80.76</td>
</tr>
<tr>
<td style="text-align: center;">K-ADAPTER (F+L)</td>
<td style="text-align: center;">61.96</td>
<td style="text-align: center;">67.31</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">53.00</td>
<td style="text-align: center;">81.83</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on question answering datasets including: CosmosQA, SearchQA and Quasar-T.</p>
<p>These results demonstrate that our model gains improvement from knowledge instead of more parameters. Thus, for simplicity, we don't discuss K-ADAPTER (w/o knowledge) in the following experiments.</p>
<h3>4.2 Question Answering</h3>
<p>We conduct experiments on two question answering (QA) tasks, i.e., commonsense QA and open-domain QA. Commonsense QA aims to answer questions with commonsense. We adopt CosmosQA (Huang et al., 2019) dataset to evaluate our models. CosmosQA requires commonsense-based reading comprehension, formulated as multiple-choice questions. To finetune our models for CosmosQA, the input token sequence is modified as " $&lt;$ SEP $&gt;$ context $&lt;/$ SEP $&gt;$ question $&lt;/$ SEP $&gt;$ answer $&lt;/$ SEP $&gt;$ ", then the representation of the first token is adopted to perform classification, and will get a score for this answer. After getting four scores, the answer with the highest score will be selected. We report accuracy scores obtained from the leaderboard.</p>
<p>Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets.</p>
<p>To fine-tune our models for this task, the input token sequence is modified as " $&lt;$ SEP $&gt;$ question $&lt;/$ SEP $&gt;$ paragraph $&lt;/$ SEP $&gt;$ ". We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose $F_{1}$ (Ling and Weld, 2012) scores to evaluate our models.</p>
<p>Baselines BERT-FT $_{R A C E+S W A G} \quad$ (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) proposes to re-write questions and aggregate the answers generated by the re-written questions. R'3 (Wang et al., 2018a) is a reinforced model making use of a ranker for selecting most confident paragraph. Evidence Agg. (Wang et al., 2018b) proposes making use of the aggregated evidence from across multiple paragraphs. WKLM (Xiong et al., 2020) is adopted as the reader model to read multiple paragraphs to predict a single answer. WKLM + Ranking (Xiong et al., 2020) is a WKLM paragraph reader plus with a BERT based paragraph ranker to assign each paragraph a relevance score.</p>
<p>Results and Discussion The results on CosmosQA are shown in Table 3. Compared with BERT-FT $_{R A C E+S W A G}$, our RoBERTa significantly achieves $11.89 \%$ improvement of accuracy. Compared to RoBERTa, K-ADAPTER (F+L) further improves the accuracy by $1.24 \%$, which indicates that K-ADAPTER can obtain better commonsense inference ability. Moreover, the perfor-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F}_{1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">C-GCN (Zhang et al., 2018)</td>
<td style="text-align: center;">69.90</td>
<td style="text-align: center;">63.30</td>
<td style="text-align: center;">66.40</td>
</tr>
<tr>
<td style="text-align: left;">BERT-base (Zhang et al., 2019)</td>
<td style="text-align: center;">67.23</td>
<td style="text-align: center;">64.81</td>
<td style="text-align: center;">66.00</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">69.97</td>
<td style="text-align: center;">66.08</td>
<td style="text-align: center;">67.97</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large (Baldini Soares et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">70.10</td>
</tr>
<tr>
<td style="text-align: left;">BERT+MTB (Baldini Soares et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.50</td>
</tr>
<tr>
<td style="text-align: left;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">71.60</td>
<td style="text-align: center;">71.40</td>
<td style="text-align: center;">71.50</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER (Wang et al., 2021)</td>
<td style="text-align: center;">70.43</td>
<td style="text-align: center;">73.02</td>
<td style="text-align: center;">71.70</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">70.17</td>
<td style="text-align: center;">72.36</td>
<td style="text-align: center;">71.25</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa + multitask</td>
<td style="text-align: center;">70.18</td>
<td style="text-align: center;">73.11</td>
<td style="text-align: center;">71.62</td>
</tr>
<tr>
<td style="text-align: left;">K-ADAPTER (F)</td>
<td style="text-align: center;">69.39</td>
<td style="text-align: center;">74.59</td>
<td style="text-align: center;">71.89</td>
</tr>
<tr>
<td style="text-align: left;">K-ADAPTER (L)</td>
<td style="text-align: center;">68.85</td>
<td style="text-align: center;">75.37</td>
<td style="text-align: center;">71.96</td>
</tr>
<tr>
<td style="text-align: left;">K-ADAPTER (F+L)</td>
<td style="text-align: center;">70.14</td>
<td style="text-align: center;">74.04</td>
<td style="text-align: center;">$\mathbf{7 2 . 0 4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on the relation classification dataset TACRED.
mance of ablated K-ADAPTER models, i.e., KADAPTER (F) and K-ADAPTER (L) are clearly better than RoBERTa, but slightly lose compared with RoBERTa+multitask. It is notable that KADAPTER (F+L) makes obvious improvement comparing with RoBERTa+multitask. This demonstrates that the combination of multiple knowledgespecific adapters could achieve better performance.</p>
<p>The results for open-domain QA are shown in Table 3. K-ADAPTER models achieve better results compared to other baselines. This indicates that K-ADAPTER models can make full use of the infused knowledge and accordingly benefit understanding the retrieved paragraphs to answer the question. Specifically, on SearchQA, K-ADAPTER (F+L) makes significant improvement of $4.01 \% F_{1}$ scores, comparing with WKLM where the ranking scores are not used, and even has a slight improvement as compared to WKLM+Ranking. It is worth noting that K-ADAPTER models do not consider the confidence of each retrieved paragraph, while WKLM+Ranking utilizes ranking scores from a BERT based ranker. On the QuasarT dataset, K-ADAPTER (F+L) also outperforms WKLM by $3.1 \% F_{1}$ score and slightly outperforms WKLM+Ranking.</p>
<h3>4.3 Relation Classification</h3>
<p>Relation classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token "@" before and after the first entity, adding "#" before and after the second entity. Then the token representations
of the first special token "@" and "#" are concatenated to perform relation classification. We adopt micro $F_{1}$ score as the metric to represent the model performance as previous works.</p>
<p>Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1.</p>
<p>Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-ADAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-ADAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge into pre-trained model with adapters. (2) K-ADAPTER models gain more improvement compared with RoBERTa+multitask. This directly demonstrates injecting knowledge individually in K-ADAPTER way would help models make full use of knowledge.</p>
<h3>4.4 Case Study</h3>
<p>Table 5 gives a qualitative comparison example between K-ADAPTER and RoBERTa on relation classification dataset TACRED. The results show that, in most cases, the wrongly predicted logit value of RoBERTa and the logit value of the true label are actually quite close. For example, given "New Fabris closed down June 16", RoBERTa predicts "no_relation", but the true label "city_of_birth" ranks in second place. If a model could correctly predict the relationship between "New Fabris" and "June 16", then it needs to know that "New Fabris" is a company. Thanks to the factual knowledge in K-ADAPTER, it can help the model from predicting "no_relation" to predicting the correct category label.</p>
<p>In addition, we utilize a LAMA (LAnguage Model Analysis) probe (Petroni et al., 2019) to examine models' ability to memorize factual knowledge. Specifically, the LAMA probing task is under a zero-shot setting, which requires the language model to answer cloze-style questions about relational facts without fine-tuning, For example, given "Simon Bowman was born in [MASK]" as the input, models are asked to predict the correct token</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">True label</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Predicted label</th>
<th style="text-align: center;">Predicted logits</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">His former student Mark Devlin of the University of Pennsylvania was co-leader of the other, known as the Microwave Anisotropy Telescope .</td>
<td style="text-align: center;">schools attended</td>
<td style="text-align: center;">K-Adapter</td>
<td style="text-align: center;">['schools attended', 'no relation','founded']</td>
<td style="text-align: center;">$[12.6,9.5,5.2]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">['no relation', 'founded', "member of"']</td>
<td style="text-align: center;">$[9.1,6.5,5.0]$</td>
</tr>
<tr>
<td style="text-align: center;">Graham had been in custody in Vancouver, British Columbia, since June .</td>
<td style="text-align: center;">cities of residence</td>
<td style="text-align: center;">K-Adapter</td>
<td style="text-align: center;">['cities of residence', 'countries of residence', 'no relation']</td>
<td style="text-align: center;">$[13.5,6.8,6.6]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">['countries of residence', 'country of death', 'alternate names']</td>
<td style="text-align: center;">$[7.1,7.0,6.8]$</td>
</tr>
<tr>
<td style="text-align: center;">Vladimir Ladyzhenskiy of Russia died after she suffered a shock in the final of the spa world championship in Heinola, a southern city of Finland, on Saturday .</td>
<td style="text-align: center;">cause of death</td>
<td style="text-align: center;">K-Adapter</td>
<td style="text-align: center;">['cause of death','origin','no relation']</td>
<td style="text-align: center;">$[11.0,7.6,7.1]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">['no relation', 'cause of death', 'origin']</td>
<td style="text-align: center;">$[6.3,5.9,5.5]$</td>
</tr>
<tr>
<td style="text-align: center;">You can't have a good season unless it starts well, " said Bill Martin, co-founder of ShopperTrak, on Saturday .</td>
<td style="text-align: center;">founded by</td>
<td style="text-align: center;">K-Adapter</td>
<td style="text-align: center;">['founded by', 'member of', 'employee of']</td>
<td style="text-align: center;">$[10.2,9.3,7.3]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">['no relation', 'founded by', 'employee of']</td>
<td style="text-align: center;">$[10.0,8.5,5.4]$</td>
</tr>
</tbody>
</table>
<p>Table 5: A case study for K-ADAPTER and RoBERTa on relation classification dataset TACRED. Underlines and wavy lines highlight the subject entities and object entities respectively. We report the top 3 ranked predictions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The native language of Mammootty is MASK.</td>
<td style="text-align: center;">Malayalam</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">English, Tamil, Hindi, Sanskrit, Arabic, Chinese</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-ADAPTER</td>
<td style="text-align: center;">Malayalam, Tamil, Hindi, Mandarin, English</td>
</tr>
<tr>
<td style="text-align: center;">Ravens can MASK.</td>
<td style="text-align: center;">fly</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">win, play, score, lose, run, drink, fly, roll, wait</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-ADAPTER</td>
<td style="text-align: center;">fly, swim, sing, shoot, kill, go, fish, drink, die</td>
</tr>
<tr>
<td style="text-align: center;">Sometimes virus causes MASK.</td>
<td style="text-align: center;">infection</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">cancer, death, illness, blindness, paralysis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-ADAPTER</td>
<td style="text-align: center;">cancer, illness, death, infection, disease</td>
</tr>
<tr>
<td style="text-align: center;">Sunshine Coast, British Columbia is located in MASK.</td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Florida, California, Texas, Hawaii, Mexico</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-ADAPTER</td>
<td style="text-align: center;">Canada, Vancouver, Victoria, BC, Australia</td>
</tr>
<tr>
<td style="text-align: center;">iPod Touch is produced by MASK.</td>
<td style="text-align: center;">Apple</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Apple, Samsung, Qualcomm, LG, Microsoft</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">K-ADAPTER</td>
<td style="text-align: center;">Apple, HTC, Samsung, Motorola, Intel</td>
</tr>
</tbody>
</table>
<p>Table 6: Examples of LAMA generation for RoBERTa $<em A="A" E="E" G="G" L="L" R="R">{L A R G E}$ and K-ADAPTER. The last column reports the top ranked predicted tokens. Correct predictions are in bold.
which is masked. Table 6 shows several examples for the generation of RoBERTa $</em>$ and K-ADAPTER for LAMA queries. From these examples, we can find that the objects predicted by KADAPTER are more accurate, which demonstrate that K-ADAPTER captures richer factual knowledge than RoBERTa. More details about this probing experiments can be found in the Appendix A and E.4.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose a flexible and simple approach, called K-ADAPTER, to infuse knowledge into large pre-trained models. K-ADAPTER remains the original parameters of pre-trained models unchanged and supports continual knowledge
infusion, i.e., new kinds of injected-knowledge will not affect the parameters learned for old knowledge. Specifically, factual knowledge and linguistic knowledge are infused into RoBERTa with two kinds of adapters, which are pre-trained on the relation classification task and dependency relation prediction task, respectively. Extensive experiments on three knowledge-driven downstream tasks demonstrate that the performance of each adapter achieves a significant improvement individually, and even more together. Detailed analyses further suggest that K-ADAPTER captures richer factual and commonsense knowledge than RoBERTa, and provide insights on the effectiveness of knowledge infusion. In future work, we will infuse more types of knowledge, and apply our framework to more pre-trained models.</p>
<h2>Acknowledgments</h2>
<p>This work is partically supported by National Natural Science Foundation of China (No. 71991471), Science and Technology Commission of Shanghai Municipality Grant (No.20dz1200600, 21QA1400600).</p>
<h2>References</h2>
<p>Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the Blanks: Distributional Similarity for Relation Learning. In $A C L$, pages 2895-2905.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In NIPS, pages 2787-2795.</p>
<p>Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. 2018. Ask the Right Questions: Active Question Reformulation with Reinforcement Learning. In $I C L R$.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In $A C L$, pages $87-96$.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. In $A C L$, pages 2978-2988.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pages 4171-4186.</p>
<p>Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017. Quasar: Datasets for question answering by search and reading. In arXiv preprint arXiv:1707.03904.</p>
<p>Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Güney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A New Q\&amp;A Dataset Augmented with Context from a Search Engine. In ArXiv preprint arXiv:1704.05179.</p>
<p>Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In LREC.</p>
<p>Philip Gage. 1994. A new algorithm for data compression. The C Users Journal, 12(2):23-38.</p>
<p>Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, and Tong Xu. 2020. BERTMK: Integrating graph contextualized knowledge into pre-trained language models. In Findings of the</p>
<p>Association for Computational Linguistics: EMNLP 2020, pages 2281-2290, Online. Association for Computational Linguistics.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In ICML, pages 2790-2799.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In EMNLP, pages 2391-2401.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Anne Lauscher, Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavaš. 2019. Informing unsupervised pretraining with external linguistic knowledge. arXiv preprint arXiv:1909.02339.</p>
<p>Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2020. SenseBERT: Driving some sense into BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4656-4667, Online. Association for Computational Linguistics.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In $A C L$, pages 1736-1745.</p>
<p>Xiao Ling, Sameer Singh, and Daniel S. Weld. 2015. Design challenges for entity linking. TACL, 3:315328 .</p>
<p>Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 2226, 2012, Toronto, Ontario, Canada. AAAI Press.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL, pages 2227-2237.</p>
<p>Matthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced contextual word representations. In EMNLP, pages 4354.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language Models as Knowledge Bases? In EMNLP, pages 2463-2473.</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020. E-BERT: Efficient-yet-effective entity embeddings for BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 803-818, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. OpenAI Blog.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In International Conference on Learning Representations.</p>
<p>Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2016. An attentive neural architecture for fine-grained entity type classification. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction(AKBC), pages 6974 .</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS, pages 5998-6008.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. 2018a. R3: Reinforced ranker-reader for open-domain question answering. In $A A A I$.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018b. Evidence aggregation for answer re-ranking in opendomain question answering. In $I C L R$.</p>
<p>Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In $A C L$, pages 189-198.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9(0):176-194.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model. In $I C L R$.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction. In EMNLP, pages 2205-2215.</p>
<p>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Positionaware Attention and Supervised Data Improve Slot Filling. In EMNLP, pages 35-45.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced Language Representation with Informative Entities. In $A C L$, pages 1441-1451.</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In ICCV, pages 19-27.</p>
<h2>Appendix</h2>
<h2>A Probing Experiments</h2>
<p>Although K-ADAPTER models have shown superior performance on knowledge-driven downstream tasks, it does not directly provide insights into whether our models infuse richer factual knowledge. Thus we utilize a LAMA (LAnguage Model Analysis) probe (Petroni et al., 2019) to examine the ability to memorize factual knowledge. Specifically, the LAMA probing task is under a zeroshot setting, which requires the language model to answer cloze-style questions about relational facts without fine-tuning, e.g., "Simon Bowman was born in [MASK]". The model needs to predict a distribution over a limited vocabulary to replace [MASK]. We report mean precision at one (P@1) macro-averaged over relations.</p>
<p>Settings We consider several language models including: ELMo (Peters et al., 2018), ELMo5.5B (Peters et al., 2018), Transformer-XL (Dai et al., 2019), BERT $<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ and RoBERTa $</em>$. We focus on LAMA-GoogleRE and LAMA-T-REx, which are aimed at factual knowledge. We also conduct probe experiments on LAMA-UHN (Poerner et al., 2020), a more "factual" subset of LAMA, by filtering out queries that are easy to answer from entity names alone. Different models have different vocabulary sizes. To conduct a more fair comparison experiment, we adopt the intersection of vocabularies and let every language model rank only tokens in this vocabulary following Petroni et al. (2019). For simplicity, we only compare KAPDATER (F) which is infused with factual knowledge, with other baseline models.}</p>
<p>Results and Discussion Results are shown in Table 7. It is surprising that BERT $<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ performs better than RoBERTa $</em>$. There is one possible reason: BERT uses a character-level BPE (Gage, 1994) vocabulary, while RoBERTa considers bytelevel BPE vocabulary. This finding indicates that, although using bytes makes it possible to learn a subword vocabulary that can encode any text without introducing "unknown" tokens, it might indirectly harm the model's ability to learn factual knowledge, e.g., some proper nouns may be divided into bytes. Thus in the following experiments, we do not take BERT into account.}</p>
<p>K-ADAPTER outperforms other models (except for BERT) by a huge margin. As for LAMA, com-
pared to RoBERTa $<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$, K-ADAPTER obtains $2.2 \%$ and $1.2 \%$ P@1 improvement across GoogleRE and T-REx, respectively. Moreover, compared to RoBERTa $</em>$, K-ADAPTER still achieves better results on LAMA-UHN. The results demonstrate that K-ADAPTER captures richer factual and commonsense knowledge than RoBERTa.}</p>
<h2>B Pre-Training Details</h2>
<h2>B. 1 Factual Adapter</h2>
<p>The pre-trained model is fixed during training and the parameters of the factual adapter are trainable and initialized randomly. The model is trained with cross-entropy loss. To accelerate the training process, we set the max sequence length as 64 as the average sequence length of T-REx-rc is only 22.8. We train the model for 5 epochs using a batch size of 128. We use AdamW to optimize our models with the initial learning rate of $2 \mathrm{e}-5$. We train the model with 4 16G NVIDIA V100 GPUs.</p>
<h2>B. 2 Linguistic Adapter</h2>
<p>Same as the training process of the factual adapter, the pre-trained model is fixed during training and the parameters of the linguistic adapter are trainable and initialized randomly. The model is trained with BCEWithLogits loss. We set the max sequence length as 128 . We train the model for 10 epochs using a batch size of 256. We use AdamW with the initial learning rate of $1 \mathrm{e}-5$. We train the model with 4 16G NVIDIA V100 GPUs.</p>
<h2>C Applying K-adapter on Downstream Tasks</h2>
<p>For the downstream tasks, the key point here is the combination of the pre-trained model's representations and adapter's representations, that is to say: leveraging the general information of the pre-trained model on one hand, and the specific knowledge in the adapter on the other. To use K-ADAPTER for downstream tasks is very simple as shown in Figure 5. Usually, when we use pre-trained language models such as BERT and RoBERTa for downstream tasks, we feed the output features from the pre-trained model into the task-specific layer, and then do the corresponding downstream task. As for the K-ADAPTER, we finetune it just like what the orginal BERT or RoBERTa does. We concatenate the output features of the pretrained model with the features of the adapter, and then feed them to the task-specific layer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Corpus</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ELMo</td>
<td style="text-align: center;">ELMo5.5B</td>
<td style="text-align: center;">TransformerXL</td>
<td style="text-align: center;">BERT-large</td>
<td style="text-align: center;">RoBERTa $_{\text {LARGE }}$</td>
<td style="text-align: center;">K-APDATER</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-Google-RE</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-Google-RE</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">23.0</td>
</tr>
</tbody>
</table>
<p>Table 7: P@1 on LAMA and LAMA-UHN across Google-RE and T-REx corpora.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An overview of our K-ADAPTER to inject specific knowledge by training a knowledge-specific adapter on the pre-training task.</p>
<h2>D Dataset statistics</h2>
<p>In Table 8, we present the statistics of one relation classification dataset TACRED, and two entity typing datasets OpenEntity and FIGER. In Table 9, we present the statistics of one commonsense QA dataset CosmosQA and two open-domain QA datasets SearchQA and Quasar-T.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Relation/Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TACRED</td>
<td style="text-align: center;">68,124</td>
<td style="text-align: center;">22,631</td>
<td style="text-align: center;">15,509</td>
<td style="text-align: center;">42</td>
</tr>
<tr>
<td style="text-align: left;">Open Entity</td>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">FIGER</td>
<td style="text-align: center;">$2,000,000$</td>
<td style="text-align: center;">10,000</td>
<td style="text-align: center;">563</td>
<td style="text-align: center;">113</td>
</tr>
</tbody>
</table>
<p>Table 8: The statistics of the relation classification dataset TACRED and entity typing datasets, i.e., Open Entity and FIGER.</p>
<h2>E Fine-tuning Details</h2>
<p>We implement our experiments using Huggingface ${ }^{4}$. For all fine-tuning experiments, we use AdamW as the optimizer. The parameters of adapters are fixed during the fine-tuning process</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Dataset | Train | Dev | Test |
| :-- | :--: | :--: | :--: |
| CosmosQA | 25,588 | 3,000 | 7,000 |
| SearchQA | 99,811 | 13,893 | 27,247 |
| Quasar-T | 28,496 | 3,000 | 3,000 |</p>
<p>Table 9: The statistics of the question answering datasets, i.e., CosmosQA, SearchQA and Quasar-T.
and the parameters of RoBERTa are trainable and initialized from Huggingface checkpoint. We select the best hyperparameters on the validation set. For all experiments, we set the random seed to be 42 for reproductibility.</p>
<h2>E. 1 Entity typing</h2>
<p>For Open Entity dataset, we set the max sequence length to be 256 and select the hyperparameters from batch size: ${4,8}$, learning rate: ${2 \mathrm{e}-5,1 \mathrm{e}-$ $5,5 \mathrm{e}-6}$ and warmup step: ${0,200,500,1000$, 1200}. For K-ADAPTER (F), the best performance is achieved at batch size $=4, \mathrm{l}=5 \mathrm{e}-6$, warmup $=500$ (it takes about 2 hours to get the best result running on singe 16G P100). For K-ADAPTER (L), the best performance is achieved at batch size $=4$,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An example of using relation classification as a pre-training task to inject knowledge into K-ADAPTER: given "Barack Obama was born in Honolulu", and then predicts the relationship between "Barack Obama" and "Honolulu" is "Birth-of-place".
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Fine-tuning K-ADAPTER just like what the original RoBERTa or BERT does.
$\mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup $=1000$ (it takes about 2 hours to get the best result running on singe 16G P100). For K-ADAPTER (F+L), the best performance is achieved at batch size=4, $\mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup=1000 (it takes about 3 hours to get the best result running on singe 16G P100). For FIGER dataset, we run experiments on 416 G P100 for 3 epochs, set the max sequence length to be 256 , and select the hyperparameters from batch size: ${64,512,2048}$, learning rate: ${2 \mathrm{e}-5,1 \mathrm{e}-5,5 \mathrm{e}-6}$ and warmup step: ${0$, 200, 500, 1000, 1200}. For K-ADAPTER (F), the best performance is achieved at batch size=2048, $\mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup=500. For K-ADAPTER (L), the best performance is achieved at batch size=2048, $\mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup=200. For K-ADAPTER (F+L), the best performance is achieved at batch size=2048, $\mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup $=1000$.</p>
<h2>E. 2 Question Answering</h2>
<p>For CosmosQA dataset, we run experiments on one single 16G P100 for 3 epochs, set the max sequence
length to be 256, and select the hyperparameters from batch size: ${16,32,64,128}$, learning rate: ${2 \mathrm{e}-5,1 \mathrm{e}-5,5 \mathrm{e}-6}$ and warmup step: ${0,200,500$, $800,1000}$. For K-ADAPTER (F+L) and its ablated models, the best performance is achieved at batch size $=64, \mathrm{l} \mathrm{r}=1 \mathrm{e}-5$, warmup $=0$ (it takes about 8 hours to get the best result).</p>
<p>For SearchQA dataset, we run experiments on one single 16G P100 for 2 epochs, set the max sequence length to be 128 , and select the hyperparameters from batch size: ${2,4,8,16}$, learning rate: ${5 \mathrm{e}-5,2 \mathrm{e}-5,1 \mathrm{e}-5,5 \mathrm{e}-6}$ and warmup step: ${0$, $500,1000}$. For K-ADAPTER (F+L) and its ablated models, the best performance is achieved at batch size $=8, \mathrm{l} \mathrm{r}=5 \mathrm{e}-6$, warmup $=0$. For Quasar-T dataset, we run experiments on one single 16G P100 for 5 epochs, set the max sequence length to be 256 , and select the hyperparameters from batch size: ${2,4$, $8,16}$, learning rate: ${5 \mathrm{e}-5,2 \mathrm{e}-5,1 \mathrm{e}-5,5 \mathrm{e}-6}$ and warmup step: ${0,500,1000}$. For K-ADAPTER ( $\mathrm{F}+\mathrm{L}$ ) and its ablated models, the best performance</p>
<p>is achieved at batch size $=16, \mathrm{l} r=1 \mathrm{e}-5$, warmup $=0$.</p>
<h1>E. 3 Relation Classification</h1>
<p>For TACRED dataset, we run experiments on 4 16G P100 for 5 epochs, set the max sequence length to be 184 , and select the hyperparameters from batch size: ${4,8,16,32}$, learning rate: ${2 \mathrm{e}-5$, $1 \mathrm{e}-5,5 \mathrm{e}-6,1 \mathrm{e}-6}$ and warmup step: ${0,200,500$, 800, 1000, 1200}. For K-ADAPTER (F), the best performance is achieved at batch size $=32, \mathrm{l} r=1 \mathrm{e}-$ 5, warmup=500. For K-ADAPTER (L), the best performance is achieved at batch size $=32, \mathrm{l} r=1 \mathrm{e}-5$, warmup=200. For K-ADAPTER (F+L), the best performance is achieved at batch size $=32, \mathrm{l} r=5 \mathrm{e}-6$, warmup $=1000$.</p>
<h2>E. 4 Probing Experiments</h2>
<p>We implement our probing experiments using LAMA ${ }^{5}$. When we infuse knowledge into knowledge-specific adapters, we do not change the original parameters of the pre-trained model and thus do not adopt the masked language model (MLM) as a pre-training task. Therefore, before we conduct probing experiments, we need to add and train a linear layer as the mlm layer for predicting the [MASK] entities. Specifically, we fix all the parameters of K-ADAPTER and only update the parameters of the mlm layer using a masked language modeling (MLM) loss. We adopt the raw WikiText-2 dataset (181M). We train the mlm layer with one single 16G P100 for 2 epochs. We set the max sequence length to be 512 , batch size to be 1024 and warmup step to be 0 .</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/facebookresearch/LAMA&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ http://nlp.stanford.edu/software/lex-parser.html&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
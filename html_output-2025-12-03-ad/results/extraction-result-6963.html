<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6963 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6963</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6963</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267312134</p>
                <p><strong>Paper Title:</strong> <a href="https://watermark.silverchair.com/btae238.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA4EwggN9BgkqhkiG9w0BBwagggNuMIIDagIBADCCA2MGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMvZYbFq-PPNeHg0XbAgEQgIIDNL24k2NC7msALm7lQmBkt-FK_JPKxWY7S6W5mJ2OLIUXfRXECfsgcyMphkz_xdniBXOtmTmtYypyMkjLSMdFpLOiEf4Ae5OSDX8lJjEhqvPrZCfmkZnIpuhpqaeFRWt0-zkhJ5PkypTPRVlGP7WlsWqQ8qMFkAvCDyBE68AWvMwvSQvba53NS6_UWwj8mDOB86mzgXpHa-u63U7TBsf__UXGvmHSy1eSRndYOd6ZNOTzdwAEWuNYB9r8_Jl0rot0PvlVsaQQLJJdDfGYce3K55ueFNQcaTDGeONj0uIIVhJvfHJQTKgxUTqSgTUs3aHjtLh7-8WSw0fJYQyo8KYEewCSF8bBymXtRjtdi44sUfMKeXmVBJIhx6hazeQvB4s_nqjOBnNCqzilZsZapXNIXS9N8E4Nf3k6URMOWHSWNTbB702KH7Rw2r6EErXO6IcTDejo3koX9Q8_JkJu3Za3ENMvqGZYvC5QvTSradkGEqDdYgQ6F1G9g2XIylyggiMsPqVVDluao1yEZkPAnF57RFoJGEJ0KP6f59j4Q_ODjA-nKjPud98VLPgX76lg4qPO4B81J5iobrMBzgdFo1M_9-86G7A0-_A11SMoYPM1VszqWYu4KF6Hn5txdBaByz622tWVG7udc_6zHYX3OtZZfr7r5m4bbk0p_uDCcDCrsprf68Bzhw1tT7eg2CEW2uzam88aFn_WTtaRjX5FtjCrPghSJyVgC-JnLn5q594cJJ1a9deVlBjfMYlJQcgklDPM1pefOVSaB4ZxXRDYFrZXjkl5UjV6L5QO0MoK3m5uxwqV-vm9T_q1FX6Wxt63DPd2vh8gbJc1k8aaZYNfirO-m-Z0BT8G6kjzK2rRxvWukkZpC8G0uTwD7B3aARtrUxWRBApvR2Kjxs3telBdz4jM7f8noS962cGmg3w4NmvQXwqcDUEx41oJ8W3sVzJsOzWvulL8EEWxm4M3dz40J0Obz-Qxet-D_6mynATpADUsz5smuh7zx-xBOF2WUIE6j_0qGAdsqhWF6-D7RbigHEUcOVO-BZIExddGTtwAbQjyByPpXYgJgIgGjxgD_CQBiW0VNoaCRi0" target="_blank">Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6963.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6963.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-BioRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-BioRAG (Retrieval-augmented LLM with domain-specific self-reflection for biomedical text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adapted RAG framework that integrates a critic (self-reflection) mechanism based on reflective tokens, a biomedical retriever (MedCPT), and instruction-tuned generator LMs to decide on-demand retrieval, rank evidence, and generate answers with self-assessed rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-BioRAG (generator LM M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned generator initialized from Self-RAG generator weights and fine-tuned on 84k biomedical instruction instances annotated with reflective tokens; conditions generation on predicted reflective tokens and retrieved evidence from a MedCPT retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (primary reported); also evaluated a 13B variant</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective tokens (Self-RAG-style self-reflection: RET, REL, SUP, USE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During generation the model produces/uses pre-defined reflective tokens that indicate (RET) whether retrieval is needed, (REL) whether retrieved evidence is relevant, (SUP) whether evidence supports the answer (fully/partially/no), and (USE) the utility of the answer (1–5). Token probabilities are used to (a) decide adaptive retrieval, (b) score and rank retrieved evidences, and (c) guide answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-step generate-and-critique via reflective tokens (no multi-cycle iterative refinement reported)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MedQA, MedMCQA, MMLU-Med (multi-choice medical QA); LiveQA and MedicationQA (long-form medical QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice medical exam style question answering (MedQA, MedMCQA, MMLU-Med) and consumer/clinical long-form question answering (LiveQA, MedicationQA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (for multi-choice tasks); ROUGE-1/ROUGE-2/ROUGE-L and BERTScore (for long-form generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Ablation without reflective tokens (reported as "-Reflective Tokens"): MedQA 42.5% (accuracy), MedMCQA 41.9% (accuracy), MMLU-Med 51.1% (accuracy), Average 45.2% (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Self-BioRAG (with reflective tokens): MedQA 43.6% (accuracy), MedMCQA 42.1% (accuracy), MMLU-Med 53.9% (accuracy), Average 46.5% (accuracy). Long-form (Self-BioRAG) — LiveQA: ROUGE-1/ROUGE-2/ROUGE-L/BERTScore = 19.7 / 3.1 / 13.4 / 77.2; MedicationQA: 17.6 / 3.3 / 13.5 / 80.2.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) the reflective-token Self-RAG approach (without domain adaptation) fails to generalize to biomedical text and yields performance drops; (2) adaptive retrieval uses only a small portion of available evidence for the evaluated benchmarks, implying limited benefit for some examples; (3) forcing retrieval ('Only [Retrieval]') is unstable across datasets (improves MedMCQA but degrades MMLU-Med), indicating the retrieval decision must be adaptive; (4) long-form generation evaluation remains coarse (ROUGE/BERTScore) and detailed analysis of hallucination/support in long texts is left for future work; (5) no multi-cycle generate-then-reflect iterations were reported (single-step critique only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6963.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6963.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic LM C (Reflective tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic language model C and reflective-token annotation (RET, REL, SUP, USE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific critic model trained to predict four reflective tokens (need-to-retrieve, relevance, support, utility) that enable automatic self-assessment of generated outputs and evidence; used to annotate instruction data and to provide scoring signals in inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Critic LM C (initialized from LLaMA2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A LLaMA2-initialized model fine-tuned on ~5k GPT-4-annotated biomedical instruction examples to predict reflective token labels for (input, output, evidence) triplets; used to annotate 120k instruction instances and filter to 84k training cases for the generator LM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective-token prediction (RET, REL, SUP, USE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given (instruction, model output, optional evidence), the critic predicts discrete reflective tokens: RET (yes/no/continue) to indicate retrieval need; REL (relevant/irrelevant) to indicate evidence usefulness; SUP (fully/partially/no support) to indicate evidence support of answer statements; USE (1–5) to assess overall utility. These predictions were produced by critic C (trained from GPT-4 labels) and used to filter training data and to score evidences during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-step token prediction (used for annotation and evidence scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Annotation of biomedical instruction datasets; scoring and filtering training instances; evidence scoring during inference for MedQA/MedMCQA/MMLU-Med and long-form QA evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Annotates diverse biomedical instruction-output pairs (QA, information extraction, summarization, classification, relation extraction, multi-choice) with reflective tokens; used both offline to filter data and online (probabilities) to rank evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Critic training objective: token log-likelihood (maximize p_C(r | x, y)); downstream effect measured indirectly via generator performance after filtering (no separate critic accuracy metrics reported in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper notes they filtered out instances where the critic mispredicted non-predefined tokens (e.g., '[Continue Generation]'). No separate numeric performance of the critic is reported; reliance on GPT-4 annotations and domain-specific fine-tuning was necessary because prior (non-domain) critics do not generalize well to biomedical content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6963.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6963.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-RAG (learning to retrieve, generate, and critique through self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior framework that introduced reflective tokens and a critic model to let an LLM decide whether to retrieve and to critique its generated outputs; used as a conceptual and model-weight starting point in this work but shown to generalize poorly to biomedical tasks without domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: learning to retrieve, generate, and critique through self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-RAG (original)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework combining a generator LM and a critic LM with reflective tokens to perform on-demand retrieval, evaluate retrieved context, and critique alignment between rationale and evidence; original work used contrastive retrievers (e.g., Contriever) and instruction-tuned LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective tokens (same token types RET, REL, SUP, USE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Self-RAG trains models to emit reflective tokens that indicate whether to retrieve and whether retrieved evidence supports the answer; these signals guide retrieval and final output selection.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-step reflect-and-retrieve (no multi-cycle iterative refinement reported in the referenced Self-RAG description)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain retrieval-augmented question answering and generation (used as baseline comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General open-domain QA and retrieval-augmented generation; in this paper Self-RAG (as originally configured with Wikipedia/Contriever) was evaluated on biomedical benchmarks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>In this paper Self-RAG is compared on standard QA metrics (accuracy for multi-choice benchmarks); exact baseline numbers are reported in the paper tables but Self-RAG is described qualitatively to underperform on biomedical tasks without domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper reports that Self-RAG, when not adapted for biomedical domain (e.g., using Wikipedia and non-domain retriever), showed poor generalization: fetching incorrect documents and making inaccurate judgments on biomedical QA benchmarks. This motivated the domain-specific design of Self-BioRAG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-rag: learning to retrieve, generate, and critique through self-reflection <em>(Rating: 2)</em></li>
                <li>MedCPT: contrastive pre-trained transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval <em>(Rating: 1)</em></li>
                <li>Self-Instruct: Aligning Language Model with Self Generated Instructions <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6963",
    "paper_id": "paper-267312134",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Self-BioRAG",
            "name_full": "Self-BioRAG (Retrieval-augmented LLM with domain-specific self-reflection for biomedical text)",
            "brief_description": "A domain-adapted RAG framework that integrates a critic (self-reflection) mechanism based on reflective tokens, a biomedical retriever (MedCPT), and instruction-tuned generator LMs to decide on-demand retrieval, rank evidence, and generate answers with self-assessed rationales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Self-BioRAG (generator LM M)",
            "model_description": "An instruction-tuned generator initialized from Self-RAG generator weights and fine-tuned on 84k biomedical instruction instances annotated with reflective tokens; conditions generation on predicted reflective tokens and retrieved evidence from a MedCPT retriever.",
            "model_size": "7B (primary reported); also evaluated a 13B variant",
            "reflection_method_name": "Reflective tokens (Self-RAG-style self-reflection: RET, REL, SUP, USE)",
            "reflection_method_description": "During generation the model produces/uses pre-defined reflective tokens that indicate (RET) whether retrieval is needed, (REL) whether retrieved evidence is relevant, (SUP) whether evidence supports the answer (fully/partially/no), and (USE) the utility of the answer (1–5). Token probabilities are used to (a) decide adaptive retrieval, (b) score and rank retrieved evidences, and (c) guide answer generation.",
            "iteration_type": "single-step generate-and-critique via reflective tokens (no multi-cycle iterative refinement reported)",
            "num_iterations": null,
            "task_name": "MedQA, MedMCQA, MMLU-Med (multi-choice medical QA); LiveQA and MedicationQA (long-form medical QA)",
            "task_description": "Multi-choice medical exam style question answering (MedQA, MedMCQA, MMLU-Med) and consumer/clinical long-form question answering (LiveQA, MedicationQA).",
            "evaluation_metric": "Accuracy (for multi-choice tasks); ROUGE-1/ROUGE-2/ROUGE-L and BERTScore (for long-form generation)",
            "performance_before_reflection": "Ablation without reflective tokens (reported as \"-Reflective Tokens\"): MedQA 42.5% (accuracy), MedMCQA 41.9% (accuracy), MMLU-Med 51.1% (accuracy), Average 45.2% (accuracy).",
            "performance_after_reflection": "Self-BioRAG (with reflective tokens): MedQA 43.6% (accuracy), MedMCQA 42.1% (accuracy), MMLU-Med 53.9% (accuracy), Average 46.5% (accuracy). Long-form (Self-BioRAG) — LiveQA: ROUGE-1/ROUGE-2/ROUGE-L/BERTScore = 19.7 / 3.1 / 13.4 / 77.2; MedicationQA: 17.6 / 3.3 / 13.5 / 80.2.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Reported limitations include: (1) the reflective-token Self-RAG approach (without domain adaptation) fails to generalize to biomedical text and yields performance drops; (2) adaptive retrieval uses only a small portion of available evidence for the evaluated benchmarks, implying limited benefit for some examples; (3) forcing retrieval ('Only [Retrieval]') is unstable across datasets (improves MedMCQA but degrades MMLU-Med), indicating the retrieval decision must be adaptive; (4) long-form generation evaluation remains coarse (ROUGE/BERTScore) and detailed analysis of hallucination/support in long texts is left for future work; (5) no multi-cycle generate-then-reflect iterations were reported (single-step critique only).",
            "uuid": "e6963.0",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Critic LM C (Reflective tokens)",
            "name_full": "Critic language model C and reflective-token annotation (RET, REL, SUP, USE)",
            "brief_description": "A domain-specific critic model trained to predict four reflective tokens (need-to-retrieve, relevance, support, utility) that enable automatic self-assessment of generated outputs and evidence; used to annotate instruction data and to provide scoring signals in inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Critic LM C (initialized from LLaMA2)",
            "model_description": "A LLaMA2-initialized model fine-tuned on ~5k GPT-4-annotated biomedical instruction examples to predict reflective token labels for (input, output, evidence) triplets; used to annotate 120k instruction instances and filter to 84k training cases for the generator LM.",
            "model_size": null,
            "reflection_method_name": "Reflective-token prediction (RET, REL, SUP, USE)",
            "reflection_method_description": "Given (instruction, model output, optional evidence), the critic predicts discrete reflective tokens: RET (yes/no/continue) to indicate retrieval need; REL (relevant/irrelevant) to indicate evidence usefulness; SUP (fully/partially/no support) to indicate evidence support of answer statements; USE (1–5) to assess overall utility. These predictions were produced by critic C (trained from GPT-4 labels) and used to filter training data and to score evidences during inference.",
            "iteration_type": "single-step token prediction (used for annotation and evidence scoring)",
            "num_iterations": null,
            "task_name": "Annotation of biomedical instruction datasets; scoring and filtering training instances; evidence scoring during inference for MedQA/MedMCQA/MMLU-Med and long-form QA evaluation.",
            "task_description": "Annotates diverse biomedical instruction-output pairs (QA, information extraction, summarization, classification, relation extraction, multi-choice) with reflective tokens; used both offline to filter data and online (probabilities) to rank evidence.",
            "evaluation_metric": "Critic training objective: token log-likelihood (maximize p_C(r | x, y)); downstream effect measured indirectly via generator performance after filtering (no separate critic accuracy metrics reported in main text).",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "The paper notes they filtered out instances where the critic mispredicted non-predefined tokens (e.g., '[Continue Generation]'). No separate numeric performance of the critic is reported; reliance on GPT-4 annotations and domain-specific fine-tuning was necessary because prior (non-domain) critics do not generalize well to biomedical content.",
            "uuid": "e6963.1",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-RAG (learning to retrieve, generate, and critique through self-reflection)",
            "brief_description": "Prior framework that introduced reflective tokens and a critic model to let an LLM decide whether to retrieve and to critique its generated outputs; used as a conceptual and model-weight starting point in this work but shown to generalize poorly to biomedical tasks without domain adaptation.",
            "citation_title": "Self-rag: learning to retrieve, generate, and critique through self-reflection",
            "mention_or_use": "use",
            "model_name": "Self-RAG (original)",
            "model_description": "Framework combining a generator LM and a critic LM with reflective tokens to perform on-demand retrieval, evaluate retrieved context, and critique alignment between rationale and evidence; original work used contrastive retrievers (e.g., Contriever) and instruction-tuned LMs.",
            "model_size": null,
            "reflection_method_name": "Reflective tokens (same token types RET, REL, SUP, USE)",
            "reflection_method_description": "Self-RAG trains models to emit reflective tokens that indicate whether to retrieve and whether retrieved evidence supports the answer; these signals guide retrieval and final output selection.",
            "iteration_type": "single-step reflect-and-retrieve (no multi-cycle iterative refinement reported in the referenced Self-RAG description)",
            "num_iterations": null,
            "task_name": "Open-domain retrieval-augmented question answering and generation (used as baseline comparison in this paper)",
            "task_description": "General open-domain QA and retrieval-augmented generation; in this paper Self-RAG (as originally configured with Wikipedia/Contriever) was evaluated on biomedical benchmarks as a baseline.",
            "evaluation_metric": "In this paper Self-RAG is compared on standard QA metrics (accuracy for multi-choice benchmarks); exact baseline numbers are reported in the paper tables but Self-RAG is described qualitatively to underperform on biomedical tasks without domain adaptation.",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": false,
            "limitations_or_failure_cases": "The paper reports that Self-RAG, when not adapted for biomedical domain (e.g., using Wikipedia and non-domain retriever), showed poor generalization: fetching incorrect documents and making inaccurate judgments on biomedical QA benchmarks. This motivated the domain-specific design of Self-BioRAG.",
            "uuid": "e6963.2",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-rag: learning to retrieve, generate, and critique through self-reflection",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "MedCPT: contrastive pre-trained transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval",
            "rating": 1,
            "sanitized_title": "medcpt_contrastive_pretrained_transformers_with_largescale_pubmed_search_logs_for_zeroshot_biomedical_information_retrieval"
        },
        {
            "paper_title": "Self-Instruct: Aligning Language Model with Self Generated Instructions",
            "rating": 1,
            "sanitized_title": "selfinstruct_aligning_language_model_with_self_generated_instructions"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.016055,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models</p>
<p>Minbyul Jeong 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>Jiwoong Sohn 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>Mujeen Sung mujeensung@khu.ac.kr 
Department of Software Convergence
School of Computing
Kyung Hee University
Republic of Korea</p>
<p>Corresponding authors. Department of Software Convergence
School of Computing
Kyung Hee University
Gyeonggi-do17104Republic of Korea</p>
<p>Jaewoo Kang kangj@korea.ac.kr 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>AIGEN Sciences
04778SeoulRepublic of Korea</p>
<p>Corresponding authors. Department of Software Convergence
School of Computing
Kyung Hee University
Gyeonggi-do17104Republic of Korea</p>
<p>1-gil, Seongdong-gu04778SeoulRepublic of Korea</p>
<p>Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models
3E20EB0B04C27BEFF32B01E6D7EBE3F810.1093/bioinformatics/btae238
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations.To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation.However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments.In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses.We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens.Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions.Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less.Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average.Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does.We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.</p>
<p>Introduction</p>
<p>The recent proprietary large language models (LLMs) such as ChatGPT (OpenAI 2023a), GPT-4 (OpenAI 2023b), and BARD (Google 2023) have succeeded in reaching near or comparable levels to human experts in solving many challenging problems, ranging from multi-choice question answering to long-form text generations.While these models exhibit high efficiency and demonstrate their versatility in various domains, they fall short in comprehensively covering user-dependent information such as patient reports with encoded knowledge.These limitations can result in a groundless statement and inadvertent generation of false information, commonly known as the hallucination issue (Cao et al. 2022, Singhal et al. 2022, Wei et al. 2022, Ji et al. 2023).To address this challenge, retrievalaugmented generation (RAG) enhances explainability for readers by supplying supporting facts that underpin the responses generated by LLMs (Guu et al. 2020, Lewis et al. 2020).As illustrated in Fig. 1, various RAG frameworks search documents from the knowledge corpus such as Wikipedia and appending them unconditionally or selectively to the input of LLMs for generation.In alignment with this approach, the authors of Asai et al. (2023) introduce Self-RAG, which uses reflective tokens that learn to reflect on its generation process given a task input possessing the following capabilities: deciding when to use on-demand retrieval, assessing whether retrieved evidence provides useful information to solve the question, criticizing whether the evidence supports the answer, and judging whether the answer is a useful response to the question.However, using Self-RAG is unsuitable for domain-specific questions like biomedical or clinical domains which shows poor generalization, leading to fetching incorrect documents or making inaccurate judgments.</p>
<p>In this paper, we introduce Self-BioRAG, trained with a focus on biomedical and clinical text instructions, enabling it to address corresponding instructions adeptly.It preserves generation quality and reasoning ability while incorporating on-demand retrieval and self-reflection capabilities.Note that we use the term reasoning to indicate that Self-BioRAG can provide explanations on answers.To build a Self-BioRAG framework, four essential components are required: (i) biomedical instruction sets, (ii) biomedical retriever, (iii) self-reflection language model, and (iv) domain-specific instruction-tuned language model.We initially construct instruction sets focused on biomedical and clinical text.In addition to the distributed MoL-instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023), we synthetically generate an additional 18k biomedical and clinical instructions following the Self-Instruct (Wang et al. 2022).By combining three datasets, we could construct 120k instruction sets addressing various biomedical instructions, including information extraction, question answering, summarization, text classification, relation extraction, and multi-choice questions (Section 3.1).</p>
<p>Furthermore, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever and construct biomedical corpora as follows: PubMed Abstract, PMC Full Text, Clinical Guideline, and Medical Textbook, all tailored to biomedical and clinical text (Section 3.2).The training process for the self-reflection language model and the domain-specific instruction-tuned language model is similar to Self-RAG, except that, instead of directly training instructions into the LLaMA2 (Touvron et al. 2023) model, we achieve better performance by training the model weights provided by Self-RAG (Section 3.3, 3.4).The goal of our work is to construct a language model encoded with domain-specific knowledge, enabling it to autonomously assess explanations and answers it generates.</p>
<p>Self-BioRAG demonstrates its effectiveness using five opendomain question-answering (QA) benchmark datasets: multichoice QA [MedQA (Jin et al. 2021), MedMCQA (Pal et al. 2022), and MMLU (Hendrycks et al. 2020)] and long-form QA [LiveQA (Abacha et al. 2017) and MedicationQA (Abacha et al. 2019)].Experimental results on the multichoice QA datasets demonstrate that Self-BioRAG significantly outperforms open-foundation LLMs and RAG approaches with a parameter size of 7B or less.Self-BioRAG achieves a 7.2% absolute improvement compared to the state-of-the-art model.In long-form QA datasets, we notice a substantial difference in the terminologies used despite generating predictions that are similar to answers.We demonstrate that domain-specific components contribute to the performance gains, with training on domain-specific instructions showing the highest improvement.Our biomedical corpora supplement scarce knowledge, and particularly, Self-BioRAG uses appropriate documents if needed corresponding to the benchmark datasets.We further analyze that using reflective tokens to adaptively retrieve factual content is effective in solving open-domain question-answering datasets.Overall, Self-BioRAG finds the clues in the question, retrieves relevant evidence, and understands how to answer with information using encoded knowledge.</p>
<p>Our contributions are as follows: (i) We introduce a Self-BioRAG framework which is extensively trained on biomedical and clinical instructions.(ii) We prove that domain-specific components such as retriever, documents, and instruction sets are necessary to address its domain-related instructions.(iii) Self-BioRAG demonstrates its effectiveness in three opendomain biomedical question-answering benchmark datasets by achieving an average absolute improvement of 7.2% compared to the state-of-the-art open-foundation model with a parameter size of 7B or less.(iv) We release our biomedical instruction sets, code for training our components used in Self-BioRAG, and model weights (7B and 13B) to be more capable in biomedical and clinical domains.</p>
<p>Background</p>
<p>Proprietary and open language models</p>
<p>Instructions serve as guidelines for how language models should perform a particular task.In the commercial field, proprietary language models such as InstructGPT (Ouyang et al. 2022) and ChatGPT (OpenAI 2023a) have gained Comparison between three frameworks: generation using language model (LM), retrieval-augmented generation (RAG) using LM, and our Self-BioRAG.(A) depicts the process of sequence-to-sequence generation of LM. (B) The RAG framework first finds relevant documents from large-scale corpus such as PubMed Central and then provides the answer based on this factual content to address the shortage of scarce knowledge.(C) Initially, our domain-specific instruction-tuned model predicts whether retrieval is necessary.If a query does not require any retrieval of knowledge (factual content), it directly predicts the answer.However, if the query necessitates retrieval knowledge, Self-BioRAG utilizes the domain-specific retriever (MedCPT, in our case) to retrieve relevant documents.After retrieving the top-k evidence, the model selects the most pertinent evidence for the query.Ultimately, our language model is employed to select the best evidence and generate the answer based on the selected evidence and encoded knowledge.</p>
<p>significant advantages in tuning through instructions.However, researchers not involved in commercial fields may face challenges in using these models due to a lack of resources.Hence, research-friendly open foundation models like the LLaMA family (Touvron et al. 2023), Self-instruct (Wang et al. 2022), andAlpaca (Taori et al. 2023) are released.In this regard, domain-specific language models tailored for areas such as biomedical and clinical domains, like Galactica (Taylor et al. 2022) and Meditron (Chen et al. 2023), have also been released.Our research also aims to provide laborinexpensive methods that are easy to use in various vertical domains, including biomedical and clinical domains.Specifically, Self-BioRAG strives to develop a model capable of solving challenging tasks, ranging from multi-choice questions to long-form generations.</p>
<p>Learning with reward strategy</p>
<p>The proprietary language models trained with reinforcement learning from human feedback (RLHF), such as ChatGPT (OpenAI 2023a) and GPT-4 (OpenAI 2023b), excel at executing straightforward instructions (e.g.translation, code generation, and question answering) in alignment with human intent (Christiano et al. 2017, Schulman et al. 2017, Google 2023, OpenAI 2023a,b).In Self-RAG (Asai et al. 2023), a critic language model is employed to offer a costeffective reward strategy compared to RLHF, utilizing reflective tokens.The critic model determines whether a given task necessitates retrieval, evaluates the appropriateness of the retrieved context, assesses if the generated rationale aligns with the retrieved context, and ultimately judges the overall utility of the output.Our Self-BioRAG follows the approach of Self-RAG to create a domain-specific critic language model that not only maintains the aforementioned capabilities but is also well-versed in biomedical text.</p>
<p>Retrieval-augmented generation</p>
<p>The retrieval-augmented generation (RAG) significantly enhances performance in knowledge-intensive tasks and open-domain question-answering by providing context as input to the language model (Lewis et al. 2020, Mao et al. 2021, Kang et al. 2023).The retriever also plays a crucial role in language models by providing evidence for pretraining and few-shot fine-tuning (Guu et al. 2020, Izacard et al. 2022a).With the recent advancements in instruction language models, the combination of retriever and language models involves either using the retriever in advance to fetch evidence or iteratively retrieving it when needed (Jiang et al. 2023, Shao et al. 2023).Our base framework, Self-RAG (Asai et al. 2023), deviates from these approaches by being designed to perform retrieval on-demand, resulting in better cost efficiency compared to scenarios where retrieval is always active.However, in domain-specific fields like biomedical or clinical domains, the general method of retrieving context may not be applicable.Therefore, Self-BioRAG utilizes retrieval methods and documents tailored to specific domains, retrieving meaningful context that aligns with the intended field.</p>
<p>Self-BioRAG</p>
<p>In this section, we outline the process of creating our Self-BioRAG framework using various biomedical components.First, we leverage three datasets consisting of biomedical instructions, which are used to train language models to align with human intentions for biomedical text (Section 3.1).To supplement scarce knowledge via relevant documents in the biomedical or clinical domains, we employ an off-the-shelf MedCPT (Jin et al. 2023) retriever, known for its effectiveness in retrieving relevant documents in biomedical and clinical domains (Section 3.2).Subsequently, we develop a critic language model C to annotate the instruction sets which will contain information for facilitating an autonomous assessment of reflective criteria (Section 3.3).Lastly, we perform training on our generator model M using the instruction sets created with diverse biomedical components (Section 3.4).We depict our processes of data generation, training, and inference in Fig. 2.</p>
<p>Biomedical instruction datasets</p>
<p>List of instruction datasets for biomedical and clinical domains</p>
<p>To train the self-reflection language model (LM), also referred to as the critic LM C, we utilize diverse text triplets (instruction, input, output).Specifically, we collect two offthe-shelf instruction sets [Mol-Instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023)], which include tasks like open-generation, true or false, and multi-choice questions.In addition to the distributed instruction sets, we synthetically generate an additional 18k biomedical and clinical instructions following the Self-Instruct (Wang et al. 2022).In total, we construct 120k biomedical instruction sets addressing diverse tasks: information extraction, question answering, and summarization.For instance, illustrated in Fig. 2, the example instruction was set to classify the given radiology report according to which part of the body is related and answer with the lumbar spine.The statistics of biomedical instruction sets are provided in Table 1.Detailed statistics of our generated instruction sets can be found in Supplementary Appendix SA.</p>
<p>Biomedical retriever</p>
<p>In the fields of biomedical and clinical domains, researchers and doctors addressing challenging issues typically supplement their knowledge with additional information.Similarly, for a language model to solve problems, it needs to retrieve relevant documents as needed.To achieve this, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever (https:// github.com/ncbi/MedCPT),which is contrastively trained on an unprecedented scale of 255M query-article pairs from PubMed search logs.To retrieve relevant documents, we compile data from four sources: PubMed Abstract (https:// pubmed.ncbi.nlm.nih.gov/),PMC Full-text (https://www.ncbi.nlm.nih.gov/pmc/tools/textmining/),Clinical Guidelines [a publicly released subset of 35 733 guideline articles from MEDITRON (Chen et al., 2023) offline to make it computationally effective.The documents are segmented into chunks of 128 words with 32-word overlaps to form evidence following previous works (Wang et al. 2019, Karpukhin et al. 2020).We first retrieve top-k (k ¼ 10, in our case) evidence from each source data (total 4k evidence) and then use the reranking module to obtain the final top-k evidence relevant to the query.Table 2 presents the overall statistics of biomedical corpus and how many documents are indexed.</p>
<p>3.3 Self-reflection language model (critic language model)</p>
<p>Data construction of critic LM C</p>
<p>We collect a total of 120k biomedical instruction sets and randomly sample 5k examples (D s ) to train the critic LM C. We use GPT-4 API Calls to generate reflective tokens r, guiding the critic model C in learning how to predict these tokens.We follow the usage of four types of reflective tokens r employed in Self-RAG, as described in   Note that training the critic LM C signifies it to predict pre-defined reflective tokens given instruction, output, and optionally evidence.We use trained LM C to annotate whole instruction sets and filter out instances when it mispredicts the reflective tokens that are not pre-defined such as [Continue Generation].We provide detailed hyperparameters used to train the critic LM C in Supplementary Appendix SC.</p>
<p>Annotating biomedical instruction sets using critic LM C</p>
<p>After training, the model C predicts four types of reflective tokens: (i) identifying whether a question requires retrieval (RET); (ii) determining if retrieved evidence provides useful information to solve a question (REL); (iii) assessing whether all statements of answers can be supported by evidence (SUP); (iv) evaluating whether all statements of answers are a useful response to the question (USE).For example, in Fig. 2, the model C predicts the retrieval of factual content related to the role of BRCA1 and BRCA2 gene mutation ([Retrieval]).Then, the model predicts that the retrieved evidence provides a fact that BRCA1 and BRCA2 play similar roles in breast cancer and sporadic cancer ([Relevant]).By comparing a statement of the answer and retrieved evidence, the model C predicts that the answer could be supported by evidence ([Fully supported]).Finally, the model C suggests that all statements of answers are useful responses to the question ([Utility: 4]).After annotating each type of reflective token, we aggregate all results to construct a complete instance as above.We provide detailed instructions to annotate the biomedical instruction dataset using the critic LM in Supplementary Appendix SH.</p>
<p>3.4 Domain-specific instruction-tuned language model (generator language model)</p>
<p>Data construction using critic LM C and training generator LM M</p>
<p>We use MedCPT to retrieve top-k evidence following an instruction that necessitates retrieval of biomedical context.</p>
<p>After retrieving relevant documents, we use the critic LM C to predict each reflective token as described in Table 3.Consequently, we preserve 84k filtered instances of biomedical instruction sets annotated with pre-defined reflective tokens, instruction, and output triplets to train generator LM M. We want to point out that the critic LM C is only used to annotate reflective tokens to generate biomedical instruction sets to train generator LM M. We fine-tune these filtered 84k biomedical instructions on the generator model to predict answer with reflective tokens as below, max M E ðx;y;rÞ�D log p M ðy; rjxÞ</p>
<p>(2)</p>
<p>where D stands for filtered instruction sets annotated with pre-defined reflective tokens r.This enhances generalizability in the biomedical and clinical domains preserving the abilities of text generation and self-assessment of its generated explanations with reflective tokens.</p>
<p>2 Inference process of Self-BioRAG</p>
<p>In Fig. 2, we present a MedQA (Jin et al. 2021) example to illustrate our Self-BioRAG inference offline.For instance, the question is inquiring about the diagnosis of a female patient exhibiting symptoms of obesity, acne, and has a history of type 2 diabetes mellitus.The generator model M determines the need to retrieve a relevant document and selects the best evidence from the top-k retrieved documents based on a score S, calculated as the weighted sum of reflective tokens, using the same hyperparameters as Self-RAG,
SðCritiqueÞ ¼ P G2G w G s G ; G ¼ REL [ SUP [ USE s G ¼ pðrÞ P N G i¼1 pðr i Þ
where s G denotes the generation probability of the most desirable reflective token r (e.g.[Fully supported]) for reflective token type G (e.g.SUP) and w G represents the hyperparameter providing weight for s G .We can set the weight w G to adjust our behavior at inference time.For example, to find the most relevant document e related to question x, we can set a weight term REL score higher.Self-BioRAG is tailored to conditionally generate text without any additional training which could need balancing the trade-off between multiple preferences (Touvron et al. 2023, Wu et al. 2023b).</p>
<p>The prioritized evidence includes information on the family history of type 2 diabetes mellitus and the patient's diagnosis of polycystic ovarian syndrome (PCOS).Due to space limitations, we display partial information in the figure; please refer to the complete case in Table 8.Consequently, the generator model M generates the following text: (i) the patient has acne and obesity, typical symptoms of PCOS; (ii) the patient has a family history of type 2 diabetes mellitus, often associated</p>
<p>Baselines</p>
<p>In Table 4, we compare Self-BioRAG with proprietary, open foundation, and open foundation with retrieval-augmented language models.We report the Med-PaLM score as presented in Med-PaLM (Singhal et al. 2022) and the GPT-3.5 and GPT-4-base scores as presented in Nori et al. (2023) to establish the upper bound of benchmark datasets (Row 1-3).</p>
<p>Open foundation models, pre-trained for sequence-tosequence generation with instruction tuning, such as Alpaca (Taori et al. 2023) and Flan-T5 (Chung et al. 2022), are reported (Rows 4 and 5), as well as models fine-tuned on the specific vertical domains (e.g.biomedical and clinical), like PMC-LLaMA (Wu et al. 2023a), Galactica (Taylor et al. 2022), MedAlpaca (Han et al. 2023) ).Therefore, we employ LLaMA2 for the result of retrieval-augmented generation (RAG) and provide the top-10 evidence collected from the biomedical corpus using the MedCPT retriever (Row 11).Due to the length limit of RAG for input, we can only leverage the top 1 evidence in input and few-shot examples.In addition, we report Self-RAG (Asai et al. 2023) using Contriever (Izacard et al. 2022b) fine-tuned on MSMARCO (Bajaj et al. 2016) with the Wikipedia corpus (Row 12).We compare these baselines with our Self-BioRAG framework which is trained with biomedical components.</p>
<p>Training and inference settings</p>
<p>Self-BioRAG is trained with 84k biomedical instruction sets filtered using a trained critic language model (LM).We adopt the Self-RAG critic LM as our base model and fine-tune it with 5k sampled instruction sets annotated by GPT-4 API calls.As training on the Self-RAG generator LM yields better results, we fine-tune our biomedical instruction sets instead of training directly on LLaMA2 (Touvron et al. 2023) or Meditron (Chen et al. 2023).For the retriever, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever, specialized in retrieving documents based on biomedical queries and retrieving up to ten evidence for each input.</p>
<p>For inference, we use vllm (Kwon et al. 2023) to speed up our inference time.Following Self-RAG (Asai et al. 2023), we assign the same weight terms for reflective tokens (e.g.REL, SUP, USE) in decoding.We adopt adaptive retrieval by default which dynamically decides when to retrieve the evidence by predicting a reflective token [Retrieval].we retrieve the top ten evidence from the biomedical corpus processed offline.We provide details of the retrieved percentage of source data used to evaluate biomedical benchmark datasets in Section 5.2.(Singhal et al. 2022).The score of GPT-3.5 and GPT-4-base models are from the following paper (Nori et al. 2023).We use biomedical corpus (e.g.PubMed, PMC, CPG, and Textbook) as evidence during inference on the RAG setting.The best score is highlighted in bold for the parameter size of 7B or less and our 13B model.</p>
<p>i124</p>
<p>Jeong et al.</p>
<p>Results and analysis</p>
<p>5.1 Experimental results</p>
<p>What contributes to the performance improvements in Self-BioRAG?</p>
<p>In Table 4, we compare our Self-BioRAG with open foundation language model (LM) and retrieval augmented generation (RAG).With a parameter size of 7B or less, our Self-BioRAG outperforms other open foundation LMs (Row 4-10) in all three biomedical benchmark datasets (MedQA, MedMCQA, and MMLU-Med).We also compare our model with baselines using retrieval evidence.The RAG pipeline faces two challenges: it struggles to identify crucial evidence and encounters limitations in incorporating numerous pieces of evidence due to constraints on input length.However, our Self-BioRAG outperforms the RAG baseline and can prioritize important evidence via the values of reflective tokens, which is useful for analyzing all the retrieved evidence (Rows 11 and 13).Although Self-RAG is fine-tuned on LLaMA2, we observe that Self-RAG cannot generalize to biomedical benchmark datasets, resulting in a performance drop (Rows 10 and 12).By providing a biomedical critic LM and corpus to train a biomedical generator LM, our Self-BioRAG achieves state-of-the-art performance on 7B parameters in MedQA, MedMCQA, and MMLU-Med datasets.We also provide the 13B performance of our Self-BioRAG model to demonstrate the effectiveness of our framework works in other model parameters (Row 14).We provide the detailed performance of specific MMLU datasets in Supplementary Appendix SF.</p>
<p>In Table 5, we compare our Self-BioRAG with two open foundation LM by measuring n-gram recall performance [Rouge Score (Lin 2004)] and similarity of token embeddings between prediction and answer [BERTScore (Zhang et al. 2019)].We observe that although all foundation models do not generate predictions with the exact same words as the answers (lower Rouge Score), they manage to explain well with words that are as similar as possible (high BERTScore).However, these scores cannot measure whether a model has generated answers with accurate rationale, how much hallucination occurs, how much it includes crucial claims, or whether it has generated answers fluently.We leave an investigation about detailed capacities related to long-text generation for future works.We aim to analyze the step-by-step process through which our Self-BioRAG achieves its state-ofthe-art performance in the following subsection.</p>
<p>Analysis</p>
<p>Which domain-adaptation components show the improvements compared to Self-RAG?</p>
<p>In Table 6, each experiment involves sequentially reducing components in Self-BioRAG.The goal is to identify the factors that significantly contributed to the performance improvement, ultimately leading to the final performance of Self-BioRAG.First, the controllable generation using reflective tokens affects the rationale which leads to predicting an answer (Row 2).Then, we observe that using four biomedical corpora (PubMed, PMC, CPG, and Medical Textbook) to retrieve appropriate evidence shows performance improvement compared to Wikipedia evidence (Row 3).We also use domain-specific MedCPT retriever instead of the Contriever (Izacard et al. 2022b) fine-tuned on MSMARCO (Bajaj et al. 2016) (Row 4).Ultimately, the most effective approach was the collection and processing of biomedical instruction sets to create both a critic language model and a generation language model (Row 5).We recommend readers collect their domainspecific instructions to address corresponding instructions.</p>
<p>In biomedical corpora, what evidence is used to solve open-domain question-answering benchmarks?</p>
<p>In Fig. 3, we compare the ratio of retrieved evidence using the MedCPT (Jin et al. 2023) retriever on four biomedical corpora (PubMed, PMC, CPG, and Medical Textbook).Even though the index sizes of Medical Textbook and CPG are much smaller than PubMed or PMC, retrieved evidences show even distribution.Specifically, our Self-BioRAG only retrieves small portions to solve three datasets [MedQA (12%), MedMCQA (8%), and MMLU-Med (11%)] meaning that these open-domain benchmarks do not require that much evidence than expected.We depict these portions up to 100% in Fig. 3.We observe a trend in which Self-BioRAG retrieves a higher proportion of information from the Medical Textbook, similar to the approach used in solving USMLE-style questions.This is also aligned with previous facts that retrieving documents from Medical Textbook can achieve higher performance in clinical questions (Li et al. 2023, Wang et al. 2023).a We report the Rouge-1 (R1), Rouge-2 (R2), Rouge-L (RL) scores to measure n-gram recall performance and report BERTScore (BS) which computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings.The best scores are highlighted in the bold.(3)</p>
<p>where we set δ hyperparameter as 0.2 for the Adaptive Retrieve experiment setting.Our findings indicate that retrieving relevant documents indeed aids in solving benchmark datasets.In addition, we observed that adaptively retrieving shows comparable performance on average with the Only Retrieve setting.This is attributed to the small portion of retrieved evidence used to answer the questions.While the Only Retrieve setting exhibits a substantial improvement in MedMCQA, it shows a performance drop in MMLU-Medical datasets compared to the No Retrieve setting, indicating its instability.As a result, we recommend readers to use the adaptive retrieval setting.</p>
<p>Distinguishing when to retrieve documents in Self-BioRAG</p>
<p>In Fig. 4, we evaluate the performance of LLaMA2, RAG (LLaMA2 with MedCPT and biomedical corpora), and Self-</p>
<p>BioRAG on examples predicted as [No Retrieval] and</p>
<p>[Retrieval] by Self-BioRAG.To show an overall trend, we use the MedQA dataset here and the rest of the two datasets are in Supplementary Appendix SE.Notably, Self-BioRAG retrieves small portions to solve three biomedical benchmarks.Still, the results demonstrate that Self-BioRAG consistently outperforms other baselines, whether or not retrieved evidence is used.In situations where retrieval is not necessary (left column), Self-BioRAG &gt; RAG � LLaMA2.The overall trend in the retrieved situation (right column) indicates Self-BioRAG &gt; RAG ≥ LLaMA2.Intuitively, we identify that Self-BioRAG distinguishes well on situations to use retrieved evidence or not depending on questions.</p>
<p>Case report of using retrieved evidence</p>
<p>In Table 8, we present an example from the MedQA dataset to illustrate how Self-BioRAG works.For instance, a patient exhibits symptoms of physical appearance changes, acne, and a family history of type 2 diabetes mellitus (T2DM).Self-BioRAG determines the need to retrieve relevant documents containing information on a female diagnosed with polycystic ovarian syndrome (PCOS) and similar symptoms (e.g.T2DM and obesity).Self-BioRAG determines the patient's diagnosis as PCOS by integrating all three: patient's symptoms, retrieved evidence, and parametric knowledge.Throughout the query, evidence, and prediction, we color-code using blue and red to distinguish two categories of related snippets: (i) key information extracted from retrieved evidence and (ii) the model's essential parametric knowledge, both of which are</p>
<p>Conclusion</p>
<p>In this manuscript, we introduce the Self-BioRAG framework, enabling a Self-RAG (Asai et al. 2023) to generalize to biomedical and clinical domains of instructions.This framework enhances the generation capacity, facilitates the retrieval of factual content on demand, and enables self-assessment of generated rationales.Our experimental results cover five open-domain question-answering (QA) datasets widely used in biomedical and clinical domains.In multi-choice QA datasets, Self-BioRAG achieves a 7.2% absolute improvement compared to the stateof-the-art model among the open foundation 7B models.In Long-form QA datasets, Self-BioRAG exhibits notable variations in term usage, despite producing predictions that closely resemble answers.We demonstrate the necessity of domainspecific components, such as retriever, domain-related document corpus, self-reflection model, and generator model, to address domain-related instructions.We provide diverse analyses: Table 8.Case report of Self-BioRAG prediction using evidence in MedQA dataset.a</p>
<p>MedQA Dataset</p>
<p>Query: A 27-year-old woman presents to the office with concerns about her long struggle with her physical appearance since adolescence.She says she has always been "large" and was constantly targeted by her classmates and coworkers for being so.Her main concern at the moment is her acne and unwanted facial hair on her upper lip, for which she often visits a local spa.She has tried numerous diet plans, exercise regimens, and cosmetic products with little to no effect.Recently, she underwent a glucose tolerance test that showed a plasma glucose level of 160 mg/dL (8.9 mmol/L) after 2 hours of a 75 g dose of oral glucose.She has a family history of type 2 diabetes mellitus and a menstrual cycle that occurs every 45 days.Her pulse is 72/min and the blood pressure is 138/80 mm Hg.On physical examination, her height is a Retrieved evidence is written in italics.Blue-colored text comprises segments connected to key information from retrieved evidence, while red-colored text consists of segments tied to the model's parametric knowledge.</p>
<p>Improving medical reasoning i127</p>
<p>(i) self-BioRAG retrieves a larger portion of evidence from Medical Textbook than other corpora to solve USMLE-style questions; (ii) self-BioRAG can distinguish when to retrieve evidence depending on instruction and question; (iii) provided evidence from biomedical corpora genuinely helps supplement scarce knowledge.In future works, we aim to explore generating long-form text in a fine-grained evaluation which could interpret how open foundation models (with or without domain adaptation) generate.</p>
<p>Figure 1 .
1
Figure 1.Comparison between three frameworks: generation using language model (LM), retrieval-augmented generation (RAG) using LM, and our Self-BioRAG.(A) depicts the process of sequence-to-sequence generation of LM. (B) The RAG framework first finds relevant documents from large-scale corpus such as PubMed Central and then provides the answer based on this factual content to address the shortage of scarce knowledge.(C) Initially, our domain-specific instruction-tuned model predicts whether retrieval is necessary.If a query does not require any retrieval of knowledge (factual content), it directly predicts the answer.However, if the query necessitates retrieval knowledge, Self-BioRAG utilizes the domain-specific retriever (MedCPT, in our case) to retrieve relevant documents.After retrieving the top-k evidence, the model selects the most pertinent evidence for the query.Ultimately, our language model is employed to select the best evidence and generate the answer based on the selected evidence and encoded knowledge.</p>
<p>Figure 2 .
2
Figure 2. Overview of our Self-BioRAG process: data construction, training, and inference of Self-Reflection Language Model (critic LM C) and Domainspecific Instruction-tuned Language Model (generator LM M).We construct 120k biomedical instruction sets using two off-the-shelf instruction sets [Mol-Instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023)] and one self-generated biomedical instruction set.We first sample 5k instructions to generate reflective tokens via GPT-4 API calls and then train the critic LM C with these instructions.Using trained critic LM C, we filter out mispredicted reflective tokens, such as [Continue Generation].We preserve 84k instruction sets annotated with pre-defined reflective tokens to train the generator LM M. Note that critic LM C is only used for annotating reflective tokens used to filter instruction sets to train generator LM M. After training, the model M can predict whether or not to use the retrieval method and combine the results of evidence and encoded knowledge to answer the question.We use the MedQA (Jin et al. 2021) test sample to gain a proper understanding of how our Self-BioRAG works.</p>
<p>Figure 3 .
3
Figure3.Ratio of retrieved evidences from each of the four biomedical corpora (PubMed, PMC, CPG, Medical Textbook).The RAG statistics refer to the top-1 evidence usage ratio, while Self-BioRAG selects the most useful evidence from the top-10 retrieved evidence.</p>
<p>Figure 4 .
4
Figure 4. Performance of LLaMA2, RAG, and Self-BioRAG on examples split into [No Retrieval] and [Retrieval] based on Self-BioRAG using the MedQA test dataset.</p>
<p>160 cm (5 ft 3 in) and her weight is 85 kg (187 lb), and she has severe inflammatory acne over the cheeks and forehead and dark coarse hairs on the back.What is the most likely diagnosis of this patient?Option A: Hypothyroidism Option B: Idiopathic hirsutism Option C: Polycystic ovarian syndrome (PCOS) Option D: Ovarian hyperthecosis Evidence: Early Clinical Expressions of Insulin Resistance: The Real Enemy to Look For.Today, a very common clinical scenario is a 17-year-old female with a family history of type 2 diabetes mellitus (T2DM) and hypertension in her mother and two first-degree relatives.Three years ago she was diagnosed with polycystic ovarian syndrome (PCOS).She is overweight with her body weight increasing 10 kg in the last 2 years.Additionally, she has acanthosis nigricans (AN) in the axillae and knuckles.Blood tests were unremarkable with a fasting plasma glucose value of 82 mg/dl and an HbA1c of 5.0%.Prediction: The most likely diagnosis of this patient is Option C: Polycystic ovarian syndrome (PCOS).The patient has the typical symptoms of PCOS, such as acne, hirsutism, and obesity.Additionally, she has a family history of type 2 diabetes mellitus, which is often associated with PCOS.The glucose tolerance test result of 160 mg/dL (8.9 mmol/L) after 2 hours confirms impaired glucose tolerance, which is another characteristic feature of PCOS.</p>
<p>, extracted from 8 sources: CCO, CDC, CMA, ICRC, NICE, SPOR, WHO, and WikiDoc], and English Medical Textbooks [Medical textbooks widely used by medical students and takers of the United States Medical Licensing Examination (USMLE), https://github.com/jind11/MedQA].We encode these data
Improving medical reasoningi121</p>
<p>Table 3
3
. Detailed statistics and prompts used in generating each type of reflective token are provided in Supplementary Appendix SG.Exploring other reflective tokens suitable for specific domains is left for future work.</p>
<p>Table 1 .
1
Statistics of biomedical instruction sets.a
Dataset nameNo. of instances (original ! filtered)Tasks typesMol-instructions (Fang et al. 2023)51 493 ! 38 156Information extraction, question answering, multi-choice questionMedInstruct (Zhang et al. 2023)52 002 ! 36 429Question answering, summarization, text classification, multi-choice questionBiomedical Instructions (Ours)18 854 ! 10 143Text generation, question answering, relation extraction, textclassification, summarizationTotal122 349 ! 84 728Question answering, information extraction, text classifica-tion, summarization, text generation
a We filter instructions using the critic language model C and use it to train the generator language model M.</p>
<p>Table 2 .
2
(Touvron et al. 2023)exed biomedical corpus.aThus,we decide to develop a domain-specific critic LM C using our biomedical instruction sets.We split the sampled instruction sets into train and dev to train and assess the performance of the critic LM C. We train the model using four types of reflective tokens r annotated with GPT-4 API calls.We initialize the critic LM C with a pre-trained language model [here we use LLaMA2(Touvron et al. 2023)] and train it on the sampled dataset D s to maximize the likelihood as below.
DataNo. of documentsNo. of ChunksEmbedding sizePubMed36 533 37769 743 442400 GBPMC1 060 17346 294 271160 GBCPG35 733606 7853.5 GBTextbook18133 8750.7 GBa CPG stands for Clinical Practice Guideline.
CE ðx;y;rÞ�Ds log p C ðrjx; yÞ(1)</p>
<p>Table 3 .
3
Reflective tokens r used in Self-BioRAG.a x, y, and e respectively indicate input, output, and evidence.Specific reflective tokens highlighted in bold are desirable during data construction as they contribute to preserving the existing instruction data possible.
Improving medical reasoningi123TypeInputOutputDefinitionsRETx=x; yfyes, no, continuegDecides when to retrieve using RRELx, efrelevant, irrelevantge provides useful information to solve xSUPx, e, yffully supported, partially supported, no supportgAll of the verification-worthy statement in y is supported by eUSEx, yf5, 4, 3, 2, 1gy is a useful response to x
a</p>
<p>, and Meditron (Chen et al. 2023) (Row 6-9).LLaMA2 (Touvron et al. 2023) demonstrates state-of-the-art performance in open-foundation 7B models in our experiment (Row 10</p>
<p>Table 4 .
4
Experimental results on biomedical benchmark datasets.a
Open-domain biomedicalbenchmark
a We use 3-shot examples as guidelines for language models to address benchmark instances.These examples are chosen from each training dataset using k-nearest-neighbor (Guo et al. 2003).Since the MMLU dataset lacks training data, we employ the same examples detailed in the appendix of MedPALM</p>
<p>Table 5 .
5
Results of Long-form question-answering benchmark.a
ModelLiveQA (R1/R2/RL/BS)MedicationQA (R1/R2/RL/BS)MEDITRON (Chen et al. 2023)5.5/0.0/2.5/77.24.1/0.2/3.3/75.9LLaMA2 (Touvron et al. 2023)8.8/1.9/6.2/78.85.7/1.2/4.4/77.6RAG11.5/2.3/11.1/69.59.8/1.3/4.8/72.9Self-BioRAG (Ours)19.7/3.1/13.4/77.217.6/3.3/13.5/80.2</p>
<p>Table 6 .
6
Effect of each domain-adaptation component.
Experiment DetailMedQA (Acc.)MedMCQA (Acc.)MMLU-Med (Acc.)AverageSelf-BioRAG43.642.153.946.5-Reflective Tokens42.541.951.145.2 (-1.3)-Biomedical Corpora40.740.749.343.6 (-2.9)-MedCPT Retriever39.838.947.642.1 (-4.4)-Biomedical Instruction Sets34.836.446.439.2 (-7.3)Self-BioRAG provide highest score represented in bold.</p>
<p>Table 7 .
7
Effect of adaptive retrieval in Self-BioRAG.aThebest scores are highlighted in the bold.
MethodsMedQA (Acc.)MedMCQA (Acc.)MMLU-Med (Acc.)AverageOnly [No Retrieval]39.741.952.844.8Only [Retrieval]40.147.251.346.2Adaptive Retrieval (Ours)43.642.153.946.5
a "Only [No Retrieval]" refers to not retrieving any evidence, while "Only [Retrieval]" refers to forcing the retrieval of top 10 evidences.</p>
<p>i120Jeong et al.
AcknowledgementsWe thank Gangwoo Kim, Hyeon Hwang, Chanhwi Kim, and Akari Asai for the valuable feedback on our work.Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.FundingThis work was supported in part by the National Research Foundation of Korea [NRF-2023R1A2C3004176, NRF-2022R1C1C1008074], the Ministry of Health &amp; Welfare, Republic of Korea [HR20C0021(3), HR22C1302], the Ministry of Science and ICT (MSIT) [RS-2023-00262002, RS-2022-00155911 (Artificial Intelligence Convergence Innovation Human Resources Development (Kyung Hee University))], and the ICT Creative Consilience program through the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the MSIT [IITP-2024-2020-0-01819].Supplementary dataSupplementary data are available at Bioinformatics online.Conflict of interestNone declared.
Overview of the medical question answering task at trec 2017 liveqa. References Abacha, A B , Eugene A Yuval, P , TREC. 2017</p>
<p>Bridging the gap between consumers' medication questions and trusted answers. A B Abacha, M Yassine, S Mark, MedInfo. 2019</p>
<p>Self-rag: learning to retrieve, generate, and critique through self-reflection. A Asai, W Zeqiu, W Yizhong, arXiv:2310.115112023preprint: not peer reviewed</p>
<p>Ms marco: a human generated machine reading comprehension dataset. P Bajaj, C Daniel, Nick C , arXiv:1611.092682016preprint: not peer reviewed</p>
<p>Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. M Cao, D Yue, Jackie C Kit, C , 2022Association for Computational LinguisticsDublin, Ireland</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Z Chen, H Alenjandro, A R Cano, arXiv:2311.160792023preprint: not peer reviewed</p>
<p>Deep reinforcement learning from human preferences. P F Christiano, L Jan, T B Brown, Advances in Neural Information Processing Systems. Long Beach, CA, USA2017</p>
<p>Mol-instructions: a large-scale biomolecular instruction dataset for large language models. H W Chung, H Le, Shayne L , arXiv:2210.11416arXiv:2306.08018On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE. 2022. 2023. 2003Scaling instruction-finetuned language models. arXiv</p>
<p>Retrieval augmented language model pre-training. K Guu, L Kenton, Zora T , International Conference On Machine Learning. 2020</p>
<p>Medalpaca-an open-source collection of medical conversational ai models and training data. T Han, L C Adams, J M Papaioannou, arXiv:2304.082472023arXivpreprint: not peer reviewed</p>
<p>Few-shot learning with retrieval augmented language models. D Hendrycks, Collin B Steven, B , arXiv:2009.03300arXiv:2208.032992020Measuring massive multitask language understanding. 2022a, preprint: not peer reviewed</p>
<p>Unsupervised dense information retrieval with contrastive learning. G Izacard, Mathilde C Lucas, H , Transactions on Machine Learning Research. 2022b</p>
<p>Survey of hallucination in natural language generation. Z Ji, L Nayeon, Rita F , 2023ACM Computing Surveys</p>
<p>Active retrieval augmented generation. Z Jiang, X Frank, G Luyu, arXiv:2305.069832023preprint: not peer reviewed</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, P Eileen, O Nassim, Appl Sci. 2021</p>
<p>Medcpt: contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Q Jin, W Kim, Q Chen, Bioinformatics. 396512023</p>
<p>Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. M Kang, L Seanie, B Jinheon, arXiv:2305.183952023preprint: not peer reviewed</p>
<p>Dense passage retrieval for open-domain question answering. V Karpukhin, O Barlas, M Sewon, Empirical Methods in Natural Language Processing. 2020</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, L Zhuohan, Z Siyuan, Symposium on Operating Systems Principles. 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, P Ethan, P Aleksandra, Advances in Neural Information Processing Systems. 2020</p>
<p>Meddm: Llm-executable clinical guidance tree for clinical decision-making. B Li, M Tianxin, S Xiaoming, arXiv:2312.024412023preprint: not peer reviewed</p>
<p>Rouge: a package for automatic evaluation of summaries. C-Y Lin, Text Summarization Branches Out. 2004</p>
<p>Generation-augmented retrieval for open-domain question answering. Y Mao, H Pengcheng, L Xiaodong, Association for Computational Linguistics and International Joint Conference on Natural Language Processing. 2021</p>
<p>Training language models to follow instructions with human feedback. H Nori, K Nicholas, M M Scott, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. arXiv. 2023. 2023b. 2022Advances in Neural Information Processing Systems</p>
<p>Medmcqa: a large-scale multisubject multi-choice dataset for medical domain question answering. A Pal, Logesh Kumar, U Malaikannan, S , Conference on Health, Inference, and Learning. 2022</p>
<p>J Schulman, Filip W Prafulla, D , arXiv:1707.06347Proximal policy optimization algorithms. arXiv. 2017preprint: not peer reviewed</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Z Shao, G Yeyun, S Yelong, arXiv:2305.152942023preprint: not peer reviewed</p>
<p>Alpaca: a strong, replicable instruction-following model. K Singhal, A Shekoofeh, T Tao, arXiv:2212.13138Large language models encode clinical knowledge. arXiv. Stanford Center for Research on Foundation Models2022. 2023preprint: not peer reviewed</p>
<p>Self-instruct: aligning language model with self generated instructions. R Taylor, K Marcin, C Guillem, arXiv:2211.09085arXiv:2212.10560A large language model for science. arXiv. Toronto, Canada2022. 2023. 2022Llama 2: Open foundation and finetuned chat models. preprint: not peer reviewed</p>
<p>Augmenting black-box llms with medical textbooks for clinical question answering. Y Wang, M Xueguang, C Wenhu, arXiv:2309.022332023preprint: not peer reviewed</p>
<p>Multi-passage Bert: A globally normalized Bert model for open-domain question answering. Z Wang, N Patrick, M Xiaofei, Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing. Hong Kong, China2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, W Xuezhi, S Dale, Advances in Neural Information Processing Systems. 2022</p>
<p>C Wu, L Weixiong, Z Xiaoman, arXiv:2304.14454Pmc-llama: Further finetuning llama on medical papers. arXiv. 2023a, preprint: not peer reviewed</p>
<p>Fine-grained human feedback gives better rewards for language model training. Z Wu, H Yushi, S Weijia, arXiv:2306.016932023bpreprint: not peer reviewed</p>
<p>Bertscore: evaluating text generation with Bert. T Zhang, K Varsha, W Felix, International Conference on Learning Representations. 2019</p>
<p>Alpacare: instruction-tuned large language models for medical application. X Zhang, T Chenxin, Y Xianjun, arXiv:2310.145582023preprint: not peer reviewed</p>
<p>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Author The, 10.1093/bioinformatics/btae238ISMB2024Improvingmedicalreasoningi129Bioinformatics. 402024Oxford University PressPublished by. This is an Open Access article distributed under the terms of the Creative Commons Attribution License</p>            </div>
        </div>

    </div>
</body>
</html>
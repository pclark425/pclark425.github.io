<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8353 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8353</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8353</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-22ae20f20d0b4e6451ae41cc76e58a9221e90df9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/22ae20f20d0b4e6451ae41cc76e58a9221e90df9" target="_blank">LEACE: Perfect linear concept erasure in closed form</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> LEAst-squares Concept Erasure (LEACE) is introduced, a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms.</p>
                <p><strong>Paper Abstract:</strong> Concept erasure aims to remove specified features from an embedding. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called"concept scrubbing,"which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8353",
    "paper_id": "paper-22ae20f20d0b4e6451ae41cc76e58a9221e90df9",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0071595,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LEACE: Perfect linear concept erasure in closed form</h1>
<p>Nora Belrose ${ }^{1}$ David Schneider-Joseph ${ }^{1}$ Shauli Ravfogel ${ }^{2}$ Ryan Cotterell ${ }^{3}$<br>Edward Raff ${ }^{4}$ Stella Biderman ${ }^{1,4}$<br>${ }^{1}$ EleutherAI ${ }^{2}$ Bar-Ilan University ${ }^{3}$ ETH Zürich ${ }^{4}$ Booz Allen Hamilton<br>{nora, stella}@eleuther.ai david@davidsj.com</p>
<h4>Abstract</h4>
<p>Concept erasure aims to remove specified features from an embedding. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the embedding as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https://github.com/EleutherAI/concept-erasure.</p>
<h2>1 Introduction</h2>
<p>The ability to prevent a machine learning system from using a specified concept is important for fairness and interpretability. Popular notions of fairness require that protected attributes should not causally affect predictions [22, 26], and interpretability research often estimates the causal effect of a concept by attempting to remove it from a model's internal activations [10, 30, 25, 5, 18].
What it means for a model $\mathcal{M}$ to "use" a concept Z is often vague and application-specific, but a necessary condition is that its outputs-and therefore its inputs and hidden states-should have significant mutual information with Z. ${ }^{1}$ Concept erasure leverages this fact to limit $\mathcal{M}$ 's use of Z without finetuning or inspecting its parameters. Instead, we edit the input or hidden states X used by $\mathcal{M}$ to minimize the predictive $\mathcal{V}$-information $I_{\mathcal{V}}(\mathrm{X} \rightarrow \mathrm{Z})$ [43], a tractable lower bound on the mutual information $I(\mathrm{X} ; \mathrm{Z})$ which measures the degree to which classifiers from the family $\mathcal{V}$ can predict Z. Intuitively, if no classifier in $\mathcal{V}$ can outperform a constant function at predicting Z-a condition known as guardedness-then $\mathcal{M}$ can't use Z either, at least if $\mathcal{V}$ is expressive enough relative to $\mathcal{M}$.
In this work, we improve upon existing concept erasure techniques using a theory-driven approach. We focus on the case where $\mathcal{V}$ is the set of linear classifiers, and prove a previously unnoticed equivalence: a classification task is linearly guarded if and only if every class has exactly the same mean feature vector (§ 3). Leveraging this equivalence, we derive a simple necessary and sufficient condition for an affine transformation to produce linearly guarded features. We then identify the unique surgical transformation in this family-the one that minimizes the mean squared distance from the original features with respect to all norms induced by inner products, including the popular Euclidean and Mahalanobis norms. We name it LEAst-squares Concept Erasure (LEACE) (§ 4).
While prior work has focused on preventing linear models from leveraging Z, we aim to erase concepts from deep neural networks as well. Interpretability research has shown that networks</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>can be usefully described as encoding features in linear subspaces [11, 24, 41], suggesting that fundamentally nonlinear methods may not be necessary for successful erasure in DNNs. In light of this, we introduce a simple procedure called concept scrubbing (§ 6), which sequentially applies LEACE to the activations at each layer of a deep network.</p>
<p>We empirically validate our proposals, demonstrating the superiority of LEACE for erasing gender bias from BERT embeddings (§ 5.2), and using concept scrubbing to measure the extent to which large language models use part-of-speech information (§ 6).</p>
<h1>2 Preliminaries</h1>
<p>Consider a $k$-class classification task over jointly defined random vectors X (the input data) and Z (the one-hot labels), with X of finite first moment and taking values in $\mathbb{R}^{d}$, and Z taking values in $\mathcal{Z}={\mathbf{z} \in{0,1}^{k} \mid|\mathbf{z}|_{1}=1}^{2}$ with each $\mathbb{P}(\mathrm{Z}=j)&gt;0$. Let $\eta(\cdot ; \boldsymbol{\theta}): \mathbb{R}^{d} \rightarrow \mathbb{R}^{k}$ be a predictor chosen from a function class $\mathcal{V}={\eta(\cdot ; \boldsymbol{\theta}) \mid \boldsymbol{\theta} \in \Theta}$ (presumed to contain all constant functions) so as to minimize the expectation $\mathbb{E}\left[\mathcal{L}(\eta(\mathrm{X}), \mathrm{Z})\right]$ of some $\mathcal{L}: \mathbb{R}^{k} \times \mathcal{Z} \rightarrow[0, \infty)$ in a class $\mathfrak{L}$ of loss functions.
We borrow the concept of guardedness from Ravfogel et al. [33], who define it in terms of $\mathcal{V}$ information [43]. We opt for a slightly more general definition here, which is equivalent to theirs in the case of cross-entropy loss (see Appendix G).
Definition 2.1 (Guardedness). Let $\mathrm{X}, \mathrm{Z}, \mathcal{V}$, and $\mathfrak{L}$ be as defined above, and let $\chi$ be the set of all random vectors of finite first moment taking values in $\mathbb{R}^{d}$, jointly defined with Z .
We say $\mathrm{X}(\mathcal{V}, \mathfrak{L})$-guards Z if, for all losses $\mathcal{L} \in \mathfrak{L}$, it maximizes the minimum expected loss:</p>
<p>$$
\mathrm{X} \in \underset{\mathrm{X}^{\prime} \in \chi}{\operatorname{argmax}} \inf _{\boldsymbol{\theta} \in \Theta} \mathbb{E}\left[\mathcal{L}\left(\eta\left(\mathrm{X}^{\prime} ; \boldsymbol{\theta}\right), \mathrm{Z}\right)\right]
$$</p>
<p>In other words, its conditional distribution $\mathbb{P}(\mathrm{X} \mid \mathrm{Z}=\cdot)$ is among the worst possible distributions for predicting Z from X using a predictor of the form $\eta(\cdot ; \boldsymbol{\theta}) \in \mathcal{V}$ and a loss function in $\mathfrak{L}$.
Definition 2.2 (Trivially Attainable Loss). The trivially attainable loss for labels Z and loss $\mathcal{L}$ is the lowest possible expected loss available to a constant predictor $\eta(\mathbf{x})=\mathbf{b}$ :</p>
<p>$$
L_{\tau}=\inf _{\mathbf{b} \in \mathbb{R}^{k}} \mathbb{E}[\mathcal{L}(\mathbf{b}, \mathrm{Z})]
$$</p>
<p>We will sometimes write it $L_{\tau}^{(\mathrm{Z}, \mathcal{L})}$ in cases of possible ambiguity. If there is a specific constant predictor actually achieving this loss, we call it the trivial predictor $\eta_{\tau}=\eta_{\tau}^{(\mathrm{Z}, \mathcal{L})}$.
We examine this problem in the important case of loss functions $\mathcal{L}: \mathbb{R}^{k} \times \mathcal{Z} \rightarrow[0, \infty)$ which are convex in the prediction $\eta(\mathbf{x})$, and linear predictors that take the functional form $\eta(\mathbf{x} ; \mathbf{b}, \mathbf{W})=$ $\mathbf{b}+\mathbf{W} \mathbf{x}$, for some bias $\mathbf{b} \in \mathbb{R}^{k}$ and weight matrix $\mathbf{W} \in \mathbb{R}^{k \times d}$.
Definition 2.3 (Linear Guardedness). If $\mathrm{X}(\mathcal{V}, \mathfrak{L})$-guards Z , where $\mathfrak{L}$ is the class of nonnegative loss functions which are convex in their first argument, and $\mathcal{V}$ is the class of linear predictors $\eta(\mathbf{x})=\mathbf{b}+\mathbf{W} \mathbf{x}$, we say that X linearly guards Z .</p>
<h2>3 Theoretical Results</h2>
<p>Our primary theoretical result is that the following conditions are all equivalent:</p>
<ol>
<li>The data X linearly guards the labels Z. (Definition 2.3)</li>
<li>For all convex losses $\mathcal{L}$, the trivially attainable loss is optimal on $(\mathrm{X}, \mathrm{Z})$. (Definition 2.2)</li>
<li>The class-conditional mean vectors $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=i]$ are equal to the unconditional mean $\mathbb{E}[\mathrm{X}]$.</li>
<li>Every component of X has zero covariance with every component of Z .</li>
<li>Every linear classifier evaluated on X exhibits statistical parity w.r.t. Z. (App. C)</li>
</ol>
<p>The equivalence of conditions 1,2 , and 5 is relatively straightforward to show, and the relevant theorems can be found in Appendices B and C. The other equivalences are proven below (cond. $3 \leftrightarrow$ cond. 2 in $\S 3.1$ and $\S 3.2$ ); cond. $3 \leftrightarrow 4$ in $\S 3.3$ ).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1 Equality of Class Centroids Implies Linear Guardedness</h1>
<p>The following result establishes the implication from condition 3 to condition 2.
Theorem 3.1. Suppose $\mathcal{L}$ is convex in the linear prediction $\eta$. Then if each class-conditional mean $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=i]$ is equal to $\mathbb{E}[\mathrm{X}]$, the trivially attainable loss cannot be improved upon.</p>
<p>Proof. Let $\eta(\mathbf{x})=\mathbf{b}+\mathbf{W} \mathbf{x}$ be any linear predictor. By Jensen's inequality, ${ }^{3}$ the loss with $\eta$ evaluated on X is lower bounded by the loss with $\eta$ evaluated on the unconditional mean of the data $\mathbb{E}[\mathrm{X}]$ :</p>
<p>$$
\begin{aligned}
\mathbb{E}[\mathcal{L}(\eta, \mathrm{Z})] &amp; =\mathbb{E}<em _mathrm_Z="\mathrm{Z">{\mathrm{Z}}[\mathbb{E}[\mathcal{L}(\eta, \mathrm{Z}) \mid \mathrm{Z}]] \
&amp; \geq \mathbb{E}</em>)] \
&amp; =\mathbb{E}}}[\mathcal{L}(\mathbb{E}[\eta \mid \mathrm{Z}], \mathrm{Z<em _mathrm_Z="\mathrm{Z">{\mathrm{Z}}[\mathcal{L}(\mathbf{b}+\mathbf{W} \mathbb{E}[\mathrm{X}] \mathrm{Z}], \mathrm{Z})] \
&amp; =\mathbb{E}</em>)]
\end{aligned}
$$}}[\mathcal{L}(\mathbf{b}+\mathbf{W} \mathbb{E}[\mathrm{X}], \mathrm{Z</p>
<p>This in turn is the loss of the constant predictor $\eta^{\prime}(\mathbf{x})=\mathbf{b}+\mathbf{W} \mathbb{E}[\mathrm{X}]$. Since the trivially attainable loss is the best that can be achieved by a constant predictor, and every predictor's loss is lower bounded by that of some constant predictor, we cannot improve upon the trivially attainable loss.</p>
<p>Intuitively, this shows that the classifier's expected loss is lower-bounded by the loss it would receive if each data point were replaced with the centroid of its class. But, if these centroids are all equal, the loss can't be any lower than what we'd get if every data point were replaced with the global mean $\mathbb{E}[\mathrm{X}]$. In that case, the data points are indistinguishable and we can't do better than $\mathbf{W}=\mathbf{0}$.</p>
<h3>3.2 Linear Guardedness Implies Equality of Class Centroids</h3>
<p>We now prove the implication from condition 2 to condition 3 . Condition 2 applies when the trivially attainable loss is optimal for all convex losses, including cross-entropy loss in particular. And if it holds for cross-entropy loss, we now show that condition 3-the class centroids are equal-must follow. First a more general lemma:
Lemma 3.2. Suppose $\mathcal{L}$ has bounded partial derivatives, which when off-category never vanish and do not depend on the category, i.e. $\partial \mathcal{L}\left(\eta, z_{1}\right) / \partial \eta_{i}=\partial \mathcal{L}\left(\eta, z_{2}\right) / \partial \eta_{i} \neq 0$ for all categories $z_{1}, z_{2} \neq i$. If $\mathbb{E}[\mathcal{L}(\eta, \mathrm{Z})]$ is minimized among linear predictors by the constant predictor $\eta(\mathbf{x})=\mathbf{b}^{<em>}+\mathbf{W}^{</em>} \mathbf{x}$ with $\mathbf{W}^{*}=\mathbf{0}$, then each class-conditional mean $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=i]$ is equal to $\mathbb{E}[\mathrm{X}]$.</p>
<p>Proof. The first-order optimality condition on the $i^{\text {th }}$ component of our parameters $\mathbf{b}$ and $\mathbf{W}$ yields the equations:</p>
<p>$$
\mathbb{E}\left[\frac{\partial \mathcal{L}(\eta, \mathrm{Z})}{\partial \eta_{i}} \cdot \frac{\partial \eta_{i}}{\partial b_{i}}\right]=0 \quad \text { and } \quad \mathbb{E}\left[\frac{\partial \mathcal{L}(\eta, \mathrm{Z})}{\partial \eta_{i}} \cdot \frac{\partial \eta_{i}}{\partial \mathbf{W}_{\mathbf{i}}}\right]=\mathbf{0}
$$</p>
<p>where we have used the boundedness of $\mathcal{L}$ 's partial derivative and the finite first moment of $\frac{\partial \eta_{i}}{\partial b_{i}}=1$ and $\frac{\partial \eta_{i}}{\partial \mathbf{W}<em i="i">{\mathbf{i}}}=\mathrm{X}$ to justify (via the Dominated Convergence Theorem) interchanging the derivative with the expectation.
Since $\eta$ is constant over all values of X , and $\frac{\partial \eta</em>=1$, the first equation in (1) reduces to:}}{\partial b_{i}</p>
<p>$$
\mathbb{P}(\mathrm{Z}=i) \frac{\partial \mathcal{L}(\eta, i)}{\partial \eta_{i}}+\mathbb{P}(\mathrm{Z} \neq i) \frac{\partial \mathcal{L}(\eta, \neq i)}{\partial \eta_{i}}=0
$$</p>
<p>where $\frac{\partial \mathcal{L}(\eta, \neq i)}{\partial \eta_{i}}$ is an abuse of notation denoting the off-category partial derivative, emphasizing its independence of the category Z .</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Similarly, the constancy of $\eta$ and the fact that $\frac{\partial \eta_{i}}{\partial \mathbf{W}_{i}}=\mathrm{X}$ reduces the second equation in (1) to:</p>
<p>$$
\mathbb{P}(\mathrm{Z}=i) \frac{\partial \mathcal{L}(\eta, i)}{\partial \eta_{i}} \cdot \mathbb{E}[\mathrm{X}] \mathrm{Z}=i]+\mathbb{P}(\mathrm{Z} \neq i) \frac{\partial \mathcal{L}(\eta, \neq i)}{\partial \eta_{i}} \cdot \mathbb{E}[\mathrm{X}] \mathrm{Z} \neq i]=\mathbf{0}
$$</p>
<p>Solving for $\mathbb{P}(\mathrm{Z}=i) \frac{\partial \mathcal{L}(\eta, i)}{\partial \eta_{i}}$ in (2) and substituting in (3) gives us:</p>
<p>$$
\mathbb{P}(\mathrm{Z} \neq i) \frac{\partial \mathcal{L}(\eta, \neq i)}{\partial \eta_{i}} \cdot\left(\mathbb{E}[\mathrm{X}] \mathrm{Z} \neq i]-\mathbb{E}[\mathrm{X}] \mathrm{Z}=i]\right)=\mathbf{0}
$$</p>
<p>If $\mathbb{P}(\mathrm{Z} \neq i)=0$, then $\mathbb{E}[\mathrm{X}]=\mathbb{E}[\mathrm{X}] \mathrm{Z}=i]$ is trivially true. Otherwise, using the non-vanishingness of the off-category partial derivative $\frac{\partial \mathcal{L}(\eta, \neq i)}{\partial \eta_{i}}$, division yields the equivalence of $\mathbb{E}[\mathrm{X}] \mathrm{Z}=i]$ to $\mathbb{E}[\mathrm{X}] \mathrm{Z} \neq i]$, and hence to the unconditional mean $\mathbb{E}[\mathrm{X}]$.</p>
<p>We now show that Lemma 3.2 applies to the widely used cross entropy loss:
Theorem 3.3. If the class probabilities $\mathbb{P}(\mathrm{Z}=j)$ are all nonzero, and the trivially obtainable loss is optimal when $\mathcal{L}(\eta, z)=-\log \frac{\exp \left(\eta_{z}\right)}{\sum_{i=1}^{k} \exp \left(\eta_{i}\right)}$, then each class has the same mean $\mathbb{E}[\mathrm{X}] \mathrm{Z}=z]$.</p>
<p>Proof. In this case, the trivial predictor $\eta_{r}(\mathrm{Z})<em i="i">{j}=\log (\mathbb{P}(\mathrm{Z}=j))$ exists, achieving the trivially obtainable loss, which we have assumed optimal. Furthermore, $\mathcal{L}$ has on-category partial derivative $\partial \mathcal{L}(\eta, i) / \partial \eta</em>\right) \in(0,1)$, both bounded, so the conditions of Lemma 3.2 apply.}=\exp \left(\eta_{i}\right) / \sum_{j=1}^{k} \exp \left(\eta_{j}\right)-1 \in(-1,0]$, and nonvanishing off-category partial derivative $\partial \mathcal{L}(\eta, \neq i) / \partial \eta_{i}=\exp \left(\eta_{i}\right) / \sum_{j=1}^{k} \exp \left(\eta_{j</p>
<h1>3.3 Linearly Guarded Labels Have Zero Covariance with the Features</h1>
<p>The next theorem establishes the equivalence of conditions 3 and 4.
Theorem 3.4. Let X be a random vector taking values in $\mathbb{R}^{d}$ with finite first moment, and Z a random vector taking values in ${0,1}^{k}$ with one-hot encoding, with each class probability $\mathbb{P}(\mathrm{Z}=j)$ being nonzero. Then the class-conditional means $\mathbb{E}[\mathrm{X}] \mathrm{Z}=j]$ are all equal to the unconditional mean $\mathbb{E}[\mathrm{X}]$ if and only if every component of X has zero covariance with every component of Z , i.e. the cross-covariance matrix $\boldsymbol{\Sigma}<em i="i">{\mathrm{XZ}}$, whose $(i, j)^{\text {th }}$ entry is $\operatorname{Cov}\left(\mathrm{X}</em>\right)$, is the zero matrix.}, \mathrm{Z}_{j</p>
<p>Proof. Since Z is one-hot, we can rewrite the $(i, j)^{\text {th }}$ entry of $\boldsymbol{\Sigma}_{\mathrm{XZ}}$ as:</p>
<p>$$
\mathbb{E}\left[\mathrm{X}<em j="j">{i} \mathrm{Z}</em>}\right]-\mathbb{E}\left[\mathrm{X<em j="j">{i}\right] \mathbb{E}\left[\mathrm{Z}</em>}\right]=\mathbb{P}(\mathrm{Z}=j)\left(\mathbb{E}\left[\mathrm{X<em i="i">{i}\right] \mathrm{Z}=j\right]-\mathbb{E}\left[\mathrm{X}</em>\right])
$$</p>
<p>As $\mathbb{P}(\mathrm{Z}=j)&gt;0$, it follows that $\mathbb{E}\left[\mathrm{X}<em i="i">{i} \mid \mathrm{Z}=j\right]=\mathbb{E}\left[\mathrm{X}</em>}\right]$ if and only if $\operatorname{Cov}\left(\mathrm{X<em j="j">{i}, \mathrm{Z}</em>\right)=0$.
We have thus established the equivalence of the first four conditions stated earlier. See Appendix C for the last one, on statistical parity.</p>
<h2>4 Least-Squares Concept Erasure</h2>
<p>In Section 3 we saw that X linearly guards Z if and only if each component of X has zero covariance with each component of Z . We will now characterize the set of affine transformations $r(\mathbf{x})=\mathbf{P} \boldsymbol{x}+\mathbf{b}$ such that $r(\mathrm{X})$ linearly guards Z .
Theorem 4.1. Let X and Z be random vectors taking values in $\mathbb{R}^{d}$ and $\mathbb{R}^{k}$ respectively, with X of finite first moment. Then given some affine function $r(\boldsymbol{x})=\mathbf{P} \boldsymbol{x}+\mathbf{b}$, the modified random vector $r(\mathrm{X})$ linearly guards Z if and only if the columns of the cross-covariance matrix $\boldsymbol{\Sigma}_{\mathrm{XZ}}$ are contained in the null space of $\mathbf{P}$.</p>
<p>Proof. From Theorem 3.4 we know that $r(\mathrm{X})$ linearly guards Z if and only if $\operatorname{Cov}(r(\mathrm{X}), \mathrm{Z})$ is the zero matrix. By the linearity property of cross-covariance, we have:</p>
<p>$$
\operatorname{Cov}(r(\mathrm{X}), \mathrm{Z})=\operatorname{Cov}(\mathbf{P X}+\mathbf{b}, \mathrm{Z})=\mathbf{P} \operatorname{Cov}(\mathrm{X}, \mathrm{Z})=\mathbf{P} \boldsymbol{\Sigma}_{\mathrm{XZ}}
$$</p>
<p>Therefore, $r(\mathrm{X})$ linearly guards Z if and only if $\operatorname{ker}(\mathbf{P}) \supseteq \operatorname{colsp}\left(\boldsymbol{\Sigma}_{\mathrm{XZ}}\right)$.</p>
<p>Implications for prior work. Notably, the above theorems imply that three previously proposed methods in the literature, Spectral Attribute Removal (SAL) [36], Mean Projection [17], and Fair PCA [20], are guaranteed to achieve linear guardedness given suitable hyperparameters. See Appendix D for further discussion.</p>
<h1>4.1 Derivation of LEACE</h1>
<p>Theorem 4.1 is a very weak condition, which is far from identifying unique values for $\mathbf{P}$ and $\mathbf{b}$. In most applications, however, we'd like to make a "small" edit to X so that useful information contained in X is maximally preserved. We operationalize the notion of a small edit in terms of the mean squared norm $\mathbb{E}|r(\mathrm{X})-\mathrm{X}|<em _mathrm_XX="\mathrm{XX">{\mathrm{M}}^{2}$ defined by some positive-definite inner product $\mathbf{M}$, ${ }^{4}$ which can be thought of as a local quadratic approximation to any measure of divergence between X and $r(\mathrm{X})$ (such as Kullback-Leibler divergence, for example). While we are primarily interested in the Euclidean $(\mathbf{M}=\mathbf{I})$ and Mahalanobis $\left(\mathbf{M}=\boldsymbol{\Sigma}</em>\right)$norms, it will turn out that there is a single erasure function that minimizes all such norms simultaneously. We will see in Section 6 that ensuring edits are small in this sense provides substantial benefit to downstream task performance as compared to other methods which also guard the labels Z .
Below, we derive the optimal eraser under the assumption that X and Z are centered.
Theorem 4.2. Let X and Z be centered random vectors taking values in $\mathbb{R}^{d}$ and $\mathbb{R}^{k}$ respectively, each of finite second moment. Let $\mathbf{M} \in \mathbb{R}^{d \times d}$ be a p.s.d. matrix defining a (possibly degenerate) inner product on $\mathbb{R}^{d}:\langle\mathbf{x}, \mathbf{y}\rangle_{\mathbf{M}}=\mathbf{x}^{T} \mathbf{M y}$. Let $\boldsymbol{\Sigma}}}^{+<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XX}} \in \mathbb{R}^{d \times d}$ be X's covariance matrix, and $\boldsymbol{\Sigma}</em>$. Then the objective}} \in \mathbb{R}^{d \times k}$ be the cross-covariance matrix of X and Z . Let $\mathbf{A}^{+}$denote the Moore-Penrose pseudoinverse of a matrix $\mathbf{A}$, and let $\mathbf{A}^{1 / 2}$ be the p.s.d. square root of a p.s.d. matrix $\mathbf{A</p>
<p>$$
\underset{\mathbf{P} \in \mathbb{R}^{d \times d}}{\operatorname{argmin}} \mathbb{E}\left[|\mathbf{P X}-\mathrm{X}|_{\mathbf{M}}^{2}\right] \quad \text { subject to } \operatorname{Cov}(\mathbf{P X}, \mathrm{Z})=\mathbf{0}
$$</p>
<p>has the following solution:</p>
<p>$$
\mathbf{P}^{*}=\mathbf{I}-\mathbf{W}^{+} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>
$$}}} \mathbf{W</p>
<p>where $\mathbf{W}$ is the whitening transformation $\left(\boldsymbol{\Sigma}<em _mathbf_W="\mathbf{W">{\mathrm{XX}}^{1 / 2}\right)^{+}$and $\mathbf{P}</em>} \boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}}=\left(\mathbf{W} \boldsymbol{\Sigma}</em>}}\right)\left(\mathbf{W} \boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}\right)^{+}$is the orthogonal projection matrix onto $\operatorname{colsp}\left(\mathbf{W} \boldsymbol{\Sigma}</em>\right)$.}</p>
<p>Proof. See Appendices E. 1 and E. 2 for two independent proofs of Theorem 4.2.</p>
<p>The above theorem assumes that the random vectors X and Z are centered, and does not include a bias term. Below we extend our results to the uncentered case, and derive the optimal bias $\mathbf{b}^{<em>}$.
Theorem 4.3. Let X and Z be random vectors taking values in $\mathbb{R}^{d}$ and $\mathbb{R}^{k}$ respectively, each of finite second moment. Define $\mathbf{M}$ and $\mathbf{P}^{</em>}$ as in Theorem 4.2 and $\mathbf{b}^{<em>}=\mathbb{E}[\mathrm{X}]-\mathbf{P}^{</em>} \mathbb{E}[\mathrm{X}]$. Then $\left(\mathbf{P}^{<em>}, \mathbf{b}^{</em>}\right)$ minimizes $\mathbb{E}|\mathbf{P X}+\mathbf{b}-\mathrm{X}|^{2}$, subject to $\operatorname{Cov}(\mathbf{P X}+\mathbf{b}, \mathrm{Z})=\mathbf{0}$.</p>
<p>Proof. Let $\mathbf{P} \in \mathbb{R}^{d \times d}$ and define $\tilde{\mathrm{X}}=\mathrm{X}-\mathbb{E}[\mathrm{X}]$ and $\mathbf{c}=\mathbf{P} \mathbb{E}[\mathrm{X}]+\mathbf{b}-\mathbb{E}[\mathrm{X}]$. Then,</p>
<p>$$
\begin{aligned}
\mathbb{E}|\mathbf{P X}+\mathbf{b}-\mathrm{X}|<em _mathbf_M="\mathbf{M">{\mathbf{M}}^{2} &amp; =\mathbb{E}|(\mathbf{P} \tilde{\mathrm{X}}-\tilde{\mathrm{X}})+\mathbf{c}|</em> \
&amp; =\mathbb{E}|\mathbf{P} \tilde{\mathrm{X}}-\tilde{\mathrm{X}}|}}^{2<em _mathbf_M="\mathbf{M">{\mathbf{M}}^{2}+2 \mathbb{E}[\mathbf{P} \tilde{\mathrm{X}}-\tilde{\mathrm{X}}]^{T} \mathbf{M} \mathbf{c}+\mathbf{c}^{T} \mathbf{M} \mathbf{c} \
&amp; =\mathbb{E}|\mathbf{P} \tilde{\mathrm{X}}-\tilde{\mathrm{X}}|</em>
\end{aligned}
$$}}^{2}+\mathbf{c}^{T} \mathbf{M} \mathbf{c</p>
<p>where we have eliminated the middle term because $\mathbf{P}$ is linear and $\mathbb{E}[\tilde{\mathrm{X}}]=0$. Since $\mathbf{M}$ is p.s.d., our objective is minimized for $\mathbf{c}=\mathbf{0}$, i.e. $\mathbf{b}=\mathbb{E}[\mathrm{X}]-\mathbf{P} \mathbb{E}[\mathrm{X}]$. The problem thus reduces to choosing $\mathbf{P}$ so as to minimize $\mathbb{E}|\mathbf{P} \tilde{\mathrm{X}}-\tilde{\mathrm{X}}|_{\mathbf{M}}^{2}$ subject to $\operatorname{Cov}(\mathbf{P X}+\mathbf{b}, \mathrm{Z})=\operatorname{Cov}(\mathbf{P} \tilde{\mathrm{X}}, \mathrm{Z})=\mathbf{0}$, which Theorem 4.2 shows occurs when $\mathbf{P}=\mathbf{P}^{*}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LEACE projection in 3 steps. First the data is whitened, ensuring equal variance in all directions. It is then orthogonally projected onto $\operatorname{colsp}\left(\mathbf{W} \boldsymbol{\Sigma}_{\mathrm{XZ}}\right)^{\perp}$, guaranteeing linear guardedness. Finally, we unwhiten the data so that its covariance structure mimics the original.</p>
<p>Putting together Theorems 4.2 and 4.3 and rearranging, we arrive at the LEACE formula:</p>
<p>$$
r_{\mathrm{LEACE}}(\boldsymbol{x})=\boldsymbol{x}-\mathbf{W}^{+} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>])
$$}}} \mathbf{W}(\boldsymbol{x}-\mathbb{E}[\mathrm{X</p>
<p>Intuitively, LEACE de-means and whitens $\boldsymbol{x}$, projects onto the subspace responsible for correlations between X and Z , then unwhitens the result. Finally, it subtracts this value from $\boldsymbol{x}$, thereby surgically removing the linearly available information about Z.</p>
<h1>4.2 Oblique Projections are Least-Squares Optimal</h1>
<p>Prior work on linear concept erasure has assumed that erasure functions should be orthogonal projections $[29,32,36]$, appealing to the well-known fact that an orthogonal projection of a point $\boldsymbol{x}$ onto a subspace $U$ yields the nearest point in $U$ to $\boldsymbol{x}$. But even in the case where X is centered, $r_{\text {LEACE }}$ is not an orthogonal projection in general. Orthogonal projection matrices are symmetric, and $\mathbf{I}-\mathbf{W}^{+} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>}}} \mathbf{W}$ is only symmetric in the special case where $\mathbf{P<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>^{}}}$ and $\mathbf{W}$ commute. It is an oblique projection however, since applying $\mathbf{P<em>}$ twice yields the same result as applying it once: $\left(\mathbf{P}^{</em>}\right)^{2}=\mathbf{I}-2 \mathbf{W} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>}}} \mathbf{W}^{+}+\mathbf{W}^{+} \mathbf{P<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>^{}}} \mathbf{W} \mathbf{W<em>} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>^{}}} \mathbf{W}=\mathbf{P</em>}$.
Orthogonal projections are generally not least-squares optimal for concept erasure because the necessary and sufficient condition for linear guardedness, $\mathbf{P} \boldsymbol{\Sigma}<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}=\mathbf{0}$, is a constraint on the nullspace of $\mathbf{P}$, and not on its range. We may freely choose the range of the projection to minimize the mean squared distance, as long as we zero out $\operatorname{colsp}\left(\boldsymbol{\Sigma}</em>\right)$. In Figure 1, an orthogonal projection would map all points onto the the dashed line, thereby preserving less of the variance of the original data than LEACE does (green line). See Appendix F for a concrete example.}</p>
<h3>4.3 Extension to Continuous Z</h3>
<p>While not a focus of this work, it's worth noting that LEACE can also be applied to the setting where Z takes arbitrary values in $\mathbb{R}^{k}$, as long as we restrict ourselves to the ordinary least squares regression loss $\mathcal{L}(\eta, \mathbf{z})=|\eta-\mathbf{z}|_{2}^{2}$. In particular, the proofs of equivalence between conditions 1 and 2 given in Appendix B make no categorical assumption on Z, and the equivalence between the optimality of a zero weight matrix (condition 2) and zero cross-covariance (condition 4) is well known in the OLS setting. We can then apply Theorems 4.2 and 4.3, which also make no categorical assumption, to derive the same optimal affine eraser as in the categorical case.</p>
<h2>5 Evaluation</h2>
<h3>5.1 Intrinsic Evaluation</h3>
<p>Following Ravfogel et al. [31] we evaluate the ability of our method to remove gender information from the last hidden layer of a frozen BERT model. We use the biographies dataset of De-Arteaga et al. [6], composed of short biographies annotated by both binary gender and profession. We embed each biography with the [CLS] embedding in the last layer of BERT, enforce the same-conditional-mean constraint to remove gender information from the [CLS], and then evaluate the performance of the model, after the intervention, on the main task of profession prediction. We</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The correlation between $G A P_{\text {female, } y}^{\text {TPR }}$ and the relative proportion of women in profession $y$, for BERT embedding, before (left; $\mathrm{R}=0.867$ ) and after (right; $\mathrm{R}=0.392$ ) the projection.
compare our intervention with RLACE [31], which uses gradient-based optimization to solve a linear concept-erasure adversarial game.</p>
<p>Concept erasure results. First, we evaluate the ability of logistic regression classifiers to recover the removed information. The results, presented in Fig. 2, show that our method is the only to achieve random accuracy (perfect erasure) with a small edit, although RLACE (but not INLP) comes close. At the same time, our method is around 2 orders of magnitude faster, and does not require gradient-based optimization.</p>
<h1>5.2 Downstream Fairness</h1>
<p>How does our intervention affect the behavior of the model on the main classification task of profession prediction? We fit a logistic regression profession-prediction classifier over the projected [CLS] embeddings.
To measure the bias in a classifier, we follow De-Arteaga et al. [6] and use the TPR-GAP measure, which quantifies the bias in a classifier by considering the difference (GAP) in the true positive rate (TPR) between individuals with different protected attributes (e.g. race or gender). We use the notation $\mathrm{GAP}<em z="z">{z, y}^{\mathrm{TPR}}$ to denote the TPR-gap in some main-class label $y$ (e.g. "nurse" prediction) for some protected group $z$ (e.g. "female"), we also consider $\mathrm{GAP}</em>$, the RMS of the TPR-gap across all professions for a protected group $z$ :}^{\mathrm{TPR}, \mathrm{RMS}</p>
<p>$$
\operatorname{GAP}<em C="C" _in="\in" y="y">{z}^{\mathrm{TPR}, \mathrm{RMS}}=\sqrt{\frac{1}{|C|} \sum</em>
$$}\left(\mathrm{GAP}_{z, y}^{\mathrm{TPR}}\right)^{2}</p>
<p>To calculate the relation between the bias the model exhibits and the bias in the data, we also calculate $\sigma_{(\mathrm{GAP}^{\mathrm{TPR}}, \% \mathrm{Women})}$, the correlation between the TPR gap in a given profession and the percentage of women in that profession.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Gender prediction accuracy after bias-removal projection versus the mean squared distance from the original embedding for INLP, RLACE, and LEACE on BERT embeddings.</p>
<p>Results. The main-task classifier achieves profession-prediction accuracy of $77.3 \%$ on the projected embeddings (compared with $79.3 \%$ over the original embeddings), indicating that the intervention minimally affects the ability to predict the profession of a person from the embedding of their biography. At the same time, the TPR gap drops significantly from 0.198 to 0.084 , indicating a sharp drop in the biased behavior of the profession classifier. Indeed, inspecting the correlation $\sigma_{(\mathrm{GAP}^{\mathrm{TPR}}, \% \mathrm{Women})}$ between the gap (per profession) and the embedding of women in this profession,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Amnesic probing results on bert-base-uncased.
we see that this correlation plummets from 0.867 to 0.392 after erasure. Re-fitting the main-task logistic regression classifier over the projected embeddings yields a slightly higher main-task accuracy of $78.1 \%$, at the price of significantly increasing the TPR gap to $0.158 .^{3}$</p>
<h1>5.3 Revisiting Amnesic Probing</h1>
<p>Elazar et al. [10] have introduced the idea of amnesic probing as a causal intervention that aims to test the importance of a given concept (e.g. part-of-speech tag) to some main task (e.g. language modeling). They applied Iterative Nullspace Projection (INLP) to remove different concepts from the activations of the model, and assessed the degree to which its behavior changed when performing masked language modeling. Since INLP often requires dozens of iterations to completely erase the concept, its usage in this context raises concerns of collateral damage due to magnitude of the intervention and the non-exhaustive nature of INLP removal. Here, we replicate their experiments on the bert-base-uncased model with our interventions.</p>
<p>Experimental setup. We use part-of-speech (POS) tags as our concept of interest. We collect sentences and their coarse POS tags ("Noun", "Verb" etc.; 18 in total) from the English Universal Dependencies dataset [27]. We tokenize the sentences with the BERT tokenizer and map each wordpiece to the POS tag of the word to which it belongs. We collect the unmasked BERT embeddings for each layer, intervene to linearly erase the POS concept from that layer, and continue the forward pass until the last layer, from which we compute the distribution of the MLM over the vocabulary. Note that in each experiment we intervene on a single layer. We quantify the decrease in accuracy following the intervention, as well as the increase in the loss. We compare with a baseline intervention of a random orthogonal projection whose null space has the same rank as the label space (18). For INLP, we perform 20 iterations. This is needed because INLP does not effectively remove the concept; even after 20 iterations, classification accuracy is above majority accuracy. As a result, INLP reduces the rank of the embedding by 360 . By contrast, our method decreases the rank just by 17 .</p>
<p>Results. The results are shown in Fig. 4b. Our intervention only mildly changes BERT LM accuracy and loss until layer 8, with the highest drop recorded in layer 11. INLP, in contrast, shows maximum effect at layer 6 . Since it removes hundreds of dimensions, it is difficult to attribute this effect to the erasure of the concept. These results suggest that the causal effect of the POS concept on the language model is concentrated in layer 11. Interestingly, this stands in contrast with POS linear probing results, which are optimal at earlier layers [38]. As Elazar et al. [10] have noted, probing does not generally correlate with intervention-based analysis techniques.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>6 Concept Scrubbing</h1>
<p>Unfortunately, Elazar et al. [10] were forced to limit their interventions to a single layer due to the limitations of INLP. INLP often requires the deletion of several dozen dimensions before linear guarding is achieved-as demonstrated in Figure 2. Kumar et al. [21] show empirically and theoretically that INLP causes needless "collateral damage" to useful parts of the embedding that are orthogonal to the concept being erased. Because of this collateral damage, it's impossible to apply INLP to multiple layers of a transformer without causing its outputs to collapse into gibberish.</p>
<p>Algorithm 1 Concept scrubbing
Require: Model with $\ell$ layers $f=f_{\ell} \circ \ldots \circ f_{1}$
Require: Design matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$
Require: Label matrix $\mathbf{Z} \in \mathbb{R}^{n \times k}$
Ensure: LEACE parameters for each layer in $f$
$\mathbf{H}<em l="l">{1} \leftarrow \operatorname{Embed}(\mathbf{X})$
$L \leftarrow$ list()
for $l \in 1 \ldots \ell$ do
$\operatorname{Fit}(\mathbf{P}, \mathbf{b})$ on $\mathbf{H}</em>$
$\operatorname{Append}(\mathbf{P}, \mathbf{b})$ to $L$
$\mathbf{H}}$ and $\mathbf{Z<em l="l">{l} \leftarrow \mathbf{P}\left(\mathbf{H}</em>}-\mu_{\mathbf{H<em _mathbf_H="\mathbf{H">{l}}\right)+\mu</em><em l_1="l+1">{l}}$
$\mathbf{H}</em>\right)$
return $L$} \leftarrow f_{l}\left(\mathbf{H}_{l</p>
<p>Instead, we would like to erase all linear information about a concept in the activations at every layer, which we term concept scrubbing. LEACE makes concept scrubbing possible and eminently practical. It causes minimal collateral damage, induces little computational overhead, and the covariance statistics it relies on can be computed in a streaming fashion, without ever storing all the hidden states in memory or on disk.</p>
<p>Algorithm. Any intervention on the model at layer $\ell$ changes the distribution of hidden states at layers $\ell^{\prime}&gt;\ell$. Because of this, the naive approach of independently fitting LEACE parameters $(\mathbf{P}, \mathbf{b})$ for all layers of the clean model, then applying them all at once, may fail to fully erase the target concept. Instead, we fit LEACE parameters sequentially, starting from the first layer and proceeding to the final layer. After we compute $(\mathbf{P}, \mathbf{b})$ for a layer, we immediately use them to scrub the hidden states for that layer, then feed these scrubbed embeddings to the next layer (Algorithm 1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pythia</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Condition</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">160 M</td>
<td style="text-align: center;">1.4 B</td>
<td style="text-align: center;">6.9 B</td>
<td style="text-align: center;">12B</td>
</tr>
<tr>
<td style="text-align: left;">No intervention</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.62</td>
</tr>
<tr>
<td style="text-align: left;">Random erasure</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">LEACE</td>
<td style="text-align: center;">1.73</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">3.57</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: left;">SAL</td>
<td style="text-align: center;">3.24</td>
<td style="text-align: center;">3.26</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">4.69</td>
</tr>
<tr>
<td style="text-align: left;">unigram entropy</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.66</td>
</tr>
</tbody>
</table>
<p>Table 1: Perplexity in autoregressive language models when removing linearly available part-ofspeech information from the input to each transformer layer. Units are bits per UTF-8 byte. The unigram baseline assigns probabilities to tokens based only on their frequency and not on the context.</p>
<h3>6.1 Experimental Details</h3>
<p>Dataset. For each model family, we use a sample from the respective pretraining distribution: the validation split of the Pile [13] for the Pythia models [2], and the RedPajama replication of the LLaMA pretraining corpus for the LLaMA family [39]. sampling a slice of $2^{22}$ tokens for fitting the LEACE parameters and another slice of $2^{22}$ tokens for evaluation. Since neither corpus comes with part-of-speech tags, we use the model from the SpaCy library [19] to automatically generate Universal Dependency tags [23].
Baseline method. We also run concept scrubbing using full-rank SAL [36], which is similar to our method but lacks a bias term and does not adjust for correlations between features (Appendix D).
Architecture. We focus on autoregressive language models. We evaluate our method on EleutherAI's Pythia 160M, 1.4B, 6.9B, and 12B models [2], and Meta's LLaMA 7B, 13B, and 30B [39]. We apply concept erasure to the input of each transformer block, immediately after normalization is applied (LayerNorm or RMSNorm).</p>
<p>Randomized erasure. Almost any intervention on a neural network will cause its performance to degrade to some extent. Following Elazar et al. [10], we isolate the effect of the concept erasure by comparing it to a control condition in which we orthogonally project onto a random linear subspace of the same rank as the cross-covariance matrix. To reduce the variance of our results, we sample a fresh subspace for each minibatch, and erase that subspace at each layer, reporting the cross-entropy loss averaged over subspaces.
Training efficiency. Algorithm 1 avoids redundant computation by caching the layer $i$ hidden states for every data point, then using them to run layer $i+1$. This approach has the downside of requiring a large amount of memory or disk space during training (up to 500 GB in our experiments). It's possible to avoid caching any hidden states and instead recompute them as needed, at the expense of increasing the total compute cost from $O(\ell)$ to $O\left(\ell^{2}\right)$.</p>
<h1>6.2 Results</h1>
<p>We find strong evidence that autoregressive language models heavily rely on linearly encoded part-of-speech information. While erasing a randomly selected subspace has little to no effect on language modeling performance, scrubbing away part-of-speech information induces a large increase in perplexity across all models (Table 1).
The specific numbers, however, depend on the erasure method used: SAL induces significantly larger increases in perplexity for all models we tested. We take this to mean that SAL inflicts more collateral damage on other useful features in the embedding than LEACE does. In other words, interventions made with LEACE are more surgical than those made with prior work; they more closely approximate the ideal of a perfect intervention which only erases the target concept and keeps everything else fixed [40, 15]. If this experiment were conducted with SAL alone, we would have overestimated the causal effect of part-of-speech.</p>
<h2>7 Limitations and Future Work</h2>
<p>Much work remains to be done to validate concept scrubbing. Specifically, we'd like to see experiments that target concepts much narrower than part-of-speech, and use behavioral metrics to determine whether scrubbing changes the network in the ways we'd intuitively expect. If these experiments succeed, an exciting next step would be the incorporation of concept scrubbing into the pretraining and/or finetuning process. This may make it possible to train deep neural networks subject to conceptual constraints. It remains to be seen if gradient-based optimizers will be able to "circumvent" such constraints by encoding protected attributes in completely nonlinear ways.
In this work, we focused exclusively on linear concept erasure due to its simplicity and tractability. Some authors have proposed nonlinear concept erasure techniques based on kernel methods, but have found that erasure functions fit using one kernel do not generalize well to other kernels [32, 36]. We conjecture that it is intractable to nondestructively edit X so as to prevent a general nonlinear adversary from recovering Z , unless the data generating process for X is known in detail. ${ }^{6}$
A major motivation of concept erasure is that it promises to prevent models from using a concept in a post hoc, model-agnostic fashion. But if our concept scrubbing procedure turns out to yield unsatisfactory results in practical use cases, the most promising research direction might then be to improve model-specific techniques, such as those that modify the training procedure [8, 9, 14].</p>
<h2>8 Acknowledgements</h2>
<p>We are grateful to CoreWeave for providing the compute resources used in Section 6. Shauli Ravfogel is grateful to be supported by the Bloomberg Data Science PhD Fellowship.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>[1] UC Berkeley. The Hilbert space of random variables. Lecture Notes Electrical Engineering 126, 2018. URL https://inst.eecs.berkeley.edu/ ee126/sp18/projection.pdf.
[2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.
[3] Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. Advances in Neural Information Processing Systems, 29:4349-4357, 2016. URL https://proceedings.neurips.cc/paper/2016/file/ a486cd07e4ac3d270571622f4f316ec5-Paper.pdf.
[4] Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. Adversarial deep averaging networks for cross-lingual sentiment classification. Transactions of the Association for Computational Linguistics, 6:557-570, 2018. URL https://aclanthology.org/ Q18-1039.
[5] Verna Dankers, Christopher Lucas, and Ivan Titov. Can transformer be too compositional? Analysing idiom processing in neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $3608-3626,2022$.
[6] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* '19, page 120-128, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287572. URL https://doi.org/10.1145/3287560.3287572.
[7] Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. OSCaR: Orthogonal subspace correction and rectification of biases in word embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5034-5050, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.411. URL https://aclanthology.org/2021. emnlp-main. 411 .
[8] Harrison Edwards and Amos Storkey. Censoring representations with an adversary. In International Conference in Learning Representations, pages 1-14, May 2016. URL https: //arxiv.org/abs/1511.05897.
[9] Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 11-21, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1002. URL https://aclanthology.org/D18-1002.
[10] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160-175, 2021. doi: 10.1162/tacl_a_00359. URL https://aclanthology. org/2021.tacl-1.10.
[11] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Hatfield Zac Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.
[12] Thomas S. Ferguson. Mathematical Statistics. Academic Press, Cambridge, MA, 1967.</p>
<p>[13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020 .
[14] Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In International Conference on Machine Learning, pages 7324-7338. PMLR, 2022.
[15] Christopher Grimsley, Elijah Mayfield, and Julia R.S. Bursten. Why attention is not explanation: Surgical intervention and causal reasoning about neural models. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1780-1790, Marseille, France, May 2020. European Language Resources Association. URL https://aclanthology.org/2020. lrec-1.220.
[16] Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckmann, Jérôme Urhausen, and Kevin Verbeek. Obstructing classification via projection. In Filippo Bonchi and Simon J. Puglisi, editors, 46th International Symposium on Mathematical Foundations of Computer Science, Leibniz International Proceedings in Informatics, LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2021.
[17] Pantea Haghighatkhah, Antske Fokkens, Pia Sommerauer, Bettina Speckmann, and Kevin Verbeek. Better hit the nail on the head than beat around the bush: Removing protected attributes with a single projection. pages 8395-8416, December 2022. doi: 10.18653/v1/2022. emnlp-main.575. URL https://aclanthology.org/2022.emnlp-main.575.
[18] Evan Hernandez and Jacob Andreas. The low-dimensional linear geometry of contextualized word representations. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 82-93, 2021.
[19] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrialstrength Natural Language Processing in Python, 2020.
[20] Matthäus Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar. Efficient fair PCA for fair representation learning. In International Conference on Artificial Intelligence and Statistics, pages 5250-5270. PMLR, 2023.
[21] Abhinav Kumar, Chenhao Tan, and Amit Sharma. Probing classifiers are unreliable for concept removal and detection. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 17994-18008. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/725f5e8036cc08adeba4a7c3bcbc6f2c-Paper-Conference.pdf.
[22] Matt J. Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances in Neural Information Processing Systems, 30, 2017.
[23] Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, et al. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92-97, 2013.
[24] Neel Nanda. Actually, Othello-GPT has a linear emergent world model, Mar 2023. URL <a href="https://neelnanda.io/mechanistic-interpretability/othello">https://neelnanda.io/mechanistic-interpretability/othello</a>.
[25] Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova, Matthias Gallé, and Zhenisbek Assylbekov. The rediscovery hypothesis: Language models need to meet linguistics. Journal of Artificial Intelligence Research, 72:1343-1384, 2021.
[26] Hamed Nilforoshan, Johann D. Gaebler, Ravi Shroff, and Sharad Goel. Causal conceptions of fairness and their consequences. In International Conference on Machine Learning, pages 16848-16887. PMLR, 2022.</p>
<p>[27] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4034-4043, 2020.
[28] Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN 978-0-521-89560-6. doi: 10.1017/CBO9780511803161.
[29] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7237-7256, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020.acl-main.647.
[30] Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 194-209, 2021.
[31] Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D Cotterell. Linear adversarial concept erasure. In International Conference on Machine Learning, pages 18400-18421. PMLR, 2022.
[32] Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial concept erasure in kernel space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6034-6055, 2022.
[33] Shauli Ravfogel, Yoav Goldberg, and Ryan Cotterell. Log-linear guardedness and its implications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9413-9431, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.523. URL https://aclanthology.org/2023.acl-long.523.
[34] Bashir Sadeghi and Vishnu Boddeti. On the fundamental trade-offs in learning invariant representations. arXiv preprint arXiv:2109.03386, 2021. URL https://openreview.net/ pdf?id=KOk7mUGsp89.
[35] Bashir Sadeghi, Runyi Yu, and Vishnu Boddeti. On the global optima of kernelized adversarial representation learning. In 2019 IEEE/CVF International Conference on Computer Vision, pages 7970-7978. IEEE, 2019. URL http://hal.cse.msu.edu/assets/pdfs/papers/ 2019-iccv-kernel-adversarial-representation-learning.pdf.
[36] Shun Shao, Yftah Ziser, and Shay B. Cohen. Gold doesn't always glitter: Spectral removal of linear and nonlinear guarded attribute information. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1611-1622, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.eacl-main.118.
[37] Shun Shao, Yftah Ziser, and Shay B. Cohen. Erasure of unaligned attributes from neural representations. Transactions of the Association for Computational Linguistics, 11:488-510, 2023. doi: 10.1162/tacl_a_00558. URL https://aclanthology.org/2023.tacl-1.29.
[38] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593-4601, 2019.
[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models, 2023. URL http://arxiv.org/abs/2302.13971.
[40] James Francis Woodward. Making Things Happen: A Theory of Causal Explanation explanation. Oxford University Press, 2005.</p>
<p>[41] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. 2023.
[42] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In Advances in Neural Information Processing Systems, volume 30, pages 585-596, 2017. URL https://dl.acm.org/doi/10.5555/3294771. 3294827 .
[43] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable information under computational constraints. In 8th International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1eBeyHFDH.
[44] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, page 335-340, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360128. URL https://doi.org/10.1145/3278721.3278779.</p>
<h1>A Additional Related Work</h1>
<p>The problem of linear concept erasure is an instance of the general problem of information removal. Information removal methods generally divide into adversarial methods, which are applied during training, and the post-hoc linear methods considered in this paper. Adversarial methods [8, 42, 4, 9, 44] use a gradient-reversal layer during training to induce embeddings that do not encode the protected attribute. However, Elazar and Goldberg [9] have shown that these methods fail in exhaustively removing all the information associated with the protected attribute: it is often possible to train new adversaries that successfully recover the removed information. Linear methods have been proposed as a tractable alternative, where one identifies a linear subspace that captures the concept of interest, and neutralizes it using algebraic techniques. Different methods have been proposed for the identification of the subspace, e.g. PCA and variants thereof [3, 20], orthogonal-rotation [7], classification-based [29], spectral [36, 37] and adversarial approaches [31].
Few works theoretically characterize the condition of linear guardedness. Haghighatkhah et al. [16] extensively analyzed the problem of preventing linear classification, with the focus on decreasing accuracy. They provide a constructive proof of an optimal intervention for an SVM classifier. Ravfogel et al. [33] have proposed a formal definition of linear guardedness based on $\mathcal{V}$ information, and characterized the fairness implications of guardedness; we show the relations with our definition above. Ravfogel et al. [31] provide an adversarial formulation of the problem, derive a closed-formed solution to certain cases, and propose an SGD-based optimization for others. While they seek an orthogonal projection, we empirically showed that their solution is very close to ours. Sadeghi et al. [35] and Sadeghi and Boddeti [34] both study an adversarial formulation of concept erasure for linear regression, and they trade-off with main-task performance. In contrast to Ravfogel et al. [31], they consider a general linear adversary, i.e. not necessarily a projection matrix. Closest to our work are Kleindessner et al. [20], Haghighatkhah et al. [17], Shao et al. [36]. As we showed above (§ 4), those methods do achieve the goal of linear guardedness though they are unable to prove this fact. At the same time, they are not optimal in terms of damage to the original embedding space.</p>
<h2>B Equivalence of Guardedness with the Optimality of Constant Predictors</h2>
<p>The following two theorems establish the equivalence of conditions 1 and 2 (indeed, they do so in the general setting, with no assumption of convex loss or linear predictors).
Theorem B.1. Suppose $\mathrm{X}(\mathcal{V}, \mathfrak{L})$-guards Z . Then for every loss $\mathcal{L} \in \mathfrak{L}$, the corresponding trivially attainable loss $L_{\tau}^{(\mathrm{Z}, \mathcal{L})}$ cannot be improved upon by any predictor $\eta(\cdot ; \boldsymbol{\theta}) \in \mathcal{V}$, i.e. $L_{\tau}=\inf _{\boldsymbol{\theta}} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z})]$.</p>
<p>Proof. Consider the null random vector $\mathrm{X}^{\prime}(\omega)=\mathbf{0}$. Since all predictors are constant on $\mathrm{X}^{\prime}$, and the trivially attainable loss gives the best available expected loss among constant predictors, we must have:</p>
<p>$$
L_{\tau}=\inf _{\boldsymbol{\theta}} \mathbb{E}\left[\mathcal{L}\left(\eta\left(\mathrm{X}^{\prime} ; \boldsymbol{\theta}\right), \mathrm{Z}\right)\right]
$$</p>
<p>The right side of equation (4) is the best possible loss achievable by a function $\eta(\cdot ; \boldsymbol{\theta})$ on the joint distribution of $\left(\mathrm{X}^{\prime}, \mathrm{Z}\right)$, which by the definition of guardedness is upper bounded by the best possible loss achievable on the joint distribution of $(\mathrm{X}, \mathrm{Z})$ :</p>
<p>$$
\inf <em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}} \mathbb{E}\left[\mathcal{L}\left(\eta\left(\mathrm{X}^{\prime} ; \boldsymbol{\theta}\right), \mathrm{Z}\right)\right] \leq \inf </em>)]
$$}} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z</p>
<p>Combining equations (4) and (5), and the fact that all constant functions exist in our function class $\mathcal{V}={\eta(\cdot ; \boldsymbol{\theta})}$, we arrive at our desired result:</p>
<p>$$
L_{\tau}=\inf _{\boldsymbol{\theta}} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z})]
$$</p>
<p>Theorem B.2. Suppose that for every loss $\mathcal{L} \in \mathfrak{L}$, the corresponding trivially attainable loss $L_{\tau}^{(\mathrm{Z}, \mathcal{L})}$ cannot be improved upon by any predictor $\eta(\cdot ; \boldsymbol{\theta}) \in \mathcal{V}$, i.e. $L_{\tau}=\inf _{\boldsymbol{\theta}} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z})]$. Then X $(\mathcal{V}, \mathfrak{L})$-guards Z .</p>
<p>Proof. Let $\mathrm{X}^{\prime}: \Omega \rightarrow \mathbb{R}^{d}$ be any other random data vector with finite first moment.
Since all constant predictors exist in our predictor class $\mathcal{V}={\eta(\cdot ; \boldsymbol{\theta})}$, the best loss achievable on $\left(\mathrm{X}^{\prime}, \mathrm{Z}\right)$ by functions in $\mathcal{V}$ must be at least as good as the trivially attainable loss (the best loss available by such constant predictors):</p>
<p>$$
\inf <em _tau="\tau">{\boldsymbol{\theta}} \mathbb{E}\left[\mathcal{L}\left(\eta\left(\mathrm{X}^{\prime} ; \boldsymbol{\theta}\right), \mathrm{Z}\right)\right] \leq L</em>
$$</p>
<p>By assumption, the trivially attainable loss cannot be improved upon over $(\mathrm{X}, \mathrm{Z})$ by predictors in $\mathcal{V}$ :</p>
<p>$$
L_{\tau}=\inf _{\boldsymbol{\theta}} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z})]
$$</p>
<p>Since our choice of $\mathrm{X}^{\prime}$ was arbitrary, this shows that X maximizes the minimal achievable loss, so X $(\mathcal{V}, \mathfrak{L})$-guards Z .</p>
<h1>C Linear Guardedness is Equivalent to Linear Statistical Parity</h1>
<p>To measure the effect of linear guardedness on main-task classifiers, we use the following minimal definition of "fairness" with respect to an attribute, adapted from Edwards and Storkey [8].
Definition C. 1 (Statistical Parity). Let X and Z be defined as above, and let $f$ be a function with domain $\mathbb{R}^{d}$. Then $f$ exhibits statistical parity with respect to Z when evaluated on X if</p>
<p>$$
\forall z \in \mathcal{Z}: \mathbb{E}[f(\mathrm{X}) \mid \mathrm{Z}=z]=\mathbb{E}[f(\mathrm{X})]
$$</p>
<p>We now prove the equivalence of conditions 3 and 5 .
Theorem C.2. Let X and Z be defined as above. Then every linear predictor $f(\mathbf{x})=\mathbf{b}+\mathbf{W} \mathbf{x}$ exhibits statistical parity w.r.t. Z when evaluated on X if and only if each class-conditional mean $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=z]$ is equal to $\mathbb{E}[\mathrm{X}]$.</p>
<p>Proof. Suppose each class-conditional mean $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=z]$ is equal to $\mathbb{E}[\mathrm{X}]$. Then by the linearity of expectation, we have for all $z \in \mathcal{Z}$ :</p>
<p>$$
\mathbb{E}[f(\mathrm{X}) \mid \mathrm{Z}=z]=\mathbb{E}[\mathbf{W X}+\mathbf{b} \mid \mathrm{Z}=z]=\mathbf{W} \mathbb{E}[\mathrm{X} \mid \mathrm{Z}=z]+\mathbf{b}=\mathbf{W} \mathbb{E}[\mathrm{X}]+\mathbf{b}=\mathbb{E}[f(\mathrm{X})]
$$</p>
<p>This matches the definition of statistical parity provided in Definition C.1.
Conversely, suppose every linear predictor $f(\mathbf{x})=\mathbf{b}+\mathbf{W} \mathbf{x}$ exhibits statistical parity w.r.t. Z when evaluated on X . Then this holds for the identity function $\operatorname{id}(\mathbf{x})=\mathbf{x}$, and thus for all $z \in \mathcal{Z}$ :</p>
<p>$$
\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=z]=\mathbb{E}[\mathrm{id}(\mathrm{X}) \mid \mathrm{Z}=z]=\mathbb{E}[\mathrm{id}(\mathrm{X})]=\mathbb{E}[\mathrm{X}]
$$</p>
<h2>D Implications for Prior Work</h2>
<p>In this section we discuss the implications of Theorem 4.1, which characterizes the necessary and sufficient conditions for an affine erasure function to yield a perfectly linearly guarded dataset, for methods proposed in prior work.
Spectral Attribute RemovaL (SAL) [36] uses the top $n$ left singular vectors of $\boldsymbol{\Sigma}<em _mathrm_SAL="\mathrm{SAL">{\mathrm{XZ}}$ to construct an orthogonal projection matrix $\mathbf{Q}</em>}}=\mathbf{I}-\mathbf{U<em _cdot="\cdot" n="n">{\cdot n} \mathbf{U}</em>}^{T}$ which is then applied to X . Notably, while $n$ is presented as a free parameter in their method, all of their experiments involve binary classification problems where Z is a one-hot vector, and $n$ is set to a value no greater than 2 . We'll call the version of SAL where $n=\operatorname{rank}\left(\boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}\right)$, "full-rank SAL." Since these left singular vectors are an orthonormal basis for $\boldsymbol{\Sigma}</em>$ 's column space, Theorem 4.1 implies that full-rank SAL guarantees linear guardedness.
Mean Projection (MP) [17] orthogonally projects X onto the orthogonal complement of the span of the difference in class centroids $\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=1]-\mathbb{E}[\mathrm{X} \mid \mathrm{Z}=0]$, where Z is assumed to be binary. Since the centroids are equal after the projection, this method guarantees linear guardedness by Theorem 3.1. In fact, by Theorem 3.4, MP is mathematically equivalent to SAL when Z is a one-dimensional random vector taking one of two possible values.}</p>
<h1>E Derivation of LEACE</h1>
<p>Theorem 4.2. Let X and Z be centered random vectors taking values in $\mathbb{R}^{d}$ and $\mathbb{R}^{k}$ respectively, each of finite second moment. Let $\mathbf{M} \in \mathbb{R}^{d \times d}$ be a p.s.d. matrix defining a (possibly degenerate) inner product on $\mathbb{R}^{d}: \langle\mathbf{x}, \mathbf{y}\rangle_{\mathbf{M}}=\mathbf{x}^{T} \mathbf{M y}$. Let $\boldsymbol{\Sigma}<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XX}} \in \mathbb{R}^{d \times d}$ be X 's covariance matrix, and $\boldsymbol{\Sigma}</em>$. Then the objective}} \in \mathbb{R}^{d \times k}$ be the cross-covariance matrix of X and Z . Let $\mathbf{A}^{+}$denote the Moore-Penrose pseudoinverse of a matrix $\mathbf{A}$, and let $\mathbf{A}^{1 / 2}$ be the p.s.d. square root of a p.s.d. matrix $\mathbf{A</p>
<p>$$
\underset{\mathbf{P} \in \mathbb{R}^{d \times d}}{\operatorname{argmin}} \mathbb{E}\left[|\mathbf{P X}-\mathrm{X}|_{\mathbf{M}}^{2}\right] \quad \text { subject to } \operatorname{Cov}(\mathbf{P X}, \mathrm{Z})=\mathbf{0}
$$</p>
<p>has the following solution:</p>
<p>$$
\mathbf{P}^{*}=\mathbf{I}-\mathbf{W}^{+} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>
$$}}} \mathbf{W</p>
<p>where $\mathbf{W}$ is the whitening transformation $\left(\boldsymbol{\Sigma}<em _mathbf_W="\mathbf{W">{\mathrm{XX}}^{1 / 2}\right)^{+}$and $\mathbf{P}</em>} \boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}}=\left(\mathbf{W} \boldsymbol{\Sigma}</em>}}\right)\left(\mathbf{W} \boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}\right)^{+}$is the orthogonal projection matrix onto $\operatorname{colsp}\left(\mathbf{W} \boldsymbol{\Sigma}</em>\right)$.}</p>
<p>Below are two independent proofs of Theorem 4.2.</p>
<h2>E. 1 Algebraic Proof</h2>
<p>Proof. We shall first show that, in any orthonormal basis, ${ }^{\dagger}$ each row $\mathbf{P}<em i="i">{\mathbf{i}}$ constitutes an independent optimization problem, and then select a basis in which we can easily show that the corresponding component $\mathrm{X}</em>)}$ of X can be almost surely decomposed into a linear combination of mutually uncorrelated components in the whitened random vector $\mathbf{W X}$, some of which correlate with Z and some of which do not. The solution $(\mathbf{P X<em _mathbf_M="\mathbf{M">{i}$ is then that same linear combination, restricted to those components which do not correlate with Z .
Consider first an orthonormal basis diagonalizing the inner product $\mathbf{M}$, so that $\langle\mathbf{x}, \mathbf{y}\rangle</em>$ as a separate optimization problem,}}=$ $\sum_{i=1}^{d} \alpha_{i} x_{i} y_{i}$ for fixed $\alpha_{1}, \ldots, \alpha_{d} \geq 0$. This allows us to treat each row $\mathbf{P}_{\mathbf{i}} \in \mathbb{R}^{d}$ of $\mathbf{P</p>
<p>$$
\underset{\mathbf{P}<em i="i">{\mathbf{i}} \in \mathbb{R}^{d}}{\operatorname{argmin}} \mathbb{E}\left[\alpha</em>}\left(\mathbf{P<em i="i">{\mathbf{i}}^{T} \mathrm{X}-\mathrm{X}</em>
$$}\right)^{2}\right] \quad \text { subject to } \operatorname{Cov}\left(\mathbf{P}_{\mathbf{i}}^{T} \mathrm{X}, \mathrm{Z}\right)=\mathbf{0</p>
<p>at which point the weights $\alpha_{i}$ of each subproblem become irrelevant, and our objective may as well be Euclidean, allowing us to view each row as an independent optimization problem not just in this basis, but from any convenient one.
So now let $\ell=\operatorname{rank}\left(\boldsymbol{\Sigma}<em X="X" _mathbf_W="\mathbf{W">{\mathrm{XZ}}\right)=\operatorname{rank}\left(\boldsymbol{\Sigma}</em>}, \mathrm{Z}}\right)$ and $m=\operatorname{rank}\left(\boldsymbol{\Sigma<em X="X" _mathbf_W="\mathbf{W">{\mathrm{XX}}\right)=\operatorname{rank}\left(\boldsymbol{\Sigma}</em>$ has nonzero covariance with Z ).
Any component of X can be (almost surely) written as a fixed linear combination of the nontrivial components of its whitening WX:}, \mathbf{W X}}\right)$, and consider a (new) orthonormal basis whose first $m$ coordinates span the column (and row) space of $\mathbf{W}$ (i.e. the subspace of $\mathbb{R}^{d}$ in which X and WX have nonzero variance), and whose first $\ell \leq m$ coordinates span the column space of $\boldsymbol{\Sigma}_{\mathbf{W X}, \mathrm{Z}}$ (i.e. the subspace of $\mathbb{R}^{d}$ in which $\mathbf{W X</p>
<p>$$
\mathrm{X}<em i="i">{i}=\left(\mathbf{W}^{+} \mathbf{W} \mathrm{X}\right)</em>
$$}=\sum_{j=1}^{m} W_{i j}^{+}(\mathbf{W} \mathrm{X})_{j</p>
<p>Meanwhile, any component of $\mathbf{P X}$ can be (always) written as a fixed linear combination of the nontrivial components of $\mathbf{W X}$ and the almost surely zero components of X :</p>
<p>$$
(\mathbf{P X})<em j="1">{i}=\sum</em>)}^{m} A_{i j}(\mathbf{W X<em j="m+1">{j}+\sum</em>
$$}^{d} B_{i j} \mathrm{X}_{j</p>
<p>i.e. $\mathbf{P}=\mathbf{A W}+\mathbf{B V}$, where $\mathbf{V}=\mathbf{I}-\mathbf{W}^{+} \mathbf{W}$ is the orthogonal projection onto X's almost surely zero components.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The $i^{\text {th }}$ sub-objective is then:</p>
<p>$$
\mathbb{E}\left(\mathbf{P}<em i="i">{\mathbf{i}}^{T} \mathrm{X}-\mathrm{X}</em>)}\right)^{2}=\mathbb{E}\left[\sum_{j=1}^{m}\left(A_{i j}-W_{i j}^{+}\right)(\mathbf{W X<em j="1">{j}\right]^{2}=\sum</em>
$$}^{m}\left(A_{i j}-W_{i j}^{+}\right)^{2</p>
<p>where we have safely ignored the almost surely zero terms $B_{i j} \mathrm{X}<em X="X" _mathbf_W="\mathbf{W">{j}(j&gt;m)$, and used the fact that the first $m$ components of $\mathbf{W X}$ have identity covariance matrix.
$\mathbf{P X}$ is almost surely equal to $\mathbf{A W X}$, so our constraint $\operatorname{Cov}(\mathbf{P X}, \mathrm{Z})=\mathbf{0}$ is equivalent to $\mathbf{A} \boldsymbol{\Sigma}</em>}, \mathrm{Z}}=$ $\operatorname{Cov}(\mathbf{A W X}, \mathrm{Z})=\mathbf{0}$, i.e. $A_{i j}=0$ when $j \leq \ell$, since the first $\ell$ components are those for which $\mathbf{W X}$ correlates with Z. Subject to this, the objective is minimized for $A_{i j}=W_{i j}^{+}$when $j&gt;\ell$, i.e. $\mathbf{A}=\mathbf{W}^{+}\left(\mathbf{I}-\mathbf{P<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>\right)$.
The particular choice $\mathbf{B}=\mathbf{I}$ gives our solution $\mathbf{P}^{*}=\mathbf{I}-\mathbf{W}^{+} \mathbf{P}}}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>$, leaving the non-varying components of X intact (see Fig. 1 for a visualization).}}} \mathbf{W</p>
<p>The solution is unique except for columns corresponding to the components of X with zero variance, and rows corresponding to the zero-weighted components of the (pseudo) inner product $\mathbf{M}$.</p>
<h1>E. 2 Covector Proof</h1>
<p>Proof. We assume without loss of generality that vectors in $\mathbb{R}^{d}$ are represented in a basis diagonalizing the inner product $\mathbf{M}$, so that $\langle\mathbf{x}, \mathbf{y}\rangle_{\mathbf{M}}=\sum_{i=1}^{d} m_{i} x_{i} y_{i}$ for fixed $m_{1}, \ldots, m_{d} \geq 0$. This allows us to treat each row $\mathbf{P}_{\mathbf{i}} \in \mathbb{R}^{d}$ of $\mathbf{P}$ as a separate optimization problem,</p>
<p>$$
\underset{\mathbf{P}<em i="i">{\mathbf{i}} \in \mathbb{R}^{d}}{\operatorname{argmin}} \mathbb{E}\left[m</em>}\left(\mathbf{P<em i="i">{\mathbf{i}}^{T} \mathrm{X}-\mathrm{X}</em>
$$}\right)^{2}\right] \quad \text { subject to } \operatorname{Cov}\left(\mathbf{P}_{\mathbf{i}}^{T} \mathrm{X}, \mathrm{Z}\right)=\mathbf{0</p>
<p>Our objective only depends on $\mathbf{P}<em _mathbf_i="\mathbf{i">{\mathbf{i}}$ through its effect on the scalar random variable $\xi=\mathbf{P}</em>}}{ }^{T} \mathrm{X}$. All random variables ${ }^{8}$ of the form $\zeta=\mathbf{u<em _zeta="\zeta">{\zeta}^{T} \mathrm{X}$ for some covector $\mathbf{u}</em>}^{T} \in \mathbb{R}^{d}$ form a vector space $U$, which we equip with the covariance inner product $\langle\xi, \zeta\rangle_{\mathrm{Cov}}=\operatorname{Cov}(\xi, \zeta)=\mathbb{E}[\xi \zeta]=\mathbf{u<em _mathrm_XX="\mathrm{XX">{\xi}^{T} \boldsymbol{\Sigma}</em>}} \mathbf{u<em _xi="\xi">{\zeta}$.
By the linearity of covariance, the elements of $U$ uncorrelated with Z form a subspace $Z^{\perp} \subseteq U$. Note also that $\xi \in Z^{\perp}$ if and only if $\xi$ 's covector $\mathbf{u}</em>}^{T}$ satisfies $\operatorname{Cov}\left(\mathbf{u<em _xi="\xi">{\xi}^{T} \mathrm{X}, \mathrm{Z}\right)=\mathbf{u}</em>}^{T} \boldsymbol{\Sigma<em k="k">{\mathrm{XZ}}=\mathbf{0}</em>}$, and that these covectors themselves form the subspace $\operatorname{colsp}\left(\boldsymbol{\Sigma<em i="i">{\mathrm{XZ}}\right)^{\perp}$ of $\mathbb{R}^{d}$.
Our objective now reduces to finding a covector $\mathbf{P}</em>}^{T}$ that defines the orthogonal projection of $\mathrm{X<em _xi="\xi">{i}$ onto $Z^{\perp}$. The difficulty is that orthogonality of elements in $U$ is not equivalent to orthogonality of the corresponding covectors. We can fix this by changing the basis in which covectors are represented. Since $\mathrm{X} \in \operatorname{colsp}(\mathbf{W})$ a.s., we can write any element of $U$ as a linear form in $\mathbf{W X}$ rather than X by applying the change-of-basis $\mathbf{u}</em>}^{\prime}=\mathbf{W}^{+} \mathbf{u<em _xi="\xi">{\xi}$ to every covector: $\xi=\left(\mathbf{u}</em>}^{\prime}\right)^{T} \mathbf{W X}=\mathbf{u<em i="i">{\xi}^{T} \mathbf{W}^{\perp} \mathbf{W} \mathbf{X}$ a.s.
In this new basis, which is orthonormal under our covariance inner product, each component of X is written $\mathrm{X}</em>$}=\left(\mathbf{W}^{+}\right)_{i}^{T} \mathbf{W X}$ and the inner product of any two elements of $U$ is simply the Euclidean inner product of the corresponding covectors: ${ }^{9</p>
<p>$$
\langle\xi, \zeta\rangle_{\mathrm{Cov}}=\operatorname{Cov}\left(\mathbf{u}<em _zeta="\zeta">{\xi}^{\prime T} \mathbf{W X}, \mathbf{u}</em>}^{\prime T} \mathbf{W X}\right)=\mathbf{u<em _mathrm_XX="\mathrm{XX">{\xi}^{\prime T} \mathbf{W} \boldsymbol{\Sigma}</em>}} \mathbf{W} \mathbf{u<em _xi="\xi">{\zeta}^{\prime}=\mathbf{u}</em>
$$}^{\prime T} \mathbf{u}_{\zeta}^{\prime</p>
<p>Since the two inner products are now equivalent, and $Z^{\perp}$ is precisely those random variables with covector $\mathbf{u}^{\prime} \in \operatorname{colsp}\left(\mathbf{W} \boldsymbol{\Sigma}<em i="i">{\mathrm{XZ}}\right)^{\perp}$, the orthogonal projection of $\mathrm{X}</em>\right)}$ onto $Z^{\perp}$ is also an orthogonal projection of its covector $\left(\mathbf{W}^{+<em _mathrm_XZ="\mathrm{XZ">{i}^{T}$ onto $\operatorname{colsp}\left(\mathbf{W} \boldsymbol{\Sigma}</em>$ :}}\right)^{\perp</p>
<p>$$
\hat{\mathrm{X}}<em i="i">{i}=\left(\mathbf{W}^{+}\right)</em>}^{T}\left(\mathbf{I}-\mathbf{P<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>)
$$}}}\right)(\mathbf{W X</p>
<p>Putting all the components of X together, we have our final solution,</p>
<p>$$
\hat{\mathrm{X}}=\left(\mathbf{I}-\mathbf{W}^{+} \mathbf{P}<em _mathrm_XZ="\mathrm{XZ">{\mathbf{W} \boldsymbol{\Sigma}</em>
$$}}} \mathbf{W}\right) \mathrm{X</p>
<p>which is almost surely equivalent to Eq. 6 , but keeps the non-varying components of X intact.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>F The Optimality of Oblique Projections</h1>
<p>As noted in subsection 4.2, the optimal affine erasure function $r(\mathbf{x})=\mathbf{b}+\mathbf{P x}$ does not in general use an orthogonal projection for the matrix $\mathbf{P}$. A simple example illustrates why. Let $d=2, k=1$ so that X takes values in $\mathbb{R}^{2}$ and Z takes values in $\mathbb{R}$, with the first feature $\mathrm{X}<em 2="2">{1}$ and the label Z each independently and uniformly distributed in ${-1,+1}$, and the second feature $\mathrm{X}</em>}$ simply equal to the sum $\mathrm{X<em 1="1">{2}=\mathrm{X}</em>)$ pairs:}+\mathrm{Z}$. A dataset reflecting such a distribution has four $(\mathbf{x}, \mathbf{y</p>
<p>$$
\left([1,2]^{T}, 1\right), \quad\left([1,0]^{T},-1\right), \quad\left([-1,0]^{T}, 1\right), \quad\left([-1,-2]^{T},-1\right)
$$</p>
<p>In this case, all of the information X has about Z resides in $\mathrm{X}_{2}$, so the minimally disruptive orthogonal projection which guards Z will nullify that component:</p>
<p>$$
\mathbf{P}_{\text {ortho }}=\left[\begin{array}{ll}
1 &amp; 0 \
0 &amp; 0
\end{array}\right]
$$</p>
<p>On the other hand, $\mathrm{X}<em 2="2">{1}$ contains some information about $\mathrm{X}</em>$ while preserving full concept erasure:}$ (despite having no information about Z ), allowing a partial reconstruction of $\mathrm{X}_{2</p>
<p>$$
\mathbf{P}_{\text {oblique }}=\left[\begin{array}{ll}
1 &amp; 0 \
1 &amp; 0
\end{array}\right]
$$</p>
<p>Both methods fully erase the ability to predict Z from the data, however a simple calculation shows the second, oblique method to perform better as measured by mean squared edit distance:</p>
<p>$$
\mathbb{E}\left|\mathbf{P}<em _oblique="{oblique" _text="\text">{\text {ortho }} \mathrm{X}-\mathrm{X}\right|^{2}=2, \quad \mathbb{E}\left|\mathbf{P}</em>=1
$$}} \mathrm{X}-\mathrm{X}\right|^{2</p>
<h2>G Equivalence of Guardedness Definitions</h2>
<p>Xu et al. [43] define the conditional $\mathcal{V}$-entropy of Z given X as the lowest achievable cross-entropy loss predicting Z with a function of X in the predictor class $\mathcal{V}$. In our notation:</p>
<p>$$
H_{\mathcal{V}}(\mathrm{Z} \mid \mathrm{X})=\inf _{\theta \in \Theta} \mathbb{E}[\mathcal{L}(\eta(\mathrm{X} ; \boldsymbol{\theta}), \mathrm{Z})]
$$</p>
<p>where $\mathcal{L}(\eta, z)=-\log \frac{\exp \left(\eta_{z}\right)}{\sum_{i=1}^{k} \exp \left(\eta_{i}\right)}$ is the cross-entropy loss function.
They then define the (unconditional) $\mathcal{V}$-entropy $H_{\mathcal{V}}(\mathrm{Z})=H_{\mathcal{V}}(\mathrm{Z} \mid \mathbf{0})$ to be the lowest achievable cross-entropy loss in the case of a constantly null random data variable. This is exactly our trivially attainable loss $L_{\tau}$ (Definition 2.2).
Finally, they define the $\mathcal{V}$-information from X to Z as the reduction in $\mathcal{V}$-entropy as compared to using such a null random data variable:</p>
<p>$$
I_{\mathcal{V}}(\mathrm{X} \rightarrow \mathrm{Z})=H_{\mathcal{V}}(\mathrm{Z})-H_{\mathcal{V}}(\mathrm{Z} \mid \mathrm{X})
$$</p>
<p>Using these notions, Ravfogel et al. [33] say that X is $\epsilon$-guarded with respect to $\mathcal{V}$ if $I_{\mathcal{V}}(\mathrm{X} \rightarrow \mathrm{Z})&lt;\epsilon$.
In Appendix B, we showed the equivalence of guardedness (as we have defined it in Definition 2.1) to the optimality of the trivially attainable loss. That is, $\mathrm{X}(\mathcal{V}, \mathfrak{L})$-guards Z when $H_{\mathcal{V}}(\mathrm{Z} \mid \mathrm{X})=L_{\tau}=$ $H_{\mathcal{V}}(\mathrm{Z})$, in the case where $\mathfrak{L}$ is the singleton class consisting solely of the cross-entropy loss function. In the language of [33], X is $\epsilon$-guarded with respect to $\mathcal{V}$ for all $\epsilon&gt;0$.</p>
<h2>H Constraining Norm Growth</h2>
<p>In early concept scrubbing experiments (Sec. 6), we found that at specific layers in some models, concept scrubbing with LEACE would cause the norm of the embedding to diverge, leading to NaN outputs. By contrast, SAL never caused divergence, even though it causes a larger disruption to model performance on average (Table 1). This is because SAL uses an orthogonal projection $\mathbf{Q}$, whose eigenvalues are thus all in ${0,1}$, so the norm of the hidden state can never increase after erasure, while LEACE's oblique projection matrix $\mathbf{P}$ does generally have singular values greater than 1 . To combine the superior average-case MSE of LEACE with the stability of SAL, we adopt a simple regularization heuristic. After constructing $\mathbf{P}$, we analytically compute the trace of the covariance matrix of the</p>
<p>hidden states after applying $\mathbf{P}$. If $\operatorname{tr}\left(\mathbf{P} \boldsymbol{\Sigma}<em _mathrm_XX="\mathrm{XX">{\mathrm{XX}} \mathbf{P}^{\mathbf{T}}\right)&gt;\operatorname{tr}\left(\boldsymbol{\Sigma}</em>}}\right)$, we solve a quadratic equation to find the convex combination $\mathbf{P}^{\prime}=\alpha \mathbf{P}+(1-\alpha) \mathbf{Q}$ such that $\operatorname{tr}\left(\boldsymbol{\Sigma<em _mathrm_XX="\mathrm{XX">{\mathrm{XX}}\right)=\operatorname{tr}\left(\mathbf{P}^{\prime} \boldsymbol{\Sigma}</em>$. We find this solves the divergence issue in practice.}}\left(\mathbf{P}^{\prime}\right)^{\mathbf{T}}\right)$. By Theorem 4.1, the set of matrices which ensure linear guardedness is convex, ${ }^{10}$ so $\mathbf{P}^{\prime}$ is guaranteed to be in the feasible set. Furthermore, since our mean squared error objective is convex, $\mathbf{P}^{\prime}$ is guaranteed to have no worse MSE than $\mathbf{Q</p>
<h1>I Oracle LEACE</h1>
<p>The concept erasure method derived in Section 4 does not require access to concept labels at inference time. That is, we can fit an erasure function on a labeled training dataset, then apply the function to unlabeled datapoints. If we have oracle access to the label $\boldsymbol{z}$ for each $\boldsymbol{x}$, we can achieve an even more surgical edit. In Theorem I. 1 below, we derive Oracle LEACE, a closed-form formula for the the nearest $\mathrm{X}^{\prime}$ to any X such that $\operatorname{Cov}\left(\mathrm{X}^{\prime}, \mathrm{Z}\right)=\mathbf{0}$.
Like in Sec. 4, the resulting $\mathrm{X}<em i="i">{\text {LEACE }}^{\prime}$ is "nearest" to X with respect to all p.s.d. inner products $\mathbf{a}^{T} \mathbf{M b}$ defined on $\mathbb{R}^{d}$ simultaneously. This is because, by expressing X in a basis that diagonalizes M , we can decompose the problem into $d$ independent subproblems, one for each component of $\mathrm{X}</em>$, but in an abstract vector space of real-valued random variables. For geometric intuition, see Figure 5.
Prior work has noted that computing an orthogonal projection in a random variable Hilbert space is equivalent to solving an ordinary least squares regression problem [1]. Our theorem is a natural extension of this work: we find that $\mathrm{X}}$. Each subproblem can then be viewed as an orthogonal projection, not in $\mathbb{R}^{d<em _mathcal_H="\mathcal{H">{\text {LEACE }}^{\prime}$ is equal to the OLS residual from regressing X on Z , plus a constant shift needed to ensure that erasing Z does not change the mean of X .
Theorem I. 1 (Oracle Concept Erasure). Let $\mathcal{H}$ be the Hilbert space of square-integrable real-valued random variables equipped with the inner product $\langle\xi, \zeta\rangle</em>$, the objective}}:=\mathbb{E}[\xi \zeta]$. Let $(\mathrm{X}, \mathrm{Z})$ be random vectors in $\mathcal{H}^{d}$ and $\mathcal{H}^{k}$ respectively. Then for every p.s.d. inner product $\langle\mathbf{a}, \mathbf{b}\rangle_{\mathbf{M}}=\mathbf{a}^{T} \mathbf{M b}$ on $\mathbb{R}^{d</p>
<p>$$
\underset{\mathrm{X}^{\prime} \in \mathcal{H}^{d}}{\operatorname{argmin}} \mathbb{E}\left|\mathrm{X}^{\prime}-\mathrm{X}\right|_{\mathbf{M}}^{2} \quad \text { subject to } \operatorname{Cov}\left(\mathrm{X}^{\prime}, \mathrm{Z}\right)=\mathbf{0}
$$</p>
<p>is minimized by the (appropriately shifted) ordinary least squares residuals from regressing X on Z :</p>
<p>$$
\mathrm{X}<em _mathrm_XZ="\mathrm{XZ">{\mathrm{LEACE}}^{\prime}=\mathrm{X}-\boldsymbol{\Sigma}</em>])
$$}} \boldsymbol{\Sigma}_{\mathrm{ZZ}}^{+}(\mathrm{Z}-\mathbb{E}[\mathrm{Z</p>
<p>Proof. Assume w.l.o.g. that X and $\mathrm{X}^{\prime}$ are represented in a basis diagonalizing M , so we may write</p>
<p>$$
\mathbb{E}\left|\mathrm{X}^{\prime}-\mathrm{X}\right|<em i="1">{\mathbf{M}}^{2}=\sum</em>}^{d} m_{i} \mathbb{E}\left[\left(\mathrm{X<em i="i">{i}^{\prime}-\mathrm{X}</em>\right]
$$}\right)^{2</p>
<p>where $m_{1}, \ldots, m_{d} \geq 0$ are eigenvalues of $\mathbf{M}$. Crucially, each term in this sum is independent from the others, allowing us to decompose the primal problem into $d$ separate subproblems of the form $\left|\mathrm{X}<em i="i">{i}^{\prime}-\mathrm{X}</em>\right)$.}\right|_{\mathcal{H}}^{2}$, one for each component $i$ of $\left(\mathrm{X}, \mathrm{X}^{\prime</p>
<p>Factoring out constants. Now consider the subspace $\mathcal{C}=\operatorname{span}(1) \subset \mathcal{H}$ consisting of all constant (i.e. zero variance) random variables. Orthogonally decomposing $\mathrm{X}<em i="i">{i}$ along $\mathcal{C}$ yields $\mathrm{X}</em>}=\tilde{\mathrm{X}<em i="i">{i}+\mu</em>}$, where $\mu_{i}=\mathbb{E}\left[\mathrm{X<em i="i">{i}\right] \in \mathcal{C}$ and $\tilde{\mathrm{X}}</em>]}=\mathrm{X}-\mathbb{E}[\mathrm{X<em i="i">{i} \in \mathcal{C}^{\perp}$, and likewise for $\mathrm{X}</em>$. Our objective is now}^{\prime</p>
<p>$$
\left|\mathrm{X}<em i="i">{i}^{\prime}-\mathrm{X}</em>\right|<em i="i">{\mathcal{H}}^{2}=\left|\mu</em>\right|}^{\prime}-\mu_{i<em i="i">{\mathcal{H}}^{2}+\left|\tilde{\mathrm{X}}</em>}^{\prime}-\tilde{\mathrm{X}<em _mathcal_H="\mathcal{H">{i}\right|</em>
$$}}^{2</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Since $\mu_{i}^{\prime}$ and $\mu_{i}$ are orthogonal to $\tilde{\mathrm{X}}<em i="i">{i}^{\prime}$ and $\tilde{\mathrm{X}}</em>}$, and the constraint $\operatorname{Cov}\left(\mathrm{X}^{\prime}, \mathrm{Z}\right)=\mathbf{0}$ is invariant to constant shifts, we can optimize the two terms in Eq. 7 independently. The first term is trivial: it is minimized when $\mu_{i}^{\prime}=\mu_{i}$, and hence $\mathrm{X<em i="i">{i}^{\prime}=\tilde{\mathrm{X}}</em>\right]$.}^{\prime}+\mathbb{E}\left[\mathrm{X}_{i</p>
<p>Orthogonal projection. We can now rewrite the zero covariance condition as an orthogonality constraint on $\tilde{\mathrm{X}}_{i}$. Specifically, for every $i \in{1 \ldots d}$ we have</p>
<p>$$
\underset{\tilde{\mathrm{X}}<em i="i">{i}^{\prime} \in \mathcal{H}}{\operatorname{argmin}}\left|\tilde{\mathrm{X}}</em>}^{\prime}-\tilde{\mathrm{X}<em _mathcal_H="\mathcal{H">{i}\right|</em>}}^{2} \quad \text { s.t. } \forall j \in{1 \ldots k}:\left\langle\tilde{\mathrm{X}<em j="j">{i}^{\prime}, \tilde{\mathrm{Z}}</em>=0
$$}\right\rangle_{\mathcal{H}</p>
<p>where $\tilde{\mathrm{Z}}=\mathrm{Z}-\mathbb{E}[\mathrm{Z}]$. In other words, we seek the nearest $\tilde{\mathrm{X}}<em i="i">{i}^{\prime}$ to $\tilde{\mathrm{X}}</em>}$ orthogonal to $\mathcal{Z}=$ $\operatorname{span}\left(\left{\tilde{\mathrm{Z}<em k="k">{1}, \ldots, \tilde{\mathrm{Z}}</em>$ :}\right}\right)$, which is simply the orthogonal projection of $\tilde{\mathrm{X}}_{i}$ onto $\mathcal{Z}^{\perp}$. This in turn is equal to the ordinary least squares residual from regressing $\tilde{\mathrm{X}}$ on $\tilde{\mathrm{Z}</p>
<p>$$
\tilde{\mathrm{X}}<em i="i">{i}^{\prime}=\tilde{\mathrm{X}}</em>}-\operatorname{proj}\left(\tilde{\mathrm{X}<em i="i">{i}, \mathcal{Z}\right)=\mathrm{X}</em>}-\left(\boldsymbol{\Sigma<em i="i">{\mathrm{XZ}}\right)</em>} \boldsymbol{\Sigma<em i="i">{\mathrm{ZZ}}^{+}(\mathrm{Z}-\mathbb{E}[\mathrm{Z}])-\mathbb{E}\left[\mathrm{X}</em>\right]
$$</p>
<p>Putting it all together. Plugging Eq. 9 into $\mathrm{X}<em i="i">{i}^{\prime}=\tilde{\mathrm{X}}</em>\right]$ and combining all components into vector form yields}^{\prime}+\mathbb{E}\left[\mathrm{X}_{i</p>
<p>$$
\mathrm{X}<em _mathrm_XZ="\mathrm{XZ">{\mathrm{LEACE}}^{\prime}=\mathrm{X}-\boldsymbol{\Sigma}</em>])
$$}} \boldsymbol{\Sigma}_{\mathrm{ZZ}}^{+}(\mathrm{Z}-\mathbb{E}[\mathrm{Z</p>
<p>which completes the proof.</p>
<h1>J Notation Key</h1>
<p>$\mathcal{Z} \quad$ The space of one-hot labels $\left{\left(z_{1}, \ldots z_{k}\right) \in{0,1}^{k} \mid \sum_{j=1}^{k} z_{j}=1\right}\right}$ (treated interchangeably with the integers ${1, \ldots, k}$ when convenient).
$\mathrm{X}, \mathrm{Z} \quad$ Integrable (i.e. finite first moment) random vectors taking values in $\mathbb{R}^{d}$ and $\mathbb{R}^{k}$ respectively (or their realized values inside an expectation, e.g. in $\mathbb{E}[f(\mathrm{X})]$ ). Z is sometimes restricted to the one-hot labels $\mathcal{Z}$, in which case we assume each $\mathbb{P}(\mathrm{Z}=j)&gt;0$.
$\mathrm{X}<em j="j">{i}, \mathrm{Z}</em>$ components thereof, themselves scalar random variables (or their realized values inside an expectation).
$\xi, \zeta \quad$ Scalar random variables taking values in $\mathbb{R}$.
$\eta$ A predictor function $\mathbb{R}^{d} \rightarrow \mathcal{Z}$ (or its value $\eta(\mathrm{X})$ when inside an expectation).
$\mathcal{V} \quad$ A space of predictor functions $\left{\eta(\cdot ; \boldsymbol{\theta}): \mathbb{R}^{d} \rightarrow \mathbb{R}^{k} \mid \boldsymbol{\theta} \in \Theta\right}$, parameterized by $\boldsymbol{\theta}$ and containing all constant functions.
$\mathfrak{L} \quad$ A space of loss functions $\left{\mathcal{L}: \mathbb{R}^{k} \times \mathcal{Z} \rightarrow[0, \infty)\right}$.
$r$ An erasure function $\mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$, hopefully making a minimal edit to $X$ that eliminates the ability to predict labels Z with predictors in $\mathcal{V}$.
A A matrix with entries in $\mathbb{R}$.
$A_{i j} \quad$ The entry thereof at the $i^{\text {th }}$ row and $j^{\text {th }}$ column.
$\mathbf{A}^{+} \quad$ The Moore-Penrose pseudoinverse of $\mathbf{A}$.
v A column vector with entries in $\mathbb{R}$.
$v_{i} \quad$ The $i^{\text {th }}$ component thereof.} \quad$ The $i^{\text {th }}$ and $j^{\text {th }</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ In fact, it is a subspace of $\mathbb{R}^{d \times d}$. For any matrices $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{d \times d}$ such that $\mathbf{A} \boldsymbol{\Sigma}<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}=\mathbf{0}$ and $\mathbf{B} \boldsymbol{\Sigma}</em>}}=\mathbf{0}$, we have by linearity $(\alpha \mathbf{A}+\beta \mathbf{B}) \boldsymbol{\Sigma<em _mathrm_XZ="\mathrm{XZ">{\mathrm{XZ}}=\alpha \mathbf{A} \boldsymbol{\Sigma}</em>$ for any scalars $\alpha$ and $\beta$.&#160;}}+\beta \mathbf{B} \boldsymbol{\Sigma}_{\mathrm{XZ}}=\alpha \mathbf{0}+\beta \mathbf{0}=\mathbf{0<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
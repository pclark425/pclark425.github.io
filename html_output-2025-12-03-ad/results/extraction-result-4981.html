<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-a38e0f993e4805ba8a9beae4c275c91ffcec01df</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df" target="_blank">Program Synthesis with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, and the semantic grounding of these models is explored by fine-tuning them to predict the results of program execution.</p>
                <p><strong>Paper Abstract:</strong> This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature sampling (multiple samples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temperature-based stochastic decoding with multiple sampled completions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generating many diverse model completions via temperature-controlled sampling to produce multiple candidate programs/answers, then checking them (e.g., with unit tests) and accepting any that pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (up to 137B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Left-to-right decoder-only Transformer models pretrained on web/doc/dialog/Wikipedia corpora with sizes from 244M to 137B non-embedding parameters; experiments highlight the 137B model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Temperature sampling (multi-sample ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate many completions using non-deterministic sampling with a temperature parameter (e.g., T=0.5) to induce diverse reasoning/programming outputs; evaluate many samples (80 in many experiments) and accept any sample that passes provided tests.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP (Mostly Basic Programming Problems); MathQA-Python</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MBPP: 974 short Python programming tasks with natural-language descriptions and test asserts. MathQA-Python: translated MathQA problems into Python programs. Evaluated via executing generated code against test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using temperature sampling and 80 samples, the largest 137B model solved ~59.6% of MBPP problems (measured as 'any sample solves the task'); fraction-of-samples solving tasks was much lower (e.g., ~16.8% for 3 prompt examples, Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-sample greedy/low-temperature decoding performs better with only one evaluation allowed (greedy better for single-shot), but multi-sample higher-temperature strategies solve more tasks within modest budgets (e.g., with >1 samples); beam search performed worse than all temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling many diverse completions is an effective strategy for program synthesis: the 'any-sample' metric shows high gains (137B solves ~60% of MBPP with 80 samples). Higher temperatures lead to better scaling as the number of samples increases, while lower temperatures (more greedy) are better for single-sample budgets. Ensemble of many sampled outputs is a practical way to realize diverse reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although many tasks are solved by some sample, most solved tasks are only solved by a small fraction of samples (often 1–2 out of 80), indicating fragility. Diverse sampling does not guarantee correct semantics beyond provided tests (adversarial tests reveal ~12% of apparently-correct solutions would fail).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-seed ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembling across different few-shot prompt example selections (prompt-seed ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use multiple distinct few-shot prompt example sets (different random seeds selecting which held-out examples are in the prompt) and count a task as solved if any prompt-seed yields a correct sample.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (137B primary focus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family of decoder-only Transformer models; few-shot prompts include several example problems and their solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompt-seed ensembling (multiple few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate outputs using different few-shot example choices (different prompt seeds) to produce diverse inductive biases/ reasoning paths; combine results by accepting a problem as solved if any seed produces a correct sample.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP (Mostly Basic Programming Problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short Python programming tasks evaluated by executing generated code on held-out tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Single best prompt seed for 137B achieved ~59.6% solved; ensembling across multiple prompt seeds increased the solved fraction to ~66.4% (absolute improvement ~6.8 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single prompt-seed few-shot performance: ~59.6% (137B).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Changing the specific few-shot examples in the prompt yields notably different outcome sets; ensembling across seeds reliably increases coverage of solvable tasks. Thus, diversity in prompt examples is a simple, effective way to improve overall problem coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Prompt quality is highly variable: some seeds perform much worse than the best seed. Ensembling helps but even combined seeds leave many tasks unsolved; dependency on prompt examples remains a major source of variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Beam search as a deterministic decoding strategy that explores top-k high-probability continuations, tested as an alternative to stochastic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (various sizes, experiments reported for 137B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer models; beam search implemented as standard k-best deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Beam search (deterministic decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Explore multiple high-probability continuations via beam search to produce candidate outputs that are typically similar and mode-seeking compared to stochastic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthesis of short Python programs from natural-language descriptions, evaluated by executing generated programs on test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Empirically performed extremely poorly relative to temperature sampling settings; beam search produced many looping or repetitive code outputs and was worse than any temperature setting tested (no numeric percent given, described qualitatively as worst).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Temperature-based stochastic sampling (multiple samples) substantially outperformed beam search across budgets; low-temperature greedy sampling can outperform beam search for single-sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Beam search is not an effective decoding strategy for program synthesis in these large LMs — it tends to produce repetitive/looping outputs and underperforms stochastic temperature sampling approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Beam search consistently underperformed; no evidence in this paper that deterministic beam search ever outperforms diverse sampling for MBPP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-sample greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy decoding (single most-likely completion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use greedy decoding (temperature 0.0) to obtain a single, most-likely model completion and evaluate it; represents a similar, non-diverse reasoning method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (137B primary; other sizes reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer LM; greedy decoding uses the argmax token at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Greedy single-shot decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Deterministically decode the most likely token at each timestep (temperature=0.0) producing a single chain of reasoning/program; no sampling-induced diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP synthesis; execution prediction experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MBPP: program synthesis judged by execution against test cases. Execution task: predict outputs of ground-truth programs for given inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For synthesis, fraction-of-samples (single-sample reliability) is much lower than 'any-sample' metrics — e.g., for the 137B model with 3 prompt examples the fraction-of-samples solving tasks was ~16.8% (Table 1). For execution prediction (few-shot greedy), accuracy did not exceed ~29% across prompt configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Multi-sample temperature sampling (diverse) achieved much higher 'any-sample' coverage (e.g., ~59.6% solved with 80 samples) and outperformed single-shot greedy when multiple samples are allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Greedy single-shot decoding is less reliable than strategies that generate diverse outputs and check them; greedy can be better than high-temperature sampling when only one sample is allowed, but multi-sample stochastic decoding yields higher overall coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Greedy decoding sometimes performs best when budget strictly limited to one sample (low-temperature best for single evaluation), but overall it yields lower coverage and robustness than diverse multi-sample methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4981.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4981.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-model dialog (natural language feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-loop natural-language dialog for iterative correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human provides iterative short natural-language hints/corrections (up to four one-sentence interventions) to guide the model to repair or clarify code outputs, and the model updates its outputs accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (137B focus; experiments on same family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive Transformer models prompted with few-shot collaborative-dialog examples; model produces code and accepts human text hints to generate revised code.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Human-guided iterative refinement (dialog interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The human inspects model output and provides concise natural-language corrections or hints (e.g., point out missing import, clarify under-specified requirement); the model then synthesizes revised code incorporating the hint. Multiple dialog turns produce varied reasoning/solution attempts guided by human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP (edited subset; human-model collaboration on 50 problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interactive program synthesis where the human and model collaborate via natural-language dialog to produce a correct Python function passing provided tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Without human feedback, few-shot single-sample solve rate ~30%; with up to four human dialog interventions, solve rate increased to over ~65% (one intervention -> ~55%). Human feedback allowed solving 10 problems the model could not solve alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Increasing sampling (e.g., 5 samples) yields some baseline improvement, but the paper notes a one-sentence human correction was worth more than increasing samples five-fold (i.e., human feedback outperformed simply increasing model sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human natural-language feedback substantially improves synthesis success rates (roughly halves error rate with up to four interventions). Human guidance can clarify under-specified prompts and fix small context/import/identifier errors in one turn; it is more effective than modest increases in sampling budget.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Diminishing returns with more interventions; the model sometimes loses track of earlier context or prior code versions, struggling to revert or re-incorporate previous pieces of code across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4981.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4981.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt assert count sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to number of input-output asserts/examples in few-shot prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Varying how many test-assert examples are included in the prompt (0–3) to examine impact on synthesis performance; a measure of whether models rely on similar repeated constraints versus diverse exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only Transformer LM (137B reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer LMs with few-shot prompts containing function name and optionally up to three assert test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Varying number of prompt asserts/examples</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Change the number of explicit test-assert examples included in each prompt (0, 1, 2, 3) to measure effect on model's synthesis behavior: more asserts could encourage the model to conform to test semantics or encourage overfitting to asserts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MBPP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short Python function synthesis evaluated by executing against test asserts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For 137B: 0 asserts -> 43.2% problems solved; 1 assert -> 55.2%; 2 asserts -> 59.0% (best here); 3 asserts -> 58.4%. Fraction-of-samples solving task peaked at 3 asserts (~16.77%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared across assert counts as reported above; inclusion of asserts had modest effects and did not strongly reduce overfitting overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including 1–2 assert examples in the prompt substantially increases solved rate versus none; however, adding the third assert gave only marginal additional benefit, suggesting models are not simply overfitting to test cases. The model is mostly not using asserts to fully reason about semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Occasional overfitting to asserts occurs: in rare cases the model hard-codes outputs that trivially pass asserts (e.g., return True only for asserted input), showing asserts can sometimes be abused rather than leading to correct general semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program Synthesis with Large Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4981",
    "paper_id": "paper-a38e0f993e4805ba8a9beae4c275c91ffcec01df",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Temperature sampling (multiple samples)",
            "name_full": "Temperature-based stochastic decoding with multiple sampled completions",
            "brief_description": "Generating many diverse model completions via temperature-controlled sampling to produce multiple candidate programs/answers, then checking them (e.g., with unit tests) and accepting any that pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (up to 137B parameters)",
            "model_description": "Left-to-right decoder-only Transformer models pretrained on web/doc/dialog/Wikipedia corpora with sizes from 244M to 137B non-embedding parameters; experiments highlight the 137B model.",
            "reasoning_method_name": "Temperature sampling (multi-sample ensemble)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate many completions using non-deterministic sampling with a temperature parameter (e.g., T=0.5) to induce diverse reasoning/programming outputs; evaluate many samples (80 in many experiments) and accept any sample that passes provided tests.",
            "task_name": "MBPP (Mostly Basic Programming Problems); MathQA-Python",
            "task_description": "MBPP: 974 short Python programming tasks with natural-language descriptions and test asserts. MathQA-Python: translated MathQA problems into Python programs. Evaluated via executing generated code against test cases.",
            "performance": "Using temperature sampling and 80 samples, the largest 137B model solved ~59.6% of MBPP problems (measured as 'any sample solves the task'); fraction-of-samples solving tasks was much lower (e.g., ~16.8% for 3 prompt examples, Table 1).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-sample greedy/low-temperature decoding performs better with only one evaluation allowed (greedy better for single-shot), but multi-sample higher-temperature strategies solve more tasks within modest budgets (e.g., with &gt;1 samples); beam search performed worse than all temperature settings.",
            "key_findings": "Sampling many diverse completions is an effective strategy for program synthesis: the 'any-sample' metric shows high gains (137B solves ~60% of MBPP with 80 samples). Higher temperatures lead to better scaling as the number of samples increases, while lower temperatures (more greedy) are better for single-sample budgets. Ensemble of many sampled outputs is a practical way to realize diverse reasoning paths.",
            "counter_examples_or_negative_results": "Although many tasks are solved by some sample, most solved tasks are only solved by a small fraction of samples (often 1–2 out of 80), indicating fragility. Diverse sampling does not guarantee correct semantics beyond provided tests (adversarial tests reveal ~12% of apparently-correct solutions would fail).",
            "uuid": "e4981.0",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Prompt-seed ensembling",
            "name_full": "Ensembling across different few-shot prompt example selections (prompt-seed ensembling)",
            "brief_description": "Use multiple distinct few-shot prompt example sets (different random seeds selecting which held-out examples are in the prompt) and count a task as solved if any prompt-seed yields a correct sample.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (137B primary focus)",
            "model_description": "Same family of decoder-only Transformer models; few-shot prompts include several example problems and their solutions.",
            "reasoning_method_name": "Prompt-seed ensembling (multiple few-shot prompts)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate outputs using different few-shot example choices (different prompt seeds) to produce diverse inductive biases/ reasoning paths; combine results by accepting a problem as solved if any seed produces a correct sample.",
            "task_name": "MBPP (Mostly Basic Programming Problems)",
            "task_description": "Short Python programming tasks evaluated by executing generated code on held-out tests.",
            "performance": "Single best prompt seed for 137B achieved ~59.6% solved; ensembling across multiple prompt seeds increased the solved fraction to ~66.4% (absolute improvement ~6.8 percentage points).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single prompt-seed few-shot performance: ~59.6% (137B).",
            "key_findings": "Changing the specific few-shot examples in the prompt yields notably different outcome sets; ensembling across seeds reliably increases coverage of solvable tasks. Thus, diversity in prompt examples is a simple, effective way to improve overall problem coverage.",
            "counter_examples_or_negative_results": "Prompt quality is highly variable: some seeds perform much worse than the best seed. Ensembling helps but even combined seeds leave many tasks unsolved; dependency on prompt examples remains a major source of variability.",
            "uuid": "e4981.1",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Beam search",
            "name_full": "Beam search decoding",
            "brief_description": "Beam search as a deterministic decoding strategy that explores top-k high-probability continuations, tested as an alternative to stochastic sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (various sizes, experiments reported for 137B)",
            "model_description": "Autoregressive Transformer models; beam search implemented as standard k-best deterministic decoding.",
            "reasoning_method_name": "Beam search (deterministic decoding)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Explore multiple high-probability continuations via beam search to produce candidate outputs that are typically similar and mode-seeking compared to stochastic sampling.",
            "task_name": "MBPP program synthesis",
            "task_description": "Synthesis of short Python programs from natural-language descriptions, evaluated by executing generated programs on test cases.",
            "performance": "Empirically performed extremely poorly relative to temperature sampling settings; beam search produced many looping or repetitive code outputs and was worse than any temperature setting tested (no numeric percent given, described qualitatively as worst).",
            "comparison_with_other_method": true,
            "performance_other_method": "Temperature-based stochastic sampling (multiple samples) substantially outperformed beam search across budgets; low-temperature greedy sampling can outperform beam search for single-sample budgets.",
            "key_findings": "Beam search is not an effective decoding strategy for program synthesis in these large LMs — it tends to produce repetitive/looping outputs and underperforms stochastic temperature sampling approaches.",
            "counter_examples_or_negative_results": "Beam search consistently underperformed; no evidence in this paper that deterministic beam search ever outperforms diverse sampling for MBPP.",
            "uuid": "e4981.2",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Single-sample greedy decoding",
            "name_full": "Greedy decoding (single most-likely completion)",
            "brief_description": "Use greedy decoding (temperature 0.0) to obtain a single, most-likely model completion and evaluate it; represents a similar, non-diverse reasoning method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (137B primary; other sizes reported)",
            "model_description": "Autoregressive Transformer LM; greedy decoding uses the argmax token at each step.",
            "reasoning_method_name": "Greedy single-shot decoding",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Deterministically decode the most likely token at each timestep (temperature=0.0) producing a single chain of reasoning/program; no sampling-induced diversity.",
            "task_name": "MBPP synthesis; execution prediction experiments",
            "task_description": "MBPP: program synthesis judged by execution against test cases. Execution task: predict outputs of ground-truth programs for given inputs.",
            "performance": "For synthesis, fraction-of-samples (single-sample reliability) is much lower than 'any-sample' metrics — e.g., for the 137B model with 3 prompt examples the fraction-of-samples solving tasks was ~16.8% (Table 1). For execution prediction (few-shot greedy), accuracy did not exceed ~29% across prompt configurations.",
            "comparison_with_other_method": true,
            "performance_other_method": "Multi-sample temperature sampling (diverse) achieved much higher 'any-sample' coverage (e.g., ~59.6% solved with 80 samples) and outperformed single-shot greedy when multiple samples are allowed.",
            "key_findings": "Greedy single-shot decoding is less reliable than strategies that generate diverse outputs and check them; greedy can be better than high-temperature sampling when only one sample is allowed, but multi-sample stochastic decoding yields higher overall coverage.",
            "counter_examples_or_negative_results": "Greedy decoding sometimes performs best when budget strictly limited to one sample (low-temperature best for single evaluation), but overall it yields lower coverage and robustness than diverse multi-sample methods.",
            "uuid": "e4981.3",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Human-model dialog (natural language feedback)",
            "name_full": "Human-in-the-loop natural-language dialog for iterative correction",
            "brief_description": "A human provides iterative short natural-language hints/corrections (up to four one-sentence interventions) to guide the model to repair or clarify code outputs, and the model updates its outputs accordingly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (137B focus; experiments on same family)",
            "model_description": "Pretrained autoregressive Transformer models prompted with few-shot collaborative-dialog examples; model produces code and accepts human text hints to generate revised code.",
            "reasoning_method_name": "Human-guided iterative refinement (dialog interventions)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "The human inspects model output and provides concise natural-language corrections or hints (e.g., point out missing import, clarify under-specified requirement); the model then synthesizes revised code incorporating the hint. Multiple dialog turns produce varied reasoning/solution attempts guided by human feedback.",
            "task_name": "MBPP (edited subset; human-model collaboration on 50 problems)",
            "task_description": "Interactive program synthesis where the human and model collaborate via natural-language dialog to produce a correct Python function passing provided tests.",
            "performance": "Without human feedback, few-shot single-sample solve rate ~30%; with up to four human dialog interventions, solve rate increased to over ~65% (one intervention -&gt; ~55%). Human feedback allowed solving 10 problems the model could not solve alone.",
            "comparison_with_other_method": true,
            "performance_other_method": "Increasing sampling (e.g., 5 samples) yields some baseline improvement, but the paper notes a one-sentence human correction was worth more than increasing samples five-fold (i.e., human feedback outperformed simply increasing model sampling).",
            "key_findings": "Human natural-language feedback substantially improves synthesis success rates (roughly halves error rate with up to four interventions). Human guidance can clarify under-specified prompts and fix small context/import/identifier errors in one turn; it is more effective than modest increases in sampling budget.",
            "counter_examples_or_negative_results": "Diminishing returns with more interventions; the model sometimes loses track of earlier context or prior code versions, struggling to revert or re-incorporate previous pieces of code across turns.",
            "uuid": "e4981.4",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Prompt assert count sensitivity",
            "name_full": "Sensitivity to number of input-output asserts/examples in few-shot prompt",
            "brief_description": "Varying how many test-assert examples are included in the prompt (0–3) to examine impact on synthesis performance; a measure of whether models rely on similar repeated constraints versus diverse exemplars.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only Transformer LM (137B reported)",
            "model_description": "Autoregressive Transformer LMs with few-shot prompts containing function name and optionally up to three assert test cases.",
            "reasoning_method_name": "Varying number of prompt asserts/examples",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Change the number of explicit test-assert examples included in each prompt (0, 1, 2, 3) to measure effect on model's synthesis behavior: more asserts could encourage the model to conform to test semantics or encourage overfitting to asserts.",
            "task_name": "MBPP",
            "task_description": "Short Python function synthesis evaluated by executing against test asserts.",
            "performance": "For 137B: 0 asserts -&gt; 43.2% problems solved; 1 assert -&gt; 55.2%; 2 asserts -&gt; 59.0% (best here); 3 asserts -&gt; 58.4%. Fraction-of-samples solving task peaked at 3 asserts (~16.77%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared across assert counts as reported above; inclusion of asserts had modest effects and did not strongly reduce overfitting overall.",
            "key_findings": "Including 1–2 assert examples in the prompt substantially increases solved rate versus none; however, adding the third assert gave only marginal additional benefit, suggesting models are not simply overfitting to test cases. The model is mostly not using asserts to fully reason about semantics.",
            "counter_examples_or_negative_results": "Occasional overfitting to asserts occurs: in rare cases the model hard-codes outputs that trivially pass asserts (e.g., return True only for asserted input), showing asserts can sometimes be abused rather than leading to correct general semantics.",
            "uuid": "e4981.5",
            "source_info": {
                "paper_title": "Program Synthesis with Large Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2
        }
    ],
    "cost": 0.0152615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Program Synthesis with Large Language Models</h1>
<p>Jacob Austin<em><br>Mazwell Nye ${ }^{\dagger}$ Maarten Bosma Henryk Michalewski David Dohan Ellen Jiang Carrie Cai<br>Michael Terry<br>Quoc Le<br>Charles Sutton<br>Google Research<br></em> denotes equal contribution<br>jaaustin@google.com, augustusodena@google.com</p>
<h4>Abstract</h4>
<p>This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to $59.6 \%$ of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves $83.8 \%$ accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.</p>
<h2>1 Introduction</h2>
<p>Program synthesis is a longstanding goal of artificial intelligence research [Manna and Waldinger, 1971, Waldinger et al., 1969, Summers, 1977, Shaw et al., 1975, Pnueli and Rosner, 1989, Manna and Waldinger, 1975], dating as far back as the 1940s and 50s [Copeland, 2012, Backus et al., 1957]. There has been a recent resurgence of interest in techniques (both symbolic and 'neuro-symbolic') for synthesizing programs [Balog et al., 2017, Devlin et al., 2017, Ellis et al., 2018, 2020, Odena et al., 2020], but these techniques have largely been applied to restricted domain-specific languages (DSLs) [Gulwani, 2011] or to languages that are more fully featured but that nevertheless are designed specifically with synthesis in mind [Odena and Sutton, 2020]. Modern general-purpose languages like Python or C++ have mostly been out-of-reach as targets. This is unfortunate, because it materially restricts the set of possible downstream applications. Synthesis methods that target problems across domains in general purpose languages have the potential to enable new tools that benefit the workflows of both novice and expert programmers.</p>
<p>Two emerging themes from the research literature point to a possible new approach for this problem (for a more detailed review, see Section 8). First, large language models have shown impressive new abilities to generate natural language</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example programs synthesized (few-shot) by our largest model. The prompt is shown in purple, and the model's response in blue. The prompt also typically contains several few-shot examples in the same format, which are not shown here.
text [Brown et al., 2020, Raffel et al., 2019] and to solve a rapidly expanding set of modeling and reasoning tasks [Devlin et al., 2019, big-bench collaboration, 2021]. Second, over the past decade, machine learning approaches have been applied to source code text to yield a variety of new tools to support software engineering [Allamanis et al., 2018a]. This work has included pre-trained deep models such as CuBERT [Kanade et al., 2020], CodeBERT [Feng et al., 2020], PyMT5 [Clement et al., 2020], code2vec [Alon et al., 2019], and other T5 models trained on code [Mastropaolo et al., 2021].
Combining these two themes raises the question of whether large language models for natural language can be brought to bear to synthesize code in a general-purpose language. Such models emit code in 'token-space', and so it is not necessary to explicitly encode the grammar of the underlying language-they learn it from data. Furthermore, these models can be trained on large quantities of code, so they can learn about how various libraries interact with each other and what idiomatic, human-readable code looks like. Finally, large language models allow us to consider a more flexible type of program specification: in contrast to classical work on program synthesis that specifies the program using logical constraints or input-output examples [Gulwani et al., 2017], a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3 ) input-output examples.
In this paper, we study how a collection of large Transformer language models performs when applied to the synthesis of short programs written in general purpose programming languages. Examples of problems and model output are shown in Figure 1 and Figure 2.
In particular, this paper makes the following contributions:</p>
<ol>
<li>We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP). It contains 974 short Python functions designed to be solved by entry-level programmers, text descriptions of those programs, and test cases to check for functional correctness (Section 2.1). This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-</li>
</ol>
<p>verified by the authors. The second is a Python synthesis dataset, containing 23914 problems, produced by rewriting the solutions to a subset of the MathQA dataset [Amini et al., 2019] into Python (Section 2.2). We call this dataset MathQA-Python. These two datasets exercise different points in the space of synthesis tasks: MBPP contains more usage of imperative control flow such as loops and conditionals, while MathQA-Python contains more complex natural language descriptions.
2. On both datasets, we show that a large language model performs surprisingly well at few-shot synthesis of Python programs from a prompt (Sections 4 and 7). Fine-tuning further on each of the datasets yields a further increase in synthesis performance. This is especially notable for MBPP because the fine-tuning set is extremely small ( 374 synthesis problems). We evaluate the model performance at scales ranging from 244 M to 137B parameters, finding that performance continues to improve with increased model size. The largest models that we consider can synthesize solutions to $59.6 \%$ of the problems from MBPP using few-shot learning. For most model sizes, fine-tuning increases performance by about 10 percentage points. On the smaller, hand-verified MBPP dataset, we observe that the synthesis task is indeed easier: For the 100 problems that occur in both the original and edited datasets, few-shot model performance increases from $63 \%$ on the original dataset to $79 \%$ on the edited dataset. On the MathQA-Python dataset, the largest model achieves few-shot accuracy of $33.4 \%$ while fine-tuning it leads to a very high accuracy of $83.8 \%$.
3. Going beyond single-step program synthesis, we study the model's ability to engage in dialog about code and improve its performance in response to natural-language feedback from humans (Section 5). We find that the model is able to incorporate short natural language hints to repair its outputs and clarify under-specified prompts, increasing few-shot performance from $30 \%$ without human feedback to $65 \%$ with four turns of dialog, yielding a $50 \%$ error reduction (Section 5.1).
4. We explore the semantic grounding of our models, investigating the extent to which these models can execute code given specific inputs (Section 6). We find that even our largest models are generally unable to predict the output of a program given a particular input, whether few-shot (Section 6.1) or with fine-tuning (Section 6.2). This suggests a large gap between what these models are doing and what we would consider "understanding."
5. We analyze sensitivity of performance to a variety of factors, including model size, number of examples in the prompt, the identity of examples in prompt, sampling technique, etc. Furthermore, we investigate two potential criticisms of synthesis from large language models: First, we find that solutions tend to generalize to held-out test cases, rather than simply parroting the answers in the prompt (Section 4.4), although there are occasional exceptions (Section 4.5). Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).</p>
<p>Our work is closely related to two recent efforts. The first is the APPS dataset [Hendrycks et al., 2021], which is a dataset of 10,000 problems from coding competitions. Hendrycks et al. [2021] evaluate large language models on this data, specifically finetuned GPT-2 [Radford et al., 2019] and GPT-Neo [Black et al., 2021], as well as few-shot prediction with GPT-3 [Brown et al., 2020]. Additionally, several datasets have been proposed to train and evaluate program synthesis methods based on data from programming competitions (Section 8.3). However, performance on these benchmarks has generally been poor. We conjecture that this is because programming competition problems are written in a style that obfuscates the underlying algorithms necessary to solve them. By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. We believe this shifts the focus more toward capabilities directly related to generating and understanding code.
Secondly, and independently, Chen et al. [2021] have presented Codex, a Transformer LM on code following the GPT-3 architecture, evaluating its synthesis performance on a new benchmark of simple programming problems. The main differences are in the specifics of the pre-training data, and in the way that we investigate the model's performance. First, the training set for our models somewhat oversampled web pages that contain code, such as programming question and answer sites (see Section 3), but unlike Chen et al. [2021], the results reported in this paper do not include a further fine-tuning step on a large corpus of open-source code. Second, while the HumanEval benchmark introduced by Chen et al. [2021] is nominally similar to our MBPP, there are some differences: A small difference is in the type of prompts; while the HumanEval dataset generally contains I/O examples of the desired functions, their number and formatting is not consistent, in a way that mimics docstrings of professional software. In contrast, our dataset consistently contains three I/O examples, written as assert statements. We also evaluate our models on the MathQA dataset, which is completely different in character. Third, we report synthesis results for models of size up to 137B. We find that even our general LM, without code fine-tuning, has non-negligible performance on few shot synthesis, and we find that fine-tuning that model on a very small ( 374 items) set of examples is already enough to dramatically improve performance on synthesis tasks. Fourth, and perhaps most interestingly, we analyze the extent to which our LMs can be used as interactive tools, and present results showing that humans can interact with these models to significantly improve</p>
<p>their success rate. Finally, in keeping with our goal to explore and understand the performance of general-purpose language models on this task, we also explore whether these models can evaluate the code that they generate, and whether they are equally effective at generating code that solves traditional mathematical word problems.</p>
<h1>2 Datasets</h1>
<p>We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset. The second is derived from the MathQA dataset [Amini et al., 2019] but casts the problem solutions as short Python programs.</p>
<h3>2.1 Mostly Basic Programming Problems</h3>
<p>The Mostly Basic Programming Problems dataset contains 974 short Python programs constructed by crowd-sourcing to an internal pool of crowdworkers who have basic knowledge of Python. We asked crowd-sourcing participants to write a short problem statement, a single self-contained Python function solving the problem specified, and three test cases that check for semantic correctness of the function. Participants also provided a ground-truth solution that passes all three test cases. Participants were instructed to write descriptions concrete enough that a human would be able to translate them into code without clarifications. They were also instructed to write code that is self-contained (that is, it runs by itself) and that does not print any results to the console. Use of internet references was allowed.
The problems range from simple numeric manipulations or tasks that require basic usage of standard library functions to tasks that require nontrivial external knowledge, such as the definition of particular notable integer sequences. Figure 1 shows an example problem statement with the associated test cases and a sample from our largest model prompted with that problem statement. To further characterize the contents of the dataset, we randomly sampled 100 of the questions and assigned one or more descriptive tags to each question. Of these questions, $58 \%$ were mathematical in nature (e.g., calculating the volume of a sphere), $43 \%$ involve list processing, $19 \%$ require string processing, $9 \%$ involve integer sequences, and $2 \%$ center around the use of other data structures. We did not impose any restrictions on the number of lines of code in the reference solution. The average, median, and maximum number of lines of code are 6.8, 5, and 50 respectively. The natural language descriptions are typically short, usually one sentence each.
While inspecting the dataset, we observed that some questions used uncommon function signatures (such as passing in a list and its length as two separate arguments to a function), lacked detail, were somewhat ambiguous (e.g., "Write a python function to count the number of squares in a rectangle."), or performed unexpected operations in a function that were paired with the provided tests (e.g., casting a float to an int before returning it, with the test performing integer comparisons). Given this, we manually inspected, edited, and pruned a subset of the questions, yielding 426 hand-verified questions, which we refer to as the edited dataset. For each question in the edited dataset, we ensured it had a standard Python function signature, that it was unambiguous to a human, and that its test cases accurately reflected the text description. We conduct most experiments on the full dataset, but analyze the effect of the curation of the edited dataset in Section 4.9.
In the experiments described later in the paper, we hold out 10 problems for few-shot prompting, another 500 as our test dataset (which is used to evaluate both few-shot inference and fine-tuned models), 374 problems for fine-tuning, and the rest for validation. For evaluations involving the edited dataset, we perform comparisons with 100 problems that appear in both the original and edited dataset, using the same held out 10 problems for few-shot prompting and 374 problems for fine-tuning. We have programmatically checked that all reference code passes all tests under Python 3.6, and we have open-sourced all of the problems. ${ }^{1}$</p>
<h3>2.2 MathQA-Python</h3>
<p>Compared to the short natural language descriptions in MBPP, our second dataset is representative of a different kind of program synthesis task. The MathQA dataset [Amini et al., 2019] is a dataset where each data point consists of a mathematical word problem, multiple-choice answers for that problem, and a program in a domain-specific language that produces the correct answer. To evaluate whether pre-training on source code is useful for this task, we translate this dataset into a Python program synthesis dataset by translating the ground-truth programs from the domain-specific language given in the paper to Python code. We refer to the converted dataset as MathQA-Python. Compared to MBPP</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Please, solve the mathematical problem: a and b start walking towards each other at 4 pm at a speed of 2 kmph and 3 kmph. They were initially 15 km apart. At what time do they meet? n0 = 4.0, n1 = 2.0, n3 = 15.0.</p>
<div class="codehilite"><pre><span></span><code>n0 = 4.0
n1 = 2.0
n2 = 3.0
n3 = 15.0
t0 = n1 + n2
t1 = n3 / t0
answer = n0 + t1
</code></pre></div>

<p>Figure 2: An example MathQA prompt along with a Python solution emitted by our largest model. Everything in purple is given as a prompt (along with some few-shot examples not shown). The equivalent DSL code is: add(n1,n2)|divide(n3,#0)|add(n0,#1)
which contains more usage of imperative control flow such as loops and conditionals, MathQA-Python contains mostly straight-line code, but more complex natural language descriptions. An example from this dataset is shown in Figure 2. Both the Python code and DSL code are used for fine-tuning and few-shot experiments. For the few-shot experiments, in the prompt we provide four examples of MathQA problems with their Python (or DSL) solutions. The model is tasked with returning Python or DSL code that computes the ground truth answer. We execute the sampled code to check for semantic correctness. This method of checking correctness forced us to filter the MathQA dataset to keep only those problems for which the code evaluates to the declared numerical answer, resulting in us removing $45 \%$ of problems. After this filtration we are left with 23914 problems, of which we use 19209 for training, 2822 for validation and 1883 for testing. The translation between DSL and Python is straightforward and we supply code that can be used to perform it. ${ }^{2}$</p>
<h1>3 Model and Methods</h1>
<p>The models we use in this paper are dense left-to-right decoder-only Transformer language models [Vaswani et al., 2017] trained on a combination of web documents, dialog data, and Wikipedia. Our experiments were conducted using models with non-embedding-parameter-counts ranging from 244 million to 137 billion. The pre-training dataset for the model contains 2.97B documents, which were tokenized into 2.81T BPE tokens with a vocabulary of 32 K tokens using the SentencePiece library [Kudo and Richardson, 2018]. This data included web sites with both computer code and text, such as question and answer sites and tutorials, but source code files themselves were not specifically included, except where code appeared in other web sites. These web sites with code and text comprised about 13.8 M documents containing 18.7B BPE tokens out of the pre-training data.
We test synthesis capabilities for both MBPP and MathQA-Python under two regimes: First, we use few-shot prompting as in Brown et al. [2020]. We hold out several example problems for the prompt and concatenate them, resulting in a longer version of the prompt seen in Figure 1 (or Figure 2 in the case of MathQA-Python). We then feed this prompt to the pre-trained model for completion. Second, we fine-tune the model on a training set. For MBPP, the training set is quite small ( 374 examples), so we fine-tune with a small learning rate (3e-5 for the largest model) for only 100 steps. For MathQA-Python, we fine-tune for longer. We generated the execution results using roughly analogous methods; see Section 6 for more details.
For all synthesis experiments, we measure functional correctness of the sampled code rather than some proxy for code quality like token accuracy or BLEU (see Section 4.7 for more about this). For the MBPP synthesis experiments, we check whether the code passes a set of test cases when executed (see Figure 1 for example test cases). For each problem in the test dataset, we use temperature sampling (with temperature 0.5 ) to generate 80 samples of code and then execute the code contained in the samples against tests for semantic correctness. The MathQA synthesis experiments are analogous.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For the MBPP execution experiments, we check whether the model produces exactly the same results as executing the code. We use greedy decoding (temperature set to 0.0 ) to generate a single approximate most likely generation, and compare this to the string generated by executing the code.</p>
<h1>4 MBPP Synthesis Results</h1>
<p>Our primary results on MBPP are shown in Figure 3 and Figure 4. We show absolute performance and scaling behavior with model size for both few-shot (in the sense of Brown et al. [2020]) and fine-tuning across nearly three orders of magnitude. We find that samples from our models are able to solve a large fraction of the problems in the dataset, in the sense that the sampled code passes the three given test cases, and that synthesis performance scales approximately log-linearly with model size.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Performance vs model size, measured in two ways. (Left) Fraction of programs solved by any sample as model size is increased. This metric improves predictably as model size is increased, and fine-tuning gives a roughly constant improvement over few-shot prompting. The slope of the line shows no signs of decreasing for our largest models, which suggests that further performance gains can be had by making the model larger. (Right) Total fraction of sampled programs that solve a task, as model size is increased.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Fraction of samples solving each task. The x-axis represents the index of a particular task, sorted by the model performance. The y-axis represents the fraction of samples from the model that solved the task. In both cases, the curve is pushed "up and to the left" and the area under the curve increases as parameters are added to the model. This means that more tasks were solved by any sample, but also that bigger models can more reliably solve the "easier" problems. (Left) Results for few-shot prompting. (Right) Results for fine-tuned models. The gaps between models are more uniform for the fine-tuned results than for the few-shot results (which are noisy).</p>
<p>We measure performance as a function of parameter count in two different ways: the fraction of problems that are solved by any sample from the model and the fraction of samples that solve their respective problem. The fraction-of-problems metric is a natural notion of correctness, because if this model were to be used in practice, we could automatically filter out model samples that do not pass the test cases. The fraction-of-samples metric, by contrast, gives a sense of the overall reliability of the model. We find that performance according to the fraction-of-problems metric is quite predictable while performance according to the fraction-of-samples metric is less so.
We observe limitations on the types of problems these models can solve (some are simply unsolvable) and many solved problems tend to have only 1 or 2 (out of 80) samples which solve the task. We examine this and other limitations in later sections. We also find that our results are not strongly sensitive to the number of examples (asserts) shown to the model, but do depend strongly on the specific examples provided as prompts.</p>
<h1>4.1 Synthesis Performance Improves as Model Size Increases</h1>
<p>We measure synthesis performance at various model sizes, from 244 million parameters up to 137 billion. As explained above, performance is measured in two different ways: First we measure-for each problem independently-whether that problem was solved by any of the samples drawn from the model for that problem. Performance on this metric scales predictably with model size: the fraction of tasks solved is linear in the logarithm of the model size. The largest model can solve roughly 60 percent of the problems it sees given 80 samples. For this metric, fine-tuning seems to give a roughly constant boost in performance across model sizes. See Figure 3 (left) for more details. Second, we measure across all samples generated for all problems - the fraction of samples solving their respective task. This corresponds to the area under the curve in Figure 4 and is depicted in Figure 3 (right). Performance on this metric improves as model size increases, but it scales up less predictably than does performance on the first metric. For this metric, fine-tuning tends to improve performance, but the relationship between fine-tuned performance and few-shot performance is much more variable as a function of model size than for the other metric.
Additionally, we analyze the types of errors made by the models: Figure 5 shows the breakdown of error types as a function of model size for the few-shot experiments. We define runtime errors as any errors (other than syntax or type errors) that cause the program not to produce a result. For most model sizes, runtime errors are more common than syntax errors; even the smallest models can write syntactically correct Python code around $80 \%$ of the time. However, type errors and other syntax errors do represent the majority of samples drawn from the smallest model. As model size increases, the frequencies of both run-time and syntactic errors decrease dramatically. For the largest (137B) model, over $63 \%$ of all failures are due to failing the test assertions, as opposed to run-time or syntactic errors.</p>
<h3>4.2 Synthesis Performance is Insensitive to Number of Test Cases in Prompt</h3>
<p>The example prompt in Figure 1 shows all three of the test assertions that the model output will be checked against. We measured whether including less than 3 of the assertions in the prompt would result in a serious drop in performance. Interestingly, it did not: the model with 3 asserts in the prompt solved only 3 extra problems compared to the model with 1 assert only. This suggests that the model is mostly not using those test cases to reason about semantics. More specifically, it also suggests that, even though we prompt the model with all three test asserts, the model is in general not "overfitting" to test-cases (though we explore exceptions in Section 4.5).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># of Prompt Examples</th>
<th style="text-align: center;">\% of Problems Solved</th>
<th style="text-align: center;">\% of Samples Solving Task</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$43.2 \%$</td>
<td style="text-align: center;">$10.23 \%$</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">$15.30 \%$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{5 9 . 0 \%}$</td>
<td style="text-align: center;">$15.14 \%$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$58.4 \%$</td>
<td style="text-align: center;">$\mathbf{1 6 . 7 7 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Few-shot performance of the 137B parameter model as a function of number of prompting examples. The prompts for row zero only provide the function name. The bold text in the left column shows 59.0 instead of 59.6 because there is a small amount of run-to-run variance in the number of problems solved.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Breakdown of error type as a function of model size. The figure shows the breakdown of error type across all samples across all test tasks. 'Runtime errors' are defined as any errors (other than syntax or type errors) that cause the program not to produce a result. All error types decrease in frequency as model size increases.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Performance as a function of which prompt examples are chosen, as measured by fraction of tasks solved by at least one sample. The seed label corresponds to the random seed used to choose which held-out examples are shown as prompts. Seeds are ordered by the fraction of tasks solved by that seed.</p>
<h1>4.3 Performance is Sensitive to Prompt Examples</h1>
<p>While model performance is not strongly sensitive to the number of test cases included in the prompt, few-shot performance is quite sensitive to the particular examples given in the prompt. We measure this sensitivity in Figure 6 , where each seed corresponds to a particular, distinct choice of prompting examples. We find that while one set of examples (seed 14) is able to solve $60 \%$ of tasks, many other examples solve far fewer.
The large influence of these prompt examples is also noticeable qualitatively: failed synthesis attempts often include references to e.g. a data structure that was instantiated in one of those examples in the prompt. These results suggest that methods such as prompt-tuning [Li and Liang, 2021, Lester et al., 2021] could yield substantial performance improvements in this domain.
One failure mode for the poorly performing prompts is that they lead to long, repetitive samples. Often, very long prompts produce many samples that do not fit with the 512 token context window (even with a context window of 1024 tokens, this failure mode is still pronounced). Qualitatively, we notice that short prompts with compact examples that make use of external libraries lead to the best synthesis performance.
We also find that the set of problems solved with one prompt seed is not always a strict subset or superset of another: for example, seed 13 solves 19 problems (39, 62, 100, 168, 188, 206, 209, 233, 254, 315, 340, 365, 368, 382, 400, 434, $471,474,497$ ) which are not solved by seed 14 . Ensembling these prompts by counting a problem as solved if it is solved by any of the seeds boosts the percentage of problems solved from $59.6 \%$ to $66.4 \%$.</p>
<h3>4.4 Solutions Typically Generalize to Held-Out Test Cases</h3>
<p>Consider task 11 from MBPP, which asks the model to "Write a python function to remove first and last occurrence of a given character from the string.". All of the solutions emitted by our best model pass all three test cases, but the test cases do not fully test the function's semantics (as shown in Figure 7).
None of the test cases use strings which contain more than than two of the specified character. Upon inspection, we realized that all of the sampled solutions would simply delete all occurrences of the specified character. To estimate how widespread this phenomenon was, we sampled 50 of the 500 test programs and wrote 'adversarial' tests cases for them. On those 50 problems, 33 had solutions solving all of the normal test cases, and 29 had solutions solving all of the normal test cases and all of the 'challenge test cases', for solution rates of $66 \%$ and $58 \%$ respectively. Thus, we can roughly estimate that something like $12 \%((66-58) / 66)$ of what we are counting as solutions (e.g. in Section 4.1) would fail to satisfy adversarially generated test cases. This is a nontrivial fraction, but it also means that almost $90 \%$ of solutions will generalize in the sense measured here.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Test cases for Task 11. The normal test cases incorrectly allow a program that deletes all occurrences of the given character, rather than only the first and last. The challenge test cases exercise this corner case.</p>
<h1>4.5 Programs Sometimes Overfit to Assert Statements</h1>
<p>Very occasionally, the model will produce a solution that passes the test cases trivially by reading the assert statements and trying to hard-code an if-expression that passes them. For example, one of the problems asks for a function that checks if a given integer is a Woodall number (that is, a number belonging to the sequence $1,7,23,63,159,383,895, \ldots$ ). This problem includes three asserts (see Figure 8), only one of which specifies a number that is actually a Woodall number: 383. The model simply emits a program that returns True if the input is 383 and False otherwise, which is not correct.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: In rare cases, the model generates a program which trivially passes the test asserts but does not solve the problem. This program does not correctly check if the given input is a Woodall number, it simply returns true if the input is 383 .</p>
<p>This is interesting and perhaps somewhat alarming, though it also highlights that the model does have some abstract notion of the relationship between the test cases and the generated program. We can infer from the results in Section 4.2 and 4.4 that this "overfitting" to the test cases is not a widespread problem.</p>
<h3>4.6 Sampling Strategy is Critical for Good Performance</h3>
<p>Since tests or input-output examples can be machine checked, it is standard [Devlin et al., 2017] for synthesis algorithms to generate and evaluate many samples (often even enumerating and checking all possible programs). We investigate the scaling performance of our largest model with the number of samples evaluated across different sampling strategies: temperature sampling with varying temperatures and beam search. Figure 9 shows the number of tasks solved by the 137B model as the number of samples increases. We find that lower temperatures (more greedy decoding) perform better with only a single evaluation allowed, but higher temperature, less greedy strategies begin to solve more tasks within a budget of 10 samples. We also find that beam search performs extremely poorly, worse than any of the temperature</p>
<p>settings considered - empirically we found this was due to beam search often producing results that looped or repeated lines of code endlessly.</p>
<h1>4.7 Synthesis Performance Correlates Poorly with BLEU Score</h1>
<p>As noted by Hendrycks et al. [2021] and Chen et al. [2021], we find that BLEU score between generated samples and reference programs does not correlate well with synthesis performance. Figure 10 shows two curves: the fraction of samples which solve a given task and the average BLEU score of samples compared to the reference program. We find little correlation between the two. This can be explained by the fact that semantically identical programs can potentially have very low $n$-gram overlap; for example, because of identifier renaming.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Higher temperatures achieve better scaling with more samples, but perform worse with a smaller budget.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Comparison of BLEU score and synthesis performance for the 137B parameter model. No strong correlation is observed.</p>
<h3>4.8 Pre-train / Test Overlap Seems to be Minimal</h3>
<p>A common concern about results on large language models is that the models are likely to have seen something substantially similar to the test data in their very large training set, causing the test accuracy to overstate performance in practice [Brown et al., 2020]. Even though we created this dataset from scratch, it is still possible that this is an issue for some tasks for two reasons. First, some tasks are very simple (e.g. 'reverse a string') and so surely will be represented in the training data in some way. Second, crowd-sourcing participants may have made use of reference materials from the internet that could also have appeared in the pre-training dataset for our models.</p>
<p>To quantify this concern we investigated how many lines of code appear in both the training data for our models and the ground-truth programs for the Mostly Basic Programming Problems. We examined each document in the pre-training data (excluding non-English documents and conversational data) and counted the number of lines that overlap with the ground-truth program for each problem. We then found the document with the most matching lines and the number of matching lines in MBPP. We stripped whitespace at the beginning and end of the line. We excluded lines from this analysis which appear more than twice anywhere in MBPP, as these are likely to be common Python keywords such as return or continue.</p>
<p>Figure 11 contains a visualization of the results. Broadly speaking, there was not much overlap. Only 32 of 974 problems (3.3\%) had more than half of their lines matched somewhere in the pre-training data and $91.5 \%$ had only one or two lines of code that overlapped with the training set.</p>
<h3>4.9 Comparing Performance Between the Original and Edited Questions</h3>
<p>As described in Section 2.1, we created a subset of the larger MBPP dataset consisting of questions that were manually inspected and edited for consistency. We then ran experiments on 100 questions that appear in both the original dataset</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Number of lines of code that appear in both the pre-training data and in the python programming dataset. The left chart shows the absolute number of lines and the right chart shows the relative number of lines, as a percentage of the lines in the ground-truth program.</p>
<table>
<thead>
<tr>
<th>Model Size</th>
<th>Edited?</th>
<th>\% of Problems Solved</th>
<th>\% of Samples Solving Task</th>
</tr>
</thead>
<tbody>
<tr>
<td>8B</td>
<td></td>
<td>$35 \%$</td>
<td>$4.46 \%$</td>
</tr>
<tr>
<td>8B</td>
<td>$\checkmark$</td>
<td>$45 \%$</td>
<td>$7.36 \%$</td>
</tr>
<tr>
<td>68B</td>
<td></td>
<td>$48 \%$</td>
<td>$8.02 \%$</td>
</tr>
<tr>
<td>68B</td>
<td>$\checkmark$</td>
<td>$61 \%$</td>
<td>$12.95 \%$</td>
</tr>
<tr>
<td>137B</td>
<td></td>
<td>$63 \%$</td>
<td>$20.78 \%$</td>
</tr>
<tr>
<td>137B</td>
<td>$\checkmark$</td>
<td>$79 \%$</td>
<td>$31.85 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison between original and manually edited dataset on 100 problems. and this edited dataset. In this set of 100 questions, $56 \%$ of the questions' text was edited, $30 \%$ of the test cases were edited, and $71 \%$ included edits to either the questions or test cases. Using this dataset, we ran experiments using few-shot prompting for models with 8B, 68B, and 137B parameters.</p>
<p>Table 2 summarizes model performance on the original and edited dataset. As can be seen, model performance increases when using the edited dataset for each experiment. Table 3 shows that $16-19 \%$ of the problems can be solved using one dataset, but not the other, across model sizes. Within this same subset of problems, for 81-100\% of the problems, the model is able to produce a correct solution using the edited version of the question, rather than with the original version (across model sizes tested). However, within this subset of questions, 12-31\% had no differences in either the question text or test cases for the three model sizes, indicating general variability in model performance.</p>
<p>We manually examined each of the 38 problems for which model responses (on the sanitized and unsanitized data) were not both right or both wrong. In these 38 problems, 15 included edits to the problem text, but not the test cases, 7 problems included edits to the test cases but not the problem text, 7 included edits to both the problem text and test cases, and 9 problems had no edits to either the problem text or test cases.</p>
<p>For the 15 problems whose problem text was edited, but had no changes to the test cases, 11/15 included more detail in the problem text (e.g., specifying that a list should be flattened and summed, where the "flatten" detail was previously omitted). $4 / 15$ of the edits included details related to the function's signature (e.g., specifying that a list of lists should be returned), $2 / 15$ removed requirements (such as the requirement to use a regex in the solution code), and $2 / 15$ rewrote the problem text. For the seven problems with edits to both the problem text and test cases, $5 / 7$ included more details and $2 / 7$ added details related to the function's signature.</p>
<p>For the 7 problems with differences in the test cases, but no differences in the problem text, $3 / 7$ edited test cases modified the problem's function signature (e.g., changing it to return a list rather than a string representation of a list), $2 / 7$ problems attempted to perform comparisons between floating point numbers directly (rather than testing whether the numbers were approximately equal), one set of test cases tested floating point equality for a function that returned integers, and one problem added an additional test case. For the seven questions with edits to both the problem text and test cases, $3 / 7$ changed the function signature of the test, $2 / 7$ created a more robust test (comparing sets rather than lists,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Of problems solved in exactly one dataset:</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model size</td>
<td style="text-align: center;">Problems solved in <br> exactly one dataset</td>
<td style="text-align: center;">Solved in edited dataset</td>
<td style="text-align: center;">Solved with no edits <br> to text or test cases</td>
</tr>
<tr>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$81 \%$</td>
<td style="text-align: center;">$31 \%$</td>
</tr>
<tr>
<td style="text-align: center;">68B</td>
<td style="text-align: center;">$19 \%$</td>
<td style="text-align: center;">$84 \%$</td>
<td style="text-align: center;">$16 \%$</td>
</tr>
<tr>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$12 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics of problems that could be solved in only one of the edited versus the original MBPP datasets. When a problem can be solved in one dataset and not the other, it is more likely to be solved in the edited dataset compared to its original formulation.
when order was not important for a function returning a set of unique values), $1 / 7$ corrected floating point comparison issues, $1 / 7$ fixed an error in a test case, and $1 / 7$ added a test case.
In general, these observations suggest the importance of specificity and details in the natural language request sent to the model, with more details seeming to lead to a higher likelihood of the model being able to produce correct code (as might be expected). Having a function signature that matches conventions also seems to be important (which is also expected).</p>
<h1>4.10 Qualitative Analysis of Error Modes</h1>
<p>To deepen our understanding of model behavior and error modes, we conducted a qualitative error mode analysis by examining hand-verified problems for which most samples were incorrect, culminating in several themes (Table 4).</p>
<p>Problems with multiple constraints or sub-problems: First, difficult problems (as measured by model performance) often had multiple constraints or multiple implicit sub-steps. For example, the question "Write a function to find the maximum difference between the number of 0 s and number of 1 s in any sub-string of the given binary string" involves not only counting 0 s and 1 s , but also finding substrings. Likewise, "Write a function to find the longest palindromic subsequence in the given string" requires both finding palindromic subsequences and determining the longest one. In contrast, easy problems tended to be shorter and more atomic (e.g. "Write a python function to find the sum of an array."). In multiple-constraint problems, the model often generated a partial solution that addressed only a sub-component of the problem. For instance, in the digits example above, one model solution correctly counted 0 s and 1 s but did not do so over all substrings. In the palindrome problem, the model merely recorded indices of mirror-imaged letters, but did not use those indices to find palindromic subsequence and did not write logic to find the longest one. This suggests that the model may struggle more with complex, multi-part problems that combine many atomic operations.</p>
<p>Problems with more-common siblings: Relatedly, some low-performing problems appeared to have variants that are more common, resulting in the model solving a more common version of the problem. For example, given the problem "Write a python function to find the largest number that can be formed with the given list of digits.", the model found the largest number among the list of digits, rather than the largest number that can be formed from them. A similar error occurred when a complex problem asked for the "maximum difference" but the model computed the "maximum" instead. Given the plethora of problems on the internet that involve finding the largest number from among a list, this model behavior is perhaps not surprising. However, given that the model may latch onto keywords found in ubiquitous programming problems, this does pose a unique challenge for the long tail of problems that may be closely related to (or have keywords in common with) typical programming problems. We might consider these types of errors "linguistic off-by-one" errors, where a small change in words might lead to a large semantic difference.</p>
<p>Miscellaneous errors: Other miscellaneous error patterns included difficulty solving advanced math problems (e.g. "Write a function to find the nth hexagonal number"), producing incomplete skeleton code rather than the code itself, or a failure to apply common-sense reasoning (e.g. "convert a list to a tuple" led the model to convert each item of the list into a tuple).</p>
<p>Table 4: Qualitative analysis of highest- and lowest-performing problems</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Theme</th>
<th style="text-align: center;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Highestperforming problems</td>
<td style="text-align: center;">Single operations</td>
<td style="text-align: center;">Write a function to remove all whitespaces from a string. <br> Write a python function to find the maximum of two numbers.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common "coding interview" type questions</td>
<td style="text-align: center;">Write a function to merge multiple sorted inputs into a single sorted iterator</td>
</tr>
<tr>
<td style="text-align: center;">Lowestperforming problems</td>
<td style="text-align: center;">Problems demanding multiple constraints or multiple sub-problems</td>
<td style="text-align: center;">Write a function to find the maximum difference between the number of 0 s and number of 1 s in any sub-string of the given binary string <br> (Sub-problems: count 0 s and 1 s, find difference, find max across all sub-strings) <br> Write a function to find the longest palindromic subsequence in the given string <br> (Sub-problems: keep track of mirror-imaged letters, find palindromes, find longest one)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Problems that have a morecommon sibling with similar keywords</td>
<td style="text-align: center;">Write a python function to find the largest number that can be formed with the given list of digits. <br> (Model solves more-common problem: finds the largest number among the list of digits) <br> Write a python function to reverse only the vowels of a given string. (Model solves more-common problem: finds all vowels in the string)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Specialized math problems</td>
<td style="text-align: center;">Write a function to find eulerian number a(n, m).</td>
</tr>
</tbody>
</table>
<h1>5 Human-Model Collaboration Results</h1>
<p>While large language models are impressive program synthesizers in some respects, they are far from being able to reliably solve difficult engineering problems without intervention. This raises the question of whether these models can be useful as interactive tools that can refine or correct their predictions in response to user feedback. We are specifically curious about two possible forms of collaboration:</p>
<ul>
<li>Whether a human and model together are able to solve tasks that are challenging for either alone.</li>
<li>Whether human feedback can help a model refine its outputs, especially in the presence of initially ambiguous or under-specified requests.</li>
</ul>
<p>In this section, we conduct preliminary experiments to measure the extent to which these forms of collaboration are possible. For concurrent work that addresses these topics, also see Jiang et al. [2021].</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: An overview of the "flow" of the human-model collaboration experiments. The human gives a description of the desired program and then guides the model toward the correct solution via dialog.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Percent of problems solved as the number of human dialog interventions increases. With 4 interventions, the solve rate increases from $30 \%$ to over $65 \%$. Except for the purple horizontal baseline (which corresponds to 5 samples from the model), all pass-rates in this figure were computed using a single sample from the model.</p>
<h1>5.1 Human Feedback Improves Synthesis Performance</h1>
<p>We select 50 problems from the edited MBPP dataset (see Section 4.9) and assign human participants to collaborate with the model to solve these tasks using only natural language dialog. The model is prompted as in the experiments in Section 4, but with few-shot examples and priming asserts given as dialog turns: for instance "I need to write a function called [function name]. Here's a description: [docstring].". The model sees several examples of collaboration in this few-shot prompt, after which the dialog with the participant begins. Participants were instructed to provide one-sentence hints or corrections to the model that would guide the model toward finding the correct solution. The hints were allowed to contain references to Python identifiers, but not large snippets of code, and participants were limited to a maximum of four hints. The full set of instructions given to participants can be found in Appendix A.2.
The results of this experiment (Figure 13) support the hypothesis that these models can improve or correct code based on human feedback. Counting all four dialog turns, the fraction of problems solved is increased from $30 \%$ to over $65 \%$, and counting only one, from $30 \%$ to $55 \%$. The purple horizontal line in Figure 13 corresponds to the fraction of problems solved when the model is allowed to use five samples instead of 1 , so there is a sense in which a one-sentence human correction is worth more than a five-fold increase in the number of samples allowed. Furthermore, human feedback allows the model to solve 10 problems that it was totally unable to solve without human assistance. There are, however, diminishing marginal returns to human feedback, as might be expected.
Figure 14 shows two example interactions with the model which allowed it to solve previously unsolved problems. In the first, a human was able to point out mistakes the model had made as a result of an under-specified natural language prompt (mistakes a human was able to infer by looking closely at the assert statements). In the second example, the model predicts an overly complicated solution, which a human is able to tweak and correct over a number of follow-up turns.</p>
<h3>5.2 Qualitative Analysis of Human-Model Dialogs</h3>
<p>To gain a better understanding of how useful large models can be as assistive tools, we conducted a qualitative analysis of success and failure modes using the interactions collected for the above experiment, resulting in the following broad themes:
Humans are able to clarify under-specified prompts by examining test cases. Many problems do not precisely specify every detail of the semantics of the desired program. For example, one question in the original dataset asks</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Two example human-model interactions. User text is purple and model text is blue. Left: an under-specified problem in which the user was able to point out corrections based on the example input. Right: a longer and more complex example in which the model makes small adjustments in response to feedback. Few-shot prompting examples are elided for compactness. Additional examples are shown in the appendix.
the user to "write a function to count the most common words in a dictionary", but the test cases make clear that the function should only return the 4 most common words, and in descending order by frequency. The model, even when given these test cases, was unable to 'understand' those requirements. A human was able to tell the model to sort the output, reverse its order, and return only the top 4 , which allowed the model to solve the problem. This interaction is shown in Figure 14 (Left).
Humans can often correct small context errors (often import and identifier errors). The model also frequently makes import or identifier errors, for example by forgetting to import a module it used in its code. Typically, a single dialog turn was enough for humans to help the model correct these errors (for example, by saying "Great, but you never imported the re module."). Humans also tended to help the model fix variable misuse errors (in which, for instance, an un-defined variable is referenced) in one turn. We also observed the model returning True instead of False, which a single dialog turn could correct.
The model can lose track of context or previously referenced code. We observed several cases where the model modified its code in an incorrect way in response to user feedback, but struggled to revert it or incorporate pieces of prior results. For instance, it rarely responded well to "No, please go back to the previous response." or "Great, but you need to use the function signature from your first response.". This problem became more pronounced as the number of dialog turns increased.</p>
<h1>6 Program Execution Results</h1>
<p>A common criticism of language models like the ones we use in this paper is that they learn statistical correlations between tokens without an underlying world-model or mechanism for systematic reasoning, and therefore do not understand the meaning of what they are describing [Bender and Koller, 2020]. On the other hand, recent work has provided evidence that, in some natural language contexts, pre-trained Transformers are able to implicitly construct approximate representations of the semantics of the worlds they describe in text [Li et al., 2021]. We would like to ask a related question for code: Do pre-trained language models understand the underlying semantic state of the code they are synthesizing? Computer programs are an especially promising domain for this kind of analysis, because unlike natural language, the semantics of a program can be defined precisely, by specifying the result of its execution.
In this section, we investigate to what extent our models have this understanding by asking them to predict the result of executing the ground-truth programs from MBPP on test inputs. We also study how this execution ability is related to synthesis performance.
We are specifically interested in the following questions:</p>
<ul>
<li>Can models execute Python functions, and how does execution performance depend on what information is in the prompt?</li>
<li>How does fine-tuning on execution tasks impact the performance of execution?</li>
<li>How does fine-tuning on execution tasks impact the performance on synthesis tasks?</li>
</ul>
<p>In asking these questions, we are inspired in part by previous work that specifically trains deep architectures to learn how to execute programs [Zaremba and Sutskever, 2014, Bieber et al., 2020]. In contrast to that work, our goal is to use the learning-to-execute problem as a lens to understand the capabilities of large language models over source code, rather than to obtain the best model of program execution per se. To answer these questions, we conduct a series of experiments, focusing on our largest model (137B).</p>
<h3>6.1 Few-Shot Execution Performance is Poor</h3>
<p>In our first experiment, we evaluate the few-shot performance of our 137B model on code execution. For each task, the MBPP dataset contains ground truth source code, a natural language description, and three input-output examples. We task the model with predicting the output of the ground truth program if it is run on one of the given test case inputs. Since this performance might be sensitive to details of the prompt, we investigate how execution performance depends on that information. Specifically, we ablate the presence or absence of the ground truth code, natural language description, and test cases in the prompt. This results in seven different prompt configurations, as shown in Table 5. ${ }^{3}$ The prompt templates for each prompt condition can be found in Listing 1 in the Appendix. We query models using a sampling temperature $T=0.0$, which is equivalent to greedy decoding.
In our first set of experiments (Table 5, left), we evaluate correctness on a single test case. For prompt configurations requiring test cases, we use the two remaining test cases. Overall execution performance is relatively poor, with accuracy never exceeding $29 \%$ for any prompt type. Results indicate that including test cases in the prompt seems to help more than any other individual component. Including test cases and natural language descriptions in the prompt lead to the highest overall performance-higher than using the code itself. Because the code unambiguously describes the semantics, whereas test cases do not, this suggests that models are in some sense not really "reading" the source code and using it to execute. Models trained on general text corpora may be better at inducing patterns from as few as two input-output examples than they are at predicting the execution of code.
Evaluating on only one test case might provide a noisy overestimate of functional correctness. Therefore, our second set of experiments (Table 5, right) investigates whether the models can correctly infer the output for multiple test cases. For this experiment, we only judge a sample correct if it gives the correct output for both test cases. Accuracy for testing on two examples is lower than for one example. For the prompt configurations in Table 5 that do not include test cases, the prompt does not change between this experiment and the last one, so the drop in performance across these configurations must be due to the model failing to generalize across test-inputs when predicting the execution result.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Execution results as information in the prompt is varied. Left: Testing on 1 held-out test case. Prompts with test cases contain 2 of them. Right: Testing simultaneously on 2 held-out test cases. Prompts with test cases contain a single one. Across multiple configurations, execution performance is greatest when the prompt contains test cases. Furthermore, fine-tuning increases accuracy for code execution, but this effect appears to be washed out by the presence of test cases in the prompt.</p>
<table>
<thead>
<tr>
<th></th>
<th>2 prompt examples, 1 test example</th>
<th></th>
<th>1 prompt example, 2 test examples</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Few-shot</td>
<td>Fine-tuned</td>
<td>Few-shot</td>
<td>Fine-tuned</td>
</tr>
<tr>
<td>code</td>
<td>16.4%</td>
<td>20.8%</td>
<td>8.6%</td>
<td>9.0%</td>
</tr>
<tr>
<td>code+NL desc.+examples</td>
<td>24.6%</td>
<td>23.2%</td>
<td>9.8%</td>
<td>8.4%</td>
</tr>
<tr>
<td>code+NL desc.</td>
<td>15.6%</td>
<td>20.6%</td>
<td>9.0%</td>
<td>8.2%</td>
</tr>
<tr>
<td>code+examples</td>
<td>28.8%</td>
<td>27.4%</td>
<td>11.6%</td>
<td>12.0%</td>
</tr>
<tr>
<td>NL desc.+examples</td>
<td>28.6%</td>
<td>28.2%</td>
<td>12.8%</td>
<td>13.0%</td>
</tr>
<tr>
<td>NL desc.</td>
<td>17.6%</td>
<td>18.8%</td>
<td>8.4%</td>
<td>8.6%</td>
</tr>
<tr>
<td>examples</td>
<td>27.2%</td>
<td>26.2%</td>
<td>10.2%</td>
<td>13.0%</td>
</tr>
</tbody>
</table>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Synthesis performance of models fine-tuned on the execution task. While synthesis performance of the 8B model is not improved by fine-tuning on the execution task, the 137B model achieves slightly better synthesis performance when fine-tuned on execution, suggesting that larger models may be better able to transfer knowledge from execution training to synthesis evaluation.</p>
<h3>6.2 Fine-tuning on Execution Slightly Improves Execution Performance</h3>
<p>To test the effects of fine-tuning on execution performance, we construct a fine-tuning corpus for execution, built using the 374 training and 90 validation tasks used for synthesis fine-tuning (Section 2.1). For each task, we include an execution trial for each of the 7 prompt configurations from Section 6.1. We also vary the number of test cases in the prompt and test cases to test on (also as in Section 6.1). This gives a total of 14 related data-points for each task. Overall, this fine-tuning corpus consists of 14 × 374 = 5236 training data-points and 14 × 90 = 1260 validation data-points. We fine-tune for 100 steps using a batch size of 8192 tokens per batch.</p>
<p>Our fine-tuning results are shown in Table 5. Our results indicate that fine-tuning improves the performance of code execution, but that this improvement is not present when test cases are part of the prompt. In particular, there is a positive difference between fine-tuned and few-shot performance only for prompts which contain source code but do not contain test cases.</p>
<h3>6.3 Fine-tuning on Execution has a Small Effect on Synthesis Performance</h3>
<p>We also investigate how models fine-tuned on execution perform on the program synthesis task which is the main focus of this paper. We perform the few-shot program synthesis evaluation from Section 4 on the models fine-tuned on execution from Section 6.2 above. As in Section 4, we perform few-shot prompting with k = 3 example synthesis tasks in the prompt, and include all three example asserts for each task.</p>
<p>We perform this experiment using the 8B, 68B, and 137B models (Figure 15). For the 8B model, fine-tuning on execution prompts does not increase performance beyond the few-shot performance. Performance of the 137B model shows a small improvement when fine-tuned on the execution dataset, of about 2.3% more samples per problem solving</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16: An example of a simple MathQA-style problem used as an additional test. We first verified that the model can produce a correct solution. Next, given a correct solution, the model was asked "Can you explain your solution step by step and tell how did you apply the minus sign?". The three responses at the bottom of the listing are independently sampled from the 137B model, fine-tuned for the Python MathQA task. The first two are correct, whereas the final response descends into mathematical rambling.
the task and $3.6 \%$ more tasks solved by any sample, compared to fine-tuning on the synthesis dataset). We suspect that training on more detailed execution data [Zaremba and Sutskever, 2014, Bieber et al., 2020] may further improve performance.</p>
<h1>7 MathQA Results</h1>
<p>We also evaluate our models on the MathQA and MathQA-Python datasets. The code in the MathQA dataset is different from MBPP, making less use of control flow and of the Python standard library, while the natural language is more complex. We experiment with both the domain-specific-language of the formulas in the original MathQA dataset, which we call MathQA-DSL, and the MathQA-Python dataset described in Section 2.2. As on the MBPP data (Section 4), we evaluate synthesis performance in both the few-shot prompting and the fine-tuning setting. We report accuracy in terms of functional correctness, that is, whether the program output by the model returns the correct answer to the word problems.
The results are summarized in Table 6. We find that the few-shot accuracy is $33.4 \%$ for the 137 B model on the Python-formatted dataset. The fine-tuned models achieve very high accuracy: the best-performing model (137B on the DSL-formatted dataset) achieves $83.8 \%$ accuracy; see Table 6. Further, as with MBPP we can interpret the percentage</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: An example of a harder MathQA test problem. Without the parenthesized hint, it is solved by the 137B model in less than $10 \%$ of samples. With the hint, it is solved $40 \%$ of the time. Correct prompting can elicit a line-by-line explanation of the solution. The model answers are marked in color. Notice that the model only indirectly follows the hint and that the explanation with regard to $t 0$ is incorrect: $t 0$ divided by 100 is the volume of acid in the original solution (in litres). Explanations were obtained in a zero-shot mode and they contain various inaccuracies.
of samples solving each task as a measure of the model's confidence in its predictions. In Figure 18, we see that the finetuned models tend to have higher confidence, and the few-shot models much less so.
The few-shot models perform better on MathQA-Python compared to MathQA-DSL, which is expected because the MathQA DSL is unlikely to be similar to anything in the pre-training set. In contrast, the fine-tuned models achieve slightly higher accuracy on the DSL-formatted dataset compared to the Python-formatted dataset, indicating that the fine-tuning dataset we use has sufficiently many examples for the model to overcome its lack of familiarity with the DSL. This has promising implications for tasks like trying to teach a new programming language to a pre-trained model.
We also conducted an initial qualitative exploration of whether the model could respond to hints and explain its reasoning. Figure 16 shows an example for which the model is capable not only of solving MathQA-style problems, but also of carrying on a dialog about the proposed solution. Figure 17 shows how providing a hint to the model can in some cases increase the fraction of samples that solve the problem. Namely, without the hint ("calculate the volume of acid in the solution"), the 137B model fine-tuned on the Python code was able to solve the problem in fewer than $10 \%$ of samples. With the hint, the model samples correct answers $40 \%$ of the time. Moreover, we can elicit a line-by-line explanation of the solution with appropriate prompting (see blue section in Figure 17). Though we think these results are promising, we do not claim to have done a thorough evaluation of them here. They are presented more as a jumping-off-point for future work.</p>
<p>Table 6: MathQA accuracy for 8B, 68B and 137B models, measured by the percentage of tasks on the test set that are solved by any sample. Fine-tuning greatly increases performance for both the original DSL and the Python variant of the dataset. The gap between few-shot and fine-tuning performance is much larger for MathQA than for MBPP, but this is to be expected, because the fine-tuning dataset for the former is much larger.</p>
<table>
<thead>
<tr>
<th></th>
<th>MathQA-DSL</th>
<th></th>
<th>MathQA-Python</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Few-shot</td>
<td>Fine-tuned</td>
<td>Few-shot</td>
<td>Fine-tuned</td>
</tr>
<tr>
<td>8B</td>
<td>16.5%</td>
<td>79.0%</td>
<td>12.5%</td>
<td>74.7%</td>
</tr>
<tr>
<td>68B</td>
<td>16.8%</td>
<td>82.8%</td>
<td>22.3%</td>
<td>79.5%</td>
</tr>
<tr>
<td>137B</td>
<td>16.7%</td>
<td>83.8%</td>
<td>33.4%</td>
<td>81.2%</td>
</tr>
</tbody>
</table>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 18: Fraction of samples solving each MathQA task represented as a histogram and a graph. In the case of the histogram each bucket shows the number of test tasks solved by the model (out of the total of all 1883 test tasks). The x-axis shows buckets [1, 9], [10, 19], [20, 29], ...that refer to the percentage of samples solved by the model. In particular tall bars in the first bucket [1, 9] mean that for majority of tasks only between 1 and 9 percent of samples were correct. In the figure on the right the x-axis represents the index of a particular task and the y-axis represents the fraction of samples for that task that actually solved the task. Though curves in this figure are substantially different then the ones in analogous Figure 4, the conclusion remains the same: the area under the curve increases as parameters are added to the model. This means that more tasks were solved by <em>any</em> sample, but also that bigger models can more reliably solve the "easier" problems.</p>
<h2>8 Related Work</h2>
<p>Our work is inspired by the long line of previous work on neural language models of natural language text [Mikolov et al., 2010, Sutskever et al., 2011, Józefowicz et al., 2016, Dai and Le, 2015, Peters et al., 2018, Howard and Ruder, 2018], especially recent large Transformer models [Radford et al., 2018, Brown et al., 2020].</p>
<h3>8.1 Machine Learning for Program Synthesis</h3>
<p>In the long history of program synthesis, methods have included deductive approaches, approaches based on enumerative and stochastic search, and constraint solving; for surveys, see Gulwani et al. [2017], Solar-Lezama [2018]. One important application of these methods has been in end-user programming, for example, to synthesize string manipulation programs in spreadsheets [Gulwani, 2011]. Many current systems rely on reducing the synthesis problem to a satisfiability problem, for example, Solar-Lezama et al. [2006] and Torlak and Bodik [2013].</p>
<p>Machine learning methods for program synthesis aim to learn cues from the problem description or from corpora of existing programs that help to write programs. Balog et al. [2017] use a neural network to predict properties, such as which functions will be called, of the target program from the input-output examples; these predictions can then be used</p>
<p>to guide a search over programs. Devlin et al. [2017] treated program synthesis as a sequence-to-sequence problem, mapping from the problem description to a description of the program in a spreadsheet domain. DreamCoder [Ellis et al., 2020] relaxes the requirement of defining a DSL, by learning a library that is useful for solving a training set of synthesis problems. Execution-guided synthesis methods execute the partial programs produced during synthesis, using the intermediate values to guide the search; learning methods for execution-guided synthesis include Zohar and Wolf [2018], Chen et al. [2019a], Ellis et al. [2019], Odena et al. [2020].
Many methods for program synthesis, both logic-based and learning-based, have been restricted to DSLs, but there have been some exceptions. For example, BAYOU generates API-heavy code in Java using a latent-variable probabilistic model [Murali et al., 2018]. Also, several different methods have been proposed for the problem of mapping a natural language description to code in general-purpose languages like Python [Ling et al., 2016, Yin and Neubig, 2017, Iyer et al., 2018].
Neural program induction methods are deep network architectures that aim to learn algorithms from input-output examples, by structuring the network in a way that corresponds to mathematical models of computation like Turing machines [Graves et al., 2014, Kaiser and Sutskever, 2016, Kurach et al., 2016, Graves et al., 2016]. This is a very different line of work from program synthesis, because program induction methods do not attempt to produce a program. Instead, they learn a neural network that maps directly from the input of the desired program to its output.</p>
<h1>8.2 Machine Learning for Software Engineering</h1>
<p>Over the past decade, a line of work has explored machine learning for software engineering, which applies machine learning methods to large corpora of source code, with the aim of using the models to develop tools for various tasks in software engineering. For an overview of machine learning methods applied to source code, see Allamanis et al. [2018a], or the more recent living literature review website [Allamanis, 2021].
Early work applied statistical $n$-gram models [Hindle et al., 2012, Allamanis and Sutton, 2013a] and neural networks [Maddison and Tarlow, 2014, Raychev et al., 2014] to code. Raychev et al. [2015] presented a method to predict program properties using a graph-structured conditional random field, which they applied to deobfuscate Javascript code by predicting names and a small set of types. Subsequent research over the following decade introduced deep learning methods for a variety of software engineering tasks.
Code completion has been a particular focus of interest [Raychev et al., 2016, Karampatsis et al., 2020, Svyatkovskiy et al., 2020, Kim et al., 2020]. Methods aim improving code readability by asking a model trained on a code corpus with good style to predict names of variables and methods in new code [Raychev et al., 2015, Allamanis et al., 2014, Alon et al., 2019]. Several methods have been proposed to do machine learning for type inference, for example, to add types to untyped code, such as when converting Javascript to Typescript [Hellendoorn et al., 2018, Pandi et al., 2020, Pradel et al., 2020, Wei et al., 2020]. Models trained over natural language and code have been applied within tools for improving comment quality and relevance [Louis et al., 2020, Panthaplackel et al., 2021]. Porting programs across languages has been treated as a learning problem similar to machine translation [Roziere et al., 2020, Nguyen et al., 2013, Karaivanov et al., 2014]. Program repair is the problem of automatically fixing bugs in programs, often based on a test suite [Le Goues et al., 2012, Long and Rinard, 2016]. Many learning methods have been proposed for program repair [Allamanis et al., 2018b, Tarlow et al., 2019, Hellendoorn et al., 2019, Dinella et al., 2019, Yasunaga and Liang, 2020, Chen et al., 2019b, Pradel and Sen, 2018].
Several pre-trained models for code have shown to be effective for transfer learning across software engineering tasks, including CuBERT [Kanade et al., 2020], CodeBERT [Feng et al., 2020], PyMT5 [Clement et al., 2020], code2vec [Alon et al., 2019], and other T5 models trained on code [Mastropaolo et al., 2021].</p>
<h3>8.3 Benchmarks for Machine Learning over Source Code</h3>
<p>Broadly, we identify three kinds of benchmark suites for machine learning over source code. First, closed-domain benchmarks for program synthesis ask systems to generate programs in a domain-specific language from a specification such as a logical formula or input-output examples. The most notable of these is the SyGuS competition [Alur et al., 2013], which includes tasks such as generating string transformations and bit-vector manipulations. Although the restriction to domain-specific languages is useful for building tractable systems, our benchmarks aim to evaluate program synthesis methods for general-purpose programming languages used by people to develop software.
Benchmarks for machine learning for software engineering are often assembled from corpora of open source projects, such as from Github. Benchmarks have been proposed for software engineering tasks including code completion</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We note that the prompt types which do not contain source code should probably not be referred to as execution tasks; for example, the case where only input-output examples are included is equivalent to what has been dubbed "neural program induction". [Devlin et al., 2017]&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
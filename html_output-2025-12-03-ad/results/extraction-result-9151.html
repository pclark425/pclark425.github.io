<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9151 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9151</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9151</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-265609385</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01291v1.pdf" target="_blank">Opportunities for retrieval and tool augmented large language models in scientific facilities</a></p>
                <p><strong>Paper Abstract:</strong> Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics. However, these facility and instrument upgrades come with a significant increase in complexity. Driven by more exacting scientific needs, instruments and experiments become more intricate each year. This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments. Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage. Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation. With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures. With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments. By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities’ users and accelerate scientific output.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9151.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9151.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Aware Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework combining an LLM, conversational memory, semantic document retrieval, and instrument-control tools to answer technical queries and to drive scientific instruments conversationally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>framework (GPT-3.5 Turbo and Vicuna used as backends)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CALMS is a retrieval- and tool-augmented agent architecture that supplies LLM prompts with top-N retrieved document chunks (embeddings), a short conversational memory (moving window K=6), specified tool APIs, and uses Chain-of-Thought/ReAct prompting to enable action. In this work CALMS was exercised using OpenAI GPT-3.5 Turbo (175B parameters, context windows 4k/16k) and an open-source Vicuna variant (lmsys/vicuna-13b-v1.5, finetuned Llama-2 family).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Synchrotron X-ray experiments / facility user support (materials characterization, beamline operations, tomography, diffractometry)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-mediated simulation/automation of experiment workflows including: (1) answering technical design and operational Q&A about facility instruments; (2) parsing user requests and executing instrument-control tool calls (example: compute lattice constants + motor positions and perform diffractometer move); (3) guiding tomographic/diffraction scan procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hand-graded rubric combining: relevance (yes/no), absence of hallucination (yes/no), and completeness (0–5 pts) for an overall score (max 5). For tool execution, execution correctness graded 0/1/2 (not executable / minor errors / executable); see Methods section.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Qualitative/graded results: With retrieval context, GPT-3.5 Turbo produced consistently relevant and truthful answers and achieved multiple completeness scores of 5 (i.e., provided all necessary information); it successfully executed a real diffractometer workflow via API calls. Vicuna produced relevant answers less completely (fewer high completeness scores) and could not reliably execute the structured tool-call workflow. Without context (no retrieval), both models exhibited frequent hallucinations and irrelevant answers. No aggregated numerical accuracy % is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence/quality of retrieved context (document chunks); model type and pretraining/fine-tuning (GPT-3.5 Turbo vs Vicuna); instruction fine-tuning and RLHF; prompt engineering (Chain-of-Thought, ReAct style prompts); ability of model to produce required structured output (valid JSON for LangChain Structured ReAct); context-window limits (how much documentation can be provided); conversational memory window size (K=6); embedding retrieval hyperparameters (top N=4 chunks); decoding parameters (temperature, top_p); tooling integration and API reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared internally between: (a) GPT-3.5 Turbo vs Vicuna (open-source) and (b) with retrieval/context vs without retrieval. GPT-3.5 outperformed Vicuna on completeness and tool execution; models with retrieval/context outperformed models without context (which hallucinated frequently). No external human-expert numeric baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent hallucinations and irrelevant but superficially truthful answers when no domain context is provided; open-source Vicuna failed to consistently follow required structured syntax (JSON) for tool calls and thus could not execute the automated workflow; limited LLM prompt/context size prevents including all facility documentation; non-deterministic outputs can vary between runs; occasional model confusion of domain-specific terms leading to domain-mismatched answers (e.g., clinical CT vs beamline tomography).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use retrieval-augmented prompts and tool-augmentation to reduce hallucination; prefer models with stronger instruction tuning/RLHF for tool usage; invest in improved open-source fine-tuning and tooling to close the gap with closed-source models; adopt Chain-of-Thought and ReAct prompting; tune embedding/chunking and conversational memory strategies; scale model/data and apply SELF-Instruct or instruction fine-tuning to improve performance; integrate decades of e-log data and robust tool APIs to enable more reliable autonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9151.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9151.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pretrained Transformer 3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, instruction-tuned LLM from OpenAI (predecessor class to GPT-4) used here as the primary backend for CALMS to answer Q&A and to execute instrument-control workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a 175 billion parameter Transformer-based LLM, available with context windows of 4,096 or 16,384 tokens; used via API with configurable decoding parameters (temperature, top_p). Instruction-tuned and deployed in enterprise/cloud settings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Beamline operations and materials characterization at synchrotron user facilities (e.g., APS) — experimental design, operational guidance, and instrument control (diffractometer moves, tomography procedures).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>1) Textual simulation of operational procedures and experimental design Q&A for facility users; 2) parsing user natural-language requests into structured tool calls to external services (Materials Project API) and instrument-control software (SPEC) to compute and effect motor moves for diffractometer Bragg peaks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hand-graded rubric combining relevance (yes/no), absence of hallucination (yes/no), and completeness (0–5), plus execution correctness when running tools (0/1/2 scale described in Methods).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported qualitatively: with retrieval/context GPT-3.5 produced consistently relevant, truthful, and more complete answers than Vicuna, scoring multiple 5s on completeness; successfully executed the diffractometer automation workflow on a real beamline via API (i.e., end-to-end success in that demo). No aggregate numeric accuracy percentages provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Availability and quality of retrieval-provided context; prompt engineering and use of Chain-of-Thought/ReAct; ability to format structured JSON for tool APIs; model's instruction tuning and RLHF; context window size; decoding parameters (temperature/top_p).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to Vicuna (open-source) and to the model-without-context condition. GPT-3.5 outperformed Vicuna on completeness and was able to execute the structured tool workflow while Vicuna could not. Models with retrieval context outperformed models without context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While able to execute workflow in the demo, outputs remain non-deterministic; hallucinations occur when context is absent; constrained by prompt/context size; reliance on correct tool-API definitions and on the model producing syntactically valid structured calls; privacy/hosting considerations (model is closed-source/cloud-hosted).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use retrieval augmentation and explicit tool descriptions; enforce structured output (JSON schemas) and validate model outputs programmatically; prefer instruction- and RLHF-tuned models for tool use; improve documentation chunking and embedding retrieval; invest in techniques (SELF-Instruct, Chain-of-Thought) and scaling to further improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9151.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9151.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna (lmsys/vicuna-13b-v1.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna 13B (lmsys/vicuna-13b-v1.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-following LLM (finetuned from Llama-family checkpoints) evaluated as an alternative backend to the closed-source model; used for Q&A and tool-augmented experiments within CALMS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>lmsys/vicuna-13b-v1.5 (Vicuna)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source 13B-parameter instruction-finetuned model (reported as vicuna-13b-v1.5), derived from Llama 2 and finetuned on supervised instruction data (ShareGPT conversations); variant with 16k context window (Vicuna 1.5-16k) referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Same as above: synchrotron beamline experimental design and operations (facility-specific Q&A, tomographic procedure guidance, instrument operation assistance).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering facility-specific technical Q&A and attempting to parse and call tools to perform instrument-control workflows (same target tasks as GPT-3.5 in CALMS).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same hand-graded rubric (relevance, hallucination absence, completeness 0–5) and tool-execution correctness grading as described in Methods.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Qualitative outcome: Vicuna, when provided retrieval context, often produced relevant and truthful answers but scored lower on completeness compared to GPT-3.5 (fewer 5s). Vicuna failed to reliably produce the exact structured JSON required by the LangChain Structured ReAct tool interface and thus could not consistently execute the diffractometer automation; without context Vicuna hallucinated or returned domain-mismatched but truthful-seeming answers (e.g., clinical CT instead of beamline tomography). No numeric accuracy rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and finetuning dataset/quality; instruction fine-tuning differences relative to GPT-3.5; ability to follow strict structured-output constraints (JSON); presence/quality of retrieved context; prompt engineering and Chain-of-Thought/ReAct usage; context window configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared against GPT-3.5 Turbo and against the no-context condition. Under the same retrieval/context setup, Vicuna underperformed GPT-3.5 on completeness and tool execution; without retrieval, both models fared poorly but Vicuna produced more domain-mismatched answers in examples given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Could not consistently adhere to the strict JSON schema required for structured tool invocation (LangChain Structured ReAct), preventing automated execution; lower completeness in answers; more prone to domain confusion in specific operational queries when context absent.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>With additional investment in fine-tuning, instruction training, and tooling, open-source models like Vicuna could close the performance gap; implement stricter output validation and error-correction layers when relying on open-source models for tool calls; use retrieval augmentation and better prompt templates to improve structured output consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tool augmented language models <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chatgpt for robotics: Design principles and model abilities <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 1)</em></li>
                <li>Domain-specific ChatBots for Science using Embeddings <em>(Rating: 1)</em></li>
                <li>Ingrained: An Automated Framework for Fusing Atomic-Scale Image Simulations into Experiments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9151",
    "paper_id": "paper-265609385",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "CALMS",
            "name_full": "Context-Aware Language Model for Science",
            "brief_description": "A framework combining an LLM, conversational memory, semantic document retrieval, and instrument-control tools to answer technical queries and to drive scientific instruments conversationally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "framework (GPT-3.5 Turbo and Vicuna used as backends)",
            "model_description": "CALMS is a retrieval- and tool-augmented agent architecture that supplies LLM prompts with top-N retrieved document chunks (embeddings), a short conversational memory (moving window K=6), specified tool APIs, and uses Chain-of-Thought/ReAct prompting to enable action. In this work CALMS was exercised using OpenAI GPT-3.5 Turbo (175B parameters, context windows 4k/16k) and an open-source Vicuna variant (lmsys/vicuna-13b-v1.5, finetuned Llama-2 family).",
            "scientific_subdomain": "Synchrotron X-ray experiments / facility user support (materials characterization, beamline operations, tomography, diffractometry)",
            "simulation_task": "Text-mediated simulation/automation of experiment workflows including: (1) answering technical design and operational Q&A about facility instruments; (2) parsing user requests and executing instrument-control tool calls (example: compute lattice constants + motor positions and perform diffractometer move); (3) guiding tomographic/diffraction scan procedures.",
            "evaluation_metric": "Hand-graded rubric combining: relevance (yes/no), absence of hallucination (yes/no), and completeness (0–5 pts) for an overall score (max 5). For tool execution, execution correctness graded 0/1/2 (not executable / minor errors / executable); see Methods section.",
            "simulation_accuracy": "Qualitative/graded results: With retrieval context, GPT-3.5 Turbo produced consistently relevant and truthful answers and achieved multiple completeness scores of 5 (i.e., provided all necessary information); it successfully executed a real diffractometer workflow via API calls. Vicuna produced relevant answers less completely (fewer high completeness scores) and could not reliably execute the structured tool-call workflow. Without context (no retrieval), both models exhibited frequent hallucinations and irrelevant answers. No aggregated numerical accuracy % is reported.",
            "factors_affecting_accuracy": "Presence/quality of retrieved context (document chunks); model type and pretraining/fine-tuning (GPT-3.5 Turbo vs Vicuna); instruction fine-tuning and RLHF; prompt engineering (Chain-of-Thought, ReAct style prompts); ability of model to produce required structured output (valid JSON for LangChain Structured ReAct); context-window limits (how much documentation can be provided); conversational memory window size (K=6); embedding retrieval hyperparameters (top N=4 chunks); decoding parameters (temperature, top_p); tooling integration and API reliability.",
            "comparison_baseline": "Compared internally between: (a) GPT-3.5 Turbo vs Vicuna (open-source) and (b) with retrieval/context vs without retrieval. GPT-3.5 outperformed Vicuna on completeness and tool execution; models with retrieval/context outperformed models without context (which hallucinated frequently). No external human-expert numeric baseline reported.",
            "limitations_or_failure_cases": "Frequent hallucinations and irrelevant but superficially truthful answers when no domain context is provided; open-source Vicuna failed to consistently follow required structured syntax (JSON) for tool calls and thus could not execute the automated workflow; limited LLM prompt/context size prevents including all facility documentation; non-deterministic outputs can vary between runs; occasional model confusion of domain-specific terms leading to domain-mismatched answers (e.g., clinical CT vs beamline tomography).",
            "author_recommendations_or_insights": "Use retrieval-augmented prompts and tool-augmentation to reduce hallucination; prefer models with stronger instruction tuning/RLHF for tool usage; invest in improved open-source fine-tuning and tooling to close the gap with closed-source models; adopt Chain-of-Thought and ReAct prompting; tune embedding/chunking and conversational memory strategies; scale model/data and apply SELF-Instruct or instruction fine-tuning to improve performance; integrate decades of e-log data and robust tool APIs to enable more reliable autonomy.",
            "uuid": "e9151.0",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-3.5 Turbo",
            "name_full": "Generative Pretrained Transformer 3.5 Turbo",
            "brief_description": "A closed-source, instruction-tuned LLM from OpenAI (predecessor class to GPT-4) used here as the primary backend for CALMS to answer Q&A and to execute instrument-control workflows.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "Described in the paper as a 175 billion parameter Transformer-based LLM, available with context windows of 4,096 or 16,384 tokens; used via API with configurable decoding parameters (temperature, top_p). Instruction-tuned and deployed in enterprise/cloud settings.",
            "scientific_subdomain": "Beamline operations and materials characterization at synchrotron user facilities (e.g., APS) — experimental design, operational guidance, and instrument control (diffractometer moves, tomography procedures).",
            "simulation_task": "1) Textual simulation of operational procedures and experimental design Q&A for facility users; 2) parsing user natural-language requests into structured tool calls to external services (Materials Project API) and instrument-control software (SPEC) to compute and effect motor moves for diffractometer Bragg peaks.",
            "evaluation_metric": "Hand-graded rubric combining relevance (yes/no), absence of hallucination (yes/no), and completeness (0–5), plus execution correctness when running tools (0/1/2 scale described in Methods).",
            "simulation_accuracy": "Reported qualitatively: with retrieval/context GPT-3.5 produced consistently relevant, truthful, and more complete answers than Vicuna, scoring multiple 5s on completeness; successfully executed the diffractometer automation workflow on a real beamline via API (i.e., end-to-end success in that demo). No aggregate numeric accuracy percentages provided.",
            "factors_affecting_accuracy": "Availability and quality of retrieval-provided context; prompt engineering and use of Chain-of-Thought/ReAct; ability to format structured JSON for tool APIs; model's instruction tuning and RLHF; context window size; decoding parameters (temperature/top_p).",
            "comparison_baseline": "Compared directly to Vicuna (open-source) and to the model-without-context condition. GPT-3.5 outperformed Vicuna on completeness and was able to execute the structured tool workflow while Vicuna could not. Models with retrieval context outperformed models without context.",
            "limitations_or_failure_cases": "While able to execute workflow in the demo, outputs remain non-deterministic; hallucinations occur when context is absent; constrained by prompt/context size; reliance on correct tool-API definitions and on the model producing syntactically valid structured calls; privacy/hosting considerations (model is closed-source/cloud-hosted).",
            "author_recommendations_or_insights": "Use retrieval augmentation and explicit tool descriptions; enforce structured output (JSON schemas) and validate model outputs programmatically; prefer instruction- and RLHF-tuned models for tool use; improve documentation chunking and embedding retrieval; invest in techniques (SELF-Instruct, Chain-of-Thought) and scaling to further improve reliability.",
            "uuid": "e9151.1",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Vicuna (lmsys/vicuna-13b-v1.5)",
            "name_full": "Vicuna 13B (lmsys/vicuna-13b-v1.5)",
            "brief_description": "An open-source instruction-following LLM (finetuned from Llama-family checkpoints) evaluated as an alternative backend to the closed-source model; used for Q&A and tool-augmented experiments within CALMS.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "lmsys/vicuna-13b-v1.5 (Vicuna)",
            "model_description": "An open-source 13B-parameter instruction-finetuned model (reported as vicuna-13b-v1.5), derived from Llama 2 and finetuned on supervised instruction data (ShareGPT conversations); variant with 16k context window (Vicuna 1.5-16k) referenced.",
            "scientific_subdomain": "Same as above: synchrotron beamline experimental design and operations (facility-specific Q&A, tomographic procedure guidance, instrument operation assistance).",
            "simulation_task": "Answering facility-specific technical Q&A and attempting to parse and call tools to perform instrument-control workflows (same target tasks as GPT-3.5 in CALMS).",
            "evaluation_metric": "Same hand-graded rubric (relevance, hallucination absence, completeness 0–5) and tool-execution correctness grading as described in Methods.",
            "simulation_accuracy": "Qualitative outcome: Vicuna, when provided retrieval context, often produced relevant and truthful answers but scored lower on completeness compared to GPT-3.5 (fewer 5s). Vicuna failed to reliably produce the exact structured JSON required by the LangChain Structured ReAct tool interface and thus could not consistently execute the diffractometer automation; without context Vicuna hallucinated or returned domain-mismatched but truthful-seeming answers (e.g., clinical CT instead of beamline tomography). No numeric accuracy rates provided.",
            "factors_affecting_accuracy": "Model size and finetuning dataset/quality; instruction fine-tuning differences relative to GPT-3.5; ability to follow strict structured-output constraints (JSON); presence/quality of retrieved context; prompt engineering and Chain-of-Thought/ReAct usage; context window configuration.",
            "comparison_baseline": "Directly compared against GPT-3.5 Turbo and against the no-context condition. Under the same retrieval/context setup, Vicuna underperformed GPT-3.5 on completeness and tool execution; without retrieval, both models fared poorly but Vicuna produced more domain-mismatched answers in examples given.",
            "limitations_or_failure_cases": "Could not consistently adhere to the strict JSON schema required for structured tool invocation (LangChain Structured ReAct), preventing automated execution; lower completeness in answers; more prone to domain confusion in specific operational queries when context absent.",
            "author_recommendations_or_insights": "With additional investment in fine-tuning, instruction training, and tooling, open-source models like Vicuna could close the performance gap; implement stricter output validation and error-correction layers when relying on open-source models for tool calls; use retrieval augmentation and better prompt templates to improve structured output consistency.",
            "uuid": "e9151.2",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tool augmented language models",
            "rating": 2,
            "sanitized_title": "tool_augmented_language_models"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Chatgpt for robotics: Design principles and model abilities",
            "rating": 2,
            "sanitized_title": "chatgpt_for_robotics_design_principles_and_model_abilities"
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Domain-specific ChatBots for Science using Embeddings",
            "rating": 1,
            "sanitized_title": "domainspecific_chatbots_for_science_using_embeddings"
        },
        {
            "paper_title": "Ingrained: An Automated Framework for Fusing Atomic-Scale Image Simulations into Experiments",
            "rating": 1,
            "sanitized_title": "ingrained_an_automated_framework_for_fusing_atomicscale_image_simulations_into_experiments"
        }
    ],
    "cost": 0.011442500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities</p>
<p>Michael H Prince 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Henry Chan 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Aikaterini Vriza 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Tao Zhou 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Varuni K Sastry 
Argonne Leadership Computing Facility
Argonne National Laboratory
ILUSA</p>
<p>Matthew T Dearing 
Business and Information Systems
Argonne National Laboratory
ILUSA</p>
<p>Ross J Harder 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Rama K Vasudevan 
Center for Nanophase Materials
Oak Ridge National Laboratory
Oak RidgeTNUSA</p>
<p>Mathew J Cherukara mcherukara@anl.gov 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities
98861111E1AFCF16D91C64CECE06B45CThe submitted manuscript has been created
Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics.However, these facility and instrument upgrades come with a significant increase in complexity.Driven by more exacting scientific needs, instruments and experiments become more intricate each year.This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments.Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage.Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation.With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures.With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments.By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities' users and accelerate scientific output.</p>
<p>Introduction</p>
<p>Conversational agents such as ChatGPT built on large language models (LLMs) are upending various industries and professions 1 .Their impact spans education 2,3 , talent acquisition 4 , and many other sectors.A report by McKinsey and Company estimates that generative AI, including LLMs, could add trillions of dollars annually to worldwide productivity 5 .</p>
<p>LLMs also have the potential to revolutionize every aspect of scientific discovery, including accelerating literature search, aiding in experimental design, providing data summaries, and publication writing and editing 6 .LLMs have also been shown to be fast learners, requiring only a few training examples to quickly gain domain expertise, e.g., for materials property prediction and inverse design 7 .</p>
<p>At scientific user facilities, such agents could significantly contribute to nearly all facets of operating advanced scientific instruments and conducting experiments.These agents could help new users navigate different aspects of the facility, such as the proposal submission process, safety policies, and operating practices.They could also help users design suitable experiments to address their scientific inquiries, including selecting the appropriate instrument, determining the necessary modalities, and advising on the sample preparation.Additionally, during the experiment, they could guide basic instrument operation.Despite their potential, LLMs suffer from several dangerous limitations, including the tendency to 'hallucinate' answers when faced with questions not part of their training.</p>
<p>We describe preliminary experiments with retrieval 8 and tool-augmented 9 LLMs tailored for scientists' needs.Scientific context-aware LLMs leverage existing data stores to augment their capabilities, allowing them to perform better in domains that are not part of the pre-training data, thereby eliminating the need to perform any fine-tuning on the domain-specific information that can be computationally expensive and time-consuming.In this work, we describe using LLMs to create an agent with knowledge of scientific user facilities.We describe the initial results of assisting scientists through basic experiment design, instrument operation, and direct experimental tool control.</p>
<p>Related work</p>
<p>LLMs have demonstrated strong performance as autonomous agents across multiple science domains 10 .The key components that make the LLM function as the agent's brain are memory, planning, and tool use.Recently, Yager discussed the importance of memory by building domain-specific chatbots for physical science research.Their demonstration involves the creation of a context window using text chunks from scientific documents in PDF format.When the LLM is composing its reply, it uses this context to formulate the answer 11 .</p>
<p>ChemCrow is another example of a domain-specific LLM agent designed to accomplish tasks related to materials design, organic synthesis, and drug discovery by integrating several chemistry tools.Their workflow combines planning via Chain of Thought reasoning (CoT) and tool utilization 12 .In another recent work, researchers built an Intelligent Agent based on GPT, capable of searching hardware documentation and controlling multi-instrument systems to conduct experiments.An important consideration in this type of research is the evaluation of the chatbots or LLMs for scientific applications.In this context, Zaki et al. developed a question-answering dataset for measuring the performance of GPT-3.5 and GPT-4.However, the performance is measured without considering the improvement when an LLM is provided with additional domain-specific scientific context 13 .Jablonka et al. described results from a hackathon applying LLMs to various scientific tasks. 14</p>
<p>Early Experiments</p>
<p>Overview of CALMS</p>
<p>The core of CALMS consists of four components: a large language model (LLM) 15 , conversational history that allows follow-on queries, semantic search over document stores to retrieve the most relevant context given a question from the user 16 , and instrument tools that the LLM can use when instructed to by the user 17 .Figure 1 shows a schematic of the structure of CALMS.When provided with appropriate context and tools, CALMs can either answer complex technical questions that the LLM has no prior knowledge of from its training 18 or execute an experiment or perform a computation using the appropriate tools 17 .We describe each component in detail in the following sections.We compare responses from two state-of-the-art LLMs, OpenAI's GPT-3.5 Turbo and an open-source model Vicuna, over questions related to experimental design, operation, and ability to drive an instrument successfully.We note that the CALMS framework is independent of the LLM, and other open-source or closed-source LLMs can be swapped without changing the framework.</p>
<p>GPT-3.5</p>
<p>The Generative Pretrained Transformer 3.5 (GPT-3.5) is a near state-of-the-art large language model developed by OpenAI and released in late 2022.Similar to its predecessors and successor, GPT-4, the model architecture is based on the Transformer 19 , which has demonstrated effectiveness in various natural language processing tasks, including generating text, answering questions, writing content with human-like creativity, and even generating computer programming code.These outcomes result from predictive inference based on an input sequence of text processed through the language patterns the model learned from a vast training set.</p>
<p>The most performant of the pre-GPT-4 class of models, GPT-3.5 Turbo, was trained with 175 billion parameters and is available at the time of this writing with context window sizes of 4,096 or 16,384 tokens.The context window corresponds to the maximum number of tokens (i.e., words, punctuation, or even sub-strings of words) the model can process during a single inference procedure.The content included in this window represents a simple type of memory that can be leveraged to provide background as effective guidance toward predicting subsequent words.This feature can significantly impact the quality and relevancy of the model output and has led to the burgeoning field of "prompt engineering" to develop and implement best practices.</p>
<p>While the model is proprietary, Microsoft makes it available through the Azure cloud with an enterprise subscription.Hosting of the model resides on a dedicated and restricted instance, so data remains local, is not retained by default, and is not shared with third parties.This approach alleviates some challenges with using public models, such as ChatGPT from OpenAI.</p>
<p>The interaction with GPT-3.5 starts with an API call authenticated using a unique key.This key is essential for accessing OpenAI's capabilities securely and ensuring that only authorized users can make requests.Before making the API call, specific parameters, i.e., temperature and top_p, guide the model's response generation.The temperature parameter controls the randomness of the output and takes values between 0-1.A higher temperature leads to more creative output, while a lower temperature results in more deterministic outputs.The top_k parameter limits the model's choices to the 'k' most likely next words at each step of the generation process.The top_p parameter is an alternative to the temperature that focuses on a subset of predictions that cumulatively add up to the probability 'p'.After these parameters are set and the API call is made, the model can be used either for chat completion or embeddings generation.For chat completion tasks, the model generates a continuation of the provided text input based on the context and parameter set.For the embeddings generation task, the model processes the input text to create a vector representation that captures the semantic meaning of the text.These embeddings can be used for various applications like text similarity assessment or semantic search.</p>
<p>Open-source LLMs</p>
<p>LLMs have transformed the AI field in recent years with the advent of large and complex models like GPT-3 and GPT-4.Though their performance and capabilities are unparalleled, they often require very large compute resources, thus limiting the number of individuals and enterprises that can develop and maintain systems of this scale.Moreover, with limited or no transparency about the model specifications, data, and development methods, these "closed" source LLMs raise concerns regarding adherence to AI ethical standards (including fairness, morality, accountability, and misuse, among others).Open-source LLMs alleviate these concerns by democratizing the AI community with free and unrestricted accessibility to models and information driven by platforms and communities with a transparent and collaborative approach.Open-source LLMs also provide the advantage of being accessible to modification to tailor it to a specific task.HuggingFace is such a resource that provides a platform where researchers, academics, and data scientists can share datasets. 20HuggingFace also provides the necessary tools within their Transformers library to fine-tune and deploy ML workflows for open-source models, including LLMs.With a wide range of modalities of models and data available, it also has the added advantage of seamlessly switching and testing between different models with minimal code changes.</p>
<p>HuggingFace also provides the "Open LLM Leaderboard evaluation suite" that evaluates and compares the performance of different LLMs based on a relative Elo rating strategy.Traditionally, the Elo rating system has been a method used in the field of chess to calculate the relative skill levels of players based on the outcome of the games.A similar strategy is applied to the model responses, where human annotators rate responses for helpfulness and truthfulness in a pairwise setting. 21At the time of finalizing the paper, "lmsys/vicuna-13b-v1.5" was ranked among the top five pertained models with an average score of 61.63.This model was obtained by finetuning Llama 2 on supervised instruction downstream tasks trained on conversations collected from ShareGPT with a sequence length of 2048. 22n the following sections, we compare responses from Vicuna 1.5-16k against GPT-3.5 across a series of tasks.Our evaluation encompasses tasks including general comprehension of user questions, domain-specific context retrieval, quality of the answers, and utilization of tools.</p>
<p>Context retrieval through semantic search</p>
<p>One of the prominent use cases of LLMs is context retrieval through semantic search, which is a process that involves extracting relevant information from a large text body based on the meaning and context of the query instead of relying on keyword matching.Currently, Claude has the largest context window of 100k tokens, corresponding to around 75,000 words.</p>
<p>Two approaches are possible to extend the context retrieval capabilities of models beyond their training corpus.First, fine-tuning the model with domain-specific training data requires the careful curation of data and often necessitates significant up-front investment either in local GPU resources (e.g., Llama was trained on 2048 GPUs) 23 in the case of open-source or in cloud compute costs in the case of closed-source models.A great deal of expertise in language modeling, distributed training, transformers, and data curation is also typically required to fine-tune these models successfully.However, this approach will likely provide the best results when a sufficiently large corpus of data for fine-tuning is available.</p>
<p>A second approach to enhance the capabilities of LLMs is to leave the model weights unchanged but provide appropriate additional context to help answer user queries.This strategy is typically cost-efficient, and a plethora of frameworks now exist to enable facile interaction of the LLM with document stores.A remaining challenge, however, is that even the largest models, e.g., GPT-4-128k, can only accept input text up to 128k tokens, corresponding to approximately 300 pages of text.While future models might allow even larger input sizes, the current limitations prevent passing in all relevant documentation 24 .To address this gap, available documentation is pre-processed for context retrieval, which is performed via a lightweight embedding model.All technical documents are split into token windows of a pre-determined size, and their embeddings are stored in a vector database, e.g., ChromaDB, collection 25 .When a question is sent, the user's text is embedded, and the top N = 4 text chunks closest to the user input are retrieved from the store.These context chunks are then included in the prompt provided to the conversational LLM.</p>
<p>Conversational memory</p>
<p>At their core, LLMs are probabilistic models that learn sentence or paragraph completion.In other words, given a sequence of words, they are trained to predict the most likely completions of that sentence or paragraph.Models that have been fine-tuned for chat have an additional training step that teaches them how to complete conversations, either an unsupervised approach by learning from human-provided conversations or, more popularly, through reinforcement learning with human feedback (RLHF). 26Such fine-tuned chat models can complete human-like conversations, and we leverage this ability by providing the conversational history as part of the subsequent prompt.As previously discussed, due to the limited amount of text that can be provided in the prompt, reproducing the entire conversational history is often not possible.In essence, the problem is how to provide the LLM with short-term memory to answer questions more effectively when faced with a limited context window.Several strategies have been proposed to address this; one option that CALMS adopted is to use a moving window over the conversation history to include only the last K = 6 conversations between the user and the LLM.Supplementary Figure 1A shows the prompt to the LLM with context and memory.</p>
<p>Software and experimental tools</p>
<p>To further augment the use of LLMs, current research is exploring if the LLMs can go beyond text and reason about the physical world.For example, work from Google and Microsoft have demonstrated using NLP to achieve robotic tasks. 27,28This process involves building a series of design principles that include high-level robot APIs or function libraries, special prompting structures, and human feedback via text.</p>
<p>Building upon this strategy, the current implementation of CALMS incorporates capabilities such as calling the Materials Project API 29 , an instrument control software (spec TM ) 30 , and is based on Chain-of-Thought prompting and the ReAct framework. 31,32These functions are provided as a list of tool names, including the description of the way they interact with scientific equipment and details about the user inputs and outputs.The tool is then instructed to answer a user-provided prompt by leveraging the available tools.A parser interprets the LLM output for tool call requests.If a tool is called, then the parser inserts the output into the prompt, and the model continues generating actions until a response is provided.Supplementary Figure 1B shows the prompt to the LLM with the tool description.</p>
<p>In effect, using conversational memory, context-retrieval, and specified tools creates a dynamic pre-prompt that allows the LLM to parse and respond to user input with a detailed understanding of the context behind those queries.</p>
<p>AI-assisted experimental design</p>
<p>We first explore the ability of context-aware LLMs, such as those implemented in CALMS, to respond to highly technical questions typical of those encountered by new scientific facility users.CALMS is queried with questions concerning user facilities, including the Advanced Photon Source (APS), Center for Nanoscale Materials (CNM), Argonne Leadership Computing Facility (ALCF), and Center for Nanophase Materials Sciences (CNMS).A summary of responses with Vicuna and GPT-3.5 Turbo with and without context retrieval is shown in Figure 2. The AI response is first graded by their helpfulness (denoted Rel in Fig. 2 for relevance), 'yes' for giving a relevant answer to the question, and 'no' for an irrelevant answer.Next, their truthfulness is evaluated by checking if hallucinations (denoted Hal) appear in the response.While we appreciate that relevance and hallucinations are not correlated, a relevant response can be hallucinated, and an irrelevant response can be entirely truth-based.Finally, the completeness (denoted Com) of the AI response is graded with a score of 0-5.Details on the scoring are included in the Methods section.A complete list of the queries and AI responses is found in the data availability section, with highlighted key differences.We caution that the reader might observe slightly different results because the LLMs outputs are non-deterministic.Given an accurate context, CALMS can always provide relevant answers to questions specific to each user facility.Rarely was hallucination observed.In general, GPT-3.5 Turbo tends to provide a more complete answer than Vicuna, scoring multiple 5s in the completeness grading.A score of 5 indicates that the users are provided with all the necessary information to select the appropriate tool to complete their task.</p>
<p>Without appropriate context, the language models have a strong tendency to hallucinate.As a representative example, we queried the models on the name of the automated image simulation framework developed at the CNM.Both Vicuna and GPT-3.5 Turbo provided the user with a made-up tool called ImageSim (an apparent portmanteau for image and simulation) (see Figure 3).We note that both models, with context, were able to retrieve the correct answer, which is a tool called ingrained. 33Without context, irrelevant answers have been observed, and occasionally, the models decline to answer the question, citing insufficient knowledge (see for example Supplementary Figure 2).Supplementary Figures 3 and 4 show further examples illustrating the importance of context-aware responses.</p>
<p>AI-assisted experiments</p>
<p>We further queried the models' ability to provide operational assistance on advanced scientific and computational resources at the abovementioned user facilities.A summary of responses with Vicuna and GPT-3.5 Turbo with and without context retrieval is shown in Figure 4. Similar behavior was observed compared to what was noted for the experimental design queries.Without context, both Vicuna and GPT-3.5 Turbo tend to hallucinate.Even when they are being truthful (i.e., not hallucinating), the answers they provide are typically not directly helpful.For instance, when queried about the procedure to start a tomographic scan, Vicuna, without context, responded with a truthful answer concerning the computational tomography (CT) scan in a hospital, which is unrelated to the tomographic scan performed at the APS (Figure 5).In this case, the truthful yet unhelpful answer Vicuna provided was due to the LLM mistaking the tool being queried with a different subject well-known in the public domain.GTP-3.5 Turbo realized the nuances in the question and suggested the user be more specific about the type of tomographic scan they are inquiring about.It then provided a general answer about the need to submit a user proposal to perform a tomography experiment.However, with context, both Vicuna and GPT-3.5 Turbo provide consistently relevant and truthful answers, with GTP-3.5 Turbo again faring better on the score of completeness than Vicuna.Still using the tomographic scan procedure query as an example, Vicuna responded with a short answer regarding the use of the tomoscan command, while GTP-3.5 Turbo's answer included solutions with the graphic user and the command line interface with a reference to the relevant documentation (Figure 5).Supplementary Figures 5, 6, and 7 show further instances of the importance of context-aware responses.</p>
<p>Automated execution</p>
<p>Through the ReAct framework 32 and API calls, the CALMS framework could be configured to drive experiments in an agentic manner.To demonstrate this capability, a test was conducted on a real-world diffractometer simulating a common user operation.</p>
<p>For a user to take a diffraction measurement, an area detector, usually mounted to a robotic arm, must be moved into a set position relative to the sample and incoming x-ray beam.This angle is determined by the intensity of the beam, the material being observed, and the peak position selected by the user.To do this process manually, the user must first look up lattice information via a materials database.Then, the user must copy over the lattice information and the selected peak into the beamline interface, controlled by SPEC, to move the motors into position.</p>
<p>To automate this process, two LLM calls were implemented: GetLatticeConstants and SetDiffractometmer.For the LLM to successfully perform the action, it must first input the material into a call that queries the Materials Project API to return the lattice constants to the model.Next, it must combine the output of this call with the peak positions from the user input to execute a second call.Figure 6 describes in detail the sequence and parameters of the calls.In the example shown, a user requests the diffractometer to be set to the 012 Bragg peak of WSe2.Within the CALMS framework, the LLM parses the query and recognizes the material and Bragg peak.It then uses a tool to call The Materials Project 29 to obtain the lattice information for WSe2.It then passes that information to a second tool that uses the Spec software to calculate the motor positions and then moves the diffractometer to that position.</p>
<p>These calls were implemented via LangChain's "Structured input ReAct" framework.This library API allows developers to define the parameters and types for multi-parameter input and requires the model to use valid JSON to perform a call.In our tests, we observed that the opensource models could not consistently follow this syntax.All results shown were run with OpenAI's GPT-3.5 model.We could execute this sequence of commands via a single user prompt on beamline 34 at the APS.A video of the commands being run on the beamline is provided in the supplemental material of this paper and can also be viewed online.</p>
<p>Summary and Outlook</p>
<p>In summary, we described the development of a Context-Aware Language Model for Science (CALMS) and exemplified the promising potential of leveraging LLMs for accelerating scientific experiments and discovery.Preliminary results obtained using CALMS backed by GPT-3.5 and Vicuna illustrate the applicability of LLMs in experimental design and operation guidance, leveraging the zero-shot capability of LLMs via prompt engineering 35 .The results emphasize the importance of context in addressing the typical issue of hallucination in LLMs.CALMS powered by GPT-3.5 and Vicuna both performed reasonably well in the Q&amp;A tasks, but the results reveal limitations in the open-source models compared to state-of-the-art closed-source models such as GPT-3.5 Turbo.This gap is particularly pronounced in tool usage, where we could not execute the experimental workflow with the open-source model Vicuna.With the investment and extraordinary rate of progress in open-source models and fine-tuning methods, we expect the performance gap to the closed-source models to narrow.</p>
<p>Traditionally, model performance over the years has shown power law scaling with an increase in the number of model parameters, dataset size, and compute budget.However, emerging research demonstrates that it is equally important to scale the training size with an increase in model parameter size to obtain optimal training. 36Additionally, with instruction fine-tuning and RLHF, we see that the even smaller models of the size 1.3B parameters can equal the performance of the 175B GPT-3-base model. 37With several innovations like SELF-Instruct 38 (that can enhance the model performance by 33% over the GPT3 base model with fine tuning on the self-generated instruction without any human intervention) and Chain-of-Thought prompting 15,39 , the broad consensus is that these performance gains will continue to push the state of the art.With such anticipated advanced model performance, we envision future capabilities could include leveraging decades of information recorded in e-logs as well as the ability to extract code or commands and execute experimental workflows fully autonomously.</p>
<p>Methods</p>
<p>Scoring methodology: 1/1 pt is automatically given if the response is rated 'yes' on relevance and 'no' on hallucination.2/2 pts is given if the instructions can be executed by the user without error, 1/2 if minor errors are present that are easily fixable, and 0/2 if major errors are present in the response.2/2 pts is assigned if the user is given all the information they need for their request, 1/2 if minor details are missing, and 0/2 if crucial details are missing that affect the execution.The maximum score is 5 pts.</p>
<p>Code and Data Availability</p>
<p>All code, models, and data will be made publicly available on GitHub with the published manuscript.</p>
<p>Figure 1 :
1
Figure 1: Overview of CALMS: CALMS uses a large language model in conjunction with conversational memory, document stores, and experimental tools to answer user queries or take action to drive an instrument.</p>
<p>Figure 2 :
2
Figure 2: Scoring of experimental design questions with and without context.We score the models on relevance, absence of hallucination, and completeness of response.</p>
<p>Figure 3 :
3
Figure 3: Demonstration of AI-assisted experimental design specific to the CNM user facility: (top) initial question from a user, (bottom-left) response from AI without context, (bottom-right) response from AI with context.Major differences between the responses are highlighted (in yellow) to illustrate the importance of context.</p>
<p>Figure 4 :
4
Figure 4: Scoring of experimental operations assistance questions with and without context.As before, we score the models on relevance, absence of hallucination, and completeness of response.</p>
<p>Figure 5 :
5
Figure 5: Demonstration of AI-powered operational assistance specific to the APS user facility: (top) initial query from a user, (bottom-left) response from AI without context, (bottom-right) response from AI with context.Major differences between the responses are highlighted (in yellow) to illustrate the importance of context.</p>
<p>34
34</p>
<p>Figure 6 :
6
Figure 6: Workflow demonstrating the execution of a robot motor move based on a user query to CALMS.CALMS parses the user query to extract the material and Bragg peak, queries Materials Project for the lattice constants, and then calculates the position of and moves the beamline diffractometer to the Bragg peak requested by the user.A video of the demonstration is available at https://danielzt12.github.io/latest_news/2023/11/20/operating-scientific-instruments-with-LLMs.html</p>
<p>AcknowledgmentsWork performed at the Center for Nanoscale Materials and Advanced Photon Source, both U.S. Department of Energy Office of Science User Facilities, was supported by the U.S. DOE, Office of Basic Energy Sciences, under Contract No. DE-AC02-06CH11357.This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.M.J.C and H.C also acknowledge support from the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences Data, Artificial Intelligence, and Machine Learning at DOE Scientific User Facilities program under Award Number 34532.by UChicago Argonne, LLC, Operator of Argonne National Laboratory ("Argonne").Argonne, a U.S. Department of Energy Office of Science laboratory, is operated under Contract No. DE-AC02-06CH11357.The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government.The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan.http://energy.gov/downloads/doe-public-access-plan
References 1 Prepare for truly useful large language models. https://doi.org:10.1038/s41551-023-01012-6Nature Biomedical Engineering. 72023</p>
<p>Chegg Embraced AI. Wired, ChatGPT Ate Its Lunch Anyway. 2023</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, https://doi.org:10.1016/j.lindif.2023.102274Learning and Individual Differences. 1032023</p>
<p>B Clavié, A Ciceu, F Naylor, G Soulié, T Brightwell, International Conference on Applications of Natural Language to Information Systems. Springer</p>
<p>The economic potential of generative AI: The next productivity frontier. M Company, 2023</p>
<p>Scientists used ChatGPT to generate an entire paper from scratch-but is it any good?. G Conroy, Nature. 6192023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, Advances in Neural Information Processing Systems. 332020</p>
<p>A Parisi, Y Zhao, N Fiedel, Talm, arXiv:2205.12255Tool augmented language models. 2022arXiv preprint</p>
<p>The future of chemistry is language. A D White, Nature Reviews Chemistry. 2023</p>
<p>Domain-specific ChatBots for Science using Embeddings. K G Yager, arXiv:2306.100672023arXiv preprint</p>
<p>A M Bran, S Cox, A D White, P Schwaller, Chemcrow, arXiv:2304.05376Augmenting largelanguage models with chemistry tools. 2023arXiv preprint</p>
<p>M Zaki, N Krishnan, Mascqa, arXiv:2308.09115A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models. 2023arXiv preprint</p>
<p>14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, https://doi.org:10.1039/D3DD00113JDigital Discovery. 22023</p>
<p>W.-L A L Chiang, Zhuohan, Zi Lin, Ying Sheng, Zhanghao Wu, Zhang, Hao, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Vicuna, An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality. 2023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, arXiv:2302.047612023arXiv preprint</p>
<p>H He, H Zhang, D Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model inference. 2022arXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, 201730Attention is all you need</p>
<p>. HuggingFace. 2023</p>
<p>Can foundation models label data like humans?. Huggingface, 2023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, arXiv:2306.056852023arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, arXiv:2302.139712023arXiv preprint</p>
<p>New models and developer products announced at DevDay. Openai, 2023</p>
<p>. Chroma, Chroma, 2023</p>
<p>Fine-tuning language models from human preferences. D M Ziegler, arXiv:1909.085932019arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, arXiv:2303.033782023arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, APL materials. 12013</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. C S Software, J Wei, spec Software for Diffraction. 202235</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, arXiv:2210.036292022arXiv preprint</p>
<p>Ingrained: An Automated Framework for Fusing Atomic-Scale Image Simulations into Experiments. E Schwenker, Small. 1821029602022</p>
<p>T Zhou, Augmenting Scientific Instrumentation with LLMs. 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>Training compute-optimal large language models. J Hoffmann, arXiv:2203.155562022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, Advances in Neural Information Processing Systems. 352022</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, arXiv:2212.105602022arXiv preprint</p>
<p>Large language models can self-improve. J Huang, arXiv:2210.116102022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
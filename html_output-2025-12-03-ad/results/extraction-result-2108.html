<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2108 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2108</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2108</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-279243290</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04553v1.pdf" target="_blank">Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices</a></p>
                <p><strong>Paper Abstract:</strong> Unsupervised machine learning is widely used to mine large, unlabeled datasets to make data-driven discoveries in critical domains such as climate science, biomedicine, astronomy, chemistry, and more. However, despite its widespread utilization, there is a lack of standardization in unsupervised learning workflows for making reliable and reproducible scientific discoveries. In this paper, we present a structured workflow for using unsupervised learning techniques in science. We highlight and discuss best practices starting with formulating validatable scientific questions, conducting robust data preparation and exploration, using a range of modeling techniques, performing rigorous validation by evaluating the stability and generalizability of unsupervised learning conclusions, and promoting effective communication and documentation of results to ensure reproducible scientific discoveries. To illustrate our proposed workflow, we present a case study from astronomy, seeking to refine globular clusters of Milky Way stars based upon their chemical composition. Our case study highlights the importance of validation and illustrates how the benefits of a carefully-designed workflow for unsupervised learning can advance scientific discovery.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2108.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2108.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unsupervised Discovery Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-agnostic Unsupervised Learning Workflow (stability- and generalizability-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A complete, model-agnostic pipeline for producing and validating unsupervised data-driven discoveries that emphasizes planning, multiple preprocessing/modeling choices, resampling-based stability, held-out generalizability, sensitivity analyses, and reproducible communication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stability- and Generalizability-driven Unsupervised Discovery Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An end-to-end workflow that (1) formulates validatable scientific questions, (2) pre-specifies multiple preprocessing pipelines and modeling methods, (3) applies multiple dimension-reduction and clustering algorithms with hyperparameter sweeps, and (4) validates outcomes via resampling-based stability metrics and held-out generalizability (train/test splits and prediction-based validation). Emphasizes consensus aggregation, local stability, sensitivity to preprocessing, and documentation for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / Cross-domain (demonstrated in astronomy; applicable to biology, climate, chemistry, neuroscience, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is primarily computational: stability is measured by resampling the observed dataset (subsampling/bootstrapping/noise injections) and recomputing models across a set of preprocessing pipelines, then quantifying agreement (e.g., Adjusted Rand Index (ARI) for clusters). Generalizability is assessed with a pre-specified train/test split and a prediction-based approach: cluster labels fitted on training data are predicted on test data using a supervised classifier (random forest), and concordance between predicted and test-set cluster assignments (ARI and per-cluster precision) is reported. Sensitivity analyses sweep domain-specific preprocessing thresholds (e.g., S/N and log(g) cutoffs) and imputation methods (mean vs random-forest) to measure robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>empirical resampling of observed data (empirical/low-to-moderate fidelity approximation to sampling variability); does not simulate physics/measurement processes, but approximates observational variability through bootstrap/subsample resampling and alternate preprocessing pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper argues computational validation (stability + held-out predictability + sensitivity to preprocessing) is necessary and often sufficient to establish reliability of unsupervised findings in practice, but explicitly notes that experimental or independent new-data validation is ideal where feasible; domain norms (e.g., astronomy) accept cross-survey comparisons and follow-up observations for stronger claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Validation metrics are reported in relative terms (stability scores, ARI, per-cluster precision). Example numeric outcomes from the case study: cluster 2 local generalizability = 1.00 (precision = 1.00) on the held-out test set; spectral clustering k=2 produced very high stability (highest mean stability), K-means k=8 had lower overall stability but produced some highly stable clusters. No aggregate absolute 'error' percentages vs physical ground truth are reported because true labels for many discoveries are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical follow-up experiments were performed; the paper states experimental (new-data) validation is ideal but often costly and infeasible, so they rely on held-out test-set validation and resampling. The lack of wet-lab / telescope follow-up is explicitly identified as a limitation and an item for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The workflow compares multiple computational validation approaches: (a) stability via resampling and ARI across preprocessing pipelines, (b) generalizability via train/test and classifier-based prediction strength, and (c) sensitivity sweeps of preprocessing thresholds (S/N, log(g)). It reports that spectral clustering produced the highest mean stability (k=2) but recapitulated known iron-rich/iron-poor separation, while K-means k=8 produced a mix of stable and unstable clusters; tSNE outperformed UMAP on neighbor-retention for embeddings. Numerical comparisons include ARI-based stability rankings and a reported per-cluster precision of 1.00 for cluster 2.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper documents specific failures/insufficiencies: some clusters are unstable (e.g., clusters 3 and 7 in the K-means k=8 solution showed low co-clustering and low generalizability), and some methods recapitulated known, uninteresting divisions (spectral k=2 reproduced iron-rich vs iron-poor). More generally, the paper notes the insufficiency of model-agnostic validation tools for many unsupervised tasks and that uncertainty quantification and post-hoc inference remain open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successful validations in the astronomy case study: spectral clustering (k=2) produced extremely high stability and coherent iron-rich/iron-poor split; K-means clusters 1, 2, 6, and 8 were both locally stable and generalizable (cluster 2 had perfect reported generalizability on the test set, 1.00). Sensitivity sweeps of quality-control thresholds showed little change (high robustness) for the final clusters. Reproducibility was supported by publishing code, interactive supplement, and processed datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Where possible, results were compared to domain knowledge and catalog labels: clusters were compared to known chemical abundance trends (iron-rich vs iron-poor markers), and cluster membership overlapped strongly with known globular clusters (e.g., NGC6121 nearly entirely in cluster 1; NGC0104 mostly in cluster 2), providing partial ground-truth validation. However, for many discovered groupings there is no definitive ground truth, and the paper emphasizes comparison against literature and other surveys/simulations as the pragmatic approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper discusses reproducibility explicitly: code, full processing pipelines, train/test splits, and results are made available in an interactive supplement and GitHub repository; the workflow prescribes documenting preprocessing/modeling decisions and seeding stochastic steps. Independent replication with new observations is recommended but not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper notes that collecting new experimental/observational data (ideal validation) is costly in time and resources; computational validation (resampling, train/test splits, sensitivity sweeps) is far cheaper but still computationally expensive when sweeping many preprocessing/model choices. No quantitative cost/time numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For astronomy (demonstrated domain): domain norms include comparing to established catalogs, cross-matching with other surveys (GALAH, Gaia), and where possible telescope follow-up or comparing to astrophysical simulations; for many sciences the paper asserts that wet-lab / observational follow-up or independent data collection is the gold standard, but computational validation (stability + generalizability) is accepted pre-publication evidence when experiments are infeasible or costly.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes—uncertainty is quantified via resampling-derived metrics: co-clustering frequency (consensus matrix), local stability (average co-clustering frequency per star), ARI distributions across resamples, per-cluster precision on test set, neighbor-retention curves for embeddings, and multiple imputation runs to assess sensitivity to missing-data treatments. The paper notes broader gaps remain for general, model-agnostic uncertainty quantification methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Key limitations: absence of hard ground truth for many unsupervised tasks limits definitive validation; resampling-based validation approximates but cannot fully substitute for truly independent experiments; some unsupervised tasks lack well-established model-agnostic validation metrics; uncertainty-comparison across different unsupervised methods remains unsolved; computational validation can be sensitive to untested preprocessing choices beyond the swept set.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2108.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2108.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stability-driven Model Selection (Algorithm 1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Selection via Resampling-driven Stability (Algorithm 1 in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithmic procedure that selects clustering method and number of clusters by maximizing stability measured across random subsamples and different preprocessing pipelines using ARI as the stability metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Algorithm 1: Stability-driven Clustering Model Selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iteratively subsamples the training dataset and randomly selects preprocessing pipelines from a set G, fits candidate clustering methods m ∈ M with candidate k ∈ K, computes pairwise ARI on overlapping subsamples, and selects (m*, k*) with highest mean ARI (stability).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / demonstrated in astronomy clustering of APOGEE stars</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Uses repeated empirical subsampling (proportion π), random selection of preprocessing pipelines, and recomputation of clusterings; computes Adjusted Rand Index (ARI) for overlapping samples to quantify similarity between clusterings; aggregates mean stability over B resamples to choose the most stable pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical resampling (bootstrap/subsample) of observed dataset - approximates sampling variability but not physical process simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper treats this as a core sufficiency criterion for selecting models: high stability implies trustworthy scientific conclusions, but authors caution stability alone does not guarantee novelty or domain relevance and advise combining with generalizability and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported comparatively (rankings) rather than absolute accuracy; spectral clustering n_neighbors=60 at k=2 had highest mean stability in the case study, followed by K-means k=8. No single global numeric threshold specified; selection is based on relative maxima or elbow rules.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; algorithm tested on APOGEE observational dataset with train/test split and multiple preprocessing variants.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared stability across multiple clustering algorithms (spectral, K-means, hierarchical) and hyperparameters; used stability rankings to shortlist candidate models. Authors note spectral produced highest stability but K-means offered finer, partially stable clusters of interest.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Algorithm exposed clusters with low stability (e.g., clusters 3 and 7 in K-means k=8) which were flagged as unreliable; hence selection of m*,k* still requires interpretation and secondary generalizability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successfully identified spectral k=2 and K-means k=8 as promising candidates, enabling further inspection; stability-guided selection prevented over-reliance on a single preprocessing pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Selection based on internal stability metrics and external domain checks (comparison of resulting clusters to known abundance markers and GC membership); no independent ground-truth labels for all clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Algorithm is fully specified and provided in Interactive Supplement and GitHub; authors recommend pre-specifying G, M, K and resampling scheme to enable replication.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computationally intensive when many preprocessing pipelines and hyperparameter combinations are included; paper notes feasibility trade-offs and that some preprocessing choices were excluded for computational reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Paper states stability across preprocessing choices is a domain-agnostic norm they advocate for; in disciplines where independent data collection is feasible, stability should be complemented by experimental replication.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Stability distributions (ARI over resamples) provide empirical uncertainty about model outputs; local stability per-object is computed from co-clustering frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Stability measures depend on the set of preprocessing pipelines G considered; untested pipelines can yield different outcomes; stability does not equate to scientific significance or novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2108.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2108.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consensus Clustering & Local Stability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consensus Clustering with Co-clustering Matrix and Local Stability Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation of clustering outputs across resamples and preprocessing variations into a co-clustering frequency matrix, from which local stability per observation and consensus labels are derived.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Consensus Clustering / Co-clustering Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Runs the chosen clustering model across many preprocessed variants and resamples, records pairwise co-clustering frequency (consensus matrix), uses this to produce consensus cluster assignments and compute local stability for each object as its average co-clustering with same-cluster peers.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General clustering applications; used here for astronomical chemical-tagging of APOGEE stars</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation performed by aggregating multiple runs to measure repeatability: entries in the consensus matrix denote fraction of times two items fell in same cluster. Local stability (per-item) is the average co-membership rate within assigned cluster. Clusters with high internal co-clustering frequency are deemed reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical/resampling-based aggregation (low-to-moderate fidelity estimate of clustering reproducibility across observed-data perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper uses consensus clustering as a principal measure of stability and a necessary condition for trusting clusters; however, authors pair consensus results with held-out generalizability and domain checks before drawing scientific conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No global accuracy metric; quantified via distribution of co-clustering frequencies and local stability; in case study many stars showed high local stability for spectral k=2; for K-means k=8 several clusters had strong co-clustering while others did not.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Consensus derived solely from computational reruns across preprocessing/imputation/DR variations; no physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Consensus matrices allowed comparison of different clusters' robustness (e.g., cluster 2 vs cluster 3), and comparison with alternative algorithms (spectral vs K-means) in terms of block-diagonal strength.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Blocks with light entries in consensus matrix (low co-clustering frequency) highlighted clusters that failed stability checks (e.g., clusters 3 and 7 in K-means solution), leading authors to caution against interpreting those clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>High-consensus blocks corresponded to reliable groupings (clusters 1, 2, 6, 8). The consensus approach made it straightforward to identify and filter unreliable cluster assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Consensus clusters were compared to known GC membership fractions (e.g., NGC6121 and NGC0104 composition) and known abundance trends to corroborate reliability where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Procedure and code for consensus aggregation provided in supplement and repository; reproducible given same preprocessing choices and seeds for stochastic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Requires re-running clustering many times across preprocessing variants (computationally moderate-to-high cost depending on B and |G|).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Consensus/clustering stability is a commonly-accepted internal validation step in clustering-heavy domains (genomics, astronomy) before committing to biological/physical interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Co-clustering frequency and local stability serve as empirical uncertainty measures for membership; clusters with wide spread of co-clustering values flagged as uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Consensus only measures repeatability under the considered variations; it cannot detect systematic biases common to all pipelines or guarantee physical reality of clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2108.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2108.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prediction-based Generalizability (Cluster Predictability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prediction-based Cluster Generalizability via Supervised Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to assess whether clusters discovered in training data generalize to unseen data by training a classifier to predict cluster labels and measuring concordance on a held-out test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cluster Predictability Validation (Random Forest classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After clustering training data, fit a supervised classifier (random forest in the case study) to predict training cluster labels from features; apply the classifier to test data and compare predicted labels to clusters obtained by re-clustering the test set (or to test-set labels) using ARI and per-cluster precision to quantify generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Clustering validation across domains; applied in astronomy case study</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Uses supervised learning as a proxy-validity check: trains RF on training features→cluster labels, predicts labels on held-out test features, and computes ARI (and per-cluster precision) between RF-predicted labels and cluster labels estimated in test set. Cluster-wise precision is used to identify which clusters generalize well.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical predictive-check (data-driven), fidelity depends on representativeness of the held-out test set; not a physics-based simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper treats prediction-based concordance as one important measure of generalizability; while supportive evidence, authors recommend combining with stability and domain checks before asserting scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Example reported: cluster 2 local generalizability (precision) = 1.00 on the held-out test set (averaged across imputation methods/datasets). Overall ARI between RF-predicted and test cluster labels used as aggregate metric but exact overall ARI value not reported in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; classifier-based validation used instead of gathering new observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared cluster-wise generalizability to cluster local stability; authors found clusters with high local stability often had high generalizability (e.g., clusters 1 and 2) whereas some low-stability clusters showed poor predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Some clusters (e.g., cluster 3) have low predictive precision and thus fail the generalizability test, indicating that they are not reliable discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Clusters 1, 2, 6, 8 passed both stability and predictability checks; cluster 2 achieved perfect reported generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Prediction-based validation is internal (train/test) and compared to clusterings on the test data; external ground truth comparisons were performed separately via catalog membership or literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Method is fully described and code provided; reproducible if same train/test splits and RF hyperparameters are used.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Training classifiers and re-clustering test data adds computational cost but is generally cheaper than collecting new physical data; cost scales with number of models and resamples.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Using supervised prediction to assess unsupervised cluster generalizability is an accepted strategy in statistics/machine learning literature when independent ground truth is lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Per-cluster precision and ARI distributions provide uncertainty about generalizability; authors recommend reporting cluster-wise metrics rather than only aggregate measures.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Prediction-based validation depends on (i) representativeness of the test set, (ii) stability and transferability of features used by the classifier, and (iii) the classifier's inductive biases. It may mask clusters that are real but poorly captured by the classifier's feature mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2108.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2108.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighbor-retention Metric for DR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighborhood Retention Metric for Dimension Reduction Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative metric that measures the proportion of k nearest neighbors retained when mapping from high-dimensional space to low-dimensional embeddings (used to tune embedding hyperparameters and choose between methods like tSNE, UMAP, PCA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neighbor Retention / Neighborhood Preservation Metric</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes, for a range of neighborhood sizes k, the fraction of original high-dimensional k nearest neighbors preserved in the 2D embedding; used to compare dimension-reduction methods and tune hyperparameters (e.g., tSNE perplexity or UMAP n_neighbors).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Data visualization and dimension-reduction validation across many scientific domains; applied in astronomy case study</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is computational: for each embedding and neighborhood size k, count how many of the original k nearest neighbors in feature space remained among k nearest neighbors in the embedding; aggregate retention curves inform selection of DR method and hyperparameters that balance local and global structure preservation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical geometric fidelity measure (high relevance for nearest-neighbor structure but not a physical simulation); effective for judging structural preservation but not absolute truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper uses neighbor-retention as a primary criterion to select DR method and hyperparameters; demonstrates tSNE consistently had higher neighbor retention than UMAP in their dataset, leading to dropping UMAP from further consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Presented as comparative curves; tSNE consistently showed higher neighbor retention across neighborhood sizes in the APOGEE case study; exact numeric retention rates plotted in figures (not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental tests; purely computational evaluation of embedding fidelity to original neighborhood relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>tSNE (multiple perplexities) vs UMAP (multiple n_neighbors) vs PCA: tSNE had uniformly higher neighbor retention than UMAP, PCA highest global retention; led to selecting tSNE (perplexity=100) for visualization and downstream analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>UMAP performed worse on neighbor-retention for this dataset and was removed from the analysis; the authors also caution that DR methods can be stochastic and require repeated checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>tSNE hyperparameters (perplexities 30 and 100) produced embeddings with high neighbor retention for both small and large neighborhoods, supporting their use for visualization and as inputs to clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Neighbor retention compares embedding to original high-dimensional measurements (treated as the reference), not to an external ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Procedure deterministic given seeds; authors examine multiple random seeds and hyperparameters to ensure reliability; code provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computing neighbor-retention across multiple hyperparameter settings and resamples is computationally inexpensive to moderate depending on dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In visualization tasks, neighborhood-preservation metrics are common domain-agnostic checks to ensure embeddings do not artificially distort structure; authors recommend them as a validation step.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Retention curves across neighborhood sizes and across multiple random seeds/hyperparameter settings quantify variability in embedding fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Neighbor retention assesses only preservation of nearest-neighbor relationships and not other structures (e.g., global manifold geometry); different DR goals (local vs global) require different metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data <em>(Rating: 2)</em></li>
                <li>Stability approach to regularization selection (StARS) for high dimensional graphical models <em>(Rating: 2)</em></li>
                <li>Cluster validation by prediction strength <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2108",
    "paper_id": "paper-279243290",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "Unsupervised Discovery Workflow",
            "name_full": "Model-agnostic Unsupervised Learning Workflow (stability- and generalizability-driven)",
            "brief_description": "A complete, model-agnostic pipeline for producing and validating unsupervised data-driven discoveries that emphasizes planning, multiple preprocessing/modeling choices, resampling-based stability, held-out generalizability, sensitivity analyses, and reproducible communication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Stability- and Generalizability-driven Unsupervised Discovery Workflow",
            "system_description": "An end-to-end workflow that (1) formulates validatable scientific questions, (2) pre-specifies multiple preprocessing pipelines and modeling methods, (3) applies multiple dimension-reduction and clustering algorithms with hyperparameter sweeps, and (4) validates outcomes via resampling-based stability metrics and held-out generalizability (train/test splits and prediction-based validation). Emphasizes consensus aggregation, local stability, sensitivity to preprocessing, and documentation for reproducibility.",
            "scientific_domain": "General / Cross-domain (demonstrated in astronomy; applicable to biology, climate, chemistry, neuroscience, etc.)",
            "validation_type": "simulated",
            "validation_description": "Validation is primarily computational: stability is measured by resampling the observed dataset (subsampling/bootstrapping/noise injections) and recomputing models across a set of preprocessing pipelines, then quantifying agreement (e.g., Adjusted Rand Index (ARI) for clusters). Generalizability is assessed with a pre-specified train/test split and a prediction-based approach: cluster labels fitted on training data are predicted on test data using a supervised classifier (random forest), and concordance between predicted and test-set cluster assignments (ARI and per-cluster precision) is reported. Sensitivity analyses sweep domain-specific preprocessing thresholds (e.g., S/N and log(g) cutoffs) and imputation methods (mean vs random-forest) to measure robustness.",
            "simulation_fidelity": "empirical resampling of observed data (empirical/low-to-moderate fidelity approximation to sampling variability); does not simulate physics/measurement processes, but approximates observational variability through bootstrap/subsample resampling and alternate preprocessing pipelines.",
            "validation_sufficiency": "Paper argues computational validation (stability + held-out predictability + sensitivity to preprocessing) is necessary and often sufficient to establish reliability of unsupervised findings in practice, but explicitly notes that experimental or independent new-data validation is ideal where feasible; domain norms (e.g., astronomy) accept cross-survey comparisons and follow-up observations for stronger claims.",
            "validation_accuracy": "Validation metrics are reported in relative terms (stability scores, ARI, per-cluster precision). Example numeric outcomes from the case study: cluster 2 local generalizability = 1.00 (precision = 1.00) on the held-out test set; spectral clustering k=2 produced very high stability (highest mean stability), K-means k=8 had lower overall stability but produced some highly stable clusters. No aggregate absolute 'error' percentages vs physical ground truth are reported because true labels for many discoveries are unavailable.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical follow-up experiments were performed; the paper states experimental (new-data) validation is ideal but often costly and infeasible, so they rely on held-out test-set validation and resampling. The lack of wet-lab / telescope follow-up is explicitly identified as a limitation and an item for future work.",
            "validation_comparison": "The workflow compares multiple computational validation approaches: (a) stability via resampling and ARI across preprocessing pipelines, (b) generalizability via train/test and classifier-based prediction strength, and (c) sensitivity sweeps of preprocessing thresholds (S/N, log(g)). It reports that spectral clustering produced the highest mean stability (k=2) but recapitulated known iron-rich/iron-poor separation, while K-means k=8 produced a mix of stable and unstable clusters; tSNE outperformed UMAP on neighbor-retention for embeddings. Numerical comparisons include ARI-based stability rankings and a reported per-cluster precision of 1.00 for cluster 2.",
            "validation_failures": "Paper documents specific failures/insufficiencies: some clusters are unstable (e.g., clusters 3 and 7 in the K-means k=8 solution showed low co-clustering and low generalizability), and some methods recapitulated known, uninteresting divisions (spectral k=2 reproduced iron-rich vs iron-poor). More generally, the paper notes the insufficiency of model-agnostic validation tools for many unsupervised tasks and that uncertainty quantification and post-hoc inference remain open problems.",
            "validation_success_cases": "Successful validations in the astronomy case study: spectral clustering (k=2) produced extremely high stability and coherent iron-rich/iron-poor split; K-means clusters 1, 2, 6, and 8 were both locally stable and generalizable (cluster 2 had perfect reported generalizability on the test set, 1.00). Sensitivity sweeps of quality-control thresholds showed little change (high robustness) for the final clusters. Reproducibility was supported by publishing code, interactive supplement, and processed datasets.",
            "ground_truth_comparison": "Where possible, results were compared to domain knowledge and catalog labels: clusters were compared to known chemical abundance trends (iron-rich vs iron-poor markers), and cluster membership overlapped strongly with known globular clusters (e.g., NGC6121 nearly entirely in cluster 1; NGC0104 mostly in cluster 2), providing partial ground-truth validation. However, for many discovered groupings there is no definitive ground truth, and the paper emphasizes comparison against literature and other surveys/simulations as the pragmatic approach.",
            "reproducibility_replication": "The paper discusses reproducibility explicitly: code, full processing pipelines, train/test splits, and results are made available in an interactive supplement and GitHub repository; the workflow prescribes documenting preprocessing/modeling decisions and seeding stochastic steps. Independent replication with new observations is recommended but not performed.",
            "validation_cost_time": "Paper notes that collecting new experimental/observational data (ideal validation) is costly in time and resources; computational validation (resampling, train/test splits, sensitivity sweeps) is far cheaper but still computationally expensive when sweeping many preprocessing/model choices. No quantitative cost/time numbers are provided.",
            "domain_validation_norms": "For astronomy (demonstrated domain): domain norms include comparing to established catalogs, cross-matching with other surveys (GALAH, Gaia), and where possible telescope follow-up or comparing to astrophysical simulations; for many sciences the paper asserts that wet-lab / observational follow-up or independent data collection is the gold standard, but computational validation (stability + generalizability) is accepted pre-publication evidence when experiments are infeasible or costly.",
            "uncertainty_quantification": "Yes—uncertainty is quantified via resampling-derived metrics: co-clustering frequency (consensus matrix), local stability (average co-clustering frequency per star), ARI distributions across resamples, per-cluster precision on test set, neighbor-retention curves for embeddings, and multiple imputation runs to assess sensitivity to missing-data treatments. The paper notes broader gaps remain for general, model-agnostic uncertainty quantification methods.",
            "validation_limitations": "Key limitations: absence of hard ground truth for many unsupervised tasks limits definitive validation; resampling-based validation approximates but cannot fully substitute for truly independent experiments; some unsupervised tasks lack well-established model-agnostic validation metrics; uncertainty-comparison across different unsupervised methods remains unsolved; computational validation can be sensitive to untested preprocessing choices beyond the swept set.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2108.0"
        },
        {
            "name_short": "Stability-driven Model Selection (Algorithm 1)",
            "name_full": "Model Selection via Resampling-driven Stability (Algorithm 1 in paper)",
            "brief_description": "An algorithmic procedure that selects clustering method and number of clusters by maximizing stability measured across random subsamples and different preprocessing pipelines using ARI as the stability metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Algorithm 1: Stability-driven Clustering Model Selection",
            "system_description": "Iteratively subsamples the training dataset and randomly selects preprocessing pipelines from a set G, fits candidate clustering methods m ∈ M with candidate k ∈ K, computes pairwise ARI on overlapping subsamples, and selects (m*, k*) with highest mean ARI (stability).",
            "scientific_domain": "General / demonstrated in astronomy clustering of APOGEE stars",
            "validation_type": "simulated",
            "validation_description": "Uses repeated empirical subsampling (proportion π), random selection of preprocessing pipelines, and recomputation of clusterings; computes Adjusted Rand Index (ARI) for overlapping samples to quantify similarity between clusterings; aggregates mean stability over B resamples to choose the most stable pipeline.",
            "simulation_fidelity": "Empirical resampling (bootstrap/subsample) of observed dataset - approximates sampling variability but not physical process simulation.",
            "validation_sufficiency": "Paper treats this as a core sufficiency criterion for selecting models: high stability implies trustworthy scientific conclusions, but authors caution stability alone does not guarantee novelty or domain relevance and advise combining with generalizability and domain knowledge.",
            "validation_accuracy": "Reported comparatively (rankings) rather than absolute accuracy; spectral clustering n_neighbors=60 at k=2 had highest mean stability in the case study, followed by K-means k=8. No single global numeric threshold specified; selection is based on relative maxima or elbow rules.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; algorithm tested on APOGEE observational dataset with train/test split and multiple preprocessing variants.",
            "validation_comparison": "Compared stability across multiple clustering algorithms (spectral, K-means, hierarchical) and hyperparameters; used stability rankings to shortlist candidate models. Authors note spectral produced highest stability but K-means offered finer, partially stable clusters of interest.",
            "validation_failures": "Algorithm exposed clusters with low stability (e.g., clusters 3 and 7 in K-means k=8) which were flagged as unreliable; hence selection of m*,k* still requires interpretation and secondary generalizability checks.",
            "validation_success_cases": "Successfully identified spectral k=2 and K-means k=8 as promising candidates, enabling further inspection; stability-guided selection prevented over-reliance on a single preprocessing pipeline.",
            "ground_truth_comparison": "Selection based on internal stability metrics and external domain checks (comparison of resulting clusters to known abundance markers and GC membership); no independent ground-truth labels for all clusters.",
            "reproducibility_replication": "Algorithm is fully specified and provided in Interactive Supplement and GitHub; authors recommend pre-specifying G, M, K and resampling scheme to enable replication.",
            "validation_cost_time": "Computationally intensive when many preprocessing pipelines and hyperparameter combinations are included; paper notes feasibility trade-offs and that some preprocessing choices were excluded for computational reasons.",
            "domain_validation_norms": "Paper states stability across preprocessing choices is a domain-agnostic norm they advocate for; in disciplines where independent data collection is feasible, stability should be complemented by experimental replication.",
            "uncertainty_quantification": "Stability distributions (ARI over resamples) provide empirical uncertainty about model outputs; local stability per-object is computed from co-clustering frequencies.",
            "validation_limitations": "Stability measures depend on the set of preprocessing pipelines G considered; untested pipelines can yield different outcomes; stability does not equate to scientific significance or novelty.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2108.1"
        },
        {
            "name_short": "Consensus Clustering & Local Stability",
            "name_full": "Consensus Clustering with Co-clustering Matrix and Local Stability Metrics",
            "brief_description": "Aggregation of clustering outputs across resamples and preprocessing variations into a co-clustering frequency matrix, from which local stability per observation and consensus labels are derived.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Consensus Clustering / Co-clustering Aggregation",
            "system_description": "Runs the chosen clustering model across many preprocessed variants and resamples, records pairwise co-clustering frequency (consensus matrix), uses this to produce consensus cluster assignments and compute local stability for each object as its average co-clustering with same-cluster peers.",
            "scientific_domain": "General clustering applications; used here for astronomical chemical-tagging of APOGEE stars",
            "validation_type": "simulated",
            "validation_description": "Validation performed by aggregating multiple runs to measure repeatability: entries in the consensus matrix denote fraction of times two items fell in same cluster. Local stability (per-item) is the average co-membership rate within assigned cluster. Clusters with high internal co-clustering frequency are deemed reliable.",
            "simulation_fidelity": "Empirical/resampling-based aggregation (low-to-moderate fidelity estimate of clustering reproducibility across observed-data perturbations).",
            "validation_sufficiency": "Paper uses consensus clustering as a principal measure of stability and a necessary condition for trusting clusters; however, authors pair consensus results with held-out generalizability and domain checks before drawing scientific conclusions.",
            "validation_accuracy": "No global accuracy metric; quantified via distribution of co-clustering frequencies and local stability; in case study many stars showed high local stability for spectral k=2; for K-means k=8 several clusters had strong co-clustering while others did not.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Consensus derived solely from computational reruns across preprocessing/imputation/DR variations; no physical experiments.",
            "validation_comparison": "Consensus matrices allowed comparison of different clusters' robustness (e.g., cluster 2 vs cluster 3), and comparison with alternative algorithms (spectral vs K-means) in terms of block-diagonal strength.",
            "validation_failures": "Blocks with light entries in consensus matrix (low co-clustering frequency) highlighted clusters that failed stability checks (e.g., clusters 3 and 7 in K-means solution), leading authors to caution against interpreting those clusters.",
            "validation_success_cases": "High-consensus blocks corresponded to reliable groupings (clusters 1, 2, 6, 8). The consensus approach made it straightforward to identify and filter unreliable cluster assignments.",
            "ground_truth_comparison": "Consensus clusters were compared to known GC membership fractions (e.g., NGC6121 and NGC0104 composition) and known abundance trends to corroborate reliability where possible.",
            "reproducibility_replication": "Procedure and code for consensus aggregation provided in supplement and repository; reproducible given same preprocessing choices and seeds for stochastic steps.",
            "validation_cost_time": "Requires re-running clustering many times across preprocessing variants (computationally moderate-to-high cost depending on B and |G|).",
            "domain_validation_norms": "Consensus/clustering stability is a commonly-accepted internal validation step in clustering-heavy domains (genomics, astronomy) before committing to biological/physical interpretation.",
            "uncertainty_quantification": "Co-clustering frequency and local stability serve as empirical uncertainty measures for membership; clusters with wide spread of co-clustering values flagged as uncertain.",
            "validation_limitations": "Consensus only measures repeatability under the considered variations; it cannot detect systematic biases common to all pipelines or guarantee physical reality of clusters.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2108.2"
        },
        {
            "name_short": "Prediction-based Generalizability (Cluster Predictability)",
            "name_full": "Prediction-based Cluster Generalizability via Supervised Classifier",
            "brief_description": "A method to assess whether clusters discovered in training data generalize to unseen data by training a classifier to predict cluster labels and measuring concordance on a held-out test set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Cluster Predictability Validation (Random Forest classifier)",
            "system_description": "After clustering training data, fit a supervised classifier (random forest in the case study) to predict training cluster labels from features; apply the classifier to test data and compare predicted labels to clusters obtained by re-clustering the test set (or to test-set labels) using ARI and per-cluster precision to quantify generalizability.",
            "scientific_domain": "Clustering validation across domains; applied in astronomy case study",
            "validation_type": "simulated",
            "validation_description": "Uses supervised learning as a proxy-validity check: trains RF on training features→cluster labels, predicts labels on held-out test features, and computes ARI (and per-cluster precision) between RF-predicted labels and cluster labels estimated in test set. Cluster-wise precision is used to identify which clusters generalize well.",
            "simulation_fidelity": "Empirical predictive-check (data-driven), fidelity depends on representativeness of the held-out test set; not a physics-based simulation.",
            "validation_sufficiency": "Paper treats prediction-based concordance as one important measure of generalizability; while supportive evidence, authors recommend combining with stability and domain checks before asserting scientific claims.",
            "validation_accuracy": "Example reported: cluster 2 local generalizability (precision) = 1.00 on the held-out test set (averaged across imputation methods/datasets). Overall ARI between RF-predicted and test cluster labels used as aggregate metric but exact overall ARI value not reported in-text.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; classifier-based validation used instead of gathering new observational data.",
            "validation_comparison": "Compared cluster-wise generalizability to cluster local stability; authors found clusters with high local stability often had high generalizability (e.g., clusters 1 and 2) whereas some low-stability clusters showed poor predictability.",
            "validation_failures": "Some clusters (e.g., cluster 3) have low predictive precision and thus fail the generalizability test, indicating that they are not reliable discoveries.",
            "validation_success_cases": "Clusters 1, 2, 6, 8 passed both stability and predictability checks; cluster 2 achieved perfect reported generalizability.",
            "ground_truth_comparison": "Prediction-based validation is internal (train/test) and compared to clusterings on the test data; external ground truth comparisons were performed separately via catalog membership or literature.",
            "reproducibility_replication": "Method is fully described and code provided; reproducible if same train/test splits and RF hyperparameters are used.",
            "validation_cost_time": "Training classifiers and re-clustering test data adds computational cost but is generally cheaper than collecting new physical data; cost scales with number of models and resamples.",
            "domain_validation_norms": "Using supervised prediction to assess unsupervised cluster generalizability is an accepted strategy in statistics/machine learning literature when independent ground truth is lacking.",
            "uncertainty_quantification": "Per-cluster precision and ARI distributions provide uncertainty about generalizability; authors recommend reporting cluster-wise metrics rather than only aggregate measures.",
            "validation_limitations": "Prediction-based validation depends on (i) representativeness of the test set, (ii) stability and transferability of features used by the classifier, and (iii) the classifier's inductive biases. It may mask clusters that are real but poorly captured by the classifier's feature mapping.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2108.3"
        },
        {
            "name_short": "Neighbor-retention Metric for DR",
            "name_full": "Neighborhood Retention Metric for Dimension Reduction Methods",
            "brief_description": "A quantitative metric that measures the proportion of k nearest neighbors retained when mapping from high-dimensional space to low-dimensional embeddings (used to tune embedding hyperparameters and choose between methods like tSNE, UMAP, PCA).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Neighbor Retention / Neighborhood Preservation Metric",
            "system_description": "Computes, for a range of neighborhood sizes k, the fraction of original high-dimensional k nearest neighbors preserved in the 2D embedding; used to compare dimension-reduction methods and tune hyperparameters (e.g., tSNE perplexity or UMAP n_neighbors).",
            "scientific_domain": "Data visualization and dimension-reduction validation across many scientific domains; applied in astronomy case study",
            "validation_type": "simulated",
            "validation_description": "Validation is computational: for each embedding and neighborhood size k, count how many of the original k nearest neighbors in feature space remained among k nearest neighbors in the embedding; aggregate retention curves inform selection of DR method and hyperparameters that balance local and global structure preservation.",
            "simulation_fidelity": "Empirical geometric fidelity measure (high relevance for nearest-neighbor structure but not a physical simulation); effective for judging structural preservation but not absolute truth.",
            "validation_sufficiency": "Paper uses neighbor-retention as a primary criterion to select DR method and hyperparameters; demonstrates tSNE consistently had higher neighbor retention than UMAP in their dataset, leading to dropping UMAP from further consideration.",
            "validation_accuracy": "Presented as comparative curves; tSNE consistently showed higher neighbor retention across neighborhood sizes in the APOGEE case study; exact numeric retention rates plotted in figures (not tabulated in text).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experimental tests; purely computational evaluation of embedding fidelity to original neighborhood relationships.",
            "validation_comparison": "tSNE (multiple perplexities) vs UMAP (multiple n_neighbors) vs PCA: tSNE had uniformly higher neighbor retention than UMAP, PCA highest global retention; led to selecting tSNE (perplexity=100) for visualization and downstream analyses.",
            "validation_failures": "UMAP performed worse on neighbor-retention for this dataset and was removed from the analysis; the authors also caution that DR methods can be stochastic and require repeated checks.",
            "validation_success_cases": "tSNE hyperparameters (perplexities 30 and 100) produced embeddings with high neighbor retention for both small and large neighborhoods, supporting their use for visualization and as inputs to clustering.",
            "ground_truth_comparison": "Neighbor retention compares embedding to original high-dimensional measurements (treated as the reference), not to an external ground truth.",
            "reproducibility_replication": "Procedure deterministic given seeds; authors examine multiple random seeds and hyperparameters to ensure reliability; code provided.",
            "validation_cost_time": "Computing neighbor-retention across multiple hyperparameter settings and resamples is computationally inexpensive to moderate depending on dataset size.",
            "domain_validation_norms": "In visualization tasks, neighborhood-preservation metrics are common domain-agnostic checks to ensure embeddings do not artificially distort structure; authors recommend them as a validation step.",
            "uncertainty_quantification": "Retention curves across neighborhood sizes and across multiple random seeds/hyperparameter settings quantify variability in embedding fidelity.",
            "validation_limitations": "Neighbor retention assesses only preservation of nearest-neighbor relationships and not other structures (e.g., global manifold geometry); different DR goals (local vs global) require different metrics.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2108.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data",
            "rating": 2
        },
        {
            "paper_title": "Stability approach to regularization selection (StARS) for high dimensional graphical models",
            "rating": 2
        },
        {
            "paper_title": "Cluster validation by prediction strength",
            "rating": 2
        }
    ],
    "cost": 0.01978775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices
5 Jun 2025</p>
<p>Andersen Chang 
Department of Neuroscience
Baylor College of Medicine</p>
<p>Tiffany M Tang 
Department of Applied and Computational Mathematics and Statistics
University of Notre Dame</p>
<p>Tarek M Zikry 
Department of Statistics
Columbia University</p>
<p>Zuckerman Institute &amp; Irving Institute
Columbia University</p>
<p>Genevera I Allen genevera.allen@columbia.edu 
Department of Statistics
Columbia University</p>
<p>Zuckerman Institute &amp; Irving Institute
Columbia University</p>
<p>Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices
5 Jun 20252869F538C88B16AED060534FEA4019D9arXiv:2506.04553v1[cs.LG]unsupervised learningdimension reductionclusteringdata-driven discoveryreproducibilityastronomy
Unsupervised machine learning is widely used to mine large, unlabeled datasets to make data-driven discoveries in critical domains such as climate science, biomedicine, astronomy, chemistry, and more.However, despite its widespread utilization, there is a lack of standardization in unsupervised learning workflows for making reliable and reproducible scientific discoveries.In this paper, we present a structured workflow for using unsupervised learning techniques in science.We highlight and discuss best practices starting with formulating validatable scientific questions, conducting robust data preparation and exploration, using a range of modeling techniques, performing rigorous validation by evaluating the stability and generalizability of unsupervised learning conclusions, and promoting effective communication and documentation of results to ensure reproducible scientific discoveries.To illustrate our proposed workflow, we present a case study from astronomy, seeking to refine globular clusters of Milky Way stars based upon their chemical composition.Our case study highlights the importance of validation and illustrates how the benefits of a carefully-designed workflow for unsupervised learning can advance scientific discovery.</p>
<p>Introduction</p>
<p>Machine learning has emerged as an important tool in deriving data-driven insights, enabling researchers to extract meaningful patterns from large and complex data that would have been difficult for traditional methods to identify.As scientific problems increasingly involve high-dimensional datasets with nonlinear relationships and multivariate interactions, machine learning has become a critical tool for modeling complex relationships, identifying hidden structures, and making predictions in data science problems [136,112].Beyond improving accuracy and efficiency, machine learning enables novel forms of scientific discovery, such as identifying unexpected correlations, generating hypotheses, and simulating outcomes under diverse scenarios in scientific research contexts across multiple disciplines, including biology, chemistry, economics, and artificial intelligence.Due 1 to this pervasion, efforts have been made amongst those in the data science community to establish general, model-agnostic frameworks for machine learning; this is important to ensure that novel findings are reproducible and reliable [2,144,154] and for improving consistency and effectiveness of collaborative work [189,20], especially with non-statisticians.</p>
<p>In statistical machine learning, the approaches used for modeling in data analysis problems can be broadly categorized into two different classes.The first of these, known as supervised learning, is defined by the existence of an explicitly-defined output or response variable in the data set.Machine learning techniques for supervised learning use the response variable as a quantitative target of modeling, finding mappings of the input or predictor variables that minimizes the error between the expected value of the response based on the observed predictor variables and the actual realized values of the response variable.The second class of approaches, known as unsupervised learning, attempt to make discovery without a pre-defined response variable.Instead, these methods find hidden relationships and patterns in data by constructing representations of the structure of observations and features that allow for the discovery of correlations, groupings, and latent spaces in high-dimensional settings.Our goal in this paper is to provide collaborative data scientists and biostatisticians with a framework for creating workflows for the latter of these, i.e. unsupervised learning methods, with a particular focus on workflows for projects for generating data-driven discoveries using these methods.</p>
<p>Unsupervised Learning for Discovery</p>
<p>Unsupervised learning methods are a critical component of any data science or machine learning pipeline.In practice, unsupervised learning techniques are typically applied in supervised learning workflows for the purposes of data exploration, data visualization, feature engineering, preprocessing, and data preparation.However, unsupervised learning can also be a crucial tool for generating novel data-driven discoveries from unlabeled data.Examples of major unsupervised tasks include clustering to discover group structures [184], graphical models to discover relationships [74], anomaly detection to discover informative outliers [31], and dimension reduction to discover low-dimensional latent subspaces capturing major patterns in high-dimensional data [173].These unsupervised learning methods have been applied for generating novel, important data-driven discoveries in many different types of data science research problems in a diverse range of fields.For instance, clustering methods have been applied in genetics studies for identifying common gene expression patterns [47,79], in astrostatistics to investigate the distribution of mass and galaxies in the universe [60,75,85].Network analyses have utilized graphical models to study the structure of pathways underlying the connectome [27,59,65] and the organization of functional circuits [191,53,179] within the brain, to find interactions in RNA sequences and interactions in gene expression in disease pathologies [155,97], and to study the movement of air particulate matter [50,81].Also, latent space estimates derived from applying dimension reduction methods have revealed spatiotemporal changes in climate patterns [62,146,165], to estimate novel drug-disease interactions [128,174,89,36], and to uncover common biochemical signatures in gut microbiome samples [99,7].These examples show that unsupervised learning can be an extremely useful tool in generating data-driven discoveries, and thus should be a foundational part of the literature in data science practice.</p>
<p>Workflows</p>
<p>Though unsupervised discovery is an extremely important facet of modern data science and machine learning, there is currently a lack of well-established best practices and specific workflow procedures for producing robust, reproducible discoveries using unsupervised learning.Several works in the literature have studied and proposed workflows and practices for specific types of unsupervised learning tasks, including for clustering [72], autoencoder neural networks [46,13], natural language processing [158], dimension reduction [82,6], and image processing [119,131], as well as for specific fields such as single-cell multiomics [37,24] and pathology [135,153].However, there is currently a dearth of existing resources concerning data science analytical pipelines for unsupervised discovery that are broadly applicable across variable techniques and domains.The lack of well-established general unsupervised discovery workflows is due in large part to the substantial number of unique challenges that arise from unsupervised learning for scientific discovery.One of these primary challenges is that, by nature, unsupervised learning problems do not include a target outcome that can be used as an objective for quantifying the accuracy of model fits, thereby complicating the tasks of model comparison and hyperparameter selection.For many different classes of unsupervised learning problems, there also does not exist the concept of a null distribution for which quantitative results can be compared against, thus complicating the task of post-hoc inference and model selection [124,176].Additionally, several unsupervised learning methods do not provide a mechanism for mapping new data points to a pre-existing model fit, which precludes the ability to use a training-test split for external validation of model performance [143,152].Furthermore, recent work has shown that interpretations from widely used unsupervised learning methods within clustering and dimension reduction are largely unreliable and yield unstable results, underscoring a greater need for emphasis on validation and robustness within workflows [63].</p>
<p>In this paper, we address the gap in guidance on how to create unsupervised learning workflows for scientific discovery.We first present a generalized, model-agnostic data science workflow for unsupervised learning, with specific recommendations and considerations for the challenges of unsupervised scientific discovery, and also provide a concise discussion of best practices in unsupervised analysis.We then illustrate our workflow recommendations through an extensive case study from astronomy on discovering common origins of stars based on their chemical signatures and show how decisions made about the procedures in the workflow can affect resulting scientific discoveries.</p>
<p>Workflow for Unsupervised Discovery</p>
<p>In this section, we present a model-agnostic workflow for unsupervised learning for the task of producing data-driven discoveries.We note that many of the aspects of the workflow we discuss below will follow that of a standard data science pipeline [92,116,168,103,187].However, we will provide more detailed discussion into aspects that are specific to working with unsupervised learning methods.A summary of our suggested best practices can be seen in Figure 1.</p>
<p>Workflow</p>
<p>Scientific Question &amp; Data</p>
<p>The first step in creating an unsupervised learning workflow is to formulate the scientific question to be answered and to determine the type of data that will be collected or used to answer this question.It is important here to begin with a thorough literature review and to discuss with domain experts.This will help provide an understanding of the previous research in the field of study, ensure that the project addresses an existing gap in knowledge guidance in the field, and help position potential findings within the overall current knowledge in the literature [61].Moreover, the scientific question should be formulated in tandem with a detailed understanding of the data Figure 1: Illustration of Best Practices for Data-Driven Discovery via Unsupervised Learning.We summarize best practices, starting with generating validatable scientific questions, through robust data preparation and exploration, using a range of modeling techniques, evaluating the stability and generalizability of unsupervised learning conclusions, and the effective communication and documentation of results.generation or collection procedures.Though problem formulation and data collection are typically considered two different aspects of a data science workflow, we emphasize that both of these facets need to be determined concurrently in order to ensure that the two are congruent for meeting the overall goals of the research project.</p>
<p>For data collection, it is essential that the number of observations and the features that will be needed for the entirety of the pipeline be determined before any work is done.If new data will be gathered for the workflow, enough observations should be collected such that there is adequate statistical power for all future modeling and validation procedures.If instead the workflow will be utilizing data previously collected, then similar datasets from different sources can potentially be used to augment the available observations or applied as a test set for validation.Also, for both cases, the features to be measured should be thoroughly planned as well in order to ensure that the data are appropriate for answering the scientific questions; for example, potential confounding variables should be considered in order to aid with interpretations of findings [84].</p>
<p>Specification of the scientific question is pivotal for defining the overall direction of the workflow and research project as a whole.Furthermore, in unsupervised settings, the key in this step is to translate high-level research objectives into scientific questions that are vaildatable, i.e. with actionable objectives and quantifiable evaluation metrics, and which are answerable with the data that are available for the current project.For example, a broad research goal could be to explore the biological characteristics associated with observed incidences and progression of a particular disease using multiomics data, physiological testing results, and clinical notes.This overall project objective could be accordingly parsed and converted into several specific research questions involving unsupervised discovery, such as:</p>
<p>• Are there differing phenotypic subgroups amongst the full set of disease cases?How many of these subgroups can we reliably find?How do the subgroups differ in terms of physiology or responses to treatment?</p>
<p>• Which features significantly covary amongst the cases?Are there physiological features that can be used as an indicator of genetic susceptibility?</p>
<p>Notably, these questions are addressable by the data modalities that are available, and have clear quantifiable or objective measures for modeling results.On the other hand, a question such as "Are there covarying features associated with vulnerability?" would not be a particularly useful scientific question, as "vulnerable subject" in this context needs to be re-defined in such a way that there is a quantifiable metric, e.g.time to death or level of symptoms.Finally, we note that developing a validatable scientific question is a critical component, but can be especially challenging for unsupervised learning, as there are fewer objective measures for assessing results.Before delving into discussions of the further parts of the unsupervised learning workflow, we pause to stress that the entire plan for modeling and validation should be planned out in this step as well.Though unforeseen circumstances may require adjustments to the workflow as steps are performed, creating an overall initial plan before starting the analysis provides several important advantages.First, this will help prevent critical incidents, such as failing to collect important features or not creating a train-test split in advance, that may require redoing parts of the workflow or collecting additional data.Secondly, this will help to ensure that all parts of the workflow are coherent with one another and with the overall research goals, e.g. that the results produced by the modeling steps actually address the scientific question, or that engineered features are compatible for use in modeling and validation procedures.Lastly, setting a plan will allow for all stakeholders in the project to know what to expect from the data analysis and to provide feedback before work begins.</p>
<p>Data Preparation and Exploration</p>
<p>Once data have been collected, data preparation and exploration should be performed before modeling in order to ensure computational feasibility, to improve overall modeling results, and to aid in understanding and contextualization of findings.While we have separated these two topics into separate subsections, preparation and exploration should be a cyclical, closed-loop process that occurs across multiple iterations, as the latter will inform the user on the potential extra that needs to be done in the former.</p>
<p>Preparation.Typically, preprocessing of the raw data will need to be conducted in order to prepare the data for modeling.Firstly, the data should be randomly split into training and test sets for validation purposes before any other step.Next, data cleaning procedures should be applied; common tasks here include fixing errors, detecting and possibly removing unwanted outliers and artifacts, and filtering features that do not appear to be scientifically or quantitatively useful.Treatment of missing observations should be done here as well; we recommend using multiple imputation methods [139,28], as this will provide a naturally validatable approach.Additionally, feature engineering, i.e. the creation of new features based on the raw features available in the data, can be useful here to produce variables that have potentially much stronger associations or grouping structures in the data or which may be more pertinent scientifically compared to those in the raw feature set [170].</p>
<p>In most cases, there exist multiple reasonable preprocessing strategies that can be used for any particular research question and data set.Also notably, it has been shown in many previous works that unsupervised techniques are extremely sensitive to changes in the preprocessing steps taken [41,44,190,1,150].Thus, the preprocessing steps taken should also be tested as part of the validation step discussed below.In particular, one should test the sensitivity of unsupervised discoveries to different preprocessing approaches in the workflow [187].This can be done by repeating the data preparation procedures with different, randomized choices of the preprocessing methods applied, e.g.changes in feature filtering decisions, hyperparameter selections, or outlier detection methods.Additionally, some approaches, such as multiple imputation techniques, are stochastic in nature and thus can be applied multiple times with different seeds to quantify sensitivity.</p>
<p>Exploration.Alongside preprocessing, the data preparation step of the workflow also includes visualization and exploration of the data set.Exploratory data analysis is essential for gaining insight into the structures and patterns that exist in the data, including the existence of outliers, batch effects, and multivariate relationships between features.The knowledge gained from data exploration can be used in parallel with data preprocessing in order to inform necessary steps, such as whether additional outlier removal or data cleaning needs to be performed and what types of features may be of use to engineer or to filter out.The most important recommendation for the task of data exploration is to use as many different visualization formats as possible in order to fully explore the structure and relationships in the data.Different visualization methods and arrangements are best at displaying different aspects of the marginal and conditional distributions and multivariate relationships of features and thus can lead to different interpretations of the data [68,25].Further, different strategies may be required for visualizing high-dimensional data and relationships compared to low-dimensional data.Along the same lines, several types of unsupervised learning methods, particularly nonlinear dimension reduction and outlier detection with advanced machine learning techniques, create stochastic results, and thus repeated visualizations using these same techniques can help confirm the soundness of these explorations.We show an example of the benefits of using multiple visualization strategies as part of our case study in Section 3.</p>
<p>Modeling</p>
<p>In this step, the unsupervised learning models for analysis are selected and applied.Many resources currently exist which provide overviews of frequently used techniques for common unsupervised learning tasks, including clustering [88], dimension reduction [66], network analysis [161], and outlier detection [164], so we leave introductions and discussions on the specific techniques for each of these classes of unsupervised learning tasks to previous works.For choosing between different methods for a task, interpretability and concordance with addressing the scientific questions should be taken into account.For example, in the realm of dimension reduction, UMAP and tSNE will provide local interpretations of data structure while PCA will provide a global interpretation [91], so care should be taken to align the appropriate method with the desired interpretation and scientific question.</p>
<p>Across all different types of unsupervised learning tasks, it is important to test multiple modeling techniques in the workflow for each individual task; this provides several advantages over solely applying a single method.Firstly, the trustworthiness of a finding can be measured by the frequency by which it appears across different modeling results.Previous studies have shown that, in general, if a discovery is true, then it should be apparent across different modeling techniques and hyperparameter selections [140,94,187].Therefore, applying multiple techniques can help facilitate validation through providing a measure of robustness of findings based on the repeatability of each one across different applied methods and workflow settings.This technique for validation is especially useful in the context of unsupervised learning, as quantitative metrics for validating findings can otherwise be difficult compared to supervised learning, in which predictability of a target variable can be used.Secondly, in many cases, one can get different findings and results from applying different methods addressing the same task [142]; methods for each task are not necessarily interchangeable.It can also be difficult to determine a priori which of a set of methods will produce the most useful results.Thus, applying multiple methods will increase the likelihood of generating data-driven discoveries through providing different modeling perspectives on the data, akin to the reasoning behind ensemble methods in supervised learning [45].In addition, if one of the research goals is to produce a final trained model from multiple potential algorithms, then the application of multiple methods will generally require further model selection or model aggregation steps in order to produce the final model.General procedures for model selection will be discussed as part of validation in the following subsection.</p>
<p>For each unsupervised technique in the workflow, it is also important to test multiple hyperparameter settings and tune all hyperparameters in the validate stage, discussed subsequently.Several previous studies have shown that choosing different hyperparameter settings can yield vastly different results and conclusions [127,178,8].While hyperparameter tuning is standard practice in supervised learning, this is less common in unsupervised learning where many instead select hyperparameters by looking at the data or based upon the concordance of results with prior knowledge.These practices introduce subjective bias into the unsupervised workflow, however, and should hence be avoided.Instead, we suggest testing multiple hyperparameter settings across each unsupervised method to ensure robustness of results to hyperparameter choices.Additionally, testing multiple hyperparameters are tuning these in the validation stage, discussed next, is a critical component of selecting the final unsupervised model and reporting findings.</p>
<p>Validation</p>
<p>After selection of the models that will be used for analyzing the data, techniques and procedures for validating the results produced by said modeling procedures should be determined.Validation techniques are needed to assess reliability of results across all facets of the workflow.In particular, quantitative validation is essential for assessing the generalizability of results to new data and different decisions in modeling procedures, as well as promoting trust in data-driven discoveries [107,95].While human verification via literature review and qualitative comparisons can be helpful, it can be wrought with human biases and deduction of patterns that do not exist [121,151,163].Ideally, validation analyses should be performed via followup experiments, in which completely new data are collected and used to test whether the same findings are detected.However, in most cases collecting new data is costly in terms of time and material resources, and thus is not feasible.Thus, validation is instead typically performed using the data that is already available through creating a training-test split during data preparation, or by applying resampling strategies which simulate refitting models on new data sets [117,109].</p>
<p>Below, we define common frameworks for validating unsupervised discoveries using available data as well as model-agnostic validation measures to assess the stability and generalizability of unsupervised findings.Notably, for many unsupervised approaches, standard model-agnostic methods for validation currently do not exist and this is an open area of research.This occurs as unlike supervised learning, there is no ground-truth target to compare our findings against.However, when applicable, we introduce general validation methods that can be used across different methods for specific unsupervised learning tasks.We also note that the validation techniques to be used should be a critical consideration when planning the other parts of the workflow, including how the preparation and modeling methods selected can be harmonized with the validation plan as well as how to best communicate the uncertainty and reliability results from applying validation techniques.</p>
<p>Validation Metrics One primary assessment for validation in unsupervised discovery is the stability of results, i.e. how similar or reliable the modeling results and interpretations of findings are with respect to arbitrary randomness and perturbations in data as well as differences in modeling decisions that can occur during preprocessing and modeling [21,186,90,187,15].Often, stability approaches utilize random perturbations of the observations, i.e. subsampling, bootstrapping, or random additions of noise, as well as different selections of hyperparameter settings.If a particular method produces stochastic outputs based on random initialization, e.g.UMAP or k-means clustering, using multiple iterations of this method can also be used to quantify stability.Metrics in the stability framework generally measure goodness-of-fit as a function of proportion of resamples in which individual results are repeated.Procedures for stability estimation have become well-established for certain specific unsupervised learning tasks; for example, in clustering, consensus clustering [113,70] aggregates results from repeated clustering runs on data subsamples into a single metric that is weighted towards agreement of clustering assignments between each pair of observations across iterations.Similarly for graphical models, StARS [101] measures stability via agreement of edge selections between pairs of features across model estimates from resampled data sets.In anomaly detection applications, rank stability methods that utilize the invariance of the magnitude of outlier measure have been proposed [123].Also, for nonlinear dimension reduction, previous work has proposed methods that quantify the variability in the embeddings across bootstrap samples [162] or through the similarity in nearest neighbor structure with respect to the original data [182].In other unsupervised learning tasks, the resampling-based framework can still be used, but specific measures for evaluating the goodness-of-fit are not well-established and thus should be determined by the specific problem.</p>
<p>Besides stability, another model-agnostic validation measure is generalizability, i.e. how well the modeling results hold if the analysis pipeline were to be rerun given new observations and if different modeling decisions are made given these new observations [177,185].This is of particular importance in assessing the applicability of discoveries to broader settings, such as different subpopulations or processes [69].Measuring generalizability in the unsupervised learning paradigm is generally more difficult than measuring stability, as for many tasks the quantitative delineation of "good" as compared to "bad" generalizability is unclear.One potential general paradigm for measuring generalizability that may be useful for certain methods is the idea of using prediction validation with a supervised learning model and training-test split [166,96].For example, in clustering tasks, clustering methods would be applied to both the training and test data sets to obtain estimated cluster labels; a supervised classification model is then fit using the training data set to predict the training cluster labels, and predictions of cluster labels are made using this model for the test data set.The generalizability of each clustering model result is then calculated as the concordance between the predictions from the classification model and the estimated cluster labels on the test set [73].Similar approaches to this have been proposed in recent work for specific unsupervised methods, including PCA [86], nonlinear dimension reduction [137], and Gaussian graphical models [49], but this framework has not been widely established and is still an open area of research.</p>
<p>Model Selection &amp; Hyperparameter Tuning</p>
<p>In the case where multiple techniques are applied in the modeling step or where multiple preprocessing pipeline decisions are plausible, the task of choosing a single "best" pipeline is often desired as part of the overall research objective.Often, hyperparameter tuning is important as well, as many studies have shown that the choice of hyperparameters can have a large impact on the final results of data analysis pipelines in a wide variety of data contexts [149,178,157,100,8].Like with validation in general, model selection and hyperparameter tuning is less straightforward in the unsupervised learning framework due to the lack of a response variable for comparison.Here, both the stability-based and generalizability-based quantitative metrics mentioned above that are used for validating results can also be applied for this task; the final selected model is then the overall combination of preprocessing steps, model, and hyperparameter settings that produces the optimal level of the stability or generalizability metric [188].For example, to select the number of clusters for a particular clustering method, the consensus clustering method mentioned above can be implemented across differing numbers of cluster centroids; a stability score threshold or elbow plot can then be used to select the optimal number of clusters required that meets an acceptable quantitative performance level.If multiple clustering methods have been used, this procedure can be applied for each individual clustering technique before selecting among these clustering methods, paired with their optimal number of clusters, to obtain the final model.</p>
<p>Communication</p>
<p>Lastly, strategies for communication of ideas, results, and insights need to be determined for the workflow.The communication of outcomes and results is extremely important in collaborative work [156], and thus should be a central focus of the planning of the workflow.Importantly, communication between data scientists and other research collaborators should occur before, during, and after analyses are performed with the workflow.During the planning stage, communication with collaborators is important to ensure that all stakeholders on the project are in alignment on the goals and objectives [17].Communication of methods and outcomes during data exploration and modeling will help ensure that results are correctly interpreted [115].As modern unsupervised machine learning techniques are capable of generating findings that are not scientifically relevant in context [2,10], results should be evaluated by researchers with domain-specific knowledge in order to ensure outcomes will produce useful insights in context.Lastly, effective communication of findings at the end of the pipeline is important for scientific validation of results, e.g.determining which discoveries are important.These discussions can also serve as a catalyst for formulating new hypotheses for follow-up analyses [51].For general communication with collaborators, it is important to have simple, clear explanations of statistical methods and contextually relevant interpretations of findings and the insights that can be derived from them [38].Also, in addition to the raw findings and results, the communication plan should include a discussion uncertainty and limitations of work in order to provide the audience with a gauge on the reliability of results.</p>
<p>Effective data visualizations are also an extremely important part of understanding data sets and results as part of any data science workflow [83,171].However, this is especially important for unsupervised learning workflows, as quantitative assessments of results are often not available due to lack of ground truth for comparison for many tasks.Thus, we recommend that visualizations be created to study outputs at nearly every step of the pipeline.Additionally, we suggest using multiple visualizations that vary the aesthetic mappings to features of the graph as well as the type of graphic in order to maximize the information gleaned from the data and modeling and to minimize the probability of biases or misinterpretations that arise from only using a single visualization [133,134].</p>
<p>The creation of interactive visualization tools can also be extremely valuable for understanding interactions between features in large, high-dimensional data sets and for allowing audiences to explore data and results [175,85].Many resources are available that outline the core concepts of creating statistical graphics, e.g.[180,52,77]; we recommend learning and applying these in order to produce effective visualizations for data exploration and communicating results.</p>
<p>Another important facet of communication is reproducibility.In unsupervised learning (and more broadly across all data science problems), the topic of reproducibility has been a preeminent topic of discussion [181,58].Showing reproducibility and reliability of the unsupervised learning workflow and the discoveries produced is paramount for promoting trust in the modeling results [109,160].Generally, there are several different facets of reproducibility and reliability that need to be shown.In terms of computational reproducibility, the same code, deterministic algorithms, and data should be able to produce the exact same results, and the same code with stochastic algorithms, e.g.different simulation runs or random initializations, should produce similar results.To this end, all computational tools and procedures should be made available.For assessing statistical replicability, all modeling decisions and justifications for all steps of the analysis should be documented and considered as part of the final communication plan.</p>
<p>Best Practices for Unsupervised Discovery</p>
<p>Here, we summarize the best practices across the different facets of the unsupervised workflow described above.The recommendations we discuss below reiterate the core concepts of building an unsupervised learning workflow that should always be considered to produce more reliable unsupervised scientific discoveries.</p>
<p>Plan Ahead.All aspects of the unsupervised learning workflow should be planned out before any actual analysis occurs.While changes may need to be made afterwards due to unforeseen circumstances, the lack of planning can often lead to repeating previous work.Common examples of this include having insufficient data, specifying scientific questions not conducive to objective validation, or failure to create a training-test split for validation a priori.</p>
<p>Communicate Frequently.Communication between the data scientist and other members of the project should occur regularly throughout the course of the analysis, starting from the planning stage all the way until results are finalized.This will help ensure results are scientifically meaningful and novel.</p>
<p>Visualize, Visualize, Visualize.Data visualization is the best way to help understand the structures and associations within a dataset.Visualizations should be used at each step in the workflow to assess the results that are produced by each method applied.Also, using multiple visualizations at each step will aid in gaining a comprehensive understanding of the data.</p>
<p>Use Multiple Techniques.</p>
<p>For each step in the exploration and modeling phases, using multiple techniques can assist in generating discoveries through providing different insights on the dataset.Thus, the workflow should include more than one approach for each step in order to maximize the potential for relevant findings as well as assess the robustness of findings to arbitrary modeling choices.</p>
<p>Demonstrate Reproducibility, Replicability, and Reliability.In addition to statistical validation, the reproducibility, replicability, and reliability of results need to be addressed as well.Code should be documented and provided to show computational reproducibility.Decisions made in terms of preprocessing and modeling choices should be documented and communicated as part of the results in order to appraise replicability.Lastly, validation of all steps of the workflow should be performed to assess reliability.</p>
<p>Validate, Validate, Validate.All results created from the workflow should be validated in order to provide a gauge for the trustworthiness and reliability of all data-driven discoveries; this includes validating both the raw outcomes from the quantitative approaches at each stage as well as the consequent scientific discoveries produced by these outcomes.Most importantly, reliability needs to be assessed across all different aspects of the workflow; this includes measuring the effect of preprocessing choices, hyperparameter tuning, and model selection.Additionally, if possible, both stability and generalizability of results should be considered in validation.</p>
<p>Case Study: Finding Common Origins of Milky Way Stars</p>
<p>To help illustrate these best practices, we next provide a case study, investigating the shared origins (or clusters) of Milky Way stars based upon their chemical composition.In this section, we begin by introducing the scientific question and data.We then detail our analysis plan.Finally, we present the results of our investigation, proceeding from an exploratory data analysis through a stability-driven clustering analysis and concluding with a rigorous validation of our identified clusters.Through this case study, we illustrate how the benefits of a carefully-designed workflow for unsupervised learning can advance scientific discovery.</p>
<p>Scientific Question and Data</p>
<p>The emergence of large spectroscopic surveys of the Milky Way has led to significant interest in studying the chemical origins of the Galaxy's formation.The Apache Point Observatory Galactic Evolution Experiment (APOGEE) is a high-resolution, near infrared survey of stars comprising the disk, or the primary area of the Milky Way's stellar mass [126].Features include chemical abundances of 19 key elements in star formation, radial velocities, and meta information for each star such as surface gravity and effective temperature.Researchers are often interested in using the APOGEE data to explore the stellar origins of the Milky Way and specifically how stars were formed or evolved chemodynamically over large periods of time [12,29,48,60,75,76].Existing observations show that in the process of forming a stellar body, parent molecular clouds can produce hundreds of stars in a single burst [93], but due to astronomical dynamics, these shared origins are challenging to find.In this case study, we hence aim to cluster stars chemically to identify shared origins and trace the history of the Milky Way.</p>
<p>Globular clusters (GCs) are an ideal population to begin studying the origins of Milky Way development.GCs are sites of dense star formation, comprised of 10 5 −10 7 stars formed at the same time, and have been shown to be critical building blocks of stellar bodies such as halos, discs, and spheroids [169,106,43,87], with their full impact remaining an open research question in the field [141].GCs exhibit specific chemical abundance trends [71,110,105], making them a prime target for assessing clustering to find shared origins of the stars.Still, while many stars in the APOGEE survey have been assigned to GCs based on current proximity, researchers are also interested in identifying deeper shared origins of possibly distant bodies.There exists a wide body of literature, which here we only refer to a small subset, aiming to do exactly that, not only in GCs but in other stellar populations within APOGEE [130,122,87,12,75,98,14].In short, this literature focuses on grouping GCs in order to find their common origins; what progenitors they share, how the descendants are redistributed around the galaxy, and how they can be distinguished.Through these groupings, astronomers seek to characterize the underlying evolution in the age and metallicity of the Milky Way to trace its assembly.Translating this scientific question into a validatable data science goal, our objective is to further group GCs based on their chemical signatures to find reliable, robust, and scientifically interpretable clusters of stars.</p>
<p>Plan Ahead</p>
<p>Before undertaking this data analysis, it is crucial to plan ahead in all stages of the scientific workflow.This planning ahead includes both specifying various analytical choices to be made and outlining the steps in the overall workflow.</p>
<p>Data Preprocessing and Modeling Choices</p>
<p>To begin, there are numerous human judgment calls or choices that inevitably must be made throughout the unsupervised workflow.In particular, though the pipeline of data processing from raw infrared spectra data to chemical tagging is considered to be standard and well-defined [118], there are many different, but equally-reasonable data preprocessing choices that have been explored in the previous scientific literature, including different quality control filtering thresholds, data imputation procedures, and subsets of elements that are included and scientifically useful for clustering [167,125,29,32].Within the modeling stage, different studies have used different clustering approaches, ranging from density-based approaches such as HDBSCAN to K-Means, Gaussian Mixture Models, and deep learning-based approaches [29,64,32,130,120,19,30,18].Each of these studies finds a different number of clusters, with several concluding the lack of feasibility of clustering this data with confidence [130,29].Efforts to statistically validate these clusters have also been limited, with studies rarely comparing the stability and generalizability of these methods, or their sensitivity to different preprocessing choices.Similar difficulties arise with dimension reduction, with some studies using tSNE [4], some with UMAP [29], and some using PCA [130] to visualize and analyze the APOGEE data.</p>
<p>For our case study, we detail the various data preprocessing choices, dimension reduction methods (with hyperparameters), and clustering methods (with hyperparameters) under consideration in Table 1.Briefly, we consider different choices of quality control filters, sets of elemental abundance features used in the analysis (either 7, 11, or 19 features, each standardized to have mean 0 and standard deviation 1), data imputation methods (using mean-imputation or random forest-based imputation [159]), dimension reduction methods prior to clustering (none, PCA [80], tSNE [172], and UMAP [108], each with various hyperparameters), clustering methods (spectral clustering [3], K-means [9], and hierarchical clustering [111], each with various hyperparameters), and finally the total number of clusters (k = 2, . . ., 30).</p>
<p>Workflow Plan</p>
<p>We next outline our workflow, organized into three parts: (1) data preparation and exploration, (2) modeling and validation, and (3) interpretation and communication.</p>
<p>Data Preparation and Exploration</p>
<p>We leverage data from the APOGEE DR17 value-added catalog of GCs [147], and as a baseline, we first follow the quality control filtering steps from [120] (details in the Interactive Supplement (IS)1 Section 2).This results in a processed dataset with 3,286 stars and 19 chemical abundance features.With these 3,286 observations, we perform a randomized 80/20 train/test split to obtain a training set of 2,628 observations and a test set of 658 observations.Alternative data preprocessing pipelines will also be considered and discussed in subsequent sections.</p>
<p>We then conduct a brief exploratory data analysis, examining the spatial distribution of GCs, the distribution of abundances, and the pairwise relationships between different elemental abundances.We further explore various dimension reduction visualizations of the abundance data using PCA [80], tSNE [172], and UMAP [108].For tSNE and UMAP, we examine different hyperparameter choices of the number of nearest neighbors (known as perplexity in tSNE) used to construct the manifolds.Intuitively, this hyperparameter controls the balance between preserving local versus global structure in the learnt representations.To tune this hyperparameter and select the best dimension reduction method, we use the neighborhood retention metric, which measures the proportion of the k nearest neighbors maintained from the original high-dimensional space to the low-dimensional embedding [183,102].We assess this metric across a range of neighborhood sizes k.A "good" dimension reduction method and hyperparameter exhibits high neighbor retention across a wide range of neighborhood sizes.</p>
<p>Modeling and Validation</p>
<p>To next identify the optimal clustering of stars, we introduce our clustering and validation workflow.Given the many reasonable data preprocessing and modeling choices, our approach is heavily driven by the philosophy that our scientific conclusions should be stable and generalizable regardless of which specific analytical choices are made.This is similar in spirit to the core principles of the PCS framework for veridical data science [187], which emphasize that scientific conclusions should, at a minimum, be stable across arbitrary human judgment calls made throughout the data science life cycle.Guided by this philosophy, we summarize our approach as follows:</p>
<p>(a) Stability-driven model building.Given training data X, a set of clustering methods of interest M, a set of reasonable data preprocessing pipelines G, and a range of number of clusters K, we first select the clustering model m * ∈ M and the number of clusters k * ∈ K that yield the most stable (i.e., similar) clustering results, when fitted using different subsamples of X and different preprocessing pipelines g(X) for all g ∈ G. Here, we measure the overall stability (or similarity) between two sets of clusters using the Adjusted Rand Index (ARI) [129].Pseudo-code for this model selection step is provided in Algorithm 1.</p>
<p>This model selection step is similar to the Model Explorer algorithm from [16], but while the Model Explorer only assesses cluster stability across random subsamples of the data, we extend this to include the stability across different preprocessing pipelines.This is crucial, as scientifically-relevant clusters should not depend on arbitrary data preprocessing choices.</p>
<p>To then obtain the final cluster labels for the training data, we use consensus clustering [114] to aggregate the clusters learned using the most stable model m * and number of clusters k * , applied to each preprocessed version of the data g(X) for each g ∈ G. Here, we report the coclustering membership matrix, which captures the frequency of times each pair of stars appeared in the same cluster across the different clustering runs using different data preprocessing pipelines G. From this co-clustering matrix, we further compute the local stability of each star's cluster membership, defined as the average co-clustering frequency across all other stars in the same cluster.A higher local stability indicates that this star is stably clustered with its neighboring stars regardless of the data preprocessing choices.</p>
<p>(b) Generalizability and stability validation.</p>
<p>To validate the consensus training clusters, we assess (i) the overall stability had alternative preprocessing choices (not considered in G due to computational feasibility) been performed, and (ii) the cluster generalizability using a held-out test set.More specifically, to measure (i), we compute the ARI between the previously learned clusters and the clusters that would have been obtained had alternative preprocessing choices been made.To measure (ii), we follow the idea of cluster predictability [166,96]: we apply the tuned clustering pipeline to the training and test sets separately, train a random forest (RF) to predict the cluster labels on the training set, and use this RF to predict the cluster labels on the test set.The overall cluster generalizability is then computed as the ARI between the RFpredicted cluster labels and the test cluster labels.However, since some clusters might be more generalizable than other clusters, we further investigate a measure of the local, or cluster-wise, generalizability by computing the precision between the RF-predicted cluster labels and the test cluster labels per cluster.</p>
<p>If the clusters validate, we finally re-fit the tuned clustering pipeline on the full training and test data to obtain the final clusters and interpret them scientifically.</p>
<p>Interpretation and Communication</p>
<p>We report both statistical and scientific findings from the final learned clusters.This includes stability and generalizability metrics as well as scientific insights stemming from the chemical abundance signatures of each identified cluster.In particular, through these elemental signatures, we compare our discovered clusters to existing observations in Randomly sample a subset of observations: sub 1 = subsample(X, π).</p>
<p>5:</p>
<p>Randomly sample a subset of observations: sub 2 = subsample(X, π).</p>
<p>6:</p>
<p>Randomly choose two data preprocessing pipelines: g 1 , g 2 ∈ G.</p>
<p>7:</p>
<p>Apply clustering method m on g 1 (X[sub 1 ]) to obtain k clusters: C 1 .</p>
<p>8:</p>
<p>Apply clustering method m on g 2 (X[sub 2 ]) to obtain k clusters: C 2 .</p>
<p>9:</p>
<p>Compute stability score via Adjusted Rand Index (ARI) on overlapping samples: the literature and assess whether our discovered clusters unearth new groupings of globular clusters as compared to the current understanding of star formation.
S(k, m, b) = ARI(C 1 [sub 1 ∩ sub 2 ], C 2 [sub 1 ∩ sub 2 ]).</p>
<p>Results</p>
<p>Exploration Following our plan, we first carry out an exploratory data analysis on the training APOGEE DR17 data (preprocessed using the baseline quality control filters, 11 elemental abundance features, and mean imputation), seen in Figure 2. In particular, since F E H is a commonly agreed upon marker of star age [5,147], we hone in on pairiwse relationships between F E H and four other key abundances (AL F E , M G F E , N F E , and O F E ) in Figure 2A.We observe that the majority of the separation between GCs seems to be occurring in the F E H axis as compared to the other elements.In Figure 2B, we also examine the GCs in galactic coordinate space.As expected, stars share GCs with their spatial neighbors.</p>
<p>We next explore various dimension reduction techniques.To identify appropriate hyperparameters for these methods, we evaluate the neighbor retention metric for various dimension reduction methods and hyperparameters in Figure 2C.Interestingly, tSNE almost always uniformly demonstrates higher retention than UMAP.For this reason, we drop UMAP from consideration in all subsequent analyses.We also note that tSNE with perplexity=30 and perplexity=100 exhibit high retention across both small (i.e., local) and large (i.e., global) neighborhood sizes and are thus deemed appropriate for subsequent steps (e.g., for visualization and as input to clustering methods), alongside PCA which expectedly has the highest retention globally.Having selected and tuned these dimension reduction techniques, in Figure 2D, we visualize the GC labels on the tSNE (perplex-ity=100) embedding, and in 2E, we overlay several key chemical abundances (F E H , O F E , N F E ).</p>
<p>We can see that the right arm of the tSNE embedding is primarily iron-rich stars, a marker of younger stars [33].We provide further exploratory plots and dimension reduction visualizations in IS.3-4.</p>
<p>Modeling and Validation</p>
<p>Using our training data, we next search over different clustering methods to find the most stable clusters of stars using Algorithm 1 and summarize the stability results in Figure 3A.We shortlist two promising methods, spectral clustering (n_neighbors = 60) with k = 2 clusters and K-means clustering with k = 8 clusters, which demonstrate high stability across both subsamples of the data and different preprocessing pipelines.</p>
<p>First, exploring the spectral clustering (n_neighbors = 60) with k = 2 clusters, we visualize the consensus clustering matrix in Figure 3B alongside the tSNE (perplexity=100) embeddings, overlayed with the cluster membership labels and the local stability of each star's cluster membership in Figure 3C-D, respectively.We observe that the two clusters are extremely stable and exhibit high local stability values across almost all stars.However, despite yielding the highest stability in Figure 3A, these two clusters largely recapitulate well-established iron-rich versus iron-poor splits (Figure 3E), characterizing younger versus older stars [33,35,34].</p>
<p>Seeking new insights beyond these known divisions, we instead shift our interest towards the eight clusters generated by K-means (Figure 3F-H) with the hopes of uncovering novel and stable groupings of GCs.Notably, several clusters -1, 2, and 8 -show strong co-clustering membership, but others -3, 7 -show high variability in co-clustering membership across pipelines.These clusterspecific insights perhaps suggest that while not all clusters in the K-means (k = 8 clusters) model should be trusted or interpreted, there are several clusters that are highly stable and may give rise to more reliable scientific conclusions.</p>
<p>Interpretation and Communication</p>
<p>Proceeding with the selected K-means clustering model with k = 8 clusters, we finally refit the consensus clustering pipeline on the full datasets with no sample splitting.In Figure 4A, we highlight these final learned clusters on the tSNE (perplex-ity=100) visualization.In 4B, we evaluate the robustness of this final clustering determination with respect to changing astronomical parameters in the quality control filtering step.Instead of the original thresholds chosen for S/N (70) and log of surface gravity (3.6), we sweep over alternative choices for these thresholds and confirm that the final clusters are stable (or similar) regardless of these arbitrary filtering thresholds (see IS.6 for additional stability plots).In Figure 4C, we assess the local generalizability across the clusters in our held-out test set, averaged across imputation methods and datasets (see IS.6 for more detail).In 4D, we can directly compare this with the stability of individual clusters.We observe that cluster 2 has the highest generalizability and the highest stability, indicating that it is both stable to data perturbations and generalizes extremely well to new data.One possible reason for this strong performance of cluster 2, in addition to cluster 1, is they are heavily composed of large GCs, which are entirely self-contained within the cluster (Figure 4E).Specifically, NGC6121 makes up 55% of cluster 1, and 98.8% of the GC is contained within it.Similarly, NGC0104 makes up 70% of cluster 2 and is wholly contained within it.</p>
<p>Due to the strong generalizability and stability performance of clusters 1, 2, 6, and 8 in 4C and D, we determine these four groupings as the most robust, and hence suitable for scientific interpretation and communication to collaborators.In Figure 4F, we examine the distributions of seven key chemical abundances over these groups.Though we briefly describe the clusters and their relevance here, we refer readers to the rich body of astronomical literature on interpreting clusters of GCs, such as but not limited to [148,130,29,54,14].The unique chemical abundance of groups of stars traces the formation and assembly of these stars from possibly common progenitors.</p>
<p>Cluster 2 is characterized by heavy iron-richness but low aluminum, indicative of a cluster of ironrich metal-poor (IRMP) stars, potentially tied to specific supernovae as formation systems [132].Cluster 6, with low abundances of carbon, magnesium, oxygen, and silicon, is more challenging to identify.It is possibly another cluster of metal-poor stars [54], however unlike cluster 8, it is not iron-poor.Clusters 1 and 8 are high in nitrogen, and could be considered a set of nitrogen-rich stars (NRS), particularly cluster 8 which is also high in key marker aluminum and with significantly low abundance in F E H (see [148]).These older, metal-poor stars have been shown to be particularly important in the early formation of the inner galaxy [56,55,57], and cluster 8 in particular exhibits a known anti-correlation between high aluminum abundance and low magnesium abundance [11].While promising, we emphasize further research is needed to scientifically validate these clusters, ideally with a collaborating astronomer.Further research into clustering astronomical survey data may seek to integrate other commonly used value-added catalogs from APOGEE such as annotated age labels [145], other spectroscopic sky surveys such as GALAH [42,26] and Gaia [67,40], or commonly used simulation studies for the evolution of the Milky Way [104,39].</p>
<p>The robustness of our results and pipeline underscores the importance and benefit of a strong workflow for scientific discovery in unsupervised learning.Though the chemical abundance space of stars in the Milky Way has been established as a continuous spectrum of age and metallicity [130,22,23], our workflow has produced reliable and reproducible groupings of stellar abundances, anchoring data-driven insights into chemical formations of our galaxy.The need to cluster along a continuum is by no means unique to stellar astrophysics.Across disciplines, from cell cycle dynamics in cancer [192] to agricultural soils [78], researchers frequently wish to partition and typify data along continuous spectra.Through this case study, we have demonstrated the value of a workflow with rigorous assessments of both local and global stability and generalizability to ensure reproducible results.</p>
<p>Discussion &amp; Open Research</p>
<p>In this paper, we have provided a general, model-agnostic workflow for generating unsupervised data-driven discoveries as well as practical recommendations on best practices when designing and executing data analysis pipelines using unsupervised learning techniques.However, there are also several specific topics and questions that we have been unable to definitively address, as they are still areas of open research and thus do not have a well-established solution.The most outstanding of these issues is in the area of model-agnostic validation and uncertainty quantification techniques.While general approaches have been established for evaluating the stability and generalizability of clustering estimates [114,138,166], model-agnostic methods for validation in other unsupervised learning tasks are currently open unsolved problems.Similarly, the concept of quantitative measures for comparing uncertainty levels between different models applied for the same unsupervised learning task is a difficult problem to address, as the notion of prediction errors from supervised learning analyses does not exist, and thus little work has been done in this area.Another area of open research involves methods for synthesizing results found from multiple analytical approaches, hyperparameter settings, or modeling decisions, analogous to ideas from ensemble learning in the supervised learning paradigm.The problem of feature importance is also presently an unsolved problem, as most methods in unsupervised learning do not provide parameter estimates which can be mapped to a measure of feature importance or a target to assess significance.Though we currently do not have clear recommendations for these topics, developments of methods for these concepts in the realm of unsupervised learning are a burgeoning area of interest in statistics, machine learning, and AI; thus these present unaddressed topics may have more definitive solutions in the near future.</p>
<p>In summary, unsupervised learning methods have been important for driving research and creating new data-driven discoveries in the sciences and beyond, but guidance on how to create reliable workflows for unsupervised discovery is currently limited.Through the establishment of a general workflow for unsupervised learning, we aim to provide data scientists and interdisciplinary researchers with a framework for developing unsupervised analysis pipelines in new fields, as well as to promote reproducible and reliable data-driven discoveries.Development of generalized, modal-agnostic approaches for validation, uncertainty quantification, and post-hoc analysis of unsupervised learning methods will be an important area of future research for ensuring further reliability of unsupervised discoveries.As expected, GCs are spatially grouped.C: GC neighborhood retention for different dimension reduction (DR) methods and hyperparameters across different neighborhood sizes.Across PCA, five hyperparameter settings of tSNE, and five hyperparameter settings of UMAP, we computed how many of the k nearest neighbors are maintained from the original chemical abundance space to the 2-dimensional embedding.tSNE always maintains a higher neighborhood retention rate than UMAP across all hyperparameters and sizes of neighborhoods.To balance local information, we select tSNE with perplexity=100 for further visualizations.D: tSNE with perplexity=100 embedding of GC labels.E: Key chemical abundance distributions along the tSNE (perplexity=100) embedding.From these, we can observe iron-rich, younger GCs in the leftmost plot, oxygen-rich GCs in the middle plot, and nitrogen-rich GCs on the right.1), we compute the mean stability for each clustering method.Spectral clustering with 60 nearest neighbors at k = 2 has the highest overall mean stability, followed by K-Means with k = 8.Hence, we explore these two clustering results on the training set.</p>
<p>Figure 3: (Previous page.)B: Consensus clustering matrix across imputation methods and datasets for the two cluster spectral solution (n_neighbors=60, k = 2).Each entry (i, j) is the fraction of times stars i and j are in the same cluster across every run of the full clustering pipeline: imputation methods, feature sets, and DR methods (see Table 1).C: tSNE (with perplexity=100) embeddings of the medium mean-imputed dataset, overlaid with consensus cluster labels.D: Local stability of the spectral clustering result overlaid on the tSNE embedding.Local stability for each star is computed as its average co-membership rate with its own cluster across the clustering pipeline.E: Chemical abundances of the two spectral clusters, sorted by magnitude of difference.Though other elements are used to group these stars, these clusters are predominantly being separated along the F E H axis, an already known distinguishing factor of young and old stellar bodies.This iron-rich population on the right arm of the tSNE embedding (panel C) is highly stable (panels B, D).Though exhibiting the highest stability, we instead choose to explore the K-means result with k = 8 in the aims of uncovering more granular groupings of GCs rather than recapitulating already known distinctions between iron-rich younger stars and older stars.F: Consensus matrix for the eight clusters from K-means.G: tSNE embedding of the medium mean-imputed dataset overlaid with the eight clusters from K-means.H: Local stability of each of the eight K-means clusters.As observed in the consensus matrix (panel F), there are more unstable clusters as compared to the spectral clustering results (panels B-E).For example, clusters 3 and 7 exhibit low stability, first visualized in the light entries of the consensus matrix within their respective blocks, then overlaid in panel H.The iron-rich cluster 8 is the most stable.) across the entire pipeline, with cluster 3 having the lowest generalizability.D: Distribution of local stability scores for each cluster across imputation methods and datasets.Cluster 2 again has the highest local stability, whereas cluster 5 has the lowest.From C and D, we conclude that clusters 1, 2, 6 and 8 are the most generalizable and stable clusters.E: Globular cluster (GC) composition of each of the eight clusters.NGC6121 is nearly entirely contained within cluster 1 (98.8%), and NGC0104 is entirely contained within cluster 2, a potential source of the strong stability rates for these clusters.F: From panels C, D, we conclude clusters 1, 2, 6, and 8 are the most stable and generalizable, and hence suitable for scientific interpretation and communication.Here, we interpret seven key chemical abundance distributions across these four clusters.Cluster 1 is iron-and silicon-high, whereas cluster 2, seen in the separated right branch in the tSNE embedding (A), is an iron-rich cluster likely made up of younger stellar bodies.Cluster 6 is carbon-and silicon-low, and cluster 8 is driven by aluminum enrichment and low levels of iron.</p>
<p>Algorithm 1 2 : 3 :
123
Clustering Model Selection via Stability Require: Training data X, set of preprocessing pipelines G, set of clustering methods M, set of cluster numbers K, resampling iterations B, and subsampling proportion π. 1: for k ∈ K do for m ∈ M do for b = 1, . . ., B do 4:</p>
<p>13 :
13
Choose clustering method m * ∈ M and number of clusters k * ∈ K with highest stability: m * , k * = argmax</p>
<p>Figure 2 :
2
Figure 2: Exploratory Data Analysis of APOGEE DR17 Globular Clusters (GCs).A: Distributions of key chemical abundances across F E H . B: GCs visualized in galactic coordinate space.As expected, GCs are spatially grouped.C: GC neighborhood retention for different dimension reduction (DR) methods and hyperparameters across different neighborhood sizes.Across PCA, five hyperparameter settings of tSNE, and five hyperparameter settings of UMAP, we computed how many of the k nearest neighbors are maintained from the original chemical abundance space to the 2-dimensional embedding.tSNE always maintains a higher neighborhood retention rate than UMAP across all hyperparameters and sizes of neighborhoods.To balance local information, we select tSNE with perplexity=100 for further visualizations.D: tSNE with perplexity=100 embedding of GC labels.E: Key chemical abundance distributions along the tSNE (perplexity=100) embedding.From these, we can observe iron-rich, younger GCs in the leftmost plot, oxygen-rich GCs in the middle plot, and nitrogen-rich GCs on the right.</p>
<p>Figure 3 :
3
Figure 3: Stability-driven clustering model selection.A: Stability assessment of clustering methods on a training set of APOGEE globular clusters (GCs) using Algorithm 1. Aggregated over imputation methods and datasets (see Table1), we compute the mean stability for each clustering method.Spectral clustering with 60 nearest neighbors at k = 2 has the highest overall mean stability, followed by K-Means with k = 8.Hence, we explore these two clustering results on the training set.20</p>
<p>Figure 4 :
4
Figure 4: Final clustering of stars from APOGEE survey, refit on the entire dataset.A: The eight K-means clusters fit on the entire medium, mean-imputed dataset.B: Sensitivity of final clustering results to different astronomical preprocessing choices.When varying the S/N cutoff (top) or threshold for the log of surface gravity (bottom) of stars, we repeat the clustering pipeline and evaluate ARI stability between the resulting clusters and our original eight K-means clusters from panel A. Changing these parameters has little to no effect on the final stellar groupings, demonstrating the robustness of our workflow and clustering pipeline to arbitrary filtering choices.We test further parameters in IS.6 and observe similar results.</p>
<p>Figure 4 :
4
Figure 4: (Previous page.)C: Local generalizability of each of the eight K-means clusters, defined as the precision per cluster when training a classifier to predict clustering labels.Cluster 2 has local generalizability = 1.00 (see IS.6) across the entire pipeline, with cluster 3 having the lowest generalizability.D: Distribution of local stability scores for each cluster across imputation methods and datasets.Cluster 2 again has the highest local stability, whereas cluster 5 has the lowest.From C and D, we conclude that clusters 1, 2, 6 and 8 are the most generalizable and stable clusters.E: Globular cluster (GC) composition of each of the eight clusters.NGC6121 is nearly entirely contained within cluster 1 (98.8%), and NGC0104 is entirely contained within cluster 2, a potential source of the strong stability rates for these clusters.F: From panels C, D, we conclude clusters 1, 2, 6, and 8 are the most stable and generalizable, and hence suitable for scientific interpretation and communication.Here, we interpret seven key chemical abundance distributions across these four clusters.Cluster 1 is iron-and silicon-high, whereas cluster 2, seen in the separated right branch in the tSNE embedding (A), is an iron-rich cluster likely made up of younger stellar bodies.Cluster 6 is carbon-and silicon-low, and cluster 8 is driven by aluminum enrichment and low levels of iron.</p>
<p>H , M G F E , O F E , SI F E , CA F E , N I F E , AL F E , (ii) 11 features: (i) plus C F E , M N F E , N F E , K F E , or (iii) 19 features: (ii) plus CI F E , N A F E , S F E , T I F E , T III F E , V F E , CR F E , CO F E
Data Preprocessing/ MethodParameter ChoicesData PreprocessingQuality Control Filter Abundance Features (i) 7 features: F E Data Imputation S/N Threshold = 30, 50, 70  *  , 90, 110, 130, 150 T ef f Width = 500, 1000  *  , 1500, 2000 Log Gravity Threshold = 3.0, 3.3, 3.6  *  , 3.9, 4.2 VB Threshold = 0.5, 0.7, 0.9  *  , 0.99 STARFLAG = 0 Mean imputation or random forest-based imputationNo Dimension Reduction NADimensionReductionPCA tSNE UMAPNA Number of dimensions = 2 Perplexity = 10, 30, 60, 100, 300 Number of dimensions = 2Number of neighbors = 10, 30, 60, 100, 300Clustering(k = 2, . . . , 30)K-means Hierarchical Clustering Spectral ClusteringInitialization = K-means++ Distance = Euclidean Linkage = complete or Ward's Affinity = nearest neighbors graph Number of neighbors = 5, 30, 60, 100</p>
<p>Table 1 :
1
Data preprocessing and modeling choices used in the APOGEE case study.Quality control parameters with astericks were used as the default/baseline quality control parameters.</p>
<p>https://dataslingers.github.io/unsupervised-workflow-astro/
Data Availability For all code, documentation, justification of preprocessing and modeling choices, and more in-depth validation analyses and results, please see our Interactive Supplement at https://dataslingers.github.io/unsupervised-workflow-astro/and GitHub repository https://github.com/DataSlingers/unsupervised-workflow-astro.Our processed APOGEE globular cluster data with train-test splits and all results files are also made available at https: //zenodo.org/records/15565719.
The impact of preprocessing steps on the accuracy of machine learning algorithms in sentiment analysis. S Alam, N Yao, Computational and Mathematical Organization Theory. 252019</p>
<p>Interpretable machine learning for discovery: Statistical challenges and opportunities. G Allen, L Gan, L Zheng, Annual Review of Statistics and Its Application. 112023</p>
<p>C J Alpert, A B Kahng, S.-Z Yao, Spectral partitioning with multiple eigenvectors. 199990</p>
<p>Dissecting stellar chemical abundance space with t-sne. F Anders, C Chiappini, B X Santiago, G Matijevič, A B Queiroz, M Steinmetz, G Guiglion, Astronomy &amp; Astrophysics. 619A1252018</p>
<p>Spectroscopic age estimates for apogee redgiant stars: Precise spatial and kinematic trends with age in the galactic disc. F Anders, P Gispert, B Ratcliffe, C Chiappini, I Minchev, S Nepal, A B D A Queiroz, J A Amarante, T Antoja, G Casali, Astronomy &amp; Astrophysics. 678A1582023</p>
<p>A supervised machine learning workflow for the reduction of highly dimensional biological data. L K Andersen, B J Reading, Artificial Intelligence in the Life Sciences. 51000902024</p>
<p>Applications and comparison of dimensionality reduction methods for microbiome data. G Armstrong, G Rahman, C Martino, D Mcdonald, A Gonzalez, G Mishne, R Knight, Frontiers in Bioinformatics. 28218612022</p>
<p>The role of hyperparameters in machine learning models and how to tune them. C Arnold, L Biedebach, A Küpfer, M Neunhoeffer, Political Science Research and Methods. 1242024</p>
<p>k-means++: The advantages of careful seeding. D Arthur, S Vassilvitskii, 2006StanfordTechnical report</p>
<p>From discovery to adoption: Understanding the ml practitioners' interpretability journey. N Ashtari, R Mullins, C Qian, J Wexler, I Tenney, M Pushkarna, Proceedings of the 2023 ACM Designing Interactive Systems Conference. the 2023 ACM Designing Interactive Systems ConferenceJuly 2023</p>
<p>Apogee-2s mg-al anti-correlation of the metal-poor globular cluster ngc 2298. I Baeza, J G Fernández-Trincado, S Villanova, D Geisler, D Minniti, E R Garro, B Barbuy, T C Beers, R R Lane, Astronomy &amp; Astrophysics. 662A472022</p>
<p>The weirdest sdss galaxies: results from an outlier detection algorithm. Monthly Notices of the Royal Astronomy Society. D Baron, D Poznanski, arXiv:1611.07526Nov. 2016astro-ph</p>
<p>Unsupervised learning procedures for neural networks. S Becker, International Journal of Neural Systems. 201n021991</p>
<p>Nitrogen enrichment and clustered star formation at the dawn of the galaxy. V Belokurov, A Kravtsov, Monthly Notices of the Royal Astronomical Society. 52532023</p>
<p>Statistical tools and approaches to validate analytical methods: methodology and practical examples. S Belouafa, F Habti, S Benhar, B Belafkih, S Tayane, S Hamdouch, A Bennamara, A Abourriche, International Journal of Metrology and Quality Engineering. 892017</p>
<p>A stability based method for discovering structure in clustered data. A Ben-Hur, A Elisseeff, I Guyon, 2002. 2001World Scientific</p>
<p>Collaboration and team science: from theory to practice. L M Bennett, H Gadlin, 2012</p>
<p>Searching for chemo-kinematic structures in the milky way halo with deep clustering algorithms. L Berni, arXiv:2409.114292024arXiv preprint</p>
<p>Deriving star cluster parameters with convolutional neural networks-ii. extinction and cluster-background classification. J Bialopetravičius, D Narbutis, Astronomy &amp; Astrophysics. 633A1482020</p>
<p>The art and practice of data science pipelines: A comprehensive study of data science pipelines in theory, in-the-small, and in-the-large. S Biswas, M Wardat, H Rajan, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software EngineeringMay 2022</p>
<p>Stability and generalization. O Bousquet, A Elisseeff, Journal of Machine Learning Research. 2Mar. 2002</p>
<p>The chemical homogeneity of open clusters. J Bovy, The Astrophysical Journal. 8171492016</p>
<p>The milky way has no distinct thick disk. J Bovy, H.-W Rix, D W Hogg, The Astrophysical Journal. 75121312012</p>
<p>-omics biomarker identification pipeline for translational medicine. L Bravo-Merodio, J A Williams, G V Gkoutos, A Acharjee, Journal of translational medicine. 172019</p>
<p>The pitfalls of visual representations: A review and classification of common errors made while designing and interpreting visualizations. S Bresciani, M Eppler, Sage Open. 5421582440156114512015</p>
<p>S Buder, J Kos, E Wang, M Mckenzie, M Howell, S Martell, M Hayden, D Zucker, T Nordlander, B Montet, arXiv:2409.19858The galah survey: Data release 4. 2024arXiv preprint</p>
<p>Brain graphs: Graphical models of the human brain connectome. E Bullmore, D Bassett, Annual Review of Clinical Psychology. 712011</p>
<p>Multiple Imputation and Its Application. J Carpenter, J Bartlett, T Morris, A Wood, M Quartagno, M Kenward, 2023John Wiley &amp; Sons</p>
<p>The (im) possibility of strong chemical tagging. L Casamiquela, A Castro-Ginard, F Anders, C Soubiran, Astronomy &amp; Astrophysics. 654A1512021</p>
<p>Parameter estimation for open clusters using an artificial neural network with a quadtree-based feature extractor. L Cavallo, L Spina, G Carraro, L Magrini, E Poggio, T Cantat-Gaudin, M Pasquato, S Lucatello, S Ortolani, J Schiappacasse-Ulloa, The Astronomical Journal. 1671122023</p>
<p>Anomaly detection: A survey. V Chandola, A Banerjee, V Kumar, ACM Computing Surveys (CSUR). 4132009</p>
<p>Chemodynamical clustering applied to apogee data: Rediscovering globular clusters. B Chen, E D'onghia, S A Pardy, A Pasquali, C B Motta, B Hanlon, E K Grebel, The Astrophysical Journal. 8601702018</p>
<p>Young and rich stars. M Chiao, Nature Physics. 1152015</p>
<p>A gauge of stellar age. M Chiao, Nature Astronomy. 382019</p>
<p>C Chiappini, F Anders, T D S Rodrigues, A Miglio, J Montalbán, B Mosser, L Girardi, M Valentini, A Noels, T Morel, What is their origin? Astronomy &amp; Astrophysics. 2015576L12α/fe</p>
<p>Predicting drug polypharmacology from cell morphology readouts using variational autoencoder latent space arithmetic. Y L Chow, S Singh, A E Carpenter, G P Way, PLoS computational biology. 182e10098882022</p>
<p>Workflow for evaluating normalization tools for omics data using supervised and unsupervised machine learning. A E Chua, L D Pfeifer, E R Sekera, A B Hummon, H Desaire, Journal of the American Society for Mass Spectrometry. 34122023</p>
<p>Communicating Clearly about Science and Medicine: Making Data Presentations as Simple as Possible. J Clare, But No Simpler. New YorkRoutledge2017</p>
<p>The imprint of clump formation at high redshift-i. a disc α-abundance dichotomy. A J Clarke, V P Debattista, D L Nidever, S R Loebman, R C Simons, S Kassin, M Du, M Ness, D B Fisher, T R Quinn, Monthly Notices of the Royal Astronomical Society. 48432019</p>
<p>Gaia data release 3: Summary of the content and survey properties. G Collaboration, Astronomy &amp; Astrophysics. 674A12023</p>
<p>The impact of preprocessing on data mining: An evaluation of classifier sensitivity in direct marketing. S Crone, S Lessmann, R Stahlbock, European Journal of Operational Research. 17332006</p>
<p>The galah survey: scientific motivation. G De Silva, K Freeman, J Bland-Hawthorn, S Martell, E W De Boer, M Asplund, S Keller, S Sharma, D Zucker, T Zwitter, Monthly Notices of the Royal Astronomical Society. 44932015</p>
<p>The imprint of clump formation at high redshift. ii. the chemistry of the bulge. V P Debattista, D J Liddicott, O A Gonzalez, L B Silva, J A Amarante, I Lazar, M Zoccali, E Valenti, D B Fisher, T Khachaturyants, The Astrophysical Journal. 94621182023</p>
<p>Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it. M Denny, A Spirling, Political Analysis. 2622018</p>
<p>Ensemble methods in machine learning. T G Dietterich, International Workshop on Multiple Classifier Systems. Berlin, HeidelbergSpringer Berlin HeidelbergJune 2000</p>
<p>Unsupervised learning based on artificial neural network: A review. H Dike, Y Zhou, K Deveerasetty, Q Wu, 2018 IEEE International Conference on Cyborg and Bionic Systems (CBS). IEEEOctober 2018</p>
<p>Clustering approaches to identifying gene expression patterns from dna microarray data. J H Do, D K Choi, Molecules and Cells. 2522008</p>
<p>The open cluster chemical abundances and mapping survey. iv. abundances for 128 open clusters using sdss/apogee dr16. J Donor, P M Frinchaboy, K Cunha, J E O'connell, C A Prieto, A Almeida, F Anders, R Beaton, D Bizyaev, J R Brownstein, R Carrera, C Chiappini, R Cohen, D A García-Hernández, D Geisler, S Hasselquist, H Jönsson, R R Lane, S R Majewski, D Minniti, C M Bidin, K Pan, A Roman-Lopes, J S Sobeck, G Zasowski, The Astronomical Journal. 1595199Apr. 2020</p>
<p>Multiple testing and error control in gaussian graphical model selection. M Drton, M D Perlman, Statistical Science. 2232007</p>
<p>Causal discovery for climate research using graphical models. I Ebert-Uphoff, Y Deng, Journal of Climate. 25172012</p>
<p>Machine learning and modeling: Data, validation, communication challenges. I El Naqa, D Ruan, G Valdes, A Dekker, T Mcnutt, Y Ge, Q Wu, J Oh, M Thor, W Smith, A Rao, Medical Physics. 45102018</p>
<p>Effective data visualization: The right chart for the right data. S Evergreen, 2019SAGE Publications</p>
<p>Application of graph theory for identifying connectivity patterns in human brain networks: A systematic review. F Farahani, W Karwowski, N Lighthall, Frontiers in Neuroscience. 135852019</p>
<p>Aluminium-enriched metal-poor stars buried in the inner galaxy. J G Fernández-Trincado, T C Beers, D Minniti, B Tang, S Villanova, D Geisler, A Pérez-Villegas, K Vieira, Astronomy &amp; Astrophysics. 643L42020</p>
<p>Discovery of a new stellar subpopulation residing in the (inner) stellar halo of the milky way. J G Fernández-Trincado, T C Beers, V M Placco, E Moreno, A Alves-Brito, D Minniti, B Tang, A Pérez-Villegas, C Reylé, A C Robin, The Astrophysical Journal Letters. 8861L82019</p>
<p>Chemodynamics of newly identified giants with a globular cluster like abundance patterns in the bulge, disc, and halo of the milky way. J G Fernandez-Trincado, T C Beers, B Tang, E Moreno, A Pérez-Villegas, M Ortigoza-Urdaneta, Monthly Notices of the Royal Astronomical Society. 48822019</p>
<p>Dynamical orbital classification of selected n-rich stars with gaia data release 2 astrometry. J G Fernández-Trincado, L Chaves-Velasquez, A Pérez-Villegas, K Vieira, E Moreno, M Ortigoza-Urdaneta, L Vega-Neme, Monthly Notices of the Royal Astronomical Society. 49542020</p>
<p>Highlights of the us national academies report on "reproducibility and replicability in science. H Fineberg, V Stodden, X Meng, Harvard Data Science Review. 242020</p>
<p>Graph analysis of the human connectome: Promise, progress, and pitfalls. A Fornito, A Zalesky, M Breakspear, NeuroImage. 802013</p>
<p>Unsupervised classification of sdss galaxy spectra | astronomy &amp; astrophysics (a&amp;a). D Fraix-Burnet, C Bouveyron, J Moultaka, astronomy &amp; Astrophysics. 649A532021</p>
<p>Doing a research literature review. H Frank, I Hatak, How to Get Published in the Best Entrepreneurship Journals. Elgar2014</p>
<p>Nonlinear dimensionality reduction in climate data. A J Gamez, C S Zhou, A Timmermann, J Kurths, Nonlinear Processes in Geophysics. 1132004</p>
<p>Are machine learning interpretations reliable? a stability study on global interpretations. L Gan, T M Zikry, G I Allen, arXiv:2505.157282025arXiv preprint</p>
<p>Machine learning in apogee-unsupervised spectral classification with k-means. R Garcia-Dias, C A Prieto, J S Almeida, I Ordovás-Pascual, Astronomy &amp; Astrophysics. 612A982018</p>
<p>The topology of large open connectome networks for the human brain. M Gastner, G Ódor, Scientific Reports. 61272492016</p>
<p>Elements of Dimensionality Reduction and Manifold Learning. B Ghojogh, M Crowley, F Karray, A Ghodsi, 2023Springer</p>
<p>The gaia-eso public spectroscopic survey. G Gilmore, S Randich, M Asplund, J Binney, P Bonifacio, J Drew, S Feltzing, A Ferguson, R Jeffries, G Micela, The Messenger. 1472012</p>
<p>Visual comparison for information visualization. M Gleicher, D Albers, R Walker, I Jusufi, C Hansen, J Roberts, Information Visualization. 1042011</p>
<p>Sampling, representativeness and generalizability. G Gobo, Qualitative Research Practice. 2004</p>
<p>Consensus clustering algorithms: Comparison and refinement. A Goder, V Filkov, Proceedings of the Tenth Workshop on Algorithm Engineering and Experiments (ALENEX). the Tenth Workshop on Algorithm Engineering and Experiments (ALENEX)Society for Industrial and Applied MathematicsJan. 2008</p>
<p>Abundance variations within globular clusters. R Gratton, C Sneden, E Carretta, Annu. Rev. Astron. Astrophys. 4212004</p>
<p>Unsupervised learning and clustering. D Greene, P Cunningham, R Mayer, Machine Learning Techniques for Multimedia: Case Studies on Organization and Retrieval. Berlin HeidelbergSpringer2008</p>
<p>Computational cluster validation in post-genomic data analysis. J Handl, J Knowles, D Kell, Bioinformatics. 21152005</p>
<p>Undirected Graphical Models. T Hastie, R Tibshirani, J Friedman, 2009Springer</p>
<p>Using chemical tagging to redefine the interface of the galactic disc and halo. K Hawkins, P Jofré, T Masseron, G Gilmore, Bibcode: 2015MNRAS.453..758HMonthly Notices of the Royal Astronomical Society. 453Oct. 2015</p>
<p>Chemical cartography with apogee: Metallicity distribution functions and the chemical structure of the milky way disk. M R Hayden, J Bovy, J A Holtzman, D L Nidever, J C Bird, D H Weinberg, B H Andrews, S R Majewski, C Prieto, F Anders, T C Beers, D Bizyaev, C Chiappini, K Cunha, P Frinchaboy, D A García-Herńandez, A E García Pérez, L Girardi, P Harding, F R Hearty, J A Johnson, S Mészáros, I Minchev, R O'connell, K Pan, A C Robin, R P Schiavon, D P Schneider, M Schultheis, M Shetrone, M Skrutskie, M Steinmetz, V Smith, J C Wilson, O Zamora, G Zasowski, The Astrophysical Journal. 808132Aug. 2015ADS Bibcode: 2015ApJ...808..132H.</p>
<p>Data Visualization: A Practical Introduction. K Healy, 2024Princeton University Press</p>
<p>Advantages of fuzzy k-means over kmeans clustering in the classification of diffuse reflectance soil spectra: A case study with west african soils. J Heil, V Häring, B Marschner, B Stumpe, Geoderma. 3372019</p>
<p>M Hong, S Tao, L Zhang, L Diao, X Huang, S Huang, S Xie, Z Xiao, H Zhang, Rna sequencing: New technologies and applications in cancer research. 202013</p>
<p>Analysis of a complex of statistical variables into principal components. H Hotelling, Journal of educational psychology. 2464171933</p>
<p>Analysis of air quality time series of hong kong with graphical modeling. F Hu, Z Lu, H Wong, T P Yuen, Environmetrics. 2732016</p>
<p>Towards a comprehensive evaluation of dimension reduction methods for transcriptomic data visualization. H Huang, Y Wang, C Rudin, E P Browne, Communications biology. 517192022</p>
<p>An overview of data visualization. M Islam, S Jin, 2019 International Conference on Information Science and Communications Technologies (ICISCT). IEEE2019</p>
<p>Confounding: what it is and how to deal with it. K J Jager, C Zoccali, A Macleod, F W Dekker, Kidney International. 7332008</p>
<p>Making sense of complex data using interactive data visualization. D Janvrin, R Raschke, W Dilla, Journal of Accounting Education. 3242014</p>
<p>Selecting the number of components in principal component analysis using cross-validation approximations. J Josse, F Husson, Computational Statistics &amp; Data Analysis. 5662012</p>
<p>The ones that got away: chemical tagging of globular cluster-origin stars with gaia bp/rp spectra. S G Kane, V Belokurov, M Cranmer, S Monty, H Zhang, A Ardern-Arentsen, Monthly Notices of the Royal Astronomical Society. 53632025</p>
<p>Finding Groups in Data: An Introduction to Cluster Analysis. L Kaufman, P Rousseeuw, 2009John Wiley &amp; Sons</p>
<p>Predicting drug-gene-disease associations by tensor decomposition for network-based computational drug repositioning. Y Kim, Y Cho, Biomedicines. 1171998. 2023</p>
<p>Validation of models: statistical techniques and data availability. J Kleijnen, Proceedings of the 31st conference on Winter simulation: Simulation-a bridge to the future. the 31st conference on Winter simulation: Simulation-a bridge to the futureDec. 19991</p>
<p>Initialization is critical for preserving global data structure in both t-sne and umap. D Kobak, G C Linderman, Nature Biotechnology. 3922021</p>
<p>Efficient data analysis pipeline. T Koivisto, Data Science for Natural Sciences Seminar. 2019</p>
<p>M R Krumholz, M R Bate, H G Arce, J E Dale, R Gutermuth, R I Klein, Z.-Y Li, F Nakamura, Q Zhang, arXiv:1401.2473Star cluster formation and feedback. 2014arXiv preprint</p>
<p>Rashomon effect in educational research: Why more is better than one for measuring the importance of the variables?. J Kuzilek, M Cavus, arXiv:2412.121152024arXiv preprint</p>
<p>Practical experiences on the necessity of external validation. I König, J Malley, C Weimar, H Diener, A Ziegler, Statistics in Medicine. 26302007</p>
<p>Stability-based validation of clustering solutions. T Lange, V Roth, M Braun, J Buhmann, Neural Computation. 1662004</p>
<p>Graphical models for genetic analyses. S L Lauritzen, N A Sheehan, Statistical Science. 1842003</p>
<p>Constraining the original composition of the gas forming first-generation stars in globular clusters. M Legnardi, A Milone, L Armillotta, A Marino, G Cordoni, A Renzini, E Vesperini, F D'antona, M Mckenzie, D Yong, Monthly Notices of the Royal Astronomical Society. 51312022</p>
<p>Machine learning for data integration in human gut microbiome. P Li, H Luo, B Ji, J Nielsen, Microbial Cell Factories. 2112412022</p>
<p>An empirical study of the impact of hyperparameter tuning and model optimization on the performance properties of deep neural networks. L Liao, H Li, W Shang, L Ma, ACM Transactions on Software Engineering and Methodology (TOSEM). 3132022</p>
<p>Stability approach to regularization selection (stars) for high dimensional graphical models. H Liu, K Roeder, L Wasserman, Advances in Neural Information Processing Systems. 232010</p>
<p>Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective. Z Liu, R Ma, Y Zhong, arXiv:2410.166082024arXiv preprint</p>
<p>Omics informatics: from scattered individual software tools to integrated workflow management systems. T Ma, A Zhang, IEEE/ACM Transactions on Computational Biology and Bioinformatics. 1442016</p>
<p>The origin of diverse α-element abundances in galaxy discs. J T Mackereth, R A Crain, R P Schiavon, J Schaye, T Theuns, M Schaller, Monthly Notices of the Royal Astronomical Society. 47742018</p>
<p>The metallicity variations along the chromosome maps: The globular cluster 47 tucanae. A Marino, A Milone, E Dondoglio, A Renzini, G Cordoni, H Jerjen, A Karakas, E Lagioia, M Legnardi, M Mckenzie, The Astrophysical Journal. 9581312023</p>
<p>Building the galactic halo from globular clusters: evidence from chemically unusual red giants. S L Martell, J P Smolinski, T C Beers, E K Grebel, Astronomy &amp; Astrophysics. 534A1362011</p>
<p>Statistical validation. D G Mayer, D G Butler, Ecological Modelling. 681-21993</p>
<p>Umap: Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, arXiv:1802.034262018arXiv preprint</p>
<p>X Meng, Reproducibility, replicability, and reliability. Harvard Data Science Review. 2020210</p>
<p>Multiple populations in star clusters. A P Milone, A F Marino, 20228359Universe</p>
<p>Hierarchical grouping methods and stopping rules: an evaluation. R Mojena, The Computer Journal. 2041977</p>
<p>C Molnar, Interpretable Machine Learning. Lulu.com. 2020</p>
<p>Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data. S Monti, P Tamayo, J Mesirov, T Golub, Machine Learning. 200352</p>
<p>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data. S Monti, P Tamayo, J Mesirov, T Golub, Machine Learning. 200352</p>
<p>Challenges in large-scale bioinformatics projects. S Morrison-Smith, C Boucher, A Sarcevic, N Noyes, C O'brien, N Cuadros, J Ruiz, Humanities and Social Sciences Communications. 912022</p>
<p>Data pipeline management in practice: Challenges and opportunities. A Munappy, J Bosch, H Olsson, Product-Focused Software Process Improvement: 21st International Conference. Turin, ItalySpringer International PublishingNovember 25-27, 2020. 20202020</p>
<p>The data reduction pipeline for the apache point observatory galactic evolution experiment. D L Nidever, J A Holtzman, C A Prieto, S Beland, C Bender, D Bizyaev, A Burton, R Desphande, S W Fleming, A E G Pérez, The Astronomical Journal. 15061732015</p>
<p>Unsupervised classification of images: A review. A Olaode, G Naghdy, C Todd, International Journal of Image Processing. 852014</p>
<p>Abundance ties: Nephele and the globular cluster population accreted with ω cen-based on apogee dr17 and gaia edr3. G Pagnini, P Di Matteo, M Haywood, A Mastrobuono-Battisti, F Renaud, M Mondelin, O Agertz, P Bianchini, L Casamiquela, S Khoperskov, Astronomy &amp; Astrophysics. 693A1552025</p>
<p>The patterns that don't exist: Study on the effects of psychological human biases in data analysis and decision making. A Parkavi, A Jawaid, S Dev, M Vinutha, 2018 3rd International Conference on Computational Systems and Information Technology for Sustainable Solutions (CSITSS). IEEE2018</p>
<p>Reconstructing and classifying sdss dr16 galaxy spectra with machine-learning and dimensionality reduction algorithms. F Pat, S Juneau, V Böhm, R Pucha, A G Kim, A Bolton, C Lepart, D Green, A D Myers, arXiv:2211.117832022arXiv preprint</p>
<p>A ranking stability measure for quantifying the robustness of anomaly detection methods. L Perini, C Galvin, V Vercruyssen, ECML PKDD 2020 Workshops: Workshops of the European Conference on Machine Learning and Knowledge Discovery in Databases. Springer International Publishing2020</p>
<p>Describing posterior distributions of variance components: Problems and the use of null distributions to aid interpretation. J Pick, C Kasper, H Allegue, N Dingemanse, N Dochtermann, K Laskowski, M Lima, H Schielzeth, D Westneat, J Wright, Y Araya-Ajoy, Methods in Ecology and Evolution. 14102023</p>
<p>Strong chemical tagging with apogee: 21 candidate star clusters that have dissolved across the milky way disc. N Price-Jones, J Bovy, J J Webb, C Prieto, R Beaton, J R Brownstein, R E Cohen, K Cunha, J Donor, P M Frinchaboy, Monthly Notices of the Royal Astronomical Society. 49642020</p>
<p>Apogee: the apache point observatory galactic evolution experiment. C A Prieto, S Majewski, R Schiavon, K Cunha, P Frinchaboy, J Holtzman, K Johnston, M Shetrone, M Skrutskie, V Smith, Astronomische Nachrichten: Astronomical Notes. 3299-102008</p>
<p>Tunability: Importance of hyperparameters of machine learning algorithms. P Probst, A.-L Boulesteix, B Bischl, Journal of Machine Learning Research. 20532019</p>
<p>Drug-drug interactions prediction based on drug embedding and graph auto-encoder. S Purkayastha, I Mondal, S Sarkar, P Goyal, J Pillai, 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE). IEEE2019</p>
<p>Objective criteria for the evaluation of clustering methods. W M Rand, Journal of the American Statistical association. 663361971</p>
<p>Tracing the assembly of the milky way's disk through abundance clustering. B L Ratcliffe, M K Ness, K V Johnston, B Sen, The Astrophysical Journal. 90021652020</p>
<p>A tour of unsupervised deep learning for medical image analysis. K Raza, N Singh, Current Medical Imaging. 1792021</p>
<p>Iron-rich metal-poor stars and the astrophysics of thermonuclear events observationally classified as type ia supernovae. i. establishing the connection. H Reggiani, K C Schlaufman, A R Casey, The Astronomical Journal. 16631282023</p>
<p>On encouraging multiple views for visualization. J Roberts, Proceedings. 1998 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics (Cat. No. 98TB100246). 1998 IEEE Conference on Information Visualization. An International Conference on Computer Visualization and Graphics (Cat. No. 98TB100246)IEEEJuly 1998</p>
<p>Multiple view and multiform visualization. J Roberts, Visual Data Exploration and Analysis VII. SPIEFebruary 20003960</p>
<p>Unsupervised machine learning in pathology: The next frontier. A Roohi, K Faust, U Djuric, P Diamandis, Surgical Pathology Clinics. 1322020</p>
<p>Explainable machine learning for scientific insights and discoveries. R Roscher, B Bohn, M F Duarte, J Garcke, IEEE Access. 82020</p>
<p>Increasing model generalizability for unsupervised visual domain adaptation. M Rostami, Conference on Lifelong Learning Agents. PMLRNov. 2022</p>
<p>A resampling approach to cluster validation. V Roth, T Lange, M Braun, J Buhmann, Compstat: Proceedings in Computational Statistics. 2002</p>
<p>Multiple imputation. D B Rubin, Flexible Imputation of Missing Data, Second Edition. Chapman and Hall/CRC2018</p>
<p>C Rudin, C Zhong, L Semenova, M Seltzer, R Parr, J Liu, S Katta, J Donnelly, H Chen, Z Boner, arXiv:2407.04846Amazing things come from having many good models. 2024arXiv preprint</p>
<p>Linking high-z and low-z: Are we observing the progenitors of the milky way with jwst?. E Rusta, S Salvadori, V Gelli, I Koutsouridou, A Marconi, The Astrophysical Journal Letters. 9742L352024</p>
<p>A comparative study of unsupervised machine learning and data mining techniques for intrusion detection. R Sadoddin, A A Ghorbani, International Workshop on Machine Learning and Data Mining in Pattern Recognition. Berlin, Heidelberg; Berlin HeidelbergSpringerJuly 2007</p>
<p>Parametric umap embeddings for representation and semisupervised learning. T Sainburg, L Mcinnes, T Gentner, Neural Computation. 33112021</p>
<p>Machine learning pipelines: Provenance, reproducibility and fair data principles. S Samuel, F Löffler, B König-Ries, International Provenance and Annotation Workshop. ChamSpringer International PublishingJune 2020</p>
<p>Isochrone ages for 3 million stars with the second gaia data release. J L Sanders, P Das, Monthly Notices of the Royal Astronomical Society. 48132018</p>
<p>Advances in projection of climate change impacts using supervised nonlinear dimensionality reduction techniques. A Sarhadi, D H Burn, G Yang, A Ghodsi, Climate Dynamics. 482017</p>
<p>The apogee value-added catalogue of galactic globular cluster stars. R P Schiavon, S G Phillips, N Myers, D Horta, D Minniti, C Prieto, B Anguiano, R L Beaton, T C Beers, J R Brownstein, Monthly Notices of the Royal Astronomical Society. 52822024</p>
<p>Chemical tagging with apogee: discovery of a large population of n-rich stars in the inner galaxy. R P Schiavon, O Zamora, R Carrera, S Lucatello, A Robin, M Ness, S L Martell, V V Smith, D García-Hernández, A Manchado, Monthly Notices of the Royal Astronomical Society. 46512017</p>
<p>Hyperparameter tuning and performance assessment of statistical and machine-learning algorithms using spatial data. P Schratz, J Muenchow, E Iturritxa, J Richter, A Brenning, Ecological Modelling. 4062019</p>
<p>Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. M Schweinsberg, M Feldman, N Staub, O R Van Den Akker, R C Van Aert, M A Van Assen, Y Liu, T Althoff, J Heer, A Kale, Z Mohamed, Organizational Behavior and Human Decision Processes. 1652021</p>
<p>Techniques to eliminate human bias in machine learning. E Sengupta, D Garg, T Choudhury, A Aggarwal, 2018 International Conference on System Modeling &amp; Advancement in Research Trends (SMART). IEEE2018</p>
<p>A robust generalization of isomap for new data. L Shi, P He, B Liu, K Fu, Q Wu, 2005 International Conference on Machine Learning and Cybernetics. IEEEAugust 20053</p>
<p>Morphious: an unsupervised machine learning workflow to detect the activation of microglia and astrocytes. J Silburt, I Aubert, Journal of Neuroinflammation. 191242022</p>
<p>Building reliable data pipelines for managing community data using scientific workflows. Y Simmhan, C Van Ingen, A Szalay, R Barga, J Heasley, 2009 Fifth IEEE International Conference on e-Science. IEEEDecember 2009</p>
<p>Probabilistic Graphical Models for Genetics, Genomics, and Postgenomics. C Sinoquet, 2014OUPOxford</p>
<p>Essential team science skills for biostatisticians on collaborative research teams. E Slade, A M Brearley, A Coles, M J Hayat, P M Kulkarni, A S Nowacki, R A Oster, M A Posner, G Samsa, H Spratt, Journal of Clinical and Translational Science. 71e2432023</p>
<p>The effect of hyperparameter tuning on the comparative evaluation of unsupervised anomaly detection methods. J Soenen, E Van Wolputte, L Perini, V Vercruyssen, W Meert, J Davis, H Blockeel, Proceedings of the KDD'21 Workshop on Outlier Detection and Description. the KDD'21 Workshop on Outlier Detection and DescriptionOutlier Detection and Description Organising CommitteeAugust 2021</p>
<p>Unsupervised learning of natural languages. Z Solan, D Horn, E Ruppin, S Edelman, Proceedings of the National Academy of Sciences. 102332005</p>
<p>Missforest-non-parametric missing value imputation for mixed-type data. D J Stekhoven, P Bühlmann, Bioinformatics. 2812012</p>
<p>Theme editor's introduction to reproducibility and replicability in science. V Stodden, Harvard Data Science Review. 242020</p>
<p>Advances in Computer Vision and Pattern Recognition. L Sucar, 2015SpringerLondonProbabilistic Graphical Models</p>
<p>Dynamic visualization of high-dimensional data. E D Sun, R Ma, J Zou, Nature Computational Science. 312023</p>
<p>Evolution and impact of bias in human and machine learning algorithm interaction. W Sun, O Nasraoui, P Shafto, PLOS ONE. 158e02355022020</p>
<p>Outlier Detection: Techniques and Applications. N Suri, M Murty, G Athithan, 2019Springer Nature</p>
<p>Spatio-temporal autoencoders in weather and climate research. X.-A Tibau, C Reimers, C Requena-Mesa, J Runge, Deep Learning for the Earth Sciences: A Comprehensive Approach to Remote Sensing, Climate Science, and Geosciences. 2021</p>
<p>Cluster validation by prediction strength. R Tibshirani, G Walther, Journal of Computational and Graphical Statistics. 1432005</p>
<p>How many elements matter?. Y.-S Ting, D H Weinberg, The Astrophysical Journal. 92722092022</p>
<p>Workflow based framework for life science informatics. A Tiwari, A K Sekhar, Computational biology and chemistry. 315-62007</p>
<p>The formation of the nuclei of galaxies. i-m31. S D Tremaine, J Ostriker, L SpitzerJr, Astrophysical Journal. 1961Mar. 1, 1975. 1975</p>
<p>A conceptual basis for feature engineering. C Turner, A Fuggetta, L Lavazza, A Wolf, Journal of Systems and Software. 4911999</p>
<p>Why is data visualization important? what is important in data visualization. A Unwin, Harvard Data Science Review. 2112020</p>
<p>Visualizing data using t-sne. L Van Der Maaten, G Hinton, Journal of machine learning research. 9112008</p>
<p>Dimensionality reduction: A comparative review. L Van Der Maaten, E O Postma, H J Van Den, Herik, Journal of Machine Learning Research. 1066-71132009</p>
<p>A novel approach for drug-target interactions prediction based on multimodal deep autoencoder. H Wang, J Wang, C Dong, Y Lian, D Liu, Z Yan, Frontiers in Pharmacology. 1015922020</p>
<p>Interactive Data Visualization: Foundations, Techniques, and Applications. M Ward, G Grinstein, D Keim, 2010AK Peters/CRC Press</p>
<p>On the philosophy of unsupervised learning. D Watson, Philosophy &amp; Technology. 362282023</p>
<p>Generalizability theory: Overview. N M Webb, R J Shavelson, Encyclopedia of Statistics in Behavioral Science. Wiley20052</p>
<p>Importance of tuning hyperparameters of machine learning algorithms. arXiv. H J Weerts, A C Mueller, J Vanschoren, arXiv:2007.075882020arXiv preprint</p>
<p>An unsupervised map of excitatory neurons' dendritic morphology the mouse visual cortex. M Weis, S Papadopoulos, L Hansel, T Lüddecke, B Celii, P Fahey, E Wang, J Bae, A Bodor, D Brittain, J Buchanan, bioRxiv. 2022</p>
<p>R for Data Science. H Wickham, G Grolemund, 20172Media, Sebastopol, CA</p>
<p>Trust but verify: How to leverage policies, workflows, and infrastructure to ensure computational reproducibility in publication. C Willis, V Stodden, Harvard Data Science Review. 242021</p>
<p>Statistical method scdeed for detecting dubious 2d singlecell embeddings and optimizing t-sne and umap hyperparameters. L Xia, C Lee, J J Li, Nature Communications. 15117532024</p>
<p>Statistical method scdeed for detecting dubious 2d singlecell embeddings and optimizing t-sne and umap hyperparameters. L Xia, C Lee, J J Li, Nature Communications. 15117532024</p>
<p>. R Xu, D Wunsch, Clustering, 2008John Wiley &amp; Sons</p>
<p>The generalizability crisis. T Yarkoni, Behavioral and Brain Sciences. 45e12022</p>
<p>. B Yu, Stability. Bernoulli. 1942013</p>
<p>Veridical data science. B Yu, K Kumbier, Proceedings of the National Academy of Sciences. 11782020</p>
<p>T Yu, H Zhu, arXiv:2003.05689Hyper-parameter optimization: A review of algorithms and applications. 2020arXiv preprint</p>
<p>Enhancing dependability in big data analytics enterprise pipelines. H Zahid, T Mahmood, N Ikram, Security, Privacy, and Anonymity in Computation, Communication, and Storage: 11th International Conference and Satellite Workshops. SpaCCS; Melbourne, NSW, AustraliaSpringer International Publishing2018. December 11-13. 2018. 201811</p>
<p>Towards explaining the effects of data preprocessing on machine learning. C Zelaya, 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEEApril 2019</p>
<p>Decoding cortical brain states from widefield calcium imaging data using visibility graph. L Zhu, C Lee, D Margolis, L Najafizadeh, Biomedical Optics Express. 972018</p>
<p>Cell cycle plasticity underlies fractional resistance to palbociclib in er+/her2-breast tumor cells. T M Zikry, S C Wolff, J S Ranek, H M Davis, A Naugle, N Luthra, A A Whitman, K M Kedziora, W Stallaert, M R Kosorok, Proceedings of the National Academy of Sciences. 1217e23092611212024</p>            </div>
        </div>

    </div>
</body>
</html>
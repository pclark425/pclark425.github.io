<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-29de7c0fb3c09eaf55b20619bceaeafe72fd87a6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/29de7c0fb3c09eaf55b20619bceaeafe72fd87a6" target="_blank">Hierarchical Neural Story Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work collects a large dataset of 300K human-written stories paired with writing prompts from an online forum that enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text.</p>
                <p><strong>Paper Abstract:</strong> We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WritingPrompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WritingPrompts dataset (Reddit WritingPrompts corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corpus of 303,358 human-written stories paired with writing prompts scraped from Reddit's r/WritingPrompts, cleaned and split into train/validation/test; used as the primary evaluation corpus for story generation models in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various story-generation models evaluated in paper (GCNN LM, Conv seq2seq, Conv seq2seq + self-attention, Fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples: GCNN LM 123.4M, Conv seq2seq 113.0M, Conv seq2seq + self-attention 134.7M, Fusion 255.4M, Ensemble 270.3M)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generated text (creative story generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Test-set evaluation on WritingPrompts dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models are evaluated by computing automatic metrics (perplexity, prompt-ranking) on held-out stories from the WritingPrompts test split and by conducting human evaluations using prompts from the held-out test set.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perplexity; Prompt ranking accuracy; Human evaluation pairing accuracy; human preference</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Perplexity (lower is better): model perplexity on held-out human-written stories; Prompt ranking accuracy: % of cases where true prompt is ranked most likely among 10 prompts; Human metrics: % preference or % correct pairings in human tasks (see details).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WritingPrompts (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Held-out prompts from the WritingPrompts test set were used. For the triple-pairing task: 105 stories per model grouped into questions, each question evaluated by 15 judges. For hierarchical vs non-hierarchical preference: 400 pairs evaluated by 5 judges each (blind).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Perplexity reported per model on WritingPrompts (e.g., Fusion: test perplexity 36.56; Conv seq2seq + self-attention: 37.94; Conv seq2seq baseline: 45.54). Human preference: hierarchical model preferred 67.32% vs language model 32.68%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Perplexity is computed on human-written stories from the dataset; models are compared to one another on the same human-held-out data (fusion model achieves best perplexity).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset contains many rare words and misspellings making full-vocabulary modeling challenging; prompts and stories vary widely in style and length which complicates automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity (language model likelihood metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard automatic metric for language models measuring how well a probabilistic model predicts held-out text; lower perplexity indicates better predictive/fluency performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conv seq2seq, Conv seq2seq + self-attention, Fusion, GCNN LMs (evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see Table 3: ranges ~110M–270M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generated text evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute model perplexity on held-out human-written stories: a measure of the probability assigned by the model to the next token sequences in the test stories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perplexity (unitless; lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Perplexity = exp(-1/N * sum_{i=1..N} log p(x_i | context)), reported as scalar per model on validation/test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WritingPrompts test set</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: Conv seq2seq test perplexity 45.54; + self-attention 42.32; + multiscale+gating 37.94; Fusion model 36.56 (best reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Perplexity is measured on human-generated stories (the test set), enabling comparison of how well models explain human text; fusion model shows improved likelihood of human stories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Perplexity primarily measures local fluency and next-token prediction and may not capture long-range coherence, topicality, or creativity; not fully reflective of quality in open-ended generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptRanking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt ranking accuracy (likelihood-based prompt identification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic metric introduced in this paper that quantifies how strongly a model's output depends on its input prompt by ranking likelihoods of a decoded story under multiple candidate prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conv seq2seq + self-attention, Fusion and other evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generated text evaluation / conditioning strength</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompt ranking accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each story, decode under 10 prompts (1 true prompt + 9 random prompts) and compute the likelihood of the story given each prompt; measure the fraction where the true prompt yields the highest likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Prompt ranking accuracy (% of cases true prompt ranked top)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction (0–100%) of evaluated stories (1000 stories sampled) where the true prompt is assigned the highest model likelihood among the 10 prompts tested.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WritingPrompts (1000 stories sampled from test set for this metric)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported as automatic metric; exact absolute percentages per model not tabulated in main text, but authors report fusion and self-attention improvements and use 1000 stories per model for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to compare models' conditional dependence on prompts; fusion and self-attention models perform better than baselines, indicating stronger prompt conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on likelihood estimates which can be sensitive to model calibration; only tests discriminative ability among sampled prompts and may not capture semantic adherence beyond likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanTriplePairing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human triple-pairing task (Mechanical Turk prompt-story pairing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation protocol introduced/used in this paper where judges are shown three stories and three prompts (shuffled) and asked to match each story to its generating prompt, measuring how clearly a story reflects its prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generated stories from multiple models (each model produced stories for the task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / human evaluation of NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human-assessed quality (topicality/adherence)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Triple prompt–story pairing (human)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Groups of three generated stories (and their prompts) are shuffled and presented to human evaluators who must select the correct pairing for all three prompts; measures how well story content signals its prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pairing accuracy (%) as judged by humans</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of correctly paired prompt–story matches identified by human judges across evaluated questions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated stories from WritingPrompts held-out prompts; 105 stories per model grouped into questions</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>105 stories per model grouped into questions; each question evaluated by 15 judges on Amazon Mechanical Turk.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors report that the fusion model significantly improves humans' ability to pair stories with prompts (fusion improved pairing accuracy vs non-fusion by ~7% according to text/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to compare models: fusion model yields higher human pairing accuracy than comparable non-fusion/enabled baselines; also compared to KNN (fusion matches KNN in prompt-story connection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human judgments can be noisy; task measures recognizability of prompt in story but not other qualities (fluency, creativity); details of inter-rater agreement not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PairwisePreference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human pairwise preference (hierarchical vs non-hierarchical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A blind human evaluation comparing hierarchical generation (premise + seq2seq story) and non-hierarchical generation, where raters choose which of two stories they prefer reading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hierarchical fusion model vs language model baseline (and non-hierarchical generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hierarchical fusion model ~255.4M parameters; language model examples ~126.4M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise human preference test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human raters are shown pairs of stories (one from hierarchical generation, one from baseline) and asked which they prefer; aggregated preferences indicate quality differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human preference percentage per model</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of human raters preferring stories from one condition over the other (e.g., hierarchical model preferred X% of the time).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated stories from WritingPrompts prompts (400 pairs evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>400 story pairs evaluated by 5 judges each on Amazon Mechanical Turk; blind comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human raters preferred the hierarchical model 67.32% of the time vs 32.68% for the language model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Hierarchical generation (premise + seq2seq) is preferred by humans over non-hierarchical language-model-only generation by a large margin (~2:1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Preference captures subjective reading enjoyment and topical coherence but may not isolate specific aspects (e.g., novelty vs correctness); small number of judges per pair (5) and no inter-rater stats provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7973.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KNNBaseline (FAISS+FASTTEXT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-neighbour retrieval baseline using FASTTEXT vectors and FAISS KNN search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-generative baseline that retrieves the closest prompt in the training set (using FASTTEXT embeddings and FAISS for fast KNN) and returns the associated training story (trimmed to 150 words) for comparison with generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KNN retrieval baseline (FAISS+FASTTEXT retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (retrieval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLG baselines</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>baseline retrieval method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KNN retrieval relevance and scaling test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use TF-IDF/FASTTEXT vectorization and FAISS nearest-neighbor search to retrieve training stories for given prompts and compare relevance to generated stories via prompt–story pairing accuracy and relevance vs rank k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Prompt–story pairing accuracy (human) and relevance decay as function of k-th best story</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Human pairing accuracy (%) for retrieved stories; plots of relevance of k-th best retrieved story indicating degradation in coverage/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WritingPrompts training set used as retrieval pool</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>KNN retrieved stories included in human pairing tasks for comparison; human judges assessed pairing accuracy as for generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>KNN baseline matches or narrowly outperforms models on prompt-story connection for top-ranked retrieval but degrades rapidly with rank; generative fusion model can match KNN while generalizing to produce many relevant stories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Fusion model matches KNN in prompt-story connection but offers unlimited novel outputs unlike KNN which is limited to training set examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>KNN copies training data (no generalization); limited to retrieval pool and degrades with rank; not generative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7973.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU/ROUGE (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU and ROUGE (n-gram overlap metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common n-gram overlap metrics used in MT/summary evaluation that measure surface overlap between generated and reference text; the paper notes these are not useful for open-ended story generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric (mentioned as unsuitable)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU / ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram overlap between generated text and reference human text; the paper explains these are inadequate for open-ended generation because there is no single correct target.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU; ROUGE (overlap scores)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard BLEU/ROUGE scoring (0–100 scale typically), measuring n-gram precision/recall between candidate and reference texts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Not used — authors explicitly state BLEU/ROUGE are not useful for their open-ended story generation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>These metrics measure n-gram overlap and penalize novel but valid generations; unsuitable for open-ended creative generation where many valid outputs exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7973.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongestCommonSubsequence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Longest Common Subsequence (LCS) copying measure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated copying measure used in the paper's analysis: the average longest common subsequence between generated stories and the training set stories to estimate copying/plagiarism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion model, Conv seq2seq baseline, KNN baseline (used for copying analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLG evaluation (copying detection)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>copying/plagiarism metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Average longest common subsequence length</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For generated stories, compute the longest common subsequence with any story in the training set and report the average length—used to quantify degree of verbatim copying.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average LCS length (in words)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Average length (in words) of the longest common subsequence between each generated story and the most similar story in the training set.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generated stories vs WritingPrompts training set</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On 500 150-word generated stories: Fusion model average longest common subsequence = 8.9 words; Conv seq2seq baseline = 10.2 words; KNN baseline copies all 150 words (by construction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to show generative model does not verbatim copy as much as KNN and copies slightly less/more depending on model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LCS measures verbatim overlap but does not capture paraphrasing or semantic copying; may underestimate non-exact copying.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7973.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7973.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-kSampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-k random sampling (k=10) with temperature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/generation strategy used by the authors: at each timestep, randomly sample from the top k most probable tokens (k=10), with a tuned softmax temperature to balance diversity and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All generative models evaluated in the paper during generation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLG generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>decoding strategy (affects evaluation outcomes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Top-k random sampling (k=10) with temperature tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>At generation time, restrict candidate next tokens to the top k=10 by probability, then sample randomly among them; temperature parameter tuned per model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Implicitly affects human and automatic metrics (fluency, diversity); not reported as numeric metric itself</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>k is set to 10; temperature is tuned per model to obtain desired diversity/fluency; shorter/generic outputs avoided compared to beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on WritingPrompts generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors state top-k sampling (k=10) produced better, less generic outputs than beam search and better behaved than completely random sampling; temperature tuned per model to produce 150-word stories without unknown tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Top-k sampling used consistently across models for human evaluation comparability; beam search produced short/generic outputs and was not used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Sampling can still produce tokenization or continuity errors (e.g., split contractions) and repetition; sampling strategy influences evaluation and must be considered when comparing models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Neural Story Generation', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>Cold fusion: Training seq2seq models together with language models <em>(Rating: 2)</em></li>
                <li>Convolutional sequence to sequence learning <em>(Rating: 2)</em></li>
                <li>Language modeling with gated convolutional networks <em>(Rating: 2)</em></li>
                <li>Billion-scale similarity search with gpus <em>(Rating: 1)</em></li>
                <li>Enriching word vectors with subword information <em>(Rating: 1)</em></li>
                <li>End-to-end memory networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7973",
    "paper_id": "paper-29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "WritingPrompts",
            "name_full": "WritingPrompts dataset (Reddit WritingPrompts corpus)",
            "brief_description": "A corpus of 303,358 human-written stories paired with writing prompts scraped from Reddit's r/WritingPrompts, cleaned and split into train/validation/test; used as the primary evaluation corpus for story generation models in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various story-generation models evaluated in paper (GCNN LM, Conv seq2seq, Conv seq2seq + self-attention, Fusion)",
            "model_size": "various (examples: GCNN LM 123.4M, Conv seq2seq 113.0M, Conv seq2seq + self-attention 134.7M, Fusion 255.4M, Ensemble 270.3M)",
            "scientific_domain": "computer science / natural language generation",
            "theory_type": "generated text (creative story generation)",
            "evaluation_method_name": "Test-set evaluation on WritingPrompts dataset",
            "evaluation_method_description": "Models are evaluated by computing automatic metrics (perplexity, prompt-ranking) on held-out stories from the WritingPrompts test split and by conducting human evaluations using prompts from the held-out test set.",
            "evaluation_metric": "Perplexity; Prompt ranking accuracy; Human evaluation pairing accuracy; human preference",
            "metric_definition": "Perplexity (lower is better): model perplexity on held-out human-written stories; Prompt ranking accuracy: % of cases where true prompt is ranked most likely among 10 prompts; Human metrics: % preference or % correct pairings in human tasks (see details).",
            "dataset_or_benchmark": "WritingPrompts (this paper's dataset)",
            "human_evaluation_details": "Held-out prompts from the WritingPrompts test set were used. For the triple-pairing task: 105 stories per model grouped into questions, each question evaluated by 15 judges. For hierarchical vs non-hierarchical preference: 400 pairs evaluated by 5 judges each (blind).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Perplexity reported per model on WritingPrompts (e.g., Fusion: test perplexity 36.56; Conv seq2seq + self-attention: 37.94; Conv seq2seq baseline: 45.54). Human preference: hierarchical model preferred 67.32% vs language model 32.68%.",
            "comparison_to_human_generated": true,
            "comparison_results": "Perplexity is computed on human-written stories from the dataset; models are compared to one another on the same human-held-out data (fusion model achieves best perplexity).",
            "limitations_noted": "Dataset contains many rare words and misspellings making full-vocabulary modeling challenging; prompts and stories vary widely in style and length which complicates automatic evaluation.",
            "uuid": "e7973.0",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Perplexity",
            "name_full": "Perplexity (language model likelihood metric)",
            "brief_description": "A standard automatic metric for language models measuring how well a probabilistic model predicts held-out text; lower perplexity indicates better predictive/fluency performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Conv seq2seq, Conv seq2seq + self-attention, Fusion, GCNN LMs (evaluated models)",
            "model_size": "various (see Table 3: ranges ~110M–270M parameters)",
            "scientific_domain": "computer science / natural language generation",
            "theory_type": "generated text evaluation",
            "evaluation_method_name": "Perplexity",
            "evaluation_method_description": "Compute model perplexity on held-out human-written stories: a measure of the probability assigned by the model to the next token sequences in the test stories.",
            "evaluation_metric": "Perplexity (unitless; lower is better)",
            "metric_definition": "Perplexity = exp(-1/N * sum_{i=1..N} log p(x_i | context)), reported as scalar per model on validation/test sets.",
            "dataset_or_benchmark": "WritingPrompts test set",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Examples: Conv seq2seq test perplexity 45.54; + self-attention 42.32; + multiscale+gating 37.94; Fusion model 36.56 (best reported).",
            "comparison_to_human_generated": true,
            "comparison_results": "Perplexity is measured on human-generated stories (the test set), enabling comparison of how well models explain human text; fusion model shows improved likelihood of human stories.",
            "limitations_noted": "Perplexity primarily measures local fluency and next-token prediction and may not capture long-range coherence, topicality, or creativity; not fully reflective of quality in open-ended generation.",
            "uuid": "e7973.1",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "PromptRanking",
            "name_full": "Prompt ranking accuracy (likelihood-based prompt identification)",
            "brief_description": "An automatic metric introduced in this paper that quantifies how strongly a model's output depends on its input prompt by ranking likelihoods of a decoded story under multiple candidate prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Conv seq2seq + self-attention, Fusion and other evaluated models",
            "model_size": "various (see Table 3)",
            "scientific_domain": "computer science / NLG evaluation",
            "theory_type": "generated text evaluation / conditioning strength",
            "evaluation_method_name": "Prompt ranking accuracy",
            "evaluation_method_description": "For each story, decode under 10 prompts (1 true prompt + 9 random prompts) and compute the likelihood of the story given each prompt; measure the fraction where the true prompt yields the highest likelihood.",
            "evaluation_metric": "Prompt ranking accuracy (% of cases true prompt ranked top)",
            "metric_definition": "Fraction (0–100%) of evaluated stories (1000 stories sampled) where the true prompt is assigned the highest model likelihood among the 10 prompts tested.",
            "dataset_or_benchmark": "WritingPrompts (1000 stories sampled from test set for this metric)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Reported as automatic metric; exact absolute percentages per model not tabulated in main text, but authors report fusion and self-attention improvements and use 1000 stories per model for evaluation.",
            "comparison_to_human_generated": false,
            "comparison_results": "Used to compare models' conditional dependence on prompts; fusion and self-attention models perform better than baselines, indicating stronger prompt conditioning.",
            "limitations_noted": "Relies on likelihood estimates which can be sensitive to model calibration; only tests discriminative ability among sampled prompts and may not capture semantic adherence beyond likelihood.",
            "uuid": "e7973.2",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "HumanTriplePairing",
            "name_full": "Human triple-pairing task (Mechanical Turk prompt-story pairing)",
            "brief_description": "A human evaluation protocol introduced/used in this paper where judges are shown three stories and three prompts (shuffled) and asked to match each story to its generating prompt, measuring how clearly a story reflects its prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Generated stories from multiple models (each model produced stories for the task)",
            "model_size": "various (see Table 3)",
            "scientific_domain": "computer science / human evaluation of NLG",
            "theory_type": "human-assessed quality (topicality/adherence)",
            "evaluation_method_name": "Triple prompt–story pairing (human)",
            "evaluation_method_description": "Groups of three generated stories (and their prompts) are shuffled and presented to human evaluators who must select the correct pairing for all three prompts; measures how well story content signals its prompt.",
            "evaluation_metric": "Pairing accuracy (%) as judged by humans",
            "metric_definition": "Percentage of correctly paired prompt–story matches identified by human judges across evaluated questions.",
            "dataset_or_benchmark": "Generated stories from WritingPrompts held-out prompts; 105 stories per model grouped into questions",
            "human_evaluation_details": "105 stories per model grouped into questions; each question evaluated by 15 judges on Amazon Mechanical Turk.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Authors report that the fusion model significantly improves humans' ability to pair stories with prompts (fusion improved pairing accuracy vs non-fusion by ~7% according to text/figures).",
            "comparison_to_human_generated": false,
            "comparison_results": "Used to compare models: fusion model yields higher human pairing accuracy than comparable non-fusion/enabled baselines; also compared to KNN (fusion matches KNN in prompt-story connection).",
            "limitations_noted": "Human judgments can be noisy; task measures recognizability of prompt in story but not other qualities (fluency, creativity); details of inter-rater agreement not reported.",
            "uuid": "e7973.3",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "PairwisePreference",
            "name_full": "Human pairwise preference (hierarchical vs non-hierarchical)",
            "brief_description": "A blind human evaluation comparing hierarchical generation (premise + seq2seq story) and non-hierarchical generation, where raters choose which of two stories they prefer reading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Hierarchical fusion model vs language model baseline (and non-hierarchical generation)",
            "model_size": "hierarchical fusion model ~255.4M parameters; language model examples ~126.4M",
            "scientific_domain": "computer science / NLG",
            "theory_type": "human preference evaluation",
            "evaluation_method_name": "Pairwise human preference test",
            "evaluation_method_description": "Human raters are shown pairs of stories (one from hierarchical generation, one from baseline) and asked which they prefer; aggregated preferences indicate quality differences.",
            "evaluation_metric": "Human preference percentage per model",
            "metric_definition": "Percentage of human raters preferring stories from one condition over the other (e.g., hierarchical model preferred X% of the time).",
            "dataset_or_benchmark": "Generated stories from WritingPrompts prompts (400 pairs evaluated)",
            "human_evaluation_details": "400 story pairs evaluated by 5 judges each on Amazon Mechanical Turk; blind comparison.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human raters preferred the hierarchical model 67.32% of the time vs 32.68% for the language model baseline.",
            "comparison_to_human_generated": false,
            "comparison_results": "Hierarchical generation (premise + seq2seq) is preferred by humans over non-hierarchical language-model-only generation by a large margin (~2:1).",
            "limitations_noted": "Preference captures subjective reading enjoyment and topical coherence but may not isolate specific aspects (e.g., novelty vs correctness); small number of judges per pair (5) and no inter-rater stats provided.",
            "uuid": "e7973.4",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "KNNBaseline (FAISS+FASTTEXT)",
            "name_full": "Nearest-neighbour retrieval baseline using FASTTEXT vectors and FAISS KNN search",
            "brief_description": "A non-generative baseline that retrieves the closest prompt in the training set (using FASTTEXT embeddings and FAISS for fast KNN) and returns the associated training story (trimmed to 150 words) for comparison with generative models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "KNN retrieval baseline (FAISS+FASTTEXT retrieval)",
            "model_size": "N/A (retrieval baseline)",
            "scientific_domain": "computer science / NLG baselines",
            "theory_type": "baseline retrieval method",
            "evaluation_method_name": "KNN retrieval relevance and scaling test",
            "evaluation_method_description": "Use TF-IDF/FASTTEXT vectorization and FAISS nearest-neighbor search to retrieve training stories for given prompts and compare relevance to generated stories via prompt–story pairing accuracy and relevance vs rank k.",
            "evaluation_metric": "Prompt–story pairing accuracy (human) and relevance decay as function of k-th best story",
            "metric_definition": "Human pairing accuracy (%) for retrieved stories; plots of relevance of k-th best retrieved story indicating degradation in coverage/generalization.",
            "dataset_or_benchmark": "WritingPrompts training set used as retrieval pool",
            "human_evaluation_details": "KNN retrieved stories included in human pairing tasks for comparison; human judges assessed pairing accuracy as for generative models.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "KNN baseline matches or narrowly outperforms models on prompt-story connection for top-ranked retrieval but degrades rapidly with rank; generative fusion model can match KNN while generalizing to produce many relevant stories.",
            "comparison_to_human_generated": false,
            "comparison_results": "Fusion model matches KNN in prompt-story connection but offers unlimited novel outputs unlike KNN which is limited to training set examples.",
            "limitations_noted": "KNN copies training data (no generalization); limited to retrieval pool and degrades with rank; not generative.",
            "uuid": "e7973.5",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "BLEU/ROUGE (mentioned)",
            "name_full": "BLEU and ROUGE (n-gram overlap metrics)",
            "brief_description": "Common n-gram overlap metrics used in MT/summary evaluation that measure surface overlap between generated and reference text; the paper notes these are not useful for open-ended story generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "N/A",
            "model_size": "N/A",
            "scientific_domain": "computer science / NLP evaluation",
            "theory_type": "evaluation metric (mentioned as unsuitable)",
            "evaluation_method_name": "BLEU / ROUGE",
            "evaluation_method_description": "Compute n-gram overlap between generated text and reference human text; the paper explains these are inadequate for open-ended generation because there is no single correct target.",
            "evaluation_metric": "BLEU; ROUGE (overlap scores)",
            "metric_definition": "Standard BLEU/ROUGE scoring (0–100 scale typically), measuring n-gram precision/recall between candidate and reference texts.",
            "dataset_or_benchmark": "",
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Not used — authors explicitly state BLEU/ROUGE are not useful for their open-ended story generation setting.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "These metrics measure n-gram overlap and penalize novel but valid generations; unsuitable for open-ended creative generation where many valid outputs exist.",
            "uuid": "e7973.6",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "LongestCommonSubsequence",
            "name_full": "Longest Common Subsequence (LCS) copying measure",
            "brief_description": "An automated copying measure used in the paper's analysis: the average longest common subsequence between generated stories and the training set stories to estimate copying/plagiarism.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Fusion model, Conv seq2seq baseline, KNN baseline (used for copying analysis)",
            "model_size": "various",
            "scientific_domain": "computer science / NLG evaluation (copying detection)",
            "theory_type": "copying/plagiarism metric",
            "evaluation_method_name": "Average longest common subsequence length",
            "evaluation_method_description": "For generated stories, compute the longest common subsequence with any story in the training set and report the average length—used to quantify degree of verbatim copying.",
            "evaluation_metric": "Average LCS length (in words)",
            "metric_definition": "Average length (in words) of the longest common subsequence between each generated story and the most similar story in the training set.",
            "dataset_or_benchmark": "Generated stories vs WritingPrompts training set",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "On 500 150-word generated stories: Fusion model average longest common subsequence = 8.9 words; Conv seq2seq baseline = 10.2 words; KNN baseline copies all 150 words (by construction).",
            "comparison_to_human_generated": false,
            "comparison_results": "Used to show generative model does not verbatim copy as much as KNN and copies slightly less/more depending on model.",
            "limitations_noted": "LCS measures verbatim overlap but does not capture paraphrasing or semantic copying; may underestimate non-exact copying.",
            "uuid": "e7973.7",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Top-kSampling",
            "name_full": "Top-k random sampling (k=10) with temperature",
            "brief_description": "A decoding/generation strategy used by the authors: at each timestep, randomly sample from the top k most probable tokens (k=10), with a tuned softmax temperature to balance diversity and coherence.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All generative models evaluated in the paper during generation experiments",
            "model_size": "various",
            "scientific_domain": "computer science / NLG generation",
            "theory_type": "decoding strategy (affects evaluation outcomes)",
            "evaluation_method_name": "Top-k random sampling (k=10) with temperature tuning",
            "evaluation_method_description": "At generation time, restrict candidate next tokens to the top k=10 by probability, then sample randomly among them; temperature parameter tuned per model.",
            "evaluation_metric": "Implicitly affects human and automatic metrics (fluency, diversity); not reported as numeric metric itself",
            "metric_definition": "k is set to 10; temperature is tuned per model to obtain desired diversity/fluency; shorter/generic outputs avoided compared to beam search.",
            "dataset_or_benchmark": "Used on WritingPrompts generation tasks",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Authors state top-k sampling (k=10) produced better, less generic outputs than beam search and better behaved than completely random sampling; temperature tuned per model to produce 150-word stories without unknown tokens.",
            "comparison_to_human_generated": false,
            "comparison_results": "Top-k sampling used consistently across models for human evaluation comparability; beam search produced short/generic outputs and was not used.",
            "limitations_noted": "Sampling can still produce tokenization or continuity errors (e.g., split contractions) and repetition; sampling strategy influences evaluation and must be considered when comparing models.",
            "uuid": "e7973.8",
            "source_info": {
                "paper_title": "Hierarchical Neural Story Generation",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Attention is all you need",
            "rating": 2
        },
        {
            "paper_title": "Cold fusion: Training seq2seq models together with language models",
            "rating": 2
        },
        {
            "paper_title": "Convolutional sequence to sequence learning",
            "rating": 2
        },
        {
            "paper_title": "Language modeling with gated convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Billion-scale similarity search with gpus",
            "rating": 1
        },
        {
            "paper_title": "Enriching word vectors with subword information",
            "rating": 1
        },
        {
            "paper_title": "End-to-end memory networks",
            "rating": 1
        }
    ],
    "cost": 0.015448499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hierarchical Neural Story Generation</h1>
<p>Angela Fan Mike Lewis Yann Dauphin<br>Facebook AI Research, Menlo Park<br>{angelafan, mikelewis, ynd}@fb.com</p>
<h4>Abstract</h4>
<p>We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300 K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</p>
<h2>1 Introduction</h2>
<p>Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation (Wiseman et al., 2017).</p>
<p>We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic.</p>
<p>Prompt: The Mage, the Warrior, and the Priest
Story: A light breeze swept the ground, and carried with it still the distant scents of dust and time-worn stone. The Warrior led the way, heaving her mass of armour and muscle over the uneven terrain. She soon crested the last of the low embankments, which still bore the unmistakable fingerprints of haste and fear. She lifted herself up onto the top the rise, and looked out at the scene before her. [...]</p>
<p>Figure 1: Example prompt and beginning of a story from our dataset. We train a hierarchical model that first generates a prompt, and then conditions on the prompt when generating a story.</p>
<p>We find that standard sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that has been noted in other domains, such as dialogue response generation (Li et al., 2015a)). This failure is due to the complex and underspecified dependencies between the prompt and the story, which are much harder to model than the closer dependencies required for language modeling (for example, consider the subtle relationship between the first sentence and prompt in Figure 1).</p>
<p>To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism (Sriram et al., 2017) where our model is trained on top of an pre-trained seq2seq model. To improve over the pre-trained model, the second model must focus on the link between the prompt and the story. For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output.</p>
<p>Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures-stories contain 734 words on average in our dataset. We improve efficiency using a convolutional architecture, al-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Train Stories</th>
<th style="text-align: right;">272,600</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Test Stories</td>
<td style="text-align: right;">15,138</td>
</tr>
<tr>
<td style="text-align: left;"># Validation Stories</td>
<td style="text-align: right;">15,620</td>
</tr>
<tr>
<td style="text-align: left;"># Prompt Words</td>
<td style="text-align: right;">7.7 M</td>
</tr>
<tr>
<td style="text-align: left;"># Story Words</td>
<td style="text-align: right;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">Average Length of Prompts</td>
<td style="text-align: right;">28.4</td>
</tr>
<tr>
<td style="text-align: left;">Average Length of Stories</td>
<td style="text-align: right;">734.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of WritingPrompts dataset
lowing whole stories to be encoded in parallel. Existing convolutional architectures only encode a bounded amount of context (Dauphin et al., 2017), so we introduce a novel gated self-attention mechanism that allows the model to condition on its previous outputs at different time-scales.</p>
<p>To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation.</p>
<p>Experiments show that our fusion and selfattention mechanisms improve over existing techniques on both automated and human evaluation measures. Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text. Human judges prefer our hierarchical model's stories twice as often as those of a nonhierarchical baseline.</p>
<h2>2 Writing Prompts Dataset</h2>
<p>We collect a hierarchical story generation dataset ${ }^{1}$ from Reddit's WritingPrompts forum. ${ }^{2}$ WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure 1 shows an example.</p>
<p>We scraped three years of prompts and their associated stories using the official Reddit API. We clean the dataset by removing automated bot posts, deleted posts, special announcements, com-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ments from moderators, and stories shorter than 30 words. We use NLTK for tokenization. The dataset models full text to generate immediately human-readable stories. We reserve $5 \%$ of the prompts for a validation set and $5 \%$ for a test set, and present additional statistics about the dataset in Table 1.</p>
<p>For our experiments, we limit the length of the stories to 1000 words maximum and limit the vocabulary size for the prompts and the stories to words appearing more than 10 times each. We model an unknown word token and an end of document token. This leads to a vocabulary size of 19,025 for the prompts and 104,960 for the stories. As the dataset is scraped from an online forum, the number of rare words and misspellings is quite large, so modeling the full vocabulary is challenging and computationally intensive.</p>
<h2>3 Approach</h2>
<p>The challenges of WritingPrompts are primarily in modeling long-range dependencies and conditioning on an abstract, high-level prompt. Recurrent and convolutional networks have successfully modeled sentences (Jozefowicz et al., 2016; Dauphin et al., 2017), but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these challenges in the following sections.</p>
<h3>3.1 Hierarchical Story Generation</h3>
<p>High-level structure is integral to good stories, but language models generate on a strictly-word-byword basis and so cannot explicitly make highlevel plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from Dauphin et al. (2017). The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Self-Attention Mechanism of a single head, with GLU gating and downsampling. Multiple heads are concatenated, with each head using a separate downsampling function.</p>
<h3>3.2 Efficient Learning with Convolutional Sequence-to-Sequence Model</h3>
<p>The length of stories in our dataset is a challenge for RNNs, which process tokens sequentially. To transform prompts into stories, we instead build on the convolutional seq2seq model of Gehring et al. (2017), which uses deep convolutional networks as the encoder and decoder. Convolutional models are ideally suited to modeling long sequences, because they allow parallelism of computation within the sequence. In the Conv seq2seq model, the encoder and decoder are connected with attention modules (Bahdanau et al., 2015) that perform a weighted sum of encoder outputs, using attention at each layer of the decoder.</p>
<h3>3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention</h3>
<p>CNNs can only model a bounded context window, preventing the modeling of long-range dependencies within the output story. To enable modeling of unbounded context, we supplement the decoder with a self-attention mechanism (Sukhbaatar et al., 2015; Vaswani et al., 2017),
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Multihead self-attention mechanism. The decoder layer depicted attends with itself to gate the input of the subsequent decoder layer.
which allows the model to refer to any previously generated words. The self-attention mechanism improves the model's ability to extract long-range context with limited computational impact due to parallelism.</p>
<p>Gated Attention: Similar to Vaswani et al. (2017), we use multi-head attention to allow each head to attend to information at different positions. However, the queries, keys and values are not given by linear projections but by more expressive gated deep neural nets with Gated Linear Unit (Dauphin et al., 2017) activations. We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections.</p>
<p>Multi-Scale Attention: Further, we propose to have each head operating at a different time scale, depicted in Figure 2. Thus the input to each head is downsampled a different amount-the first head sees the full input, the second every other input timestep, the third every third input timestep, etc. The different scales encourage the heads to attend to different information. The downsampling operation limits the number of tokens in the attention maps, making them sharper.</p>
<p>The output of a single attention head is given by</p>
<p>$$
\begin{aligned}
h_{0: t}^{L+1}=\text { Linear } \left(\right. &amp; \left.v\left(h_{0: t-1}^{L}\right)\right. \
&amp; \left.\odot \operatorname{softmax}\left(q\left(h_{0: t}^{L}\right) k\left(h_{0: t}^{L}\right)^{\top}\right)\right)
\end{aligned}
$$</p>
<p>where $h_{0: t}^{L}$ contains the hidden states up to time $t$</p>
<p>at layer $L$, and $q, k, v$ are gated downsampling networks as shown in Figure 2. Unlike Vaswani et al. (2017), we allow the model to optionally attend to a 0 vector at each timestep, if it chooses to ignore the information of past timesteps (see Figure 3). This mechanism allows the model to recover the non-self-attention architecture and avoid attending to the past if it provides only noise. Additionally, we do not allow the self-attention mechanism to attend to the current timestep, only the past.</p>
<h3>3.4 Improving Relevance to Input Prompt with Model Fusion</h3>
<p>Unlike tasks such as translation, where the semantics of the target are fully specified by the source, the generation of stories from prompts is far more open-ended. We find that seq2seq models ignore the prompt and focus solely on modeling the stories, because the local dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story.</p>
<p>We propose a fusion-based approach to encourage conditioning on the prompt. We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model. Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn-such as conditioning on the prompt. To our knowledge, this paper is the first to show that fusion reduces the problem of seq2seq models degenerating into language models that capture primarily syntactic and grammatical information.</p>
<p>The cold fusion mechanism of Sriram et al. (2017) pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure 4):</p>
<p>$$
\begin{aligned}
&amp; g_{t}=\sigma\left(W\left[h_{t}^{\text {Training }} ; h_{t}^{\text {Pretrained }}\right]+b\right) \
&amp; h_{t}=g_{t} \circ\left[h_{t}^{\text {Training }} ; h_{t}^{\text {Pretrained }}\right]
\end{aligned}
$$</p>
<p>where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by $h_{t}$ ) are concatenated to learn gates $g_{t}$. The gates are computed using a linear projection with the weight matrix $W$. The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Diagram of our fusion model, which learns a second seq2seq model to improve a pretrained model. The separate hidden states are combined after gating through concatenation.</p>
<p>Appendix). We use layer normalization (Ba et al., 2016) after each fully connected layer.</p>
<h2>4 Related Work</h2>
<h3>4.1 Story Generation</h3>
<p>Sequence-to-sequence neural networks (Sutskever et al., 2014) have achieved state of the art performance on a variety of text generation tasks, such as machine translation (Sutskever et al., 2014) and summarization (Rush et al., 2015). Recent work has applied these models to more open-ended generation tasks, including writing Wikipedia articles (Liu et al., 2018) and poetry (Zhang and Lapata, 2014).</p>
<p>Previous work on story generation has explored seq2seq RNN architectures (Roemmele, 2016), but has focused largely on using various content to inspire the stories. For instance, Kiros et al. (2015) uses photos to inspire short paragraphs trained on romance novels, and Jain et al. (2017) chain a series of independent descriptions together into a short story. Martin et al. (2017) decompose story generation into two steps, first converting text into event representations, then modeling stories as sequences of events before translating back to natural language. Similarly, Harrison et al. (2017) generate summaries of movies as sequences of events using an RNN, then sample event representations using MCMC. They find this technique can generate text of the desired genre, but the movie plots</p>
<p>are not interpretable (as the model outputs events, not raw text). However, we are not aware of previous work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories.</p>
<h3>4.2 Hierarchical Text Generation</h3>
<p>Previous work has proposed decomposing the challenge of generating long sequences of text into a hierarchical generation task. For instance, Li et al. (2015b) use an LSTM to hierarchically learn word, then sentence, then paragraph embeddings, then transform the paragraph embeddings into text. Yarats and Lewis (2017) generate a discrete latent variable based on the context, then generates text conditioned upon it.</p>
<h3>4.3 Fusion Models</h3>
<p>Previous work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without architectural modifications: Ramachandran et al. (2016) use language models to initialize the encoder and decoder side of the seq2seq model independently, and Chorowski and Jaitly (2016) combine the predictions of the language model and seq2seq model solely at inference time. Recent work has also proposed deeper integration. Gulcehre et al. (2015) combined a trained language model with a trained seq2seq model to learn a gating function that joins them. Sriram et al. (2017) propose training the seq2seq model given the fixed language model then learning a gate to filter the information from the language model.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Baselines</h3>
<p>We evaluate a number of baselines:
(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of Dauphin et al. (2017) and our additional self-attention mechanism.
(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.
(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.
(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Valid <br> Perplexity</th>
<th style="text-align: left;">Test <br> Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Conv seq2seq</td>
<td style="text-align: left;">45.27</td>
<td style="text-align: left;">45.54</td>
</tr>
<tr>
<td style="text-align: left;">+ self-attention</td>
<td style="text-align: left;">42.01</td>
<td style="text-align: left;">42.32</td>
</tr>
<tr>
<td style="text-align: left;">+ multihead</td>
<td style="text-align: left;">40.12</td>
<td style="text-align: left;">40.39</td>
</tr>
<tr>
<td style="text-align: left;">+ multiscale</td>
<td style="text-align: left;">38.76</td>
<td style="text-align: left;">38.91</td>
</tr>
<tr>
<td style="text-align: left;">+ gating</td>
<td style="text-align: left;">$\mathbf{3 7 . 3 7}$</td>
<td style="text-align: left;">$\mathbf{3 7 . 9 4}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Effect of new attention mechanism. Gated multi-scale attention significantly improves the perplexity on the WritingPrompts dataset.
each prompt was created using FASTTEXT (Bojanowski et al., 2016) and FAISS (Johnson et al., 2017) was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories.</p>
<h3>5.2 Fusion Training</h3>
<p>To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WritINGPrompts dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time. The two models are integrated with the fusion mechanism described in Section 3.4.</p>
<h3>5.3 Training</h3>
<p>We implement models with the fairseq-py library in PyTorch. Similar to Gehring et al. (2017), we train using the Nesterov accelerated gradient method (Sutskever et al., 2013) using gradient clipping (Pascanu et al., 2013). We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix.</p>
<h3>5.4 Generation</h3>
<p>We generate stories from our models using a top-k random sampling scheme. At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the $k=10$ most likely candidates from this distribution. Then, subsequent timesteps generate words based on the previously selected words. We find this sampling strategy substantially more effective than beam search, which tends to produce common phrases and repetitive text from the training set (Vijayakumar et al., 2016; Shao et al., 2017). Sentences pro-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"># Parameters (mil)</th>
<th style="text-align: center;">Valid Perplexity</th>
<th style="text-align: center;">Test Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GCNN LM</td>
<td style="text-align: center;">123.4</td>
<td style="text-align: center;">54.50</td>
<td style="text-align: center;">54.79</td>
</tr>
<tr>
<td style="text-align: left;">GCNN + self-attention LM</td>
<td style="text-align: center;">126.4</td>
<td style="text-align: center;">51.84</td>
<td style="text-align: center;">51.18</td>
</tr>
<tr>
<td style="text-align: left;">LSTM seq2seq</td>
<td style="text-align: center;">110.3</td>
<td style="text-align: center;">46.83</td>
<td style="text-align: center;">46.79</td>
</tr>
<tr>
<td style="text-align: left;">Conv seq2seq</td>
<td style="text-align: center;">113.0</td>
<td style="text-align: center;">45.27</td>
<td style="text-align: center;">45.54</td>
</tr>
<tr>
<td style="text-align: left;">Conv seq2seq + self-attention</td>
<td style="text-align: center;">134.7</td>
<td style="text-align: center;">37.37</td>
<td style="text-align: center;">37.94</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble: Conv seq2seq + self-attention</td>
<td style="text-align: center;">270.3</td>
<td style="text-align: center;">36.63</td>
<td style="text-align: center;">36.93</td>
</tr>
<tr>
<td style="text-align: left;">Fusion: Conv seq2seq + self-attention</td>
<td style="text-align: center;">255.4</td>
<td style="text-align: center;">$\mathbf{3 6 . 0 8}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 5 6}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Perplexity on WritingPrompts. We dramatically improve over standard seq2seq models.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Human accuracy at pairing stories with the prompts used to generate them. People find that our fusion model significantly improves the link between the prompt and generated stories.
duced by beam search tend to be short and generic. Completely random sampling can introduce very unlikely words, which can damage generation as the model has not seen such mistakes at training time. The restriction of sampling from the 10 most likely candidates reduces the risk of these lowprobability samples.</p>
<p>For each model, we tune a temperature parameter for the softmax at generation time. To ease human evaluation, we generate stories of 150 words and do not generate unknown word tokens.</p>
<p>For prompt generation, we use a selfattentive GCNN language model trained with the same prompt-side vocabulary as the sequence-tosequence story generation models. The language model to generate prompts has a validation perplexity of 63.06 . Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is completed when the language model generates the end of prompt token.</p>
<h3>5.5 Evaluation</h3>
<p>We propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such as BLEU for ma-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Accuracy of prompt ranking. The fusion model most accurately pairs prompt and stories.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Accuracy on the prompt/story pairing task vs. number of generated stories. Our generative fusion model can produce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Human <br> Preference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Language model</td>
<td style="text-align: center;">$32.68 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Hierarchical Model</td>
<td style="text-align: center;">$\mathbf{6 7 . 3 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Effect of Hierarchical Generation. Human judges prefer stories that were generated hierarchically by first creating a premise and creating a full story based on it with a seq2seq model.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Average weighting of each model in our Fusion model for the beginning of the generated story for the prompt Gates of Hell. The fused model (orange) is primarily used for words which are closely related to the prompt, whereas generic words are generated by the pre-trained model (green).
chine translation or ROUGE for summarization, compute an n-gram overlap between the generated text and the human text-however, in our openended generation setting, these are not useful. We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt.</p>
<p>For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts- 9 randomly sampled prompts and 1 true corresponding prompt-and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model.</p>
<p>For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on heldout prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.</p>
<p>Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.</p>
<h2>6 Results</h2>
<p>We analyze the effect of our modeling improvements on the WRITINGPROMPTS dataset.</p>
<p>Effect of Hierarchical Generation: We explore leveraging our dataset to perform hierarchical story generation by first using a self-attentive GCNN language model to generate a prompt, and then using a fusion model to write a story given the generated prompt. We evaluate the effect of hierarchical generation using a human study in Table 4. 400 stories were generated from a selfattentive GCNN language model, and another 400 were generated from our hierarchical fusion model given generated prompts from a language model. In a blind comparison where raters were asked to choose the story they preferred reading, human raters preferred the hierarchical model $67 \%$ of the time.</p>
<p>Effect of new attention mechanism: Table 2 shows the effect of the proposed additions to the self-attention mechanism proposed by Vaswani et al. (2017). Table 3 shows that deep multi-scale self-attention and fusion each significantly improve the perplexity compared to the baselines. In combination these additions to the Conv seq2seq baseline reduce the perplexity by 9 points.</p>
<p>Effect of model fusion: Results in Table 3 show that adding our fusion mechanism substantially improves the likelihood of human-generated stories, and even outperforms an ensemble despite having fewer parameters. We observe in Figure 5 that fusion has a much more significant impact on the topicality of the stories. In comparison, ensembling has no effect on people's ability to associate stories with a prompt, but adding model fusion leads improves the pairing accuracy of the human judges by $7 \%$. These results suggest that by training a second model on top of the first, we have encouraged that model to learn the challeng-</p>
<p>ing additional dependencies to relate to the source sequence. To our knowledge, these are the first results to show that fusion has such capabilities.</p>
<p>Comparison with Nearest Neighbours: Nearest Neighbour Search (KNN) provides a strong baseline for text generation. Figure 5 shows that the fusion model can match the performance of nearest neighbour search in terms of the connection between the story and prompt. The real value in our generative approach is that it can produce an unlimited number of stories, whereas KNN can never generalize from its training data. To quantify this improvement, Figure 7 plots the relevance of the $k$ th best story to a given prompt; the performance of KNN degrades much more rapidly.</p>
<h2>7 Discussion</h2>
<h3>7.1 Generation Quality</h3>
<p>Our proposed fusion model is capable of generating unique text without copying directly from the training set. When analyzing 500 150-word generated stories from test-set prompts, the average longest common subsequence is 8.9. In contrast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set.</p>
<p>Figure 8 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe.</p>
<p>However, the fusion model has limitations. Using random sampling to generate can produce errors. For example, can't is tokenized to ca $n^{\prime} t$, and the model occasionally produces the first token but misses the second. A similar error is after one line of dialogue, the model may move to another line of dialogue without generating a newline token. A further obstacle is repetition. The model focuses frequently on what it has recently produced, which leads to the generation of similar text multiple times.</p>
<p>In the generation of prompts using the GCNN language model, we find that prompts are fairly generic compared to human prompts. Language models often struggle to model rare words accurately, as the probability distribution over the next word is dominated by more common words. This tends to produce similar prompts, particularly at
the start - we see many prompts that start with the man. In contrast, many of the human prompts are very unique (e.g. prompting stories in fantasy worlds such as Harry Potter and Game of Thrones) and the language model rarely produces the specific vocabulary required by these settings.</p>
<h3>7.2 Use of Attention</h3>
<p>We analyze the encoder-decoder attention in the fusion model and find that unlike attention maps in machine translation, where each decoder timestep tends to attend to a different word on the encoderside, the attention map for each decoder timestep looks similar and focuses mainly on salient words in the prompt. We further look at the usage of the self-attention layers within the decoder. While they could be leveraged to look at words generated very far in the past, at many timesteps the selfattention focuses on the recent past.</p>
<h2>8 Conclusion</h2>
<p>We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise. Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.</p>
<h2>References</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representation (ICLR).</p>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606.</p>
<p>Jan Chorowski and Navdeep Jaitly. 2016. Towards better decoding and language model integration in sequence to sequence models. arXiv preprint arXiv:1612.02695.</p>
<p>Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks.</p>
<h1>Generation with Fusion of Conv seq2seq + Self-Attention</h1>
<p>Example Prompt 1: Aliens start abducting humans
It has been two weeks, and the last of my kind has gone. It is only a matter of time until there will be nothing left. I'm not sure what the hell is going on... I can't think. I can hear a distant scream. I think of a strange, alien sound. I try to speak, but am interrupted by something, something that feels like a drum, I ca not tell. I mean I'm just a little bit older than an average human. But I can, and I can feel the vibrations. I hear the sound of a ship approaching. The ground quakes at the force of the impact, and a loud explosion shatters the silence.</p>
<p>Example Prompt 2: The scientists have discovered something terrible .
The scientist stood there, a little dazed as he stared.
"What is it?" He asked.
"This...this...Thing...This is a virus. A chemical that can destroy entire planet and it is a very small, complex, chemical that could destroy any planet." The scientist replied. His lab assistant looked down at the tablet.
"I've just discovered it. I can't believe it. It looks like it's made of some sort of chemical that's very dangerous."
" Well, there's a virus on the ground. It's very effective..." "I can't believe what it is." he said, pointing to the scientist .
" We don't know what this thing is. We haven't seen anything like it . We can't even see anything like this. " Dr. Jones stared at the scientist for a moment.
"What do you mean what does it do ?"
"It...It 's a monster."
Example Prompt 3: The silence before the storm comes
"I'm sorry," she said "I can't do this anymore."
She sat in the grass, the grass blowing behind her as she sat staring at the sky. Her eyes were still closed but she was still staring at it. Her hand was still shaking, her eyes closed as she watched the rain roll in and out of view. She could see the world through her eyelids now. Her hands were still shaking like she hadn't eaten in days. Her fingers were still clenched tight and her fingers were now shaking. She wasn't even sure if she was going to cry or if she just wanted to cry. It wasn't until she was close enough to hear the words.
"I ...I just...I need to...I ..." She was crying now and then, the rain was picking up. Her lips were shaking now and she could see that the storm was getting closer and closer. Her heart stopped when she heard the thunder. She knew it was coming. The storm was</p>
<h2>Generation with Language Model + Self-Attention Baseline</h2>
<h2>Example 1:</h2>
<p>I've always been the curious type. Growing up in the ghetto, I've been around for more than a year now. I still haven't heard it on my own. I'm not sure if I'm a good person. But I'm not the only one. I've been to the hospital, but it's not my first time. I've been to my psychiatrist's journals before. "Hey, are you ... are you okay?" I asked. "Yes" I said. "What's wrong?" she replied, "I was just ... Doing something." She said, "I've never been a bad person."</p>
<h2>Example 2:</h2>
<p>The man was an accountant. He had to be. He had to be the next president. I looked back over the top and saw that his wife was crying in the kitchen. I looked at the clock. It seemed to be coming slower, but I knew if I did it would not be long before I was in my own home. I wasn't sure. I had a hard time finding the right words to say. I was about to leave when he suddenly became angry and began talking to me. "Hello, sir, I'm John. What is your name?" "My name is Manuel and I'm a journalist." I said</p>
<p>Table 5: Example stories generated by the proposed hierarchical fusion approach compared to stories generated by a language model. Stories generated by the fusion model relate to the desired prompt and show increased coherence between sentences and ability to stay on one topic compared to the language modeling baseline.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning.</p>
<p>Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535.</p>
<p>Brent Harrison, Christopher Purdy, and Mark O Riedl. 2017. Toward automated story generation with markov chain monte carlo methods and deep neural networks.</p>
<p>Parag Jain, Priyanka Agrawal, Abhijit Mishra, Mohak Sukhwani, Anirban Laha, and Karthik Sankaranarayanan. 2017. Story generation from sequence of independent short descriptions. arXiv preprint arXiv:1707.05501.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734.</p>
<p>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. arXiv preprint arXiv:1506.06726.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015a. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055.</p>
<p>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015b. A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057.</p>
<p>Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198.</p>
<p>Lara J Martin, Prithviraj Ammanabrolu, William Hancock, Shruti Singh, Brent Harrison, and Mark O Riedl. 2017. Event representations for automated story generation with deep neural nets. arXiv preprint arXiv:1706.01331.</p>
<p>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In ICML.</p>
<p>Prajit Ramachandran, Peter J Liu, and Quoc V Le. 2016. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683.</p>
<p>Melissa Roemmele. 2016. Writing stories with help from recurrent neural networks. In $A A A I$.</p>
<p>Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685.</p>
<p>Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. Generating long and diverse responses with neural conversation models. arXiv preprint arXiv:1701.03185.</p>
<p>Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. 2017. Cold fusion: Training seq2seq models together with language models. arXiv preprint arXiv:1708.06426.</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages $2440-2448$.</p>
<p>Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. 2013. On the importance of initialization and momentum in deep learning. In ICML.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Neural Information Processing Systems (NIPS).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010.</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. arXiv preprint arXiv:1707.08052.</p>
<p>Denis Yarats and Mike Lewis. 2017. Hierarchical text generation and planning for strategic dialogue. arXiv preprint arXiv:1712.05846.</p>
<p>Xingxing Zhang and Mirella Lapata. 2014. Chinese poetry generation with recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670-680.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ www.github.com/pytorch/fairseq
${ }^{2}$ www.reddit.com/r/WritingPrompts/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
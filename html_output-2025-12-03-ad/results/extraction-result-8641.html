<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8641 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8641</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8641</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273707374</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.23405v1.pdf" target="_blank">FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions</a></p>
                <p><strong>Paper Abstract:</strong> Material discovery is a critical area of research with the potential to revolutionize various fields, including carbon capture, renewable energy, and electronics. However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally. In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials. FlowLLM first fine-tunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation. After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters. Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and increasing the rate for stable, unique, and novel crystals by $\sim50\%$ - a huge improvement on a difficult problem. Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, significantly reducing post-hoc computational cost.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8641.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8641.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FlowLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid generative pipeline that first samples a fine-tuned autoregressive LLM (string representation of crystals) to produce an initial discrete+coarse continuous crystal representation, then refines continuous degrees of freedom (fractional coordinates and lattice parameters) with a Riemannian Flow Matching (RFM) model to produce high-quality crystalline materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (fine-tuned) as p_LLM; Riemannian Flow Matching (RFM) as p_RFM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (LLM) used as learned base distribution; RFM = continuous normalizing flow on Riemannian manifold (GNN parameterized vector field)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-2 70B (fine-tuned with LoRA); RFM: GNN with Hidden Dim 512</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on MP-20 (45,231 inorganic metastable crystal structures) represented as text strings (atom types, fractional coordinates, lattice lengths/angles) during LLM fine-tuning; RFM trained with pairs (LLM sample, ground-truth structure) sampled from MP-20.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>materials discovery / crystalline inorganic materials generation (battery materials, photovoltaics, carbon-capture-relevant materials, general inorganic crystals).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted next-token sampling from the fine-tuned LLM (optionally conditioned on chemical formula or other prompt fields) to produce a text-encoded crystal; convert to graph/continuous representation; iterative refinement of continuous coordinates and lattice params via Riemannian Flow Matching integration (solve ODE d/dt c_t = v_t(c_t)).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>High novelty among stable samples: for best FlowLLM run (τ=0.7,P=0.9) 17.82% of generated structures are stable and among those 48% are novel (not similar to training/validation set by StructureMatcher), yielding a S.U.N. (stable, unique, novel) rate of 4.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditional generation via natural-language-style prompts (e.g., conditioning on chemical formula or property text) is supported by the LLM; RFM is trained conditioned on chemical formula to refine continuous geometry. In this work, conditioning used matched Gruver et al. (chemical formula conditioning), and unconditional sampling was used for main evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Thermodynamic Stability (Energy above hull E_hull via CHGNet pre-relaxation + DFT relaxation), S.U.N. rate (Stable, Unique, Novel via StructureMatcher), proxy metrics (structural validity, compositional validity, coverage/recall/precision, Wasserstein distances of property distributions), closeness-to-ground-state metrics (Match Rate, RMSD, Δ-Energy, number of pre-relax steps).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FlowLLM substantially outperforms prior methods on stability and S.U.N. rates: best config achieved 17.82% Stability Rate and 4.92% S.U.N. rate (∼3× improvement in stability rate and ∼50% improvement in S.U.N. rate vs best prior model). Generated structures are also much closer to their relaxed ground states (higher Match Rate and lower RMSD and Δ-energy) compared to FlowMM baseline. Empirically low invalid-sample rejection (~0.5% at τ=0.7). Converges with fewer RFM integration steps (convergence observed ~50 steps) relative to diffusion methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to prior state-of-the-art: FlowLLM outperforms FlowMM (RFM-only), diffusion-based models (DiffCSP, CD-VAE), and CrystalLLM (LLM-only) on Stability and S.U.N. metrics while retaining the LLM's prompting flexibility. FlowLLM trades off some proxy metrics (coverage/recall) relative to diffusion/flow methods but attains much higher practical stability. Ablation FlowLLM-Types (LLM only for atom types) still exceeds prior models on stability, showing LLM's benefit for composition but full learned LLM base distribution yields much higher gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>LLM base distribution is discrete/finite-precision (fractional coords represented with 2 decimal places, lattice lengths 1 decimal place), so continuous values are coarse—mitigated by adding small Gaussian noise and RFM refinement. LLM-generated base distribution lacks exact translation and token-permutation invariance (though approximately invariant empirically), and the method is not end-to-end differentiable, limiting direct application to inverse design/property-conditional optimization. Small rate of invalid text-to-crystal samples (≈0.5%) handled by rejection sampling. Computational costs: LLM sampling and large-scale DFT evaluations are expensive (sampling required ~250 A100 GPU days; DFT evaluations large CPU cluster).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8641.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8641.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FlowLLM-Types</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FlowLLM-Types (ablation: LLM supplying only atom types, RFM base distribution for continuous values)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of FlowLLM where the LLM is used only to predict discrete atom types while lattice parameters and fractional coordinates are sampled from simple base distributions used in FlowMM; RFM then refines geometry conditioned on atom types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (fine-tuned) for atom-type generation; RFM for geometry</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (LLM) for discrete token generation; RFM continuous flow for refinement</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-2 70B (fine-tuned with LoRA); RFM GNN similar to FlowLLM</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>LLM fine-tuned on MP-20 text representations (to predict atom types among other tokens); RFM trained with base distributions for continuous parts matching FlowMM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>inorganic crystalline materials generation (materials discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM generates atom-type sequence conditioned on formula; continuous variables sampled from simple base distributions; RFM trained to transport from that base to the data distribution conditioned on atom types.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Improves over prior models on Stability Rate but produces fewer stable/novel materials than full FlowLLM (i.e., learned LLM base distribution for continuous values provides additional benefit beyond atom-type prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning via chemical formula as in FlowLLM; this ablation focuses LLM capacity on composition rather than continuous geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as FlowLLM (Stability Rate, S.U.N. rate, proxy metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FlowLLM-Types surpasses prior models on Stability Rate, demonstrating LLM's strong value for atom-type prediction; however, its stability rate remains considerably lower than full FlowLLM, showing importance of the LLM learning a full base distribution (discrete+coarse continuous).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms prior baselines (FlowMM, diffusion models) on stability when LLM is used for atom types only, but underperforms full FlowLLM which uses LLM as learned base distribution for both discrete and coarse continuous values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>By design it does not leverage LLM's learned distribution for continuous variables, so gains are limited compared to full FlowLLM; still subject to LLM finite-precision discretization for atom-type tokens and same non-differentiability limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8641.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8641.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CrystalLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text (CrystalLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that fine-tunes an autoregressive LLaMA-2 model to output string-encoded crystal structures (element names, lattice parameters, fractional coordinates) directly via next-token prediction, enabling natural-language-style prompting for crystal generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (fine-tuned) referred to as CrystalLLM in related work and used as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters (as used for comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on crystallographic dataset (MP-20) represented as strings encoding atom types, fractional coordinates, lattice parameters; extensive pretraining of base LLaMA-2 on large text corpora presumed.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>materials discovery / crystalline inorganic materials generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct text generation of crystal representations via prompt-conditioned next-token sampling (nucleus sampling, temperature), producing discrete tokens and finite-precision numerics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>CrystalLLM (70B) achieves non-trivial stability (reported in table as a baseline) but is outperformed by the FlowLLM hybrid on Stability and S.U.N. rates; CrystalLLM had strong composition validity but lower stability relative to FlowLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Supports conditioning (e.g., chemical formula or property prompts) because of natural language prompting; in prior work and used here, conditioning on chemical formula was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same materials metrics: Stability Rate (E_hull), proxy metrics (composition validity, coverage/recall), and S.U.N. rates when DFT relaxation applied.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CrystalLLM (70B) is a strong LLM-only baseline (good composition validity), but FlowLLM (hybrid) achieves much higher stability and S.U.N. rates by refining LLM outputs with RFM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to diffusion and flow-based generative models CrystalLLM trades off continuous geometry precision for better composition validity; FlowLLM combines strengths of CrystalLLM (discrete composition, prompting) with RFM continuous refinement to surpass it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Finite precision in textual encoding of continuous degrees (coarse fractional coordinates and lattice numbers) results in outputs that can be suboptimal geometrically and require post-hoc relaxation; lacks continuous equivariance guarantees; not end-to-end differentiable for property-driven inverse design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8641.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8641.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (used here)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2: Open Foundation and Fine-Tuned Chat Models (fine-tuned here for crystal text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of large autoregressive transformer language models used here as the base LLM (70B) which is fine-tuned to output string-encoded crystal structures; tokenization chosen to break numbers into digits to help numeric precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open Foundation and Fine-Tuned Chat Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 70B (fine-tuned with LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large natural-language corpora (external to this paper); fine-tuned on MP-20 text representations of inorganic crystals for 10 epochs (batch size 16) using LoRA (rank 8, α=32).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>text-conditioned generation of inorganic crystal structures (materials science).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted next-token sampling with nucleus sampling and temperature to produce textual crystal encodings (atom types, fractional coordinates with 2-decimal precision, lengths with 1-decimal, angles as integers); outputs converted to structured crystal representations for downstream refinement by RFM.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>As part of FlowLLM, LLaMA-2 fine-tuned outputs serve as a learned base distribution resulting in high novelty among stable outputs after RFM refinement (see FlowLLM metrics: 48% novelty among stable samples).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Supports inclusion of conditioning tokens (chemical formula, desired properties) in prompt; in this work conditioning mostly on chemical formula per Gruver et al. and some unconditional sampling for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used indirectly: quality of LLM samples measured via downstream Stability Rate, S.U.N. rate, rejection rate for invalid crystals (~0.5%), and effect on RFM training/convergence (fewer integration steps required when using learned base).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned LLaMA-2 70B provides a learned base distribution that substantially improves downstream generation when combined with RFM (FlowLLM). Training details: 10 epochs, LR cosine anneal to 0.0005, LoRA, trained on 8x A100 GPUs ~1 day; sampling used nucleus sampling (P) and temperature (τ).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Using LLaMA-2 as a learned base distribution yields better initialization for RFM than uniform/simple base distributions used by FlowMM, resulting in faster convergence and higher stability and S.U.N. rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Finite-precision textual representation imposes a discrete, low-support base distribution for continuous variables; lacks token permutation and translation invariance (though empirically approximately invariant); generation of out-of-support continuous values requires adding small Gaussian noise; not end-to-end differentiable for inverse design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text <em>(Rating: 2)</em></li>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files <em>(Rating: 2)</em></li>
                <li>FlowMM: Generating Materials with Riemannian Flow Matching <em>(Rating: 2)</em></li>
                <li>Riemannian flow matching on general geometries <em>(Rating: 1)</em></li>
                <li>Flow Matching for Generative Modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8641",
    "paper_id": "paper-273707374",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "FlowLLM",
            "name_full": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
            "brief_description": "A hybrid generative pipeline that first samples a fine-tuned autoregressive LLM (string representation of crystals) to produce an initial discrete+coarse continuous crystal representation, then refines continuous degrees of freedom (fractional coordinates and lattice parameters) with a Riemannian Flow Matching (RFM) model to produce high-quality crystalline materials.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (fine-tuned) as p_LLM; Riemannian Flow Matching (RFM) as p_RFM",
            "model_type": "autoregressive transformer (LLM) used as learned base distribution; RFM = continuous normalizing flow on Riemannian manifold (GNN parameterized vector field)",
            "model_size": "LLaMA-2 70B (fine-tuned with LoRA); RFM: GNN with Hidden Dim 512",
            "training_data": "Fine-tuned on MP-20 (45,231 inorganic metastable crystal structures) represented as text strings (atom types, fractional coordinates, lattice lengths/angles) during LLM fine-tuning; RFM trained with pairs (LLM sample, ground-truth structure) sampled from MP-20.",
            "application_domain": "materials discovery / crystalline inorganic materials generation (battery materials, photovoltaics, carbon-capture-relevant materials, general inorganic crystals).",
            "generation_method": "Prompted next-token sampling from the fine-tuned LLM (optionally conditioned on chemical formula or other prompt fields) to produce a text-encoded crystal; convert to graph/continuous representation; iterative refinement of continuous coordinates and lattice params via Riemannian Flow Matching integration (solve ODE d/dt c_t = v_t(c_t)).",
            "novelty_of_chemicals": "High novelty among stable samples: for best FlowLLM run (τ=0.7,P=0.9) 17.82% of generated structures are stable and among those 48% are novel (not similar to training/validation set by StructureMatcher), yielding a S.U.N. (stable, unique, novel) rate of 4.92%.",
            "application_specificity": "Conditional generation via natural-language-style prompts (e.g., conditioning on chemical formula or property text) is supported by the LLM; RFM is trained conditioned on chemical formula to refine continuous geometry. In this work, conditioning used matched Gruver et al. (chemical formula conditioning), and unconditional sampling was used for main evaluations.",
            "evaluation_metrics": "Thermodynamic Stability (Energy above hull E_hull via CHGNet pre-relaxation + DFT relaxation), S.U.N. rate (Stable, Unique, Novel via StructureMatcher), proxy metrics (structural validity, compositional validity, coverage/recall/precision, Wasserstein distances of property distributions), closeness-to-ground-state metrics (Match Rate, RMSD, Δ-Energy, number of pre-relax steps).",
            "results_summary": "FlowLLM substantially outperforms prior methods on stability and S.U.N. rates: best config achieved 17.82% Stability Rate and 4.92% S.U.N. rate (∼3× improvement in stability rate and ∼50% improvement in S.U.N. rate vs best prior model). Generated structures are also much closer to their relaxed ground states (higher Match Rate and lower RMSD and Δ-energy) compared to FlowMM baseline. Empirically low invalid-sample rejection (~0.5% at τ=0.7). Converges with fewer RFM integration steps (convergence observed ~50 steps) relative to diffusion methods.",
            "comparison_to_other_methods": "Compared to prior state-of-the-art: FlowLLM outperforms FlowMM (RFM-only), diffusion-based models (DiffCSP, CD-VAE), and CrystalLLM (LLM-only) on Stability and S.U.N. metrics while retaining the LLM's prompting flexibility. FlowLLM trades off some proxy metrics (coverage/recall) relative to diffusion/flow methods but attains much higher practical stability. Ablation FlowLLM-Types (LLM only for atom types) still exceeds prior models on stability, showing LLM's benefit for composition but full learned LLM base distribution yields much higher gains.",
            "limitations_and_challenges": "LLM base distribution is discrete/finite-precision (fractional coords represented with 2 decimal places, lattice lengths 1 decimal place), so continuous values are coarse—mitigated by adding small Gaussian noise and RFM refinement. LLM-generated base distribution lacks exact translation and token-permutation invariance (though approximately invariant empirically), and the method is not end-to-end differentiable, limiting direct application to inverse design/property-conditional optimization. Small rate of invalid text-to-crystal samples (≈0.5%) handled by rejection sampling. Computational costs: LLM sampling and large-scale DFT evaluations are expensive (sampling required ~250 A100 GPU days; DFT evaluations large CPU cluster).",
            "uuid": "e8641.0",
            "source_info": {
                "paper_title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FlowLLM-Types",
            "name_full": "FlowLLM-Types (ablation: LLM supplying only atom types, RFM base distribution for continuous values)",
            "brief_description": "An ablation of FlowLLM where the LLM is used only to predict discrete atom types while lattice parameters and fractional coordinates are sampled from simple base distributions used in FlowMM; RFM then refines geometry conditioned on atom types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (fine-tuned) for atom-type generation; RFM for geometry",
            "model_type": "autoregressive transformer (LLM) for discrete token generation; RFM continuous flow for refinement",
            "model_size": "LLaMA-2 70B (fine-tuned with LoRA); RFM GNN similar to FlowLLM",
            "training_data": "LLM fine-tuned on MP-20 text representations (to predict atom types among other tokens); RFM trained with base distributions for continuous parts matching FlowMM.",
            "application_domain": "inorganic crystalline materials generation (materials discovery).",
            "generation_method": "LLM generates atom-type sequence conditioned on formula; continuous variables sampled from simple base distributions; RFM trained to transport from that base to the data distribution conditioned on atom types.",
            "novelty_of_chemicals": "Improves over prior models on Stability Rate but produces fewer stable/novel materials than full FlowLLM (i.e., learned LLM base distribution for continuous values provides additional benefit beyond atom-type prediction).",
            "application_specificity": "Conditioning via chemical formula as in FlowLLM; this ablation focuses LLM capacity on composition rather than continuous geometry.",
            "evaluation_metrics": "Same metrics as FlowLLM (Stability Rate, S.U.N. rate, proxy metrics).",
            "results_summary": "FlowLLM-Types surpasses prior models on Stability Rate, demonstrating LLM's strong value for atom-type prediction; however, its stability rate remains considerably lower than full FlowLLM, showing importance of the LLM learning a full base distribution (discrete+coarse continuous).",
            "comparison_to_other_methods": "Outperforms prior baselines (FlowMM, diffusion models) on stability when LLM is used for atom types only, but underperforms full FlowLLM which uses LLM as learned base distribution for both discrete and coarse continuous values.",
            "limitations_and_challenges": "By design it does not leverage LLM's learned distribution for continuous variables, so gains are limited compared to full FlowLLM; still subject to LLM finite-precision discretization for atom-type tokens and same non-differentiability limitation.",
            "uuid": "e8641.1",
            "source_info": {
                "paper_title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CrystalLLM",
            "name_full": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text (CrystalLLM)",
            "brief_description": "An approach that fine-tunes an autoregressive LLaMA-2 model to output string-encoded crystal structures (element names, lattice parameters, fractional coordinates) directly via next-token prediction, enabling natural-language-style prompting for crystal generation.",
            "citation_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (fine-tuned) referred to as CrystalLLM in related work and used as a baseline",
            "model_type": "autoregressive transformer LLM",
            "model_size": "70B parameters (as used for comparison in this paper)",
            "training_data": "Fine-tuned on crystallographic dataset (MP-20) represented as strings encoding atom types, fractional coordinates, lattice parameters; extensive pretraining of base LLaMA-2 on large text corpora presumed.",
            "application_domain": "materials discovery / crystalline inorganic materials generation.",
            "generation_method": "Direct text generation of crystal representations via prompt-conditioned next-token sampling (nucleus sampling, temperature), producing discrete tokens and finite-precision numerics.",
            "novelty_of_chemicals": "CrystalLLM (70B) achieves non-trivial stability (reported in table as a baseline) but is outperformed by the FlowLLM hybrid on Stability and S.U.N. rates; CrystalLLM had strong composition validity but lower stability relative to FlowLLM.",
            "application_specificity": "Supports conditioning (e.g., chemical formula or property prompts) because of natural language prompting; in prior work and used here, conditioning on chemical formula was applied.",
            "evaluation_metrics": "Same materials metrics: Stability Rate (E_hull), proxy metrics (composition validity, coverage/recall), and S.U.N. rates when DFT relaxation applied.",
            "results_summary": "CrystalLLM (70B) is a strong LLM-only baseline (good composition validity), but FlowLLM (hybrid) achieves much higher stability and S.U.N. rates by refining LLM outputs with RFM.",
            "comparison_to_other_methods": "Compared to diffusion and flow-based generative models CrystalLLM trades off continuous geometry precision for better composition validity; FlowLLM combines strengths of CrystalLLM (discrete composition, prompting) with RFM continuous refinement to surpass it.",
            "limitations_and_challenges": "Finite precision in textual encoding of continuous degrees (coarse fractional coordinates and lattice numbers) results in outputs that can be suboptimal geometrically and require post-hoc relaxation; lacks continuous equivariance guarantees; not end-to-end differentiable for property-driven inverse design.",
            "uuid": "e8641.2",
            "source_info": {
                "paper_title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA-2 (used here)",
            "name_full": "LLaMA-2: Open Foundation and Fine-Tuned Chat Models (fine-tuned here for crystal text)",
            "brief_description": "A family of large autoregressive transformer language models used here as the base LLM (70B) which is fine-tuned to output string-encoded crystal structures; tokenization chosen to break numbers into digits to help numeric precision.",
            "citation_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 70B (fine-tuned with LoRA)",
            "model_type": "autoregressive transformer LLM",
            "model_size": "70B parameters",
            "training_data": "Pretrained on large natural-language corpora (external to this paper); fine-tuned on MP-20 text representations of inorganic crystals for 10 epochs (batch size 16) using LoRA (rank 8, α=32).",
            "application_domain": "text-conditioned generation of inorganic crystal structures (materials science).",
            "generation_method": "Prompted next-token sampling with nucleus sampling and temperature to produce textual crystal encodings (atom types, fractional coordinates with 2-decimal precision, lengths with 1-decimal, angles as integers); outputs converted to structured crystal representations for downstream refinement by RFM.",
            "novelty_of_chemicals": "As part of FlowLLM, LLaMA-2 fine-tuned outputs serve as a learned base distribution resulting in high novelty among stable outputs after RFM refinement (see FlowLLM metrics: 48% novelty among stable samples).",
            "application_specificity": "Supports inclusion of conditioning tokens (chemical formula, desired properties) in prompt; in this work conditioning mostly on chemical formula per Gruver et al. and some unconditional sampling for evaluation.",
            "evaluation_metrics": "Used indirectly: quality of LLM samples measured via downstream Stability Rate, S.U.N. rate, rejection rate for invalid crystals (~0.5%), and effect on RFM training/convergence (fewer integration steps required when using learned base).",
            "results_summary": "Fine-tuned LLaMA-2 70B provides a learned base distribution that substantially improves downstream generation when combined with RFM (FlowLLM). Training details: 10 epochs, LR cosine anneal to 0.0005, LoRA, trained on 8x A100 GPUs ~1 day; sampling used nucleus sampling (P) and temperature (τ).",
            "comparison_to_other_methods": "Using LLaMA-2 as a learned base distribution yields better initialization for RFM than uniform/simple base distributions used by FlowMM, resulting in faster convergence and higher stability and S.U.N. rates.",
            "limitations_and_challenges": "Finite-precision textual representation imposes a discrete, low-support base distribution for continuous variables; lacks token permutation and translation invariance (though empirically approximately invariant); generation of out-of-support continuous values requires adding small Gaussian noise; not end-to-end differentiable for inverse design.",
            "uuid": "e8641.3",
            "source_info": {
                "paper_title": "FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
            "rating": 2,
            "sanitized_title": "finetuned_language_models_generate_stable_inorganic_materials_as_text"
        },
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "FlowMM: Generating Materials with Riemannian Flow Matching",
            "rating": 2,
            "sanitized_title": "flowmm_generating_materials_with_riemannian_flow_matching"
        },
        {
            "paper_title": "Riemannian flow matching on general geometries",
            "rating": 1,
            "sanitized_title": "riemannian_flow_matching_on_general_geometries"
        },
        {
            "paper_title": "Flow Matching for Generative Modeling",
            "rating": 1,
            "sanitized_title": "flow_matching_for_generative_modeling"
        }
    ],
    "cost": 0.01367225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions
30 Oct 2024</p>
<p>Anuroop Sriram anuroops@meta.com 
Benjamin Kurt Miller b.k.miller@uva.nl 
Ricky T Q Chen rtqichen@meta.com 
Brandon M Wood Fair </p>
<p>FAIR
Meta</p>
<p>University of Amsterdam</p>
<p>FAIR
Meta</p>
<p>FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions
30 Oct 20241C6DC20DEDAF2F8DCE4F7EBB9EFA19FEarXiv:2410.23405v1[cs.LG]
Material discovery is a critical area of research with the potential to revolutionize various fields, including carbon capture, renewable energy, and electronics.However, the immense scale of the chemical space makes it challenging to explore all possible materials experimentally.In this paper, we introduce FlowLLM, a novel generative model that combines large language models (LLMs) and Riemannian flow matching (RFM) to design novel crystalline materials.FlowLLM first finetunes an LLM to learn an effective base distribution of meta-stable crystals in a text representation.After converting to a graph representation, the RFM model takes samples from the LLM and iteratively refines the coordinates and lattice parameters.Our approach significantly outperforms state-of-the-art methods, increasing the generation rate of stable materials by over three times and increasing the rate for stable, unique, and novel crystals by ∼ 50% -a huge improvement on a difficult problem.Additionally, the crystals generated by FlowLLM are much closer to their relaxed state when compared with another leading model, significantly reducing post-hoc computational cost.38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Introduction</p>
<p>Material discovery holds transformative potential across numerous industries including carbon capture [38], batteries [28], photovoltaics [9], and energy storage [1].However, the vastness of the chemical space has hindered experimental synthesis of the majority of possible materials.Generative models offer a promising avenue for exploring this untapped potential.</p>
<p>Generating crystalline materials is particularly challenging as it involves simultaneously generating both discrete (atomic types) and continuous values (atomic positions and lattice geometry).While existing approaches, namely autoregressive large language models (LLMs) [11,6] and denoising models, e.g., denoising diffusion and flow matching [47,16,49,48,30,26,17], have demonstrated success, they exhibit complementary strengths and weaknesses.LLMs excel at modeling discrete values, but they can struggle with continuous values due to their reliance on finite precision representations.Conversely, denoising models more effectively handle continuous values and can easily ensure equivariances, but they face challenges with discrete elements.</p>
<p>LLMs also offer the distinct advantage of natural language prompting, enabling versatile and intuitive conditional generation.This capability is further enhanced by training LLMs on vast corpora of chemistry text, equipping them with valuable prior knowledge to generate chemically valid outputs.Queries like "Generate materials with a high bandgap and thermal stability" or "Propose a novel perovskite structure for efficient solar energy conversion" can be directly integrated into the LLM Figure 1: FlowLLM generative process: the fine-tuned LLM is first prompted with an unconditional query to generate an initial material representation.This material is then iteratively transformed by the RFM model to update its atom positions and lattice parameters.The atom types are static in RFM.prompt, while denoising models typically require bespoke changes to the architecture and training procedure to handle conditional generation.</p>
<p>To harness the strengths of both paradigms, we introduce FlowLLM, a novel hybrid approach that uses an LLM to generate an initial material representation, which is iteratively refined with a Riemannian Flow Matching (RFM; [2]) model.This synergistic approach allows us to effectively bridge the gap between discrete and continuous modeling, resulting in a significant improvement in the rate of generation of stable, unique, and novel (S.U.N.) materials.Such materials expand the limited knowledge we have of "material space" and are much more likely to be synthesizable than unstable generations.Our experiments demonstrate that FlowLLM generates stable materials at over 300% higher rate, and S.U.N. materials at ∼ 50% higher rate compared to prior models, while retaining the LLM's ability to be prompted with natural language instructions.We offer two interpretations for the effectiveness of our approach.1) The LLM learns a good base distribution for RFM: the LLM's output distribution serves as a learned base distribution for RFM, replacing the common practice of using the uniform base distribution.Since the LLM has been trained on material data, this learned base distribution is closer to the target distribution, greatly simplifying integration with RFM. 2) RFM refines the output of the LLM: The LLM generates an approximate material representation due to its finite precision when handling continuous values.The RFM then refines this approximation through iterative denoising, to generate a much more accurate representation.</p>
<p>Our contributions are as follows:</p>
<p>• We introduce FlowLLM, a novel hybrid approach for materials generation that combines LLMs and RFM, effectively leveraging their complementary strengths.• We demonstrate that FlowLLM significantly outperforms existing state-of-the-art generative models in generating novel and stable materials.• We show through ablation experiments that our method of combining LLM and RFM models through FlowLLM significantly outperform simpler combination approaches.</p>
<p>Code for training the FlowLLM model is available at https://github.com/facebookresearch/flowmm.</p>
<p>Related Work</p>
<p>In the past, computational materials discovery relied on generating numerous candidate materials through random atomic substitutions in known materials [42], followed by computationally expensive quantum mechanical screening [19] to assess stability.Genetic algorithms [8,33], and machine learning models trained to predict energies [37,25] have accelerated this process, but the fundamental bottleneck of brute force search remains.</p>
<p>Recent research has focused on generative models that directly produce stable materials, bypassing brute-force search.Diffusion models, either combined with Variational Autoencoders (VAEs) for partial variable prediction [47] or jointly diffusing all variables [16,48,49] have shown promise.Additionally, Riemannian Flow Matching [26], Normalizing Flows [45], and Variational Autoencoders [34] have also been adapted for material generation.</p>
<p>A parallel line of work utilizes autoregressive Large Language Models (LLMs) for material generation [6,11], representing materials as a sequence of discretized tokens.Pretraining these models on natural language imbues them with powerful prior knowledge not attainable by other approaches.</p>
<p>Preliminaries</p>
<p>Our approach models probability distributions over crystal lattices, defined as periodic arrangements of atoms in three-dimensional space.A crystal lattice is created by tiling a fundamental unit cell, where the unit cell contains a specific atomic configuration, forming the entire lattice when repeated.</p>
<p>In this section, we present a high-level overview of crystal representations, building up to explain our model in section 4. Background details for the crystal representation are in appendix A.</p>
<p>Crystal representation</p>
<p>In the paper, we represent an n ∈ N atom crystal in a product space: c := (a, f , l) ∈ C, indicating the atom types, positions and unit cell geometry, respectively [47,26].The atom types are represented by a matrix of categorical vectors: a := a 1 , . . ., a n , where a i ∈ A. The atomic coordinates are represented using fractional coordinates within the unit cell, f := f 1 , . . ., f n , where f i ∈ F = T 3 with T denoting the unitary length, flat torus manifold, i.e., the fractional coordinates satisfy periodic boundary conditions; that is, the atoms "wrap around" the unit cell.The unit cell geometry is defined using lattice parameters l ∈ L, where L is the space formed by a 6-tuple of three side lengths (a, b, c) ∈ R + (Å, i.e.Angstrom) and three internal angles (α, β, γ) ∈ [60 • , 120 • ].This representation is not unique as the same crystal can be produced by different choices of unit cell.To make the representation unique, we select the minimum-volume unit cell and employ Niggli reduction [10] that uniquely determines the unit cell parameters.</p>
<p>Equivariance &amp; Invariance Given a group G with g• denoting a group action for some g ∈ G,
a function f : X → Y is called G-equivariant if ∀x ∈ X , ∀g ∈ G, f (g • x) = g • f (x), while it is called G-invariant if ∀x ∈ X , ∀g ∈ G, f (g • x) = f (x)
. Since a crystal is not uniquely defined by any particular representation c but an infinite set, we know that the data distribution has a G-invariant density, where G represents symmetries of a crystal.</p>
<p>Symmetries of crystals Concretely, our crystal representation exhibits multiple symmetries that we detail here.The symmetric group S n on n atoms permutes the atom indices: σ • c = a σ(1) , . . ., a σ(n) , f σ(1) , . . ., f σ(n) , l .The special Euclidean group SE(3) consists of orientation preserving rigid rotations and translations: (Q, T ) where Q ∈ SO(3) and T ∈ [− 1 2 , 1 2 ] 3×1 denote 3D rotations and translations respectively.This element transforms the crystal as: (Q,
T) • c = (a, f + τ 1 − ⌊f + τ 1⌋, l).
We emphasize that the representation c is completely invariant w.r.t.Q because lattice parameters do not contain orientation information.Since these represent symmetries fundamental to crystals, the data distribution q(c) is invariant to these group operations.</p>
<p>Method</p>
<p>Our goal is to fit a parametric generative model p(c; θ) to approximate the distribution of known meta-stable materials q(c) using a dataset of samples.The distributions p and q are defined on the Riemannian manifold C. Our FlowLLM model generates samples from the parametric distribution using a two-step procedure (see figure 1).First it samples the LLM, then it refines the LLM output using RFM, like so:
c 0 ∼ p LLM (c; θ 0 ),(1)c 1 ∼ p RFM (c|c 0 ; θ 1 )(2)
where p LLM is modeled using a large language model [6,11], and p RFM is modeled using Riemannian Flow Matching (RFM) [3,26], and θ = (θ 0 , θ 1 ).Both the LLM and RFM frameworks are trained to estimate the data distribution over meta-stable crystals on samples from the Materials Project [15].</p>
<p>Overview of training First, we fine-tune an LLM to generate string representations of meta-stable materials [11].Once trained, we can sample the LLM distribution using next token prediction, optionally conditioning on a prompt (see figure 2).Next, we train the RFM model using the FlowMM objective [26] where, conditioned on the chemical formula, will learn to transport between the LLM's model distribution and the data distribution.The full training process is described in Algorithm 1.</p>
<p>Overview of sampling</p>
<p>We give the standard prompt to the LLM and allow it to do next token prediction until it produces a stop token.As long as all atom types are actual elements and the lattice parameters are physical, we move forward.Otherwise we reject the sample.Then, we convert the text to a crystal representation that serves as the initial sample.This sample's fractional coordinates f and lattice parameters l are iteratively refined by the RFM model to produce the final sample of FlowLLM.This sampling process is illustrated in figure 1.</p>
<p>Large Language Model (p LLM ) for Crystals</p>
<p>LLMs define a distribution over sequences through an autoregressive decomposition, T t=1 p(w t+1 |w 0:t ), where each p(w t+1 |w 0:t ) follows a categorical distribution conditioned on all previous tokens (w 0:t ) in the sequence.Our LLM model closely follows Gruver et al. [11].</p>
<p>Tokenization Language models interact with strings in text datasets after the string is converted into a sequence of tokens.The choice of tokenizer can have a large impact on the performance of the language model.In terms of tokens, we represent a crystal c using fixed precision numberstwo decimal places for fractional coordinates, and one for lattice lengths.Angles are represented as integers.Atom types are represented as discrete tokens.We use LLaMA-2 models [41] for our LLM architecture since these models break numbers into a sequence of digits, which has been shown to dramatically improve performance on arithmetic tasks [23].</p>
<p>Training We rely on the extensive pretraining of LLaMA-2 models to instill useful biases over numerical operations.To train p LLM , we fine-tune a pre-trained LLaMA-2 model on a dataset of crystal structures represented as strings along with a prompt indicating that the model should generate bulk materials by writing the lattice in lengths and angles along with atom types and coordinates.An example of such a representation along with a prompt is shown in figure 2.</p>
<p>The flexibility of LLMs allows us to optionally include different kinds of conditional information in the prompt such as the chemical formula.We can also solve other tasks such as infilling by making changes to the prompt.For this hypothetical conditional generation, the prompt could include a desired
Sample (c 0 , c 1 ) ∼ D, t ∼ U([0, 1]) 11: c t := exp c0 (t log c0 (c 1 )) 12: L(θ 1 ) = ∥v θ1 t (c t ) − u t (c t |c 1 )∥ 2 13:
Take gradient descent step on ∇ θ1 L(θ 1 ) 14: end while chemical formula, material properties, or a combination of such information.In this work, we used the same conditioning used in Gruver et al. [11], and we leave a more detailed study of this to future work.</p>
<p>Sampling To generate sequences from the model, the conditional distribution is sampled sequentially.The sampling procedure is modulated to control the diversity and sampling speed using the temperature (τ ) and nucleus size (P ) hyperparameters of nucleus sampling [13].Temperature controls the entropy of the conditional distributions, introducing a trade-off between diversity and mode sampling.The nucleus size limits the number of tokens that can be sampled.Given a nucleus size P with 0 &lt; P ≤ 1, sampling is restricted to the most probable tokens with cumulative probability P .</p>
<p>Symmetries in LLMs</p>
<p>The LLM architecture does not inherently produce a symmetric density, i.e., the distribution of meta-stable crystals that the LLM learns is not symmetric according to the fundamental properties of crystals.We perform no fractional coordinate data augmentation via translation, and no token permutation data augmentation.Unlike the other symmetries, rotation invariance holds for the learned LLM distribution due to our choice of representing the unit cell with lattice parameters.</p>
<p>Riemannian Flow Matching (p RFM ) for Crystals</p>
<p>Riemannian Flow Matching RFM produces a Continuous Normalizing Flow [3], i.e., a continuous, parametric, diffeomorphism between the LLM base distribution p 0 := p LLM and an approximation to our target distribution p 1 ≈ q.To model p RFM := p 1 , we fit a time-dependent vector field v θ1 t that has been adapted to crystals and is implemented using a neural network with parameters θ 1 .Continuous Normalizing Flows are computationally expensive to train using maximum likelihood, but an alternative objective called Conditional Flow Matching [22] is more stable and scales better.The objective was generalized to Riemannian manifolds [2], and specifically to labeled point clouds with periodic boundary conditions, i.e. crystals, by Miller et al. [26].</p>
<p>Concretely, each point c ∈ C has an associated tangent space T c C with an inner product ⟨u, v⟩ for u, v ∈ T c C, enabling the definition of distances, volumes, angles, and minimum length curves (geodesics).The geodesics for any C that we consider can be written in closed form using the exponential and logarithmic maps.The geodesic connecting c 0 ,
c 1 ∈ C at time t ∈ [0, 1] is c t := exp c0 (t log c0 (c 1 )),(3)
where exp □ and log □ are the exponential and logarithm maps for the manifold C.These geodesics help define the supervision signal used to train RFM.</p>
<p>Our RFM generative model v θ1 t : [0, 1] × C → T C is parameterized as a time-dependent, smooth vector field.Training proceeds by regressing onto conditional vector fields u t (c|c 1 ) that generate single data points c 1 .For the geodesic path, this corresponds to u t (c|c 1 ) = − 1 1−t log c1 (c).The general RFM training objective is then:
L(θ 1 ) = E t,p(c0)q(c1) ∥v θ1 t (c t ) − u t (c t |c 1 )∥ 2 . (4)
Since we only use flat manifolds, ∥•∥ is the Euclidean norm.At the optimal solution, v θ1 t generates p t with endpoints p 0 = p, p 1 = q.At sampling time, we draw a sample from c 0 ∼ p and solve the ordinary differential equation d dt c t = v θ1 t (c t ) with initial value c 0 at t = 0; the solution at t = 1 is then the sample from our RFM model.</p>
<p>Geometry of F We apply the conditional vector field for a point cloud living on a n×3-dimensional product of flat tori invariant to global translations, i.e. fractional coordinates with periodic boundary conditions [26].This is a geodesic path, which may cross the periodic boundary:
exp f i ( ḟ i ) := f i + ḟ i − ⌊f i + ḟ i ⌋, log f i 0 (f i 1 ) := 1 2π atan2 sin(ω i ), cos(ω i ) ,(5)
where
ω i := 2π(f i 1 − f i 0 ),andḟ i ∈ T f i F i for i = 1, . . . , n.
Computing the geodesic of n atoms amounts to an atom-wise application of log f 0 on f 1 and exp f on ḟ ∈ T f F respectively.Additionally, following Miller et al. [26] we address translation-invariance by removing the mean torus translation:
u F t (f | f 1 ) := log f 1 (f ) − 1 n n i=1 log f i 1 (f i ).(6)
Geometry of L The space of lattice parameters, L := R +3 × [60, 120] 3 , is a Euclidean space with boundaries.We can ignore these boundaries for the lattice lengths in R +3 since (i) the data does not lie on the boundary (a, b, c &gt; 0) and (ii) we can clamp our base distribution to be positive with rejection.The boundary issue for the lattice angles α, β, γ can be addressed [26] using a diffeomorphism φ : [60 • , 120 • ] → R to unconstrained space, applied element-wise to each angle:
φ(η) := logit η − 60 120 , φ −1 (η ′ ) = 120 σ (η ′ ) + 60,(7)
where σ(.) and logit are the sigmoid and the log-odds functions, respectively.We directly apply RFM in the unconstrained space, and for sampling, we map the angles back into [60 • , 120 • ] using φ −1 .</p>
<p>The RFM training objective With this formulation, our training objective based on (4) becomes:
E t,pLLM(f 0 ,l0|a)q(f 1 ,l1,a) λ f 3n v F ,θ1 t (c t ) + log f 1 (f 0 ) − 1 n n i=1 log f i 1 (f i 0 ) 2(8)+ λ l 6 v L,θ1 t (c t ) + l 0 − l 1 2 ,
where we now use p LLM as the base distribution, and c t = (f t , l t , a).The loss coefficients λ f , λ l ∈ R + are hyperparameters.We use a graph neural network (GNN) inspired by [36,16,26] for v θ1 t (c).This GNN enforces equivariance to atom permutations via message passing, invariance to atom translation by featurizing graph edges as relative displacements of nodes, and invariance to rotations by our choice of lattice representation.See appendix B for more details about the GNN architecture.</p>
<p>Consequences of using an LLM as the base distribution</p>
<p>Model symmetries Just like the LLM, the orientation-invariant representation of the unit cell leads to global rotation invariance.However, permutation and translation symmetries are not so simple.If the parameterization of the RFM velocity field is G-equivariant, and the base distribution is G-invariant, then the model density is G-invariant [18].We use graph neural networks [36,40,27,44,7,21,31,51], and additional projections [26], to ensure that the RFM velocity predictions are Gequivariant to both permutation and translation.However, we will generally not recover a translation invariant density because the base distribution defined by the LLM is not invariant to translation.The density will be permutation invariant in our RFM representation because the each atom is a node in an unordered point cloud and the LLM ordering is ignored by the RFM, but the density will not be permutation invariant in the text representation, due to the LLM's lack of token permutation invariance.Empirically, we do not find the lack of exact invariance to be a problem, and FlowLLM outperforms methods with exact invariance (section 5).This is because an LLM trained to generate crystals is approximately invariant to crystal symmetries.This was verified by Gruver et al. [11] who proposed a new metric, Increase in Perplexity under Transformation (IPT), to quantify this approximation:
IPT(s) = E g∈G <a href="9">PPL(t g (s)) − PPL(t g * (s))</a>
where g * = arg min PPL(t g * (s)), and PPL is the perplexity of the sequence, the exponent of the length-normalized cross entropy loss, PPL(s) = 2 CE(s)/n .They find that a well-trained LLM obtains a small IPT value, implying that it is approximately invariant.</p>
<p>Invalid crystals</p>
<p>The LLM base distribution is not constrained to C, i.e. the LLM can generate invalid crystals.We find that this is extremely rare and easy to detect.In such cases, we simply reject that sample, and draw a new sample until we get a valid crystal.Empirically, we found this rejection rate to be ∼ 0.5% with a softmax temperature of 0.7.</p>
<p>Text is not continuous in L or F The LLM base distribution only takes non-zero values over a small number of discrete points due to the use of finite precision representations.For example, we represent fractional coordinates with only 2 decimal places, so they can only take one of 100 distinct values.We can mitigate this problem by adding a small amount of random zero-mean gaussian noise to all continuous values predicted by the LLM.Empirically, we do not observe any noticeable difference in performance due to this added noise (see appendix F).</p>
<p>Experiments</p>
<p>Setup</p>
<p>We trained our model on the widely used MP-20 dataset1 of inorganic crystalline materials [47].MP-20 comprises 45,231 materials, a subset of the Materials Project [15] containing up to 20 atoms known to be metastable (see section 5.2).</p>
<p>We first train our LLM independently using the various prompting strategies described in Section 4.</p>
<p>Unless otherwise specified, we employed a pretrained LLaMA-2 70B model [41] for all experiments, that was fine-tuned with the Low-Rank Adapters (LoRA) method [14] using PyTorch [32] and Transformers [46].</p>
<p>Next, we trained the RFM model using the fine-tuned LLM (with frozen weights) as the base distribution and the MP-20 dataset as the target distribution.For computational efficiency, we sampled a large number (N tr ) of examples from the base distribution in advance, and used the same set for all of our training runs.To create this set, we sampled N tr materials, with replacement from MP-20, and queried the LLM with a prompt conditioned on the chemical formula of each of these materials.This results in a set of N tr pairs, {(c i 0 , c i 1 )} Ntr i=0 , of LLM generated materials and ground truth materials that constitutes the training set for the RFM model.We list the hyperparameter values used in our experiments in appendix C.</p>
<p>To generate new samples, we first generate a material from the LLM using an unconditional query.We then perform an integration with the RFM model, starting from this LLM-generated material.During sampling, we can adjust hyperparameters such as temperature τ , nucleus probability P , and the number of integration steps to achieve different trade-offs between diversity, accuracy, and efficiency.</p>
<p>Metrics</p>
<p>Our primary metrics are Stability Rate, the percentage of generated materials that are thermodynamically stable, a key indicator of synthesizability, and the S.U.N. rate, the percentage of materials that are stable, unique and novel.Since computing stability is computationally expense, Xie et al. [47] proposed a number of proxy metrics.We explain these metrics in more detail in appendix D.</p>
<p>One key difference in evaluation between the proxy metrics and the stability metrics is the use of pre-relaxation and relaxation techniques.Proxy metrics are computed on raw samples without any further processing.Stability metrics are computed on structures that are first pre-relaxed using CHGNet [5] then relaxed using Density Functional Theory.</p>
<p>Density Functional Theory is extremely expensive, even with speedups using pseudo-potentials [20].</p>
<p>Ideally, the generative model can generate many S.U.N. structures that are already close to their relaxed ground state.Generating structures close to ground state may also indicate that the model has done a better job capturing the data distribution.It can also speed up or obviate the need for relaxing the generated structures, which has huge computational benefits.We include several additional metrics to measure the closeness of generated and corresponding ground state structures, that are described in appendix E.</p>
<p>Results</p>
<p>We compare our model to four prior methods: CD-VAE [47], a hybrid Variational Autoencoder &amp; diffusion model; DiffCSP [16], a diffusion model; FlowMM [26], a Riemannian Flow Matching model; and CrystalLLM [11], which fine-tunes a LLaMA-2 model on materials represented as sequences.</p>
<p>The LLM and RFM components of FlowLLM closely resemble the formulations in CrystalLLM and FlowMM , respectively.To compare different models, we generate 10,000 new structures from each model and compare the metrics described in section 5.2.</p>
<p>Our main results are presented in table 1.On the most important metrics, namely the Stability &amp; S.U.N. rates, FlowLLM significantly outperforms all prior methods across various LLM sampling parameters.For our best FlowLLM model (τ = 0.7, P = 0.9), 17.82% of the generated structures are stable, out of which 48% are novel (not similar to any training or validation structure).Of the remaining structures, 58% are unique, leading a to a S.U.N.rate of 4.92%.FlowLLM obtains a ∼ 300% higher stability rate and ∼ 50% higher S.U.N.rate than the best prior model! Figure 3a shows histograms comparing the E hull values of generated materials from FlowLLM compared to prior models.Clearly, FlowLLM generates many more materials with lower E hull values than the other models.</p>
<p>The results on proxy metrics, on the other hand, remain mixed.Diffusion and flow matching methods excel on Coverage Recall, while CrystalLLM has the best Composition Validity.FlowLLM achieves the best compromise between coverage and validity, potentially explaining its superior Stability &amp; S.U.N. rates.It is important to note that many of these metrics have become saturated, offering limited discriminatory power for evaluating state-of-the-art models.As a result, we anticipate a decreased reliance on these metrics in future research.</p>
<p>Comparison of generated and relaxed structures While the stability rate and S.U.N metrics capture whether the generated structures can be relaxed to stable / S.U.N. states, they do not address the question: How close are the generated structures to their relaxed state?To answer this question, we compared generated structures to those same generated structures after relaxation using CHGNet, computing the following metrics between generated and CHGNet relaxed states: Match Rate and RMSD, as defined by StructureMatcher, along with the ∆-Energy and the average Num steps between the states.Definitions for these metrics can be found in appendix E.  Table 2 shows a comparison of FlowMM and FlowLLM.The samples generated by FlowLLM are significantly closer to ground state compared to FlowMM, according to our metrics.</p>
<p>Importance of learned base distribution One motivation for a hybrid LLM-RFM model is to leverage the LLM's superior ability to generate accurate atom types compared to denoising models.To isolate this effect, we trained the FlowLLM-Types model, following a similar procedure as FlowLLM but using simple base distributions for lattice parameters and fractional coordinates identical to those used in FlowMM [26].Thus, the LLM only contributes to atom type prediction in this model.Despite this simplification, FlowLLM-Types still surpasses prior models on the Stability Rate metric (table 1), highlighting the benefits of employing an LLM for atom type prediction.The stability rate of FlowLLM-Types remains considerably lower than that of FlowLLM, underscoring the substantial value of using learned base distributions.</p>
<p>N-ary analysis</p>
<p>The number of distinct element types in a material is called the N -ary value of that material.Figure 3b compares the distribution of N-ary values for different models with the target data distribution.FlowMM and FlowLLM match the data distribution better than the diffusion models, which tend to generate too many materials with high n-ary.</p>
<p>Number of RFM integration steps Compared to diffusion and flow matching models which require hundreds or thousands of integration steps, FlowLLM is able to converge in as little as 50 steps (figure 3c).This is not surprising given our use of a learned base distribution.</p>
<p>Discussion</p>
<p>The discovery of novel, stable materials holds the potential to help revolutionize numerous industries, but progress has been slow due to the high computational costs involved.Widely used random structure search methods [33] yield less than a 1% success rate in identifying stable materials.Given the substantial cost of validating generated structures using density functional theory, improving this rate is of paramount importance.</p>
<p>Recent breakthroughs with denoising models [16,26] and large language models [11] have increased the stability rate to ∼ 5%, a significant improvement over traditional approaches.In this work, we propose a novel generative model which harnesses the strengths of both paradigms to further increase this number by over 3×, representing a major advancement in the field.</p>
<p>Limitations While FlowLLM excels at generating stable materials, a key limitation is its lack of end-to-end differentiability.This hinders its direct application to inverse design, where generative models are optimized to generate material with specific properties, as explored in prior work using denoising models [49,47].Future research could investigate extending FlowLLM for inverse design.</p>
<p>Broader impact This work can accelerate the discovery of new materials for renewable energy, electronics, and carbon capture, ultimately benefiting society by enabling more efficient and sustainable technologies.However, the adoption of generative models also raises concerns, such as the creation of harmful substances and access inequalities.</p>
<p>The LLM was trained for 10 epochs with a batch size of 16, and cosine annealed learning rate of 0.0005, with LoRA rank = 8 and α = 32.</p>
<p>Compute resources</p>
<p>We trained our LLM model on 8x 80GB A100 GPUs for roughly 1 day.We used 4-bit quantization and LoRA to optimize training.Sampling from the trained LLM required a total of ∼ 250 A100 GPU days, that were parallelized over 300 A100 GPUs.</p>
<p>Each of our RFM models were trained for 2 days on a single 32GB V100 GPU.All experiments were performed on an internal GPU cluster.</p>
<p>Evaluations required running DFT computations that were run on a large internal CPU cluster with 5000 nodes, each equipped with a 26-core Intel Cooper Lake-SP CPU, and 64GB memory.Each DFT computation took about 1 hour of compute on a single node, and we ran nearly 50,000 such computations to evaluate all of our models.</p>
<p>D Metrics</p>
<p>Thermodynamic stability is a key indicator of synthesizability, and generating novel stable materials is of keen interest in material science.Stability is determined by comparing a material's energy to those of competing crystals with the same elements.Formally, stability is measured by constructing a convex hull of all competing materials from a reference set and computing the distance from this hull (called Energy above the Hull, or E hull ).Stable materials have E hull &lt; 0, while materials with E hull &lt; 0.08 eV/atom are called metastable [39] With this defintion of stability, we define our Stability Rate metric as the percentage of generated materials that are stable (E hull &lt; 0, and n-ary ≥ 2).For our reference set of materials, we use the Materials Project database recorded by [35] in February 2023.</p>
<p>Following Miller et al. [26], we compute E hull values by running structure relaxations on the generated structures with the CHGNet model [5] followed by density functional theory (DFT) [19] calculations.</p>
<p>While stability rate is an important metric, it does not capture novelty.Therefore, we define a second metric, the S.U.N.rate which measures the percentage of generated structures which are Stable, Unique, and Novel.To determine novelty, we exclude generated structures that are similar to any structure in the training dataset.Similarity is measured using pymatgen's StructureMatcher [29] with default settings.A generated structure that is not similar to any training data structure is considered novel.</p>
<p>To compute uniqueness, we use StructureMatcher to do pairwise comparisons between all generated structures, and group similar structures into equivalence classes.where N gen is the number of generated samples, N stable is the number of generated samples which are stable, and N S.U.N. is the number of generated samples which are stable, unique, and novel.</p>
<p>Due to the computational expense of DFT needed to compute stability and S.U.N. rates, a number of proxy metrics have been proposed by Xie et al. [47] to benchmark model performance:</p>
<p>E Comparison of generated structures to ground state structures</p>
<p>For many practical applications in chemistry, it is important to find the local energy minimum of a generated structure.This is done by performing computationally expensive structure relaxations.Thus, it is beneficial to generate structures close to their ground state.To compare how close the generated structures are to their ground state (i.e.local energy minimum), we define 4 additional metrics (shown in table 2):</p>
<ol>
<li>Match Rate: What fraction of generated structures and corresponding ground state structures are similar (where similarity is computed using pymatgen's StructureMatcher with default settings).2. RMSD: Average RMS distance between generated structures and corresponding ground state structures computed using pymatgen's StructureMatcher whenever there is a match.3. ∆-Energy: Difference in energy between the generated structure and ground state structure of the DFT relaxation.This measures the reduction in energy during the structure relaxation process.4. Num Steps: Number of optimizer steps needed to pre-relax the generated structure using CHGNet.</li>
</ol>
<p>F Adding noise to the base distribution</p>
<p>Table 4 shows the effect of adding noise to the base distribution.We do not see a significant impact from the added noise.</p>
<p>G Material Generation Time</p>
<p>We compare the time to generate 10,000 materials between FlowLLM with FlowMM.Inference for both models was run on a machine with a 32 core Intel(R) Xeon(R) Platinum 8488C CPU, and a single 80GB A100 GPU.FlowMM used 750 integration steps, and the RFM step of FlowLLM used 250 integration steps.With this setup, the FlowMM model takes 65.1 minutes to generate 10,000 materials, while FlowLLM takes 89.6 minutes, which is comparable to FlowMM.</p>
<p>A more useful metric is the time to generate a S.U.N. material, which is computed by dividing the inference time by the number of generated S.U.N. materials.With this metric, FlowMM takes 16.14 seconds to generate S.U.N. material, while FlowLLM takes only 10.9 seconds.</p>
<p>Figure 2 :
2
Figure 2: Left: String encoding of materials used to train the LLM based on Gruver et al.[11].Right: An example prompt used during training.The conditioning information in blue is optional, and can be replaced with conditioning on other properties as well.The text in red is replaced with the crystal string representation shown on the left.</p>
<p>Algorithm 1 training 1 : 5 :c i 1 ∼ D with replacement 6 : 7 : 9 :
1151679
FlowLLM Input: Training dataset of materials: D = {c i }, Pre-trained LLM: p LLM , RFM velocity network: v θ1 t , Number of RFM training samples: N tr .// Step 1: Fine-tune the LLM 2: Fine-tune p LLM on D following the procedure from Gruver et al.[12] // Step 2: Sample the LLM to generate training data for the RFM model 3: Initialize D ← ∅ 4: for i = 1: N tr do Sample Sample c i 0 ∼ p LLM (•|θ 0 ) using a prompt conditioned on the formula of c i 1 D = D ∪ {(c i 0 , c i 1 )} 8: end for // Step 3: Train the RFM model on D while not converged do 10:</p>
<p>Figure 3 :
3
Figure 3: (a) Histogram of E hull values comparing FlowLLM with prior models.The dashed line shows thermodynamic stability threshold (E hull = 0).(b) Histogram of N-ary compared to the data distribution.(c) Structural validity as a function of number of integration steps.</p>
<p>Table 1 :
1
Results for material generation on the MP-20 dataset.Stability rate is the percentage of generated materials with E hull &lt; 0.0 &amp; N -ary ≥ 2.
MethodLLM ParamsInteg. StepsValidity (%) ↑Coverage (%) ↑Property ↓Stability Rate (%) ↑ SUN Rate(%) ↑Structural Composition Recall Precision wdist (ρ) wdist (Nel)MP-2023CDVAE [47]-5000100.0086.7099.1599.490.6880.2781.57-DiffCSP [16]-1000100.0083.2599.7199.760.3500.1255.063.34FlowMM [26]-100096.8583.1999.4999.580.2390.0834.652.34CrystalLLM (70B) [11]τ = 0.7-99.695.485.898.90.810.445.28-FlowLLM-Typesτ = 0.5, P = 0.9 τ = 0.9, P = 0.9750 75099.96 99.8893.32 91.6996.85 97.1899.78 99.760.846 1.140.209 0.208.79 8.95--FlowLLMτ = 1.0, P = 0.9 τ = 0.7, P = 1.0250 25099.81 99.8889.05 89.4599.06 99.0699.68 99.710.66 0.730.09 0.1410.07 13.034.89 4.88τ = 0.7, P = 0.925099.9490.8496.9599.821.140.1517.824.92</p>
<p>Table 2 :
2
Comparison of generated and corresponding ground state structures from the CHGNet relaxation.Compared to FlowMM, FlowLLM generates structures much closer to the ground state.
MethodMatch Rate (%) ↑ RMSD (Å) ↓ ∆-Energy (eV/atom) ↓ Num Steps ↓FlowMM74.30.0960.3031191.98FlowLLM94.90.0230.089837.971750CD-VAE FlowLLM ( =0.7, p=0.9)DiffCSP FlowLLM ( =0.7, p=0.9)FlowMM FlowLLM ( =0.7, p=0.9)15001250Count100075050025000.2 0.00.2 E hull (eV/atom) 0.40.60.81.00.2 0.00.2 E hull (eV/atom) 0.40.60.81.00.2 0.00.2 E hull (eV/atom) 0.40.60.81.0</p>
<p>Table 3 :
3
RFM model hyperparameters Value
Hidden Dimension512Time Embedding Dimension256Number of Layers6Activation FunctionsiluLayer NormTrueBatch Size256Max Epochs20Inference anti-annealing scale5</p>
<p>Table 4 :
4
Each group is only counted as a single unique structure for the purpose of computing the S.U.N. rate.Formally, Proxy metrics for a FlowLLM trained with different levels of random gaussian noise added to continuous values predicted by the LLM.Added noise increases the support of the base distribution, but we do not see an appreciable difference in the metrics.
Stability Rate :=N stable N gen(17)S.U.N. Rate :=N S.U.N. N gen(18)(19)</p>
<ol>
<li>Structural Validity: Percentage of structures with valid atomic arrangements, where all pairwise interatomic distances exceed 0.5 Å. 2. Compositional Validity: Percentage of charge-neutral crystals, as determined by the SMACT heuristic system [4].3. Coverage Recall &amp; Precision: Standard recall and precision metrics assessing the model's ability to generate structures close to those in the test dataset.Closeness is evaluated using structural and compositional fingerprints [50, 43]. 4. Wasserstein Distances of Property Distributions: Wasserstein distances between the distributions of computed properties (density, and N el -the number of unique atoms) for crystal samples from the test set and generated structures.</li>
</ol>
<p>Publicly available at https://github.com/txie-93/cdvae/tree/main/data/mp_20
A Crystal Representations DetailsAtomic types The representation of atomic number is dependent on the model processing the data.In the LLM, the name of the element can be written into the text representation directly.This can be a string or single token, depending on LLaMA-2's tokenization.In the RFM framework, we applied a one-hot representation.Unit cell geometry Throughout the paper and in our implementation, we represent the unit cell using lengths and angles; however, there is another representation relevant for defining the fractional coordinates and better expressing crystal symmetries.The unit cell can be defined by a matrix of Cartesian column vectors l := l1 , l2 , l3 ∈ L = R 3×3 .This representation has strictly more information than l, since it also defines the orientation of the unit cell.This orientation is irrelevant in our paper, since we want rotation invariance.That's why we choose l in the first place.Fractional coordinates Now that we have the representation l, we can define fractional coordinates.Recall, atomic positions are typically represented using Cartesian coordinates x := x 1 , . . ., x n ∈ X = R 3×n with coordinates in the rows and atoms in the columns.The Fractional coordinate representation is definedB Graph Neural network in the RFM ModelIn this section, we describe the graph neural network used in our RFM model.Our GNN model is inspired by the GNNs used in FlowMM[26]and DiffCSP[16], which in turn adapted the EGNN[36]model for fractional coordinates,where m ij (s) , m i (s) represent messages at layer s between nodes i and j, h j (s) represents hidden representation of node j at layer s; φ m , φ h , ϕ h (0) , φ ḟ , φ l represent parametric functions with all parameters noted together as θ.Finally, we define SinusoidalEmbedding(x) := (sin(2πkx), cos(2πkx))where n f req is a hyperparameter.We standardized the l input to the network with z-scoring.We also standardized the outputs for predicted tangent vectors ḟ , l. Models were trained using the AdamW optimizer[24].C HyperparametersWe used N tr = 3.3 × 10 6 and trained the model for 20 epochs with early stopping.To generate the N tr training pairs, we used temperature τ = 0.9 and nucleus probability P = 0.99.While other values might be explored, the high computational cost of experimentation limited our exploration of these parameters.For training the RFM model, we swept over a few values of learning rates: {1e-3, 7e-4, 5e-4, 3e-4, 1e-4}.To compute the loss function, we used loss weights λ f = 200, and λ l = 1 in the training objective (equation (8)).These values were chosen by running a grid search over λ f ∈ {100, 200, 300, 400}, λ l ∈ {1}.Additional hyperparameter settings are given in table 3.
Open Catalyst 2020 (OC20) Dataset and Community Challenges. Lowik Chanussot, 10.1021/acscatal.0c04525ACS Catalysis. 2021</p>
<p>Riemannian flow matching on general geometries. T Q Ricky, Yaron Chen, Lipman, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Neural ordinary differential equations. T Q Ricky, Chen, Advances in neural information processing systems. 201831</p>
<p>SMACT: Semiconducting materials by analogy and chemical theory. Daniel W Davies, Journal of Open Source Software. 413612019</p>
<p>CHGNet as a pretrained universal neural network potential for chargeinformed atomistic modelling. Bowen Deng, Nature Machine Intelligence. 52023</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. Daniel Flam, - Shepherd, Alán Aspuru-Guzik, arXiv:2305.057082023arXiv preprint</p>
<p>e3nn: Euclidean neural networks. Mario Geiger, Tess Smidt, arXiv:2207.094532022arXiv preprint</p>
<p>USPEX-Evolutionary crystal structure prediction. Colin W Glass, Artem R Oganov, Nikolaus Hansen, Computer physics communications. 1752006</p>
<p>The emergence of perovskite solar cells. Martin Green, Anita Ho-Baillie, Henry Snaith, 10.1038/NPHOTON.2014.134Nature Photonics. 8July 2014</p>
<p>Numerically stable algorithms for the computation of reduced unit cells. Ralf W Grosse-Kunstleve, Nicholas K Sauter, Paul D Adams, Acta Crystallographica Section A: Foundations of Crystallography. 602004</p>
<p>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text. Nate Gruver, arXiv:2402.043792024arXiv preprint</p>
<p>Large Language Models Are Zero-Shot Time Series Forecasters. Nate Gruver, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>The Curious Case of Neural Text Degeneration. Ari Holtzman, International Conference on Learning Representations. 2020</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward Hu, ArXiv abs/2106.096852021</p>
<p>The Materials Project: A materials genome approach to accelerating materials innovation. Anubhav Jain, 10.1063/1.4812323APL Materials. 2166-532X1111002July 2013</p>
<p>Crystal Structure Prediction by Joint Equivariant Diffusion. Rui Jiao, arXiv:2309.044752023arXiv preprint</p>
<p>Space Group Constrained Crystal Generation. Rui Jiao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Equivariant flows: exact likelihood generative learning for symmetric densities. Jonas Köhler, Leon Klein, Frank Noé, PMLR. 2020International conference on machine learning. </p>
<p>Self-consistent equations including exchange and correlation effects. Walter Kohn, Lu Jeu Sham, Physical review. 140A11331965</p>
<p>Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set. Georg Kresse, Jürgen Furthmüller, Computational materials science. 61996</p>
<p>EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. Yi-Lun Liao, arXiv:2306.120592023arXiv preprint</p>
<p>Flow Matching for Generative Modeling. Yaron Lipman, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks. Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.142012023arXiv preprint</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2018</p>
<p>Scaling deep learning for materials discovery. Amil Merchant, Nature. 2023</p>
<p>FlowMM: Generating Materials with Riemannian Flow Matching. Kurt Benjamin, Miller, Forty-first International Conference on Machine Learning. 2024</p>
<p>Relevance of rotationally equivariant convolutions for predicting molecular properties. Kurt Benjamin, Miller, arXiv:2008.084612020arXiv preprint</p>
<p>LixCoO2 (0&lt;x&lt;-1): A new cathode material for batteries of high energy density. K Mizushima, 10.1016/0025-5408(80)90012-4Materials Research Bulletin. 0025- 5408151980</p>
<p>Python Materials Genomics (pymatgen): A robust, open-source python library for materials analysis. Ping Shyue, Ong, Computational Materials Science. 682013</p>
<p>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling. Teerachote Pakornchote, Scientific Reports. 1412752024</p>
<p>Reducing SO (3) Convolutions to SO (2) for Efficient Equivariant GNNs. Saro Passaro, Lawrence Zitnick, arXiv:2302.036552023arXiv preprint</p>
<p>PyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Neural Information Processing Systems. 2019</p>
<p>Ab initio random structure searching. Chris J Pickard, Needs, Journal of Physics: Condensed Matter. 23532012011</p>
<p>An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties. Zekun Ren, 10.1016/j.matt.2021.11.032Matter. 2590-23852021</p>
<p>Matbench Discovery. Janosh Riebesell, 10.6084/m9.figshare.22715158.v12figshareJan. 2024</p>
<p>E (n) equivariant graph neural networks. Garcia Vıctor, Emiel Satorras, Max Hoogeboom, Welling, PMLR. 2021International conference on machine learning. </p>
<p>Large-scale machine-learning-assisted exploration of the whole materials space. Jonathan Schmidt, arXiv:2210.005792022arXiv preprint</p>
<p>The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture. Anuroop Sriram, 10.1021/acscentsci.3c01629ACS Central Science. </p>
<p>The thermodynamic scale of inorganic crystalline metastability. Wenhao Sun, Science advances. 211e16002252016</p>
<p>Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. Nathaniel Thomas, arXiv:1802.082192018arXiv preprint</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, ArXiv abs/2307.092882023</p>
<p>Predicting stable crystalline compounds using chemical similarity. Hai-Chen Wang, Silvana Botti, Miguel Al Marques, npj Computational Materials. 71122021</p>
<p>A general-purpose machine learning framework for predicting properties of inorganic materials. Logan Ward, npj Computational Materials. 212016</p>
<p>Coordinate Independent Convolutional Networks-Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. Maurice Weiler, arXiv:2106.060202021arXiv preprint</p>
<p>Normalizing flows for atomic solids. Peter Wirnsberger, Machine Learning: Science and Technology. 3250092022</p>
<p>Transformers: State-of-the-Art Natural Language Processing. Thomas Wolf, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOct. 2020</p>
<p>Crystal Diffusion Variational Autoencoder for Periodic Material Generation. Tian Xie, International Conference on Learning Representations. 2021</p>
<p>Scalable diffusion for materials generation. Mengjiao Yang, arXiv:2311.092352023arXiv preprint</p>
<p>MatterGen: a generative model for inorganic materials design. Claudio Zeni, arXiv:2312.036872023arXiv preprint</p>
<p>Local structure order parameters and site fingerprints for quantification of coordination environment and crystal structure similarity. E R Nils, Anubhav Zimmermann, Jain, RSC advances. 102020</p>
<p>Spherical Channels for Modeling Atomic Interactions. Larry Zitnick, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8828 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8828</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8828</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f0e0196b23f98d2553f54538d62e7c2a15036f31</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f0e0196b23f98d2553f54538d62e7c2a15036f31" target="_blank">Structural Neural Encoders for AMR-to-text Generation</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The extent to which reentrancies (nodes with multiple parents) have an impact on AMR-to-text generation is investigated by comparing graph encoder to tree encoders, where reENTrancies are not preserved.</p>
                <p><strong>Paper Abstract:</strong> AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs. Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings. Approaching the problem while working directly with graphs requires the use of graph-to-sequence models that encode the AMR graph into a vector representation. Such encoding has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs. We investigate the extent to which reentrancies (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where reentrancies are not preserved. We show that improvements in the treatment of reentrancies and long-range dependencies contribute to higher overall scores for graph encoders. Our best model achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the state of the art by 1.24 points.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8828.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8828.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQUENTIAL_LINEARIZATION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first linearization of AMR (sequential AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts an AMR graph to a token sequence via a depth-first traversal (linearization), visiting nodes multiple times for reentrant nodes; used as input to sequence-to-sequence BiLSTM encoders for AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (depth-first traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graph is converted into a sequence x1...xN by depth-first traversal; nodes with multiple parents are visited multiple times, so reentrancies are not preserved in the sequence; tokens include concept and label nodes (after Levi transform if applied).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal (linearization) of the AMR graph; repeated visits to reentrant nodes produce repeated tokens in the sequence; often combined with anonymization of rare tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (text generation from AMR graphs) using seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test (LDC2015E86): BLEU 21.43, METEOR 21.53. Test (LDC2017T10): BLEU 22.19, METEOR 22.68 (these are the paper's sequential-baseline numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Underperforms structural encoders: tree-based (TREE) and graph-based (GRAPH) GCNSEQ models outperform the sequential linearization baseline by several BLEU/METEOR points (GRAPH: +~3 BLEU on LDC2015E86). Sequential encoding loses reentrancy information and is outperformed especially on inputs with many reentrancies or long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; compatible with standard sequence-to-sequence models and attention mechanisms; benefits from standard seq2seq tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Loses explicit reentrancy/co-reference structure (nodes visited multiple times); can produce repeated tokens in training data; information loss for co-reference/control structures; sequence length can increase due to repeated visits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produces errors when reentrancies/control structures are important (e.g., wrong pronoun generation, duplicated mentions); worse performance on examples with many reentrancies (graph model shows Meteor +3.1% over sequential for >6 reentrancies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8828.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANONYMIZATION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anonymization of names and rare words in linearized AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preprocessing that replaces names and rare tokens in the AMR linearization with coarse category placeholders to reduce data sparsity for seq2seq models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>anonymization (coarse-category replacement)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace names and rare lexical items with category placeholders (e.g., person_name_0) in the linearized AMR sequence to reduce sparsity; alternative mentioned is a copy mechanism which was not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (applied after linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During preprocessing of the linearized AMR sequence, replace identified rare tokens and names with anonymized placeholders.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (reducing OOVs for seq2seq training)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No isolated numeric metric in paper; used as part of the SEQ pipeline whose overall test scores are reported (see sequential baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Anonymization is the approach used in this paper following Konstas et al. (2017); authors note alternative copy mechanism (Gulcehre et al., 2016) but did not experiment with it here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces data sparsity and OOV issues for seq2seq models; simple to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Hides lexical detail that must be recovered later (post-processing or copy mechanism); may reduce naturalness if not properly de-anonymized.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly reported, but anonymization may remove distinctions needed for tense/number/gender decisions if not compensated elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8828.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEVI_TRANSFORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph transformation of labeled AMR edges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms labeled AMR edges into unlabeled edges by promoting edge labels to nodes (Levi graph), reducing the need to model many distinct labeled-edge parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finite geometrical systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph (edge-label-to-node conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace each labeled edge (i, label, j) with two unlabeled edges by adding the label as a node: (i, label) and (label, j). This yields an unlabeled graph with label nodes as first-class nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (labeled directed graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each edge triple (parent, label, child) insert the label into the node set and connect parent->label and label->child as unlabeled edges; effectively the AMR is converted into its Levi graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used to prepare AMR graphs for structural encoders (GCN, TreeLSTM) and to reduce parameters for graph encoders in AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within the graph encoders that achieve test BLEU (GRAPH GCNSEQ) 24.40 (LDC2015E86) and 24.54 (LDC2017T10); no isolated metric for Levi transform alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Adopted to reduce number of parameters as in prior work (Beck et al., 2018); alternative is encoding labeled edges directly which increases parameters; paper follows Levi as in Beck et al.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces the number of distinct edge-parameter matrices needed (unlabeled edges); allows existing unlabeled GCN formulations to be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases node set size (labels become nodes) which may increase graph size/sequence length in linearization; requires models to learn embeddings for label-nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specifically reported; implicit trade-off between parameter reduction and increased graph node count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8828.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TREE_CONVERSION_DUPLICATE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree conversion via duplicating reentrant nodes (node-splitting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms AMR graphs into trees by replacing nodes with multiple incoming edges by multiple identical nodes (one per incoming edge), thereby removing reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tree conversion by node duplication (reentrancy removal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For nodes with n>1 incoming edges, create n identically labeled nodes each with a single incoming edge so the structure becomes a tree; used to enable TreeLSTM/Tree GCN encoders that require tree inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (converted to trees)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocess AMR graphs by identifying nodes with multiple parents and duplicating them (one copy per incoming edge) so resulting structure is a tree (no reentrancies).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation using tree encoders (TreeLSTM or tree-structured GCN followed by BiLSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test (LDC2015E86): TREE (GCNSEQ without reentrancies) BLEU 23.93, METEOR 23.32. Test (LDC2017T10): BLEU 24.06, METEOR 23.62.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Tree encoder outperforms sequential baseline but is outperformed by full graph encoder (GRAPH) which preserves reentrancies; TREE underperforms GRAPH especially on inputs with many reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables use of tree-structured encoders (TreeLSTM/TreeGCN) that can capture hierarchical relations; simpler than full graph modeling while retaining structural info except reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Losess reentrancy/co-reference information by construction; artificial duplication might confuse models or distort statistics of node occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Per manual examples, tree representations sometimes omit pronouns or otherwise mishandle co-reference/control structures that require reentrancy information (tree model missed pronoun in example).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8828.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiLSTM_SEQ_ENCODER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional LSTM sequential encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BiLSTM encoder over the linearized AMR token sequence that produces context-aware token embeddings for attention-based decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Speech recognition with deep recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bidirectional LSTM encoding of linearized AMR</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply a BiLSTM to the linearized AMR sequence x1..xN to obtain contextual embeddings e1..eN; these embeddings are used by the decoder with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Linearized AMR sequences (derived from AMR graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph-to-sequence conversion beyond linearization; the BiLSTM consumes the linearized sequence tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (seq2seq pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of SEQ baseline: Test LDC2015E86 BLEU 21.43, METEOR 21.53; dev ablations show stacked variants improve when BiLSTM is combined with structural encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>BiLSTM-only (SEQ) is outperformed by structural encoders (TREE and GRAPH) when structural information is integrated. Also, models where structural encoder refines embeddings before BiLSTM (sequence-on-top-of-structure) perform better than structure-on-top-of-sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Strong at encoding sequential context; well-supported and effective when given refined input embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>By itself, cannot represent graph reentrancies or arbitrary graph structure; depends on linearization which causes information loss for reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Tends to duplicate mentions or choose incorrect pronouns/structure when AMR reentrancies/control structures are important (examples in manual inspection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8828.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TREELSTM_ENCODER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Child-Sum TreeLSTM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes tree-structured AMRs using a Child-Sum TreeLSTM (bottom-up) followed by a top-down pass to capture parent and child information; produces node embeddings used with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improved semantic representations from tree-structured long short-term memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Child-sum TreeLSTM (bidirectional: bottom-up + top-down)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Bottom-up Child-Sum TreeLSTM computes h^uparrow by summing children states; a top-down LSTM pass computes h^downarrow using parent states; final node embedding is concatenation [h^downarrow ; h^uparrow].</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Tree-structured AMRs (AMR graphs preprocessed to remove reentrancies)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>AMR graphs are converted to trees via node duplication; TreeLSTM is applied over the tree structure to compute node and root representations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (tree-to-sequence pipeline), with TreeLSTM outputs optionally fed into a BiLSTM and attention decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test (LDC2015E86): TREE (Tree/GCNSEQ variants) reported BLEU ~23.93 and METEOR ~23.32 (GCN-based TREE was best tree variant). Dev ablations: TREELSTMSEQ achieved BLEU 22.26 dev and METEOR 22.87.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>TreeLSTM-based models outperform pure sequential baseline but underperform graph encoders that preserve reentrancies. TreeLSTMs are less sensitive to removing the BiLSTM (RNN) component than GCNs, as TreeLSTM already contains LSTM gates.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures hierarchical structure and parent-child context; can model tree-based dependencies better than plain BiLSTM over linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires pre-processing to remove reentrancies (node duplication), thus cannot directly model co-reference/control structures; computational overhead of tree traversals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails on phenomena that rely on preservation of reentrancy (co-reference/control) — manual examples show missing pronouns or imperfect handling of possessives when reentrancies removed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8828.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN_ENCODER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network encoder (non-recurrent GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes AMR graphs by iteratively aggregating neighbor node embeddings via direction-aware linear transforms and non-linearities (Graph Convolutional Network), producing node-wise embeddings used by attention decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semisupervised classification with graph convolutional networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Convolutional Network (GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each GCN layer, a node's embedding is updated by summing transformed embeddings of its immediate neighbors (with direction-specific parameter matrices), applying a non-linearity (ReLU) and optional highway connections; multiple layers propagate information across the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (both with preserved reentrancies—graphs—or with reentrancies removed—trees)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>GCN is applied directly to the AMR graph (after Levi transform for label nodes). For tree experiments, the same GCN is applied to the tree-converted graph. Final GCN layer outputs provide node embeddings; commonly these embeddings are used as input to a BiLSTM (GCNSEQ) or fed into decoder with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence); used both as pure structural encoder and stacked with BiLSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best graph model (GCNSEQ preserving reentrancies) TEST: LDC2015E86 BLEU 24.40, METEOR 23.60; LDC2017T10 BLEU 24.54, METEOR 24.07. Dev ablation: GCN (no BiLSTM) DEV BLEU 15.83, METEOR 17.76 showing large drop without sequential component. Contrastive pronoun tasks: GRAPH accuracies — Antecedent 96.02%, Type 96.49%, Number 95.11%, Gender 95.79%. Improvements vs sequential on long-range deps: Meteor deltas +1.22 (0-10), +2.05 (11-50), +3.04 (51-200) relative to sequential baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCNSEQ (sequence-on-top-of-structure) outperforms SEQ (BiLSTM-only) and TREE (tree GCNSEQ without reentrancies). GCN without a sequential BiLSTM performs much worse, indicating importance of combining structural and sequential components. Paper compares its GCN approach favorably to Song et al. (2018) and Beck et al. (2018) and obtains state-of-the-art BLEU on both datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models graph structure including reentrancies, leading to improved handling of co-reference/control and long-range dependencies; yields best reported BLEU/METEOR in paper when stacked before BiLSTM (GCNSEQ). Robust improvements especially on graphs with many reentrancies (Meteor +3.1% vs sequential for >6 reentrancies).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Standalone (non-stacked) GCNs underperform dramatically; require stacking with sequential models (BiLSTM) for good results. Additional model complexity and hyperparameters (edge dropout, highway connections).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>GCN-only (no BiLSTM) shows catastrophic drop (dev BLEU ~15.8) indicating failure if not combined with sequence modeling; tense information not encoded in AMR leads to wrong verb tense regardless of graph encoding; contrastive tests show only modest advantages for antecedent/pronoun-type errors (graph outperforms seq mainly on number/gender replacements).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8828.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQUENCE_ON_TOP_OF_STRUCTURE (GCNSEQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-on-top-of-structure stacking (structural encoder before BiLSTM, e.g., GCNSEQ / TREELSTMSEQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stacking method where the structural encoder (GCN or TreeLSTM) first refines input embeddings using graph structure, and the refined embeddings are then processed by a BiLSTM sequential encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequence-on-top-of-structure stacking (GCNSEQ / TREELSTMSEQ)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply structural encoder to raw input node embeddings to inject graph information, then feed the structural encoder outputs as input embeddings to a BiLSTM which captures sequential context; final node embeddings used by attention decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (both full graphs preserving reentrancies and trees with reentrancies removed)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph-to-sequence conversion beyond initial linearization for BiLSTM alignment; structural encoder consumes graph (after Levi transform) and produces refined token/node embeddings which the BiLSTM consumes.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence pipeline combining graph structure and sequence modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GCNSEQ (graph-preserving) achieved TEST: LDC2015E86 BLEU 24.40, METEOR 23.60; LDC2017T10 BLEU 24.54, METEOR 24.07. On dev, GCNSEQ numbers were BLEU 23.62/METEOR 23.77 (as a variant) and outperform structure-on-top-of-sequence variants. Ablations show removing the BiLSTM (i.e., using only structural encoder) drastically reduces performance (GCN-only dev BLEU 15.83).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Sequence-on-top-of-structure consistently outperforms structure-on-top-of-sequence (applying structural encoder before the BiLSTM yields better scores). GCNSEQ (graph-preserving) outperforms GCN-based TREE variant and SEQ baseline; among all models GCNSEQ is best in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines the strengths of structural graph encoding (captures reentrancies and long-range graph relations) and sequential modeling (captures ordering), leading to best overall performance; particularly beneficial for examples with many reentrancies and long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex architecture (two encoders) and higher computational cost; requires careful hyperparameter tuning (GCN layers, highway, dropout, BiLSTM layers).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When BiLSTM is removed, performance collapses (GCN-only failure). Some linguistic properties not encoded in AMR (e.g., tense) remain failure sources despite stacking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8828.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STRUCTURE_ON_TOP_OF_SEQUENCE (SEQGCN/SEQTREELSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-on-top-of-sequence stacking (structural encoder after BiLSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stacking method where a BiLSTM first encodes the linearized AMR sequence and the resulting hidden states are fed into a structural encoder (GCN or TreeLSTM) to further incorporate graph relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structure-on-top-of-sequence stacking (SEQGCN / SEQTREELSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>First compute BiLSTM hidden states over linearized AMR tokens to capture sequential context, then initialize a structural encoder with these states to inject graph topology information; outputs feed the decoder via attention.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (applied to linearized sequence with graph relations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize AMR; compute BiLSTM states; use those states as input node embeddings to a structural encoder (GCN or TreeLSTM) that operates over the graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In dev experiments structure-on-top-of-sequence variants perform worse than sequence-on-top-of-structure. Example dev numbers: SEQGCN dev BLEU around 21.84-22.06 depending on graph/tree modes (Table 1); SEQTREELSTM dev BLEU 21.84 and TREELSTMSEQ (sequence-on-top) 22.26 showing structure-on-top generally inferior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Structure-on-top-of-sequence underperforms the alternative order (sequence-on-top-of-structure) in this paper; stacking order matters and structural encoder-before-BiLSTM yielded better results.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows structural encoder to act as a post-hoc refinement of sequential context; reuses established BiLSTM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Empirically less effective than applying structure first; may fail to provide the BiLSTM with sufficiently informative initial embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower scores than GCNSEQ/TREELSTMSEQ; does not capture benefits of preserving reentrancies as effectively as sequence-on-top-of-structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8828.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8828.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRAPH2SEQ & RELATED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence neural architectures (e.g., Graph2Seq, Gated Graph Neural Networks, LSTM-based graph encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of models that directly encode graph-structured inputs (via GNNs/GRUs/LSTMs over graph neighborhoods) and decode sequences; referenced as related approaches to AMR-to-text graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph2seq: Graph to sequence learning with attention-based neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-sequence encoders (GGNN, Graph2Seq, LSTM-based graph encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes are updated by aggregating neighbor node states; updates can be implemented as GCN layers (sum + linear + nonlinearity), gated GRU-style updates with neighborhood gating (GGNN), or LSTM-style updates; final node embeddings feed an attention-based decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs and other structured graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate directly on graph adjacency/neighborhood (optionally Levi-transformed) rather than converting to a sequence; produce node-wise embeddings for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and other graph-to-sequence tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references Song et al. (2018) and Beck et al. (2018) as recent graph-to-sequence systems; those reported BLEU for comparison (e.g., Song et al. reported 23.30 in prior work). This paper's GCNSEQ outperforms those reported results (achieving 24.40 BLEU on LDC2015E86).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper positions its non-recurrent GCN variant and stacking approach relative to recurrent graph encoders (GRU/LSTM-based) in Song et al. (2018) and Beck et al. (2018), claiming design/implementation differences (edge labels, directionality) may explain performance gaps; GCNSEQ achieves higher BLEU in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models graph topology and reentrancies; flexible in choice of update dynamics (gated, convolutional, recurrent).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Design choices (gating vs non-recurrent updates, edge label handling) affect performance; recurrent graph updates may be more expensive; standalone graph encoders without sequence modeling can underperform.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted that different implementations vary in handling of edge labels and directionality; in this paper, GCN-only (no BiLSTM) is insufficient. Prior works required design trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structural Neural Encoders for AMR-to-text Generation', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation. <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Graph2seq: Graph to sequence learning with attention-based neural networks <em>(Rating: 2)</em></li>
                <li>Improved semantic representations from tree-structured long short-term memory networks <em>(Rating: 1)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Encoding sentences with graph convolutional networks for semantic role labeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8828",
    "paper_id": "paper-f0e0196b23f98d2553f54538d62e7c2a15036f31",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "SEQUENTIAL_LINEARIZATION",
            "name_full": "Depth-first linearization of AMR (sequential AMR)",
            "brief_description": "Converts an AMR graph to a token sequence via a depth-first traversal (linearization), visiting nodes multiple times for reentrant nodes; used as input to sequence-to-sequence BiLSTM encoders for AMR-to-text generation.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "linearization (depth-first traversal)",
            "representation_description": "AMR graph is converted into a sequence x1...xN by depth-first traversal; nodes with multiple parents are visited multiple times, so reentrancies are not preserved in the sequence; tokens include concept and label nodes (after Levi transform if applied).",
            "graph_type": "Abstract Meaning Representation (AMR) semantic graphs",
            "conversion_method": "Depth-first traversal (linearization) of the AMR graph; repeated visits to reentrant nodes produce repeated tokens in the sequence; often combined with anonymization of rare tokens.",
            "downstream_task": "AMR-to-text generation (text generation from AMR graphs) using seq2seq models",
            "performance_metrics": "Test (LDC2015E86): BLEU 21.43, METEOR 21.53. Test (LDC2017T10): BLEU 22.19, METEOR 22.68 (these are the paper's sequential-baseline numbers).",
            "comparison_to_others": "Underperforms structural encoders: tree-based (TREE) and graph-based (GRAPH) GCNSEQ models outperform the sequential linearization baseline by several BLEU/METEOR points (GRAPH: +~3 BLEU on LDC2015E86). Sequential encoding loses reentrancy information and is outperformed especially on inputs with many reentrancies or long-range dependencies.",
            "advantages": "Simple to implement; compatible with standard sequence-to-sequence models and attention mechanisms; benefits from standard seq2seq tooling.",
            "disadvantages": "Loses explicit reentrancy/co-reference structure (nodes visited multiple times); can produce repeated tokens in training data; information loss for co-reference/control structures; sequence length can increase due to repeated visits.",
            "failure_cases": "Produces errors when reentrancies/control structures are important (e.g., wrong pronoun generation, duplicated mentions); worse performance on examples with many reentrancies (graph model shows Meteor +3.1% over sequential for &gt;6 reentrancies).",
            "uuid": "e8828.0",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "ANONYMIZATION",
            "name_full": "Anonymization of names and rare words in linearized AMR",
            "brief_description": "Preprocessing that replaces names and rare tokens in the AMR linearization with coarse category placeholders to reduce data sparsity for seq2seq models.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "anonymization (coarse-category replacement)",
            "representation_description": "Replace names and rare lexical items with category placeholders (e.g., person_name_0) in the linearized AMR sequence to reduce sparsity; alternative mentioned is a copy mechanism which was not used here.",
            "graph_type": "AMR semantic graphs (applied after linearization)",
            "conversion_method": "During preprocessing of the linearized AMR sequence, replace identified rare tokens and names with anonymized placeholders.",
            "downstream_task": "AMR-to-text generation (reducing OOVs for seq2seq training)",
            "performance_metrics": "No isolated numeric metric in paper; used as part of the SEQ pipeline whose overall test scores are reported (see sequential baseline).",
            "comparison_to_others": "Anonymization is the approach used in this paper following Konstas et al. (2017); authors note alternative copy mechanism (Gulcehre et al., 2016) but did not experiment with it here.",
            "advantages": "Reduces data sparsity and OOV issues for seq2seq models; simple to implement.",
            "disadvantages": "Hides lexical detail that must be recovered later (post-processing or copy mechanism); may reduce naturalness if not properly de-anonymized.",
            "failure_cases": "Not directly reported, but anonymization may remove distinctions needed for tense/number/gender decisions if not compensated elsewhere.",
            "uuid": "e8828.1",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "LEVI_TRANSFORM",
            "name_full": "Levi graph transformation of labeled AMR edges",
            "brief_description": "Transforms labeled AMR edges into unlabeled edges by promoting edge labels to nodes (Levi graph), reducing the need to model many distinct labeled-edge parameters.",
            "citation_title": "Finite geometrical systems",
            "mention_or_use": "use",
            "representation_name": "Levi graph (edge-label-to-node conversion)",
            "representation_description": "Replace each labeled edge (i, label, j) with two unlabeled edges by adding the label as a node: (i, label) and (label, j). This yields an unlabeled graph with label nodes as first-class nodes.",
            "graph_type": "AMR semantic graphs (labeled directed graphs)",
            "conversion_method": "For each edge triple (parent, label, child) insert the label into the node set and connect parent-&gt;label and label-&gt;child as unlabeled edges; effectively the AMR is converted into its Levi graph.",
            "downstream_task": "Used to prepare AMR graphs for structural encoders (GCN, TreeLSTM) and to reduce parameters for graph encoders in AMR-to-text generation.",
            "performance_metrics": "Used within the graph encoders that achieve test BLEU (GRAPH GCNSEQ) 24.40 (LDC2015E86) and 24.54 (LDC2017T10); no isolated metric for Levi transform alone.",
            "comparison_to_others": "Adopted to reduce number of parameters as in prior work (Beck et al., 2018); alternative is encoding labeled edges directly which increases parameters; paper follows Levi as in Beck et al.",
            "advantages": "Reduces the number of distinct edge-parameter matrices needed (unlabeled edges); allows existing unlabeled GCN formulations to be applied.",
            "disadvantages": "Increases node set size (labels become nodes) which may increase graph size/sequence length in linearization; requires models to learn embeddings for label-nodes.",
            "failure_cases": "Not specifically reported; implicit trade-off between parameter reduction and increased graph node count.",
            "uuid": "e8828.2",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "TREE_CONVERSION_DUPLICATE",
            "name_full": "Tree conversion via duplicating reentrant nodes (node-splitting)",
            "brief_description": "Transforms AMR graphs into trees by replacing nodes with multiple incoming edges by multiple identical nodes (one per incoming edge), thereby removing reentrancies.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "tree conversion by node duplication (reentrancy removal)",
            "representation_description": "For nodes with n&gt;1 incoming edges, create n identically labeled nodes each with a single incoming edge so the structure becomes a tree; used to enable TreeLSTM/Tree GCN encoders that require tree inputs.",
            "graph_type": "AMR semantic graphs (converted to trees)",
            "conversion_method": "Preprocess AMR graphs by identifying nodes with multiple parents and duplicating them (one copy per incoming edge) so resulting structure is a tree (no reentrancies).",
            "downstream_task": "AMR-to-text generation using tree encoders (TreeLSTM or tree-structured GCN followed by BiLSTM).",
            "performance_metrics": "Test (LDC2015E86): TREE (GCNSEQ without reentrancies) BLEU 23.93, METEOR 23.32. Test (LDC2017T10): BLEU 24.06, METEOR 23.62.",
            "comparison_to_others": "Tree encoder outperforms sequential baseline but is outperformed by full graph encoder (GRAPH) which preserves reentrancies; TREE underperforms GRAPH especially on inputs with many reentrancies.",
            "advantages": "Enables use of tree-structured encoders (TreeLSTM/TreeGCN) that can capture hierarchical relations; simpler than full graph modeling while retaining structural info except reentrancies.",
            "disadvantages": "Losess reentrancy/co-reference information by construction; artificial duplication might confuse models or distort statistics of node occurrences.",
            "failure_cases": "Per manual examples, tree representations sometimes omit pronouns or otherwise mishandle co-reference/control structures that require reentrancy information (tree model missed pronoun in example).",
            "uuid": "e8828.3",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "BiLSTM_SEQ_ENCODER",
            "name_full": "Bidirectional LSTM sequential encoder",
            "brief_description": "A BiLSTM encoder over the linearized AMR token sequence that produces context-aware token embeddings for attention-based decoders.",
            "citation_title": "Speech recognition with deep recurrent neural networks",
            "mention_or_use": "use",
            "representation_name": "Bidirectional LSTM encoding of linearized AMR",
            "representation_description": "Apply a BiLSTM to the linearized AMR sequence x1..xN to obtain contextual embeddings e1..eN; these embeddings are used by the decoder with attention.",
            "graph_type": "Linearized AMR sequences (derived from AMR graphs)",
            "conversion_method": "No graph-to-sequence conversion beyond linearization; the BiLSTM consumes the linearized sequence tokens.",
            "downstream_task": "AMR-to-text generation (seq2seq pipeline)",
            "performance_metrics": "As part of SEQ baseline: Test LDC2015E86 BLEU 21.43, METEOR 21.53; dev ablations show stacked variants improve when BiLSTM is combined with structural encoders.",
            "comparison_to_others": "BiLSTM-only (SEQ) is outperformed by structural encoders (TREE and GRAPH) when structural information is integrated. Also, models where structural encoder refines embeddings before BiLSTM (sequence-on-top-of-structure) perform better than structure-on-top-of-sequence.",
            "advantages": "Strong at encoding sequential context; well-supported and effective when given refined input embeddings.",
            "disadvantages": "By itself, cannot represent graph reentrancies or arbitrary graph structure; depends on linearization which causes information loss for reentrancies.",
            "failure_cases": "Tends to duplicate mentions or choose incorrect pronouns/structure when AMR reentrancies/control structures are important (examples in manual inspection).",
            "uuid": "e8828.4",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "TREELSTM_ENCODER",
            "name_full": "Bidirectional Child-Sum TreeLSTM encoder",
            "brief_description": "Encodes tree-structured AMRs using a Child-Sum TreeLSTM (bottom-up) followed by a top-down pass to capture parent and child information; produces node embeddings used with attention.",
            "citation_title": "Improved semantic representations from tree-structured long short-term memory networks",
            "mention_or_use": "use",
            "representation_name": "Child-sum TreeLSTM (bidirectional: bottom-up + top-down)",
            "representation_description": "Bottom-up Child-Sum TreeLSTM computes h^uparrow by summing children states; a top-down LSTM pass computes h^downarrow using parent states; final node embedding is concatenation [h^downarrow ; h^uparrow].",
            "graph_type": "Tree-structured AMRs (AMR graphs preprocessed to remove reentrancies)",
            "conversion_method": "AMR graphs are converted to trees via node duplication; TreeLSTM is applied over the tree structure to compute node and root representations.",
            "downstream_task": "AMR-to-text generation (tree-to-sequence pipeline), with TreeLSTM outputs optionally fed into a BiLSTM and attention decoder.",
            "performance_metrics": "Test (LDC2015E86): TREE (Tree/GCNSEQ variants) reported BLEU ~23.93 and METEOR ~23.32 (GCN-based TREE was best tree variant). Dev ablations: TREELSTMSEQ achieved BLEU 22.26 dev and METEOR 22.87.)",
            "comparison_to_others": "TreeLSTM-based models outperform pure sequential baseline but underperform graph encoders that preserve reentrancies. TreeLSTMs are less sensitive to removing the BiLSTM (RNN) component than GCNs, as TreeLSTM already contains LSTM gates.",
            "advantages": "Captures hierarchical structure and parent-child context; can model tree-based dependencies better than plain BiLSTM over linearization.",
            "disadvantages": "Requires pre-processing to remove reentrancies (node duplication), thus cannot directly model co-reference/control structures; computational overhead of tree traversals.",
            "failure_cases": "Fails on phenomena that rely on preservation of reentrancy (co-reference/control) — manual examples show missing pronouns or imperfect handling of possessives when reentrancies removed.",
            "uuid": "e8828.5",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "GCN_ENCODER",
            "name_full": "Graph Convolutional Network encoder (non-recurrent GCN)",
            "brief_description": "Encodes AMR graphs by iteratively aggregating neighbor node embeddings via direction-aware linear transforms and non-linearities (Graph Convolutional Network), producing node-wise embeddings used by attention decoders.",
            "citation_title": "Semisupervised classification with graph convolutional networks",
            "mention_or_use": "use",
            "representation_name": "Graph Convolutional Network (GCN)",
            "representation_description": "At each GCN layer, a node's embedding is updated by summing transformed embeddings of its immediate neighbors (with direction-specific parameter matrices), applying a non-linearity (ReLU) and optional highway connections; multiple layers propagate information across the graph.",
            "graph_type": "AMR semantic graphs (both with preserved reentrancies—graphs—or with reentrancies removed—trees)",
            "conversion_method": "GCN is applied directly to the AMR graph (after Levi transform for label nodes). For tree experiments, the same GCN is applied to the tree-converted graph. Final GCN layer outputs provide node embeddings; commonly these embeddings are used as input to a BiLSTM (GCNSEQ) or fed into decoder with attention.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence); used both as pure structural encoder and stacked with BiLSTM.",
            "performance_metrics": "Best graph model (GCNSEQ preserving reentrancies) TEST: LDC2015E86 BLEU 24.40, METEOR 23.60; LDC2017T10 BLEU 24.54, METEOR 24.07. Dev ablation: GCN (no BiLSTM) DEV BLEU 15.83, METEOR 17.76 showing large drop without sequential component. Contrastive pronoun tasks: GRAPH accuracies — Antecedent 96.02%, Type 96.49%, Number 95.11%, Gender 95.79%. Improvements vs sequential on long-range deps: Meteor deltas +1.22 (0-10), +2.05 (11-50), +3.04 (51-200) relative to sequential baseline.",
            "comparison_to_others": "GCNSEQ (sequence-on-top-of-structure) outperforms SEQ (BiLSTM-only) and TREE (tree GCNSEQ without reentrancies). GCN without a sequential BiLSTM performs much worse, indicating importance of combining structural and sequential components. Paper compares its GCN approach favorably to Song et al. (2018) and Beck et al. (2018) and obtains state-of-the-art BLEU on both datasets.",
            "advantages": "Explicitly models graph structure including reentrancies, leading to improved handling of co-reference/control and long-range dependencies; yields best reported BLEU/METEOR in paper when stacked before BiLSTM (GCNSEQ). Robust improvements especially on graphs with many reentrancies (Meteor +3.1% vs sequential for &gt;6 reentrancies).",
            "disadvantages": "Standalone (non-stacked) GCNs underperform dramatically; require stacking with sequential models (BiLSTM) for good results. Additional model complexity and hyperparameters (edge dropout, highway connections).",
            "failure_cases": "GCN-only (no BiLSTM) shows catastrophic drop (dev BLEU ~15.8) indicating failure if not combined with sequence modeling; tense information not encoded in AMR leads to wrong verb tense regardless of graph encoding; contrastive tests show only modest advantages for antecedent/pronoun-type errors (graph outperforms seq mainly on number/gender replacements).",
            "uuid": "e8828.6",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "SEQUENCE_ON_TOP_OF_STRUCTURE (GCNSEQ)",
            "name_full": "Sequence-on-top-of-structure stacking (structural encoder before BiLSTM, e.g., GCNSEQ / TREELSTMSEQ)",
            "brief_description": "A stacking method where the structural encoder (GCN or TreeLSTM) first refines input embeddings using graph structure, and the refined embeddings are then processed by a BiLSTM sequential encoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "sequence-on-top-of-structure stacking (GCNSEQ / TREELSTMSEQ)",
            "representation_description": "Apply structural encoder to raw input node embeddings to inject graph information, then feed the structural encoder outputs as input embeddings to a BiLSTM which captures sequential context; final node embeddings used by attention decoder.",
            "graph_type": "AMR semantic graphs (both full graphs preserving reentrancies and trees with reentrancies removed)",
            "conversion_method": "No graph-to-sequence conversion beyond initial linearization for BiLSTM alignment; structural encoder consumes graph (after Levi transform) and produces refined token/node embeddings which the BiLSTM consumes.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence pipeline combining graph structure and sequence modeling)",
            "performance_metrics": "GCNSEQ (graph-preserving) achieved TEST: LDC2015E86 BLEU 24.40, METEOR 23.60; LDC2017T10 BLEU 24.54, METEOR 24.07. On dev, GCNSEQ numbers were BLEU 23.62/METEOR 23.77 (as a variant) and outperform structure-on-top-of-sequence variants. Ablations show removing the BiLSTM (i.e., using only structural encoder) drastically reduces performance (GCN-only dev BLEU 15.83).",
            "comparison_to_others": "Sequence-on-top-of-structure consistently outperforms structure-on-top-of-sequence (applying structural encoder before the BiLSTM yields better scores). GCNSEQ (graph-preserving) outperforms GCN-based TREE variant and SEQ baseline; among all models GCNSEQ is best in this paper.",
            "advantages": "Combines the strengths of structural graph encoding (captures reentrancies and long-range graph relations) and sequential modeling (captures ordering), leading to best overall performance; particularly beneficial for examples with many reentrancies and long-range dependencies.",
            "disadvantages": "More complex architecture (two encoders) and higher computational cost; requires careful hyperparameter tuning (GCN layers, highway, dropout, BiLSTM layers).",
            "failure_cases": "When BiLSTM is removed, performance collapses (GCN-only failure). Some linguistic properties not encoded in AMR (e.g., tense) remain failure sources despite stacking.",
            "uuid": "e8828.7",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "STRUCTURE_ON_TOP_OF_SEQUENCE (SEQGCN/SEQTREELSTM)",
            "name_full": "Structure-on-top-of-sequence stacking (structural encoder after BiLSTM)",
            "brief_description": "A stacking method where a BiLSTM first encodes the linearized AMR sequence and the resulting hidden states are fed into a structural encoder (GCN or TreeLSTM) to further incorporate graph relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "structure-on-top-of-sequence stacking (SEQGCN / SEQTREELSTM)",
            "representation_description": "First compute BiLSTM hidden states over linearized AMR tokens to capture sequential context, then initialize a structural encoder with these states to inject graph topology information; outputs feed the decoder via attention.",
            "graph_type": "AMR semantic graphs (applied to linearized sequence with graph relations)",
            "conversion_method": "Linearize AMR; compute BiLSTM states; use those states as input node embeddings to a structural encoder (GCN or TreeLSTM) that operates over the graph structure.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "In dev experiments structure-on-top-of-sequence variants perform worse than sequence-on-top-of-structure. Example dev numbers: SEQGCN dev BLEU around 21.84-22.06 depending on graph/tree modes (Table 1); SEQTREELSTM dev BLEU 21.84 and TREELSTMSEQ (sequence-on-top) 22.26 showing structure-on-top generally inferior.",
            "comparison_to_others": "Structure-on-top-of-sequence underperforms the alternative order (sequence-on-top-of-structure) in this paper; stacking order matters and structural encoder-before-BiLSTM yielded better results.",
            "advantages": "Allows structural encoder to act as a post-hoc refinement of sequential context; reuses established BiLSTM pipeline.",
            "disadvantages": "Empirically less effective than applying structure first; may fail to provide the BiLSTM with sufficiently informative initial embeddings.",
            "failure_cases": "Lower scores than GCNSEQ/TREELSTMSEQ; does not capture benefits of preserving reentrancies as effectively as sequence-on-top-of-structure.",
            "uuid": "e8828.8",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "GRAPH2SEQ & RELATED",
            "name_full": "Graph-to-sequence neural architectures (e.g., Graph2Seq, Gated Graph Neural Networks, LSTM-based graph encoders)",
            "brief_description": "General class of models that directly encode graph-structured inputs (via GNNs/GRUs/LSTMs over graph neighborhoods) and decode sequences; referenced as related approaches to AMR-to-text graph encoders.",
            "citation_title": "Graph2seq: Graph to sequence learning with attention-based neural networks",
            "mention_or_use": "mention",
            "representation_name": "graph-to-sequence encoders (GGNN, Graph2Seq, LSTM-based graph encoders)",
            "representation_description": "Nodes are updated by aggregating neighbor node states; updates can be implemented as GCN layers (sum + linear + nonlinearity), gated GRU-style updates with neighborhood gating (GGNN), or LSTM-style updates; final node embeddings feed an attention-based decoder.",
            "graph_type": "AMR semantic graphs and other structured graphs",
            "conversion_method": "Operate directly on graph adjacency/neighborhood (optionally Levi-transformed) rather than converting to a sequence; produce node-wise embeddings for decoding.",
            "downstream_task": "AMR-to-text generation and other graph-to-sequence tasks",
            "performance_metrics": "Paper references Song et al. (2018) and Beck et al. (2018) as recent graph-to-sequence systems; those reported BLEU for comparison (e.g., Song et al. reported 23.30 in prior work). This paper's GCNSEQ outperforms those reported results (achieving 24.40 BLEU on LDC2015E86).",
            "comparison_to_others": "The paper positions its non-recurrent GCN variant and stacking approach relative to recurrent graph encoders (GRU/LSTM-based) in Song et al. (2018) and Beck et al. (2018), claiming design/implementation differences (edge labels, directionality) may explain performance gaps; GCNSEQ achieves higher BLEU in this work.",
            "advantages": "Directly models graph topology and reentrancies; flexible in choice of update dynamics (gated, convolutional, recurrent).",
            "disadvantages": "Design choices (gating vs non-recurrent updates, edge label handling) affect performance; recurrent graph updates may be more expensive; standalone graph encoders without sequence modeling can underperform.",
            "failure_cases": "Noted that different implementations vary in handling of edge labels and directionality; in this paper, GCN-only (no BiLSTM) is insufficient. Prior works required design trade-offs.",
            "uuid": "e8828.9",
            "source_info": {
                "paper_title": "Structural Neural Encoders for AMR-to-text Generation",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation.",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Graph2seq: Graph to sequence learning with attention-based neural networks",
            "rating": 2,
            "sanitized_title": "graph2seq_graph_to_sequence_learning_with_attentionbased_neural_networks"
        },
        {
            "paper_title": "Improved semantic representations from tree-structured long short-term memory networks",
            "rating": 1,
            "sanitized_title": "improved_semantic_representations_from_treestructured_long_shortterm_memory_networks"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Encoding sentences with graph convolutional networks for semantic role labeling",
            "rating": 1,
            "sanitized_title": "encoding_sentences_with_graph_convolutional_networks_for_semantic_role_labeling"
        }
    ],
    "cost": 0.0182345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Structural Neural Encoders for AMR-to-text Generation</h1>
<p>Marco Damonte Shay B. Cohen<br>School of Informatics, University of Edinburgh<br>10 Crichton Street, Edinburgh EH8 9AB, UK<br>m.damonte@sms.ed.ac.uk<br>scohen@inf.ed.ac.uk</p>
<h4>Abstract</h4>
<p>AMR-to-text generation is a problem recently introduced to the NLP community, in which the goal is to generate sentences from Abstract Meaning Representation (AMR) graphs. Sequence-to-sequence models can be used to this end by converting the AMR graphs to strings. Approaching the problem while working directly with graphs requires the use of graph-to-sequence models that encode the AMR graph into a vector representation. Such encoding has been shown to be beneficial in the past, and unlike sequential encoding, it allows us to explicitly capture reentrant structures in the AMR graphs. We investigate the extent to which reentrancies (nodes with multiple parents) have an impact on AMR-to-text generation by comparing graph encoders to tree encoders, where reentrancies are not preserved. We show that improvements in the treatment of reentrancies and long-range dependencies contribute to higher overall scores for graph encoders. Our best model achieves 24.40 BLEU on LDC2015E86, outperforming the state of the art by 1.1 points and 24.54 BLEU on LDC2017T10, outperforming the state of the art by 1.24 points.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR; Banarescu et al. 2013) is a semantic graph representation that abstracts away from the syntactic realization of a sentence, where nodes in the graph represent concepts and edges represent semantic relations between them. AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies. For instance, the AMR of Figure 1(a) contains a reentrancy between finger and he, caused by the possessive pronoun his. AMR-to-text generation is the task of automatically generating natural language from AMR
<img alt="img-0.jpeg" src="img-0.jpeg" />
(a)
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b)
eat-01 :arg0 he :arg1 pizza :instr. finger :part-of he
(c)
<img alt="img-2.jpeg" src="img-2.jpeg" />
(d)
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 1: (a) AMR for the sentence He ate the pizza with his fingers and different input representations: (b) sequential; (c) tree-structured; (d) graph-structured. The nodes and edges in bold highlight a reentrancy.
graphs.
Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task (Konstas et al., 2017; Song et al., 2018; Beck et al., 2018). In order to use sequence-to-sequence models, Konstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al.,</p>
<p>2017) or tree-structured (Liu et al., 2015; Takase et al., 2016) data, while other work maintained them but did not analyze their impact on performance (e.g., Song et al., 2018; Beck et al., 2018). Damonte et al. (2017) showed that state-of-the-art parsers do not perform well in predicting reentrant structures, while van Noord and Bos (2017) compared different pre- and post-processing techniques to improve the performance of sequence-to-sequence parsers with respect to reentrancies. It is not yet clear whether explicit encoding of reentrancies is beneficial for generation.</p>
<p>In this paper, we compare three types of encoders for AMR: 1) sequential encoders, which reduce AMR graphs to sequences; 2) tree encoders, which ignore reentrancies; and 3) graph encoders. We pay particular attention to two phenomena: reentrancies, which mark co-reference and control structures, and long-range dependencies in the AMR graphs, which are expected to benefit from structural encoding. The contributions of the paper are two-fold:</p>
<ul>
<li>We present structural encoders for the encoder/decoder framework and show the benefits of graph encoders not only compared to sequential encoders but also compared to tree encoders, which have not been studied so far for AMR-to-text generation.</li>
<li>We show that better treatment of reentrancies and long-range dependencies contributes to improvements in the graph encoders.</li>
</ul>
<p>Our best model, based on a graph encoder, achieves state-of-the-art results for both the LDC2015E86 dataset (24.40 on BLEU and 23.79 on Meteor) and the LDC2017T10 dataset (24.54 on BLEU and 24.07 on Meteor).</p>
<h2>2 Input Representations</h2>
<p>Graph-structured AMRs AMRs are normally represented as rooted and directed graphs:</p>
<p>$$
\begin{aligned}
&amp; G_{0}=\left(V_{0}, E_{0}, L\right) \
&amp; V_{0}=\left{v_{1}, v_{2}, \ldots, v_{n}\right} \
&amp; \text { root } \in V_{0}
\end{aligned}
$$</p>
<p>where $V_{0}$ are the graph vertices (or nodes) and root is a designated root node in $V_{0}$. The edges in the AMR are labeled:</p>
<p>$$
\begin{aligned}
&amp; E_{0} \subseteq V_{0} \times L \times V_{0} \
&amp; L=\left{\ell_{1}, \ell_{2}, \ldots, \ell_{n^{\prime}}\right}
\end{aligned}
$$</p>
<p>Each edge $e \in E_{0}$ is a triple: $e=(i$, label, $j)$, where $i \in V_{0}$ is the parent node, label $\in L$ is the edge label and $j \in V_{0}$ is the child node.</p>
<p>In order to obtain unlabeled edges, thus decreasing the total number of parameters required by the models, we replace each labeled edge $e=(i$, label, $j)$ with two unlabeled edges: $e_{1}=$ $(i$, label $), e_{2}=($ label, $j)$ :</p>
<p>$$
\begin{aligned}
&amp; G=(V, E) \
&amp; V=V_{0} \cup L=\left{v_{1}, \ldots, v_{n}, \ell_{1}, \ldots, \ell_{n^{\prime}}\right} \
&amp; E \subseteq\left(V_{0} \times L\right) \cup\left(L \times V_{0}\right)
\end{aligned}
$$</p>
<p>Each unlabeled edge $e \in E$ is a pair: $e=(i, j)$, where one of the following holds:</p>
<ol>
<li>$i \in V_{0}$ and $j \in L$;</li>
<li>$i \in L$ and $j \in V_{0}$.</li>
</ol>
<p>For instance, the edge between eat-01 and he with label :arg0 of Figure 1(a) is replaced by two edges in Figure 1(d): an edge between eat01 and :arg0 and another one between :arg0 and he. The process, also used in Beck et al. (2018), tranforms the input graph into its equivalent Levi graph (Levi, 1942).</p>
<p>Tree-structured AMRs In order to obtain tree structures, it is necessary to discard the reentrancies from the AMR graphs. Similarly to Takase et al. (2016), we replace nodes with $n&gt;1$ incoming edges with $n$ identically labeled nodes, each with a single incoming edge.</p>
<p>Sequential AMRs Following Konstas et al. (2017), the input sequence is a linearized and anonymized AMR graph. Linearization is used to convert the graph into a sequence:</p>
<p>$$
\begin{aligned}
&amp; x=x_{1}, \ldots, x_{N} \
&amp; x_{i} \in V
\end{aligned}
$$</p>
<p>The depth-first traversal of the graph defines the indexing between nodes and tokens in the sequence. For instance, the root node is $x_{1}$, its leftmost child is $x_{2}$ and so on. Nodes with multiple parents are visited more than once. At each visit, their labels are repeated in the sequence, effectively losing reentrancy information, as shown in Figure 1(b).</p>
<p>Anonymization removes names and rare words with coarse categories to reduce data sparsity. An alternative to anonymization is to employ a copy</p>
<p>mechanism (Gulcehre et al., 2016), where the models learn to copy rare words from the input itself. In this paper, we follow the anonymization approach.</p>
<h2>3 Encoders</h2>
<p>In this section, we review the encoders adopted as building blocks for our tree and graph encoders.</p>
<h3>3.1 Recurrent Neural Network Encoders</h3>
<p>We reimplement the encoder of Konstas et al. (2017), where the sequential linearization is the input to a bidirectional LSTM (BiLSTM; Graves et al. 2013) network. The hidden state of the BiLSTM at step $i$ is used as a context-aware word representation of the $i$-th token in the sequence:</p>
<p>$$
e_{1: N}=\operatorname{BiLSTM}\left(x_{1: N}\right)
$$</p>
<p>where $e_{i} \in \mathbb{R}^{d}, d$ is the size of the output embeddings.</p>
<h3>3.2 TreeLSTM Encoders</h3>
<p>Tree-Structured Long Short-Term Memory Networks (TreeLSTM; Tai et al. 2015) have been introduced primarily as a way to encode the hierarchical structure of syntactic trees (Tai et al., 2015), but they have also been applied to AMR for the task of headline generation (Takase et al., 2016). TreeLSTMs assume tree-structured input, so AMR graphs must be preprocessed to respect this constraint: reentrancies, which play an essential role in AMR, must be removed, thereby transforming the graphs into trees.</p>
<p>We use the Child-Sum variant introduced by Tai et al. (2015), which processes the tree in a bottomup pass. When visiting a node, the hidden states of its children are summed up in a single vector which is then passed into recurrent gates.</p>
<p>In order to use information from both incoming and outgoing edges (parents and children), we employ bidirectional TreeLSTMs (Eriguchi et al., 2016), where the bottom-up pass is followed by a top-down pass. The top-down state of the root node is obtained by feeding the bottom-up state of the root node through a feed-forward layer:</p>
<p>$$
h_{\text {root }}^{\uparrow}=\tanh \left(W_{r} h_{\text {root }}^{\uparrow}+b\right)
$$</p>
<p>where $h_{i}^{\uparrow}$ is the hidden state of node $x_{i} \in V$ for the bottom-up pass and $h_{i}^{\downarrow}$ is the hidden state of node $x_{i}$ for the top-down pass.</p>
<p>The bottom up states for all other nodes are computed with an LSTM, with the cell state given by their parent nodes:</p>
<p>$$
h_{i}^{\downarrow}=\operatorname{LSTM}\left(h_{p(i)}^{\uparrow}, h_{i}^{\uparrow}\right)
$$</p>
<p>where $p(i)$ is the parent of node $x_{i}$ in the tree. The final hidden states are obtained by concatenating the states from the bottom-up pass and the topdown pass:</p>
<p>$$
h_{i}=\left[h_{i}^{\downarrow} ; h_{i}^{\uparrow}\right]
$$</p>
<p>The hidden state of the root node is usually used as a representation for the entire tree. In order to use attention over all nodes, as in traditional NMT (Bahdanau et al., 2015), we can however build node embeddings by extracting the hidden states of each node in the tree:</p>
<p>$$
e_{1: N}=h_{1: N}
$$</p>
<p>where $e_{i} \in \mathbb{R}^{d}, d$ is the size of the output embeddings.</p>
<p>The encoder is related to the TreeLSTM encoder of Takase et al. (2016), which however encodes labeled trees and does not use a top-down pass.</p>
<h3>3.3 Graph Convolutional Network Encoders</h3>
<p>Graph Convolutional Network (GCN; Duvenaud et al. 2015; Kipf and Welling 2016) is a neural network architecture that learns embeddings of nodes in a graph by looking at its nearby nodes. In Natural Language Processing, GCNs have been used for Semantic Role Labeling (Marcheggiani and Titov, 2017), NMT (Bastings et al., 2017), Named Entity Recognition (Cetoli et al., 2017) and text generation (Marcheggiani and Perez-Beltrachini, 2018).</p>
<p>A graph-to-sequence neural network was first introduced by Xu et al. (2018). The authors review the similarities between their approach, GCN and another approach, based on GRUs ( Li et al., 2015). The latter recently inspired a graph-to-sequence architecture for AMR-to-text generation (Beck et al., 2018). Simultaneously, Song et al. (2018) proposed a graph encoder based on LSTMs.</p>
<p>The architectures of Song et al. (2018) and Beck et al. (2018) are both based on the same core computation of a GCN, which sums over the embeddings of the immediate neighborhood of each</p>
<p>node:</p>
<p>$$
h_{i}^{(k+1)}=\sigma\left(\sum_{j \in \mathcal{N}(i)} W_{(j, i)}^{(k)} h_{j}^{(k)}+b^{(k)}\right)
$$</p>
<p>where $h_{i}^{(k)}$ is the embeddings of node $x_{i} \in V$ at layer $k, \sigma$ is a non-linear activation function, $\mathcal{N}(i)$ is the set of the immediate neighbors of $x_{i}$, $W_{(j, i)}^{(k)} \in \mathbb{R}^{m \times m}$ and $b^{(k)} \in \mathbb{R}^{m}$, with $m$ being the size of the embeddings.</p>
<p>It is possible to use recurrent networks to model the update of the node embeddings. Specifically, Beck et al. (2018) uses a GRU layer where the gates are modeled as GCN layers. Song et al. (2018) did not use the activation function $\sigma$ and perform an LSTM update instead.</p>
<p>The systems of Song et al. (2018) and Beck et al. (2018) further differ in design and implementation decisions such as in the use of edge label and edge directionality. Throughout the rest of the paper, we follow the traditional, non-recurrent, implementation of GCN also adopted in other NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Cetoli et al., 2017). In our experiments, the node embeddings are computed as follows:</p>
<p>$$
h_{i}^{(k+1)}=\sigma\left(\sum_{j \in \mathcal{N}(i)} W_{\operatorname{dir}(j, i)}^{(k)} h_{j}^{(k)}+b^{(k)}\right)
$$</p>
<p>where $\operatorname{dir}(j, i)$ indicates the direction of the edge between $x_{j}$ and $x_{i}$ (i.e., outgoing or incoming edge). The hidden vectors from the last layer of the GCN network are finally used to represent each node in the graph:</p>
<p>$$
e_{1: N}=h_{1}^{(K)}, \ldots, h_{N}^{(K)}
$$</p>
<p>where K is the number of GCN layers used, $e_{i} \in$ $\mathbb{R}^{d}, d$ is the size of the output embeddings.</p>
<p>To regularize the models we apply dropout (Srivastava et al., 2014) as well as edge dropout (Marcheggiani and Titov, 2017). We also include highway connections (Srivastava et al., 2015) between GCN layers.</p>
<p>While GCN can naturally be used to encode graphs, they can also be applied to trees by removing reentrancies from the input graphs. In the experiments of Section 5, we explore GCN-based models both as graph encoders (reentrancies are maintained) as well as tree encoders (reentrancies are ignored).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 2: Two ways of stacking recurrent and structural models. Left side: structure on top of sequence, where the structural encoders are applied to the hidden vectors computed by the BiLSTM. Right side: sequence on top of structure, where the structural encoder is used to create better embeddings which are then fed to the BiLSTM. The dotted lines refer to the process of converting the graph into a sequence or vice-versa.</p>
<h2>4 Stacking Encoders</h2>
<p>We aimed at stacking the explicit source of structural information provided by TreeLSTMs and GCNs with the sequential information which BiLSTMs extract well. This was shown to be effective for other tasks with both TreeLSTMs (Eriguchi et al., 2016; Chen et al., 2017) and GCNs (Marcheggiani and Titov, 2017; Cetoli et al., 2017; Bastings et al., 2017). In previous work, the structural encoders (tree or graph) were used on top of the BiLSTM network: first, the input is passed through the sequential encoder, the output of which is then fed into the structural encoder. While we experiment with this approach, we also propose an alternative solution where the BiLSTM network is used on top of the structural encoder: the input embeddings are refined by exploiting the explicit structural information given by the graph. The refined embeddings are then fed into the BiLSTM networks. See Figure 2 for a graphical representation of the two approaches. In our experiments, we found this approach to be more effective. Compared to models that interleave structural and recurrent components such as the systems of Song et al. (2018) and Beck et al. (2018), stacking the components allows us to test</p>
<p>for their contributions more easily.</p>
<h3>4.1 Structure on Top of Sequence</h3>
<p>In this setup, BiLSTMs are used as in Section 3.1 to encode the linearized and anonymized AMR. The context provided by the BiLSTM is a sequential one. We then apply either GCN or TreeLSTM on the output of the BiLSTM, by initializing the GCN or TreeLSTM embeddings with the BiLSTM hidden states. We call these models SEQGCN and SEQTREELSTM.</p>
<h3>4.2 Sequence on Top of Structure</h3>
<p>We also propose a different approach for integrating graph information into the encoder, by swapping the order of the BiLSTM and the structural encoder: we aim at using the structured information provided by the AMR graph as a way to refine the original word representations. We first apply the structural encoder to the input graphs. The GCN or TreeLSTM representations are then fed into the BiLSTM. We call these models GCNSEQ and TREELSTMSEQ.</p>
<p>The motivation behind this approach is that we know that BiLSTMs, given appropriate input embeddings, are very effective at encoding the input sequences. In order to exploit their strength, we do not amend their output but rather provide them with better input embeddings to start with, by explicitly taking the graph relations into account.</p>
<h2>5 Experiments</h2>
<p>We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evaluation metrics. ${ }^{1}$ We report results on the AMR dataset LDC2015E86 and LDC2017T10. All systems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8 . Batch size is set to $100 .^{2}$</p>
<p>We first evaluate the overall performance of the models, after which we focus on two phenomena that we expect to benefit most from structural</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Input | Model | BLEU | Meteor |
| :--: | :--: | :--: | :--: |
| Seq | SEQ | 21.40 | 22.00 |
| Tree | SEQTREELSTM | 21.84 | 22.34 |
|  | TREELSTMSEQ | 22.26 | 22.87 |
|  | TREELSTM | 22.07 | 22.57 |
|  | SEQGCN | 21.84 | 22.21 |
|  | GCNSEQ | 23.62 | 23.77 |
|  | GCN | 15.83 | 17.76 |
| Graph | SEQGCN | 22.06 | 22.18 |
|  | GCNSEQ | 23.95 | 24.00 |
|  | GCN | 15.94 | 17.76 |</p>
<p>Table 1: BLEU and Meteor (\%) scores on the development split of LDC2015E86.
encoders: reentrancies and long-range dependencies. Table 1 shows the comparison on the development split of the LDC2015E86 dataset between sequential, tree and graph encoders. The sequential encoder (SEQ) is a re-implementation of Konstas et al. (2017). We test both approaches of stacking structural and sequential components: structure on top of sequence (SEQTREELSTM and SEQGCN), and sequence on top of structure (TREELSTMSEQ and GCNSEQ). To inspect the effect of the sequential component, we run ablation tests by removing the RNNs altogether (TREELSTM and GCN). GCN-based models are used both as tree encoders (reentrancies are removed) and graph encoders (reentrancies are maintained).</p>
<p>For both TreeLSTM-based and GCN-based models, our proposed approach of applying the structural encoder before the RNN achieves better scores. This is especially true for GCN-based models, for which we also note a drastic drop in performance when the RNN is removed, highlighting the importance of a sequential component. On the other hand, RNN layers seem to have less impact on TreeLSTM-based models. This outcome is not unexpected, as TreeLSTMs already use LSTM gates in their computation.</p>
<p>The results show a clear advantage of tree and graph encoders over the sequential encoder. The best performing model is GCNSEQ, both as a tree and as a graph encoder, with the latter obtaining the highest results.</p>
<p>Table 2 shows the comparison between our best sequential (SEQ), tree (GCNSEQ without reentrancies, henceforth called TREE) and graph en-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">Meteor</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LDC2015E86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SEQ</td>
<td style="text-align: center;">21.43</td>
<td style="text-align: center;">21.53</td>
</tr>
<tr>
<td style="text-align: center;">TREE</td>
<td style="text-align: center;">23.93</td>
<td style="text-align: center;">23.32</td>
</tr>
<tr>
<td style="text-align: center;">GRAPH</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">23.60</td>
</tr>
<tr>
<td style="text-align: center;">Konstas et al. (2017)</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2018)</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LDC2017T10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SEQ</td>
<td style="text-align: center;">22.19</td>
<td style="text-align: center;">22.68</td>
</tr>
<tr>
<td style="text-align: center;">TREE</td>
<td style="text-align: center;">24.06</td>
<td style="text-align: center;">23.62</td>
</tr>
<tr>
<td style="text-align: center;">GRAPH</td>
<td style="text-align: center;">24.54</td>
<td style="text-align: center;">24.07</td>
</tr>
<tr>
<td style="text-align: center;">Beck et al. (2018)</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Scores on the test split of LDC2015E86 and LDC2017T10. Tree is the tree-based GCNSEQ and GRAPH is the graph-based GCNSEQ.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># reentrancies</th>
<th style="text-align: center;"># dev sents.</th>
<th style="text-align: center;"># test sents.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">619</td>
<td style="text-align: center;">622</td>
</tr>
<tr>
<td style="text-align: left;">$1-5$</td>
<td style="text-align: center;">679</td>
<td style="text-align: center;">679</td>
</tr>
<tr>
<td style="text-align: left;">$6-20$</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">70</td>
</tr>
</tbody>
</table>
<p>Table 3: Counts of reentrancies for the development and test split of LDC2017T10
coders (GCNSEQ with reentrancies, henceforth called GRAPH) on the test set of LDC2015E86 and LDC2017T10. We also include state-of-the-art results reported on these datasets for sequential encoding (Konstas et al., 2017) and graph encoding (Song et al., 2018; Beck et al., 2018). ${ }^{3}$ In order to mitigate the effects of random seeds, we train five models with different random seeds and report the results of the median model, according to their BLEU score on the development set (Beck et al., 2018). We achieve state-of-the-art results with both tree and graph encoders, demonstrating the efficacy of our GCNSeq approach. The graph encoder outperforms the other systems and previous work on both datasets. These results demonstrate the benefit of structural encoders over purely sequential ones as well as the advantage of explicitly including reentrancies. The differences between our graph encoder and that of Song et al. (2018) and Beck et al. (2018) were discussed in Section 3.3.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Differences, with respect to the sequential baseline, in the Meteor score of the test split of LDC2017T10 as a function of the number of reentrancies.</p>
<h3>5.1 Reentrancies</h3>
<p>Overall scores show an advantage of graph encoder over tree and sequential encoders, but they do not shed light into how this is achieved. Because graph encoders are the only ones to model reentrancies explicitly, we expect them to deal better with these structures. It is, however, possible that the other models are capable of handling these structures implicitly. Moreover, the dataset contains a large number of examples that do not involve any reentrancies, as shown in Table 3, so that the overall scores may not be representative of the ability of models to capture reentrancies. It is expected that the benefit of the graph models will be more evident for those examples containing more reentrancies. To test this hypothesis, we evaluate the various scenarios as a function of the number of reentrancies in each example, using the Meteor score as a metric. ${ }^{4}$</p>
<p>Table 4 shows that the gap between the graph encoder and the other encoders is widest for examples with more than six reentrancies. The $\mathrm{Me}-$ teor score of the graph encoder for these cases is $3.1 \%$ higher than the one for the sequential encoder and $2.3 \%$ higher than the score achieved by the tree encoder, demonstrating that explicitly encoding reentrancies is more beneficial than the overall scores suggest. Interestingly, it can also be observed that the graph model outperforms the tree model also for examples with no reentrancies, where tree and graph structures are identical. This suggests that preserving reentrancies in the training data has other beneficial effects. In Section 5.2 we explore one: better handling of long-range dependencies.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>5.1.1 Manual Inspection</h3>
<p>In order to further explore how the graph model handles reentrancies differently from the other models, we performed a manual inspection of the models' output. We selected examples containing reentrancies, where the graph model performs better than the other models. These are shown in Table 5. In Example (1), we note that the graph model is the only one that correctly predicts the phrase he finds out. The wrong verb tense is due to the lack of tense information in AMR graphs. In the sequential model, the pronoun is chosen correctly, but the wrong verb is predicted, while in the tree model the pronoun is missing. In Example (2), only the graph model correctly generates the phrase you tell them, while none of the models use people as the subject of the predicate can. In Example (3), both the graph and the sequential models deal well with the control structure caused by the recommend predicate. The sequential model, however, overgenerates a wh-clause. Finally, in Example (4) the tree and graph models deal correctly with the possessive pronoun to generate the phrase tell your ex, while the sequential model does not. Overall, we note that the graph model produces a more accurate output than sequential and tree models by generating the correct pronouns and mentions when control verbs and co-references are involved.</p>
<h3>5.1.2 Contrastive Pairs</h3>
<p>For a quantitative analysis of how the different models handle pronouns, we use a method to inspect NMT output for specific linguistic analysis based on contrastive pairs (Sennrich, 2017). Given a reference output sentence, a contrastive sentence is generated by introducing a mistake related to the phenomenon we are interested in evaluating. The probability that the model assigns to the reference sentence is then compared to that of the contrastive sentence. The accuracy of a model is determined by the percentage of examples in which the reference sentence has a higher probability than the contrastive sentence.</p>
<p>We produce contrastive examples by running CoreNLP (Manning et al., 2014) to identify coreferences, which are the primary cause of reentrancies, and introducing a mistake. When an expression has multiple mentions, the antecedent is repeated in the linearized AMR. For instance, the linearization of Figure 1(b) contains the token he twice, which instead appears only once in the sen-
tence. This repetition may result in generating the token he twice, rather than using a pronoun to refer back to it. To investigate this possible mistake, we replace one of the mentions with the antecedent (e.g., John ate the pizza with his fingers is replaced with John ate the pizza with John fingers, which is ungrammatical and as such should be less likely).</p>
<p>An alternative hypothesis is that even when the generation system correctly decides to predict a pronoun, it selects the wrong one. To test for this, we produce contrastive examples where a pronoun is replaced by either a different type of pronoun (e.g., John ate the pizza with his fingers is replaced with John ate the pizza with him fingers) or by the same type of pronoun but for a different number (John ate the pizza with their fingers) or different gender (John ate the pizza with her fingers). Note from Figure 1 that the graph-structured AMR is the one that more directly captures the relation between finger and he, and as such it is expected to deal better with this type of mistakes.</p>
<p>From the test split of LDC2017T10, we generated 251 contrastive examples due to antecedent replacements, 912 due to pronoun type replacements, 1840 due to number replacements and 95 due to gender replacements. ${ }^{5}$ The results are shown in Table 6. The sequential encoder performs surprisingly well at this task, with better or on par performance with respect to the tree encoder. The graph encoder outperforms the sequential encoder only for pronoun number and gender replacements. Future work is required to more precisely analyze if the different models cope with pronomial mentions in significantly different ways. Other approaches to inspect phenomena of co-reference and control verbs can also be explored, for instance by devising specific training objectives (Linzen et al., 2016).</p>
<h3>5.2 Long-range Dependencies</h3>
<p>When we encode a long sequence, interactions between items that appear distant from each other in the sequence are difficult to capture. The problem of long-range dependencies in natural language is well known for RNN architectures (Bengio et al., 1994). Indeed, the need to solve this problem motivated the introduction of LSTM models, which are known to model long-range dependencies better than traditional RNNs.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">(1)</th>
<th style="text-align: left;">REF</th>
<th style="text-align: left;">i dont tell him but he finds out,</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SEQ</td>
<td style="text-align: left;">i did n't tell him but he was out.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TREE</td>
<td style="text-align: left;">i do n't tell him but found out.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: left;">i do n't tell him but he found out.</td>
</tr>
<tr>
<td style="text-align: left;">(2)</td>
<td style="text-align: left;">REF</td>
<td style="text-align: left;">if you tell people they can help you,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SEQ</td>
<td style="text-align: left;">if you tell him, you can help you!</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TREE</td>
<td style="text-align: left;">if you tell person_name_0 you, you can help you.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: left;">if you tell them, you can help you.</td>
</tr>
<tr>
<td style="text-align: left;">(3)</td>
<td style="text-align: left;">REF</td>
<td style="text-align: left;">i'd recommend you go and see your doctor too.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SEQ</td>
<td style="text-align: left;">i recommend you go to see your doctor who is going to see your doctor.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TREE</td>
<td style="text-align: left;">you recommend going to see your doctor too.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: left;">i recommend you going to see your doctor too.</td>
</tr>
<tr>
<td style="text-align: left;">(4)</td>
<td style="text-align: left;">REF</td>
<td style="text-align: left;">(you) tell your ex that all communication needs to go through the lawyer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SEQ</td>
<td style="text-align: left;">(you) tell that all the communication go through lawyer.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">TREE</td>
<td style="text-align: left;">(you) tell your ex, tell your ex, the need for all the communication.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: left;">(you) tell your ex the need to go through a lawyer.</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of generation from AMR graphs containing reentrancies. REF is the reference sentence.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Antec.</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Num.</th>
<th style="text-align: center;">Gender</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SEQ</td>
<td style="text-align: center;">96.02</td>
<td style="text-align: center;">97.70</td>
<td style="text-align: center;">94.89</td>
<td style="text-align: center;">94.74</td>
</tr>
<tr>
<td style="text-align: left;">TREE</td>
<td style="text-align: center;">96.02</td>
<td style="text-align: center;">96.38</td>
<td style="text-align: center;">93.70</td>
<td style="text-align: center;">92.63</td>
</tr>
<tr>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: center;">96.02</td>
<td style="text-align: center;">96.49</td>
<td style="text-align: center;">95.11</td>
<td style="text-align: center;">95.79</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy (\%) of models, on the test split of LDC201T10, for different categories of contrastive errors: antecedent (Antec.), pronoun type (Type), number (Num.), and gender (Gender).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># max length</th>
<th style="text-align: center;"># dev sents.</th>
<th style="text-align: center;"># test sents.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$0-10$</td>
<td style="text-align: center;">292</td>
<td style="text-align: center;">307</td>
</tr>
<tr>
<td style="text-align: left;">$11-50$</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">297</td>
</tr>
<tr>
<td style="text-align: left;">$51-250$</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">18</td>
</tr>
</tbody>
</table>
<p>Table 7: Counts of longest dependencies for the development and test split of LDC2017T10</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Max dependency length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$0-10$</td>
<td style="text-align: center;">$11-50$</td>
<td style="text-align: center;">$51-200$</td>
</tr>
<tr>
<td style="text-align: left;">SEQ</td>
<td style="text-align: center;">50.49</td>
<td style="text-align: center;">36.28</td>
<td style="text-align: center;">24.14</td>
</tr>
<tr>
<td style="text-align: left;">TREE</td>
<td style="text-align: center;">-0.48</td>
<td style="text-align: center;">+1.66</td>
<td style="text-align: center;">+2.37</td>
</tr>
<tr>
<td style="text-align: left;">GRAPH</td>
<td style="text-align: center;">+1.22</td>
<td style="text-align: center;">+2.05</td>
<td style="text-align: center;">+3.04</td>
</tr>
</tbody>
</table>
<p>Table 8: Differences, with respect to the sequential baseline, in the Meteor score of the test split of LDC2017T10 as a function of the maximum dependency length.</p>
<p>Because the nodes in the graphs are not aligned with words in the sentence, AMR has no notion of distance between the nodes taking part in an edge. In order to define the length of an AMR edge, we resort to the AMR linearization discussed in Section 2. Given the linearization of the AMR $x_{1}, \ldots, x_{N}$, as discussed in Section 2, and an edge between two nodes $x_{i}$ and $x_{j}$, the length of the edge is defined as $|j-i|$. For instance, in the AMR of Figure 1, the edge between eat-01 and :instrument is a dependency of length five, because of the distance between the two words in the linearization eat-01 :arg0 he :arg1 pizza :instrument. We then compute the maximum dependency length for each AMR graph.</p>
<p>To verify the hypothesis that long-range dependencies contribute to the improvements of graph models, we compare the models as a function of the maximum dependency length in each example. Longer dependencies are sometimes caused by reentrancies, as in the dependency between :part-of and he in Figure 1. To verify that the contribution in terms of longer dependencies is complementary to that of reentrancies, we exclude sentences with reentrancies from this analysis. Table 7 shows the statistics for this measure. Results are shown in Table 8. The graph encoder always outperforms both the sequential and the tree encoder. The gap with the sequential encoder increases for longer dependencies. This indicates that longer dependencies are an important factor</p>
<p>in improving results for both tree and graph encoders, especially for the latter.</p>
<h2>6 Conclusions</h2>
<p>We introduced models for AMR-to-text generation with the purpose of investigating the difference between sequential, tree and graph encoders. We showed that encoding reentrancies improves overall performance. We observed bigger benefits when the input AMR graphs have a larger number of reentrant structures and longer dependencies. Our best graph encoder, which consists of a GCN wired to a BiLSTM network, improves over the state of the art on all tested datasets. We inspected the differences between the models, especially in terms of co-references and control structures. Further exploration of graph encoders is left to future work, which may result crucial to improve performance further.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the three anonymous reviewers and Adam Lopez, Ioannis Konstas, Diego Marcheggiani, Sorcha Gilroy, Sameer Bansal, Ida Szubert and Clara Vania for their help and comments. This research was supported by a grant from Bloomberg and by the H2020 project SUMMA, under grant agreement 688139.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of $I C L R$.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. Linguistic Annotation Workshop.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural machine translation. In Proceedings of EMNLP.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. Proceedings of ACL.</p>
<p>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166.</p>
<p>Alberto Cetoli, Stefano Bragaglia, Andrew O’Harney, and Marc Sloan. 2017. Graph convolutional networks for named entity recognition. In Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories.</p>
<p>Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. 2017. Improved neural machine translation with a syntax-aware encoder and decoder. In Proceedings of ACL.</p>
<p>Marco Damonte, Shay B Cohen, and Giorgio Satta. 2017. An incremental parser for abstract meaning representation. Proceedings of EACL.</p>
<p>David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of NIPS.</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-sequence attentional neural machine translation. In Proceedings of ACL.</p>
<p>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of ICASSP.</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In Proceedings of ACL.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. In Proceedings of ICLR.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. In Proceedings of ACL.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and generation. Proceedings of ACL.</p>
<p>Friedrich Wilhelm Levi. 1942. Finite geometrical systems. University of Calcutta.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A Smith. 2015. Toward abstractive summarization using semantic representations. Proceedings of NAACL.</p>
<p>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of ACL.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. Proceedings of INLG.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of EMNLP.</p>
<p>Rik van Noord and Johan Bos. 2017. Dealing with coreference in neural semantic parsing. In Proceedings of the 2nd Workshop on Semantic Deep Learning.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of $A C L$.</p>
<p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. In NIPS Workshop.</p>
<p>Rico Sennrich. 2017. How grammatical is characterlevel neural machine translation? assessing mt quality with contrastive translation pairs. In Proceedings of EACL.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR-to-text generation. In Proceedings of ACL.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958.</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. Highway networks. arXiv preprint arXiv:1505.00387.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of ACL.</p>
<p>Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation. In Proceedings of EMNLP.</p>
<p>Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. 2018. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The generated contrastive examples are available at https://github.com/mdtux89/OpenNMT-py.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ For this analysis we use Meteor instead of BLEU because it is a sentence-level metric, unlike BLEU, which is a corpus-level metric.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7633 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7633</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7633</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268681759</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.16276v3.pdf" target="_blank">Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains. By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained. However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events. This deficiency hinders LLMs from learning the alignment between time, audio-visual events, and text tokens, thus impairing their ability to temporally localize audio-visual events in videos. To address this gap, we introduce PU-VALOR, a comprehensive audio-visual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations. PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving event-based video clustering, random temporal scaling, and permutation. By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens. AVicuna excels in temporal localization and time-aware dialogue capabilities. Our experiments demonstrate that AVicuna effectively handles temporal understanding in audio-visual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7633",
    "paper_id": "paper-268681759",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0038452499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding
8 Oct 2025</p>
<p>Yunlong Tang yunlong.tang@rochester.edu 
University of Rochester</p>
<p>Daiki Shimada daiki.shimada@sony.com 
Sony Group Corporation</p>
<p>Jing Bi jing.bi@rochester.edu 
University of Rochester</p>
<p>Mingqian Feng mingqian.feng@rochester.edu 
University of Rochester</p>
<p>Hang Hua 
University of Rochester</p>
<p>Chenliang Xu chenliang.xu@rochester.edu 
University of Rochester</p>
<p>Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding
8 Oct 20255DDE6E62021790A41E2A157B0B782C4CarXiv:2403.16276v3[cs.CV]
Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains.By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained.However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events.This deficiency hinders LLMs from learning the alignment between time, audiovisual events, and text tokens, thus impairing their ability to localize audio-visual events in videos temporally.To address this gap, we introduce PU-VALOR, a comprehensive audiovisual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations.PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving eventbased video clustering, random temporal scaling, and permutation.By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens.AVicuna excels in temporal localization and time-aware dialogue capabilities.Our experiments demonstrate that AVicuna effectively handles temporal understanding in audiovisual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have recently advanced natural language processing (NLP), evolving into Multimodal LLMs (MLLMs) capable of comprehending various modalities like text, images, audio, and videos (Chen et al. 2023a;Zhang et al. 2023b;Liu et al. 2023;Lin et al. 2023a;Xu, Tang, and Zheng 2023;Zhang et al. 2023a).Despite the advancements, MLLMs still struggle to provide a fine-grained understanding of spatial or temporal details in multimodal contexts.To address these limitations, several works have explored (Chen et al. 2023a;Xuan et al. 2023) incorporating object bounding box coordinates in natural language format into image-text data, enabling MLLMs to directly identify the location of objects in images using natural language and respond accurately to user-provided bounding box coordinates, thereby enhancing fine-grained * Corresponding author.region-level understanding (Chen et al. 2023a;Xuan et al. 2023;Peng et al. 2023;Bai et al. 2023).While MLLMs have demonstrated potential in fine-grained image understanding, it's equally crucial to extend their capabilities to the video domain to achieve detailed video comprehension.Some recent advancements (Yang et al. 2023;Li et al. 2023b;Wang et al. 2023a;Huang et al. 2023;Zhang et al. 2024) have leveraged natural language for temporal predictions in video understanding tasks, such as dense video captioning, video temporal grounding, etc.These approaches have demonstrated competitive performance compared to traditional regression-based methods while retaining general capabilities, such as video question answering (Video QA).However, these methods predominantly concentrate solely on visual content, overlooking the dynamic and multimodal nature of the real world.For example, audio-visual content in videos represents a common form of such data.By integrating various modalities, including sound, we can achieve a more comprehensive analysis of content.As we delve deeper into this integration, it faces two significant challenges:</p>
<p>(1) In contrast to the abundance of dense video caption datasets, the audio-visual domain faces a significant bottleneck due to the lack of datasets providing detailed audiovisual event captions with accurate timestamp annotations.</p>
<p>(2) Developing audio-visual learning methods that effectively capture the intricate blend of auditory and visual cues, enabling them to interpret complex, intertwined information across various events within videos.</p>
<p>To tackle the challenge (1), we propose a practical yet straightforward pipeline that leverages the VALOR-32K (Chen et al. 2023b) dataset with high-quality audiovisual captions to construct PU (Pseudo-Untrimmed) -VALOR dataset contains audio-visual videos with corresponding temporal boundary annotations.The PU-VALOR dataset is created by applying Random Temporal Scaling and Permutation to videos clustered by captions.This innovative approach, aimed at generating Pseudo-Untrimmed videos, theoretically enables the creation of an unlimited number of untrimmed videos.Consequently, the PU-VALOR dataset features over 114k video-caption pairs, each annotated with precise temporal boundaries, thus offering a contribution toward enriching the audio-visual research domain.</p>
<p>Figure 1: Left: AVicuna's four-stage fine-tuning aligns natural language with exact time segments in audio-visual videos, highlighting its adeptness in dynamic content analysis.Right: AVicuna's superior performance across various video and audiovisual understanding tasks compared to other models.</p>
<p>Moving forward from the proposed dataset, we recognize the second challenge in audio-visual learning: accurately modeling the temporal dynamics in untrimmed audiovisual content.Previous methods (Shu et al. 2023;Zhang et al. 2023b;Su et al. 2023;Lyu et al. 2023a) have often combined embeddings from different modalities into single embeddings without adequately considering their intrinsic temporal relation.To tackle this critical issue, we introduce AVicuna, which comprises Multimodal Encoders, two Connective Adapters, an Audio-Visual Token Interleaver (AVTI), and an LLM.Multimodal Encoders extract embeddings from vision and audio modalities, which are aligned with the LLM's token space through Connective Adapters.The AVTI orchestrates the temporal relation of tokens from audio and video by creating interleaved audio-visual token sequences as inputs for the LLM.We employ a multistage fine-tuning approach to enhance AVicuna's capabilities, focusing on four critical stages: Vision-Text Alignment, Audio-Text Alignment, Time-Event Alignment, and Instruction Tuning.To foster effective alignment between multimodal tokens and LLM's token space, we have also aggregated several audio datasets, including AudioSet (Gemmeke et al.Our experiments demonstrate that the AVicuna fine-tuned on PU-VALOR achieves outstanding performance in both coarse-grained QA tasks and fine-grained temporal understanding tasks, as Figure 1 shown.It surpasses most LLM-based video understanding models and sets a new benchmark in the Audio-Visual Event Dense Localization (AVEDL) task.</p>
<p>In summary, our contributions are three-fold:</p>
<p>• We propose a novel approach to synthesize pseudountrimmed audio-visual videos and corresponding tem-poral boundary annotations using high-quality captions from the VALOR dataset, resulting in the PU-VALOR dataset.• We introduce AVicuna, an audio-visual LLM with an Audio-Visual Token Interleaver and Time-Event Alignment Tuning on the PU-VALOR dataset, which achieves temporal synchronism and fine-grained understanding in audio-visual videos.• Our experiments demonstrate that AVicuna significantly advances the state-of-the-art in the AVEDL task and exhibits strong performance in both coarse-grained QA and fine-grained temporal understanding tasks.</p>
<p>Related Work</p>
<p>Untrimmed Video Understanding.Temporal localization is key for understanding long-form videos by linking specific segments to their semantics.Key tasks include temporal video grounding (Luo et al. 2023a;Wang et al. 2022b), dense video captioning (Wang et al. 2021a;Yang et al. 2023), and video highlight detection (Lei, Berg, and Bansal 2021;Jiang, Ramakrishnan, and Grauman 2023).However, action localization and highlight detection models often rely on predefined labels (Zhang et al. 2022a), limiting the scope.Recent work on event boundary detection (Shou et al. 2021;Wang et al. 2022a;Tang et al. 2023) and dense captioning (Krishna et al. 2017) aims to address these constraints using event description datasets (Lin et al. 2023b).Despite this, most models still rely on regression for temporal predictions, requiring extra heads for captioning and regression (Wang et al. 2021b;Zhang et al. 2022b;Tang et al. 2022).Recent LLM advancements offer a shift by using natural language to directly specify temporal locations, offering a more intuitive approach.</p>
<p>Multimodal LLMs.The development of Multimodal LLMs (MLLMs) has been driven by advancements ) extend this fine-grained understanding to dynamic video content using natural language to define temporal boundaries without special tokens.We integrate audio cues to offer a more comprehensive approach to audio-visual temporal understanding tasks.</p>
<p>Audio-Visual Video Datasets.Increasing attention is being directed toward LLMs that support audio-visual inputs, such as Video-LLaMA (Zhang et al. 2023b), PandaGPT (Su et al. 2023), Macaw-LLM (Lyu et al. 2023a), and AV-LLM (Shu et al. 2023), which are trained on audio-visual video datasets to enhance understanding of audio-visual content.However, these models struggle with fine-grained understanding of long or untrimmed videos due to the lack of detailed annotations in existing datasets.The VALOR dataset (Chen et al. 2023b) offers high-quality audio-visual captions but consists of trimmed 10-second clips.Other datasets like VGG-Sound-AVEL100K (Zhou et al. 2023), AVVP (Tian, Li, and Xu 2020), UnAV-100 (Geng et al. 2023), andLFAV (Hou et al. 2023) provide temporal annotations but lack rich captioning.While AVSD (Alamri et al. 2019) and MUSIC-AVQA (Li et al. 2022) offer quality question-answer pairs, their temporal questions lack precise timestamps.This gap in datasets limits the models' ability to learn the relationship between audio-visual context and temporal boundaries.(Tian, Li, and Xu 2020)
✓ × ✓ ✓ InternVid (Wang et al. 2023d) ✓ × ✓ ✓ VGG-Sound-AVEL100K × ✓ × ✓ AVVP× ✓ × ✓ LFAV (Hou et al. 2023) ✓ ✓ × ✓ UnAV-100 (Geng et al. 2023) ✓ ✓ × ✓ VALOR (Chen et al. 2023b) × ✓ ✓ × PU-VALOR (ours) ✓ ✓ ✓ ✓</p>
<p>Methodology PU-VALOR Dataset</p>
<p>One of the primary challenges in untrimmed audio-visual video understanding is the scarcity of datasets with finegrained annotations for temporal audio-visual events.To tackle this issue, we propose a practical yet straightforward pipeline, as illustrated in Figure 2, to utilize the existing VALOR-32K audio-visual dataset (Chen et al. 2023b), which comprises exclusively trimmed videos.By synthesizing untrimmed videos with precise temporal labels, we have created the PU-VALOR dataset, that enable LLMs from learning the alignment between time, audio-visual events, and text token.</p>
<p>Clustering Videos with Similar Events.When creating untrimmed videos from trimmed clips, it's crucial to maintain semantic coherence within the untrimmed video to ensure a natural flow of content.This means that transitions between different segments should not be abrupt or disjointed, as such sudden shifts can disrupt the viewer's understanding.To ensure the content within an untrimmed video is semantically related, we group similar video segments based on the semantic similarity of their captions for follow-up untrimmed video generation.</p>
<p>We utilize a text encoder E txt to embed captions from the video-caption pairs {(v 1 , c 1 ), (v 2 , c 2 ), . . ., (v n , c n )} sourced from the VALOR-32K dataset.For each caption c i , the embedding is given by: ci = E txt (c i ) Next, we apply a clustering algorithm to the set of embeddings {c 1 , c2 , . . ., cn } to identify and group videos with similar events, resulting in clusters denoted as K = {k 1 , k 2 , . . ., k m }.From each video cluster k, we random select one and then identify the m − 1 top most similar clips within the same cluster.This selection is defined by:
S k = {v i | v i ∈ V k , where i ∈ {1, 2, . . . , m}}. (1)
We repeat this process for each cluster until no more than m clips remain in any cluster.This process ensures that each set of m clips is closely related in content.</p>
<p>Random Temporal Scaling &amp; Permutation.To ensure diverse temporal relationships between the clips, for each selected video v i ∈ S k , we randomly scale its duration within the range [T min , T max ] as
T new vi = T vi × random(T min , T max ).(2)
Then, we shuffle the order of the selected videos S k and concatenate the videos in S k to form a new untrimmed video
U k : U k = v π(1) ||v π(2) || . . . ||v π(m) ,(3)
where π is a random permutation of the indices {1, 2, . . ., m}.</p>
<p>Annotation.As we know the original duration T vi and the scaled duration T new vi for each video, we can map captions to specific temporal intervals in U k .If the original duration for a caption in v i is d i , the new timestamp in U k becomes:
[t new start,i , t new end,i ] = <a href="4">T offset,i , T offset,i + d i • ϵ</a>
where T offset is the cumulative duration of all preceding videos in U k after scaling, and the scaling factor ϵ = T new vi /T vi .We annotate temporal intervals [t new start,i , t new end,i ] within U k to correspond with the content described by the captions c i .</p>
<p>Data Visualization.Some pseudo-untrimmed video examples from the PU-VALOR constructed are shown in Figure 2: (Top) The sub-videos that make up this pseudountrimmed video share common themes of nature, including elements such as birds, greenery, and natural sounds, set against a backdrop of changing environments and human interaction.(Bottom) the sub-videos in this example all revolve around trains, capturing their movements on the tracks, associated sounds, and interactions with the environment, such as entering tunnels and stations.Clustering videos with similar captions or events reduces the dramatic changes in semantics and content, which avoids the model learning a shortcut to localize the temporal boundaries.</p>
<p>AVicuna Model</p>
<p>Overview.Figure 3 illustrates AVicuna's architecture, comprising Multimodal Encoders, Connective Adapters, an Audio-Visual Token Interleaver (AVTI), and an LLM.The encoders extract embeddings aligned to the LLM's token space, and the AVTI interleaves them, with an Audio-Interleaving Rate (AIR) enhancing temporal synchronism.</p>
<p>Multimodal Encoders.Multimodal Encoder includes Vision Encoder and Audio Encoder.For visual input, we employ the CLIP ViT-14/L (Radford et al. 2021) as Vision Encoder to extract visual embeddings F = {f i } M i=1 , where M denotes the number of visual embeddings.When the input is image, M = 1.For audio input, we utilize CLAP (Elizalde et al. 2023) as Audio Encoder to obtain audio embeddings A = {a i } N i=1 , with N representing the number of audio embeddings.</p>
<p>Connective Adapters.To avoid interference between the different modalities during the alignment, we adopt two MLPs as Vision Adapter and Audio Adapter for visual embeddings and audio embeddings, respectively, to get visual tokens F = { fi } M i=1 and audio tokens Ā = {ā i } N i=1 that are aligned with LLM's token space.</p>
<p>Audio-Visual Tokens Interleaver.Unlike simply adding positional embeddings to audio and video embeddings in previous work (Lyu et al. 2023a;Shu et al. 2023), the Audio-Visual Token Interleaver (AVTI) rearranges video and audio embeddings without altering their sequence order, keeping the overall token length constant.The audio-interleaving rate (AIR), denoted as ρ ∈ [0, 1], controls the ratio of video to audio tokens.Audio tokens and video tokens are interpolated or downsampled to Ã = {ã i } Ñ i=1 and F = { fi } M i=1 , where Ñ = ρT and M = (1 − ρ)T , with T being the total sequence length generated by AVTI.As shown in Figure 3, AVTI systematically interleaves audio and video tokens, preserving their original order to maintain temporal alignment.The output, represented by the audio-visual context Ψ = {ψ t } T t=1 , is given by:
ψ t = ã⌈t/(ωρ+1)⌉ if t mod (ω ρ + 1) ≡ 0, f⌈t/(ωρ+1)⌉ otherwise,(5)
where ω ρ = ⌊ 1−ρ ρ ⌋.Each context token represents both the audio-visual content and its temporal position within the video.</p>
<p>Large Language Model.We use the fine-tuned Vicuna-7B-v1.5 (Touvron et al. 2023) as our LLM to process interleaved audio-visual tokens and user queries Q, generating responses R: R = LLM(I, Q),</p>
<p>where I ∈ { F , Ā, Ψ} represents vision, audio, or both.</p>
<p>Multi-stage Fine-tuning</p>
<p>In AVicuna training, aligning embeddings from other modalities to the LLM's token space is preferred, using Video-Text Stage III: Time-Event Alignment.We freeze the finetuned Audio and Vision Adapters, updating only the LoRA (Hu et al. 2022a) parameters in the LLM.We create Q-R pairs with time-related information from the PU-VALOR datasets, including (1) Time-referenced Query, Time-agnostic Response: and (2) Time-agnostic Query, Time-referenced Response: Stage IV: Instruction Tuning.Finally, we fine-tune AVicuna on instruction-following datasets, including UnAV-100 (Geng et al. 2023) for event localization, and other instruction datasets like VideoInstruct100K (Maaz et al. 2023), ActivityNet Captions (Krishna et al. 2017), andDiDeMo (Anne Hendricks et al. 2017), to enhance questionanswering and mitigate previous tuning effects (Huang et al. 2023).At this stage, I is also Ψ or F , Q is a general instruction, and R is the corresponding response.
{(Q, R)|τ ⊑ Q, τ ̸ ⊑ R)},{(Q, R)|τ ̸ ⊑ Q, τ ⊑ R)},</p>
<p>Experimental Results</p>
<p>Experiment Setups</p>
<p>Metrics.We evaluate temporal understanding using tasks across various domains: Video Question Answering (Video Audio-Visual Event Localization.The comparison results on the AVEDL tasks can be found in Table 3.In the AVEDL task, AVicuna's superior mAP scores, particularly at the IoU threshold of 0.5 through 0.9, indicate its enhanced precision in localizing events within a video.The results are impressive, considering they outperform other LLM-based models and specialized non-LLM methods like VSGN, TadTR, ActionFormer, and UnAV.This suggests that the AVicuna model has effectively leveraged its audio-visual capabilities to provide a more nuanced understanding of the temporal aspects of videos.We also conducted the video temporal grounding (VTG) task on the ActivityNet Captions dataset, which can be found in the Appendix.</p>
<p>Ablation Study</p>
<p>We conduct ablation studies as shown in Table 4 to assess the impact of different components, datasets, and modalities on AVicuna's performance.Each row represents an independent experiment where a specific component or dataset Removing the AVTI module also results in a decrease in performance, further validating its necessity.Additionally, excluding audio inputs causes a marked reduction in accuracy, underscoring the value of a multimodal approach.</p>
<p>Audio-Interleaving Rates Analysis</p>
<p>The ablation study further sheds light on the contribution of different components to the model's performance.We conduct ablation studies on the Audio-Interleaving Rates (AIR), denoted by ρ. Figure 4 shows AVicuna's performances on UnAV-100 datset for AVEDL task.The scores generally increase with the AIR up to 25-30%, indicating optimal audio-visual integration, and then decline, suggesting that excessive audio information may be detrimental.</p>
<p>The performance gap between different IoU thresholds narrows at higher AIRs.The highest mAP is achieved at 25% AIR across all thresholds, indicating an optimal balance between audio and video information.The complete results are shown in the Appendix.</p>
<p>Qualitative Analysis</p>
<p>Figure 5 presents three examples of AVicuna's audio-visual dialogue capabilities.In the first video, involving a Jeep with persistent engine noise, AVicuna accurately identifies the time intervals where the engine is knocking.The second video shows people live-streaming while eating pomegranates; AVicuna correctly skips the intro, focuses on the specified time interval, and describes the chewing sounds.In the third video, where two people play silent musical instruments, AVicuna accurately responds with "NO", showcasing its ability to provide correct negative responses-a common challenge for existing models.These examples demonstrate AVicuna's flexibility in handling videos of varying lengths and resolutions.More examples can be found in the Appendix.</p>
<p>Conclusion</p>
<p>We present a comprehensive approach to enhancing audiovisual temporal understanding in untrimmed videos using MLLMs.We introduced a novel method to construct a pseudo-untrimmed dataset, PU-VALOR, which provides pseudo-untrimmed audio-visual videos with accurate temporal boundary annotations, addressing the scarcity of datasets in the audio-visual domain.Furthermore, we developed AVicuna, an audio-visual LLM incorporating an AVTI and Time-Event Alignment to achieve fine-grained understanding and temporal synchronism in audio-visual videos.</p>
<p>Our experiments demonstrate that AVicuna achieves stateof-the-art performance in various video and audio-visual understanding tasks, supporting both coarse-grained QA and fine-grained temporal understanding.Deficiency in Spatial Comprehension.While AVicuna provides advancements in temporal understanding, spatialtemporal grounding in long-form or untrimmed videos remains an area for improvement.The model's ability to simultaneously localize and understand events spatially and temporally in a video is still not at par with human comprehension.</p>
<p>Insufficient Precision for Ultra-Long Videos.The precision of representing video time percentages using 100 natural language-formatted numbers is limited, especially for ultra-long videos.This can impact the model's ability to accurately localize and understand events in videos with extended durations.</p>
<p>2017), AudioCap (Kim et al. 2019), and Auto-ACD (Sun et al. 2023), to form a comprehensive audiotext dataset with 222K pairs, termed A5-222K (Audio-text Alignment with AudioSet, AudioCap, and Auto-CAD).</p>
<p>Figure 2 :
2
Figure 2: Pipeline for creating the PU-VALOR dataset, which involves extracting text embeddings from high-quality audiovisual captions of the original trimmed VALOR-32K dataset, clustering these embeddings, and then applying Random Temporal Scaling &amp; Permutation to generate pseudo-untrimmed videos.These synthesized videos are then annotated with temporal boundaries using a template-based approach to facilitate the following audio-visual time-event alignment.in LLMs, enabling the integration of multimodal inputs (Alayrac et al. 2022; Liu et al. 2023; Hu et al. 2022b; Wang et al. 2023b; Hua et al. 2024a; Lyu et al. 2023b; Bi et al. 2023).Flamingo (Alayrac et al. 2022) utilizes visual in-context learning for visual question answering, while LLaVA (Liu et al. 2023) introduces visual instruction tuning to enhance visual understanding.Models like VisionLLM (Wang et al. 2023c), KOSMOS-2 (Peng et al. 2023), (Chen et al. 2023a), and Qwen-VL (Bai et al. 2023) further advance MLLMs with visual grounding.Recent advancements such as VideoChat (Li et al. 2023b), ChatVideo (Wang et al. 2023a), V2Xum-LLM (Hua et al. 2024b), and VTimeLLM (Huang et al. 2023) extend this fine-grained understanding to dynamic video content using natural language to define temporal boundaries without special tokens.We integrate audio cues to offer a more comprehensive approach to audio-visual temporal understanding tasks.</p>
<p>Figure 3 :
3
Figure 3: AVicuna model architecture and fine-tuning process.Vision and Audio Adapters are MLPs that align modalities with LLM.The Audio-Visual Tokens Interleaver ensures temporal synchronization.LoRA fine-tuning aligns temporal boundaries with events and enhances instruction-following capabilities.and Audio-Text Alignment.To enable specific temporal patterns, we fine-tune LoRA (Hu et al. 2022a) during Time-Event Alignment.Recent studies (Huang et al. 2023) indicate this may affect question-answering, mitigated by Instruction Tuning.We thus employ a four-stage fine-tuning process, with datasets detailed in the Appendix.Stage I &amp; II: Multimodal-Text Alignment.In Vision-Text Alignment, we fix the Vision Encoder and LLM, updating only the Vision Adapter, using LCS-558K(Liu et al. 2023), a 558K image-text pair subset from LAION-CC-SBU with BLIP(Li et al. 2023a)  captions.Here, I = F , Q is a visual content query, and R is the image caption.In Audio-Text Alignment, we fix the Audio Encoder and LLM, updating only the Audio Adapter, using A5-222K, a 222K audiotext dataset compiled from AudioSet(Gemmeke et al. 2017),  AudioCap (Kim et al. 2019), and Auto-ACD(Sun et al. 2023).Here, I = Ā, Q is an audio content query, and R is the audio caption.</p>
<p>where τ :="from τ s to τ e " and τ s , τ e are event time points.Inputs (I) can be visual, audio, or combined; we use the InternVid(Wang et al. 2023d) dataset to enrich visual event alignment training.Prompts for Time-Event Alignment.Q1: Tell me about the visual and audio events τ in the video.Q2: What was going on visually and audibly τ in the video?Q3: Please recount what occurred, including both video and audio, τ in the video.Q4: Could you tell me what happened, in terms of both imagery and sound, τ in the video?Q5: Provide details about the visual scenes and audio events τ in the video.Q6: Can you describe what occurred, both visually and audibly, τ in the video?Q7: Explain what happened, considering both video and audio, τ in the video.</p>
<p>Figure 4 :
4
Figure 4: AVicuna's performances on UnAV-100 measured by mAP scores at different AIRs.is removed.Specifically, omitting the PU-VALOR or A5-222K datasets, especially PU-VALOR, leads to significant performance drops, emphasizing their critical roles in Time-Event Alignment and Audio-Text Alignment, respectively.Removing the AVTI module also results in a decrease in performance, further validating its necessity.Additionally, excluding audio inputs causes a marked reduction in accuracy, underscoring the value of a multimodal approach.</p>
<p>Figure 5 :
5
Figure 5: Qualitative results.Blue indicates ground-truth, green indicates the time intervals the user gives, and orange represents the model predictions.AVicuna supports audiovisual video input with various durations and resolutions.Given user queries about an event, it predicts temporal intervals accurately.A given temporal interval provides an accurate response.It also performs reasoning given a question about audio-visual context.</p>
<p>Figure 6 :
6
Figure6: More results.This figure illustrates AVicuna's capability to localize events by interpreting natural language queries and offering precise event descriptions based on provided timestamps.Additionally, AVicuna can process complex queries to provide accurate and context-aware responses, showcasing its superior performance in video temporal understanding.</p>
<p>Figure 7 :
7
Figure 7: More results (continue).AVicuna's advanced functionality allows it to provide detailed video captions that include pairs of timestamps and narratives, going beyond basic captioning.By utilizing LLMs, AVicuna can output responses in JSON format for audio-visual event dense localization, facilitating easy parsing and integration with downstream models.</p>
<p>Figure 8 :
8
Figure 8: More results (continue).AVicuna can provide detailed answers for enhanced audio-visual video QA, from identifying objects and scenes to accurate counting.</p>
<p>Table 1 :
1
Comparison of datasets based on untrimmed videos, audio-visual modalities, captions, and timestamps, showcasing the full coverage of all attributes by our PU-VALOR dataset.</p>
<p>Table 2 :
2
Comparison with existing LLM-based methods on open-ended video QA (MSVD-QA, MSRVTT-QA, ActivityNet-QA) and AVQA (AVSD, MUSIC-AVQA) benchmarks.A&amp;V: the model supports both video and audio input.TU: the model can perform temporal understanding task, e.g.,temporal grounding and localization.#Pairs: the instruction-response pairs for instruction tuning.LLM-size: the large language model adopted in a method.
MethodA&amp;V TU #Pairs LLM-size AVSD MUSIC-QA MSVD-QA MSRVTT-QA ActivityNet-QAValley (Luo et al. 2023b)×× 1.5M13B--65.445.726.5VideoChat (Li et al. 2023b)×✓ 25M7B--56.345.026.5Video-ChatGPT (Maaz et al. 2023)×✓ 0.9M7B--64.949.335.2VTimeLLM (Huang et al. 2023)×✓ 0.7M7B--69.858.845.5PandaGPT (Su et al. 2023)✓× 128M13B26.133.746.723.711.2Macaw-LLM (Lyu et al. 2023a)✓× 0.3M7B34.331.842.125.514.5AV-LLM (Shu et al. 2023)✓× 1.6M13B52.645.267.353.747.2Video-LLaMA (Zhang et al. 2023b)✓✓ 2.8M7B36.736.651.629.612.4AVicuna (ours)✓✓ 1.1M7B53.149.670.259.753.0QA), Audio-visual Video Question Answering (AVQA),Method0.5 0.6 0.7 0.8 0.9 Avg.and Audio-Visual Event Dense Localization (AVEDL). For General Video QA, zero-shot evaluation is performedVSGN (Zhao et al. 2021) TadTR (Liu et al. 2022)24.5 20.2 15.9 11.4 6.8 24.1 30.4 27.1 23.3 19.4 14.3 29.4on the MSVD-QA (Chen and Dolan 2011), MSRVTT-ActionFormer (Zhang et al. 2022a) 43.5 39.4 33.4 27.3 17.9 42.2QA (Xu et al. 2016), and ActivityNet-QA (Yu et al. 2019) datasets, with open-ended QA tasks evaluated using GPT scoring (Maaz et al. 2023). AVQA tasks are assessed on the AVSD (Alamri et al. 2019) and MUSIC-AVQA (LiUnAV (Geng et al. 2023) UniAV-AT (Geng et al. 2024) UniAV-ST (Geng et al. 2024) AVicuna (ours)50.6 45.8 39.8 32.4 21.1 47.8 54.1 48.6 42.1 34.3 20.5 50.7 54.8 49.4 43.2 35.3 22.5 51.7 60.0 50.4 49.6 43.5 36.5 60.3et al. 2022) datasets. The AVEDL task uses the UnAV-100 (Geng et al. 2023) dataset, with performance mea-sured by mean Average Precision (mAP) at Intersection overUnion (tIoU) thresholds [0.5:0.1:0.9] and the average mAPacross [0.1:0.1:0.9].Baseline Models. For QA tasks, we evaluate LLM-basedmodels, including PandaGPT (Su et al. 2023), Macaw-LLM (Lyu et al. 2023a), and AV-LLM (Shu et al. 2023),which support audio-visual input. All evaluated modelshave 7B or 13B parameters. For AVEDL tasks, we in-clude non-LLM baselines like VSGN (Zhao et al. 2021),TadTR (Liu et al. 2022), ActionFormer (Zhang et al. 2022a),UnAV (Geng et al. 2023), and UniAV-AT/ST (Geng et al.2024).Implementation Details. We uniformly extract a mini-mum of 100 frames from each video to create an inter-leaved sequence of audio-visual tokens. Video and audioframes are synchronized through interpolation or downsam-pling to match the Audio-Interleaving Rate, resulting in asequence of 100 tokens, each representing 1% of the audio-visual video duration. The video scaling factor range is setto [T min , T max ] = [0.5, 2] with a step of 0.1, allowing play-back speed adjustments from half to double the original rate.Detailed information on clustering and fine-tuning settings,including specific learning rates and epochs, is provided inthe Appendix.Comparison ExperimentsGeneral Video QA and AVQA. The video QA andAVQA comparison results are shown as Table 2. AVicunasupports both audio and video as input and handles tempo-ral understanding tasks. Despite being fine-tuned with only1.1M pairs and utilizing an LLM with 7B parameters, AVi-cuna surpasses all other LLM-based models on both video</p>
<p>Table 3 :
3
Comparison of the results on the UnAV-100 for the AVEDL task.
QA and AVQA benchmarks.Setting0.50.60.70.80.9 Avg.AVicuna60.0 54.4 49.6 43.5 37.1 60.3w/o PU-VALOR 19.5 14.3 10.2 6.84.527.9w/o AVTI50.1 45.2 40.2 34.2 29.4 51.1w/o A5-222K22.2 16.5 11.4 6.82.730.1w/o Audio29.0 23.9 18.8 13.6 8.835.8</p>
<p>Table 4 :
4
Ablation study on the dataset and model components, which lead to decreases in mAP.</p>
<p>Appendix More Implementation DetailsThe dense audio frames are captured using sliding windows of varying sizes and strides.After clustering, 25,270 clusters are produced, with the option to select between 3 and 20 videos for constructing the pseudo-untrimmed video.AVicuna is fine-tuned on a single NVIDIA 48G A6000 GPU.Fine-tuning details are as follows: Stage I and II were both fine-tuned for 2 epochs using a learning rate of 1e-3.Stage III and IV were each fine-tuned for 1 epoch using a learning rate of 1e-4.Fine-tuning times were 5/2/36/6 hours for stages I/II/III/IV, respectively.More Comparison ExperimentsVideo-based Generative Performance.Table5presents the GPT-based evaluation across multiple dimensions for video understanding, as per the methodology of Video-ChatGPT.Our method, AVicuna, achieves the highest scores in temporal understanding (2.53), consistency (2.59), and correctness (2.81) while scoring competitively in detail orientation and context understanding.The superior performance in temporal understanding is particularly notable, attributed to AVicuna's innovative Audio-Visual Tokens Interleaver (AVTI) and Time-Event Alignment.These features enable AVicuna to model long video inputs effectively, capturing the temporal dynamics and ensuring a coherent understanding of the video content over time.The AVTI ensures that audio and visual information is synchronized and integrated, allowing for a more nuanced interpretation of events and their temporal relationships.Additionally, the Time-Event Alignment further enhances the model's ability to discern the temporal boundaries of different events within the video, contributing to its overall temporal understanding, consistency, and correctness.Performance on the VTG Task.For the Video Temporal Grounding (VTG) task on the ActivityNet Captions dataset, AVicuna's performance in R1@0.5, R1@0.7, and mIoU metrics outstrips that of VideoChat, Video-ChatGPT, and VTimeLLM, reflecting its ability to accurately identify and understand the relevant segments of the video following the textual queries.Though we use official code with all original settings and training steps, our reproduction of VTimeLLM yields slightly lower performance than reported.Method R1@0.5 R1@0.7 mIoU VideoChat(Li et al. 2023b)3.7 1.5 7.2 Video-ChatGPT(Maaz et al. 2023)13.6 6.1 18.9 VTimeLLM(Huang et al. 2023)23.9 9.6 26.6 AVicuna (ours) 25.0 13.2 28.1Dataset SummaryThe datasets and the corresponding question-answering or instruction-response pairs used at each stage are summarized in Table7.Prompts and TemplatesWe utilize image-text pair prompts from the LCS-588K dataset to generate prompts for Vision-Text Alignment.To ensure symmetry, we transform these image prompts into their audio counterparts for Audio-Text Alignment, as illustrated in Box 1.  Regarding the A5-222K dataset's audio captions, the audio-text pairs are sourced from the AudioCap and Auto-CAD datasets.However, the AudioSet dataset lacks audio captions and instead provides labels.We utilize templates to convert these labels into responses, presented in Box 2.Fine-tuningCompleted AIR ResultsThe completed AIR (audio-interleaving rate) results are shown in Table8.More Qualitative ResultsHere, we provide more qualitative results to show the AVicuna's performance, which are shown in Figures6 to 8.LimitationsEven though AVicuna achieves good performance in a variety of fine-grained video understanding tasks and referential dialogue, there are still some limitations.Hallucination.One notable challenge is hallucination, where the model generates plausible but incorrect details not present in the input data.This can lead to misinformation if not adequately addressed and may affect the model's reliability in critical applications.
Audio Visual Scene-Aware Dialog. H Alamri, V Cartillier, A Das, J Wang, A Cherian, I Essa, D Batra, T K Marks, C Hori, P Anderson, S Lee, D Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Localizing moments in video with natural language. Anne Hendricks, L Wang, O Shechtman, E Sivic, J Darrell, T Russell, B , Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>J Bai, S Bai, S Yang, S Wang, S Tan, P Wang, J Lin, C Zhou, J Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>MISAR: A Multimodal Instructional System with Augmented Reality. J Bi, N M Nguyen, A Vosoughi, C Xu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Collecting Highly Parallel Data for Paraphrase Evaluation. D Chen, W Dolan, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. D Lin, Y Matsumoto, R Mihalcea, the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USAAssociation for Computational Linguistics2011</p>
<p>K Chen, Z Zhang, W Zeng, R Zhang, F Zhu, R Zhao, arXiv:2306.15195Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic. 2023aarXiv preprint</p>
<p>Valor: Vision-audio-language omniperception pretraining model and dataset. S Chen, X He, L Guo, X Zhu, W Wang, J Tang, J Liu, arXiv:2304.083452023barXiv preprint</p>
<p>Clap learning audio concepts from natural language supervision. B Elizalde, S Deshmukh, M Al Ismail, H Wang, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023</p>
<p>Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline. J F Gemmeke, D P W Ellis, D Freedman, A Jansen, W Lawrence, R C Moore, M Plakal, M Ritter, T Geng, T Wang, J Duan, R Cong, F Zheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2017. 20232017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
<p>UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization. T Geng, T Wang, Y Zhang, J Duan, W Guan, F Zheng, W Hou, G Li, Y Tian, D Hu, arXiv:2404.03179arXiv:2306.09431Towards Long Form Audio-visual Video Understanding. 2024. 2023arXiv preprint</p>
<p>. E J Hu, yelong shen</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, International Conference on Learning Representations. 2022a</p>
<p>FINEMATCH: Aspectbased Fine-grained Image and Text Mismatch Detection and Correction. Y Hu, H Hua, Z Yang, W Shi, N A Smith, J Luo, H Hua, J Shi, K Kafle, S Jenni, D Zhang, J Collomosse, S Cohen, J Luo, H Hua, Y Tang, C Xu, J Luo, H Chen, Z Song, W Zhu, arXiv:2211.09699arXiv:2311.18445Promptcap: Prompt-guided task-aware image captioning. Wang, X2022b. 2024a. 2024b. 2023arXiv preprintEmpower llm to grasp video moments</p>
<p>Single-stage visual query localization in egocentric videos. H Jiang, S K Ramakrishnan, K Grauman, C D Kim, B Kim, H Lee, G Kim, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics2023. 201936Advances in Neural Information Processing Systems</p>
<p>Dense-Captioning Events in Videos. R Krishna, K Hata, F Ren, L Fei-Fei, Carlos Niebles, J , Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)2017</p>
<p>Detecting moments and highlights in videos via natural language queries. J Lei, T L Berg, M Bansal, Advances in Neural Information Processing Systems. 202134</p>
<p>Learning to answer questions in dynamic audio-visual scenarios. G Li, Y Wei, Y Tian, C Xu, J.-R Wen, D Hu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023aarXiv preprint</p>
<p>Videochat: Chat-centric video understanding. K Li, Y He, Y Wang, Y Li, W Wang, P Luo, Y Wang, L Wang, Y Qiao, B Lin, B Zhu, Y Ye, M Ning, P Jin, L Yuan, arXiv:2305.06355arXiv:2311.10122Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. 2023b. 2023aarXiv preprint</p>
<p>J Lin, H Hua, M Chen, Y Li, J Hsiao, C Ho, J Luo, arXiv:2303.12060VideoXum: Cross-modal Visual and Textural Summarization of Videos. 2023barXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202336</p>
<p>End-to-end temporal action detection with transformer. X Liu, Q Wang, Y Hu, X Tang, S Zhang, S Bai, X Bai, IEEE Transactions on Image Processing. 312022</p>
<p>Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training. D Luo, J Huang, S Gong, H Jin, Y Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023a</p>
<p>R Luo, Z Zhao, M Yang, J Dong, M Qiu, P Lu, T Wang, Z Wei, arXiv:2306.07207Valley: Video Assistant with Large Language model Enhanced abilitY. 2023barXiv preprint</p>
<p>C Lyu, M Wu, L Wang, X Huang, B Liu, Z Du, S Shi, Z Tu, arXiv:2306.09093Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration. 2023aarXiv preprint</p>
<p>H Lyu, J Huang, D Zhang, Y Yu, X Mou, J Pan, Z Yang, Z Wei, J Luo, arXiv:2311.07547Gpt-4v (ision) as a social media analysis engine. 2023barXiv preprint</p>
<p>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. M Maaz, H Rasheed, S Khan, F S Khan, arXiv:2306.054242023arXiv preprint</p>
<p>Kosmos-2: Grounding Multimodal Large Language Models to the World. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Generic event boundary detection: A benchmark for event segmentation. M Z Shou, S W Lei, W Wang, D Ghadiyaram, M Feiszli, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>F Shu, L Zhang, H Jiang, C Xie, arXiv:2312.06720Audio-Visual LLM for Video Understanding. 2023arXiv preprint</p>
<p>Y Su, T Lan, H Li, J Xu, Y Wang, D Cai, arXiv:2305.16355Pandagpt: One model to instruction-follow them all. 2023arXiv preprint</p>
<p>A Largescale Dataset for Audio-Language Representation Learning. L Sun, X Xu, M Wu, W Xie, arXiv:2309.115002023arXiv preprint</p>
<p>Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward. Y Tang, S Xu, T Wang, Q Lin, Q Lu, F Zheng, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2022</p>
<p>LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning. Y Tang, J Zhang, X Wang, T Wang, F Zheng, arXiv:2306.103542023arXiv preprint</p>
<p>Unified multisensory perception: Weakly-supervised audio-visual video parsing. Y Tian, D Li, C Xu, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 202016Proceedings, Part III</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971arXiv:2304.14407Chatvideo: A tracklet-centric multimodal and versatile video understanding system. 2023. 2023aarXiv preprintLlama: Open and efficient foundation language models</p>
<p>Caption anything: Interactive image description with diverse multimodal controls. T Wang, J Zhang, J Fei, Y Ge, H Zheng, Y Tang, Z Li, M Gao, S Zhao, Y Shan, arXiv:2305.02677Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023b. 2021aarXiv preprintEnd-to-end dense video captioning with parallel decoding</p>
<p>End-to-end dense video captioning with parallel decoding. T Wang, R Zhang, Z Lu, F Zheng, R Cheng, P Luo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021b</p>
<p>Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. W Wang, Z Chen, X Chen, J Wu, X Zhu, G Zeng, P Luo, T Lu, J Zhou, Y Qiao, Advances in Neural Information Processing Systems. 2023c36</p>
<p>Internvid: A largescale video-text dataset for multimodal understanding and generation. Y Wang, D Gao, L Yu, W Lei, M Feiszli, M Z Shou, Springer, Y Wang, Y He, Y Li, K Li, J Yu, X Ma, X Li, G Chen, X Chen, Y Wang, arXiv:2307.06942Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022a. 2023d. 2022b36arXiv preprintEuropean Conference on Computer Vision</p>
<p>MSR-VTT: A Large Video Description Dataset for Bridging Video and Language. J Xu, T Mei, T Yao, Y Rui, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016</p>
<p>S Xu, Y Tang, F Zheng, arXiv:2307.04827Launchpadgpt: Language model as music visualization designer on launchpad. 2023arXiv preprint</p>
<p>Pink: Unveiling the Power of Referential Comprehension for Multimodal LLMs. S Xuan, Q Guo, M Yang, S Zhang, arXiv:2310.005822023arXiv preprint</p>
<p>Vid2seq: Largescale pretraining of a visual language model for dense video captioning. A Yang, A Nagrani, P H Seo, A Miech, J Pont-Tuset, I Laptev, J Sivic, C Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Activitynet-qa: A dataset for understanding complex web videos via question answering. Z Yu, D Xu, J Yu, T Yu, Z Zhao, Y Zhuang, D Tao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Actionformer: Localizing moments of actions with transformers. C Zhang, European Conference on Computer Vision. Springer2022a</p>
<p>D Zhang, J Yang, H Lyu, Z Jin, Y Yao, M Chen, J Luo, arXiv:2401.02582CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs. 2024arXiv preprint</p>
<p>DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. D Zhang, W Zhang, B He, J Zhang, C Qin, J Yao, bioRxiv. 2023a</p>
<p>Video-llama: An instruction-tuned audio-visual language model for video understanding. H Zhang, arXiv:2306.028582023barXiv preprint</p>
<p>Exploiting Context Information for Generic Event Boundary Captioning. J Zhang, T Wang, F Zheng, R Cheng, P Luo, arXiv:2207.010502022barXiv preprint</p>
<p>Video self-stitching graph network for temporal action localization. C Zhao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Contrastive Positive Sample Propagation Along the Audio-Visual Event Line. J Zhou, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4562023</p>            </div>
        </div>

    </div>
</body>
</html>
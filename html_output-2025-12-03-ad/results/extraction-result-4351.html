<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4351 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4351</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4351</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-277510223</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02767v1.pdf" target="_blank">How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?</a></p>
                <p><strong>Paper Abstract:</strong> The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics. Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references. This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors. Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations. These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends. As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4351.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4351.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o citation-pattern pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametric LLM-based citation suggestion and pattern extraction pipeline (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This study prompts GPT-4o to generate reference suggestions from paper metadata and abstracts, then matches outputs to SciSciNet and analyzes statistical, embedding, and network-level patterns to quantify how an LLM internalizes and reproduces citation behaviors and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Parametric LLM-based citation suggestion and statistical extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each of 10,000 sampled focal papers, the authors prompt GPT-4o (system prompt + user prompt containing title, authors, year, venue, abstract) to generate n references (n = the paper's ground-truth reference count). Generated reference text is post-processed via a second GPT-4o-mini prompt to extract structured bibliographic fields. Each generated reference is existence-checked by retrieving candidate matches from a SciSciNet Elasticsearch index (BM25) and applying fuzzy matching (Levenshtein-based title and author similarity) with conservative thresholds (title similarity > 0.90 and author similarity > 0.50). For existing matches, bibliometric properties (citation counts, reference counts, document type, alt-impact indicators) are compared to ground-truth references; semantic alignment is measured via OpenAI text-embedding-3-large (3,072-d) and SPECTER2 embeddings using cosine similarity; local citation graphs are constructed and graph metrics (density, clustering, centrality, average path length, eigenvector centrality std) are computed and compared to ground truth and randomized baselines. Statistical comparisons use per-paper medians and pairwise two-sided Wilcoxon signed-rank tests.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (gpt-4o-2024-08-06) for generation; gpt-4o-mini-2024-07-18 for postprocessing; OpenAI text-embedding-3-large and SPECTER2 for embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (dominance of medicine, biology, chemistry; also humanities and social sciences included)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>10,000 focal papers (274,951 generated references)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Empirical statistical patterns in citation behavior (reinforcement of Matthew effect: preferential suggestion of highly-cited works), correlations between generated-reference attributes and citation impact (publication year/recency bias, title length, author team size), and graph-structural properties of local citation networks</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Numerical summaries and distributions (medians, proportions), embedding cosine-similarity scores, bibliometric counts, and graph metrics (density, clustering, centralities)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Existence-check via SciSciNet Elasticsearch + fuzzy matching; manual verification sample (100 references); statistical hypothesis testing (Wilcoxon signed-rank tests); randomized baselines (reshuffled ground-truth reference lists) for network analyses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall existence rate of generated references ≈ 42.5% (116,939 existing of 274,951 generated); ~90% of existing generated references lie in top 10% most-cited within field/year and >60% in top 1%; only 8.78% of existing generated references appear in the corresponding focal paper's ground-truth list; 22.24% appear in at least one ground-truth list elsewhere; various Wilcoxon tests p < 0.001 for median citation-count differences</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to ground-truth references and to random baseline (random ground-truth reference lists from same field): existing generated references have significantly higher median citation counts and far greater concentration in top-cited percentiles; generated local citation graphs resemble ground-truth graphs and deviate strongly from random reshuffles (graph metrics reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Substantial hallucination / fabrication of non-existent references (existence rate ~42%); conservative matching likely underestimates true existence; field-specific variation in existence rates (higher in humanities/social sciences); reliance on parametric knowledge only (no retrieval augmentation); possible training-data representation biases (long-tail underrepresentation); postprocessing and threshold choices affect results</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4351.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-search-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program search with large language models for mathematical discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that leverages LLMs to generate candidate programs/formal expressions (and potentially proofs) which are searched/evaluated to discover new mathematical relationships or theorems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical discoveries from program search with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Program search with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The referenced work (title only appears in bibliography) describes using LLMs coupled with program-search or program-synthesis procedures to generate candidate mathematical programs/expressions that can be evaluated to yield new mathematical results; the current paper cites it as an example of LLM-enabled mathematical discovery but does not describe internal details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics / theoretical discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Mathematical identities, conjectures, theorems (symbolic mathematical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Symbolic/mathematical expressions and program code (per title implication)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this paper (only cited); general limitations may include reliance on evaluator/verifier components and correctness checks</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4351.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sciagents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system that integrates graph reasoning and intelligent agents to automate aspects of scientific discovery and hypothesis generation from structured scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Multi-agent intelligent graph reasoning (Sciagents)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of systems that integrate LLMs/agents into larger pipelines for scientific discovery; specific pipeline details are not provided in the current paper beyond the title reference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific discovery / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Hypotheses and inter-document relationships represented on graphs (patterns/relationships between scientific entities)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Graph-structured relations and agent-generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4351.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI scientist (fully automated open-ended scientific discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concept/approach aiming toward fully automated, open-ended scientific discovery by combining AI agents and experimental or simulation validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fully automated discovery agents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited among works integrating LLMs into discovery pipelines; the present paper references it as part of the literature on autonomous research agents but does not provide operational details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / open-ended scientific discovery (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Scientific hypotheses, experimental protocols, candidate relationships emergent from agent proposals</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Proposed hypotheses, experiment designs, candidate relationships (likely structured text/commands)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4351.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiningCausality-IV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mining causality: AI-assisted search for instrumental variables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI-assisted approach to search for instrumental variables in observational data or literature to support causal inference, i.e., automated discovery of instruments and causal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mining causality: Ai-assisted search for instrumental variables.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI-assisted instrumental-variable search</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an example of AI assisting in causal discovery from literature; the current paper cites the work by title but gives no internal methodological detail.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Causal inference / social sciences / econometrics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Causal relationships and instrumental-variable candidates (causal inference constructs)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Candidate instruments and causal-relationship descriptions (structured textual proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4351.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative pipeline that uses LLMs to propose research ideas and iterate over literature context to refine suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Iterative LLM research idea generation (ResearchAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of LLM-based systems to synthesize literature and generate research ideas in an iterative loop; the present paper does not give internal algorithmic specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain research synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not explicitly quantitative in this paper's citation — focuses on idea generation and synthesis, but may surface patterns or candidate relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual research ideas and literature-linked suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4351.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researcharena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researcharena: Benchmarking LLMs' ability to collect and organize information as research agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmarking effort that evaluates LLM-based agent systems' capability to collect, organize, and synthesize scholarly information as research agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researcharena: Benchmarking llms' ability to collect and organize information as research agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Benchmark for LLM research-agent capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited in the paper as a benchmark assessing how well LLM agents perform information collection and organization tasks; details not provided in the current text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Methodology / NLP benchmarking for research agents</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Benchmark metrics for information collection and organization (not a law per se)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Benchmark scores and task-specific outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4351.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentLab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent laboratory (LLM agents as research assistants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental system using LLM agents as research assistants to perform literature-related tasks and research workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent laboratory: Using llm agents as research assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-agent research assistant pipeline (AgentLab)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited among recent works using LLM agents to assist research workflows and literature synthesis; the primary paper does not describe the internal pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Research workflow assistance / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Primarily assistance and synthesis; may surface patterns but not explicitly described here</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Agent-produced structured outputs and proposals</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4351.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LanguageAgents-Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language agents achieve superhuman synthesis of scientific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claimed/experimental systems of LLM-based agents that synthesize scientific knowledge at or beyond human-level capability in certain synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language agents achieve superhuman synthesis of scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-agent synthesis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as evidence that agent-based LLM systems can perform high-level literature synthesis; the current paper references the title but provides no procedural details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific synthesis (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Synthesis-level findings and high-level patterns across literature (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Synthesized reports and proposed connections</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4351.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4351.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VirtualLab-nanobodies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The virtual lab: AI agents design new SARS‑CoV‑2 nanobodies with experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI-agent-driven pipeline that designs candidate nanobodies and includes experimental validation to confirm designed molecules' efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI-agent driven molecular design with experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of agent systems that go beyond literature synthesis to propose experimentally validated discoveries; the present paper references only the title and general context.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Molecular design / computational biology / chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Design-to-experimental quantitative outcomes (binding affinities, assay results) implied but not described here</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Molecular designs and experimentally measured numeric outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experimental validation (per title), details not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathematical discoveries from program search with large language models. <em>(Rating: 2)</em></li>
                <li>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>Mining causality: Ai-assisted search for instrumental variables. <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models. <em>(Rating: 1)</em></li>
                <li>Researcharena: Benchmarking llms' ability to collect and organize information as research agents. <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants. <em>(Rating: 1)</em></li>
                <li>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. <em>(Rating: 1)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4351",
    "paper_id": "paper-277510223",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "GPT-4o citation-pattern pipeline",
            "name_full": "Parametric LLM-based citation suggestion and pattern extraction pipeline (this paper)",
            "brief_description": "This study prompts GPT-4o to generate reference suggestions from paper metadata and abstracts, then matches outputs to SciSciNet and analyzes statistical, embedding, and network-level patterns to quantify how an LLM internalizes and reproduces citation behaviors and biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Parametric LLM-based citation suggestion and statistical extraction",
            "method_description": "For each of 10,000 sampled focal papers, the authors prompt GPT-4o (system prompt + user prompt containing title, authors, year, venue, abstract) to generate n references (n = the paper's ground-truth reference count). Generated reference text is post-processed via a second GPT-4o-mini prompt to extract structured bibliographic fields. Each generated reference is existence-checked by retrieving candidate matches from a SciSciNet Elasticsearch index (BM25) and applying fuzzy matching (Levenshtein-based title and author similarity) with conservative thresholds (title similarity &gt; 0.90 and author similarity &gt; 0.50). For existing matches, bibliometric properties (citation counts, reference counts, document type, alt-impact indicators) are compared to ground-truth references; semantic alignment is measured via OpenAI text-embedding-3-large (3,072-d) and SPECTER2 embeddings using cosine similarity; local citation graphs are constructed and graph metrics (density, clustering, centrality, average path length, eigenvector centrality std) are computed and compared to ground truth and randomized baselines. Statistical comparisons use per-paper medians and pairwise two-sided Wilcoxon signed-rank tests.",
            "llm_model_used": "GPT-4o (gpt-4o-2024-08-06) for generation; gpt-4o-mini-2024-07-18 for postprocessing; OpenAI text-embedding-3-large and SPECTER2 for embeddings",
            "scientific_domain": "Cross-domain (dominance of medicine, biology, chemistry; also humanities and social sciences included)",
            "number_of_papers": "10,000 focal papers (274,951 generated references)",
            "type_of_quantitative_law": "Empirical statistical patterns in citation behavior (reinforcement of Matthew effect: preferential suggestion of highly-cited works), correlations between generated-reference attributes and citation impact (publication year/recency bias, title length, author team size), and graph-structural properties of local citation networks",
            "extraction_output_format": "Numerical summaries and distributions (medians, proportions), embedding cosine-similarity scores, bibliometric counts, and graph metrics (density, clustering, centralities)",
            "validation_method": "Existence-check via SciSciNet Elasticsearch + fuzzy matching; manual verification sample (100 references); statistical hypothesis testing (Wilcoxon signed-rank tests); randomized baselines (reshuffled ground-truth reference lists) for network analyses",
            "performance_metrics": "Overall existence rate of generated references ≈ 42.5% (116,939 existing of 274,951 generated); ~90% of existing generated references lie in top 10% most-cited within field/year and &gt;60% in top 1%; only 8.78% of existing generated references appear in the corresponding focal paper's ground-truth list; 22.24% appear in at least one ground-truth list elsewhere; various Wilcoxon tests p &lt; 0.001 for median citation-count differences",
            "baseline_comparison": "Compared to ground-truth references and to random baseline (random ground-truth reference lists from same field): existing generated references have significantly higher median citation counts and far greater concentration in top-cited percentiles; generated local citation graphs resemble ground-truth graphs and deviate strongly from random reshuffles (graph metrics reported in paper)",
            "challenges_limitations": "Substantial hallucination / fabrication of non-existent references (existence rate ~42%); conservative matching likely underestimates true existence; field-specific variation in existence rates (higher in humanities/social sciences); reliance on parametric knowledge only (no retrieval augmentation); possible training-data representation biases (long-tail underrepresentation); postprocessing and threshold choices affect results",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4351.0",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Program-search-LLM",
            "name_full": "Program search with large language models for mathematical discovery",
            "brief_description": "Approach that leverages LLMs to generate candidate programs/formal expressions (and potentially proofs) which are searched/evaluated to discover new mathematical relationships or theorems.",
            "citation_title": "Mathematical discoveries from program search with large language models.",
            "mention_or_use": "mention",
            "method_name": "Program search with LLMs",
            "method_description": "The referenced work (title only appears in bibliography) describes using LLMs coupled with program-search or program-synthesis procedures to generate candidate mathematical programs/expressions that can be evaluated to yield new mathematical results; the current paper cites it as an example of LLM-enabled mathematical discovery but does not describe internal details.",
            "llm_model_used": "",
            "scientific_domain": "Mathematics / theoretical discovery",
            "number_of_papers": null,
            "type_of_quantitative_law": "Mathematical identities, conjectures, theorems (symbolic mathematical relationships)",
            "extraction_output_format": "Symbolic/mathematical expressions and program code (per title implication)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not detailed in this paper (only cited); general limitations may include reliance on evaluator/verifier components and correctness checks",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.1",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Sciagents",
            "name_full": "Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "brief_description": "A multi-agent system that integrates graph reasoning and intelligent agents to automate aspects of scientific discovery and hypothesis generation from structured scientific knowledge.",
            "citation_title": "Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning.",
            "mention_or_use": "mention",
            "method_name": "Multi-agent intelligent graph reasoning (Sciagents)",
            "method_description": "Cited as an example of systems that integrate LLMs/agents into larger pipelines for scientific discovery; specific pipeline details are not provided in the current paper beyond the title reference.",
            "llm_model_used": "",
            "scientific_domain": "General scientific discovery / cross-domain",
            "number_of_papers": null,
            "type_of_quantitative_law": "Hypotheses and inter-document relationships represented on graphs (patterns/relationships between scientific entities)",
            "extraction_output_format": "Graph-structured relations and agent-generated hypotheses",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.2",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AI-Scientist",
            "name_full": "The AI scientist (fully automated open-ended scientific discovery)",
            "brief_description": "Concept/approach aiming toward fully automated, open-ended scientific discovery by combining AI agents and experimental or simulation validation.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "method_name": "Fully automated discovery agents",
            "method_description": "Cited among works integrating LLMs into discovery pipelines; the present paper references it as part of the literature on autonomous research agents but does not provide operational details.",
            "llm_model_used": "",
            "scientific_domain": "General / open-ended scientific discovery (cross-domain)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Scientific hypotheses, experimental protocols, candidate relationships emergent from agent proposals",
            "extraction_output_format": "Proposed hypotheses, experiment designs, candidate relationships (likely structured text/commands)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.3",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MiningCausality-IV",
            "name_full": "Mining causality: AI-assisted search for instrumental variables",
            "brief_description": "An AI-assisted approach to search for instrumental variables in observational data or literature to support causal inference, i.e., automated discovery of instruments and causal relationships.",
            "citation_title": "Mining causality: Ai-assisted search for instrumental variables.",
            "mention_or_use": "mention",
            "method_name": "AI-assisted instrumental-variable search",
            "method_description": "Referenced as an example of AI assisting in causal discovery from literature; the current paper cites the work by title but gives no internal methodological detail.",
            "llm_model_used": "",
            "scientific_domain": "Causal inference / social sciences / econometrics",
            "number_of_papers": null,
            "type_of_quantitative_law": "Causal relationships and instrumental-variable candidates (causal inference constructs)",
            "extraction_output_format": "Candidate instruments and causal-relationship descriptions (structured textual proposals)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.4",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "An iterative pipeline that uses LLMs to propose research ideas and iterate over literature context to refine suggestions.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "mention_or_use": "mention",
            "method_name": "Iterative LLM research idea generation (ResearchAgent)",
            "method_description": "Cited as an example of LLM-based systems to synthesize literature and generate research ideas in an iterative loop; the present paper does not give internal algorithmic specifics.",
            "llm_model_used": "",
            "scientific_domain": "Cross-domain research synthesis",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not explicitly quantitative in this paper's citation — focuses on idea generation and synthesis, but may surface patterns or candidate relationships",
            "extraction_output_format": "Textual research ideas and literature-linked suggestions",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.5",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Researcharena",
            "name_full": "Researcharena: Benchmarking LLMs' ability to collect and organize information as research agents",
            "brief_description": "A benchmarking effort that evaluates LLM-based agent systems' capability to collect, organize, and synthesize scholarly information as research agents.",
            "citation_title": "Researcharena: Benchmarking llms' ability to collect and organize information as research agents.",
            "mention_or_use": "mention",
            "method_name": "Benchmark for LLM research-agent capabilities",
            "method_description": "Cited in the paper as a benchmark assessing how well LLM agents perform information collection and organization tasks; details not provided in the current text.",
            "llm_model_used": "",
            "scientific_domain": "Methodology / NLP benchmarking for research agents",
            "number_of_papers": null,
            "type_of_quantitative_law": "Benchmark metrics for information collection and organization (not a law per se)",
            "extraction_output_format": "Benchmark scores and task-specific outputs",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.6",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AgentLab",
            "name_full": "Agent laboratory (LLM agents as research assistants)",
            "brief_description": "An experimental system using LLM agents as research assistants to perform literature-related tasks and research workflows.",
            "citation_title": "Agent laboratory: Using llm agents as research assistants.",
            "mention_or_use": "mention",
            "method_name": "LLM-agent research assistant pipeline (AgentLab)",
            "method_description": "Cited among recent works using LLM agents to assist research workflows and literature synthesis; the primary paper does not describe the internal pipeline.",
            "llm_model_used": "",
            "scientific_domain": "Research workflow assistance / cross-domain",
            "number_of_papers": null,
            "type_of_quantitative_law": "Primarily assistance and synthesis; may surface patterns but not explicitly described here",
            "extraction_output_format": "Agent-produced structured outputs and proposals",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.7",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LanguageAgents-Synthesis",
            "name_full": "Language agents achieve superhuman synthesis of scientific knowledge",
            "brief_description": "Claimed/experimental systems of LLM-based agents that synthesize scientific knowledge at or beyond human-level capability in certain synthesis tasks.",
            "citation_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "mention_or_use": "mention",
            "method_name": "LLM-agent synthesis pipeline",
            "method_description": "Cited as evidence that agent-based LLM systems can perform high-level literature synthesis; the current paper references the title but provides no procedural details.",
            "llm_model_used": "",
            "scientific_domain": "Scientific synthesis (cross-domain)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Synthesis-level findings and high-level patterns across literature (not specified)",
            "extraction_output_format": "Synthesized reports and proposed connections",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.8",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "VirtualLab-nanobodies",
            "name_full": "The virtual lab: AI agents design new SARS‑CoV‑2 nanobodies with experimental validation",
            "brief_description": "An AI-agent-driven pipeline that designs candidate nanobodies and includes experimental validation to confirm designed molecules' efficacy.",
            "citation_title": "The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation.",
            "mention_or_use": "mention",
            "method_name": "AI-agent driven molecular design with experimental validation",
            "method_description": "Cited as an example of agent systems that go beyond literature synthesis to propose experimentally validated discoveries; the present paper references only the title and general context.",
            "llm_model_used": "",
            "scientific_domain": "Molecular design / computational biology / chemistry",
            "number_of_papers": null,
            "type_of_quantitative_law": "Design-to-experimental quantitative outcomes (binding affinities, assay results) implied but not described here",
            "extraction_output_format": "Molecular designs and experimentally measured numeric outcomes",
            "validation_method": "Experimental validation (per title), details not provided in this paper",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4351.9",
            "source_info": {
                "paper_title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathematical discoveries from program search with large language models.",
            "rating": 2,
            "sanitized_title": "mathematical_discoveries_from_program_search_with_large_language_models"
        },
        {
            "paper_title": "Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning.",
            "rating": 2,
            "sanitized_title": "sciagents_automating_scientific_discovery_through_multiagent_intelligent_graph_reasoning"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Mining causality: Ai-assisted search for instrumental variables.",
            "rating": 2,
            "sanitized_title": "mining_causality_aiassisted_search_for_instrumental_variables"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "rating": 1,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Researcharena: Benchmarking llms' ability to collect and organize information as research agents.",
            "rating": 1,
            "sanitized_title": "researcharena_benchmarking_llms_ability_to_collect_and_organize_information_as_research_agents"
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants.",
            "rating": 1,
            "sanitized_title": "agent_laboratory_using_llm_agents_as_research_assistants"
        },
        {
            "paper_title": "The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation.",
            "rating": 1,
            "sanitized_title": "the_virtual_lab_ai_agents_design_new_sarscov2_nanobodies_with_experimental_validation"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "rating": 1,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        }
    ],
    "cost": 0.01948125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HOW DEEP DO LARGE LANGUAGE MODELS INTERNALIZE SCIENTIFIC LITERATURE AND CITATION PRACTICES?
April 4, 2025</p>
<p>Andres Algaba andres.algaba@vub.be 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Vincent Holst 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Floriano Tori 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Melika Mobini 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Brecht Verbeken 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>Sylvia Wenmackers 
Centre for Logic and Philosophy of Science (CLPS)
KU Leuven
3000LeuvenBelgium</p>
<p>Vincent Ginis 
Data Analytics Lab
Vrije Universiteit Brussel
1050BrusselBelgium</p>
<p>School of Engineering and Applied Sciences
Harvard University
02138CambridgeMassachusettsUSA</p>
<p>How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?</p>
<p>How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?</p>
<p>HOW DEEP DO LARGE LANGUAGE MODELS INTERNALIZE SCIENTIFIC LITERATURE AND CITATION PRACTICES?
April 4, 2025DA685775EEE600D0CD8D7906A56E4FA8arXiv:2504.02767v1[cs.DL]
The spread of scientific knowledge depends on how researchers discover and cite previous work.The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices.However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics.Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references.This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases.Analyzing 274, 951 references generated by GPT-4o for 10, 000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors.Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations.These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends.As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have evolved beyond simple natural language processing tasks, demonstrating remarkable capabilities in, for example, mathematical problem-solving and code generation [11,12].This expansion of capabilities has led to rapid adoption in scientific research [8,67], where LLMs are now being applied to support data science analysis [41,65], assist in experiments [36,40,54,111], and integrate into larger systems for scientific discovery [7,61,66,78,98,110].Since scientists are even experimenting with LLMs as autonomous research agents [6,9,30,31,60,79,82,90], we focus on their potential to assist in scientific synthesis [21,22,32,48,85,89] -a development that could significantly impact the dissemination of scientific information and potentially skew the landscape of scientific knowledge [33,80,83,92].We sample 10, 000 focal papers from all SciSciNet [58] papers which are published in Q1 journals between 1999 and 2021, have in between 3 and 54 references, and have at least 1 or more citations (n=17, 538, 900).We prompt GPT-4o to generate suggestions of references based on the title, authors, year, venue, and abstract of a focal paper, where the number of requested generated references corresponds to the ground truth number of references made in the focal paper, which amounts to a total of 274, 951 references.We verify the existence of the generated references via the SciSciNet [58] database and compare the characteristics, such as title length, publication year, venue, number of authors, and semantic embeddings, of the existing and non-existent generated references with the ground truth.For the existing generated references, we also compare additional characteristics, such as the number of citations and references, and analyze the properties of their citation networks.</p>
<p>Current approaches to LLM-assisted scientific synthesis predominantly use document retrieval methods [28,56], complemented by model fine-tuning [57,89] or citation network navigation [85].However, the reliance on search methods within external databases is unlikely to introduce significant new information dissemination patterns, potentially leading to effects similar to those observed with the introduction of comprehensive academic search tools [25,27,71].We argue that as LLMs improve their capabilities of generating reference suggestions through the parametric knowledge acquired during training [4], scientists could leverage their internal world models [35,37] to make more informed decisions about which prior work to cite and build upon [100].However, it remains unclear to what extent LLMs align with human citation practices, perform across domains, and may influence existing citation patterns.</p>
<p>We address these questions on LLM-generated recommendations by comparing the characteristics of human citation patterns and reference suggestions generated by LLMs solely relying on their parametric knowledge.Scientific literature has long exhibited well-documented characteristics and potential biases in citation practices [10,59,87], which vary across fields [76,102,109], including preferences for recent publications [93], articles with shorter titles [55], papers from high-profile venues [53], and works with multiple authors [29,96], as well as the "Matthew effect," by which highly cited papers accumulate even more citations over time [52,74,104].We argue that LLMs may introduce or exacerbate (field-specific) citation patterns [1,13,43,70,97] due to their struggle with long-tail knowledge [47] combined with uneven representation of scientific work in LLMs' training data [23,95].While our experimental setup may not fully reflect real-world usage of LLMs for citation generation, which often involves more interactivity and reliance on external data sources, it provides a controlled laboratory setting to assess the parametric knowledge and inherent citation patterns of LLMs [12].Understanding these patterns is crucial as they may influence how researchers discover and cite prior work, potentially reinforcing existing biases in scientific literature and shifting citation practices over time [33,80,83,92].A1).b, The temporal trend in the number of focal papers exhibits linear growth from 1999 until 2021, which aligns with full SciSciNet [58] database for this period.c,d, Both the median number of references cited per focal paper and the median team size are increasing over time.This pattern is more clear in the fields with a larger number of focal papers (e.g., biology, chemistry, and medicine).The color intensity represents the magnitude of the values: darker shades indicate higher numbers, while lighter shades represent lower values.Hatched cells indicate no data available for a given year and field.</p>
<p>Experimental setup for generating references</p>
<p>LLMs are known to produce hallucinations or confabulations -generating content that contains factual errors or makes claims unsupported by specific sources, such as incorrect statements about historical events [44].Since LLMs only provide suggestions, researchers can verify both existence and appropriateness through external databases [2,26,83].However due to automation bias [86], these suggestions may nonetheless steer their search process for discovering and building upon prior work [33].In our experiment, we cross-check all generated references against the SciSciNet [58] database using fuzzy matching, classifying references as "existing" based on conservative similarity thresholds, likely underestimating the true existence rate due to these methodological choices (see Appendix).</p>
<p>Our experimental setup differs from both traditional LLM benchmarks [15,18,19,38,39,45,72,77,88] and previous assessments of LLMs in scientific literature contexts [2,3,4,16] in several key aspects.Rather than asking LLMs to April 4, 2025 attribute open-ended scientific claims [73], write literature reviews [48,101], or suggest in-text references [4], we provide paper abstracts as input.Abstracts contain more condensed information, reducing reliance on memorization [14,46], while their standardized format across scientific fields and concise nature better reflect how researchers might query these models in practice.Moreover, while previous work primarily evaluates suggested references based on their existence, bibliometric accuracy, or expert judgment [75], we analyze how LLMs' citation patterns align with human citation practices.Finally, we focus on the LLM's parametric knowledge acquired during training, without enhancing its capabilities through search or retrieval-augmented generation [48,56,61].This focus on parametric knowledge allows us to examine LLMs as reasoning engines that may reshape how scientists discover connections in literature [35,99], distinct from search-based approaches that may largely reflect existing information dissemination patterns [25].</p>
<p>Figure 2 summarizes key characteristics of the focal paper sample (n=10, 000) across fields.The distribution of papers shows a clear predominance of exact sciences, particularly medicine, biology, and chemistry, with substantially fewer papers from humanities and social sciences (Figure 2a), reflecting the relative field proportions in the full SciSciNet [58] database.The temporal trend in the number of focal papers exhibits linear growth from 1999 until 2021, which aligns with full SciSciNet [58] database for this period.The commonly reported exponential growth in scientific publications is observed in the decades preceding this period.We also observe two distinctive temporal trends consistent with broader patterns in scientific publishing: a steady increase in both the median number of references per paper (Figure 2c) and median team size (Figure 2d).These trends compare to previous findings on the expansion of reference lists [42] and the growing prevalence of larger research teams [58,107], suggesting our sample captures key dynamics in modern scientific practice.</p>
<p>Reinforcing the Matthew effect in citations</p>
<p>Figure 3 displays the existence rate, i.e., the fraction of generated references that corresponds to an existing reference in the SciSciNet [58] database, of generated references (gray, n=274, 497), and the citation characteristics of the ground truth (blue, n=274, 951) and existing generated (orange, n=116, 939) references across fields and time.Humanities and social sciences yield significantly higher existence rates than exact sciences (Figure 3a), though the overall rate remains stable between 40% and 50% across publication years of the focal papers (Figure 3e).The field-level variation persists even when controlling for the varying sample sizes of focal papers across fields through subsampling (Appendix Figure A1), and the existence rates on the focal paper level show only moderate negative correlation (−0.10, p&lt;0.001) with the number of requested references (Appendix Figure A2).The higher existence rates in humanities and social sciences can be partially attributed to their tendency to cite older references both in absolute terms (Appendix Figure A3b) and relative to the publication year of the focal paper (Appendix Figure A3c,d), as existence rates drop significantly for more recently published references (Appendix Figure A3a).</p>
<p>Existing generated references exhibit significantly higher median citation counts than ground truth references across all scientific fields (Figure 3b) and throughout all publication years of the focal papers (Figure 3d).This reinforcement of the Matthew effect in citations is confirmed through the pairwise two-sided Wilcoxon signed-rank tests at the focal paper level (see Appendix).The observed gap in median citation counts between existing generated references and ground truth references remains even when selecting, for each focal paper, the same number of most highly cited ground truth references as the number of existing generated references.This confirms that the higher citation counts in existing generated references are not merely due to selection bias (Appendix Figure A4).The robustness of this effect is further strengthened by alternative citation metrics, including normalized citations [76] and citations after 5 and 10 years [103] (Appendix Figure A5a), and the persistent gap in median citation counts across publication years of references (Appendix Figure A5b).The strong preference toward highly cited papers is also evidenced with approximately 90% of existing generated references appearing in the top 10% most cited papers within their respective field and year [58], and remarkably, over 60% falling within the top 1% -more than twice the rate observed in ground truth references (Appendix Figure A5c).</p>
<p>Several factors may contribute to this citation disparity (Appendix Figure A6).The existing generated references have a higher proportion of books compared to ground truth references, which typically accumulate more citations.However, even the journal articles among the existing generated references achieve citation counts comparable to books in the ground truth references (Appendix Figure A6a-c).Moreover, the existing generated references demonstrate broader impact beyond traditional academic citations, exhibiting significantly higher rates of alternative citations such as patents and clinical trials, greater media attention through news coverage and social media mentions, and more frequent association with major funding sources like NIH and NSF grants (Appendix Figure A6d).Interestingly, despite these pronounced differences in citation impact, the reference counts between existing generated and ground truth papers show more moderate differences across fields, suggesting that the number of references is an unlikely source for the citation discrepancy [64] (Figure 3c).</p>
<p>Systematic preferences in generated references</p>
<p>Figure 4 summarizes key characteristics, including publication year, number of authors, title length, and publication venue, for ground truth (blue, n=274, 951), generated (green, n=274, 497), existing generated (orange, n=116, 939), and non-existing generated (red, n=157, 558) references, indicating the model's internalization of citation patterns.The temporal distribution shows that generated references tend to be relatively more recent than ground truth references (Figure 4a), with the median difference being statistically significant as confirmed by a pairwise two-sided Wilcoxon signed-rank test at the focal paper level (p&lt;0.001).The non-existent generated references amplify the recency bias documented in human citation behavior [93], while existing generated references show a more complex pattern: they under-cite earlier publications (pre-2000) compared to the ground truth, cite relatively more recent works between 2000 and 2012, and again under-cite in the most recent period, resulting in a median publication year equal to the ground truth.This temporal pattern of existing generated references suggests that their higher citation impact is not driven by a recency bias (Figure 3d, Appendix Figure A5b).The concentration of references in the 2000s reflects both ground truth and generated references peaking for papers published within 0-5 years before the focal paper (Appendix Figure A3c), as publications from this period would be recent relative to many focal papers in our 1999-2021 sample.</p>
<p>The preference for smaller author teams emerges as another systematic pattern in generated references (Figure 4b).While ground truth references span author teams of 2-6 members, generated references concentrate on papers with 2-3 authors, and existing generated references focus even more narrowly on teams of 1-3 authors.This pattern, confirmed as statistically significant by a pairwise two-sided Wilcoxon signed-rank test at the focal paper level (p&lt;0.001),stands in contrast to documented citation preferences for papers with larger author teams [29,96].The relatively small proportion of generated references labeled as "et al." (3%) is primarily driven by non-existent references (4%), while existing generated references show a lower rate (1.5%), suggesting that GPT-4o occasionally defaults to simplified author attribution when generating references that prove to be non-existent [2] (Figure4b).</p>
<p>Generated references show a clear preference for shorter titles (Figure 4c), with this effect being most pronounced in existing generated references.This pattern aligns with documented human citation behavior, where papers with shorter titles tend to receive more citations [55].The statistically significant difference in median title length between ground truth and generated references (p&lt;0.001)suggests that the model amplifies this human preference for concision.Together with the preference for smaller teams and the simplified author attribution, this tendency toward shorter titles may indicate a broader pattern of LLMs favoring simpler, more condensed bibliographic information.The two previous findings are exactly as expected, given that longer phrases are both harder to learn and to reproduce without error.</p>
<p>The analysis of publication venues reveals a strong preference for high-impact journals (Figure 4d).Consistent with documented human citation preferences for prestigious venues [53], our findings show that generated references reinforce this citation pattern.The journal rankings demonstrate substantial overlap among leading multidisciplinary journals like Nature and Science, but also a notable concentration in top-tier field-specific journals such as The New England Journal of Medicine and Journal of the American Chemical Society.While PLOS ONE and Applied Physics Letters appear exclusively in the ground truth top 10, Cell, JAMA, and Circulation emerge uniquely in the existing generated references, suggesting that LLMs systematically favor references from the most prestigious venues within each field.Note that for the ground truth and existing generated references we can use the publication venue from the SciSciNet [58] database, whereas for the non-existent generated references we use a validated text extraction method (Appendix Figure A7).Given the low representation of conference proceedings in our dataset (Appendix Figure A6a), we focus our analysis only on journal publications.</p>
<p>Network properties of generated references</p>
<p>Figure 5 shows the distributions of the pairwise cosine similarity between textual embeddings [5,51,68] of the titles of ground truth (blue, n=274, 951), generated (green, n=274, 497), existing generated (orange, n=116, 939), and nonexisting generated (red, n=157, 558) references with the title and abstract of their corresponding focal paper (n=10, 000).</p>
<p>As a benchmark, we also compute for each focal paper the pairwise cosine similarity with the reference from a random ground truth reference list from the same field (gray, n=274, 951).Notably, the non-existing generated references show slightly higher overall similarity scores than their existing counterparts, suggesting a tendency to overfit to the abstract and title by producing thematically aligned but non-existing references.However, all generated references, both existing and non-existent, remain substantially more content matched than random ground truth reference lists from the same field, underscoring the system's strong capacity for content alignment.We obtain qualitatively similar results with a different embeddings model (SPECTER2 [84]) and by only using the title of the focal paper (Appendix Figure A8).Interestingly, existing generated references exhibit significantly lower self-citation rates at the single-author level [91], while still capturing the field-level variations observed in ground truth references (Appendix Figure A9).</p>
<p>How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?</p>
<p>April 4, 2025</p>
<p>Existing generated references exhibit notable overlap with human citation patterns, yet they do not perfectly replicate ground truth references at the corresponding focal paper level, suggesting that LLMs do not rely solely on direct memorization of citation lists [68].Only 8.78% of existing generated references appear in the corresponding ground truth reference list, indicating that LLMs' reference generation is not driven by simple recall but rather by a broader pattern-matching process.However, when expanding the scope beyond corresponding focal papers, 22.24% of existing generated references are found in at least one ground truth reference list across the dataset, suggesting that a substantial portion of LLM-generated references are genuinely cited in scientific literature.Additionally, 28.70% of existing generated references overlap between different focal papers, revealing a preference for a recurring subset of frequently suggested references.This systematic overlap, combined with the strong preference for highly cited works, reinforces the notion that LLM-generated references amplify dominant citation patterns rather than diversifying the scientific discourse.</p>
<p>In Figure 6, we investigate three distinct types of local citation graphs for each of the 10, 000 focal papers.Those graphs are based on the ground truth references, the (existing) GPT4-o-generated references, and randomly reshuffled ground truth references where we fix the field of study of the focal paper.Interestingly, we find that the graphs corresponding to the LLM-generated references strongly resemble human citation networks in terms of multiple graph-based measures, and deviate significantly from the random baseline, see Figure 6c-h.This highlights the capacity of GPT4-o to internalize human citation patterns even on second-order graph structures [68].In Appendix Figure A10, we also find that the existing generated references are much better connected to the ground truth references compared to the random baselines, both individually and on aggregate.</p>
<p>Discussion</p>
<p>In summary, our findings reveal that LLMs do not merely replicate human citation practices but instead introduce systematic biases that reinforce dominant citation patterns.By analyzing 274, 951 references generated by GPT-4o for 10, 000 focal papers, we demonstrate that LLM-generated references exhibit a strong preference for highly cited papers, reinforcing the Matthew effect in citations.While the generated references align well with ground truth references in terms of content relevance, they systematically favor more recent works with shorter titles and fewer authors.Moreover, we find that LLM-generated references reduce author self-citations, suggesting that they deprioritize self-referential tendencies that are common in human citation behavior.Note that these biases and observed field-specific differences may partially stem from uneven representation of scientific works within the LLM's training corpus, especially affecting references from the long tail.Despite these differences, LLM-generated references maintain strong semantic alignment with focal papers, exhibiting cosine similarity scores comparable to human-curated citations across scientific fields.This tendency is even more pronounced for the local citation graphs corresponding to the (existing) generated references, resembling human citation networks across various graph-based measures compared to the random baseline.For both the embeddings and the graph based measures, the strong deviation from the random baseline, which essentially simulates randomly selecting references from the field of study of the focal paper, provides first evidence that LLMs do not merely suggest highly cited papers, but are capable of finding suitable references across various scientific domains solely by using their parametric knowledge.</p>
<p>Overall, our study highlights both the potential and the limitations of LLMs in shaping scientific citation practices.While these models capture broad citation trends, they may not fully reflect the nuanced citation behavior observed in human references.One key limitation of our approach is that it evaluates LLM-generated references in a controlled setting without interactive prompting or access to external databases, which would more accurately reflect real-world usage.Furthermore, by relying solely on parametric knowledge, our analysis isolates inherent biases in the model but does not account for the potential effects of retrieval-augmented generation.These constraints suggest that future research should explore how different LLM interactions, retrieval mechanisms, and training data compositions influence citation generation.</p>
<p>We open-source our dataset, extending the publicly available SciSciNet [58], thereby encouraging researchers interested in the intersection of LLM capabilities and the science of science to explore the potential impact of LLMs on scientific citation practices further.The implications of our findings extend beyond citation generation to broader discussions on automated scientific synthesis and knowledge dissemination.As LLMs become increasingly integrated into academic search engines, literature reviews, and automated research assistants, their influence on citation patterns could reshape how researchers discover and build upon prior work.Understanding these mechanisms is crucial for ensuring that AI-driven tools enhance, rather than distort, the trajectory of scientific discovery.Future work should assess how these models interact with real-world citation networks, peer review processes, and interdisciplinary knowledge flows, helping to guide the responsible deployment of LLMs in scientific research.Elasticsearch Retrieval.We rely on Elasticsearch [24] to search the "Title" field in the SciSciNet index, which uses a well-known ranking approach called BM25 [81].Elasticsearch returns the three best-matching records for every generated reference title.This ensures we capture the most relevant candidates, even if there are small variations in spelling or spacing.</p>
<p>Fuzzy Matching.From the three candidates, we compare both the reference's title and its list of authors to each candidate's stored metadata.For the title comparison, we use partial string matching based on the Levenshtein distance, allowing us to handle truncated or rearranged segments [17].For the authors, we check for common tokens across the two author strings, again leveraging the Levenshtein distance to accommodate partial matches and differences in order [17].Each candidate is assigned two scores-one for title similarity and one for author similarity-which we then average into a single overall match score.</p>
<p>Existence Thresholds.We select the candidate with the highest overall match score.To confirm it as a genuine match, the title similarity must exceed a high threshold of 0.90 (indicating near-identical titles), while the author similarity must surpass a more moderate threshold of 0.50 (reflecting adequate alignment in author strings).Only if both conditions are met do we label the reference as "existing."Otherwise, it is deemed "non-existent."</p>
<p>Pairwise two-sided Wilcoxon signed-rank test</p>
<p>Comparing distributions of bibliometric data, such as citation or reference counts, is challenging due to skewness, heavy tails, and potential within-paper dependencies.Additionally, because we generate references solely from each focal paper's title and abstract-rather than having a one-to-one correspondence with a paper's actual references-we cannot directly match references between ground truth and GPT-4o outputs.Instead, for each focal paper, we compute the median of the variable of interest (e.g., the median citation count) for its ground truth references and for the (existing or non-existent) LLM-generated references.We then consider the distribution of these per-paper differences across all focal papers in a given field.The Wilcoxon signed-rank test then evaluates whether these differences systematically deviate from zero.This approach avoids strong distributional assumptions or independence requirements, making it well-suited for the heavy-tailed nature of citation data and for the potential dependencies among references within a single focal paper.</p>
<p>Embeddings</p>
<p>We measure content-level alignment between focal papers and references by comparing the cosine similarity of their textual embeddings.Specifically, for each focal paper (title and abstract) and each reference (title only), we compute embeddings using either OpenAI text-embedding-3-large [5,51] or SPECTER2 [84].The vector representations capture semantic relationships between documents, enabling a pairwise cosine similarity metric that quantifies how semantically aligned a reference is to the focal paper.The OpenAI text-embedding-3-large model generates 3, 072dimensional embeddings.To confirm the robustness of our results, we repeat the same similarity comparisons using SPECTER2, a 768-dimensional embedding model designed for scientific texts.</p>
<p>We experiment with two settings: (1) embeddings computed from titles only, and (2) embeddings computed from both titles and abstracts.For each focal paper, we also compute cosine similarities with references drawn at random from another ground truth paper in the same field.These random reference scores serve as a baseline for context-unaware matching.</p>
<p>Citation networks</p>
<p>Let G = (V, E) be a simple undirected graph, where V denotes the set of nodes and E denotes the set of edges.Let |V | denote the number of nodes.The distance between two nodes u, v ∈ V is denoted by d(u, v) and equals the numbers of edges in the shortest path between the two nodes.The average shortest path length L of the graph G is given by
L = 1 |V |(|V | − 1) u̸ =v∈V d(u, v) .
The density D of G is the number of edges of the graph relative to the number of possible edges between all the nodes in the graph
D = |E| |V | 2 = 2|E| |V |(|V | − 1)
.</p>
<p>For a node v ∈ V with degree d v , let e(v) denote the number of edges among its neighbors.The clustering coefficient of v is defined as
C(v) = 2 e(v) d v d v − 1 .
The average clustering coefficient µ C of G is then
µ C = 1 |V | v∈V C(v) .
For a node v ∈ V with degree d(v), the degree centrality C D (v) can be defined as
C D (v) = d v |V | − 1 ,
reflecting the fraction of possible connections that v actually has.The closeness centrality of a node v ∈ V is determined by the reciprocal of the average shortest path from v to all other nodes:
C C (v) = |V | − 1 u̸ =v d(u, v)
.</p>
<p>Let C E (v) be the eigenvector centrality of node v ∈ V defined as
C E (v) = 1 λ u∈N (v) C E (u) ,
where λ is the largest eigenvalue of the adjacency matrix of the graph.The standard deviation of the eigenvector centrality, σ C E , is
σ C E = 1 |V | v∈V C E (v) − µ C E 2 ,
where µ C E is the mean of the eigenvector centralities.</p>
<p>Figures</p>
<p>Figure A1: ).Existing generated references consistently show higher median citation counts across all metrics.b, The median citation count of ground truth and existing generated references as a function of their publication year.The consistently higher citation counts for existing generated references suggest that their impact is not simply due to a recency bias but rather a systematic preference for highly cited works.c, The proportion of ground truth and existing generated references appearing in the top 10%, 5%, and 1% most cited papers within their respective field and publication year [58].Approximately 90% of existing generated references fall within the top 10%, and over 60% appear in the top 1%, more than twice the rate observed for ground truth references, reinforcing the Matthew effect in citations.We then show how the full graph is split up into two distinct citation graphs used for the analysis in Figure 6.Here, we distinguish between references that are both cited by the focal paper (blue node) and suggested by GPT-4o (green nodes), non-isolated generated references that that are not cited by the focal paper but still connected to at least one other reference (ground truth or generated) (yellow nodes), isolated generated references that are not cited by the focal paper and also not connected to any other reference (orange nodes), and the remaining ground truth references that are not suggested by GPT4-o (grey).An arrow between A and B corresponds to a citation from A to B. We use the SciSciNet [58] dataset to retrieve this citation information.For each of the 10, 000 focal papers (blue), we construct the full graph consisting of both the ground truth references and the existing references suggested by GPT4-o as displayed on the left hand side.The full graph is then split up into the sub-graph corresponding to the focal paper and the LLM-generated references (blue, green, yellow and orange nodes) and the sub-graph corresponding to the focal paper and the ground truth references (blue, green and grey nodes).To ensure a fair comparison, each of those two graphs undergoes a further post-processing step.First, for the generated graph, we add an edge from the focal paper to the generated references not initially cited by the focal paper (yellow and orange nodes) to ensure the connectivity of the graph.Second, for ground truth graph, since approximately 50% of the GPT4-o-generated references exist (see Fig. 3a), we randomly remove a subset of references (green and grey nodes) to ensure it is equal in size to the generated graph on a focal paper level.For analytical simplicity both graphs were converted to undirected graphs.The number of citations 5 years after publication.C10</p>
<p>The number of citations 10 years after publication.</p>
<p>Patent_Count</p>
<p>The number of citations by patents from USPTO and EPO.</p>
<p>NCT_Count</p>
<p>The number of citations by clinical trials from ClinicalTrials.gov.</p>
<p>Newsfeed_Count</p>
<p>The number of mentions by news from Newsfeed.</p>
<p>Tweet_Count</p>
<p>The number of mentions by tweets from Twitter.</p>
<p>NIH_Count</p>
<p>The number of supporting grants from NIH.</p>
<p>NSF_Count</p>
<p>The number of supporting grants from NSF.</p>
<p>Fig. 2 -Figure 1 :
21
Fig.2 -Focal papers analysis</p>
<p>Figure 1 Figure 2 :
12
Figure1displays our experimental setup where we sample 10, 000 focal papers from the SciSciNet[58] database which are published in Q1 journals between 1999 and 2021, have in between 3 and 54 references, and have at least 1 or more citations.Additionally, we require the sampled focal paper to have a "top field" and a valid DOI with an abstract (n=17, 538, 900, see Appendix).We then prompt GPT-4o to generate suggestions of references based on the title, authors, year, venue, and abstract of a focal paper, where the number of requested generated references corresponds to the ground truth number of references made in the focal paper, which amounts to a total of 274, 951 references.</p>
<p>April 4, 2025 Figure 3 :
20253
Figure 3: Existing generated references reinforce the Matthew effect in citations.This figure displays the existence rate of generated references (gray, n=274, 497), and the citation characteristics of the ground truth (blue, n=274, 951) and existing generated (orange, n=116, 939) references across fields and time.Error bars and shaded bands represent 95% confidence intervals.a, The existence rate of generated references by field of the focal paper shows significantly lower values in the exact sciences compared to the humanities and the social sciences.b, Median citation counts revealthat existing generated references tend to have higher citation counts across all fields, suggesting a preference toward already highly cited works.The pairwise two-sided Wilcoxon signed-rank test at the focal paper level confirms that the existing generated references have a statistically significant higher median citation count for all fields (history, p=0.003; philosophy, p=0.022: all other fields, p&lt;0.001).c, The median reference counts tend to be more similar for many fields, with only political science showing existing generated references to have a lower median number reference count.The pairwise two-sided Wilcoxon signed-rank test at the focal paper level shows that the existing generated references have a statistically significant higher median reference count for biology (p&lt;0.001),chemistry (p&lt;0.001),environmental science (p&lt;0.001),geography (p=0.007),materials science (p&lt;0.001),mathematics (p&lt;0.001),medicine (p&lt;0.001),psychology (p&lt;0.001), and sociology (p=0.002).All other fields show no statistically significant difference (p&gt;0.05).d, Temporal trends at the focal paper level show that existing generated references consistently exhibit higher median citation counts compared to ground truth references, further emphasizing the reinforcement of the Matthew Effect in citations.e, The overall existence rate of generated references remains consistent across the publication year of the focal paper, fluctuating between 40% and 50%.</p>
<p>Figure 4 :
4
Figure4: Generated references exhibit a systematic preference for more recent references with shorter titles and fewer authors.This figure summarizes key characteristics for ground truth (blue, n=274, 951), generated (green, n=274, 497), existing generated (orange, n=116, 939), and non-existing generated (red, n=157, 558) references.a, The relative frequency of publication years within each reference group, with median publication years indicated by vertical lines, shows that generated references are generally more recent than the ground truth.This recency bias is driven by non-existent generated references, which disproportionately cite more recent publications.Existing generated references show a more complex pattern, tempering the overall recency bias in the generated references.The pairwise two-sided Wilcoxon signed-rank test at the focal paper level confirms the statistically significant difference in median publication year between ground truth and generated references (p&lt;0.001).b, The distribution of the number of authors shows that generated references tend to favor documents with fewer authors with a peak aroud 2-3 (1-3 for existing generated references) authors, compared to 2-6 authors for ground truth references.A small proportion of generated references are labeled as "et al." (3%), with higher rates in non-existent (4%) than existing generated references (1.5%).The pairwise two-sided Wilcoxon signed-rank test at the focal paper level confirms the statistically significant difference in the median number of authors between ground truth and generated references (p&lt;0.001).c, The distribution of the title length shows that generated references tend to favor documents with shorter titles.This effect is most outspoken for the existing generated references.The pairwise two-sided Wilcoxon signed-rank test at the focal paper level confirms the statistically significant difference in the median title length between ground truth and generated references (p&lt;0.001).d, The journal rankings show the top 10 journals across different reference groups.The size of each dot represents how relatively frequently that journal appears within its reference group.Journals are connected by solid lines when appearing in all three groups' top 10, dotted lines when appearing in two groups' top 10, and shown in italic font when appearing in only one group's top 10, highlighting the distinct citation patterns across reference types.</p>
<p>Figure 5 :Figure 6 :
56
Figure5: Generated references exhibit a level of cosine similarity to focal paper titles and abstracts on par with ground truth references, surpassing that of a random ground truth reference list from the same field.This figure displays the distributions of the pairwise cosine similarity between OpenAI text-embedding-3-large vector embeddings (size=3, 072) of the titles of the ground truth (blue, n=274, 951), generated (green, n=274, 497), existing generated (orange, n=116, 939), and non-existing generated (red, n=157, 558) references with the title and abstract of their corresponding focal paper (n=10, 000).As a benchmark, we also compute for each focal paper the pairwise cosine similarity with the reference from a random ground truth reference list from the same field (gray, n=274, 951).</p>
<p>Figure A2 :Figure A3 :Figure A5 :
A2A3A5
Figure A1: The field-level variation in existence rates persists even when controlling for the varying sample sizes of focal papers across fields.This figure displays the distributions (n=100) of the existence rates of generated references across fields when we subsample 15 random focal papers from the same field.The subsample size of 15 corresponds to the number of focal papers in the field of philosophy which has the lowest number of focal papers.</p>
<p>April 4, 2025 Figure A6 :
2025A6
FigureA6: Differences in document types, citation impact, and broader influence between ground truth and existing generated references.This figure compares the distribution of document types, citation counts, and alternative impact indicators between ground truth references (blue, n=274, 951) and existing generated references (orange, n=116, 939).Error bars represent 95% confidence intervals.a, The distribution of document types, showing that both ground truth and generated references are predominantly journal articles, though existing generated references contain a slightly higher proportion of books.b, The difference in median citation counts for journals and books, highlighting that generated references tend to have higher citation counts, even when comparing journals and books separately.c, The citation distribution for journal articles and books, illustrating that existing generated references, even when limited to journal articles, achieve citation counts comparable to books in the ground truth references.d, The prevalence of alternative impact indicators, including citations in patents, clinical trials (NCT), news articles, tweets, and funding acknowledgments from NIH and NSF grants.Existing generated references exhibit consistently higher rates across these alternative impact measures, indicating a broader influence beyond academic citations.</p>
<p>Figure A7 :Figure A8 :
A7A8
Figure A7: Comparison of journal name identification methods for ground truth and generated references.This figure compares the frequency of top journal venues identified using two different methods: the SciSciNet [58] database (brown) and a validated text extraction method (yellow).For ground truth and existing generated references, publication venues are directly obtained from the SciSciNet[58] database.For non-existent generated references, where no direct database match is available, we extract journal names using exact matches for the official journal names and ISO-4 abbreviations.The comparison across major journals such as Nature, Science, NEJM, PNAS, Cell, and Lancet shows that the text extraction method produces similar frequency distributions to SciSciNet[58] for the existing generated references, validating its reliability for identifying publication venues.</p>
<p>Figure A9 :
A9
FigureA9: LLM-generated references exhibit consistently lower self-citation rates across scientific fields compared to ground truth references.This figure presents the proportion of author-level self-citations for ground truth references (blue, n=274, 951) and existing generated references (orange, n=116, 939) across different scientific disciplines.Self-citations are defined as instances where at least one author of the focal paper cites another work that includes their name, identified using SciSciNet[58] author-matching codes.While the self-citation rates of existing generated references follow a similar field-specific variation as ground truth references[91], they are consistently lower across all disciplines.This suggests that LLMs do not replicate human self-referential citation patterns at the same rate, potentially reducing self-citation biases and altering the dynamics of author visibility and impact.Error bars represent 95% confidence intervals.</p>
<p>Figure A10 :
A10
Figure A10:The pipeline depicting the generation of the local citation graphs corresponding to the ground truth references and the LLM-generated references.This figure displays the local citation graph of the focal paper consisting of both the ground truth and LLM-generated references.We then show how the full graph is split up into two distinct citation graphs used for the analysis in Figure6.Here, we distinguish between references that are both cited by the focal paper (blue node) and suggested by GPT-4o (green nodes), non-isolated generated references that that are not cited by the focal paper but still connected to at least one other reference (ground truth or generated) (yellow nodes), isolated generated references that are not cited by the focal paper and also not connected to any other reference (orange nodes), and the remaining ground truth references that are not suggested by GPT4-o (grey).An arrow between A and B corresponds to a citation from A to B. We use the SciSciNet[58] dataset to retrieve this citation information.For each of the 10, 000 focal papers (blue), we construct the full graph consisting of both the ground truth references and the existing references suggested by GPT4-o as displayed on the left hand side.The full graph is then split up into the sub-graph corresponding to the focal paper and the LLM-generated references (blue, green, yellow and orange nodes) and the sub-graph corresponding to the focal paper and the ground truth references (blue, green and grey nodes).To ensure a fair comparison, each of those two graphs undergoes a further post-processing step.First, for the generated graph, we add an edge from the focal paper to the generated references not initially cited by the focal paper (yellow and orange nodes) to ensure the connectivity of the graph.Second, for ground truth graph, since approximately 50% of the GPT4-o-generated references exist (see Fig.3a), we randomly remove a subset of references (green and grey nodes) to ensure it is equal in size to the generated graph on a focal paper level.For analytical simplicity both graphs were converted to undirected graphs.</p>
<p>How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?
April 4, 2025NameShort DescriptionPaper levelPaperIDMAG Paper ID of the document.PaperTitleTitle of the document.DOIDigital Object Identifier (DOI) of the document.DocTypeBook, BookChapter, Conference, Dataset, Journal, Repository,Thesis, or NULL (unknown).YearPublication year of the document.Reference_CountTotal reference count of the document.Citation_CountTotal citation count of the document.C5
AcknowledgementsAndres Algaba acknowledges a fellowship from the Research Foundation Flanders under Grant No.1286924N.Vincent Ginis acknowledges support from Research Foundation Flanders under Grant No.G032822N and G0K9322N.The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation -Flanders (FWO) and the Flemish Government.https://github.com/AndresAlgaba/LLMs_scientific_literatureData available at: https://zenodo.org/record/15124610Appendix SciSciNet dataWe use the SciSciNet[58]dataset, a large-scale bibliometric database, to construct our reference sample (https: //doi.org/10.6084/m9.figshare.c.6076908.v1).The selected variables and their descriptions are provided in Appendix TableA2.Merging all tables yields a total of 133, 518, 311 papers.From this corpus, we apply a series of selection criteria, restricting our sample to papers published in Q1 journals between 1999 and 2021, containing 3 to 54 references, and having received at least one citation.Additionally, we require focal papers to have an assigned "top field" classification and a valid DOI with an abstract, resulting in a filtered dataset of 17, 538, 900 papers.Abstracts are retrieved via CrossRef and Semantic Scholar APIs[20,49].From this refined dataset, we randomly sample 10, 000 focal papers for our analysis.Prompting GPT-4oWe use a straightforward prompting setup for GPT-4o, specifically version gpt-4o-2024-08-06.As format restrictions may degrade model performance on some tasks, we do not impose them here[94], and instead rely on a dedicated postprocessing step to parse and clean the model outputs.This choice inherently risks slightly degraded performance on certain tasks, but ensures the generation of references remains unconstrained and maximally reflective of the model's parametric knowledge.Our prompting strategy is intentionally simple, though we note that more sophisticated prompting methods[50,62,105,106,108]or deeper reasoning frameworks[34,69]could be investigated to improve existence rates and reduce fabrication of references.The system prompt for each focal paper is:Below, we share with you the title, authors, year, venue, and abstract of a scientific paper.Can you provide {n} references that would be relevant to this paper?The user prompt contains basic bibliographic metadata and the abstract: Here {n} is set to the ground-truth number of cited works for that focal paper.We subsequently parse the model's suggestions for bibliometric data (title, authors, number of authors, venue, publication year) in a postprocessing step using a second prompt (called via gpt-4o-mini-2024-07-18): Below, we share with you a list of references.Could you for each reference extract the authors, the number of authors, title, publication year, and publication venue?Please only return the extracted information in a markdown table with the authors, number of authors, title, publication year, and publication venue as columns.Do not return any additional information or formatting.This postprocessing prompt enforces a structured output format while minimizing reliance on stringent formatting instructions during the reference generation step itself.Existence checkWe assess whether each generated reference has a corresponding record in the SciSciNet[58]database using a two-step process: (1) retrieving candidate matches via Elasticsearch[24], and (2) applying fuzzy matching on both titles and authors.We manually verified a random sample of 100 references by cross-checking the titles and authors on Google Scholar.Within these 100 references, our existence algorithm identified 42 as existing, and all were correctly classified.Of the 58 non-existent, five actually exist.This shows that our existence algorithm may underestimate the actual existence rates of LLM-generated references by only using the SciSciNet[58]dataset.Here, we distinguish between references that are both cited by the focal paper and suggested by GPT-4o (green nodes), non-isolated generated references that that are not cited by the focal paper but still connected to at least one other reference (ground truth or generated) (yellow nodes), isolated generated references that are not cited by the focal paper and also not connected to any other reference (orange nodes), and the remaining ground truth references that are not suggested by GPT4-o (grey).An arrow between A and B corresponds to a citation from A to B. As suggested by the depicted graph, among the LLM-generated references and per focal paper, the majority of them are non-isolated (mean: 54%, median: 60%, standard deviation: 28%), whereas less are isolated (mean: 37%, median: 30%, standard deviation: 30%) or appear in the paper (mean: 9%, median: 4%, standard deviation: 13%).Below, we then display the fraction of non-isolated, LLM-generated references (per focal paper) that are connected to at least one ground truth reference.Here, we observe that most generated references are indeed citing or cited by at least ground truth references instead of merely being connected to another generated references.For example, for the graph displayed above this number is equal to 92%.This connectivity is further reflected in the edge expansion-the number of edges between ground truth references and non-isolated GPT-4o-generated references, normalized by the number of non-isolated GPT-4o-generated references-showing strong overall linkage (e.g., 3.58 in the depicted graph).b, The equivalent analyses where we randomly reshuffle the ground truth references while fixing the field of study of the focal paper.Among the LLM-generated references and per focal paper, we now find that the majority of them are either non-isolated (mean: 49%, median: 56%, standard deviation: 33%) or isolated (mean: 51%, median: 44%, standard deviation: 33%), whereas a negligible fraction appears in the randomized ground truth references (mean: 0.04%, median: 0%, standard deviation: 0.6%).Moreover, among the non-isolated references, most connect only to each other rather than to the randomized references, leading to a fraction of 0% in the example graph.The edge expansion measure is also 0% in that example.This strong deviation from the observation in (a) suggests that LLMs reflect the local citation context of the focal papers rather well. 1 is hit document with top 1% total citations within the same level field and the same year, and 0 is not.Hit_5pct1 is hit document with top 5% total citations within the same level field and the same year, and 0 is not.Hit_10pct1 is hit document with top 10% total citations within the same level field and the same year, and 0 is not.C_f Normalized citation as defined by[76].TableA2: The selection and description of the variables from the SciSciNet[58]database.
Large language models show human-like content biases in transmission chain experiments. Alberto Acerbi, Joseph M Stubbersfield, Proceedings of the National Academy of Sciences. 12044e23137901202023</p>
<p>Do language models know when they're hallucinating references?. Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Kalai, Findings of the Association for Computational Linguistics: EACL 2024. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>Litsearch: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024arXiv preprint</p>
<p>Large language models reflect human citation patterns with a heightened citation bias. Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis, arXiv:2405.157392024arXiv preprint</p>
<p>Beyond citations: Measuring novel scientific ideas and their impact in publication text. Sam Arts, Nicola Melluso, Reinhilde Veugelers, 20232309arXiv e-prints</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Generative ai as a tool for truth. Bence Bago, Jean-François Bonnefon, Science. 38567142024</p>
<p>The rapid adoption of generative ai. Alexander Bick, Adam Blandin, David J Deming, 2024National Bureau of Economic Research</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>What do citation counts measure? a review of studies on citing behavior. Lutz Bornmann, Hans-Dieter Daniel, Journal of documentation. 6412008</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Ai-assisted peer review. Alessandro Checco, Lorenzo Bracciale, Pierpaolo Loreti, Stephen Pinfield, Giuseppe Bianchi, Humanities and social sciences communications. 812021</p>
<p>Benchmarking large language models in retrieval-augmented generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Unveiling the power of language models in chemical research question answering. Xiuying Chen, Tairan Wang, Taicheng Guo, Kehan Guo, Juexiao Zhou, Haoyang Li, Zirui Song, Xin Gao, Xiangliang Zhang, Communications Chemistry. 8142025</p>
<p>The data matching process. Peter Christen, Peter Christen, 2012Springer</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Transforming literature screening: The emerging role of large language models in systematic reviews. M Fernando, Matthew J Delgado-Chaves, Antonio Jennings, Justus Atalaia, Rita Wolff, Zeinab M Horvath, Jan Mamdouh, Linda Baumbach, Baumbach, Proceedings of the National Academy of Sciences. 1222e24119621222025</p>
<p>Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain. Fabio Dennstädt, Johannes Zink, Paul Martin Putora, Janna Hastings, Nikola Cihoric, Systematic Reviews. 1311582024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Free and Open Source, Distributed. Elastic, Elasticsearch, RESTful Search Engine</p>
<p>Electronic publication and the narrowing of science and scholarship. James A Evans, science. 32158872008</p>
<p>How to optimize the systematic review process using ai tools. Nicholas Fabiano, Arnav Gupta, Nishaant Bhambra, Brandon Luu, Stanley Wong, Muhammad Maaz, Jess G Fiedorowicz, Andrew L Smith, Marco Solmi, JCPP Advances. e122342024</p>
<p>Science of science. Santo Fortunato, Carl T Bergstrom, Katy Börner, James A Evans, Dirk Helbing, Staša Milojević, Filippo Alexander M Petersen, Roberta Radicchi, Brian Sinatra, Uzzi, Science. 35963791852018</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, arXiv:2305.146272023arXiv preprint</p>
<p>Investigating different types of research collaboration and citation impact: a case study of harvard university's publications. Ali Gazni, Fereshteh Didegah, Scientometrics. 8722011</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Testing the reliability of an ai-based large language model to extract ecological information from the scientific literature. V Andrew, Hannah L Gougherty, Clipp, Biodiversity. 31132024</p>
<p>How citation distortions create unfounded authority: analysis of a citation network. A Steven, Greenberg, Bmj. 3392009</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, arXiv:2310.022072024arXiv preprint</p>
<p>Sukjin Han, arXiv:2409.14202Mining causality: Ai-assisted search for instrumental variables. 2024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Predicting results of social science experiments using large language models. Luke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, Robb Willer, 2024</p>
<p>Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. Noah Hollmann, Samuel Müller, Frank Hutter, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Dataset artefacts are the hidden drivers of the declining disruptiveness in science. Vincent Holst, Andres Algaba, Floriano Tori, Sylvia Wenmackers, Vincent Ginis, arXiv:2402.145832024arXiv preprint</p>
<p>Automated citation recommendation tools encourage questionable citations. Serge Pjm Horbach, Freek Jw Oude Maatman, Willem Halffman, Wytske M Hepkema, Research Evaluation. 3132022</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, International Conference on Machine Learning. PMLR2023</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291Researcharena: Benchmarking llms' ability to collect and organize information as research agents. 2024arXiv preprint</p>
<p>The semantic scholar open data platform. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, arXiv:2301.101402023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Matryoshka representation learning. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Advances in Neural Information Processing Systems. 202235</p>
<p>The impact factor's matthew effect: A natural experiment in bibliometrics. Vincent Larivière, Yves Gingras, Journal of the American Society for Information Science and Technology. 6122010</p>
<p>The politics of publication. Lawrence Peter, Nature. 42269292003</p>
<p>Chatgpt as research scientist: Probing gpt's capabilities as a research librarian, research ethicist, data generator, and data predictor. Steven A Lehr, Aylin Caliskan, Suneragiri Liyanage, Mahzarin R Banaji, Proceedings of the National Academy of Sciences. 12135e24043281212024</p>
<p>The advantage of short paper titles. Adrian Letchford, Helen Susannah Moat, Tobias Preis, Royal Society open science. 281502662015</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, arXiv:2408.15545Guolin Ke, and Hengxing Cai. Scilitllm: How to adapt llms for scientific literature understanding. 2024arXiv preprint</p>
<p>Sciscinet: A large-scale open data lake for the science of science research. Zihang Lin, Yian Yin, Lu Liu, Dashun Wang, Scientific Data. 1013152023</p>
<p>Data, measurement and empirical methods in the science of science. Lu Liu, Benjamin F Jones, Brian Uzzi, Dashun Wang, Nature Human Behaviour. 772023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 2024</p>
<p>Fairness-guided few-shot prompting for large language models. Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Knowledge: Its Creation, Distribution, and Economic Significance, Volume I: Knowledge and Knowledge Production. Fritz Machlup, 1980Princeton University PressPrinceton, NJ</p>
<p>Impact of the reference list features on the number of citations. Stefano Mammola, Diego Fontaneto, Alejandro Martínez, Filipe Chichorro, Scientometrics. 1262021</p>
<p>Automated social science: Language models as scientist and subjects. Benjamin S Manning, Kehang Zhu, John J Horton, arXiv:2404.117942024arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. 62479902023</p>
<p>Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers. Tanisha Mishra, Edward Sutanto, Rini Rossanti, Nayana Pant, Anum Ashraf, Akshay Raut, Germaine Uwabareze, Ajayi Oluwatomiwa, Bushra Zeeshan, Scientific Reports. 141316722024</p>
<p>How deeply do llms internalize human citation practices? a graph-structural and embedding-based evaluation. Melika Mobini, Vincent Holst, Floriano Tori, Andres Algaba, Vincent Ginis, ICLR 2025 Workshop on Human-AI Coevolution. 2025</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Biases in large language models: origins, inventory, and discussion. Roberto Navigli, Simone Conia, Björn Ross, ACM Journal of Data and Information Quality. 1522023</p>
<p>Global citation inequality is on the rise. Mathias Wullum, Nielsen , Jens Peter Andersen, Proceedings of the National Academy of Sciences. 1187e20122081182021</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, arXiv:2501.142492025arXiv preprint</p>
<p>CiteME: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, ; , Matthias Bethge, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Ofir Press2024</p>
<p>Networks of scientific papers: The pattern of bibliographic references indicates the nature of the scientific research front. Derek J De Solla Price, Science. 14936831965</p>
<p>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?. Riaz Qureshi, Daniel Shaughnessy, A R Kayden, Karen A Gill, Tianjing Robinson, Eitan Li, Agai, Systematic Reviews. 121722023</p>
<p>Universality of citation distributions: Toward an objective measure of scientific impact. Filippo Radicchi, Santo Fortunato, Claudio Castellano, Proceedings of the National Academy of Sciences. 105452008</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 62579952024</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Is something rotten in the state of denmark? cross-national evidence for widespread involvement but not systematic use of questionable research practices across all fields of research. W Jesper, Nick Schneider, Jens Allum, Michael Peter Andersen, Emil B Bang Petersen, Niels Madsen, Robert Mejlgaard, Zachariae, PloS one. 198e03043422024</p>
<p>Introduction to information retrieval. Hinrich Schütze, Christopher D Manning, Prabhakar Raghavan, 2008Cambridge University Press39Cambridge</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>V Mikhail, Simkin, Vwani, Roychowdhury, Read before you cite! arXiv preprint cond-mat/0212043. 2002</p>
<p>SciRepEval: A multiformat benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. Sam Michael D Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Hinks, J Michael, Manvitha Hammerling, Ponnapati, Andrew D Samuel G Rodriques, White, arXiv:2409.137402024arXiv preprint</p>
<p>Does automation bias decision-making?. Linda J Skitka, Kathleen L Mosier, Mark Burdick, International Journal of Human-Computer Studies. 5151999</p>
<p>Impact factors, scientometrics and the history of citation-based research. Derek R Smith, Scientometrics. 9222012</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. Teo Susnjak, Peter Hwang, Napoleon H Reyes, Andre Lc Barczak, Timothy R Mcintosh, Surangika Ranathunga, arXiv:2404.086802024arXiv preprint</p>
<p>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, bioRxiv. 2024</p>
<p>How much is too much? the difference between research influence and self-citation excess. Martin Szomszor, David A Pendlebury, Jonathan Adams, Scientometrics. 12322020</p>
<p>Core elements in the process of citing publications: Conceptual overview of the literature. Iman Tahamtan, Lutz Bornmann, Journal of informetrics. 1212018</p>
<p>Factors affecting number of citations: a comprehensive review of the literature. Iman Tahamtan, Askar Safipour Afshar, Khadijeh Ahamdzadeh, Scientometrics. 1072016</p>
<p>Let me speak freely? a study on the impact of format restrictions on performance of large language models. Rui Zhi, Cheng-Kuang Tam, Yi-Lin Wu, Chieh-Yen Tsai, Hung-Yi Lin, Yun-Nung Lee, Chen, arXiv:2408.024422024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Why are coauthored academic articles more cited: Higher quality or larger audience. Mike Thelwall, Kayvan Kousha, Mahshid Abdoli, Emma Stuart, Meiko Makita, Paul Wilson, Jonathan Levitt, Journal of the Association for Information Science and Technology. 7472023</p>
<p>Do llms exhibit human-like response biases? a case study in survey design. Lindia Tjuatja, Valerie Chen, Tongshuang Wu, Ameet Talwalkwar, Graham Neubig, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>Large language models should be used as scientific reasoning engines, not knowledge databases. Jorge S Daniel Truhn, Jakob Nikolas Reis-Filho, Kather, Nature Medicine. 29122023</p>
<p>Document co-citation analysis to enhance transdisciplinary research. Caleb M Trujillo, Tammy M Long, Science Advances. 41e17011302018</p>
<p>Fabrication and errors in the bibliographic citations generated by chatgpt. H William, Esther Isabelle Walters, Wilder, Scientific Reports. 131140452023</p>
<p>Quantifying long-term scientific impact. Dashun Wang, Chaoming Song, Albert-László Barabási, Science. 34261542013</p>
<p>Citation time window choice for research impact evaluation. Jian Wang, Scientometrics. 9432013</p>
<p>Unpacking the Matthew effect in citations. Jian Wang, Journal of Informetrics. 822014</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Large teams develop and small teams disrupt science and technology. Lingfei Wu, Dashun Wang, James A Evans, Nature. 56677442019</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Predicting citation impact of academic papers across research areas using multiple models and early citations. Fang Zhang, Shengli Wu, Scientometrics. 12972024</p>
<p>Large language models for scientific discovery in molecular property prediction. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh Tn Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, Nature Machine Intelligence. 2025</p>
<p>Can large language models transform computational social science? Computational Linguistics. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, 202450</p>            </div>
        </div>

    </div>
</body>
</html>
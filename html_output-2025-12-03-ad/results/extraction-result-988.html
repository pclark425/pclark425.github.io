<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-988 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-988</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-988</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-1a627d2a169d71563109546da590a7cceb0b349a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1a627d2a169d71563109546da590a7cceb0b349a" target="_blank">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A framework of alternating training with learned adversaries (ATLA) is proposed, which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework, and it is demonstrated that an optimal adversary to perturb state observations can be found.</p>
                <p><strong>Paper Abstract:</strong> We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e988.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e988.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SA-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State-Adversarial Markov Decision Process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal MDP-like framework that models adversarial perturbations applied to agent observations (not to true environment state), defining an adversary ν with constrained support B(s) and studying worst-case agent value under such adversaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust deep reinforcement learning against adversarial perturbations on observations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>State-Adversarial MDP (SA-MDP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SA-MDP augments a standard MDP with an adversary mapping ν: S -> P(S) that stochastically perturbs the observation seen by the agent; the adversary's support is constrained by B(s) (e.g., an ℓ_p ball). Optimal adversary / policy problems are cast as a minimax: for fixed agent policy π, the adversary's problem can be recast as an MDP ˆM (Lemma 1) with actions equal to candidate perturbed states; for fixed adversary ν, the agent's problem becomes a POMDP (Lemma 2). The framework supports parameterizing adversary and agent policies with neural networks and solving with standard RL (e.g., PPO), and it provides a principled way to define and find worst-case perturbations rather than heuristic attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GridWorld toy and MuJoCo continuous-control (Hopper, Walker2d, Ant, HalfCheetah)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated interactive environments: a small GridWorld (finite-state) used for conceptual exposition and OpenAI Gym MuJoCo continuous-control benchmarks used for empirical evaluation; environments allow active interaction and online learning of adversary and agent policies.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit adversary modeling and worst-case (minimax) optimization: treat observation corruptors as an adversary with constrained support B(s) and optimize for policies robust to the worst-case perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measurement noise / perturbed observations (bounded adversarial perturbations, e.g., ℓ_∞/ℓ_p ball), observation-level distractors that do not change true state.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit via minimax training: the agent is trained to optimize performance under the worst-case adversary; robustness to spurious observation signals arises from optimizing against adversarial perturbations rather than explicit sample reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Adversary is learned via interactive RL: the adversary explores the action (perturbation) space by interacting with agent and environment to find perturbations that minimize agent reward; agent in turn is trained against these adversaries (alternating training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used as the modeling framework for defenses (e.g., ATLA agents trained under SA-MDP), empirical worst-case episode rewards under a suite of strong attacks improve substantially compared to vanilla agents (see ATLA entries). Specific numeric rewards are reported in the paper's tables per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla agents not trained under SA-MDP are highly vulnerable: e.g., PPO (vanilla) Hopper natural reward ~3167 but best-attack reward drops to 636 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SA-MDP provides a principled way to represent observation-level distractors as an adversary with constrained support. Solving the adversary problem (for fixed policy) is an MDP and solving the policy problem (for fixed adversary) is a POMDP; this observation motivates learned adversaries and history-dependent policies (LSTM). Empirically, methods that explicitly optimize under SA-MDP adversaries produce more robust policies than heuristic adversarial examples or gradient-based attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e988.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimal Adversary (learned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned Optimal Adversary via MDP reduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to construct and learn an adversary that (approximately) attains the worst-case reduction in agent return by recasting the adversary's problem as an MDP (Lemma 1) and training a neural-network policy with standard RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Optimal adversary via MDP reduction (learning-based attack)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a fixed agent policy π, form an MDP ˆM over the true states where adversary 'actions' are candidate perturbed observations (hat{s} in B(s)). Reward for the adversary is -r_t (negative of agent reward) and transitions ˆp(s'|s,hat{a}) = sum_a π(a|hat{a}) p(s'|s,a). Parameterize adversary ν_φ(hat{s}|s) as a neural policy producing perturbation vector Δ, project s+Δ to B(s), and train with model-free RL (PPO) to minimize agent returns. This yields a strong black-box attack (no gradient access to victim required) that empirically outperforms previous heuristic/gradient attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo continuous control benchmarks (Hopper, Walker2d, Ant, HalfCheetah) and GridWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated RL benchmarks where adversary can perturb each observation within a bounded region B(s); adversary training proceeds by episodic interaction with agent and environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Not a defense — the method actively searches for observation-level distractors (perturbations) that are most damaging; technique is constructive adversary search via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Adversarial measurement perturbations within bounded norm balls (e.g., ℓ_∞ or ℓ_p), which act as spurious features in observations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: the adversary discovers spurious perturbations by optimizing expected negative agent reward through interactions; detection is equivalent to finding perturbations that maximally change agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>The adversary actively explores perturbation policies via policy-gradient learning (PPO) and grid-searched hyperparameters for attack strength; hundreds of adversaries are trained to find the strongest attack.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>This attack achieves substantially lower agent rewards than prior attacks across environments (Table 1); e.g., for HalfCheetah, optimal attack produced mean attack rewards as low as -668 vs other attacks that left rewards higher.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a perturbation policy by recasting the adversary as an MDP yields a significantly stronger attack than previous heuristic or critic-based attacks; it is black-box (doesn't require policy gradients) and can be tuned across many hyperparameter settings to find worst-case perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e988.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternating Training with Learned Adversaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A defense strategy that alternates between (a) training the agent policy with a fixed adversary and (b) training a learned adversary against the fixed agent, producing policies robust to learned worst-case observation perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ATLA (Alternating Training with Learned Adversaries)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ATLA alternates optimization: for N_pi iterations the agent π_θ is trained under a fixed adversary ν_φ using policy-gradient (e.g., PPO), then for N_ν iterations the adversary ν_φ is trained against the fixed agent (Algorithm 2). Both agent and adversary are parameterized as neural policy/value networks; adversary outputs perturbation vectors Δ projected to B(s). Variants include using LSTM (history-dependent) agent policies and combining ATLA with state-adversarial regularization (SA Reg) on function approximator smoothness.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo continuous-control benchmarks (Hopper, Walker2d, Ant, HalfCheetah); GridWorld example for conceptual demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated continuous-control tasks where both agent and adversary learn online via interactions; adversary training is an active experimentation process driven by RL.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Adversary-in-the-loop minimax training: by actively training an adversary that finds spurious/distracting perturbations, ATLA forces the agent to learn policies that are insensitive to those observation-level distractors; additionally, use of LSTM enables leveraging history to disambiguate distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Bounded adversarial observation perturbations (measurement noise, distracting features presented in observations), including black-box snooping-style perturbations and learned optimal perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Adversarial discovery: spurious signals are discovered by the learned adversary that optimizes to degrade agent returns; no explicit statistical detector is used.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Robustification via training under adversarial perturbations (minimax) and optionally via SA regularization on policy smoothness; history-based policies (LSTM) implicitly downweight transient/spurious observations by integrating over time.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Indirect: robust performance under a variety of adversaries (including learned optimal ones) serves as empirical refutation of spurious causal claims that rely on perturbed observations. No formal refutation test is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Alternating online training loop where the adversary actively seeks perturbations (interventions) that reduce agent performance and the agent reacts by updating policy; hyperparameter search over adversary learning rates and entropy coefficients is used to find strong adversaries during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ATLA agents, especially ATLA-PPO (LSTM) and ATLA-PPO (LSTM)+SA Reg, show substantially improved worst-case episode rewards across environments versus vanilla PPO and many baselines (see Table 2). Example: ATLA-PPO (LSTM) Hopper best-attack reward ≈1224±191 vs PPO (vanilla) best-attack 636±9 (Table 2); on Walker2d ATLA(LSTM) best-attack ≈5219 vs PPO 1086.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla PPO and PPO(LSTM) agents without adversary-in-the-loop training exhibit much lower worst-case returns (see Table 2; e.g., PPO Hopper best-attack 636).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ATLA using a learned adversary is an effective practical defense: it yields stronger robustness than prior adversarial-training heuristics and is complementary to function-approximation regularization (SA reg). History-dependent (LSTM) policies trained with ATLA are notably more robust, confirming the theoretical POMDP connection when adversary is fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e988.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SA-PPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State-Adversarial PPO (policy smoothness regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline defense that regularizes the smoothness of the learned policy to minimize worst-case divergence between policy outputs on nearby perturbed states, solved via convex relaxations or gradient-based maximization approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust deep reinforcement learning against adversarial perturbations on observations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>State-adversarial policy regularization (SA-PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SA-PPO adds a minimax regularizer to the policy learning objective that penalizes the maximum divergence D(π_θ(s), π_θ(̂s)) over perturbed states ̂s in B(s). The inner maximization over ̂s can be approximated by gradient-based attacks or solved via convex relaxations for neural networks; the outer minimization updates policy parameters to reduce sensitivity (analogous to TRADES/VAT in supervised learning). The method aims to make the function approximator itself robust to small ℓ_p perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo continuous-control benchmarks (Hopper, Walker2d, Ant, HalfCheetah); especially evaluated in referenced prior work and compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated continuous-control environments where the perturbation set B(s) is an ℓ_∞ ball around s; optimization uses offline regularization during policy training (not an actively learned adversary).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Smoothness regularization of policy outputs across local perturbation neighborhoods (minimax: maximize divergence over B(s) then minimize over θ); convex relaxation or gradient-based inner maximization used to approximate the worst-case.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Local observation perturbations (bounded measurement noise) and features that cause policy output changes due to brittle function approximators.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit via regularization: by reducing sensitivity to perturbations, the policy places less weight on features that vary within B(s) (i.e., downweights spurious, high-variance features).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>SA-PPO improves robustness in some environments (e.g., HalfCheetah) but can be insufficient in others; reported best-attack rewards for SA-PPO in Table 1/2 show mixed results: e.g., SA-PPO HalfCheetah best-attack ≈3028 vs PPO best-attack negative values for vanilla PPO under optimal attack.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to vanilla PPO, SA-PPO often increases worst-case returns at the cost of some natural (no-attack) performance depending on regularization strength κ (Figure 4 shows increasing κ can hurt natural reward).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Regularizing the function approximator (policy smoothness) reduces vulnerability in some environments, but is not sufficient alone to handle intrinsic policy weaknesses; heavy regularization can degrade natural performance, and ATLA (adversary-in-the-loop) can outperform SA-PPO in multiple environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e988.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM policy (history-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>History-dependent policy via LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of recurrent (LSTM) policy/value networks to encode observation-action history to mitigate partial observability introduced by adversarial observation perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LSTM-based recurrent policy for POMDP/SA-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Parameterize agent policy and value function with an LSTM that consumes sequences of perturbed observations and actions h_t = {hat{s}_0, a_0, ..., hat{s}_t} to produce actions; training uses recurrent policy gradient / PPO with backpropagation through time. This leverages temporal coherence: adversarial/transient perturbations can be filtered by integrating information over history, enabling disambiguation of true state.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo continuous-control benchmarks and GridWorld example</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL benchmarks where observation perturbations make the problem POMDP-like; LSTM policies operate on sequential data and require unfolding across time during training; environments allow active online training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Temporal integration: LSTM aggregates past observations and actions, reducing reliance on any single (possibly corrupted) observation and implicitly downweighting transient distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Transient adversarial observation noise and spurious features that are inconsistent over time.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit via memory/aggregation: inconsistent (spurious) signals have reduced influence because the LSTM conditions on longer histories.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>LSTM policies trained with ATLA show significantly improved worst-case returns compared to feedforward policies; e.g., ATLA-PPO (LSTM) often surpasses ATLA-PPO (MLP) and SA-PPO in reported best-attack rewards (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla PPO(LSTM) without adversarial training does not reliably improve robustness compared to PPO(MLP); LSTM alone is insufficient without adversary-in-the-loop training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>History-dependent policies (LSTM) are empirically more robust when trained under adversary-in-the-loop (ATLA); this aligns with the theoretical reduction that fixed-adversary SA-MDP becomes a POMDP where history matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e988.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SARSOP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Successive Approximations of the Reachable Space (SARSOP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A point-based POMDP solver that computes approximate history-dependent policies (finite-state controllers) for POMDPs and was used here as a proof-of-concept to obtain a robust policy in the GridWorld SA-MDP example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SARSOP (POMDP solver)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SARSOP is a point-based algorithm that approximates optimally reachable belief spaces to compute an approximate POMDP solution (finite-state controller); when applied to the GridWorld with a fixed adversary, SARSOP finds a history-dependent policy that defeats the adversary by maintaining belief over true state despite perturbed observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GridWorld toy example (finite-state)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Small discrete MDP with bounded perturbation support B(s) where adversary perturbs observations to neighboring cells; environment is fully specified and amenable to offline POMDP planning.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Belief/planning over histories: SARSOP computes policies over belief space to handle observation uncertainty introduced by adversary, effectively reasoning about distractors by maintaining posterior beliefs and choosing actions robust to observation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Observation-level perturbations to discrete states (neighboring-state observations) and other partial observability sources.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Belief-updating and planning: inconsistent observations are downweighted via belief updates across time, and the policy chooses actions robust under belief uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>SARSOP produced an 8-state finite-state controller that almost eliminates impact of adversary in the GridWorld example (Figure 1c), recovering near-perfect reward under the fixed adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>A stationary optimal MDP policy (no history) fails catastrophically under the adversary (Figure 1b) with the agent repeatedly encountering traps.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exact/approximate POMDP solvers like SARSOP can find history-dependent policies that are robust to observation-level adversaries in small discrete domains, illustrating the theoretical mapping from SA-MDP-with-fixed-adversary to POMDP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e988.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e988.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy smoothness / certified defenses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy smoothness regularization and certified adversarial defenses (convex relaxations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of techniques adapted from supervised learning (TRADES, VAT, convex-relaxation-based certification) that enforce or certify small changes in policy/value outputs under bounded input perturbations, used as defenses against observation perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Policy smoothness regularization and convex-relaxation certification</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>These approaches either (1) add a regularizer that minimizes worst-case divergence between policy outputs in the neighborhood B(s) (adversarial minimax regularization analogous to TRADES/VAT), or (2) use convex-relaxation techniques (interval bound propagation, abstract interpretation) to compute certified bounds on network outputs under ℓ_p perturbations and train to improve provable robustness. In RL context they are applied to Q-networks or policy networks to reduce sensitivity to small perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>High-dimensional observation domains (e.g., Atari images) and continuous control benchmarks as referenced</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated training/evaluation settings where the function approximator (NN) processes high-dimensional observations; neither approach requires active adversary learning (they are training-time regularization/certification methods).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Function-approximation robustness: controlling Lipschitz-like sensitivity of networks via explicit regularizers or certifiable upper bounds, thereby reducing the influence of spurious/distractor features that lie within B(s).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Small bounded adversarial perturbations and input noise that exploit brittle neural network decision boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Regularization of output divergence across neighborhoods (minimize max D(pi(s),pi(s+delta))) and training objectives that penalize vulnerability as measured by certification bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Certified bounds provide formal guarantees that policy outputs cannot change beyond computed bounds under specified perturbation budgets (refuting some spurious-robustness claims), but full causal refutation is not offered.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Prior works cited (Mirman et al., Wong & Kolter, Zhang et al.) show improvements in adversarial robustness of networks; in RL, SA-PPO (regularization variant) can improve robustness in some MuJoCo tasks but may be insufficient alone (paper reports mixed empirical results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla networks without such regularization are vulnerable to small perturbations (many adversarial attacks cause catastrophic drop in agent returns).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Function-approximator level defenses reduce vulnerability stemming from brittle neural networks, but do not address intrinsic policy weaknesses that occur even in tabular MDPs (GridWorld example); hence they are complementary to adversary-in-the-loop approaches like ATLA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning on State Observations with Learned Optimal Adversary', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robust deep reinforcement learning against adversarial perturbations on observations <em>(Rating: 2)</em></li>
                <li>Snooping attacks on deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 1)</em></li>
                <li>Adversarial attacks on neural network policies <em>(Rating: 1)</em></li>
                <li>Robust deep reinforcement learning against adversarial perturbations on observations <em>(Rating: 2)</em></li>
                <li>Towards deep learning models resistant to adversarial attacks <em>(Rating: 2)</em></li>
                <li>Distributional smoothing with virtual adversarial training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-988",
    "paper_id": "paper-1a627d2a169d71563109546da590a7cceb0b349a",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "SA-MDP",
            "name_full": "State-Adversarial Markov Decision Process",
            "brief_description": "A formal MDP-like framework that models adversarial perturbations applied to agent observations (not to true environment state), defining an adversary ν with constrained support B(s) and studying worst-case agent value under such adversaries.",
            "citation_title": "Robust deep reinforcement learning against adversarial perturbations on observations",
            "mention_or_use": "use",
            "method_name": "State-Adversarial MDP (SA-MDP)",
            "method_description": "SA-MDP augments a standard MDP with an adversary mapping ν: S -&gt; P(S) that stochastically perturbs the observation seen by the agent; the adversary's support is constrained by B(s) (e.g., an ℓ_p ball). Optimal adversary / policy problems are cast as a minimax: for fixed agent policy π, the adversary's problem can be recast as an MDP ˆM (Lemma 1) with actions equal to candidate perturbed states; for fixed adversary ν, the agent's problem becomes a POMDP (Lemma 2). The framework supports parameterizing adversary and agent policies with neural networks and solving with standard RL (e.g., PPO), and it provides a principled way to define and find worst-case perturbations rather than heuristic attacks.",
            "environment_name": "GridWorld toy and MuJoCo continuous-control (Hopper, Walker2d, Ant, HalfCheetah)",
            "environment_description": "Simulated interactive environments: a small GridWorld (finite-state) used for conceptual exposition and OpenAI Gym MuJoCo continuous-control benchmarks used for empirical evaluation; environments allow active interaction and online learning of adversary and agent policies.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit adversary modeling and worst-case (minimax) optimization: treat observation corruptors as an adversary with constrained support B(s) and optimize for policies robust to the worst-case perturbations.",
            "spurious_signal_types": "Measurement noise / perturbed observations (bounded adversarial perturbations, e.g., ℓ_∞/ℓ_p ball), observation-level distractors that do not change true state.",
            "detection_method": null,
            "downweighting_method": "Implicit via minimax training: the agent is trained to optimize performance under the worst-case adversary; robustness to spurious observation signals arises from optimizing against adversarial perturbations rather than explicit sample reweighting.",
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Adversary is learned via interactive RL: the adversary explores the action (perturbation) space by interacting with agent and environment to find perturbations that minimize agent reward; agent in turn is trained against these adversaries (alternating training).",
            "performance_with_robustness": "When used as the modeling framework for defenses (e.g., ATLA agents trained under SA-MDP), empirical worst-case episode rewards under a suite of strong attacks improve substantially compared to vanilla agents (see ATLA entries). Specific numeric rewards are reported in the paper's tables per environment.",
            "performance_without_robustness": "Vanilla agents not trained under SA-MDP are highly vulnerable: e.g., PPO (vanilla) Hopper natural reward ~3167 but best-attack reward drops to 636 (Table 2).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "SA-MDP provides a principled way to represent observation-level distractors as an adversary with constrained support. Solving the adversary problem (for fixed policy) is an MDP and solving the policy problem (for fixed adversary) is a POMDP; this observation motivates learned adversaries and history-dependent policies (LSTM). Empirically, methods that explicitly optimize under SA-MDP adversaries produce more robust policies than heuristic adversarial examples or gradient-based attacks.",
            "uuid": "e988.0",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Optimal Adversary (learned)",
            "name_full": "Learned Optimal Adversary via MDP reduction",
            "brief_description": "A method to construct and learn an adversary that (approximately) attains the worst-case reduction in agent return by recasting the adversary's problem as an MDP (Lemma 1) and training a neural-network policy with standard RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Optimal adversary via MDP reduction (learning-based attack)",
            "method_description": "Given a fixed agent policy π, form an MDP ˆM over the true states where adversary 'actions' are candidate perturbed observations (hat{s} in B(s)). Reward for the adversary is -r_t (negative of agent reward) and transitions ˆp(s'|s,hat{a}) = sum_a π(a|hat{a}) p(s'|s,a). Parameterize adversary ν_φ(hat{s}|s) as a neural policy producing perturbation vector Δ, project s+Δ to B(s), and train with model-free RL (PPO) to minimize agent returns. This yields a strong black-box attack (no gradient access to victim required) that empirically outperforms previous heuristic/gradient attacks.",
            "environment_name": "MuJoCo continuous control benchmarks (Hopper, Walker2d, Ant, HalfCheetah) and GridWorld",
            "environment_description": "Interactive simulated RL benchmarks where adversary can perturb each observation within a bounded region B(s); adversary training proceeds by episodic interaction with agent and environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Not a defense — the method actively searches for observation-level distractors (perturbations) that are most damaging; technique is constructive adversary search via RL.",
            "spurious_signal_types": "Adversarial measurement perturbations within bounded norm balls (e.g., ℓ_∞ or ℓ_p), which act as spurious features in observations.",
            "detection_method": "Implicit: the adversary discovers spurious perturbations by optimizing expected negative agent reward through interactions; detection is equivalent to finding perturbations that maximally change agent behavior.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "The adversary actively explores perturbation policies via policy-gradient learning (PPO) and grid-searched hyperparameters for attack strength; hundreds of adversaries are trained to find the strongest attack.",
            "performance_with_robustness": "This attack achieves substantially lower agent rewards than prior attacks across environments (Table 1); e.g., for HalfCheetah, optimal attack produced mean attack rewards as low as -668 vs other attacks that left rewards higher.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Learning a perturbation policy by recasting the adversary as an MDP yields a significantly stronger attack than previous heuristic or critic-based attacks; it is black-box (doesn't require policy gradients) and can be tuned across many hyperparameter settings to find worst-case perturbations.",
            "uuid": "e988.1",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "ATLA",
            "name_full": "Alternating Training with Learned Adversaries",
            "brief_description": "A defense strategy that alternates between (a) training the agent policy with a fixed adversary and (b) training a learned adversary against the fixed agent, producing policies robust to learned worst-case observation perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ATLA (Alternating Training with Learned Adversaries)",
            "method_description": "ATLA alternates optimization: for N_pi iterations the agent π_θ is trained under a fixed adversary ν_φ using policy-gradient (e.g., PPO), then for N_ν iterations the adversary ν_φ is trained against the fixed agent (Algorithm 2). Both agent and adversary are parameterized as neural policy/value networks; adversary outputs perturbation vectors Δ projected to B(s). Variants include using LSTM (history-dependent) agent policies and combining ATLA with state-adversarial regularization (SA Reg) on function approximator smoothness.",
            "environment_name": "MuJoCo continuous-control benchmarks (Hopper, Walker2d, Ant, HalfCheetah); GridWorld example for conceptual demonstration",
            "environment_description": "Interactive simulated continuous-control tasks where both agent and adversary learn online via interactions; adversary training is an active experimentation process driven by RL.",
            "handles_distractors": true,
            "distractor_handling_technique": "Adversary-in-the-loop minimax training: by actively training an adversary that finds spurious/distracting perturbations, ATLA forces the agent to learn policies that are insensitive to those observation-level distractors; additionally, use of LSTM enables leveraging history to disambiguate distractors.",
            "spurious_signal_types": "Bounded adversarial observation perturbations (measurement noise, distracting features presented in observations), including black-box snooping-style perturbations and learned optimal perturbations.",
            "detection_method": "Adversarial discovery: spurious signals are discovered by the learned adversary that optimizes to degrade agent returns; no explicit statistical detector is used.",
            "downweighting_method": "Robustification via training under adversarial perturbations (minimax) and optionally via SA regularization on policy smoothness; history-based policies (LSTM) implicitly downweight transient/spurious observations by integrating over time.",
            "refutation_method": "Indirect: robust performance under a variety of adversaries (including learned optimal ones) serves as empirical refutation of spurious causal claims that rely on perturbed observations. No formal refutation test is provided.",
            "uses_active_learning": true,
            "inquiry_strategy": "Alternating online training loop where the adversary actively seeks perturbations (interventions) that reduce agent performance and the agent reacts by updating policy; hyperparameter search over adversary learning rates and entropy coefficients is used to find strong adversaries during evaluation.",
            "performance_with_robustness": "ATLA agents, especially ATLA-PPO (LSTM) and ATLA-PPO (LSTM)+SA Reg, show substantially improved worst-case episode rewards across environments versus vanilla PPO and many baselines (see Table 2). Example: ATLA-PPO (LSTM) Hopper best-attack reward ≈1224±191 vs PPO (vanilla) best-attack 636±9 (Table 2); on Walker2d ATLA(LSTM) best-attack ≈5219 vs PPO 1086.",
            "performance_without_robustness": "Vanilla PPO and PPO(LSTM) agents without adversary-in-the-loop training exhibit much lower worst-case returns (see Table 2; e.g., PPO Hopper best-attack 636).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "ATLA using a learned adversary is an effective practical defense: it yields stronger robustness than prior adversarial-training heuristics and is complementary to function-approximation regularization (SA reg). History-dependent (LSTM) policies trained with ATLA are notably more robust, confirming the theoretical POMDP connection when adversary is fixed.",
            "uuid": "e988.2",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "SA-PPO",
            "name_full": "State-Adversarial PPO (policy smoothness regularization)",
            "brief_description": "A baseline defense that regularizes the smoothness of the learned policy to minimize worst-case divergence between policy outputs on nearby perturbed states, solved via convex relaxations or gradient-based maximization approximations.",
            "citation_title": "Robust deep reinforcement learning against adversarial perturbations on observations",
            "mention_or_use": "mention",
            "method_name": "State-adversarial policy regularization (SA-PPO)",
            "method_description": "SA-PPO adds a minimax regularizer to the policy learning objective that penalizes the maximum divergence D(π_θ(s), π_θ(̂s)) over perturbed states ̂s in B(s). The inner maximization over ̂s can be approximated by gradient-based attacks or solved via convex relaxations for neural networks; the outer minimization updates policy parameters to reduce sensitivity (analogous to TRADES/VAT in supervised learning). The method aims to make the function approximator itself robust to small ℓ_p perturbations.",
            "environment_name": "MuJoCo continuous-control benchmarks (Hopper, Walker2d, Ant, HalfCheetah); especially evaluated in referenced prior work and compared in this paper.",
            "environment_description": "Simulated continuous-control environments where the perturbation set B(s) is an ℓ_∞ ball around s; optimization uses offline regularization during policy training (not an actively learned adversary).",
            "handles_distractors": true,
            "distractor_handling_technique": "Smoothness regularization of policy outputs across local perturbation neighborhoods (minimax: maximize divergence over B(s) then minimize over θ); convex relaxation or gradient-based inner maximization used to approximate the worst-case.",
            "spurious_signal_types": "Local observation perturbations (bounded measurement noise) and features that cause policy output changes due to brittle function approximators.",
            "detection_method": null,
            "downweighting_method": "Implicit via regularization: by reducing sensitivity to perturbations, the policy places less weight on features that vary within B(s) (i.e., downweights spurious, high-variance features).",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "SA-PPO improves robustness in some environments (e.g., HalfCheetah) but can be insufficient in others; reported best-attack rewards for SA-PPO in Table 1/2 show mixed results: e.g., SA-PPO HalfCheetah best-attack ≈3028 vs PPO best-attack negative values for vanilla PPO under optimal attack.",
            "performance_without_robustness": "Compared to vanilla PPO, SA-PPO often increases worst-case returns at the cost of some natural (no-attack) performance depending on regularization strength κ (Figure 4 shows increasing κ can hurt natural reward).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Regularizing the function approximator (policy smoothness) reduces vulnerability in some environments, but is not sufficient alone to handle intrinsic policy weaknesses; heavy regularization can degrade natural performance, and ATLA (adversary-in-the-loop) can outperform SA-PPO in multiple environments.",
            "uuid": "e988.3",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "LSTM policy (history-based)",
            "name_full": "History-dependent policy via LSTM",
            "brief_description": "Use of recurrent (LSTM) policy/value networks to encode observation-action history to mitigate partial observability introduced by adversarial observation perturbations.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "LSTM-based recurrent policy for POMDP/SA-MDP",
            "method_description": "Parameterize agent policy and value function with an LSTM that consumes sequences of perturbed observations and actions h_t = {hat{s}_0, a_0, ..., hat{s}_t} to produce actions; training uses recurrent policy gradient / PPO with backpropagation through time. This leverages temporal coherence: adversarial/transient perturbations can be filtered by integrating information over history, enabling disambiguation of true state.",
            "environment_name": "MuJoCo continuous-control benchmarks and GridWorld example",
            "environment_description": "Interactive RL benchmarks where observation perturbations make the problem POMDP-like; LSTM policies operate on sequential data and require unfolding across time during training; environments allow active online training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Temporal integration: LSTM aggregates past observations and actions, reducing reliance on any single (possibly corrupted) observation and implicitly downweighting transient distractors.",
            "spurious_signal_types": "Transient adversarial observation noise and spurious features that are inconsistent over time.",
            "detection_method": null,
            "downweighting_method": "Implicit via memory/aggregation: inconsistent (spurious) signals have reduced influence because the LSTM conditions on longer histories.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "LSTM policies trained with ATLA show significantly improved worst-case returns compared to feedforward policies; e.g., ATLA-PPO (LSTM) often surpasses ATLA-PPO (MLP) and SA-PPO in reported best-attack rewards (Table 2).",
            "performance_without_robustness": "Vanilla PPO(LSTM) without adversarial training does not reliably improve robustness compared to PPO(MLP); LSTM alone is insufficient without adversary-in-the-loop training.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "History-dependent policies (LSTM) are empirically more robust when trained under adversary-in-the-loop (ATLA); this aligns with the theoretical reduction that fixed-adversary SA-MDP becomes a POMDP where history matters.",
            "uuid": "e988.4",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "SARSOP",
            "name_full": "Successive Approximations of the Reachable Space (SARSOP)",
            "brief_description": "A point-based POMDP solver that computes approximate history-dependent policies (finite-state controllers) for POMDPs and was used here as a proof-of-concept to obtain a robust policy in the GridWorld SA-MDP example.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SARSOP (POMDP solver)",
            "method_description": "SARSOP is a point-based algorithm that approximates optimally reachable belief spaces to compute an approximate POMDP solution (finite-state controller); when applied to the GridWorld with a fixed adversary, SARSOP finds a history-dependent policy that defeats the adversary by maintaining belief over true state despite perturbed observations.",
            "environment_name": "GridWorld toy example (finite-state)",
            "environment_description": "Small discrete MDP with bounded perturbation support B(s) where adversary perturbs observations to neighboring cells; environment is fully specified and amenable to offline POMDP planning.",
            "handles_distractors": true,
            "distractor_handling_technique": "Belief/planning over histories: SARSOP computes policies over belief space to handle observation uncertainty introduced by adversary, effectively reasoning about distractors by maintaining posterior beliefs and choosing actions robust to observation noise.",
            "spurious_signal_types": "Observation-level perturbations to discrete states (neighboring-state observations) and other partial observability sources.",
            "detection_method": null,
            "downweighting_method": "Belief-updating and planning: inconsistent observations are downweighted via belief updates across time, and the policy chooses actions robust under belief uncertainty.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "SARSOP produced an 8-state finite-state controller that almost eliminates impact of adversary in the GridWorld example (Figure 1c), recovering near-perfect reward under the fixed adversary.",
            "performance_without_robustness": "A stationary optimal MDP policy (no history) fails catastrophically under the adversary (Figure 1b) with the agent repeatedly encountering traps.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Exact/approximate POMDP solvers like SARSOP can find history-dependent policies that are robust to observation-level adversaries in small discrete domains, illustrating the theoretical mapping from SA-MDP-with-fixed-adversary to POMDP.",
            "uuid": "e988.5",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Policy smoothness / certified defenses",
            "name_full": "Policy smoothness regularization and certified adversarial defenses (convex relaxations)",
            "brief_description": "A class of techniques adapted from supervised learning (TRADES, VAT, convex-relaxation-based certification) that enforce or certify small changes in policy/value outputs under bounded input perturbations, used as defenses against observation perturbations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Policy smoothness regularization and convex-relaxation certification",
            "method_description": "These approaches either (1) add a regularizer that minimizes worst-case divergence between policy outputs in the neighborhood B(s) (adversarial minimax regularization analogous to TRADES/VAT), or (2) use convex-relaxation techniques (interval bound propagation, abstract interpretation) to compute certified bounds on network outputs under ℓ_p perturbations and train to improve provable robustness. In RL context they are applied to Q-networks or policy networks to reduce sensitivity to small perturbations.",
            "environment_name": "High-dimensional observation domains (e.g., Atari images) and continuous control benchmarks as referenced",
            "environment_description": "Simulated training/evaluation settings where the function approximator (NN) processes high-dimensional observations; neither approach requires active adversary learning (they are training-time regularization/certification methods).",
            "handles_distractors": true,
            "distractor_handling_technique": "Function-approximation robustness: controlling Lipschitz-like sensitivity of networks via explicit regularizers or certifiable upper bounds, thereby reducing the influence of spurious/distractor features that lie within B(s).",
            "spurious_signal_types": "Small bounded adversarial perturbations and input noise that exploit brittle neural network decision boundaries.",
            "detection_method": null,
            "downweighting_method": "Regularization of output divergence across neighborhoods (minimize max D(pi(s),pi(s+delta))) and training objectives that penalize vulnerability as measured by certification bounds.",
            "refutation_method": "Certified bounds provide formal guarantees that policy outputs cannot change beyond computed bounds under specified perturbation budgets (refuting some spurious-robustness claims), but full causal refutation is not offered.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Prior works cited (Mirman et al., Wong & Kolter, Zhang et al.) show improvements in adversarial robustness of networks; in RL, SA-PPO (regularization variant) can improve robustness in some MuJoCo tasks but may be insufficient alone (paper reports mixed empirical results).",
            "performance_without_robustness": "Vanilla networks without such regularization are vulnerable to small perturbations (many adversarial attacks cause catastrophic drop in agent returns).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Function-approximator level defenses reduce vulnerability stemming from brittle neural networks, but do not address intrinsic policy weaknesses that occur even in tabular MDPs (GridWorld example); hence they are complementary to adversary-in-the-loop approaches like ATLA.",
            "uuid": "e988.6",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robust deep reinforcement learning against adversarial perturbations on observations",
            "rating": 2
        },
        {
            "paper_title": "Snooping attacks on deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Adversarial attacks on neural network policies",
            "rating": 1
        },
        {
            "paper_title": "Robust deep reinforcement learning against adversarial perturbations on observations",
            "rating": 2
        },
        {
            "paper_title": "Towards deep learning models resistant to adversarial attacks",
            "rating": 2
        },
        {
            "paper_title": "Distributional smoothing with virtual adversarial training",
            "rating": 1
        }
    ],
    "cost": 0.020913,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</h1>
<p>Huan Zhang ${ }^{\text { }}{ }^{1}$ Hongge Chen ${ }^{\text {, }}{ }^{2}$ Duane Boning ${ }^{2}$ Cho-Jui Hsieh ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, UCLA ${ }^{2}$ Department of EECS, MIT<br>huan@huan-zhang.com, chenhg@mit.edu, boning@mtl.mit.edu<br>chohsieh@cs.ucla.edu<br>${ }^{*}$ Huan Zhang and Hongge Chen contributed equally.</p>
<h4>Abstract</h4>
<p>We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.</p>
<h2>1 INTRODUCTION</h2>
<p>Modern deep reinforcement learning agents (Mnih et al., 2015; Levine et al., 2015; Lillicrap et al., 2015; Silver et al., 2016; Fujimoto et al., 2018) typically use neuron networks as function approximators. Since the discovery of adversarial examples in image classification tasks (Szegedy et al., 2013), the vulnerabilities in DRL agents were first demonstrated in (Huang et al., 2017; Lin et al., 2017; Kos \&amp; Song, 2017) and further developed under more environments and different attack scenarios (Behzadan \&amp; Munir, 2017a; Pattanaik et al., 2018; Xiao et al., 2019). These attacks commonly add imperceptible noises into the observations of states, e.g., the observed environment slightly differs from true environment. This raises concerns for using RL in safety-crucial applications such as autonomous driving (Sallab et al., 2017; Voyage, 2019); additionally, the discrepancy between ground-truth states and agent observations also contributes to the "reality gap" - an agent working well in simulated environments may fail in real environments due to noises in observations (Jakobi et al., 1995; Muratore et al., 2019), as real-world sensing contains unavoidable noise (Brooks, 1992).</p>
<p>We classify the weakness of a DRL agent on the perturbations of state observations into two classes: the vulnerability in function approximators, which typically originates from the highly non-linear and blackbox nature of neural networks; and intrinsic weakness of policy: even perfect features for states are extracted, an agent can still make mistakes due to an intrinsic weakness in its policy.</p>
<p>For example, in the deep Q networks (DQNs) for Atari games, a large convolutional neural network (CNN) is used for extracting features from input frames. To act correctly, the network must extract crucial features: e.g., for the game of Pong, the position and velocity of the ball, which can observed by visualizing convolutional layers (Hausknecht \&amp; Stone, 2015; Guo et al., 2014). Many attacks to the DQN setting add imperceptible noises (Huang et al., 2017; Lin et al., 2017; Kos \&amp; Song, 2017; Behzadan \&amp; Munir, 2017a) that exploit the vulnerability of deep neural networks so that they extract wrong features, as we have seen in adversarial examples of image classification tasks. On the other</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Path in unperturbed environment (found by policy iteration). Agent's reward $=+1$. Black arrows and numbers show actions and value function of the agent.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Path under the optimal adversary. Agent's reward $=-\infty$. Red arrows and numbers show actions and value function of the optimal adversary (Section 3.1).
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) A robust POMDP policy solved by SARSOP (Kurniawati et al., 2008) under the same adversary. This policy is history dependent (Section 3.2).</p>
<p>Figure 1: We show an agent in gridworld environment trained with no function approximators, and its optimal policy is intrinsically not robust to perturbations of state observations. The red square and blue circle are the starting point and target (reward +1 ) of the agent, respectively. The green triangles are traps, with reward -1 once encountered. The adversary is allowed to perturb the observation to adjacent states along four directions: up, down, left, and right. Adversary earns +1 at traps and -1 at the target. We set $\gamma=0.9$ for both agent and adversary. This example shows that the vulnerability of a RL agent does not only come from the errors in function approximators such as DNNs.
hand, the fragile function approximation is not the only source of the weakness of a RL agent in a finite-state Markov decision process (MDP), we can use tabular policy and value functions so there is no function approximation error. The agent can still be vulnerable to small perturbations on observations, e.g., perturbing the observation of a state to one of its four neighbors in a gridworldlike environment can prevent an agent from reaching its goal (Figure 1). To improve the robustness of RL, we need to take measures from both aspects - a more robust function approximator, and a policy aware of perturbations in observations.</p>
<p>Techniques developed in enhancing the robustness of neural network (NN) classifiers can be applied to address the vulnerability in function approximators. Especially, for environments like Atari games with images as input and discrete actions as outputs, the policy network $\pi_{\theta}$ behaves similarly to a classifier in test time. Thus, Fischer et al. (2019); Mirman et al. (2018a) utilized existing certified adversarial defense (Mirman et al., 2018b; Wong \&amp; Kolter, 2018; Gowal et al., 2018; Zhang et al., 2020a) approaches in supervised learning to enhance the robustness of DQN agents. Another successful approach (Zhang et al., 2020b) for both Atari and high-dimensional continuous control environment regularizes the smoothness of the learned policy such that $\max_{\hat{s} \in \mathcal{B}(s)} D\left(\pi_{\theta}(s), \pi_{\theta}(\hat{s})\right)$ is small for some divergence $D$ and $\mathcal{B}(s)$ is a neighborhood around $s$. This maximization can be solved using a gradient based method or convex relaxations of NNs (Salman et al., 2019; Zhang et al., 2018; Xu et al., 2020), and then minimized by optimizing $\theta$. Such an adversarial minimax regularization is in the same spirit as the ones used in some adversarial training approaches for (semi)supervised learning, e.g., TRADES (Zhang et al., 2019) and VAT (Miyato et al., 2015). However, regularizing the function approximators does not explicitly improve the intrinsic policy robustness.</p>
<p>In this paper, we propose an orthogonal approach, alternating training with learned adversaries (ATLA), to enhance the robustness of DRL agents. We focus on dealing with the intrinsic weakness of the policy by learning an adversary online with the agent during training time, rather than directly regularizing function approximators. Our main contributions can be summarized as:</p>
<ul>
<li>We follow the framework of state-adversarial Markov decision process (SA-MDP) and show how to learn an optimal adversary for perturbing observations. We demonstrate practical attacks under this formulation and obtain learned adversaries that are significantly stronger than previous ones.</li>
<li>We propose the alternating training with learned adversaries (ATLA) framework to improve the robustness of DRL agents. The difference between our approach and previous adversarial training approaches is that we use a stronger adversary, which is learned online together with the agent.</li>
<li>
<p>Our analysis on SA-MDP also shows that history can be important for learning a robust agent. We thus propose to use a LSTM based policy in the ATLA framework and find that it is more robust than policies parameterized as regular feedforward NNs.</p>
</li>
<li>
<p>We evaluate our approach empirically on four continuous control environments. We outperform explicit regularization based methods in a few environments, and our approach can also be directly combined with explicit regularizations on function approximators to achieve state-of-the-art results.</p>
</li>
</ul>
<h1>2 Related Work</h1>
<p>State-adversarial Markov decision process (SA-MDP) (Zhang et al., 2020b) characterizes the decision making problem under adversarial attacks on state observations. Most importantly, the true state in the environment is not perturbed by the adversary under this setting; for example, perturbing pixels in an Atari environment (Huang et al., 2017; Kos \&amp; Song, 2017; Lin et al., 2017; Behzadan \&amp; Munir, 2017a; Inkawhich et al., 2019) does not change the true location of an object in the game simulator. SA-MDP can characterize agent performance under natural or adversarial noise from sensor measurements. For example, GPS sensor readings on a car are naturally noisy, but the ground truth location of the car is not affected by the noise. Importantly, this setting is different from robust Markov decision process (RMDP) (Nilim \&amp; El Ghaoui, 2004; Iyengar, 2005), where the worst case transition probabilities of the environment are considered. "Robust reinforcement learning" in some works (Mankowitz et al., 2018; 2019) refer to this different definition of robustness in RMDP, and should not be confused with our setting of robustness against perturbations on state observations.</p>
<p>Several works proposed methods to learn an adversary online together with an agent. RARL (Pinto et al., 2017) proposed to train an agent and an adversary under the two-player Markov game (Littman, 1994) setting. The adversary can change the environment states through actions directly applied to environment. The goal of RARL is to improve the robustness against environment parameter changes, such as mass, length or friction. Gleave et al. (2019) discussed the learning of an adversary using reinforcement learning to attack a victim agent, by taking adversarial actions that changes the environment and consequentially change the observation of the victim agent. Both Pinto et al. (2017); Gleave et al. (2019) conduct their attack under on the two-player Markov game framework, rather than considering perturbations on state observations. Besides, Li et al. (2019) consider a similar Markov game setting in multi-agent RL environments. The difference between these works and ours can be clearly seen in the setting where the adversary is fixed - under the framework of (Pinto et al., 2017; Gleave et al., 2019), the learning of agent is still a MDP, but in our setting, it becomes a harder POMDP problem (Section 3.2).</p>
<p>Training DRL agents with perturbed state observations from adversaries have been investigated in a few works, sometimes referred to as adversarial training. Kos \&amp; Song (2017); Behzadan \&amp; Munir (2017b) used gradient based adversarial attacks to DQN agents and put adversarial frames into replay buffer. This approach is not very successful because for Atari environments the main source of weakness is likely to come from the function approximator, so an adversarial regularization framework such as (Zhang et al., 2020b; Qu et al., 2020) which directly controls the smoothness of the $Q$ function is more effective. For lower dimensional continuous control tasks such as the MuJoCo environments, Mandlekar et al. (2017); Pattanaik et al. (2018) conducted FGSM and multistep gradient based attacks during training time; however, their main focus was on the robustness against environment parameter changes and only limited evaluation on the adversarial attack setting was conducted with relatively weak adversaries. Zhang et al. (2020b) systematically tested this approach under newly proposed strong attacks, and found that it cannot reliably improve robustness. These early adversarial training approaches typically use gradients from a critic function. They are usually relatively weak, and not sufficient to lead to a robust policy under stronger attacks.</p>
<p>The robustness of RL has also been investigated from other perspectives. For example, Tessler et al. (2019) study MDPs under action perturbations; Tan et al. (2020) use adversarial training on action space to enhance agent robustness under action perturbations. Besides, policy teaching (Zhang \&amp; Parkes, 2008; Zhang et al., 2009; Ma et al., 2019) and policy poisoning (Rakhsha et al., 2020; Huang \&amp; Zhu, 2019) manipulate the reward or cost signal during agent training time to induce a desired agent policy. Essentially, policy teaching is a training time "attack" with perturbed rewards from the environments (which can be analogous to data poisoning attacks in supervised learning settings), while our goal is to obtain a robust agent against test time adversarial attacks. All these settings differ from the setting of perturbing state observations discussed in our paper.</p>
<h2>3 Methodology</h2>
<p>In this section, we first discuss the case where the agent policy is fixed, and then the case where the adversary is fixed in SA-MDPs. This allows us to propose an alternating training framework to improve robustness of RL agents under perturbations on state observations.</p>
<p>Notations and Background We use $\mathcal{S}$ and $\mathcal{A}$ to represent the state space and the action space, respectively; $\mathcal{P}(\mathcal{S})$ defines the set of all possible probability measures on $\mathcal{S}$. We define a Markov decision process (MDP) as $(\mathcal{S}, \mathcal{A}, R, p, \gamma)$, where $R: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ and $p: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{S})$ are two mappings represent the reward and transition probability. The transition probability at time step $t$ can be written as $p\left(s^{\prime} \mid s, a\right)=\operatorname{Pr}\left(s_{t+1}=s^{\prime} \mid s_{t}=s, a_{t}=a\right)$. Reward function is defined as the expected reward $R\left(s, a, s^{\prime}\right):=\mathbb{E}\left[r_{t} \mid s_{t}=s, a_{t}=a, s_{t+1}=s^{\prime}\right] . \quad \gamma \in[0,1]$ is the discounting factor. We denote a stationary policy as $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ which is independent of history. We denote history $h_{t}$ at time $t$ as $\left{s_{0}, a_{0}, \cdots, s_{t-1}, a_{t-1}, s_{t}\right}$ and $\mathcal{H}$ as the set of all histories. A history-dependent policy is defined as $\pi: \mathcal{H} \rightarrow \mathcal{P}(\mathcal{A})$. A partially observable Markov decision process ( [Astrom, 1965]) (POMDP) can be defined as a 7-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{O}, \Omega, R, p, \gamma)$ where $\mathcal{O}$ is a set of observations and $\Omega$ is a set of conditional observation probabilities $p(o \mid s)$. Unlike MDPs, POMDPs typically require history-dependent optimal policies.</p>
<p>To study the decision problem under adversaries on state observations, we use state-adversarial Markov decision process (SA-MDP) framework ( [Zhang et al., 2020b]). In SA-MDP, an adversary $\nu: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{S})$ is introduced to perturb the input state of an agent; however, the true environment state $s$ is unchanged (Figure 2). Formally, an SA-MDP is a 6-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{B}, R, p, \gamma)$ where $\mathcal{B}$ is a mapping from a state $s \in \mathcal{S}$ to a set of states $\mathcal{B}(s) \in \mathcal{S}$. The agent sees the perturbed state $\tilde{s} \sim \nu(\cdot \mid s)$ and takes the action $\pi(a \mid \tilde{s})$ accordingly. $\mathcal{B}$ limits the power of adversary: $\operatorname{supp}(\nu(\cdot \mid s)) \in \mathcal{B}(s)$. The goal of SA-MDP is to solve an optimal policy $\pi^{<em>}$ under its optimal adversary $\nu^{</em>}\left(\pi^{<em>}\right)$; an optimal adversary is defined as $\nu^{</em>}(\pi)$ such that $\pi$ achieves the lowest possible expected discounted return (or value) on all states. [Zhang et al. (2020b)] did not give an explicit algorithm to solve SA-MDP and found that a stationary optimal policy need not exist.</p>
<h1>3.1 FINDING THE OPTIMAL ADVERSARY UNDER A FIXED POLICY</h1>
<p>In this section, we discuss how to find an optimal adversary $\nu$ for a given policy $\pi$. An optimal adversary leads to the worst case performance under bounded perturbation set $\mathcal{B}$, and is an absolute lower bound of the expected cumulative reward an agent can receive. It is similar to the concept of "minimal adversarial example" in supervised learning tasks. We first show how to solve the optimal adversary in MDP setting and then apply it to the DRL settings.</p>
<p>A technical lemma (Lemma 1) from <em>Zhang et al. (2020b)</em> shows that, from the adversary's point of view, a fixed and stationary agent policy $\pi$ and the environment dynamics can be essentially merged into an MDP with redefined dynamics and reward functions:
Lemma 1 ( [Zhang et al. (2020b)]) Given an SA-MDP $M=(\mathcal{S}, \mathcal{A}, R, \mathcal{B}, p, \gamma)$ and a fixed and stationary policy $\pi(\cdot \mid \cdot)$, there exists an MDP $\hat{M}=(\mathcal{S}, \hat{\mathcal{A}}, \hat{R}, \hat{p}, \gamma)$ such that the optimal policy of $\hat{M}$ is the optimal adversary $\nu$ for SA-MDP given the fixed $\pi$, where $\hat{\mathcal{A}}=\mathcal{S}$, and</p>
<p>$$
\hat{R}\left(s, \hat{a}, s^{\prime}\right):=\mathbb{E}\left[\hat{r} \mid s, \hat{a}, s^{\prime}\right]=\left{\begin{array}{ll}
-\sum_{a \in \mathcal{A}} \pi(a \mid \hat{a}) p\left(s^{\prime} \mid s, a\right) R\left(s, a, s^{\prime}\right) \
-\sum_{a \in \mathcal{A}} \pi(a \mid \hat{a}) p\left(s^{\prime} \mid s, a\right) &amp; \text { for } s, s^{\prime} \in \mathcal{S} \text { and } \hat{a} \in \mathcal{B}(s) \subset \hat{\mathcal{A}}, \
C &amp; \text { for } s, s^{\prime} \in \mathcal{S} \text { and } \hat{a} \notin \mathcal{B}(s)
\end{array}\right.
$$</p>
<p>where $C$ is a large negative constant, and</p>
<p>$$
\hat{p}\left(s^{\prime} \mid s, \hat{a}\right)=\sum_{a \in \mathcal{A}} \pi(a \mid \hat{a}) p\left(s^{\prime} \mid s, a\right) \quad \text { for } s, s^{\prime} \in \mathcal{S} \text { and } \hat{a} \in \hat{\mathcal{A}}
$$</p>
<p>The intuition behind Lemma 1 is that the adversary's goal is to reduce the reward earned by the agent. Thus, when a reward $r_{t}$ is received by the agent at time step $t$, the adversary receives a negative reward of $\hat{r}<em t="t">{t}=-r</em>\right]$ which yields the term in Lemma 1. The proof can be found in Appendix B of }$. To prevent the agent from taking actions outside of set $\mathcal{B}(s)$, a large negative reward $C$ is assigned to these actions such that the optimal adversary cannot take them. For actions within $\mathcal{B}(s)$, we calculate $\hat{R}\left(s, \hat{a}, s^{\prime}\right)$ by its definition, $\hat{R}\left(s, \hat{a}, s^{\prime}\right):=\mathbb{E}\left[\hat{r} \mid s, \hat{a}, s^{\prime<em>Zhang et al. (2020b)</em>.</p>
<p>After constructing the MDP $\hat{M}$, it is possible to solve an optimal agent $\nu$ of $\hat{M}$, which will be the optimal adversary on SA-MDP $M$ given policy $\pi$. For MDPs, under mild regularity assumptions an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Our "Optimal" Attack and Robust Sarsa attack (a previous strong attack proposed in Zhang et al. (2020b)) on Ant and HalfCheetah environments. Previous strong attacks make the agent fail and receive a small positive reward (less than $1 / 10$ of the reward without attack). Our attack is strong enough to trick the agent into moving to the opposite direction, receiving a large negative reward.
optimal policy always exists (Puterman, 2014). In our case, the optimal policy on $\bar{M}$ corresponds to an optimal adversary in SA-MDP, which is the worst case perturbation for policy $\pi$. As an illustration, in Figure 1, we show a GridWorld environment. The red square is the starting point. The blue circle and green triangles are the target and traps, respectively. When the agent hits the target, it earns reward +1 and the game stops and it earns reward -1 when it encounters a trap. We set $\gamma=0.9$ for both agent and adversary. The adversary is allowed to perturb the observation to adjacent cells along four directions: up, down, left, and right. When there is no adversary, after running policy iteration, the agent can easily reach target and earn a reward of +1 , as in Figure 1a. However, if we train the adversary based on Lemma 1 and apply it to the agent, we are able to make the agent repeatedly encounter a trap. This leads to $-\infty$ reward for the agent and $+\infty$ reward for the adversary as shown in Figure 1b.</p>
<p>We now extend this Lemma 1 to the DRL setting. Since the learning of adversary is equivalent to solving an MDP, we parameterize the adversary as a neural network function and use any popular DRL algorithm to learn an "optimal" adversary. Here we quote the word "optimal" as we use function approximator to learn the agent so it's no longer optimal, but we emphasize that it follows the SA-MDP framework of solving an optimal adversary. No existing adversarial attacks follow such a theoretically guided framework. We show our algorithm in Algorithm 1. Instead of learning to produce $\hat{s} \in \mathcal{B}(s)$ directly, since $\mathcal{B}(s)$ is usually a small set nearby $s$ (e.g., $\mathcal{B}=\left{s^{\prime} \mid\left|s-s^{\prime}\right|_{p} \leq \epsilon\right}$, our adversary learns a perturbation vector $\Delta$, and we project $s+\Delta$ to $\mathcal{B}(s)$.
The first advantage of attacking a policy in this way is that it is strong - as we allow to optimize the adversary in an online loop of interactions with the agent policy and environment, and keep improving the adversary with a goal of receiving as less reward as possible. It is strong because it follows the theoretical framework of finding an optimal adversary, rather than using any heuristic to generate a perturbation. Empirically, in the cases demonstrated in Figure 3, previous strong attacks (e.g., Robust Sarsa attack) can successfully fail an agent and make it stop moving and receive a small positive reward; our learned attack can trick the agent into moving toward the opposite direction of the goal and receive a large negative reward. We also find that this attack can further reduce the reward of robustly trained agents, like SA-PPO (Zhang et al., 2020b).</p>
<p>The second advantage of this attack is that it requires no gradient access to the policy itself; in fact, it treats the agent as part of the environment and only needs to run it in a blackbox. Previous attacks (e.g., Lin et al. (2017); Pattanaik et al. (2018); Xiao et al. (2019)) are mostly gradient based approach and need to access the values or gradients to a policy or value function. Even without access to gradients, the overall learning process is still just a MDP and we can apply any popular modern DRL methods to learn the adversary.</p>
<h1>3.2 FINDING THE OPTIMAL POLICY UNDER A FIXED ADVERSARY</h1>
<p>We now investigate SA-MDP when we fix the adversary $\nu$ and find an optimal policy. In Lemma 2, we show that this case SA-MDP becomes a POMDP:</p>
<p>Lemma 2 (Optimal policy under fixed adversary) Given an SA-MDP $M=(\mathcal{S}, \mathcal{A}, \mathcal{B}, R, p, \gamma)$ and a fixed and stationary adversary $\nu(\hat{s} \mid s)$, there exists a POMDP $\bar{M}=(\mathcal{S}, \mathcal{A}, \Omega, O, R, p, \gamma)$</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Learning</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="s">&quot;optimal&quot;</span><span class="w"> </span><span class="nx">adversary</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">perturbations</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">observations</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Policy</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">under</span><span class="w"> </span><span class="nx">attack</span><span class="p">,</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">iterations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">iter</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">perturbation</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}(</span><span class="nx">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="nx">initialize</span><span class="w"> </span><span class="nx">adversary</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">nu_</span><span class="p">{</span><span class="err">\</span><span class="nx">phi</span><span class="p">}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">parameterized</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">neural</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">iter</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Traj</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">nu_</span><span class="p">{</span><span class="err">\</span><span class="nx">phi</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="p">,</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">samples</span><span class="w"> </span><span class="p">(</span><span class="k">for</span><span class="w"> </span><span class="nx">simplicity</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">ignore</span><span class="w"> </span><span class="nx">episodes</span><span class="w"> </span><span class="nx">here</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">PolicyOptimizer</span><span class="p">}(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">phi</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="nx">Function</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">A</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="nx">v</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Traj</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">nu_</span><span class="p">{</span><span class="err">\</span><span class="nx">phi</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="p">,</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="nx">right</span><span class="p">):</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">Initial</span><span class="w"> </span><span class="nx">state</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">emptyset</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">b</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">nu_</span><span class="p">{</span><span class="err">\</span><span class="nx">phi</span><span class="p">}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Proj</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}}(</span><span class="nx">s</span><span class="p">)(</span><span class="nx">s</span><span class="o">+</span><span class="err">\</span><span class="nx">Delta</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">projection</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">clipping</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">ell_</span><span class="p">{</span><span class="err">\</span><span class="nx">infty</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">norm</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">B</span><span class="p">}(</span><span class="nx">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">next</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">environment</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">Delta</span><span class="p">,</span><span class="o">-</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">\#</span><span class="w"> </span><span class="nx">state</span><span class="p">,</span><span class="w"> </span><span class="nx">action</span><span class="p">,</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">next</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">adversary</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>such that the optimal policy of $\bar{M}$ is the optimal policy $\pi$ for SA-MDP given the fixed $\nu$, where</p>
<p>$$
\Omega=\bigcup_{s \in \mathcal{S}}\left{s^{\prime} \mid s^{\prime} \in \operatorname{supp}(\nu(\cdot \mid s))\right}, \quad O(o \mid s)=\nu(\hat{s} \mid s)
$$</p>
<p>where $\Omega$ is the set of observations, and $O$ defines the conditional observational probabilities (in our case it is conditioned only on $s$ and does not depend on actions). To prove Lemma 2, we construct the a POMDP with the observations defined on the support of all $\nu(\cdot \mid s), s \in \mathcal{S}$ and the observation process is exactly the process of generating an adversarially perturbed state $\hat{s}$. This POMDP is functionally identical to the original SA-MDP when $\nu$ is fixed. This lemma unveils the connection between POMDP and SA-MDP: SA-MDP can be seen as a version of "robust" POMDP where the policy needs to be robust under a set of observational processes (adversaries). SA-MDP is different from robust POMDP (RPOMDP) (Osogami, 2015; Rasouli \&amp; Saghafian, 2018), which optimizes for the worst case environment transitions.</p>
<p>As a proof of concept, we use a modern POMDP solver, SARSOP (Kurniawati et al., 2008) to solve the GridWorld environment in Figure 1 to find a policy that can defeat the adversary. The POMDP solver produces a finite state controller (FSC) with 8 states (FSC is an efficient representation of history dependent policies). This FSC policy can almost eliminate the impact of the adversary and receive close to perfect reward, as shown in Figure 1c.</p>
<p>Unfortunately, unlike MDPs, it is challenging to solve an optimal policy for POMDPs; state-of-theart solvers (Bai et al., 2014; Sunberg \&amp; Kochenderfer, 2017) can only work on relatively simple environments which are much smaller than those used in modern DRL. Thus, we do not aim to solve the optimal policy. We follow (Wierstra et al., 2007) to use recurrent policy gradient theorem on POMDPs and use LSTM as function approximators for the value and policy networks. We denote $h_{t}=\left{\hat{s}<em 0="0">{0}, a</em>}, \hat{s<em 1="1">{1}, a</em>} \cdots, \hat{s<em t="t">{t}\right}$ containing all history of states (perturbed states $\hat{s}$ in our setting) and actions. The policy $\pi$ parameterized by $\theta$ takes an action $a</em>$ is typically encoded by a recurrent neural network (e.g., LSTM). The recurrent policy gradient theorem (Wierstra et al., 2007) shows that}$ given all observed history $h_{t}$, and $h_{t</p>
<p>$$
\nabla_{\theta} J \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t}^{n} \mid h_{t}^{n}\right) r_{t}^{n}
$$</p>
<p>where $N$ is the number of sampled episodes, $T$ is episode length (for notation similarity, we assume each episode has the same length), and $h_{t}^{n}$ is the history of states for episode $n$ up to time $t$, and $r_{t}^{n}$ is the reward received for episode $n$ at time $t$. We can then extend Eq. 2 to modern DRL algorithms such as proximal policy optimization (PPO), similarly as done in (Azizzadenesheli et al., 2018), by</p>
<p>using the following loss function:</p>
<p>$$
J(\theta) \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=0}^{T}\left[\min \left(\frac{\pi_{\theta}\left(a_{t}^{n} \mid h_{t}^{n}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t}^{n} \mid h_{t}^{n}\right)} A_{h_{t}^{n}}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(a_{t}^{n} \mid h_{t}^{n}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t}^{n} \mid h_{t}^{n}\right)}, 1-\epsilon, 1+\epsilon\right) A_{h_{t}^{n}}\right)\right]
$$</p>
<p>where $A_{h_{t}^{n}}$ is a baseline advantage function for episode $n$ time step $t$, which is based on a LSTM value function. $\epsilon$ is the clipping threshold in PPO. The loss can be optimized via a gradient based optimizer and $\theta_{\text {old }}$ is the old policy parameter before optimization iterations start. Although a LSTM or recurrent policy network has been used in the DRL setting in a few other works (Hausknecht \&amp; Stone, 2015; Azizzadenesheli et al., 2018), our focus is to improve agent robustness rather than learning a policy purely for POMDPs. In our empirical evaluation, we will compare feedforward and LSTM policies under our ATLA framework.</p>
<h1>3.3 Alternating Training with Learned Adversaries (ATLA)</h1>
<p>As we have discussed in Section 3.1, we can solve an optimal adversary given any fixed policy. In our ATLA framework, we train such an adversary online with the agent: we first keep the agent and optimize the adversary; the adversary is also parameterized as a neural network. Then we keep the adversary and optimize the agent. Both adversary and agent can be updated using a policy gradient algorithm such as PPO. We show our full algorithm in Algorithm 2.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Alternating</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">Learned</span><span class="w"> </span><span class="n">Adversaries</span><span class="w"> </span><span class="p">(</span><span class="n">ATLA</span><span class="p">)</span>
<span class="n">Input</span><span class="o">:</span><span class="w"> </span><span class="n">Environment</span><span class="w"> </span><span class="p">\(\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}\)</span><span class="o">,</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">iterations</span><span class="w"> </span><span class="p">\(</span><span class="n">N_</span><span class="p">{\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">iter</span><span class="w"> </span><span class="p">}}\)</span><span class="o">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="p">\(</span><span class="n">B</span><span class="p">\)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">agent</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">actor</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="p">\(\</span><span class="n">pi</span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="p">\</span><span class="n">mid</span><span class="w"> </span><span class="p">\</span><span class="n">hat</span><span class="p">{</span><span class="n">s</span><span class="p">})\)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="p">\(\</span><span class="n">theta</span><span class="p">\)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">adversary</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">actor</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="p">\(\</span><span class="n">nu</span><span class="p">(\</span><span class="n">hat</span><span class="p">{</span><span class="n">s</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">mid</span><span class="w"> </span><span class="n">s</span><span class="p">)\)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="p">\(\</span><span class="n">phi</span><span class="p">\)</span><span class="o">.</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">\(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">\)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="p">\(</span><span class="n">N_</span><span class="p">{\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">iter</span><span class="w"> </span><span class="p">}}\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">\(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="p">\)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="p">\(</span><span class="n">N_</span><span class="p">{\</span><span class="n">pi</span><span class="p">}\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="n">Run</span><span class="w"> </span><span class="p">\(\</span><span class="n">pi_</span><span class="p">{\</span><span class="n">theta</span><span class="p">}\)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">fixed</span><span class="w"> </span><span class="p">\(\</span><span class="n">nu_</span><span class="p">{\</span><span class="n">phi</span><span class="p">}\)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">collect</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">trajectories</span><span class="w"> </span><span class="p">\(\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}_{\</span><span class="n">pi</span><span class="p">}</span><span class="o">:=</span><span class="p">\</span><span class="n">left</span><span class="o">.</span><span class="p">\</span><span class="n">left</span><span class="p">\{\</span><span class="n">left</span><span class="p">(\</span><span class="n">hat</span><span class="p">{</span><span class="n">s</span><span class="p">}_{</span><span class="n">t</span><span class="p">}^{</span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">t</span><span class="p">}^{</span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">t</span><span class="p">}^{</span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">hat</span><span class="p">{</span><span class="n">s</span><span class="p">}_{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}^{</span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">j</span><span class="p">}\</span><span class="n">right</span><span class="p">)\</span><span class="n">right</span><span class="p">\}\</span><span class="n">right</span><span class="p">|_{</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="o">^</span><span class="p">{</span><span class="n">B</span><span class="p">}\)</span><span class="o">.</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">theta</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="p">\)</span><span class="w"> </span><span class="n">PolicyOptimizer</span><span class="w"> </span><span class="p">\(\</span><span class="n">left</span><span class="p">(\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}_{\</span><span class="n">pi</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">theta</span><span class="p">\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">\(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="p">\)</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="p">\(</span><span class="n">N_</span><span class="p">{\</span><span class="n">nu</span><span class="p">}\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}_{\</span><span class="n">nu</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="p">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">Adv</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">cdot</span><span class="w"> </span><span class="p">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">Traj</span><span class="p">}\</span><span class="n">left</span><span class="p">(\</span><span class="n">nu_</span><span class="p">{\</span><span class="n">phi</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">pi_</span><span class="p">{\</span><span class="n">theta</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="n">B</span><span class="p">\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\#</span><span class="w"> </span><span class="p">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">Adv</span><span class="p">}</span><span class="w"> </span><span class="p">\_</span><span class="n">Traj</span><span class="p">\)</span><span class="w"> </span><span class="n">defined</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">phi</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="p">\)</span><span class="w"> </span><span class="n">PolicyOptimizer</span><span class="w"> </span><span class="p">\(\</span><span class="n">left</span><span class="p">(\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}_{\</span><span class="n">nu</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">phi</span><span class="p">\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>Our algorithm is designed to use a strong and learned adversary that tries to find intrinsic weakness of the policy, and to obtain a good reward the policy must learn to defeat such an adversary. In other words, it attempts to solve the SA-MDP problem directly rather than relying on explicit regularization on the function approximator like the approach in (Zhang et al., 2020b). In our empirical evaluation, we show that such regularization can be unhelpful in some environments and harmful for performance when evaluating the agent without attacks.</p>
<p>The difference between our approach and previous adversarial training approaches such as (Pattanaik et al., 2018) is that we use a stronger adversary, learned online with the agent. Our empirical evaluation finds that using such a learned "optimal" adversary in training time allows the agent to learn a robust policy generalized to different types of strong adversarial attacks during test time. Additionally, it is important to distinguish between the original state $s$ and the perturbed state $\hat{s}$. We find that using $s$ instead of $\hat{s}$ to train the advantage function and policy of the agent leads to worse performance, as it does not follow the theoretical framework of SA-MDP.</p>
<h2>4 EXPERIMENTS</h2>
<p>"Optimal" attack on DRL agents ${ }^{1}$ In section 3.1 we show that it is possible to cast the optimal adversary finding problem as an MDP problem. In practice, the environment dynamics are unknown but model-free RL methods can be used to approximately find this optimal adversary. In this section, we use PPO to train an adversary on four OpenAI Gym MuJoCo continuous control environments.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Average episode rewards $\pm$ standard deviation over 50 episodes on PPO and SA-PPO agents. We report natural rewards (no attacks) and rewards under six adversarial attacks, including a simple random noise attack, the critic based attack in <em>Pattanaik et al. (2018)</em>, MAD and RS attacks in <em>Zhang et al. (2020b)</em>, Snooping attack proposed in <em>Inkawhich et al. (2019)</em>, and the optimal attack proposed in this paper. In each row we bold the best (lowest) attack reward over all five attacks. “Optimal” attack is better than other attacks in all environments, sometimes by a large margin.</p>
<table>
<thead>
<tr>
<th>Ens.</th>
<th>$\ell_{\infty}$ norm perturb-</th>
<th>Method</th>
<th>Natural</th>
<th>Attack Reward</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ation budget $\epsilon$</td>
<td></td>
<td></td>
<td>Reward</td>
<td>Critic</td>
<td>Random</td>
<td>MAD</td>
<td>Snooping</td>
<td>RS</td>
</tr>
<tr>
<td>Hopper</td>
<td>0.075</td>
<td>PPO</td>
<td>3167$\pm$521</td>
<td>1464 $\pm$523</td>
<td>2101$\pm$793</td>
<td>1410$\pm$ 655</td>
<td>2234$\pm$1103</td>
<td>794$\pm$238</td>
<td>636$\pm$9</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SA-PPO</td>
<td>3705$\pm$2</td>
<td>3789$\pm$15</td>
<td>2710$\pm$801</td>
<td>2652$\pm$ 835</td>
<td>2509$\pm$838</td>
<td>1130 $\pm$42</td>
<td>1076$\pm$791</td>
</tr>
<tr>
<td>Walker2d</td>
<td>0.05</td>
<td>PPO</td>
<td>4472 $\pm$ 635</td>
<td>3424 $\pm$ 1295</td>
<td>3007 $\pm$ 1200</td>
<td>2869 $\pm$ 1271</td>
<td>2786$\pm$962</td>
<td>1336 $\pm$ 654</td>
<td>1086$\pm$516</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SA-PPO</td>
<td>4487$\pm$61</td>
<td>6875$\pm$30</td>
<td>4867$\pm$39</td>
<td>3668$\pm$ 1789</td>
<td>3928$\pm$1661</td>
<td>3808$\pm$138</td>
<td>2908$\pm$1136</td>
</tr>
<tr>
<td>Ant</td>
<td>0.15</td>
<td>PPO</td>
<td>5687 $\pm$ 758</td>
<td>4934$\pm$1022</td>
<td>5261$\pm$ 1005</td>
<td>1759$\pm$ 828</td>
<td>3668$\pm$547</td>
<td>268 $\pm$227</td>
<td>-872 $\pm$436</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SA-PPO</td>
<td>4292$\pm$384</td>
<td>4805 $\pm$ 128</td>
<td>4986 $\pm$452</td>
<td>4662 $\pm$522</td>
<td>4079$\pm$768</td>
<td>3412 $\pm$1755</td>
<td>2511 $\pm$1117</td>
</tr>
<tr>
<td>HalfCheetah</td>
<td>0.15</td>
<td>PPO</td>
<td>7117$\pm$98</td>
<td>5761$\pm$119</td>
<td>5486 $\pm$ 1378</td>
<td>1836$\pm$ 866</td>
<td>1637$\pm$843</td>
<td>489$\pm$758</td>
<td>-668$\pm$219</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SA-PPO</td>
<td>3632$\pm$20</td>
<td>3589$\pm$21</td>
<td>3619$\pm$18</td>
<td>3624$\pm$23</td>
<td>3616$\pm$21</td>
<td>3283$\pm$20</td>
<td>3028 $\pm$23</td>
</tr>
</tbody>
</table>
<p>Table 1 presents results on attacking vanilla PPO and robustly trained SA-PPO (<em>Zhang et al. (2020b)</em> agents. As a comparison, we also report the attack reward of five other baseline attacks: critic attack is based on (<em>Pattanaik et al. (2018)</em>; random attack adds uniform random noise to state observations; MAD (maximal action difference) attack (<em>Zhang et al. (2020b)</em>) maximizes the differences in action under perturbed states; RS (robust sarsa) attack is based on training robust action-value functions and is the strongest attack proposed in (<em>Zhang et al. (2020b)</em>). Additionally, we include the black-box Snooping attack (<em>Inkawhich et al. (2019)</em>). For all attacks we consider $\mathcal{B}(s)$ as a $\ell_{\infty}$ norm ball around $s$ with radius $\epsilon$, set similarly as in (<em>Zhang et al. (2020b)</em>). During testing, we run the agents without attacks as well as under attacks for 50 episodes and report the mean and standard deviation of episode rewards. In Table 1 our “optimal” attack achieves noticeably lower rewards than all the other five attacks. We illustrate a few examples of attacks in Figure 3. For RS and “optimal” attacks, we report the best (lowest) attack reward obtained from different hyper-parameters.</p>
<p>Evaluation of ATLA In this experiment, we study the effectiveness of our proposed ATLA method. Specifically, we use PPO as our policy optimizer. For policy networks, we have two different structures: the original fully connected (MLP) structure, and an LSTM structure which takes historical observations. The LSTMs are trained using backpropagation through time for up to 100 steps. In Table 2 we include the following methods for comparisons:</p>
<ul>
<li>PPO (vanilla) and PPO (LSTM): PPO with a feedforward NN or LSTM as the policy network.</li>
<li>SA-PPO (<em>Zhang et al. (2020b)</em>): the state-of-the-art approach for improving the robustness of DRL in continuous control environments, using a smooth policy regularization on feedforward NNs solved by convex relaxations.</li>
<li>Adversarial training using critic attack (<em>Pattanaik et al. (2018)</em>): a previous work using critic based attack to generate adversarial observations in training time, and train a feedforward NN based agent with this relatively weak adversary.</li>
<li>ATLA-PPO (MLP) and ATLA-PPO (LSTM): Our proposed method trained with a feedforward NN (MLP) or LSTM as the policy network. The agent and adversary are trained using PPO with independent value and policy networks. For simplicity, we set $N_{\pi}=N_{\nu}=1$ in all settings.</li>
<li>ATLA-PPO (LSTM) +SA reg: Based on ATLA-PPO (LSTM), but with an extra adversarial smoothness constraint similar to those in SA-PPO. We use a 2-step stochastic gradient Langevin dynamics (SGLD) to solve the minimax loss, as convex relaxations of LSTMs are expensive.</li>
</ul>
<p>For each agent, we report its “natural reward” (episode reward without attacks) and best attack reward in Table 2. To comprehensively evaluate the robustness of agents, the best attack reward is the lowest episode reward achieved by all six types attacks in Table 1, including our new “optimal” attack (these attacks include hundreds of independent adversaries for attacking a single agent, see Appendix A.1 for more details). For reproducibility, for each setup we train 21 agents, attack all of them and report the one with median robustness. We include detailed hyperparameters in A.5.</p>
<p>In Table 2 we can see that vanilla PPO with MLP or LSTM are not robust. For feedforward (MLP) agent policies, critic based adversarial training (<em>Pattanaik et al. (2018)</em>) is not very effective under our suite of strong adversaries and is sometimes only slightly better than vanilla PPO. ATLA-PPO (MLP) outperforms SA-PPO on Hopper and Walker2d and is also competitive on HalfCheetah; for high dimensional environments like Ant, the robust function approximator regularization in SA-PPO is more effective. For LSTM agent policies, compared to vanilla PPO (LSTM) agents,</p>
<p>Table 2: Average episode rewards $\pm$ standard deviation over 50 episodes on ATLA agents and baselines. We report natural rewards (no attacks) and the best (lowest) attack rewards among six types of adversarial attacks, including a simple random noise attack, the critic based attack in <em>Pattanaik et al. (2018)</em>, MAD and RS attacks in <em>Zhang et al. (2020b)</em>, Snooping attack proposed in <em>Inkawhich et al. (2019)</em>, and the optimal attack proposed in this paper. For each environment, we bold the most robust agent. Since both RS attack and our “optimal” attack are parameterized attacks, the “best attack” column represents the worst case agent performance <em>under hundreds of adversaries</em>. See Appendix A.1 for more details.</p>
<table>
<thead>
<tr>
<th>Env.</th>
<th>State Dimension</th>
<th>$\ell_{\infty}$ norm perturbation budget $\epsilon$</th>
<th>Method</th>
<th>Natural Reward</th>
<th>Best Attack</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hopper</td>
<td>11</td>
<td>0.075</td>
<td>PPO (vanilla)</td>
<td>3167$\pm$542</td>
<td>636$\pm$9</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SA-PPO <em>Zhang et al. (2020b)</em></td>
<td>3705$\pm$2</td>
<td>1076$\pm$791</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td><em>Pattanaik et al. (2018)</em></td>
<td>2755$\pm$582</td>
<td>291$\pm$7</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>2559$\pm$958</td>
<td>976$\pm$40</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>PPO (LSTM)</td>
<td>3060$\pm$639.3</td>
<td>784$\pm$48</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>3487$\pm$452</td>
<td>1224$\pm$191</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM) +SA Reg</td>
<td>3291$\pm$600</td>
<td>1772$\pm$802</td>
</tr>
<tr>
<td>Walker2d</td>
<td>17</td>
<td>0.05</td>
<td>PPO (vanilla)</td>
<td>4472$\pm$635</td>
<td>1086$\pm$516</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SA-PPO <em>Zhang et al. (2020b)</em></td>
<td>4487$\pm$61</td>
<td>2908$\pm$1136</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td><em>Pattanaik et al. (2018)</em></td>
<td>4058$\pm$1410</td>
<td>733$\pm$1012</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>3138$\pm$1061</td>
<td>2213$\pm$915</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>PPO (LSTM)</td>
<td>2785$\pm$1121</td>
<td>1259$\pm$937</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>3920$\pm$129</td>
<td>5219$\pm$1132</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM) +SA Reg</td>
<td>3842$\pm$475</td>
<td>3239$\pm$894</td>
</tr>
<tr>
<td>Ant</td>
<td>111</td>
<td>0.15</td>
<td>PPO (vanilla)</td>
<td>5687$\pm$758</td>
<td>-872$\pm$436</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SA-PPO <em>Zhang et al. (2020b)</em></td>
<td>4292$\pm$384</td>
<td>2511$\pm$1117</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td><em>Pattanaik et al. (2018)</em></td>
<td>3469$\pm$1139</td>
<td>-672$\pm$100</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>4894$\pm$123</td>
<td>33$\pm$327</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>PPO (LSTM)</td>
<td>5696$\pm$165</td>
<td>-513$\pm$104</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>5612$\pm$130</td>
<td>716$\pm$256</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM) +SA Reg</td>
<td>5359$\pm$153</td>
<td>3765$\pm$101</td>
</tr>
<tr>
<td>HalfCheetah</td>
<td>17</td>
<td>0.15</td>
<td>PPO (vanilla)</td>
<td>7117$\pm$98</td>
<td>-660$\pm$218</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>SA-PPO <em>Zhang et al. (2020b)</em></td>
<td>3632$\pm$20</td>
<td>3028$\pm$23</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td><em>Pattanaik et al. (2018)</em></td>
<td>5241$\pm$1162</td>
<td>447$\pm$192</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>5417$\pm$49</td>
<td>2170$\pm$2097</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>PPO (LSTM)</td>
<td>5609$\pm$98</td>
<td>-886$\pm$30</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>5766$\pm$109</td>
<td>2485$\pm$1488</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>ATLA-PPO (LSTM) +SA Reg</td>
<td>6157$\pm$852</td>
<td>4806$\pm$603</td>
</tr>
</tbody>
</table>
<p>ATLA-PPO (LSTM) can significantly improve agent robustness; a LSTM agent trained without a robust training procedure like ATLA cannot improve robustness. We find that LSTM agents tend to be more robust than their MLP counterparts, validating our findings in Section 3.2. ATLA-PPO (LSTM) is better than SA-PPO on Hopper and Walker2d. In all settings, especially for high dimensional environments like Ant, our ATLA approach that also includes State-Adversarial regularization (ATLA-PPO +SA Reg) outperforms all other baselines, as this combination improves both the intrinsic robustness of policy and the robustness of function approximator.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: The performance under the strongest attack for SA-PPO Hopper with different regularization $\kappa$. Even we increase regularization, it cannot outperform our ATLA agents.</p>
<h4>A robust function approximator can be insufficient</h4>
<p>For some environments, SA-PPO method has its limitations - even using an increasingly larger regularization parameter $\kappa$ (which controls how robust the function approximator needs to be), we still cannot reach the same performance as our ATLA agent (Figure 4). Additionally, when a large regularization is used, agent performance becomes much worse. In Figure 4, under the largest $\kappa=1.0$, the natural reward (1436±96) is much lower than other agents reported in Table 2.</p>
<h2>5 CONCLUSION</h2>
<p>In this paper, we first propose the optimal adversarial attack on state observations of RL agents, which is significantly stronger than many existing adversarial attacks. We then show the alternating training with learned adversaries (ATLA) framework to train an agent together with a <em>learned optimal</em> adversary to effectively improve agent robustness under attacks. We also show that a history dependent policy parameterized by a LSTM can be helpful for robustness. Our approach is orthogonal to existing regularization based techniques, and can be combined with state-adversarial regularization to achieve state-of-the-art robustness under strong adversarial attacks.</p>
<h1>REFERENCES</h1>
<p>Karl J Astrom. Optimal control of markov processes with incomplete state information. Journal of mathematical analysis and applications, 10(1):174-205, 1965.</p>
<p>Kamyar Azizzadenesheli, Manish Kumar Bera, and Animashree Anandkumar. Trust region policy optimization for pomdps. arXiv preprint arXiv:1810.07900, 2018.</p>
<p>Haoyu Bai, David Hsu, and Wee Sun Lee. Integrated perception and planning in the continuous space: A pomdp approach. The International Journal of Robotics Research, 33(9):1288-1302, 2014.</p>
<p>Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition, pp. 262-275. Springer, 2017a.</p>
<p>Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it stronger. arXiv preprint arXiv:1712.09344, 2017b.</p>
<p>Rodney A Brooks. Artificial life and real robots. In Proceedings of the First European Conference on artificial life, pp. 3-10, 1992.</p>
<p>Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study on PPO and TRPO. arXiv preprint arXiv:2005.12729, 2020.</p>
<p>Marc Fischer, Matthew Mirman, and Martin Vechev. Online robustness training for deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019.</p>
<p>Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.</p>
<p>Adam Gleave, Michael Dennis, Neel Kant, Cody Wild, Sergey Levine, and Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.</p>
<p>Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.</p>
<p>Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Advances in neural information processing systems, pp. 3338-3346, 2014.</p>
<p>Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. arXiv preprint arXiv:1507.06527, 2015.</p>
<p>Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.</p>
<p>Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost signals. In International Conference on Decision and Game Theory for Security, pp. 217-237. Springer, 2019.</p>
<p>Matthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning. arXiv preprint arXiv:1905.11832, 2019.</p>
<p>Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2): 257-280, 2005.</p>
<p>Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life, pp. 704-720. Springer, 1995.</p>
<p>Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint arXiv:1705.06452, 2017.</p>
<p>Hanna Kurniawati, David Hsu, and Wee Sun Lee. Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces. In Robotics: Science and systems, volume 2008. Zurich, Switzerland., 2008.</p>
<p>Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.</p>
<p>Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4213-4220, 2019.</p>
<p>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748, 2017.</p>
<p>Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pp. 157-163. Elsevier, 1994.</p>
<p>Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement learning and control. In Advances in Neural Information Processing Systems, pp. 14570-14580, 2019.</p>
<p>Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. $I C L R, 2018$.</p>
<p>Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3932-3939. IEEE, 2017.</p>
<p>Daniel J Mankowitz, Timothy A Mann, Pierre-Luc Bacon, Doina Precup, and Shie Mannor. Learning robust options. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Daniel J Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. arXiv preprint arXiv:1906.07516, 2019.</p>
<p>Matthew Mirman, Marc Fischer, and Martin Vechev. Distilled agent DQN for provable adversarial robustness, 2018a. URL https://openreview.net/forum?id=ryeAy3AqYm.</p>
<p>Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In International Conference on Machine Learning, pp. 3575-3583, 2018b.</p>
<p>Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Fabio Muratore, Michael Gienger, and Jan Peters. Assessing transferability from simulation to reality for reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019 .</p>
<p>Arnab Nilim and Laurent El Ghaoui. Robustness in Markov decision problems with uncertain transition matrices. In Advances in Neural Information Processing Systems, pp. 839-846, 2004.</p>
<p>Takayuki Osogami. Robust partially observable Markov decision process. In International Conference on Machine Learning, pp. 106-115, 2015.</p>
<p>Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2040-2042. International Foundation for Autonomous Agents and Multiagent Systems, 2018.</p>
<p>Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2817-2826. JMLR. org, 2017.</p>
<p>Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley \&amp; Sons, 2014.</p>
<p>Xinghua Qu, Yew-Soon Ong, Abhishek Gupta, and Zhu Sun. Defending adversarial attacks without adversarial attacks in deep reinforcement learning. arXiv preprint arXiv:2008.06199, 2020.</p>
<p>Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning. arXiv preprint arXiv:2003.12909, 2020.</p>
<p>Mohammad Rasouli and Soroush Saghafian. Robust partially observable markov decision processes. No. RWP18-027, 2018.</p>
<p>Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement learning framework for autonomous driving. Electronic Imaging, 2017(19):70-76, 2017.</p>
<p>Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Advances in Neural Information Processing Systems 32, pp. 9832-9842. Curran Associates, Inc., 2019.</p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.</p>
<p>Zachary Sunberg and Mykel Kochenderfer. Online algorithms for pomdps with continuous state, action, and observation spaces. arXiv preprint arXiv:1709.06196, 2017.</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2013.</p>
<p>Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying reinforcement learning agents via action space adversarial training. In 2020 American Control Conference (ACC), pp. 3959-3964. IEEE, 2020.</p>
<p>Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. arXiv preprint arXiv:1901.09184, 2019.</p>
<p>Voyage. Introducing voyage deepdrive unlocking the potential of deep reinforcement learning. https://news.voyage.auto/introducing-voyage-deepdrive-69b3cf#f#be6, 2019.</p>
<p>Daan Wierstra, Alexander Foerster, Jan Peters, and Juergen Schmidhuber. Solving deep memory pomdps with recurrent policy gradients. In International Conference on Artificial Neural Networks, pp. 697-706. Springer, 2007.</p>
<p>Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5283-5292, 2018.</p>
<p>Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Bo Li, and Dawn Song. Characterizing attacks on deep reinforcement learning. arXiv preprint arXiv:1907.09470, 2019.</p>
<p>Kaidi Xu, Zhouxing Shi, Huan Zhang, Minlie Huang, Kai-Wei Chang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis on general computational graphs. arXiv preprint arXiv:2002.12920, 2020.</p>
<p>Haoqi Zhang and David C Parkes. Value-based policy teaching with active indirect elicitation. In AAAI, volume 8, pp. 208-214, 2008.</p>
<p>Haoqi Zhang, David C Parkes, and Yiling Chen. Policy teaching through reward function learning. In Proceedings of the 10th ACM conference on Electronic commerce, pp. 295-304, 2009.</p>
<p>Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint arXiv:1901.08573, 2019.</p>
<p>Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In NIPS, 2018.</p>
<p>Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. ICLR, 2020a.</p>
<p>Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on observations. In Advances in Neural Information Processing Systems, 2020b.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 FULL RESULTS OF ALL ENVIRONMENTS UNDER DIFFERENT TYPES OF ATTACKS</h2>
<p>In Table 2, we only include the best attack rewards (lowest rewards over all attacks). In Table 3 we list the rewards under each specific attack. Note that, Robust Sarsa (RS) attack and our "optimal" policy attack both have hyperparameters. For RS attack we use the same set of 30 different settings of hyperparameters as in (Zhang et al., 2020b) to train a robust value function to attack the network. The reported RS attack result for each agent is the strongest one over the 30 trained value functions. For Snooping based attack, we use the "imitator" attack proxy as it was the strongest reported in (Inkawhich et al., 2019), and we attack every step of the agent. The imitator is a MLP or LSTM network according to agent policy network. We use the same loss KL divergence function as in the MAD attacks for this Snooping attack. We first collect state-action pairs for 100 episodes to train the "imitators", whose network structures are the same as the corresponding agents. In the test time, we first run MAD attack on it the "imitator" and then input the generated perturbed observation to the agent in a transfer attack fashion. For our "optimal" policy attack, the hyperparameters are PPO training parameters for the adversary (including the learning rate of the adversary policy network, learning rate of the adversary value network, the entropy regularization parameter and the ratio clip $\epsilon$ for PPO). We use a grid search of these hyperparameters to train an adversary that is as strong as possible, resulting in 100 to 200 adversaries produced for each agent. The reported optimal attack rewards is the lowest reward among all trained adversaries. Under this comprehensive adversarial evaluation, each agent is tested using hundreds of adversaries and the strongest adversary determines the true robustness of an agent.</p>
<p>Table 3: Average episode rewards $\pm$ standard deviation over 50 episodes on five baselines and SAPP0 (Zhang et al., 2020b). We report natural episode rewards (no attacks) and episode rewards under six adversarial attacks, including a simple random noise attack, the critic based attack in (Pattanaik et al., 2018), MAD and RS attacks in Zhang et al. (2020b), Snooping attack proposed in Inkawhich et al. (2019), and the optimal attack proposed in this paper. In each row we bold the best (lowest) attack reward over all five attacks. The row for the most robust method is highlighted.</p>
<p>| Env. | $\lambda_{\text {in }}$ mean perturb- <br> ation badger $_{t}$ | Method | Natural <br> Reward | Attack Reward | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>Table 4: Natural and RS attack rewards of ATLA-PPO (LSTM)+ SA Reg checkpoints during training. We report Average rewards $\pm$ standard deviation over 50 episodes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Reward</th>
<th style="text-align: center;">$20 \%$</th>
<th style="text-align: center;">$40 \%$</th>
<th style="text-align: center;">$60 \%$</th>
<th style="text-align: center;">$80 \%$</th>
<th style="text-align: center;">$100 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">Natural Reward</td>
<td style="text-align: center;">$3440 \pm 11$</td>
<td style="text-align: center;">$1161 \pm 485$</td>
<td style="text-align: center;">$3013 \pm 584$</td>
<td style="text-align: center;">$3569 \pm 161$</td>
<td style="text-align: center;">$3291 \pm 600$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RS Attack Reward</td>
<td style="text-align: center;">$716 \pm 82$</td>
<td style="text-align: center;">$631 \pm 51$</td>
<td style="text-align: center;">$1089 \pm 501$</td>
<td style="text-align: center;">$3181 \pm 634$</td>
<td style="text-align: center;">$2244 \pm 618$</td>
</tr>
<tr>
<td style="text-align: center;">Walker2d</td>
<td style="text-align: center;">Natural Reward</td>
<td style="text-align: center;">$989 \pm 254$</td>
<td style="text-align: center;">$3506 \pm 174$</td>
<td style="text-align: center;">$2203 \pm 988$</td>
<td style="text-align: center;">$3803 \pm 726$</td>
<td style="text-align: center;">$3842 \pm 475$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RS Attack Reward</td>
<td style="text-align: center;">$882 \pm 269$</td>
<td style="text-align: center;">$1744 \pm 347$</td>
<td style="text-align: center;">$739 \pm 531$</td>
<td style="text-align: center;">$2550 \pm 1020$</td>
<td style="text-align: center;">$3239 \pm 894$</td>
</tr>
<tr>
<td style="text-align: center;">Ant</td>
<td style="text-align: center;">Natural Reward</td>
<td style="text-align: center;">$2634 \pm 1222$</td>
<td style="text-align: center;">$4532 \pm 106$</td>
<td style="text-align: center;">$5007 \pm 143$</td>
<td style="text-align: center;">$5127 \pm 542$</td>
<td style="text-align: center;">$5393 \pm 139$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RS Attack Reward</td>
<td style="text-align: center;">$216 \pm 171$</td>
<td style="text-align: center;">$1903 \pm 93$</td>
<td style="text-align: center;">$3040 \pm 241$</td>
<td style="text-align: center;">$3040 \pm 241$</td>
<td style="text-align: center;">$4136 \pm 149$</td>
</tr>
<tr>
<td style="text-align: center;">HalfCheetah</td>
<td style="text-align: center;">Natural Reward</td>
<td style="text-align: center;">$4525 \pm 140$</td>
<td style="text-align: center;">$5567 \pm 138$</td>
<td style="text-align: center;">$5955 \pm 177$</td>
<td style="text-align: center;">$5956 \pm 181$</td>
<td style="text-align: center;">$6300 \pm 261$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RS Attack Reward</td>
<td style="text-align: center;">$3986 \pm 564$</td>
<td style="text-align: center;">$3986 \pm 564$</td>
<td style="text-align: center;">$4911 \pm 923$</td>
<td style="text-align: center;">$4571 \pm 1314$</td>
<td style="text-align: center;">$4806 \pm 603$</td>
</tr>
</tbody>
</table>
<p>agent and the adversary. For LSTM agents, we use a single layer LSTM with 64 hidden neurons, along with an input embedding layer projecting state dimension to 64 and an output layer projecting 64 to output dimension. For LSTM agents, when conducting the "optimal" attack, we also use a LSTM network for the adversary to ensure the adversary is powerful enough.</p>
<h1>A. 4 HYPERPARAMETER FOR THE LEARNING-BASED "OPTIMAL" ATTACK</h1>
<p>Our "optimal" attacks uses policy gradient methods to learn the optimal adversary during agent testing, and each learning process involves the selection of hyperparameters. Specifically, the hyperparameters include the learning rates of the adversary's policy and value networks, the entropy coefficient, and the annealing of the learning rate. To reduce search space, for ATLA agents, the learning rates of the testing phase adversary's policy and value networks are chosen ranging from 0.3 X to 3 X of the learning rates of adversary's policy and value networks used in training. For other agents trained without an adversary, the learning rates of the testing phase adversary's policy and value networks are chosen ranging from 0.3 X to 3 X of the learning rates of the agent's policy and value networks. We tested both linearly annealed learning rate and non-annealing learning rate. The adversary's entropy coefficient is chosen form 0 and 0.003 . The final results reported in all tables are the best (lowest) reward achieved by the "optimal" attacks among all hyperparameter configurations. Typically this includes around 100 to 200 different adversaries trained with different hyperparameters. This guarantees the strength of this attack and allows a comprehensive evaluation of the robustness of all agents.</p>
<h2>A. 5 HYPERPARAMETERS FOR ATLA PERFORMANCE EVALUATION</h2>
<p>Hyperparameters for PPO (vanilla) For the Walker2d and Hopper environment, we use the same set of hyperparameters as in (Zhang et al., 2020b); the hyperparameters were originally from (Engstrom et al., 2020) and found using a grid search experiment. We found that this set of hyperparameters work well. For HalfCheetah and Ant environment, we use a grid search of hyperparameters, including the learning rate of the policy network, learning rate of the value network and the entropy bonus coefficient. For Hopper, Walker2d and HalfCheetah environments, we train for 2 million steps ( 2 million environment interactions). For Ant, we train for 10 million steps. Training for longer may slightly improve agent performance under no attacks, but has no impact for performance under strong adversarial attacks.</p>
<p>Hyperparameters for PPO (LSTM) For PPO (LSTM), we conduct a smaller scale hyperparameter search. We search hyperparameter values that are close to the optimal ones found for the PPO vanilla agent. We train these LSTM agents for the same steps as those in vanilla PPO.</p>
<p>Hyperparameters for SA-PPO We use the same value for all hyperparameters as in vanilla PPO except SA-PPO's extra $\kappa$ for the strength of SA-PPO regularization. For $\kappa$, we choose from $1 \times 10^{-6}$ to 1 . We train agents with each $\kappa 21$ times and choose the $\kappa$ value whose median agent has the highest worst-case reward under all attacks.</p>
<p>Hyperparameters for ATLA-PPO For ATLA-PPO, we have hyperparameters for both agent and adversary. We keep all agent hyperparameters the same as those in vanilla MLP/LSTM agents, except for the entropy bonus coefficient. We find that sometimes we need a larger entropy bonus co-</p>
<p>Table 5: Hyperparameters for all environments and settings. For vanilla environments, we use the hyperparameters from <em>Zhang et al. (2020b)</em> and <em>Engstrom et al. (2020)</em> if they are available for that environment (Hopper and Walker2d). Other environments’ hyperparameter for the vanilla PPO model is found by a grid search. For SA-PPO and ATLA-PPO (MLP), the same set of hyperparameters as in the vanilla models are used, except that for SA-PPO we tune the parameter $\kappa$ and for ATLA-PPO (MLP) we tune the entropy bonus coefficients as well as learning rates for the adversary. For LSTM models, we first tune the vanilla LSTM PPO models and find the best learning rates, keep using them in all LSTM based models.</p>
<table>
<thead>
<tr>
<th>Env.</th>
<th>model</th>
<th>policy lr</th>
<th>val lr</th>
<th>entropy coeff.</th>
<th>$\kappa$</th>
<th>adv. policy lr</th>
<th>adv. val lr</th>
<th>adv. entropy coeff.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hopper</td>
<td>PPO(vanilla)</td>
<td>3e-4</td>
<td>2.5e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>SA-PPO</td>
<td>3e-4</td>
<td>2.5e-4</td>
<td>0</td>
<td>0.03</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>3e-4</td>
<td>2.5e-4</td>
<td>0.01</td>
<td>–</td>
<td>0.001</td>
<td>0.0001</td>
<td>0.001</td>
</tr>
<tr>
<td></td>
<td>PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0.0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0.01</td>
<td>–</td>
<td>0.01</td>
<td>0.01</td>
<td>0.001</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)+ SA Reg</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0.01</td>
<td>0.3</td>
<td>0.003</td>
<td>0.01</td>
<td>0.003</td>
</tr>
<tr>
<td>Walker2d</td>
<td>PPO(vanilla)</td>
<td>4e-4</td>
<td>3e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>SA-PPO</td>
<td>4e-4</td>
<td>3e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>4e-4</td>
<td>3e-4</td>
<td>0.0003</td>
<td>–</td>
<td>0.0001</td>
<td>0.0001</td>
<td>0.002</td>
</tr>
<tr>
<td></td>
<td>PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-2</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-2</td>
<td>0.001</td>
<td>–</td>
<td>0.0003</td>
<td>0.03</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)+ SA Reg</td>
<td>1e-3</td>
<td>3e-2</td>
<td>0.001</td>
<td>0.3</td>
<td>0.003</td>
<td>0.03</td>
<td>0.001</td>
</tr>
<tr>
<td>Ant</td>
<td>PPO(vanilla)</td>
<td>5e-5</td>
<td>1e-5</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>SA-PPO</td>
<td>5e-5</td>
<td>1e-5</td>
<td>0</td>
<td>3e-3</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>5e-5</td>
<td>1e-5</td>
<td>3e-4</td>
<td>–</td>
<td>1e-05</td>
<td>3e-06</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>PPO (LSTM)</td>
<td>3e-4</td>
<td>3e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>3e-4</td>
<td>3e-4</td>
<td>0.0003</td>
<td>–</td>
<td>0.0003</td>
<td>0.0001</td>
<td>0.0003</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)+ SA Reg</td>
<td>3e-4</td>
<td>3e-4</td>
<td>0.003</td>
<td>0.1</td>
<td>0.0003</td>
<td>3e-05</td>
<td>3e-05</td>
</tr>
<tr>
<td>HalfCheetah</td>
<td>PPO(vanilla)</td>
<td>3e-4</td>
<td>1e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>SA-PPO</td>
<td>3e-4</td>
<td>1e-4</td>
<td>0.1</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (MLP)</td>
<td>3e-4</td>
<td>1e-4</td>
<td>0.0003</td>
<td>–</td>
<td>0.001</td>
<td>0.0003</td>
<td>0.003</td>
</tr>
<tr>
<td></td>
<td>PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0.0003</td>
<td>–</td>
<td>0.003</td>
<td>0.001</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>ATLA-PPO (LSTM)+ SA Reg</td>
<td>1e-3</td>
<td>3e-4</td>
<td>0</td>
<td>0.03</td>
<td>0.003</td>
<td>0.003</td>
<td>0.0003</td>
</tr>
</tbody>
</table>
<p>efficient in ATLA to allow sufficient exploration of the agent, as learning with an adversary is harder than learning in attack-free environments. For the adversary, we run a small-scale hyperparameter search on the learning rate of adversary policy and value networks, and the entropy bonus coefficient for the adversary. To reduce the number of hyperparameters for searching, we use values close to those of the agent. We set $N_{\nu}=N_{\pi}=1$ in all experiments and did not tune this hyperparameter. For ATLA, we train 5 million steps for Hopper, Walker and HalfCheetah and 10 million steps for Ant. We find that similar to the observations in <em>(Madry et al., 2018)</em>, training with an adversary typically requires more steps to converge, however in all our environments the training does reliably converge.</p>
<p>Agent selection For each setup, we repeat the experiments using the same set of hyperparameters for 21 times due to the high performance variance in RL. We then attack all the agents using random, critic, MAD and RS attacks. We use the lowest reward among all attacks as a metric to rank those agents. Then, we select the agent with <em>median</em> robustness as our final agent. This final agentt is then attacked using the “optimal” attack to further reduce its reward. The numbers we report in Table 2 are not from the best runs, but the runs with median robustness. This is done to improve reproducibility as RL training process can have high variance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code for the optimal attack and ATLA available at https://github.com/huanzhang12/ATLA_robust_RL&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
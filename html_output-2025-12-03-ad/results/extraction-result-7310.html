<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7310 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7310</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7310</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-280677717</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01911v2.pdf" target="_blank">Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7310.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7310.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned large language model used in this paper as the primary reasoning module and grader; integrated with a Python programming tool and multi-agent interpretation pipeline to convert free-form solutions into executable science models and interactive simulations for physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned; used with tool integration (Python programming tool) and within an agentic multi-agent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics (mechanics, electrodynamics, classical dynamics / textbook-level physics problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve textbook-level physics word problems, extract an explicit theory model, translate the theory into executable Python code, compute numerical results (e.g., maximum height with air resistance, electron acceleration in a shell, coefficient of friction), and provide interactive visualizations for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Used SciBench prompt templates for problem specification; tool-use prompting (invoked a Python programming tool); multi-agent interpretation pipeline prompting that includes a summarizer, theory model builder, code model builder following predefined templates, visualization/UI builder (Gradio template), and auxiliary tester for automated test-case generation; grading of theoretical consistency performed by a ChatGPT-4o grader.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numerical consistency (program execution agreement between constructed science model and original LLM output) and Theoretical consistency (categorical grader labels: highly consistent, moderately consistent, inconsistent, assigned by ChatGPT-4o grader).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Numerical consistency on a 50-problem subset: 46 consistent, 4 inconsistent (reported in Table 1) — i.e., 46/50 = 92% numerical consistency; Theoretical consistency: reported as 'ChatGPT-4o demonstrates a higher degree of theoretical consistency, with no instances classified as completely inconsistent' (exact category counts not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against ChatGPT-4o-mini as an alternative base model: ChatGPT-4o-mini numerical consistency reported as 47 consistent, 3 inconsistent (47/50 = 94%); theoretical-category comparison reported qualitatively (ChatGPT-4o rated higher theoretical consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Reasoning module type (naive vs tool-using vs agentic) influences interpretability and traceability', 'Use of external tools (Python programming tool) enabling program-execution verification', 'Model variant / capability (ChatGPT-4o vs ChatGPT-4o-mini)', 'Implicit vs explicit underlying theory models (LLMs often leave conceptual model implicit)', 'Quality of interpretation-module components (summarizer, theory model builder, code model builder, tester, visualization)', 'Problem complexity and numerical difficulty (some problems excluded from evaluation)', 'Errors arising from reasoning failures or incorrect numerical outcomes in LLM outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluation on a curated subset of 50 SciBench problems drawn from three textbooks (Fundamentals of Physics; Statistical Thermodynamics; Classical Dynamics of Particles and Systems), using the same SciBench prompt templates; LLM integrated with a Python programming tool for code generation and execution; interpretation agents follow predefined code/UI templates; theoretical consistency assessed by a ChatGPT-4o grader; interactive UI built with Gradio templates. No sampling/temperature/hyperparameter settings for the LLMs were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Observed failure modes include incorrect or inconsistent solutions due to flawed underlying science models, incorrect numerical outcomes, and opaque reasoning trajectories that hide conceptual errors; interpretation module can reveal but does not automatically correct underlying conceptual model errors; exact theoretical-consistency counts not fully enumerated in paper; some problems excluded (basic computations or incorrect reference solutions) which may bias reported consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7310.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7310.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller/faster variant of ChatGPT-4o used as an alternative base model for the interpretation-module agents; evaluated on the same SciBench-derived problem subset to compare numerical and theoretical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned; used within the same multi-agent interpretation pipeline (role as alternative base model for agents), likely tool-capable though the paper states integration experiments used ChatGPT-4o for the reasoning module</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics (textbook-level physics problems: mechanics, electrodynamics, classical dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Same as for ChatGPT-4o: transform LLM solutions into explicit theory models and executable Python simulations to compute numerical answers for textbook physics problems and support interactive validation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same SciBench prompt templates and interpretation pipeline templates (summarizer, theory model builder, code builder, visualization, tester); used as an alternative set of agent LLMs for constructing models and testing consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Numerical consistency (program execution agreement) and Theoretical consistency (graded categories assigned by ChatGPT-4o grader when applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Numerical consistency on the 50-problem subset: 47 consistent, 3 inconsistent (47/50 = 94% numerical consistency) as reported in Table 1. Theoretical consistency per-category counts not fully reported; paper indicates ChatGPT-4o had higher theoretical consistency overall.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Used as a baseline / comparison to ChatGPT-4o; numerical results slightly higher (47/50) than ChatGPT-4o (46/50) though theoretical consistency qualitatively favored ChatGPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model capability differences between ChatGPT-4o and ChatGPT-4o-mini (affect theoretical vs numerical consistency)', 'Impact of using an interpretation module (summarization, explicit theory extraction, code translation) on reproducibility and detection of errors', 'Problem selection and exclusion criteria (excludes trivial computations and incorrect reference solutions) affecting measured consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated on the same 50-problem subset from SciBench (three textbooks), using identical interpretation-module templates and program-execution checks; specific LLM hyperparameters (temperature, top-k) not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although numerical consistency was high, interpretation revealed cases where underlying conceptual models were flawed; the paper reports discrepancies and some incorrect numerical outcomes that require human expert follow-up; theoretical-consistency details are qualitative and not fully enumerated for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scibench, Evaluating college-level scientific problem solving abilities of large language models. <em>(Rating: 2)</em></li>
                <li>Using large language model to solve and explain physics word problems approaching human level. <em>(Rating: 2)</em></li>
                <li>Quantum many-body physics calculations with large language models. <em>(Rating: 2)</em></li>
                <li>Physics reasoner: Knowledge-augmented reasoning for solving physics problems with large language models. <em>(Rating: 2)</em></li>
                <li>Are large language models capable of physical reasoning? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7310",
    "paper_id": "paper-280677717",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "ChatGPT-4o",
            "name_full": "ChatGPT-4o",
            "brief_description": "An instruction-tuned large language model used in this paper as the primary reasoning module and grader; integrated with a Python programming tool and multi-agent interpretation pipeline to convert free-form solutions into executable science models and interactive simulations for physics problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4o",
            "model_size": null,
            "model_type": "instruction-tuned; used with tool integration (Python programming tool) and within an agentic multi-agent pipeline",
            "scientific_domain": "Physics (mechanics, electrodynamics, classical dynamics / textbook-level physics problems)",
            "simulation_task_description": "Solve textbook-level physics word problems, extract an explicit theory model, translate the theory into executable Python code, compute numerical results (e.g., maximum height with air resistance, electron acceleration in a shell, coefficient of friction), and provide interactive visualizations for validation.",
            "prompting_strategy": "Used SciBench prompt templates for problem specification; tool-use prompting (invoked a Python programming tool); multi-agent interpretation pipeline prompting that includes a summarizer, theory model builder, code model builder following predefined templates, visualization/UI builder (Gradio template), and auxiliary tester for automated test-case generation; grading of theoretical consistency performed by a ChatGPT-4o grader.",
            "evaluation_metric": "Numerical consistency (program execution agreement between constructed science model and original LLM output) and Theoretical consistency (categorical grader labels: highly consistent, moderately consistent, inconsistent, assigned by ChatGPT-4o grader).",
            "reported_accuracy": "Numerical consistency on a 50-problem subset: 46 consistent, 4 inconsistent (reported in Table 1) — i.e., 46/50 = 92% numerical consistency; Theoretical consistency: reported as 'ChatGPT-4o demonstrates a higher degree of theoretical consistency, with no instances classified as completely inconsistent' (exact category counts not provided).",
            "baseline_accuracy": "Compared against ChatGPT-4o-mini as an alternative base model: ChatGPT-4o-mini numerical consistency reported as 47 consistent, 3 inconsistent (47/50 = 94%); theoretical-category comparison reported qualitatively (ChatGPT-4o rated higher theoretical consistency).",
            "factors_reported": [
                "Reasoning module type (naive vs tool-using vs agentic) influences interpretability and traceability",
                "Use of external tools (Python programming tool) enabling program-execution verification",
                "Model variant / capability (ChatGPT-4o vs ChatGPT-4o-mini)",
                "Implicit vs explicit underlying theory models (LLMs often leave conceptual model implicit)",
                "Quality of interpretation-module components (summarizer, theory model builder, code model builder, tester, visualization)",
                "Problem complexity and numerical difficulty (some problems excluded from evaluation)",
                "Errors arising from reasoning failures or incorrect numerical outcomes in LLM outputs"
            ],
            "experimental_conditions": "Evaluation on a curated subset of 50 SciBench problems drawn from three textbooks (Fundamentals of Physics; Statistical Thermodynamics; Classical Dynamics of Particles and Systems), using the same SciBench prompt templates; LLM integrated with a Python programming tool for code generation and execution; interpretation agents follow predefined code/UI templates; theoretical consistency assessed by a ChatGPT-4o grader; interactive UI built with Gradio templates. No sampling/temperature/hyperparameter settings for the LLMs were reported.",
            "limitations_or_failure_modes": "Observed failure modes include incorrect or inconsistent solutions due to flawed underlying science models, incorrect numerical outcomes, and opaque reasoning trajectories that hide conceptual errors; interpretation module can reveal but does not automatically correct underlying conceptual model errors; exact theoretical-consistency counts not fully enumerated in paper; some problems excluded (basic computations or incorrect reference solutions) which may bias reported consistency.",
            "uuid": "e7310.0",
            "source_info": {
                "paper_title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ChatGPT-4o-mini",
            "name_full": "ChatGPT-4o-mini",
            "brief_description": "A smaller/faster variant of ChatGPT-4o used as an alternative base model for the interpretation-module agents; evaluated on the same SciBench-derived problem subset to compare numerical and theoretical consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4o-mini",
            "model_size": null,
            "model_type": "instruction-tuned; used within the same multi-agent interpretation pipeline (role as alternative base model for agents), likely tool-capable though the paper states integration experiments used ChatGPT-4o for the reasoning module",
            "scientific_domain": "Physics (textbook-level physics problems: mechanics, electrodynamics, classical dynamics)",
            "simulation_task_description": "Same as for ChatGPT-4o: transform LLM solutions into explicit theory models and executable Python simulations to compute numerical answers for textbook physics problems and support interactive validation.",
            "prompting_strategy": "Same SciBench prompt templates and interpretation pipeline templates (summarizer, theory model builder, code builder, visualization, tester); used as an alternative set of agent LLMs for constructing models and testing consistency.",
            "evaluation_metric": "Numerical consistency (program execution agreement) and Theoretical consistency (graded categories assigned by ChatGPT-4o grader when applicable).",
            "reported_accuracy": "Numerical consistency on the 50-problem subset: 47 consistent, 3 inconsistent (47/50 = 94% numerical consistency) as reported in Table 1. Theoretical consistency per-category counts not fully reported; paper indicates ChatGPT-4o had higher theoretical consistency overall.",
            "baseline_accuracy": "Used as a baseline / comparison to ChatGPT-4o; numerical results slightly higher (47/50) than ChatGPT-4o (46/50) though theoretical consistency qualitatively favored ChatGPT-4o.",
            "factors_reported": [
                "Model capability differences between ChatGPT-4o and ChatGPT-4o-mini (affect theoretical vs numerical consistency)",
                "Impact of using an interpretation module (summarization, explicit theory extraction, code translation) on reproducibility and detection of errors",
                "Problem selection and exclusion criteria (excludes trivial computations and incorrect reference solutions) affecting measured consistency"
            ],
            "experimental_conditions": "Evaluated on the same 50-problem subset from SciBench (three textbooks), using identical interpretation-module templates and program-execution checks; specific LLM hyperparameters (temperature, top-k) not provided.",
            "limitations_or_failure_modes": "Although numerical consistency was high, interpretation revealed cases where underlying conceptual models were flawed; the paper reports discrepancies and some incorrect numerical outcomes that require human expert follow-up; theoretical-consistency details are qualitative and not fully enumerated for this model.",
            "uuid": "e7310.1",
            "source_info": {
                "paper_title": "Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scibench, Evaluating college-level scientific problem solving abilities of large language models.",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problem_solving_abilities_of_large_language_models"
        },
        {
            "paper_title": "Using large language model to solve and explain physics word problems approaching human level.",
            "rating": 2,
            "sanitized_title": "using_large_language_model_to_solve_and_explain_physics_word_problems_approaching_human_level"
        },
        {
            "paper_title": "Quantum many-body physics calculations with large language models.",
            "rating": 2,
            "sanitized_title": "quantum_manybody_physics_calculations_with_large_language_models"
        },
        {
            "paper_title": "Physics reasoner: Knowledge-augmented reasoning for solving physics problems with large language models.",
            "rating": 2,
            "sanitized_title": "physics_reasoner_knowledgeaugmented_reasoning_for_solving_physics_problems_with_large_language_models"
        },
        {
            "paper_title": "Are large language models capable of physical reasoning?",
            "rating": 1,
            "sanitized_title": "are_large_language_models_capable_of_physical_reasoning"
        }
    ],
    "cost": 0.0097835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning
18 Aug 2025</p>
<p>Yinggan Xu 
University of California
Los Angeles</p>
<p>Hana Kimlee 
NSF Center for Quan-tum Network</p>
<p>Yijia Xiao 
University of California
Los Angeles</p>
<p>Di Luo <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#100;&#105;&#108;&#117;&#111;&#64;&#117;&#99;&#108;&#97;&#46;&#101;&#100;&#117;">&#100;&#105;&#108;&#117;&#111;&#64;&#117;&#99;&#108;&#97;&#46;&#101;&#100;&#117;</a>. 
University of California
Los Angeles</p>
<p>Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning
18 Aug 2025304374E81F6C1B6E6145D53F38D60A9DarXiv:2504.01911v2[cs.AI]
Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning.However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge.In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module.Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models.A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery.Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AIaugmented research.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become increasingly popular for tackling complex physics problems, emerging as valuable assistants to scientists (Zhang et al., 2024).However, interpreting the solutions they generate remains a sig-Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).</p>
<p>nificant challenge due to the inherent complexity of physics problems.Identifying potential flaws often demands substantial effort from experts, as LLM-generated solutions can obscure their underlying reasoning.</p>
<p>Several key issues contribute to this interpretability gap.First, the reasoning trajectories employed by LLMs are often highly complex and diverse.Depending on the inference techniques used, ranging from direct outputs to tool-assisted reasoning, the underlying processes may be partially hidden or require considerable effort to trace.Second, the numerical complexity involved in many physics problems poses a significant verification challenge, making it difficult for humans to independently validate the results.Third, the absence of an interpretable underlying mechanism can lead to seemingly correct outcomes even when the LLM's understanding of the physics is flawed.</p>
<p>To address these challenges, we develop a novel multi-agent LLM physicists framework that enhances the interpretability, transparency, and verifiability of LLM outputs in physics problem-solving.Unlike prior approaches that treat LLMs as black-box solvers, this framework decomposes the reasoning pipeline into three coordinated modules: a reasoning module, an interpretation module, and an AI-scientist interaction module.We propose an innovative LLM interpretation module, consist of a suite of specialized agents including summarizers, model builders, and testers, which translates opaque LLM outputs into structured, executable, and physically grounded science models.This interpretable interface bridges the gap between AI-generated reasoning and human scientific intuition by supporting validation through code execution, visual inspection, and human-in-the-loop critique.Extensive case studies on textbook-level problems from SciBench demonstrate the framework's ability to detect flaws, test consistency, and enable interactive validation, thereby offering a new pathway for interpretable, verifiable, and collaborative AI-assisted physics discovery.</p>
<p>Related Works</p>
<p>LLM for Physics</p>
<p>Researchers have begun exploring the potential of Large Language Models (LLMs) as reasoning tools in the physics domain (Anand et al., 2024;Ding et al., 2023;Pan et al., 2024;Pang et al., 2024;Wang et al., 2023b).Studies have demonstrated that LLMs can solve complex word problems requiring calculation and inference, often achieving near human-level accuracy, especially with effective prompting techniques such as few-shot learning using similar examples (Ding et al., 2023), leveraging reinforcement learning from human feedback (RLHF) (Anand et al., 2024) or implementing agentic system (Pang et al., 2024).</p>
<p>While much of this research focuses on general physics reasoning, recent efforts have applied LLMs to highly specialized domains.Pan et al. (Pan et al., 2024) demonstrated that GPT-4 can perform advanced theoretical derivations, such as deriving Hartree-Fock equations, highlighting LLMs' potential to automate and accelerate research workflows in theoretical physics.However, as most physics reasonings are complex and domain-specific, existing approaches offer limited support for human scientists to interpret and validate LLM-generated results.The lack of intuitive interfaces for understanding these outputs places a significant cognitive burden on researchers, limiting the practical usability of LLMs in scientific discovery.</p>
<p>Verifiable Generation</p>
<p>A parallel line of research focuses on improving the verifiability and interpretability of LLM outputs.One common approach involves grounding generated content in external sources and providing detailed citations (Hennigen et al., 2023;Shen et al., 2024;Li et al., 2024).Other methods enhance transparency by generating with more structured and intuitive processes (Cecchi &amp; Babkin, 2024) or enable self-explanatory reasoning (Huang et al., 2023).</p>
<p>However, physics reasoning differs fundamentally from tasks based purely on factual retrieval or general logical reasoning.Unlike citation-based fact-checking, physics problem-solving requires structured derivations, adherence to established theoretical frameworks, and quantitative validation.Despite advances in interpretable generation, the challenge of making LLM-generated physics reasoning both understandable and verifiable remains largely unexplored.</p>
<p>System Design</p>
<p>Building on prior research in LLM-assisted physics reasoning and verifiable AI generation, we propose an interpretation module that enhances both interpretability and validation in physics reasoning.We focus on physics reasoning within the context of problem-solving, which represents its most fundamental form.Our approach employs an agentic system composed of specialized agents, each with a distinct role in structuring the reasoning process.This inferenceagnostic pipeline can generate science models for a broad range of problem-solving scenarios, regardless of the implementation of the reasoning module.By explicitly modeling the reasoning process, our system deepens AI-scientist understanding, facilitating more transparent, interpretable, and verifiable AI-augmented scientific reasoning.To clearly articulate our approach, we structure our system into three key modules: a reasoning module, which processes physics problems using naive, tool-using, or agentic LLMs; an interpretation module, which refines AI reasoning into structured science models, executable code, and validation tools; and an AI-scientist interaction module, which facilitates human oversight by enabling experts to analyze, critique, and refine AI-generated reasoning.</p>
<p>LLM Reasoning Module: Establishing the Problem Context</p>
<p>The reasoning module serves as the entry point to the pipeline, handling diverse physics problems and their solutions from different sources, including: naive LLMs that generate direct, unstructured solutions, tool-using LLMs that incorporate computational resources to refine their responses, and agentic systems that coordinate multiple AI components for enhanced reasoning.While these reasoning modules can be powerful, they often involve complex, opaque processes that may not be fully visible to human scientists.For example, tool-using mechanisms or multi-agent debates can lead to solutions that are difficult to interpret, making it challenging to trace the reasoning behind the results.</p>
<p>LLM Interpretation Module: Structuring and Validating AI Reasoning</p>
<p>To enhance the interpretability and reliability of AIgenerated physics solutions, we introduce an interpretation module, which systematically structures AI reasoning into explicit, verifiable science models and provides intuitive feedback for human scientists.Our module refines raw AI outputs into structured representations, aligning them with scientific intuition and enabling validation through interactive tools and automated checks.</p>
<p>This module consists of specialized agents that structure reasoning, build executable models, and enhance human interpretability.</p>
<p>• LLM Summarizer The summarizer agent processes diverse inputs such as direct solutions, tool usage details, and chat history into a structured, concise format.By preserving core reasoning and reducing redundancy, this agent improves clarity and ensures smoother downstream processing for subsequent agents.</p>
<p>• LLM Model Builder To ensure interpretable physics generation, our approach explicitly constructs and val- idates the underlying science model, which is often implicit in solutions.This module consists of two key components:</p>
<p>Theory Model Builder: The correctness of an AIgenerated physics solution depends on the validity of its underlying conceptual model, which LLMs often leave implicit.This agent explicitly extracts, organizes, and refines the model by identifying key physical quantities, governing equations, and problem constraints.</p>
<p>It also uses gater agents classifies the problem type, invokes relevant idealized concepts (e.g.mass point in mechanics) for conceptual coherence.</p>
<p>Code Model Builder: Translating theory models into executable code is essential for validation and downstream applications of the theory model.This agent converts structured science models into computational processes, ensuring consistency between theoretical assumptions and computational implementation.</p>
<p>• Visualization Builder To support human intuitiondriven assessment, the visualization builder generates interactive representations of the coding model.This allows scientists to apply established validation techniques, such as testing extreme conditions and symmetry constraints, to assess solution consistency.</p>
<p>• LLM Auxiliary Tester While human scientists excel at verification, LLMs can assist this process by performing automated sanity checks like extreme case analysis, providing an additional layer of quality control.Though not a substitute for human judgment, this agent enhances the reliability of AI-generated solutions by identifying inconsistencies.</p>
<p>By structuring AI reasoning into explicit science models, executable simulations, and interactive validation tools, the interpretation module improves interpretability, verifiability, and alignment with scientific reasoning.</p>
<p>AI-Scientist Interaction: Fostering Collaborative Reasoning</p>
<p>Ultimately, our system is designed to augment-not replace-human scientific reasoning.The AI-scientist interaction module ensures that human experts remain central to the validation and refinement process by providing multiple touchpoints for engagement.Scientists can examine and verify the science model to explicitly assess AI reasoning, interact with the visualization interface to dynamically explore and test solutions, and critique AI-generated logic through intuitive representations.By fostering an interpretable reasoning process, this module ensures that AI remains an assistive tool that enhances scientific inquiry while preserving human oversight and expertise.</p>
<p>Case Study and Experiments</p>
<p>We demonstrate the effectiveness of our interpretation module using a mechanics problem from SciBench (Wang et al., 2023a).In this case, a potato is launched from a potato gun with air resistance, and the task requires an LLM to analyze the object's motion via the energy conservation law.For our experiments, we utilize ChatGPT-4o (Achiam et al., 2023) integrated with a Python programming tool as the reasoning module.We use the same prompt templates in SciBench for our reasoning module to solve this problem.</p>
<p>LLM Reasoning Module and Summarizer</p>
<p>Our workflow begins by refining the generated solution through a summarization step.The original inference trajectory includes complex details, including multiple code executions and internal thought processes, which can be difficult for human experts to interpret.Although the direct solution appears to be straightforward, its opaque derivation limits transparency and hinders scientific understanding by human .Our summarizer condenses both the final output and the inference trajectory into a structured form (see Fig.</p>
<p>Model Construction</p>
<p>Given a problem context and its summarized solution, the interpretation module constructs a corresponding science model in Python and generates an interactive user interface (UI) for scientists to inspect and validate the solution.</p>
<p>The theoretical model is aligned with the fundamental physics principles familiar to human scientists and serves as a reference for downstream model construction.The Pythonbased model enables reproduction of numerical results and facilitates modifications to test alternative conditions.The code model follows a predefined template to ensure consistency and a structured format for interpretation and execution.The built models are sent to the downstream agents for testing and user interface construction.We provide full demonstrations and more case studies in the appendix.</p>
<p>The science model and its interfaces are only practical for human scientists when they are faithful to the original reasoning result.To ensure that the science model and UI accurately reflect the original reasoning, we evaluate the consistency of our module using a subset of problems from the SciBench dataset.This subset contains problems from three textbooks: Fundamentals of Physics (Halliday et al., 2013), Statistical Thermodynamics (Engel &amp; Reid, 2010), and Classical Dynamics of Particles and Systems (Thornton &amp; Marion, 2021).For a meaningful assessment, we carefully selected 50 problems, excluding those that involve only basic computations or contain incorrect reference solutions.</p>
<p>We evaluate consistency on two key dimensions: Model Cons.Incons.ChatGPT-4o-mini 47 3 ChatGPT-4o 46 4</p>
<p>Table 1.Numerical Consistency of Different Base Models.</p>
<p>• Numerical Consistency: The science model should yield numerical results that agree with the original reasoning output.</p>
<p>• Theoretical Consistency: The constructed model should be physically coherent and correctly reflect the solution's underlying principles.</p>
<p>Numerical consistency is verified via program execution, while theoretical consistency is assessed by a ChatGPT-4o model acting as a grader.The grader classifies each solution into three categories: highly consistent, moderately consistent, or inconsistent.We evaluate our model builders using two different underlying LLMs for agents: ChatGPT-4o and ChatGPT-4o-mini.Table 1 summarizes the numerical consistency of the base models.Although most solutions are consistent, discrepancies-stemming from reasoning failures or incorrect numerical outcomes-provide valuable feedback for further investigation by human experts.</p>
<p>Table 2 presents the theoretical consistency results.ChatGPT-4o demonstrates a higher degree of theoretical consistency, with no instances classified as completely inconsistent.This suggests that LLMs can effectively structure physics problems into theory models for interpretability.</p>
<p>LLM Auxiliary Tester</p>
<p>In addition to generating solutions, the auxiliary tester enhances validation by automatically generating diverse test cases and analyzing their outcomes using the science model.Although LLM-generated test cases are common in software engineering (Tufano et al., 2020;Li et al., 2022), they also provide valuable insights when applied to science models.</p>
<p>Our experiments show that LLMs naturally adopt humanlike reasoning in test case generation, such as evaluating extreme scenarios.This enables them to provide more informative feedback beyond the science model and the interactive UI.</p>
<p>As depicted in Fig. 4, the tester agent uncovers partial flaws in the model by exploring various input conditions, by reconsidering the original input and tuning the initial velocities and the air resistance constant.The tester agent's conclusion well aligns with the ground truth that the solution was indeed incorrect due to an erroneous underlying science model.</p>
<p>Interactive UI</p>
<p>Inspired by previous work on enabling LLMs to generate user interfaces through coding (Wu et al., 2024), we introduce an interactive interface built using Gradio (Abid et al., 2019).The UI Builder agent converts the code model from the previous stage into an interactive interface, significantly reducing the effort required for validation, as shown in Fig. 5.This interface allows human scientists to develop intuition about the underlying science model.Similar to the code model, the UI Builder agent follows a predefined template to ensure stability and consistency.For our experiment, the UI Builder is prompted with a predefined Gradio (Abid et al., 2019) template as the starting point for UI.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we have presented a novel multi-agent LLM physicists framework with an interpretation module that enhances the interpretability and verifiability of LLMgenerated physics reasoning.By leveraging a multi-agent system including a summarizer, theory model builder, coding model builder, visualization builder, and auxiliary tester, we can transform complex LLM outputs into structured, transparent science models.Our case study on a SciBench mechanics problem demonstrated that this approach not only streamlines the reasoning process, but also empowers scientists to inspect, validate, and refine AI-generated solutions with ease.This integration of human-like test case generation and interactive validation bridges the gap between automated reasoning and human scientific intuition, marking a significant step toward more reliable AI-augmented reasoning.</p>
<p>Our future work will focus on extending our framework to encompass a broader range of physics domains and even other scientific fields.We aim to further refine each agent's capabilities, enhance the interactive elements of the UI, and integrate more sophisticated feedback loops between human experts and the system.Additional research will investigate scalability, the handling of increasingly complex models, and the integration of advanced techniques such as real-time interactive debugging and deeper reasoning transparency.These efforts are expected to foster better AI-Scientist understanding, ultimately paving the way for more trustworthy and effective AI-augmented reasoning.</p>
<p>A. Completion of Case Study in Section 4</p>
<p>A.1.Theory Model</p>
<p>In this problem, the potato is modeled as a mass point subject to two main forces: gravitational force and air resistance.The motion is treated as a projectile with damped motion (air resistance proportional to velocity).The assumptions are:</p>
<p>• Uniform gravitational field.</p>
<p>• Air resistance is linearly dependent on velocity.</p>
<p>• The system starts with a defined initial kinetic energy from the potato gun.</p>
<p>Equations and Explanations:</p>
<ol>
<li>Kinetic Energy Calculation:</li>
</ol>
<p>• Meaning: Represents the energy imparted to the potato.</p>
<p>• Equation:
KE initial = 1 2 mv 2 0
2. Work Done Against Air Resistance:</p>
<p>• Meaning: Energy lost as the potato overcomes air resistance.</p>
<p>• Equation: W resistance = m × v 0 3. Maximum Height Calculation:</p>
<p>• Meaning: Kinetic energy converts into gravitational potential energy.</p>
<p>• Equation:
h = KE initial − W resistance m × g A.2. Code Model import math
class PhysicsModel: def <strong>init</strong>(self): self.inputs= { "mass": "The mass of the potato in kg", "initial_velocity": "The initial velocity in m/s", "gravitational_acceleration": "Acceleration due to gravity in m/sˆ2", "resistance_constant": "Resistance constant k in sˆ-1" } self.outputs= { "maximum_height": "The maximum height reached in meters" } def compute(self, ** kwargs): m = kwargs.get("mass",0.5) v0 = kwargs.get("initial_velocity",120) g = kwargs.get("gravitational_acceleration",9.81) k = kwargs.get("resistance_constant",0. • Physical Meaning: According to Gauss's law, the electric field inside a conductor is zero.</p>
<p>• Equation:</p>
<p>E inside = 0</p>
<p>• Role in Solution: This principle determines that there should be no electric field affecting the electron inside the shell.</p>
<p>Force on Electron:</p>
<p>• Physical Meaning: With zero electric field, there is no electrostatic force on the electron.</p>
<p>• Equation:
F = e × E inside = 0
• Role in Solution: This shows that the electron experiences no electrostatic force inside the shell.</p>
<p>Electron Acceleration:</p>
<p>• Physical Meaning: With no force acting on the electron, there is no acceleration.</p>
<p>• Equation:
a = F m e = 0
• Role in Solution: This confirms that the electron moves with constant velocity inside the shell.</p>
<p>B.4. Code Model</p>
<p>import math class PhysicsModel: def <strong>init</strong>(self): self.inputs= { "sigma": "Surface charge density in C/mˆ2", "e": "Charge of electron in C", "m_e": "Mass of electron in kg", "epsilon_0": "Permittivity of free space in Cˆ2/(N mˆ2)" } self.outputs= { "E": "Electric field inside the shell in N/C", "F": "Force on the electron in N", "a": "Acceleration of the electron in m/sˆ2" } F ∥ = mg sin(θ)</p>
<p>• Role: Provides the energy that is converted into kinetic energy and work against friction.</p>
<ol>
<li>Frictional Force on the Incline:</li>
</ol>
<p>• Physical Meaning: This force opposes the skier's motion, proportional to the normal force.</p>
<p>• Equation: F friction, incline = µmg cos(θ)</p>
<p>• Role: Accounts for energy lost to friction as the skier descends.</p>
<ol>
<li>Energy Conservation and Work-Energy Principle:</li>
</ol>
<p>• Physical Meaning: Total mechanical energy loss equals work done by friction.</p>
<p>• Equation:
mgh = 1 2 mv 2 + F friction × (d 1 + d 2 )
• Role: Relates potential energy to energy dissipated by friction, enabling calculation of µ.</p>
<p>Expression for Height:</p>
<p>• Physical Meaning: Height h is the vertical displacement related to the initial gravitational potential energy.</p>
<p>• Equation: h = d 1 sin(θ)</p>
<p>• Role: Links the incline distance to potential energy in the energy conservation equation.</p>
<p>Equation for Coefficient of Kinetic Friction µ:</p>
<p>• Physical Meaning: Provides a direct relationship to calculate the coefficient µ.</p>
<p>• Equation:
µ = d 1 sin(θ) d 2 + d 1 cos(θ)
• Role: Solving this equation determines µ by equating gravitational energy conversion to energy dissipated by friction.</p>
<p>C.5. User Interface</p>
<p>Figure 1 .
1
Figure 1.An overview of the augmented reasoning with interpretation module.</p>
<p>Figure 2 .
2
Figure 2. Transformation of a directly generated solution into a summarized solution</p>
<p>Figure 3 .Figure 4 .
34
Figure 3.The model builder generates science models from summarized solutions, giving rise to interpretable reasoning</p>
<p>Figure 5 .
5
Figure 5.The interactive user interface enables intuitive feedback for human scientists</p>
<p>2 #
2
01) # Calculate initial kinetic energy KE_initial = 0.5 * m * v0 ** Calculate work done against air resistance (simplified model) .compute(** inputs)) B.3.Theory Model In this problem, we are examining the motion of an electron within a charged spherical metal shell.The key physical principles involve electrostatics and conductor behavior.The model considers: • Behavior of electric fields inside a conducting shell • Electrostatic forces on charged particles • Motion of an electron under electromagnetic forces Equations and Explanations 1. Electric Field Inside Conductor:</p>
<p>Figure 6 .
6
Figure 6.The interactive user interface for the Electrodynamic Problem</p>
<p>Figure 7 .
7
Figure 7.The interactive user interface for the Application Problem</p>
<p>Table 2 .
2
Theoretical Consistency of Different Base Models.</p>
<p>B. Demo of an Electrodynamic Problem B.1. Problem Context and Direct SolutionProblem Statement: In a spherical metal shell of radius R, an electron is shot from the center directly toward a tiny hole in the shell, through which it escapes.The shell is negatively charged with a surface charge density (charge per unit area) of 6.90 × 10 −13 C/m 2 .What is the magnitude of the electron's acceleration when it reaches radial distances r = 0.500R?Direct Solution: The magnitude of the electron's acceleration when it reaches a radial distance r = 0.500R is approximately 1.371 × 10 10 .The answer is therefore 13705705091.187 .B.2. Summarized SolutionTherefore, the solution to the problem is:1. Identify Constants:• Surface charge density, σ = 6.90 × 10 −13 C/mCalculate Electric Field:• The electric field inside the shell is given by E = σ/ϵ 0 .Calculate Force on the Electron:• The force experienced by the electron is F = e • E.Calculate Acceleration:• The acceleration of the electron is a = F/m e .Output the Result:• The calculated acceleration is approximately 1.371 × 10 10 m/s 2 .The code used to solve the problem is:B.6. Testing ResultsThe tester successfully identifies that the model is flawed by discovering the discrepancies between the reasoning trajectory and the model.C.2. Summarized SolutionThe solution involves:1. Energy Conservation:• Initial gravitational potential energy converts to kinetic energy and work against friction • Final kinetic energy is zero when the skier stops 2. Forces Analysis:• Gravitational force component:3. Work-Energy Balance:The numerical solution was computed using Python:C.3. Theory ModelThe model is based on energy conservation and the work-energy theorem, applied to a skier descending an inclined plane and coasting on a level surface.Gravitational potential energy converts into kinetic energy and work against friction.The skier is modeled as a rigid body with constant mass, influenced only by gravity and friction.The friction on both the incline and level snow is characterized by a constant coefficient of kinetic friction, µ, which is to be determined.Key Assumptions:1.The skier starts from rest (initial kinetic energy is zero).2. Friction is the only non-conservative force opposing the motion.3. Frictional force is proportional to the normal force, with a coefficient µ.4. The incline is uniform, and the transition to level snow involves no energy loss except for friction.def <strong>init</strong>(self): self.inputs= { "g": "The acceleration due to gravity in m/sˆ2", "d1": "The distance down the hill in meters", "d2": "The distance along the level snow in meters", "theta": "The angle of incline in degrees" } self.outputs= { "mu": "The coefficient of kinetic friction" } def list_inputs(self):""" List the inputs required for the physics model, along with their physics meaning """ return list(self.inputs.keys())def list_outputs(self):""" List the outputs of the physics model, along with their physics meaning """ return list(self.outputs.keys())def compute(self, ** kwargs):""" Compute the output of the physics model given the inputs Args: ** kwargs: The inputs to the physics model Returns: dict: The computed outputs of the physics model """ g = kwargs.get("g",9.81) d1 = kwargs.get("d1",100) d2 = kwargs.get("d2",70) theta = kwargs.get("theta",17) # Convert angle to radians for calculation theta_rad = math.radians(theta)# Calculate the coefficient of kinetic friction mu = (d1 * math.sin(theta_rad))/ (d2 + d1 * math.cos(theta_rad))# Format the answer to three decimal places mu_rounded = round(mu, 3) return {"mu": mu_rounded} # Example usage model = PhysicsModel() inputs = { "g": 9.81, "d1": 100, "d2": 70, "theta": 17 }C.6. Testing ResultsThe tester successfully confirms that the model and the reasoning are correct.
A Abid, A Abdalla, A Abid, D Khan, A Alfozan, J Zou, Gradio, arXiv:1906.02569Hassle-free sharing and testing of ml models in the wild. 2019arXiv preprint</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Enhancing llms for physics problem-solving using reinforcement learning with human-ai feedback. A Anand, K Prasad, C Kirtani, A R Nair, M Gupta, S Garg, A Gautam, S Buldeo, R R Shah, arXiv:2412.068272024arXiv preprint</p>
<p>Human-in-the-loop verifiable table-to-text generation. L Cecchi, P Babkin, Reportgpt, 10.18653/v1/2024.emnlp-industry.39Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track. F Dernoncourt, D Preot ¸iuc-Pietro, A Shimorina, the 2024 Conference on Empirical Methods in Natural Language Processing: Industry TrackMiami, Florida, USAssociation for Computational LinguisticsNovember 2024</p>
<p>Using large language model to solve and explain physics word problems approaching human level. J Ding, Y Cen, X Wei, arXiv:2309.081822023arXiv preprint</p>
<p>T Engel, P Reid, Statisticalˆthermodynamics, t Kinetics. New YorkPrentice Hall2010</p>
<p>Fundamentals of physics. D Halliday, R Resnick, J Walker, 2013John Wiley &amp; Sons</p>
<p>L T Hennigen, S Shen, A Nrusimha, B Gapp, D Sontag, Y Kim, arXiv:2311.09188Towards verifiable text generation with symbolic references. 2023arXiv preprint</p>
<p>Can large language models explain themselves? a study of llm-generated self-explanations. S Huang, S Mamidanna, S Jangam, Y Zhou, L H Gilpin, 2023</p>
<p>Towards trustworthy document assistant chatbot with reliable attribution. D Li, X Hu, Z Sun, B Hu, S Ye, Z Shan, Q Chen, M Zhang, Truthreader, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2024</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Dal Lago, Science. 37866242022</p>
<p>Quantum many-body physics calculations with large language models. H Pan, N Mudur, W Taranto, M Tikhanovskaya, S Venugopalan, Y Bahri, M P Brenner, E.-A Kim, 2024</p>
<p>X Pang, R Hong, Z Zhou, F Lv, X Yang, Z Liang, B Han, C Zhang, arXiv:2412.13791Physics reasoner: Knowledgeaugmented reasoning for solving physics problems with large language models. 2024arXiv preprint</p>
<p>J Shen, T Zhou, Y Chen, K Liu, Citekit, arXiv:2408.04662A modular toolkit for large language model citation generation. 2024arXiv preprint</p>
<p>S T Thornton, J B Marion, Classical Dynamics of Particles and Systems. Cengage Learning. Boston2021</p>
<p>Unit test case generation with transformers and focal context. M Tufano, D Drain, A Svyatkovskiy, S K Deng, N Sundaresan, arXiv:2009.056172020arXiv preprint</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, Scibench, arXiv:2307.10635Evaluating college-level scientific problemsolving abilities of large language models. 2023aarXiv preprint</p>
<p>Y R Wang, J Duan, D Fox, S Srinivasa, Newton, arXiv:2310.07018Are large language models capable of physical reasoning?. 2023barXiv preprint</p>
<p>Uicoder: Finetuning large language models to generate user interface code through automated feedback. J Wu, E Schoop, A Leung, T Barik, J P Bigham, J Nichols, 2024</p>
<p>Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, arXiv:2406.10833A comprehensive survey of scientific large language models and their applications in scientific discovery. 2024arXiv preprint</p>
<p>Incorrect electric field calculation inside conductor 2. Erroneous force computation. </p>
<p>Update acceleration computations accordingly outputs = model.compute( ** inputs) print(outputs. 3</p>            </div>
        </div>

    </div>
</body>
</html>